{
    "title": "SJJySbbAZ",
    "content": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics.   Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam. Generative Adversarial Networks (GANs) BID4 have proven a very successful approach for fitting generative models in complex structured spaces, such as distributions over images. GANs frame the question of fitting a generative model from a data set of samples from some distribution as a zero-sum game between a Generator (G) and a discriminator (D). The Generator is represented as a deep neural network which takes as input random noise and outputs a sample in the same space of the sampled data set, trying to approximate a sample from the underlying distribution of data. The discriminator, also modeled as a deep neural network is attempting to discriminate between a true sample and a sample generated by the generator. The hope is that at the equilibrium of this zero-sum game the generator will learn to generate samples in a manner that is indistinguishable from the true samples and hence has essentially learned the underlying data distribution. Despite their success at generating visually appealing samples when applied to image generation tasks, GANs are very finicky to train. One particular problem, raised for instance in a recent survey as a major issue BID5 is the instability of the training process. Typically training of GANs is achieved by solving the zero-sum game via running simultaneously a variant of a Stochastic Gradient Descent algorithm for both players (potentially training the discriminator more frequently than the generator).The latter amounts essentially to solving the zero-sum game via running no-regret dynamics for each player. However, it is known from results in game theory, that no-regret dynamics in zerosum games can very often lead to limit oscillatory behavior, rather than converge to an equilibrium. Even in convex-concave zero-sum games it is only the average of the weights of the two players that constitutes an equilibrium and not the last-iterate. In fact recent theoretical results of Mertikopoulos et al. (2017) show the strong result that no variant of GD that falls in the large class of Follow-theRegularized-Leader (FTRL) algorithms can converge to an equilibrium in terms of the last-iterate and are bound to converge to limit cycles around the equilibrium. Averaging the weights of neural nets is a prohibitive approach in particular because the zero-sum game that is defined by training one deep net against another is not a convex-concave zero-sum game. Thus it seems essential to identify training algorithms that make the last iterate of the training be very close to the equilibrium, rather than only the average. Contributions. In this paper we propose training GANs, and in particular Wasserstein GANs BID1 , via a variant of gradient descent known as Optimistic Mirror Descent. Optimistic Mirror Descent (OMD) takes advantage of the fact that the opponent in a zero-sum game is also training via a similar algorithm and uses the predictability of the strategy of the opponent to achieve faster regret rates. It has been shown in the recent literature that Optimistic Mirror Descent and its generalization of Optimistic Follow-the-Regularized-Leader (OFTRL), achieve faster convergence rates than gradient descent in convex-concave zero-sum games (Rakhlin & Sridharan, 2013a; b) and even in general normal form games (Syrgkanis et al., 2015) . Hence, even from the perspective of faster training, OMD should be preferred over GD due to its better worst-case guarantees and since it is a very small change over GD.Moreover, we prove the surprising theoretical result that for a large class of zero-sum games (namely bi-linear games), OMD actually converges to an equilibrium in terms of the last iterate. Hence, we give strong theoretical evidence that OMD can help in achieving the long sought-after stability and last-iterate convergence required for GAN training. The latter theoretical result is of independent interest, since solving zero-sum games via no-regret dynamics has found applications in many areas of machine learning, such as boosting BID2 . Avoiding limit cycles in such approaches could help improve the performance of the resulting solutions. We complement our theoretical result with toy simulations that portray exactly the large qualitative difference between OMD as opposed to GD (and its many variants, including gradient penalty, momentum, adaptive step size etc.). We show that even in a simple distribution learning setting where the generator simply needs to learn the mean of a multi-variate distribution, GD leads to limit cycles, while OMD converges pointwise. Moreover, we give a more complex application to the problem of learning to generate distributions of DNA sequences of the same cellular function. DNA sequences that carry out the same function in the genome, such as binding to a specific transcription factor, follow the same nucleotide distribution. Characterizing the DNA distribution of different cellular functions is essential for understanding the functional landscape of the human genome and predicting the clinical consequence of DNA mutations (Zeng et al., 2015; 2016; Zeng & Gifford, 2017) . We perform a simulation study where we generate samples of DNA sequences from a known distribution. Subsequently we train a GAN to attempt to learn this underlying distribution. We show that OMD achieves consistently better performance than GD variants in terms of the Kullback-Leibler (KL) divergence between the distribution learned by the Generator and the true distribution. Finally, we apply optimism to training GANs for images and introduce the Optimistic Adam algorithm. We show that it achieves better performance than Adam, in terms of inception score, when trained on CIFAR10. We consider the problem of learning a generative model of a distribution of data points Q \u2208 \u2206(X). Our goal is given a set of samples from D, to learn an approximation to the distribution Q in the form of a deep neural network G \u03b8 (\u00b7), with weight parameters \u03b8, that takes as input random noise z \u2208 F (from some simple distribution F ) and outputs a sample G \u03b8 (z) \u2208 X. We will focus on addressing this problem via a Generative Adversarial Network (GAN) training strategy.faster convergence rates to equilibrium of = O 1 T for the average of parameters. The algorithm essentially uses the last iterations gradient as a predictor for the next iteration's gradient. This follows from the intuition that if the opponent in the game is using a stable (or regularized) algorithm, then the gradients between the two iterations will not change much. Later Syrgkanis et al. (2015) showed that this intuition extends to show faster convergence of each individual player's regret in general normal form games. Given these favorable properties of OMD when learning in games, we propose replacing GD with OMD when training WGANs. The update rule of a OMD is a small adaptation to GD. OMD is parameterized by a predictor of the next iteration's gradient which could either be simply last iteration's gradient or an average of a window of last gradient or a discounted average of past gradients. In the case where the predictor is simply the last iteration gradient, then the update rule for OMD boils down to the following simple form: DISPLAYFORM0 The simple modification in the GD update rule, is inherently different than any of the existing adaptations used in GAN training, such as Nesterov's momentum, or gradient penalty. General OMD and intuition. The intuition behind OMD can be more easily understood when GD is viewed through the lens of the Follow-the-Regularized-Leader formulation. In particular, it is well known that GD is equivalent to the Follow-the-Regularized-Leader algorithm with an 2 regularizer (see e.g. Shalev-Shwartz (2012)), i.e.: DISPLAYFORM1 It is known that if the learner knew in advance the gradient at the next iteration, then by adding that to the above optimization would lead to constant regret that comes solely from the regularization term 1 . OMD essentially augments FTRL by adding a predictor M t+1 of the next iterations gradient, i.e.: DISPLAYFORM2 For an arbitrary set of predictors, the latter boils down to the following set of update rules: DISPLAYFORM3 In the theoretical part of the paper we will focus on the case where the predictor is simply the last iteration gradient, leading to update rules in Equation (5). In the experimental section we will also explore performance of other alternatives for predictors. In practice we don't really have access to the true distribution Q and hence we replace Q with an empirical distribution Q n over samples {x 1 , . . . , x n } and F n of random noise samples {z 1 , . . . , z n }, leading to empirical loss for the zero-sum game of: DISPLAYFORM0 Even in this setting it might be impractical to compute the gradient of the expected loss with respect to Q n or F n , e.g. DISPLAYFORM1 1 The latter is a consequence of the be-the-leader lemma Kalai & Vempala (2005); Rigollet (2015) However, GD and OMD still leads to small loss if we replace gradients with unbiased estimators of them. Hence, we can replace expectation with respect to Q n or F n , by simply evaluating the gradients at a single sample or on a small batch of B samples. Hence, we can replace the gradients at each iteration with the variants: DISPLAYFORM2 Replacing \u2207 w,t and \u2207 \u03b8,t with the above estimates in Equation FORMULA39 and FORMULA0 We consider the following very simple WGAN example: The data are generated by a multivariate normal distribution, i.e. Q N (v, I) for some v \u2208 R d . The goal is for the generator to learn the unknown parameter v. In Appendix C we also consider a more complex example where the generator is trying to learn a co-variance matrix. We consider a WGAN, where the discriminator is a linear function and the generator is a simple additive displacement of the input noise z, which is drawn from F N (0, I), i.e: DISPLAYFORM3 The goal of the generator is to figure out the true distribution, i.e. to converge to \u03b8 = v. The WGAN loss then takes the simple form: DISPLAYFORM4 We first consider the case where we optimize the true expectations above rather than assuming that we only get samples of x and samples of z. Due to linearity of expectation, the expected zero-sum game takes the form: inf DISPLAYFORM5 We see here that the unique equilibrium of the above game is for the generator to choose \u03b8 = v and for the discriminator to choose w = 0. For this simple zero sum game, we have \u2207 w,t = v \u2212 \u03b8 t and \u2207 \u03b8,t = \u2212w t . Hence, the GD dynamics take the form: DISPLAYFORM6 while the OMD dynamics take the form: DISPLAYFORM7 We simulated simultaneous training in this zero-sum game under the GD and under OMD dynamics and we find that GD dynamics always lead to a limit cycle irrespective of the step size or other modifications. In Figure 1 we present the behavior of the GD vs OMD dynamics in this game for v = (3, 4). We see that even though GD dynamics leads to a limit cycle (whose average does indeed equal to the true vector), the OMD dynamics converge to v in terms of the last iterate. In FIG1 we see that the stability of OMD even carries over to the case of Stochastic Gradients, as long as the batch size is of decent size. In the appendix we also portray the behavior of the GD dynamics even when we add gradient penalty (Gulrajani et al., 2017) to the game loss (instead of weight clipping), adding Nesterov momentum to the GD update rule (Nesterov, 1983) or when we train the discriminator multiple times in between a train iteration of the generator. We see that even though these modifications do improve the stability (a) GD dynamics.(b) OMD dynamics. Figure 1: Training GAN with GD converges to a limit cycle that oscilates around the equilibrium (we applied weight-clipping at 10 for the discriminator). On the contrary training with OMD converges to equilibrium in terms of last-iterate convergence. of the GD dynamics, in the sense that they narrow the band of the limit cycle, they still lead to a non-vanishing limit cycle, unlike OMD.In the next section, we will in fact prove formally that for a large class of zero-sum games including the one presented in this section, OMD dynamics converge to equilibrium in the sense of last-iterate convergence, as opposed to average-iterate convergence. In this section, we show that Optimistic Mirror Descent exhibits final-iterate, rather than only average-iterate convergence to min-max solutions for bilinear functions. More precisely, we consider the problem min x max y x T Ay, for some matrix A, where x and y are unconstrained. In Appendix D, we also show that our convergence result appropriately extends to the general case, where the bi-linear game also contains terms that are linear in the players' individual strategies, i.e. games of the form: inf DISPLAYFORM0 In the simpler min x max y x T Ay problem, Optimistic Mirror Descent takes the following form, for all t \u2265 1: DISPLAYFORM1 DISPLAYFORM2 Initialization: For the above iteration to be meaningful we need to specify x 0 , x \u22121 , y 0 , y \u22121 . We choose any x 0 \u2208 R(A), and y 0 \u2208 R(A T ), and set x \u22121 = 2x 0 and y \u22121 = 2y 0 , where R(\u00b7) represents the column space of A. In particular, our initialization means that the first step taken by the dynamics gives x 1 = x 0 and y 1 = y 0 .We will analyze Optimistic Mirror Descent under the assumption \u03bb \u221e \u2264 1, where \u03bb \u221e = max{||A||, ||A T ||} and || \u00b7 || denotes spectral norm of matrices. We can always enforce that \u03bb \u221e \u2264 1 by appropriately scaling A. Scaling A by some positive factor clearly does not change the min-max solutions (x * , y * ), only scales the optimal value x * T Ay * by the same factor. We remark that the set of equilibrium solutions of this minimax problem are pairs (x, y) such that x is in the null space of A T and y is in the null space of A. In this section we rigorously show that Optimistic Mirror Descent converges to the set of such min-max solutions. This is interesting in light of the fact that Gradient Descent actually diverges, even in the special case where A is the identity matrix, as per the following proposition whose proof is provided in Appendix D.3. Proposition 1. Gradient descent applied to the problem min x max y x T y diverges starting from any initialization x 0 , y 0 such that x 0 , y 0 = 0.Next, we state our main result of this section, whose proof can be found in Appendix D, where we also state its appropriate generalization to the general case (14).Theorem 1 (Last Iterate Convergence of OMD). Consider the dynamics of Eq. (15) and (16) and any initialization 1 2 x \u22121 = x 0 \u2208 R(A), and DISPLAYFORM3 where for a matrix X we denote by X + its generalized inverse and by ||X|| its spectral norm. Suppose that A \u2261 \u03bb \u221e \u2264 1 and that \u03b7 is a small enough constant satisfying \u03b7 < 1/(3\u03b3 2 ). Letting DISPLAYFORM4 , the OMD dynamics satisfy the following: DISPLAYFORM5 In particular, \u2206 t \u2192 O(\u03b7\u03b3 2 \u2206 0 ), as t \u2192 +\u221e, and for large enough t, the last iterate of OMD is DISPLAYFORM6 distance from the space of equilibrium points of the game, where \u221a \u2206 0 is the distance of the initial point (x 0 , y 0 ) from the equilibrium space, and where both distances are taken with respect to the norm x T AA T x + y T A T Ay. We take our theoretical intuition to practice, applying OMD to the problem of generating DNA sequences from an observed distribution of sequences. DNA sequences that carry out the same function can be viewed as samples from some distribution. For many important cellular functions, this distribution can be well modeled by a position-weight matrix (PWM) that specifies the probability of different nucleotides occuring at each position (Stormo, 2000) . Thus, training GANs from DNA sequences sampled from a PWM distribution serves as a practically motivated problem where we know the ground truth and can thus quantify the performance of different training methods in terms of the KL divergence between the trained generator distribution and the true distribution. In our experiments, we generated 40,000 DNA sequences of six nucleotides according to a given position weight matrix. A random 10% of the sequences were held out as the validation set. Each sequence was then embedded into a 4 \u00d7 6 matrix by encoding each of the four nucleotides with an one-hot vector. On this dataset, we trained WGANs with different variants of OMD and SGD and evaluated their performance in terms of the KL divergence between the empirical distribution of the WGAN-generated samples and the true distribution described by the position weight matrix. Both the discriminator and generator of the WGAN used in this analysis were chosen to be convolutional neural networks (CNN), given the recent success of CNNs in modeling DNA-protein binding (Zeng et al., 2016; BID0 . The detailed structure of the chosen CNNs can be found in Appendix E.To account for the impact of learning rate and training epochs, we explored two different ways of model selection when comparing different optimization strategies: (1) using the iteration and learning rate that yields the lowest discriminator loss on the held out test set. This is inspired by the observation in BID1 that the discriminator loss negatively correlates with the quality of the generated samples. (2) using the model obtained after the last epoch of the training. To account for the stochastic nature of the initialization and optimizers, we trained 50 independent models for each learning rate and optimizer, and compared the optimizer strategies by the resulting distribution of KL divergences across 50 runs. For GD, we used variants of Equation FORMULA39 to examine the effect of using momentum and an adaptive step size. Specifically, we considered momentum, Nesterov momentum and Adagrad. The specific form of all these modifications is given for reference in Appendix A.For OMD we used the general predictor version of Equation FORMULA6 with a fixed step size and with the following variants of the next iteration predictor M t+1 : (v1) Last iteration gradient: M t+1 = \u2207f t , (v2) Running average of past gradients: M t+1 = 1 t t i=1 \u2207f i , (v3) Hyperbolic discounted average of past gradients: M t+1 = \u03bbM t + (1 \u2212 \u03bb)\u2207f t , \u03bb \u2208 (0, 1). We explored two training schemes: (1) training the discriminator 5 times for each generator training as suggest in BID1 . FORMULA39 training the discriminator once for each generator training. The latter is inline with the intuition behind the use of optimism: optimism hinges on the fact that the gradient at the next iteration is very predictable since it is coming from another regularized algorithm, and if we train the other algorithm multiple times, then the gradient is not that predictable and the benefits of optimism are lost. where we don't combine the models trained with different learning rates, the learning rate is appended at the end of the method name. For momentum and Nesterov momentum, we used \u03b3 = 0.9. For Adagrad, we used the default = 1e \u22128 .For all afore-described algorithms, we experimented with their stochastic variants. FIG2 shows the KL divergence between the WGAN-generated samples and the true distribution. When evaluated by the epoch and learning rate that yields the lowest discriminator loss on the validation set, WGAN trained with Stochastic OMD (SOMD) achieves lower KL divergence than the competing SGD variants. Evaluated by the last epoch, the best performance across different learning rates is achieved by optimistic Adam (see Section 6). We note that in both metrics, SOMD with 1:1 generatordiscriminator training ratio yields better KL divergence than the alternative training scheme (1:5 ratio), which validates the intuition behind the use of optimism. In this section we applying optimistic WGAN training to generating images, after training on CI-FAR10. Given the success of Adam on training image WGANs we will use an optimistic version of the Adam algorithm, rather than vanilla OMD. We denote the latter by Optimistic Adam. Optimistic Adam could be of independent interest even beyond training WGANs. We present Optimistic Adam for (G) but the analog is also used for training (D). We trained on CIFAR10 images with Algorithm 1 Optimistic ADAM, proposed algorithm for training WGANs on images. Parameters: stepsize \u03b7, exponential decay rates for moment estimates \u03b2 1 , \u03b2 2 \u2208 [0, 1), stochastic loss as a function of weights t (\u03b8), initial parameters \u03b8 0 for each iteration t \u2208 {1, . . . , T } do Compute stochastic gradient: \u2207 \u03b8,t = \u2207 \u03b8 t (\u03b8) Update biased estimate of first moment: DISPLAYFORM0 Update biased estimate of second moment: DISPLAYFORM1 Compute bias corrected first moment: DISPLAYFORM2 Return \u03b8 T Optimistic Adam with the hyper-parameters matched to Gulrajani et al. (2017) , and we observe that it outperforms Adam in terms of inception score (see FIG4 , a standard metric of quality of WGANs (Gulrajani et al., 2017; Salimans et al., 2016) . In particular we see that optimistic Adam achieves high numbers of inception scores after very few epochs of training. We observe that for Optimistic Adam, training the discriminator once after one iteration of the generator training, which matches the intuition behind the use of optimism, outperforms the 1:5 generator-discriminator training scheme. We see that vanilla Adam performs poorly when the discriminator is trained only once in between iterations of the generator training. Moreover, even if we use vanilla Adam and train 5 times (D) in between a training of (G), as proposed by BID1 , then performance is again worse than Optimistic Adam with a 1:1 ratio of training. The same learning rate 0.0001 and betas (\u03b2 1 = 0.5, \u03b2 2 = 0.9) as in Appendix B of Gulrajani et al. (2017) were used for all the methods compared. We also matched other hyper-parameters such as gradient penalty coefficient \u03bb and batch size. For a larger sample of images see Appendix G. For ease of reference we briefly describe the exact form of update rules for several modifications of GD training that we have used in our experimental results. Adagrad: DISPLAYFORM0 Momentum: DISPLAYFORM1 Nesterov momentum: DISPLAYFORM2 In FIG5 we portray example Gradient Descent dynamics in the illustrative example described in Section 3 under multiple adaptations proposed in the literature. We observe that oscillations persist in all such modified GD dynamics, though alleviated by some. We briefly describe the modifications in detail first. Gradient penalty. The Wasserstein GAN is based on the idea that the discriminator is approximating all 1-Lipschitz functions of the data. Hence, when training the discriminator we need to make sure that the function D w (x) has a bounded gradient with respect to x. One approach to achieving this is weight-clipping, i.e. clipping the weights to lie in some interval. However, the latter might introduce extra instability during training. Gulrajani et al. (2017) introduce an alternative approach by adding a penalty to the loss function of the zero-sum game that is essentially the 2 norm of the gradient of D w (x) with respect to x. In particular they propose the following regularized WGAN loss: DISPLAYFORM0 where Q is the distribution of the random vector x + (1 \u2212 )G(z) when x \u223c Q and z \u223c F . The expectations in the latter can also be replaced with sample estimates in stochastic variants of the training algorithms. For our simple example, \u2207 x D w (x) = w. Hence, we get the gradient penalty modified WGAN: DISPLAYFORM1 Hence, the gradient of the modified loss function with respect to \u03b8 remains unchanged, but the gradient with respect to w becomes: DISPLAYFORM2 Momentum. GD with momentum was defined in Equation (18). For the case of the simple illustrative example, these dynamics boil down to: DISPLAYFORM3 Nesterov momentum. GD with Nesterov's momentum was defined in Equation (19). For the illustrative example, we see that Nesterov's momentum is identical to momentum in the absence of gradient penalty. The reason being that the function is bi-linear. However, with a gradient penalty, Nesterov's momentum boils down to the following update rule. DISPLAYFORM4 Asymmetric training. Another approach to reducing cycling is to train the discriminator more frequently than the generator. Observe that if we could exactly solve the supremum problem of the discriminator after every iteration of the generator, then the generator would be simply solving a convex minimization problem and GD should converge point-wise. The latter approach could lead to slow convergence given the finiteness of samples in the case of stochastic training. Hence, we cannot really afford completely solving the discriminators problem. However, training the discriminator for multiple iterations, brings the problem faced by the generator closer to convex minimization rather than solving an equilibrium problem. Hence, asymmetric training could help with cycling. We observe below that asymmetric training is the most effective modification in reducing the range of the cycles and hence making the last-iterate be close to the equilibrium. However, it does not really eliminate the cycles, rather it simply makes their range smaller.(a) GD dynamics with a gradient penalty added to the loss. \u03b7 = 0.1 and \u03bb = 0.1.(b) GD dynamics with momentum. \u03b7 = 0.1 and \u03b3 = 0.5.(c) GD dynamics with momentum and gradient penalty. \u03b7 = .1, \u03b3 = 0.2 and \u03bb = 0.1.(d) GD dynamics with momentum and gradient penalty, training generator every 15 training iterations of the discriminator. \u03b7 = .1, \u03b3 = 0.2 and \u03bb = 0.1.(e) GD dynamics with Nesterov momentum and gradient penalty, training generator every 15 training iterations of the discriminator. \u03b7 = .1, \u03b3 = 0.2 and \u03bb = 0.1. We demonstrate the benefits of using OMD over GD in another simple illustrative example. In this case, the example is does not boil down to a bi-linear game and therefore, the simulation results portray that the theoretical results we provided for bi-linear games, carry over qualitatively beyond the linear case. Consider the case where the data distribution is a mean zero multi-variate normal with an unknown co-variance matrix, i.e., x \u223c N (0, \u03a3). We will consider the case where the discriminator is the set of all quadratic functions: DISPLAYFORM0 The generator is a linear function of the random input noise z \u223c N (0, I), of the form: DISPLAYFORM1 The parameters W and V are both d \u00d7 d matrices. The WGAN game loss associated with these functions is then: DISPLAYFORM2 Expanding the latter we get: DISPLAYFORM3 Given that the covariance matrix is symmetric positive definite, we can write it as \u03a3 = U U T . Then the loss simplifies to: DISPLAYFORM4 The equilibrium of this game is for the generator to choose V ik = U ik for all i, k, and for the discriminator to pick W ij = 0. For instance, in the case of a single dimension we have L(V, W ) = W \u00b7 (\u03c3 2 \u2212 V 2 ), where \u03c3 2 is the variance of the Gaussian. Hence, the equilibrium is for the generator to pick V = \u03c3 and the discriminator to pick W = 0.Dynamics without sampling noise. For the mean GD dynamics the update rules are as follows: DISPLAYFORM5 We can write the latter updates in a simpler matrix form: DISPLAYFORM6 Similarly the OMD dynamics are: DISPLAYFORM7 (OMD for Covariance) Due to the non-convexity of the generators problem and because there might be multiple optimal solutions (e.g. if \u03a3 is not strictly positive definite), it is helpful in this setting to also help dynamics by adding 2 regularization to the loss of the game. The latter simply adds an extra 2\u03bbW t at each gradient term \u2207 W L(V t , W t ) for the discriminator and a 2\u03bbV t at each gradient term \u2207 V L(V t , W t ) for the generator. In FIG0 we give the weights and the implied covariance matrix \u03a3 G = V V T of the generator's distribution for each of the dynamics for an example setting of the step-size and regularization parameters and for two and three dimensional gaussians respectively. We again see how OMD can stabilize the dynamics to converge pointwise. Stochastic dynamics. In FIG9 and 9 we also portray the instability of GD and the robustness of the stability of OMD under stochastic dynamics. In the case of stochastic dynamics the gradients are replaced with unbiased estimates or with averages of unbiased estimates over a small minibatch. In the case of a mini-batch of one, the unbiased estimates of the gradients in this setting take the following form:\u2207 DISPLAYFORM8 where x t , z t are samples drawn from the true distribution and from the random noise distribution respectively. Hence, the stochastic dynamics simply follow by replacing gradients with unbiased estimates: DISPLAYFORM9 The goal of this section is to show that Optimistic Mirror Descent exhibits last iterate convergence to min-max solutions for bilinear functions. In Section D.1, we provide the proof of Theorem 1, that OMD exhibits last iterate convergence to min-max solutions of the following min-max problem DISPLAYFORM0 where A is an abitrary matrix and x and y are unconstrained. In Section D.2, we state the appropriate extension of our theorem to the general case: DISPLAYFORM1 D.1 PROOF OF THEOREM 1As stated in Section 4, for the min-max problem (29) Optimistic Mirror Descent takes the following form, for all t \u2265 1: DISPLAYFORM2 DISPLAYFORM3 where for the above iterations to be meaningful we need to specify x 0 , x \u22121 , y 0 , y \u22121 .As stated in Section 4 we allow any initialization x 0 \u2208 R(A), and y 0 \u2208 R(A T ), and set x \u22121 = 2x 0 and y \u22121 = 2y 0 , where R(\u00b7) represents column space. In particular, our initialization means that the first step taken by the dynamics gives x 1 = x 0 and y 1 = y 0 .Before giving our proof of Theorem 1, we need some further notation. For all i \u2208 N, we set: DISPLAYFORM4 where k \u2208 Z and j \u2208 {0, 1} are such that: i = 2k + j. , etc. We also use the notation u, v X = u T XX T v, for vectors u, v \u2208 R d and square d \u00d7 d matrices X. We similarly define the norm notation ||u|| X = u, u X . Given our notation, we have the following claim, shown in Appendix D.3. Claim 1. For all matrices A and vectors u, v of the appropriate dimensions: DISPLAYFORM0 With our notation in place, we show (through iterated expansion of the update rule), the following lemma, proved in Appendix D.3: Lemma 2. For the dynamics of Eq. (31) and (32) and any initialization 1 2 x \u22121 = x 0 \u2208 R(A), and 1 2 y \u22121 = y 0 \u2208 R(A T ) we have the following for all i, t \u2208 N such that i \u2265 0 and t \u2265 2: DISPLAYFORM1 We are ready to prove Theorem 1. Its proof is implied by the following stronger theorem, and Corollary 7. Theorem 3. Consider the dynamics of Eq. (31) and (32) and any initialization 1 2 x \u22121 = x 0 \u2208 R(A), and DISPLAYFORM2 where for a matrix X we denote by X + its generalized inverse and by ||X|| its spectral norm. Suppose that max{||A||, ||A T ||} \u2261 \u03bb \u221e \u2264 1 and \u03b7 is a small enough constant satisfying \u03b7 < 1/(3\u03b3 2 ). Then, for all i \u2208 N: DISPLAYFORM3 DISPLAYFORM4 and, for all i, t \u2208 N such that t \u2265 3, the following condition holds: DISPLAYFORM5 Proof. Eq. (33) holds trivially as under our initialization x 1 = x 0 and y 1 = y 0 . Eq. FORMULA39 is also easy to show by noticing the following. Given our initialization: DISPLAYFORM6 Hence (using j = i mod 2): DISPLAYFORM7 Similarly: DISPLAYFORM8 It follows from FORMULA39 and FORMULA39 that DISPLAYFORM9 We use induction on t to prove (35). We start our proof by showing the inductive step, and postpone establishing the basis of our induction to the end of this proof. For the inductive step, we assume that H(i, \u03c4 ) holds for all i \u2265 0 and 1 \u2264 \u03c4 < t, for some t > 3. Assuming this, we show next that H(i, t) holds for all i. To do this, we make use of a few lemmas, whose proofs are given in Appendix D.3.Lemma 4. Under the conditions of the theorem, for all i \u2265 0, t \u2265 2: DISPLAYFORM10 Lemma 5. Under the conditions of the theorem, for all i, t \u2265 0: DISPLAYFORM11 Lemma 6. Under the conditions of the theorem, for all i \u2265 0, t \u2265 0: DISPLAYFORM12 Given these lemmas, we show our inductive step. So for t \u2265 4: DISPLAYFORM13 where for the first inequality we used Lemma 4, for the second inequality we used that DISPLAYFORM14 (which is implied by the induction hypothesis), for the third inequality we used Lemma 6, for the fourth inequality we used Lemma 5, for the fifth inequality we applied the induction hypothesis iteratively, for the sixth inequality we used Eq. (34), for the seventh and eighth inequality we used that \u03b7 is small enough, and for the last inequality we used Lemma 5. Hence: DISPLAYFORM15 This completes the proof of our inductive step. It remains to show the basis of the induction, namely that H(i, 3) holds for all i \u2208 N. From Lemma 4 we have: DISPLAYFORM16 where for the second equality we used that 0.5x \u22121 = x 0 = x 1 and 0.5y \u22121 = y 0 = y 1 (which follow from our initialization), for the third inequality we used that (34), for the fourth inequality we used Lemma 5, for the fifth inequality we used Lemma 6, and for the last inequality we used Lemma 5. Hence, for small enough \u03b7, we have: distance from the space of equilibrium points of the game, where \u2206 0 0 is the distance of the initial point (x 0 , y 0 ) from the equilibrium space, and where both distances are taken with respect to the norm x T AA T x + y T A T Ay. DISPLAYFORM17 Proof of Corollary 7: It follows from FORMULA39 , FORMULA39 and FORMULA0 that: DISPLAYFORM18 which shows the first part of our claim. For the second part of our claim recall that the solutions to (29) are all pairs (x, y) such that x is in the null space of A T and y is in the null space of A. Theorem 8. Consider OMD for the min-max problem FORMULA39 : inf DISPLAYFORM0 Under the same conditions as Corollary 7 and whenever (30) is finite, OMD exhibits last iterate convergence in the same sense as in Corollary 7. In particular, for large enough t, the last iterate of OMD is within O \u221a \u03b7 \u00b7 \u03b3 \u2206 0 0 distance from the space of equilibrium points of the game, where \u221a \u2206 0 is the distance of the point (x 0 + (A T ) + c, y 0 + A + b) from the equilibrium space, and where both distances are taken with respect to the norm x T AA T x + y T A T Ay. Whenever (30) is infinite or undefined, the OMD dynamics travels to infinity and we characterize its motion. Proof of Theorem 8: Trivially, we need only consider functions of the form x T Ay + b T x + c T y. We consider the following decompositions of b and c: DISPLAYFORM1 Given the above we can also define b 3 and c 3 as follows: DISPLAYFORM2 Then, we can make the following variable substition: \u03b1 t = x t + \u03b7tb 2 + b 3 \u03b2 t = y t \u2212 \u03b7tc 2 + c 3 so that: DISPLAYFORM3 We also state the OMD dynamics for x t and y t for problem (30): DISPLAYFORM4 Note that given this update step: x t+1 = x t \u2212 2\u03b7Ay t + \u03b7Ay t\u22121 \u2212 \u03b7b x t+1 = x t \u2212 \u03b7b 2 \u2212 2\u03b7Ay t + \u03b7Ay t\u22121 \u2212 \u03b7Ac 3 x t+1 = x t \u2212 \u03b7b 2 \u2212 2\u03b7A(y t + c 3 ) + \u03b7A(y t\u22121 + c 3 ) x t+1 = x t \u2212 \u03b7b 2 \u2212 2\u03b7A(y t \u2212 \u03b7c 2 t + c 3 ) + \u03b7A(y t\u22121 \u2212 \u03b7c 2 (t \u2212 1) + c 3 ) x t+1 + \u03b7b 2 (t + 1) = x t + \u03b7b 2 t \u2212 2\u03b7A(y t \u2212 \u03b7c 2 t + c 3 ) + \u03b7A(y t\u22121 \u2212 \u03b7c 2 (t \u2212 1) + c 3 ) x t+1 + \u03b7b 2 (t + 1) + b 3 = x t + \u03b7b 2 t + b 3 \u2212 2\u03b7A(y t \u2212 \u03b7c 2 t + c 3 ) + \u03b7A(y t\u22121 \u2212 \u03b7c 2 (t \u2212 1) + c 3 ) \u03b1 t+1 = \u03b1 t \u2212 2\u03b7A\u03b2 t + \u03b7A\u03b2 t\u22121 Analogously: DISPLAYFORM5 Note that these are precisely the dynamics for which we proved convergence in Theorem 1. Thus, by invoking Theorem 3 and Corollary 7 on the sequence (\u03b1 t , \u03b2 t ) and then substituting back (x t , y t ), we have that for all large enough t: DISPLAYFORM6 where DISPLAYFORM7 . In particular, this shows that, whenever (30) is finite (i.e. b 2 = c 2 = 0), OMD exhibits last iterate convergence. For large enough t, the last iterate of OMD is within O \u221a \u03b7 \u00b7 \u03b3 \u2206 0 0 distance from the space of equilibrium points of the game, where \u2206 0 0 is the distance of (x 0 + b 3 , y 0 + c 3 ) from the equilibrium space in the norm x T AA T x + y T A T Ay. Whenever (30) is infinite or undefined, the OMD dynamics travels to infinity linearly, with fluctuations around the divergence specified as above. Proof of Proposition 1: To show this, we consider the 2 distance of the solution at time t. First, recall the GD update step in the special case of f (x, y) = x T y: DISPLAYFORM0 Then, note that the squared 2 distance of the running iterate (x t , y t ) to the unique equilibrium solution (0, 0) is given by d(t) := ||x t || 2 2 + ||y t || 2 2 , which we can calculate: DISPLAYFORM1 This indicates that for any value of \u03b7 > 0, the running iterate of GD diverges from the equilibrium. Proof of Claim 1: For our first claim, observe that: DISPLAYFORM2 , is proven analogously. For our third claim: DISPLAYFORM3 Proof of Lemma 2: First, we note the following scaled update rule: DISPLAYFORM4 Then, taking the norm of both sides, and using the statements of Claim 1: DISPLAYFORM5 Expanding the first pair of inner products above and using Claim 1 again: DISPLAYFORM6 Then, multiplying by 2\u03b7 and substituting into the previous derivation yields: Now, we use the update step for time t \u2212 2. For all t \u2265 1, this is well-defined, since x \u22121 and y \u22121 are defined. To ensure that this step is sound for t = 0 requires we define the following, where X + denotes the generalized inverse: DISPLAYFORM7 DISPLAYFORM8 We define these such that: A T x \u22122 = 4A T x 0 + y0 \u03b7 and Ay \u22122 = 4Ay 0 \u2212 x0 \u03b7 (since x 0 \u2208 R(A) and y 0 \u2208 R(A T ), and thus the following equalities hold: DISPLAYFORM9 This allows us to use the following expansion freely for all t \u2265 2:x t\u22122 = x t\u22123 \u2212 2\u03b7Ay t\u22123 + \u03b7Ay t\u22124 =\u21d2 x t\u22123 \u2212 2\u03b7Ay t\u22123 = x t\u22122 \u2212 \u03b7Ay t\u22124 y t\u22122 = y t\u22123 + 2\u03b7A T x t\u22123 \u2212 \u03b7A T x t\u22124 =\u21d2 y t\u22123 + 2\u03b7A T x t\u22123 = y y\u22122 + \u03b7A T x t\u22124We can gather the inner product terms and use this update rule to get our final desired result: DISPLAYFORM10 t\u22122 + 2\u03b7 2 ( x t\u22122 , x t\u22123 \u2212 2\u03b7Ay t\u22123 AM T i+1 + y t\u22122 , y t\u22123 + 2\u03b7A DISPLAYFORM11 Proof of Lemma 4: To prove this, first consider the following trivial inequality: DISPLAYFORM12 DISPLAYFORM13"
}