{
    "title": "H1ecDoR5Y7",
    "content": "Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance between a data distribution and sample distribution. Recent studies have proposed stabilizing the training process for the WGAN and implementing the Lipschitz constraint. In this study, we prove the local stability of optimizing the simple gradient penalty $\\mu$-WGAN(SGP $\\mu$-WGAN) under suitable assumptions regarding the equilibrium and penalty measure $\\mu$. The measure valued differentiation concept is employed to deal with the derivative of the penalty terms, which is helpful for handling abstract singular measures with lower dimensional support. Based on this analysis, we claim that penalizing the data manifold or sample manifold is the key to regularizing the original WGAN with a gradient penalty. Experimental results obtained with unintuitive penalty measures that satisfy our assumptions are also provided to support our theoretical results. Deep generative models reached a turning point after generative adversarial networks (GANs) were proposed by BID2 . GANs are capable of modeling data with complex structures. For example, DCGAN can sample realistic images using a convolutional neural network (CNN) structure BID12 . GANs have been implemented in many applications in the field of computer vision with good results, such as super-resolution, image translation, and text-to-image generation BID7 BID6 Zhang et al., 2017; BID13 .However, despite these successes, GANs are affected by training instability and mode collapse problems. GANs often fail to converge, which can result in unrealistic fake samples. Furthermore, even if GANs successfully synthesize realistic data, the fake samples exhibit little variability. A common solution to this instability problem is injecting an instance noise and finding different divergences. The injection of instance noise into real and fake samples during the training procedure was proposed by S\u00f8nderby et al. (2017) , where its positive impact on the low dimensional support for the data distribution was shown to be a regularizing factor based on the Wasserstein distance, as demonstrated analytically by . In f -GAN, f -divergence between the target and generator distributions was suggested which generalizes the divergence between two distributions BID11 . In addition, a gradient penalty term which is related with Sobolev IPM(Integral Probability Metric) between data distribution and sample distribution was suggested by BID9 .The Wasserstein GAN (WGAN) is known to resolve the problems of generic GANs by selecting the Wasserstein distance as the divergence . However, WGAN often fails with simple examples because the Lipschitz constraint on discriminator is rarely achieved during the optimization process and weight clipping. Thus, mimicking the Lipschitz constraint on the discriminator by using a gradient penalty was proposed by BID3 .Noise injection and regularizing with a gradient penalty appear to be equivalent. The addition of instance noise in f -GAN can be approximated to adding a zero centered gradient penalty BID14 . Thus, regularizing GAN with a simple gradient penalty term was suggested by BID8 who provided a proof of its stability. Based on a theoretical analysis of the dynamic system, BID10 proved the local exponential stability of the gradient-based optimization dynamics in GANs by treating the simultaneous gradient descent algorithm with a dynamic system approach. These previous studies were useful because they showed that the local behavior of GANs can be explained using dynamic system tools and the related Jacobian's eigenvalues. In this study, we aim to prove the convergence property of the simple gradient penalty \u00b5-Wasserstein GAN(SGP \u00b5-WGAN) dynamic system under general gradient penalty measures \u00b5. To the best of our knowledge, our study is the first theoretical approach to GAN stability analysis which deals with abstract singular penalty measure. In addition, measure valued differentiation BID4 ) is applied to take the derivative on the integral with a parametric measure, which is helpful for handling an abstract measure and its integral in our proof. The main contributions of this study are as follows.\u2022 We prove the regularized effect and local stability of the dynamic system for a general penalty measure under suitable assumptions. The assumptions are written as both a tractable strong version and intractable weak version. To prove the main theorem, we also introduce the measure valued differentiation concept to handle the parametric measure.\u2022 Based on the proof of the stability, we explain the reason for the success of previous penalty measures. We claim that the support of a penalty measure will be strongly related to the stability, where the weight on the limiting penalty measure might affect the speed of convergence.\u2022 We experimentally examined the general convergence results by applying two test penalty measures to several examples. The proposed test measures are unintuitive but they still satisfy the assumptions and similar convergence results were obtained in the experiment. First, we introduce our notations and basic measure-theoretic concepts. Second, we define our SGP \u00b5-WGAN optimization problem and treat this problem as a continuous dynamic system. Preliminary measure theoretic concepts are required to justify that the dynamic system changes in a sufficiently smooth manner as the parameter changes, so it is possible to use linearization theorem. They are also important for dealing with the parametric measure and its derivative. The problem setting with a simple gradient term is also discussed. The squared gradient size and simple gradient penalty term are used to build a differentiable dynamic system and to apply soft regularization as a resolving constraint, respectively. The continuous dynamic system approach, which is a so-called ODE method, is used to analyze the GAN optimization problem with the simultaneous gradient descent algorithm, as described by Nagarajan & Kolter (2017). D(x; \u03c8) : X \u2192 R is a discriminator function with its parameter \u03c8 and G(z; \u03b8) : Z \u2192 X is a generator function with its parameter \u03b8. p d is the distribution of real data and p g = p \u03b8 is the distribution of the generated samples in X , which is induced from the generator function G(z; \u03b8) and a known initial distribution p latent (z) in the latent space Z. \u00b7 denotes the L 2 Euclidean norm if no special subscript is present. The concept of weak convergence for finite measures is used to ensure the continuity of the integral term over the measure in the dynamic system, which must be checked before applying the theorems related to stability. Throughout this study, we assume that the measures in the sample space are all finite and bounded. Definition 1. For a set of finite measures {\u00b5 i } i\u2208I in (R n , d) with euclidean distance d, {\u00b5 i } i\u2208I is referred to as bounded if there exists some M > 0 such that for all i \u2208 I, DISPLAYFORM0 For instance, M can be set as 1 if {\u00b5 i } are probability measures on R n . Assuming that the penalty measures are bounded, Portmanteau theorem offers the equivalent definition of the weak conver-gence for finite measures. This definition is important for ensuring that the integrals over p \u03b8 and \u00b5 in the dynamic system change continuously. Definition 2. (Portmanteau Theorem) For a bounded sequence of finite measures {\u00b5 n } n\u2208N on the Euclidean space R n with a \u03c3-field of Borel subsets B(R n ), \u00b5 n converges weakly to \u00b5 if and only if for every continuous bounded function \u03c6 on R n , its integrals with respect to \u00b5 n converge to \u03c6d\u00b5, i.e., DISPLAYFORM1 The most challenging problem in our analysis with the general penalty measure is taking the derivative of the integral, where the measure depends on the variable that we want to differentiate. If our penalty measure is either absolutely continuous or discrete, then it is easy to deal with the integral. However, in the case of singular penalty measure, dealing with the integral term is not an easy task. Therefore, we introduce the concept of a weak derivative of a probability measure in the following BID4 DISPLAYFORM2 , makes the dynamic system differentiable and we define the WGAN problem with the square of the gradient's norm as a simple gradient penalty. This simple gradient penalty can be treated as soft regularization based on the size of the discriminator's gradient, especially in case where \u00b5 is the probability measure BID14 . It is convenient to determine whether the system is stable by observing the spectrum of the Jacobian matrix. In the following, (D(x; \u03c8), p d , p \u03b8 , \u00b5) is defined as an SGP \u00b5-WGAN optimization problem (SGP-form) with a simple gradient penalty term on the penalty measure \u00b5.Definition 4. The WGAN optimization problem with a simple gradient penalty term \u2207 x D 2 , penalty measure \u00b5, and penalty weight hyperparameter \u03c1 > 0 is given as follows, where the penalty term is only introduced to update the discriminator. DISPLAYFORM3 According to Nagarajan & Kolter (2017) and many other optimization problem studies, the simultaneous gradient descent algorithm for GAN updating can be viewed as an autonomous dynamic system of discriminator parameters and generator parameters, which we denote as \u03c8 and \u03b8. As a result, the related dynamic system is given as follows. DISPLAYFORM4 We investigate two examples considered in previous studies by BID8 and BID10 . We then generalize the results to a finite measure case. The first example is the univariate Dirac GAN, which was introduced by BID8 . DISPLAYFORM0 The Dirac GAN with a gradient penalty with an arbitrary probability measure is known to be globally convergent BID8 . We argue that this result can be generalized to a finite penalty measure case. Lemma 1. Consider the Dirac GAN problem with SGP form (D(x; \u03c8) = \u03c8x, \u03b4 0 , \u03b4 \u03b8 , \u00b5 \u03c8,\u03b8 ). Suppose that some small \u03b7 > 0 exists such that its finite penalty measure \u00b5 \u03c8,\u03b8 with mass M (\u03c8, \u03b8) = 1d\u00b5 \u03c8,\u03b8 \u2265 0 satisfies either DISPLAYFORM1 Then, the SGP \u00b5-WGAN optimization dynamics with (D(x; \u03c8) = \u03c8x, \u03b4 0 , \u03b4 \u03b8 , \u00b5 \u03c8,\u03b8 ) are locally stable at the origin and the basin of attraction B = B R ( (0, 0) ) is open ball with radius R. Its radius is given as follows. DISPLAYFORM2 Motivated by this example, we can extend this idea to the other toy example given by BID10 , where WGAN fails to converge to the equilibrium points (\u03c8, \u03b8) = (0, \u00b11).1 In this study, we prefer to use the expectation notation on the finite measure, which can be understood as follows. Suppose that \u00b5 \u03c8,\u03b8 = M (\u03c8, \u03b8)\u03bc \u03c8,\u03b8 where\u03bc \u03c8,\u03b8 is normalized to the probability measure. Then, DISPLAYFORM3 Lemma 2. Consider the toy example (D(x; \u03c8) = \u03c8x 2 , U (\u22121, 1), U (\u2212|\u03b8|, |\u03b8|), \u00b5 \u03b8 ) where U (0, 0) = \u03b4 0 and the ideal equilibrium points are given by (\u03c8 * , \u03b8 * ) = (0, \u00b11). For a finite measure \u00b5 = \u00b5 \u03b8 on R which is independent of \u03c8, suppose that \u00b5 \u03b8 \u2192 \u00b5 * with \u00b5 * = C\u03b4 0 for C \u2265 0. The dynamic system is locally stable near the desired equilibrium (0, \u00b11), where the spectrum of the DISPLAYFORM4 We propose the convergence property of WGAN with a simple gradient penalty on an arbitrary penalty measure \u00b5 for a realizable case: \u03b8 = \u03b8 * with p d = p \u03b8 * exists. In subsection 4.1, we provide the necessary assumptions, which comprise our main convergence theorem. In subsection 4.2, we give the main convergence theorem with a sketch of the proof. A more rigorous analysis is given in the Appendix. The first assumption is made regarding the equilibrium condition for GANs, where we state the ideal conditions for the discriminator parameter and generator parameter. As the parameters converge to the ideal equilibrium, the sample distribution(p \u03b8 ) converges to the real data distribution(p d ) and the discriminator cannot distinguish the generated sample and the real data. DISPLAYFORM0 The second assumption ensures that the higher order terms cannot affect the stability of the SGP \u00b5-WGAN. In the Appendix, we consider the case where the WGAN fails to converge when Assumption 2 is not satisfied. Compared with the previous study by BID10 , the conditions for the discriminator parameter are slightly modified. Assumption 2. DISPLAYFORM1 are locally constant along the nullspace of the Hessian matrix. The third assumption allows us to extend our results to discrete probability distribution cases, as described by BID8 . DISPLAYFORM2 The fourth assumption indicates that there are no other \"bad\" equilibrium points near (\u03c8 * , \u03b8 * ), which justifies the projection along the axis perpendicular to the null space. Assumption 4. A bad equilibrium does not exist near the desired equilibrium point. Thus, (\u03c8 DISPLAYFORM3 The last assumption is related to the necessary conditions for the penalty measure. A calculation of the gradient penalty based on samples from the data manifold and generator manifold or the interpolation of both was introduced in recent studies BID3 BID14 BID8 . First, we propose strong conditions for the penalty measure. Assumption 5. The finite penalty measure \u00b5 = \u00b5 \u03b8 satisfies the followings: a \u00b5 \u03b8 \u2192 \u00b5 \u03b8 * = \u00b5 * and \u00b5 \u03b8 is independent of the discriminator parameter \u03c8. DISPLAYFORM4 The assumption given above means that the support of the penalty measure \u00b5 \u03b8 should approach the data manifolds smoothly as \u03b8 \u2192 \u03b8 * . However, the penalty measure from WGAN-GP with a simple gradient penalty still reaches equilibrium without satisfying Assumption 5c. Therefore, we suggest Assumption 6, which is a weak version of Assumption 5. Assumption 6a 2 is technically required to take the derivative of the integral DISPLAYFORM5 2 ] with respect to \u03c8. Assumption 6. (Weak version of Assumption 5) The finite penalty measure \u00b5 = \u00b5 \u03c8,\u03b8 satisfies the following.a \u00b5 \u03c8,\u03b8 \u2192 \u00b5 \u03c8 * ,\u03b8 * = \u00b5 * , where supp(\u00b5 \u03c8,\u03b8 ) only depends on \u03b8. Near the equilibrium, \u00b5 \u03c8,\u03b8 can be weakly differentiated twice with respect to \u03c8. In addition, its mass M (\u03c8, \u03b8) = 1d\u00b5 \u03c8,\u03b8 is a twice-differentiable function of \u03c8 and bounded near the equilibrium. DISPLAYFORM6 The assumption above implies the following situations; The penalty measure's support approaches to data manifold and its weight changes smoothly with respect to \u03c8 and \u03b8. At the equilibrium, penalty measure's support contains data manifold. Also, ideal discriminator will remain flat on the penalty area. In summary, the gradient penalty regularization term with any penalty measure where the support approaches B(supp(p d )) in a smooth manner works well and this main result can explain the regularization effect of previously proposed penalty measures such as \u00b5 GP , p d , p \u03b8 , and their mixtures. According to the modified assumptions given above, we prove that the related dynamic system is locally stable near the equilibrium. The tools used for analyzing stability are mainly based on those described by Nagarajan & Kolter (2017). Our main contributions comprise proposing the necessary conditions for the penalty measure and proving the local stability for all penalty measures that satisfy Assumption 6. Theorem 1. Suppose that our SGP \u00b5-WGAN optimization problem (D, p d , p \u03b8 , \u00b5) with equilibrium point (\u03c8 * , \u03b8 * ) satisfies the assumptions given above. Then, the related dynamic system is locally stable at the equilibrium. A detailed proof of the main convergence theorem is given in the Appendix. A sketch of the proof is given in three steps. First, the undesired terms in the Jacobian matrix of the system at the equilibrium are cancelled out. Next, the Jacobian matrix at equilibrium is given by DISPLAYFORM0 The system is locally stable when both Q and R T R are positive definite. We can complete the proof by dealing with zero eigenvalues by showing that N (Q T ) \u2282 N (R T ) and the projected system's stability implies the original system's stability. Our analysis mainly focuses on WGAN, which is the simplest case of general GAN minimax optimization max DISPLAYFORM1 with f (x) = x. Similar approach is still valid for general GANs with concave function f with f (x) < 0 and f (0) = 0. We claim that every penalty measure that satisfies the assumptions can regularize the WGAN and generate similar results to the recently proposed gradient penalty methods. Several penalty measures were tested based on two-dimensional problems (mixture of 8 Gaussians, mixture of 25 Gaussians, and swissroll), MNIST and CIFAR-10 datasets using a simple gradient penalty term. In the comparisons with WGAN, the recently proposed penalty measures and our test penalty measures used the same network settings and hyperparameters. The penalty measures and its detailed sampling methods are listed in TAB1 , where DISPLAYFORM0 , and \u03b1 \u223c U (0, 1). A indicates fixed anchor point in X . BID3 , which penalizes the WGAN with non-zero centered gradient penalty terms, whereas \u00b5 GP represents the simple method. In our experiment, no additional weights are applied on 5 penalty measures and they are all probability distributions. Penalty term Penalty measure, sampling method DISPLAYFORM0 By setting the previously proposed WGAN with weight-clipping and WGAN-GP BID3 as the baseline models, SGP \u00b5-WGAN was examined with various penalty measures comprising three recently proposed measures and two artificially generated measures. p \u03b8 and p d were suggested by BID8 and \u00b5 GP was introduced from the WGAN-GP. We analyzed the artificial penalty measures \u00b5 mid and \u00b5 g,anc as the test penalty measures. The experiments were conducted based on the implementation of the BID3 . The hyperparameters, generator/discriminator structures, and related TensorFlow implementations can be found at https://github.com/igul222/improved_wgan_training BID3 . Only the loss function was modified slightly from a non-zero centered gradient penalty to a simple penalty. For the CIFAR-10 image generation tasks, the inception score BID15 and FID BID5 were used as benchmark scores to evaluate the generated images. We checked the convergence of p \u03b8 for the 2D examples (8 Gaussians, swissroll data, and 25 Gaussians) and MNIST digit generation for the SGP-WGANs with five penalty measures. MNIST and 25 Gaussians were trained over 200K iterations, the 8 Gaussians were trained for 30K iterations, and the Swiss Roll data were trained for 100K iterations. The anchor A for \u00b5 g,anc was set as (2, \u22121) for the 2D examples and 784 gray pixels for MNIST. We only present the results obtained for the MNIST dataset with the penalty measures comprising \u00b5 mid and \u00b5 g,anc in Figure 1 . The others are presented in the Appendix. Figure 1: MNIST example. Images generated with \u00b5 mid (left) and \u00b5 g,anc (right). DCGAN and ResNet architectures were tested on the CIFAR-10 dataset. The generators were trained for 200K iterations. The anchor A for \u00b5 g,anc during CIFAR-10 generation was set as fixed random pixels. The WGAN, WGAN-GP, and five penalty measures were evaluated based on the inception score and FID, as shown in TAB2 , which are useful tools for scoring the quality of generated images. The images generated from \u00b5 mid and \u00b5 g,anc with ResNet are shown in FIG0 . The others are presented in the Appendix. In this study, we proved the local stability of simple gradient penalty \u00b5-WGAN optimization for a general class of finite measure \u00b5. This proof provides insight into the success of regularization with previously proposed penalty measures. We explored previously proposed analyses based on various gradient penalty methods. Furthermore, our theoretical approach was supported by experiments using unintuitive penalty measures. In future research, our works can be extended to alternative gradient descent algorithm and its related optimal hyperparameters. Stability at non-realizable equilibrium points is one of the important topics on stability of GANs. Optimal penalty measure for achieving the best convergence speed can be also investigated using a spectral theory, which provides the mathematical analysis on stability of GAN with a precise information on the convergence theory. Proof of Lemma 1. The related dynamic system of (D(x; \u03c8) = \u03c8x, \u03b4 0 , \u03b4 \u03b8 , \u00b5 \u03c8,\u03b8 ) can be written as follows.\u03c8 DISPLAYFORM0 First, the only equilibrium point is given by (\u03c8 * , \u03b8 * ) = (0, 0) from DISPLAYFORM1 The corresponding Jacobian matrix for the dynamic system is written as: DISPLAYFORM2 \u2207 \u03c8 D(x; \u03c8) = \u03c8 does not depend on x, so this can be rewritten as: DISPLAYFORM3 Therefore, if M (0, 0) > 0, then the given system is locally stable because the eigenvalues of its linearized system have negative real parts. If M (0, 0) = 0, then the stability of the system cannot be proved by the linearization theorem. In this case, we consider the following Lyapunov function. DISPLAYFORM4 By differentiating with t, we obtai\u1e45 DISPLAYFORM5 Clearly, L(\u03c8, \u03b8) \u2265 0 and the equality holds iff 0) ) for all \u03c4 \u2265 0 because the Lyapunov function (square of the distance between the origin and (\u03c8(\u03c4 ), \u03b8(\u03c4 ))) always decreases as \u03c4 \u2192 \u221e. Therefore, the given system is stable according to the Lyapunov stability theorem. DISPLAYFORM6 Again, we can check that if \u00b5 \u03c8,\u03b8 is a probability measure, then the system is globally stable, as shown by BID8 . The basin of attraction is given by the whole R 2 plane since DISPLAYFORM7 Proof of Lemma 2. From the general setup of the SGP \u00b5-WGAN optimization problem, the dynamic system corresponding to the simple-GAN in Definition 6 can be written as follows. DISPLAYFORM8 If we let E \u00b5 * [x 2 ] = A 2 , then the Jacobian matrix at the equilibrium (0, \u00b11) is given by J = \u22124\u03c1A 2 \u2213 2 3 \u00b1 2 3 0 . Therefore, the given system is locally stable when A = 0. Lemma 3. Consider the Dirac-GAN setup and SGP \u00b5-WGAN optimization system with a slightly changed discriminator function D 2 (x; \u03c8) = \u03c8x 2 . The system (D 2 , \u03b4 0 , \u03b4 \u03b8 , \u00b5 GP ) does not converge to (0, 0) but for any point (a, 0) with a < 0, the system has equilibrium points on the whole \u03c8-axis and it violates Assumption 2.Proof of Lemma 3. For the SGP \u00b5-WGAN optimization problem (D 2 , \u03b4 0 , \u03b4 \u03b8 , \u00b5 GP ), the dynamic system can be written as follows.\u03c8 = \u2212\u03b8 2 \u2212 4 3 \u03c1\u03c8\u03b8 Proof. Let us consider the Jacobian matrix DISPLAYFORM0 First, Assumption 1 implies that DISPLAYFORM1 is locally zero near the equilibrium \u03b8 * , which implies that DISPLAYFORM2 We still need to evaluate DISPLAYFORM3 According to Assumption 6a, finite signed measures \u00b5 \u03c8,\u03b8 and \u00b5 \u03c8,\u03b8 exist 5 , so they are the first and second weak derivatives of \u00b5 \u03c8,\u03b8 with respect to the parameter \u03c8 at (\u03c8 * , \u03b8 * ). Therefore, the expectations given above can be rewritten as below. DISPLAYFORM4 where DISPLAYFORM5 From Assumption 6c and the fact that the weak derivative of \u00b5 \u03c8,\u03b8 vanishes outside of supp(\u00b5 \u03c8,\u03b8 ), \u2207 x D(x; \u03c8 * ) = 0 on supp(\u00b5 \u03c8,\u03b8 ) \u2282 V for all \u03b8 with |\u03b8 \u2212 \u03b8 * | < \u00b5 and \u00b5 \u03c8,\u03b8 = \u00b5 \u03c8,\u03b8 = 0 on the outside of supp(\u00b5 \u03c8,\u03b8 ), which leads to the desired results: DISPLAYFORM6 After cancelling the undesired terms, the Jacobian matrix at the equilibrium (\u03c8 * , \u03b8 * ) is given as: DISPLAYFORM7 In standard notation, \u2207 \u03c8 g is the dim(range of g) \u00d7 dim(\u03c8) matrix. For a real-valued function f , we consider the first derivative as the column vector instead of the row vector. \u2207 \u03c8 f is considered to be the dim(\u03c8) \u00d7 1 matrix(column vector) of the total derivative. For the second derivative, \u2207 \u03c8\u03b8 f = (\u2207 \u03c8 )(\u2207 \u03b8 f ) is the dim(\u03b8) \u00d7 dim(\u03c8) matrix. The transpose notation is used in a similar manner to the matrix. 5 \u00b5 \u03c8,\u03b8 and \u00b5 \u03c8,\u03b8 will be considered as row vector(1 \u00d7 dim(\u03c8) matrix) and dim(\u03c8) \u00d7 dim(\u03c8) matrix of finite signed measures respectively. DISPLAYFORM8 that for a negative definite matrix A and full column rank matrix B, the block matrix A B \u2212B T 0 is Hurwitz, i.e., all eigenvalues of the matrix have a negative real part. Therefore, if Q is positive definite and R is full column rank, the proof is complete. We consider the complementary case. Suppose that Q or R T R have some zero eigenvalues. DISPLAYFORM9 , where T D and T G are the eigenvectors of Q and R T R that correspond to non-zero eigenvalues. First, we assume that T D and T G are not empty. We can show that (\u03c8 * + \u03bev, \u03b8 * + \u03bdw) is also an equilibrium point for a sufficiently small \u03be, \u03bd and v \u2208 N (Q), w \u2208 N (R T R) by using the techniques given by BID10 . If the system does not update at the equilibrium point (\u03c8 * , \u03b8 * ) and its small neighborhood (\u03c8 * + \u03bev, \u03b8 * + \u03bdw) is perturbed along N (Q) and N (R T R), then it is reasonable to project the system orthogonal to N (Q) and N (R T R).First, we assume that v \u2208 N (Q). By Assumption 2, h(\u03c8 * + \u03bev) = h(\u03c8 * ) = 0 for |\u03be| < \u03be d , which implies that \u2207 x D(x; \u03c8 * + \u03bev) = 0 for x \u2208 supp(\u00b5 \u03c8 * +\u03bev,\u03b8 * ) = supp(\u00b5 * ) and |\u03be| < \u03be d . Thus, we obtain Therefore, the point (\u03c8 * + \u03bev, \u03b8 * ) with |\u03be| < \u03be d is an equilibrium point. According to Assumption 4, D(x; \u03c8 * + \u03bev) is an equilibrium discriminator for |\u03be| < \u03b4 d , and thus D(x; \u03c8 * + \u03bev) is already an optimal discriminator for |\u03be| < min(\u03be d , \u03b4 d ).Suppose that w \u2208 N (R T R). By Assumption 2, g(\u03b8 * ) = g(\u03b8 * + \u03bdw) = 0 for |\u03bd| < \u03bd g , and thus E p d [\u2207 \u03c8 D(x; \u03c8 * )] \u2212 E p \u03b8 * +\u03bdw [\u2207 \u03c8 D(x; \u03c8 * )] = 0 for |\u03bd| < \u03bd g . Furthermore, Assumption 3 gives E p \u03b8 * +\u03bdw [D(x; \u03c8 * )] = 0 for a sufficiently close |\u03bd| < g , which implies tha\u1e6b DISPLAYFORM10"
}