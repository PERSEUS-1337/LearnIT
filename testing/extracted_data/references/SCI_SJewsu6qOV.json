{
    "title": "SJewsu6qOV",
    "content": "The \u201cSupersymmetric Artificial Neural Network\u201d in deep learning (denoted (x; \u03b8, bar{\u03b8})Tw), espouses the importance of considering biological constraints in the aim of further generalizing backward propagation. \n\n Looking at the progression of \u2018solution geometries\u2019; going from SO(n) representation (such as Perceptron like models) to SU(n) representation (such as UnitaryRNNs) has guaranteed richer and richer representations in weight space of the artificial neural network, and hence better and better hypotheses were generatable. The Supersymmetric Artificial Neural Network explores a natural step forward, namely SU(m|n) representation. These supersymmetric biological brain representations (Perez et al.) can be represented by supercharge compatible special unitary notation SU(m|n), or (x; \u03b8, bar{\u03b8})Tw parameterized by \u03b8, bar{\u03b8}, which are supersymmetric directions, unlike \u03b8 seen in the typical non-supersymmetric deep learning model. Notably, Supersymmetric values can encode or represent more information than the typical deep learning model, in terms of \u201cpartner potential\u201d signals for example. Introduction. Machine learning non-trivially concerns the application of families of functions that guarantee more and more variations in weight space. This means that machine learning researchers study what functions are best to transform the weights of the artificial neural network, such that the weights learn to represent good values for which correct hypotheses or guesses can be produced by the artificial neural network. The \"Supersymmetric Artificial Neural Network\" (or 'thought curvature') is reasonably yet another way to represent richer values in the weights of the model; because supersymmetric values can allow for more information to be captured about the input space. For example, supersymmetric systems can capture potential-partner signals, which is beyond the feature space of magnitude and phase signals learnt in typical real valued neural nets and deep complex neural networks respectively. As such, a brief historical progression of geometric solution spaces for varying neural network architectures follows:1. An optimal weight space produced by shallow or low dimension integer valued nodes or real valued artificial neural nets, may have good weights that lie for example, in one simple (\u2124 \u211d \u2212 ) cone per class/target group. (This may guarantee some variation, but not enough for more sophisticated tasks of higher dimension) BID4 [10]2. An optimal weight space produced by deep and high-dimension-absorbing real valued artificial neural nets, may have good weights that lie in disentangleable (\u211d * \u211d \u2212 ) manifolds per class/target group convolved by the operator * , instead of the simpler regions per class/target group seen in item (1). parameterized by , , which are supersymmetric directions, unlike seen in the typical nonsupersymmetric deep learning model. Notably, Supersymmetric values can encode or represent more information than the typical deep learning model, in terms of \"partner potential\" signals for example. This paper does not contain empirical code concerning supersymmetric artificial neural networks, although it does highlight empirical evidence, that indicates how such types of supersymmetric learning models could exceed the state of the art, due to preservation features seen in progressing through earlier related models from the days of older perceptron like models that were not supersymmetric.(This may guarantee more variation in the weight space than (1), leading to better hypotheses or guesses) BID5 3. An optimal weight space produced by shallow but high dimension-absorbing complex valued artificial neural nets, may have good weights that lie in multiple (\u2102 \u2212 ) sectors per class/target group, instead of the real regions per class/target group seen amongst the prior items. (This may guarantee more variation of the weight space than the previous items, by learning additional features, in the \"phase space\". This also leads to better hypotheses/guesses) BID6 4. An optimal weight space produced by deep or high dimension-absorbing complex valued artificial neural nets, may have good weights that lie in chi distribution bound, (\u2102 * \u2102 \u2212 ) rayleigh space per class/target group convolved by the operator * , instead of the simpler sectors/regions per class/target group seen amongst the previous items. (This may guarantee more variation of the weight space than the prior items, by learning phase space representations, and by extension, strengthen these representations via convolutional residual blocks. This also leads to better hypotheses/guesses) BID7 5. The \"Supersymmetric Artificial Neural Network\" operable on high dimensional data, may reasonably generate good weights that lie in disentangleable ( \u221e ( | ) \u2212 ) supermanifolds per class/target group, instead of the solution geometries seen in the prior items above. Supersymmetric values can encode rich partner-potential delimited features beyond the phase space of (4) in accordance with cognitive biological space BID2 , where (4) lacks the partner potential formulation describable in Supersymmetric embedding. BID8 Another view of \"solution geometry\" history, which may promote a clear way to view the reasoning behind the subsequent pseudocode sequence.1. There has been a clear progression of \"solution geometries\", ranging from those of the ancient Perceptron BID4 to complex valued neural nets BID7 , grassmann manifold artificial neural networks BID11 or unitaryRNNs. BID10 [14] BID14 These models may be denoted by ( ; ) \u22a4 parameterized by , expressible as geometrical groups ranging from orthogonal BID3 to special unitary group BID16 based: ( ) to ( )\u2026, and they got better at representing input data i.e. representing richer weights, thus the learning models generated better hypotheses or guesses. 2. By \"solution geometry\" I mean simply the class of regions where an algorithm's weights may lie, when generating those weights to do some task. 3. As such, if one follows cognitive science, one would know that biological brains may be measured in terms of supersymmetric operations. (Perez et al, \"Supersymmetry at brain scale\" BID2 ) 4. These supersymmetric biological brain representations can be represented by supercharge BID9 compatible special unitary notation ( | ), or ( ; , ) \u22a4 parameterized by , BID8 , which are supersymmetric directions, unlike seen in item BID0 . Notably, Supersymmetric values can encode or represent more information than the prior classes seen in (1), in terms of \"partner potential\" signals for example. 5. So, state of the art machine learning work forming ( ) or ( ) based solution geometries, although nonsupersymmetric, are already in the family of supersymmetric solution geometries that may be observed as occurring in biological brain or ( | ) supergroup BID15 representation. A naive supersymmetric artificial neural network architecture. (See points 1 to 5 in BID13 ) It seems feasible that a C \u221e bound atlas-based learning model, where said C \u221e is in the family of supermanifolds from supersymmetry, may be obtained from a system, which includes charts ( ) of grassmann manifold networks , and stiefel manifolds , , in ( , ) terms, where there exists some invertible submatrix entailing matrix \u2208 ( \u2229 ) for = ( ) where is a submersion mapping on some stiefel manifold , , thereafter enabling some differentiable grassmann manifold (\u211d ) wherein = { \u2208 \u211d \u00d7 : ( ) \u2260 0}. Pertinently, the \"Edward Witten/String theory powered supersymmetric artificial neural network\", is one wherein supersymmetric weights are sought. Many machine learning algorithms are not empirically shown to be exactly biologically plausible, i.e. Deep Neural Network algorithms, have not been observed to occur in the brain, but regardless, such algorithms work in practice in machine learning. Likewise, regardless of Supersymmetry's elusiveness at the LHC, as seen above, it may be quite feasible to borrow formal methods from strategies in physics even if such strategies are yet to show related physical phenomena to exist; thus it may be pertinent/feasible to try to construct a model that learns supersymmetric weights, as I proposed throughout this paper, following the progression of solution geometries going from ( ) to ( ) and onwards to ( | ) BID15 ."
}