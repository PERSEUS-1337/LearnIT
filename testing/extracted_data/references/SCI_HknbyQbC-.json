{
    "title": "HknbyQbC-",
    "content": "Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results.\n Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. \n In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. \n For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.  \n We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.\n Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge. Deep Neural Networks (DNNs) have achieved great successes in a variety of applications ranging from image recognition BID19 BID12 to speech processing and from robotics training BID23 to medical diagnostics BID7 . However, recent work has demonstrated that DNNs are vulnerable to adversarial perturbations BID32 BID10 . An adversary can add small-magnitude perturbations to inputs and generate adversarial examples to mislead DNNs. Such maliciously perturbed instances can cause the learning system to misclassify them into either a maliciously-chosen target class (in a targeted attack) or classes that are different from the ground truth (in an untargeted attack). Different algorithms have been proposed for generating such adversarial examples, such as the fast gradient sign method (FGSM) BID10 and optimization-based methods (Opt.) BID4 BID24 .Most of the the current attack algorithms BID4 BID24 rely on optimization schemes with simple pixel space metrics, such as L \u221e distance from a benign image, to encourage visual realism. To generate more perceptually realistic adversarial examples, in this paper, we propose to train a feed-forward network to generate perturbations such that the resulting example must be realistic according to a discriminator network. We apply generative adversarial networks (GANs) to produce adversarial examples in both the semi-whitebox and black-box settings. As conditional GANs are capable of producing high-quality images BID16 , we apply a similar paradigm to produce perceptually realistic adversarial instances. We name our method AdvGAN.Note that in the previous white-box attacks, such as FGSM and optimization methods, the adversary needs to have white-box access to the architecture and parameters of the model all the time. However, by deploying AdvGAN, once the feed-forward network is trained, it can instantly pro-duce adversarial perturbations for any input instances without requiring access to the model itself anymore. We name this attack setting semi-whitebox. To evaluate the effectiveness of our attack strategy AdvGAN, we first generate adversarial instances based on AdvGAN and other attack strategies on different target models. We then apply the stateof-the-art defenses to defend against these generated adversarial examples BID10 BID33 BID27 . We evaluate these attack strategies in both semi-whitebox and black-box settings. We show that adversarial examples generated by AdvGAN can achieve a high attack success rate, potentially due to the fact that these adversarial instances appear closer to real instances compared to other recent attack strategies. Our contributions are listed as follows.\u2022 Different from the previous optimization-based methods, we train a conditional adversarial network to directly produce adversarial examples, which are both perceptually realistic and achieve state-of-the-art attack success rate against different target models.\u2022 We show that AdvGAN can attack black-box models by training a distilled model. We propose to dynamically train the distilled model with query information and achieve high black-box attack success rate and targeted black-box attack, which is difficult to achieve for transferability-based black-box attacks.\u2022 We use the state-of-the-art defense methods to defend against adversarial examples and show that AdvGAN achieves much higher attack success rate under current defenses.\u2022 We apply AdvGAN on M\u0105dry et al.'s MNIST challenge (2017a) and achieve 88.93% accuracy on the published robust model in the semi-whitebox setting and 92.76% in the blackbox setting, which wins the top position in the challenge BID28 . Here we review recent work on adversarial examples and generative adversarial networks. Adversarial Examples A number of attack strategies to generate adversarial examples have been proposed in the white-box setting, where the adversary has full access to the classifier BID32 BID10 BID4 BID26 BID29 BID3 BID21 . Goodfellow et al. propose the fast gradient sign method (FGSM), which applies a first-order approximation of the loss function to construct adversarial samples. Formally, given an instance x, an adversary generates adversarial example x A = x + \u03b7 with L \u221e constraints in the untargeted attack setting as \u03b7 = \u00b7 sign(\u2207 x f (x, y)), where f (\u00b7) is the cross-entropy loss used to train the neural network f , and y represents the ground truth of x. Optimization based methods have also been proposed to optimize adversarial perturbation for targeted attacks while satisfying certain constraints BID4 BID24 . Its goal is to minimize the objective function as ||\u03b7||+\u03bb f (x A , y), where ||\u00b7|| is an appropriately chosen norm function. However, the optimization process is slow and can only optimize perturbation for one specific instance each time. In contrast, our feed-forward network can produce perturbation for any instance. It achieves higher attack success rate against different defenses and performs much faster than the current attack algorithms. Independently from our work, feed-forward networks have been applied to generate adversarial perturbation BID0 . However, Baluja & Fischer combine the re-ranking loss and an L 2 norm loss, aiming to constrain the generated adversarial instance to be close to the original one in terms of L 2 ; while we apply a deep neural network as a discriminator to help distinguish the instance with other real images to encourage the perceptual quality of the generated adversarial examples. Black-box Attacks Current learning systems usually do not allow white-box accesses against the model for security reasons. Therefore, there is a great need for black-box attacks analysis. Most of the black-box attack strategies are based on the transferability phenomenon BID30 , where an adversary can train a local model first and generate adversarial examples against it, hoping the same adversarial examples will also be able to attack the other models. Many learning systems allow query accesses to the model. However, there is little work that can leverage query-based access to target models to construct adversarial samples and move beyond transferability. Papernot FORMULA1 proposed to train a local substitute model with queries to the target model to generate adversarial samples, but this strategy still relies on transferability. In contrast, we show that the proposed AdvGAN can perform black-box attacks without depending on transferability. Generative Adversarial Networks (GANs) have achieved visually appealing results in both image generation BID31 BID11 BID2 and manipulation BID37 settings. Recently, image-to-image conditional GANs have further improved the quality of synthesis results BID16 BID16 . We adopt a similar adversarial loss and image-to-image network architecture to learn the mapping from an original image to a perturbed output such that the perturbed image cannot be distinguished from real images in the original class. Different from prior work, we aim to produce output results that are not only visually realistic but also able to mislead target learning models.3 GENERATING ADVERSARIAL EXAMPLES WITH ADVERSARIAL NETWORKS 3.1 PROBLEM DEFINITION Let X \u2286 R n be the feature space, with n the number of features. Suppose that (x i , y i ) is the ith instance within the training set, which is comprised of feature vectors x i \u2208 X , generated according to some unknown distribution x i \u223c P data , and y i \u2208 Y the corresponding true class labels. The learning system aims to learn a classifier f : X \u2192 Y from the domain X to the set of classification outputs Y, where |Y| denotes the number of possible classification outputs. Given an instance x, the goal of an adversary is to generate adversarial example x A , which is classified as f (x A ) = y (untargeted attack), where y denotes the true label; or f (x A ) = t (targeted attack) where t is the target class. x A should also be close to the original instance x in terms of L 2 or other distance metric. FIG0 illustrates the overall architecture of AdvGAN, which mainly consists of three parts: a generator G, a discriminator D, and the target neural network f . Here the generator G takes the original instance x as its input and generates a perturbation G(x). Then x + G(x) will be sent to the discriminator D, which is used to distinguish the generated data and the original instance x. The goal of D is to encourage that the generated instance is indistinguishable with the data from its original class. To fulfill the goal of fooling a learning model, we first perform the white-box attack, where the target model is f in this case. f takes x + G(x) as its input and outputs its loss L adv , which represents the distance between the prediction and the target class t (targeted attack), or the opposite of the distance between the prediction and the ground truth class (untargeted attack). Under review as a conference paper at ICLR 2018The adversarial loss can be written as: DISPLAYFORM0 (1)Here, the discriminator D aims to distinguish the perturbed data x + G(x) from the original data x. Note that the real data is sampled from the true class, so as to encourage that the generated instances are close to data from the original class. The loss for fooling the target model f in a targeted attack is: DISPLAYFORM0 where t is the target class and f denotes the loss function (e.g., cross-entropy loss) used to train the original model f . The L f adv loss encourages the perturbed image to be misclassified as target class t. Here we can also perform the untargeted attack by maximizing the distance between the prediction and the ground truth, but we will focus on the targeted attack in the rest of the paper. To bound the magnitude of the perturbation, which is a common practice in prior work BID4 BID24 BID1 , we add a soft hinge loss on the L 2 norm as DISPLAYFORM1 where c denotes a user-specified bound. This can also stabilize the GAN's training, as shown in BID16 . Finally, our full objective can be expressed as DISPLAYFORM2 where \u03b1 and \u03b2 control the relative importance of each objective. Note that L GAN here is used to encourage the perturbed data to appear similar to the original data x, while L f adv is leveraged to generate adversarial examples, optimizing for the high attack success rate. We obtain our G and D by solving the minmax game arg min G max D L. Static Distillation For black-box attack, we assume adversaries have no prior knowledge of training data or the model itself. In our experiments in Section 4, we randomly draw data that is disjoint from the training data of the black-box model to distill it, since we assume the adversaries have no prior knowledge about the training data or the model. To achieve black-box attacks, we first build a distilled network f based on the output of the black-box model b BID15 . Once we obtain the distilled network f , we carry out the same attack strategy as described in the white-box setting (see Equation FORMULA3 ). Here, we minimize the following network distillation objective: DISPLAYFORM0 where f (x) and b(x) denote the output from the distilled model and black-box model respectively for the given training image x, and H denotes the commonly used cross-entropy loss. By optimizing the objective over all the training images, we can obtain a model f which behaves very close to the black-box model b. We then carry out the attack on the distilled network. Note that unlike training the discriminator D, where we only use the real data from the original class to encourage that the generated instance is close to its original class, here we train the distilled model with data from all classes. Dynamic Distillation Only training the distilled model with all the pristine training data is not enough, since it is unclear how close the black-box and distilled model perform on the generated adversarial examples, which have not appeared in the training set before. Here we propose an alternative minimization approach to dynamically make queries and train the distilled model f and our generator G jointly. We perform the following two steps in each iteration. During iteration i: and train the generator and discriminator based on a previously distilled model f i\u22121 . We initialize the weights DISPLAYFORM1 2. Update f i given a fixed generator G i : First, we use f i\u22121 to initialize f i . Then, given the generated adversarial examples x + G i (x) from G i , the distilled model f i will be updated based on the set of new query results for the generated adversarial examples against the black-box model, as well as the original training images. DISPLAYFORM2 where we use both the original images x and the newly generated adversarial examples DISPLAYFORM3 In the experiment section, we compare the performance of both the static and dynamic distillation approaches and observe that simultaneously updating G and f produces higher attack performance. See TAB1 for more details. In this section, we first evaluate AdvGAN for both semi-whitebox and black-box settings on MNIST BID22 BID22 CIFAR-10 (Krizhevsky et al., 2014) . We also perform a semi-whitebox attack on the ImageNet dataset BID8 . We then apply AdvGAN to generate adversarial examples on different target models and test the attack success rate for them under the state-of-theart defenses and show that our method can achieve higher attack success rates compared to other existing attack strategies. We generate all adversarial examples for different attack methods based on the under L \u221e bound of 0.3 on MNIST and 8 on CIFAR-10, for a fair comparison. In general, as shown in TAB0 , AdvGAN has several advantages over other white-box and blackbox attacks. For instance, regarding computation efficiency, AdvGAN performs much faster than others even including the efficient FGSM, although AdvGAN needs extra training time to train the generator. All these strategies can perform targeted attack except transferability based attack, although the ensemble strategy can help to improve. Besides, FGSM and optimization methods can only perform white-box attack, while AdvGAN is able to attack in semi-whitebox setting. Implementation Details Our code and models will be available upon publication. We adopt a similar architecture from image-to-image translation literature BID16 BID16 . In particular, we use the architecture of generator G from BID17 , and our discriminator D's architecture is similar to model C for MNIST and ResNet-32 for CIFAR-10. We apply the loss in BID6 as our loss DISPLAYFORM0 , where t is the target class, and f represents the target network in the semi-whitebox setting and the distilled model in the black-box setting. We set the confidence \u03ba = 0 for both Opt. and AdvGAN. We use Adam as our solver BID18 , with a batch size of 128 and a learning rate of 0.001. For GANs training, we use the least squares objective proposed by LSGAN BID25 , as it has been shown to produce better results with more stable training. and Wide ResNet derived from the variant of \"w32-10 wide.\" 4 We show the classification accuracy of pristine MNIST and CIFAR-10 test data (p) and attack success rate of adversarial examples generated by AdvGAN on different models in TAB1 . First, we apply different architectures for the target model f as listed in Appendix A for MNIST and with ResNet and Wide ResNet for CIFAR-10. We first apply AdvGAN to perform semi-whitebox attack against each model on MNIST dataset. From the performance of semi-whitebox attack (Attack Rate (w)) in TAB1 , we can see that AdvGAN is able to generate adversarial instances to attack all models with high attack success rate. We also generate adversarial examples from the same original instance x, targeting other different classes, as shown in Figures 2. In the semi-whitebox setting on MNIST (a)-(c), we can see that the generated adversarial examples for different models appear close to the ground truth/pristine images (lying on the diagonal of the matrix). FIG1 show the generated adversarial examples on MNIST in black-box setting. These adversarial examples generated by AdvGAN can successfully fool the black-box model and be misclassified as the target class shown on the top. The original images are shown on the diagonal. We also generate adversarial examples based on random original images, and results are shown in Appendix C.In addition, we analyze the attack success rate based on different loss functions on MNIST. Under the same bounded perturbations (0.3), if we replace the full loss function in (4) with L = ||G(x)|| 2 + L f adv , which is similar to the objective used in BID0 , the attack success rate becomes 86.2%. If we replace the loss function with L = L hinge + L f adv , the attack success rate is 91.1%, compared to that of AdvGAN, 98.3%.Similarly, on CIFAR-10, we apply the same semi-whitebox attack for ResNet and Wide ResNet based on AdvGAN, and FIG2 (a) shows some adversarial examples, which are perceptually realistic. We show adversarial examples for the same original instance targeting different other classes. It is clear that with different targets, the adversarial examples keep similar visual quality compared to the pristine instances on the diagonal. We also apply AdvGAN to generate adversarial examples on the ImageNet as shown in FIG3 with L \u221e bound as 8. The added perturbation is unnoticeable while all the adversarial instances are misclassified into other target classes with high confidence. (b) , and (c), and black-box attack, (c), (d) , and (e). On the diagonal, the original images are shown. In this section, we evaluate the performance of AdvGAN for the black-box attack. Our black-box attack here is based on the dynamic distillation strategy. We construct a local model to distill model f , and we select the architecture of Model C as our local model. Note that we randomly select a subset of instances disjoint from the training data of AdvGAN to train the local model; that is, we assume the adversaries do not have any prior knowledge of the training data or the model itself. With the dynamic distillation strategy, the adversarial examples generated by AdvGAN achieve an attack success rate, above 90% for MNIST and 80% for CIFAR-10, compared to 30% and 10% with the static distillation approach, as shown in TAB1 .We apply AdvGAN to generate adversarial examples for the same instance targeting different classes on MNIST and randomly select some instances to show in FIG1 (d)- (f) . By comparing with the pristine instances on the diagonal, we can see that these adversarial instances can achieve high perceptual quality as the original digits. Specifically, the original digit is somewhat highlighted by adversarial perturbations, which implies a type of perceptually realistic manipulation. FIG2 b) shows similar results for adversarial examples generated on CIFAR-10. These adversarial instances appear photo-realistic compared with the original ones on the diagonal. We show additional results in Appendix C. Facing different types of attack strategies, various defenses have been provided. Among them, different types of adversarial training methods are the most effective. BID10 first propose adversarial training as an effective way to improve the robustness of DNNs, and Tram\u00e8r BID27 have also proposed robust networks against adversarial examples based on well-defined adversaries. Given the fact that AdvGAN strives to generate adversarial instances from the underlying true data distribution, it can essentially produce more photo-realistic adversarial perturbations compared with other attack strategies. Thus, AdvGAN could have a higher chance to produce adversarial examples that are resilient under different defense methods. In this section, we quantitatively evaluate this property for AdvGAN compared with other attack strategies. Threat Model As shown in the literature, most of the current defense strategies are not robust when attacking against them BID5 BID13 . Here we consider a weaker threat model, where the adversary is not aware of the defenses and directly tries to attack the original learning model, which is also the first threat model analyzed in BID5 . In this case, if an adversary can still successfully attack the model, it implies the robustness of the attack strategy. Under this setting, we first apply different attack methods to generate adversarial examples based on the original model without being aware of any defense. Then we apply different defenses to directly defend against these adversarial instances. Semi-whitebox Attack First, we consider the semi-whitebox attack setting, where the adversary has white-box access to the model architecture as well as the parameters. Here, we replace f in FIG0 with our model A, B, and C, respectively. As a result, adversarial examples will be generated against different models. We use three adversarial training defenses to train different models for each model architecture: standard FGSM adversarial training (Adv.) BID10 , ensemble adversarial training (Ensemble) BID34 , and iterative training (Iter. Adv.) BID27 . 5 We evaluate the effectiveness of these attacks against these defended models. In TAB2 , we show that the attack success rate of adversarial examples generated by AdvGAN on different models is higher than those of the fast gradient sign method (FGSM) and optimization methods (Opt.) BID4 .Black-box Attack For AdvGAN, we use model B as the black-box model and train a distilled model to perform black-box attack against model B and report the attack success rate in TAB3 . For the black-box attack comparison purpose, transferability based attack is applied for FGSM and optimization-based methods (Opt.). We use FGSM and optimization-based methods (Opt.) to attack model A on MNIST, and we use these adversarial examples to test on model B and report the corresponding classification accuracy. We can see that the adversarial examples generated by the black-box AdvGAN consistently achieve much higher attack success rate compared with other attack methods. For CIFAR-10, we use ResNet as black-box model and train a distilled model to perform black-box attack against ResNet. To evaluate black-box attack for optimization method and FGSM, we use adversarial examples generated by attacking Wide ResNet and test them on ResNet to report black-box attack results for these two methods. In addition, we apply AdvGAN to the MNIST challenge BID28 . Among all the methods, for white-box attack we achieve 88.93% accuracy on the published local model as shown in TAB4 . For the reported black-box attack, we achieved the accuracy as 92.76%, outperforming all other state-of-the-art attack strategies. To understand the adversarial perturbation pattern better, we plot out corresponding perturbations (amplified by a factor of 10) for CIFAR-10 in FIG2 (c) and (d) and ImageNet in FIG3 (b) . From the visualization of perturbation, it shows that the perturbations do not resemble anything in particular about the original image or the target class. Although training AdvGAN exposes it to realistic instances, the perturbations it generates do not simply interpolate towards an example of the target class. To evaluate AdvGAN's ability to generate high resolution adversarial examples, we generate the high resolution adversarial examples for Inception_v3 and quantify their attack success rate and perceptual realism. Experiment settings. In the following experiments, we select toy poodle as our target label for all images. We select 100 benign images from the DEV set of the NIPS 2017 targeted adversarial attack competition. 6 This competition provided a dataset compatible with ImageNet. We generate adversarial examples (299\u00d7299 pixels) under an L \u221e perturbation bound of 0.01 (pixels values are in the range \u2208 [0, 1]) for the Inception_v3 model, whose input size is 299\u00d7299. The details of architectures for generator and discriminator we used are listed in Appendix D. In Figure 8 in the appendix, we show the original images on the left with the correct label, and we show adversarial examples generated by AdvGAN on the right with the target label. Human Perceptual Study. We validate the realism of AdvGAN's adversarial examples with a user study on Amazon Mechanical Turk (AMT). We use 100 pairs of original images and adversarial examples (generated as described above) and ask workers to choose which image of a pair is more visually realistic. Our study follows a protocol from and BID16 , where a worker is shown a pair of images for 2 seconds, then the worker has unlimited time to choose. We limit each worker to at most 20 of these tasks. We collected 500 choices, about 5 per pair of images, from 50 workers on AMT.The AdvGAN examples were chosen as more realistic than the original image in 49.4% \u00b1 1.96% of the tasks (random guessing would result in about 50%). This result show that these high-resolution AdvGAN adversarial examples are about as realistic as benign images. In this paper, we propose AdvGAN to generate adversarial examples using generative adversarial networks (GANs). In our AdvGAN framework, once trained, the feed-forward generator can produce adversarial perturbations efficiently. It can also perform both semi-whitebox and black-box attacks with high attack success rate. In addition, when we apply AdvGAN to generate adversarial instances on different models without knowledge of the defenses in place, the generated adversarial examples can attack the state-of-the-art defenses with higher attack success rate than examples generated by the competing methods. This property makes AdvGAN a promising candidate for improving adversarial training defense methods. The generated adversarial examples produced by AdvGAN preserve high perceptual quality due to GANs' distribution approximation property. A ARCHITECTURE OF MODELS BID16 . Let c3s1-k denotes 3 \u00d7 3 Convolution-InstanceNorm-ReLU layer with k filter and stride 1. Rk means residual block that contains two 3 \u00d7 3 convolution layers with the same numbers of filters. dk denotes the 3 \u00d7 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2. uk denotes a 3 \u00d7 3 fractional-strided-ConvolutionInstanceNorm-ReLU layer with k filters, and stride c3s1-8, d16, d32, r32, r32, r32, r32, u16, u8, c3s1-3 Discriminator architecture We use CNNs as our discriminator network BID31 . Let Ck denote a 4 \u00d7 4 Convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2. After the last conv layer, we apply a FC layer to produce a 1 dimensional output. We do not use InstanceNorm for the first C8 layer. We use leaky ReLUs with slope 0.2. c7s1-8, d16, d32, d64, d64, d64, d64, r64, r64, r64, r64, u64, u64, u64, u64, u32, u16, u8, c7s1-3 The architecture of discriminator for ImageNet is: C8, C16, C32, FC"
}