{
    "title": "rkgU1gHtvr",
    "content": "We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete environments, we demonstrate that our algorithm offers significantly improved accuracy compared to the state-of-the-art methods. In many real-world decision-making scenarios, evaluating a novel policy by directly executing it in the environment is generally costly and can even be downright risky. Examples include evaluating a recommendation policy (Swaminathan et al., 2017; Zheng et al., 2018) , a treatment policy (Hirano et al., 2003; Murphy et al., 2001) , and a traffic light control policy ( Van der Pol & Oliehoek, 2016) . Off-policy policy evaluation methods (OPPE) utilize a set of previously-collected trajectories (for example, website interaction logs, patient trajectories, or robot trajectories) to estimate the value of a novel decision-making policy without interacting with the environment (Precup et al., 2001; Dud\u00edk et al., 2011) . For many reinforcement learning applications, the value of the decision is defined in a long-or infinite-horizon, which makes OPPE more challenging. The state-of-the-art methods for infinite-horizon off-policy policy evaluation rely on learning (discounted) state stationary distribution corrections or ratios. In particular, for each state in the environments, these methods estimate the likelihood ratio of the long-term probability measure for the state to be visited in a trajectory generated by the target policy, normalized by the probability measure generated by the behavior policy. This approach can effectively avoid the exponentially high variance compared to the more classic importance sampling (IS) estimation methods (pre; Dud\u00edk et al., 2011; Hirano et al., 2003; Wang et al., 2017; Murphy et al., 2001) , especially for infinite-horizon policy evaluation (Liu et al., 2018; Nachum et al., 2019; Hallak & Mannor, 2017) . However, learning state stationary distribution requires detailed information on distributions of the behavior policy, and we call them policy-aware methods. As a consequence, policy-aware methods are difficult to apply when off-policy data are pre-generated by multiple behavior policies or when the behavior policy's form is unknown. To address this issue, Nachum et al. (2019) proposes a policy-agnostic method, DualDice, which learns the joint state-action stationary distribution correction that is much higher dimension, and therefore needs more model parameters than the state stationary distribution. Besides, there is no theoretic comparison between policy-aware and policy-agnostic methods. In this paper, we propose a OPPE method with behavior policy learning, EMP (estimated mixture policy) for infinite-horizon off-policy policy evaluation with multiple known or unknown behavior policies. We call EMP a partially policy-agnostic method in the sense that, EMP does not require any information on each\"physical\" behavior policy, instead, it utilizes some aggregated information of the behavior policies learned from data. In detail, EMP includes a pre-estimation step using certain parametric model to learn a \"virtual\" policy (we call it the mixture policy and formally define it in Section 4). Hence, its performance depends on the accuracy of mixture policy estimation. Like the method in Liu et al. (2018) , EMP obtain OPPE also via learning the state stationary distribution correction, so it remains computationally cheap and is scalable in terms of the number of behavior policies. Besides, inspired by Hanna et al. (2019) , we provide a theoretic guarantee that EMP yields smaller mean square error (MSE) than the policy-aware methods in stationary distribution corrections learning, even in the single-behavior policy setting. On the other hand, compared to DualDice, EMP learns the state stationary distribution correction of smaller dimension, more importantly the estimation of the mixture policy can be considered as an inductive bias as far as the stationary distribution correction is concerned, and hence could achieve better performance when the pre-estimation is not expensive. In addition, we propose an ad-hoc improvement of EMP, whose theoretical analysis is left for future studies. EMP is compared with both policy-aware and policy-agnostic methods in a set of continuous and discrete control tasks and shows significant improvement. We first introduce the general setting of OPPE in infinite horizon. Then we review two families of OPPE methods, based on importance sampling (IS) and stationary distribution correction learning, respectively. We consider a Markov Decision Process (MDP) and our goal is to estimate the infinite-horizon average reward. The environment is specified by a tuple M = S, A, R, T , consisting of a state space, an action space, a reward function, and a transition probability function. A policy \u03c0 interacts with the environment iteratively, starting with an initial state s 0 . At step n = 0, 1, ... , the policy produces a distribution \u03c0(\u00b7|s n ) over the actions A, from which an action a n is sampled and applied to the environment. The environment stochastically produces a scalar reward r(s n , a n ) and a next state s n+1 \u223c T (\u00b7|s n , a n ). The infinite-horizon average reward under policy \u03c0 is Without gathering new data, off-policy policy evaluation (OPPE) considers the problem of estimating the expected reward of a target policy \u03c0 via pre-collected state-action-reward tuples from policies that are different from \u03c0, which are called behavior policies. In our paper, we consider the general setting that the data are generated by multiple behavior policies \u03c0 j (j = 1, .., m). Most OPPE literature has focused on the single-behavior-policy case where m = 1. In this case, we denote the behavior policy by \u03c0 0 to distinguish from the multiple-behavior-policy case. Roughly speaking, most OPPE methods can be grouped into two categories: importance-sampling(IS) based OPPE and stationary-distribution-correction based OPPE. As for short-horizon off-policy policy evaluation, importance sampling policy evaluation (IS) methods (Precup et al., 2001; Dud\u00edk et al., 2011; Swaminathan et al., 2017; Precup et al., 2000; Horvitz & Thompson, 1952) have shown promising empirical results. The main idea of importance sampling based OPPE is using importance weighting \u03c0/\u03c0 j to correct the mismatch between the target policy \u03c0 and the behavior policy \u03c0 j that generates the trajectory. One key element in our EMP method are inspired by importance sampling literature. Li et al. (2015) and Hanna et al. (2019) show that using estimated behavior policy in the importance weighting can reduce the mean square error (MSE). EMP also uses estimated policy, but there are two key difference between EMP and the previous works: (1) EMP is not an IS-based method, it involves a min-max problem; (2) EMP focuses on multiple-behavior-policy setting while these papers have focused on single-behavior setting. The state-of-the-art methods for long-horizon off-policy policy evaluation are stationary-distributioncorrection based (Liu et al., 2018; Nachum et al., 2019; Hallak & Mannor, 2017) . Let d \u03c00 (s) and d \u03c0 (s) be the stationary distribution of state s under the behavior policy \u03c0 0 and target policy \u03c0 respectively. The main idea of such methods is directly applying importance weighting by \u03c9 = d \u03c0 /d \u03c00 on the stationary state-visitation distributions to avoid the exploding variance suffered from IS, and estimate the average reward as When the behavior policy \u03c0 0 in (1) is unknown, a natural idea is to estimate it from data. In this section, we focus on the standard case where the data are generated by a single behavior policy so that estimation of behavior policy is more straightforward. We first breifly review the method introduced by Liu et al. (2018) in Section 3.1, which we shall refer as the BCH method in the rest of the paper, to explain the min-max problem formulation of the stationary distribution correction learning task. In Section 3.2, we show that behavior policy estimation is beneficial in two aspects. First, it extends the stationary distribution correction method to settings where behavior policy is unknown. Second, even when the behavior policy is known with exact values, we prove that the stationary distribution correction learned using behavior policy estimation has smaller MSE than that using exact values. Later, we will extend this behavior policy estimation idea to more general multiple-behavior-policy cases in Section 4. Assume the data, consisting of state-action-next-state tuples, are generated by a single behavior policy \u03c0 0 , i.e. D = {(s n , a n , s n ) : n = 1, 2, ..., N }. Recall that d \u03c00 and d \u03c0 are the stationary state distribution under the behavior and target policy respectively, and \u03c9 = d \u03c0 /d \u03c00 is the stationary distribution correction. In the rest of Section 3, by slight notation abuse, we also denote We briefly review the BCH method proposed by Liu et al. (2018) . As d \u03c0 (s) is the stationary distribution of s n as n \u2192 \u221e under policy \u03c0, it follows that: Therefore, for any function f : , so \u03c9 and the data sample satisfy the following equation BCH solves the above equation via the following min-max problem: and use kernel method to solve \u03c9. The derivation of kernel method is put in Appendix A. The objective function in the min-max problem (2), evaluated by data sample, can be viewed as a one-step importance sampling estimation. As shown in Hanna et al. (2019), importance sampling with estimated behavior policy has smaller MSE. Motivated by this fact and the heuristic that better objective function evaluation will lead to more accurate solution, we show that the BCH method can also be improved by using estimated behavior policy to obtain smaller asymptotic MSE. We will use this result to build theoretic guarantee for the performance of EMP method in Section 4. To formally state the theoretic result, we need to introduce more notation. Assume that we are given a class of stationary distribution correction \u2126 = {\u03c9(\u03b7; s) : \u03b7 \u2208 E \u03b7 }, and there exists \u03b7 0 \u2208 E \u03b7 such that the true distribution correction \u03c9(s) = \u03c9(\u03b7 0 ; s). Let \u03c9(\u03b7; s) be the stationary distribution correction learned by the min-max problem (2) and \u03c9(\u03b7; s) be that learned by a min-max problem similar to (2) with \u03c0 0 replaced with its estimation\u03c0 0 . Intuitively,\u03c0 0 is estimated from the data sample and appears in the denominator, as a result, it could cancel out a certain amount of random error in data sample. Following this intuition and applying the proof techniques in Henmi et al. (2007), we establish the following theoretic guarantee that using estimated behavior policy 1 yields better estimates of the stationary distribution correction. Theorem 1. Under Assumptions 1 and 2, we have, asymptotically As a direct consequence, we derive the finite-sample error bound for\u03b7. Corollary 1. Let N be the number of (s, a, s ) tuples in the data. Under Assumptions 1 and 2, Due to the space limit, we put the precise descriptions of Assumptions 1 and 2 (which involves details of the kernel method that solves the min-max problem (2)) for Theorem 1 and Corollary 1 to hold and their proofs are in Appendix B. In this section, we apply the behavior policy estimation idea and develop our EMP method for OPPE in settings of multiple behavior policies and establish theoretic variance reduction results for EMP. In Section 4.1, we first clarify what is the policy to estimate from data when there are multiple behavior policies. Then, we introduce EMP in Section 4.2 and establish variance reduction result in 4.3. Let's first take a closer look at the distribution of data generated by multiple behavior policies. In particular, we show that the data from different behavior policies can be pooled together as if they are generated by a virtual\"mixture policy\" \u03c0 M , which plays a key role in derivation of EMP method. In the multiple-behavior setting, we assume the state-action-next-state tuples are generated by m different unknown behavior policies \u03c0 j , j = 1, 2, ..., m. For each j, there are N j state-action-nextstate tuples generated by \u03c0 j and follows the corresponding stationary state distribution d \u03c0j . Let N = j N j and denote by w j = N j /N the proportion of data generated by policy \u03c0 j . We use D M to denote the data set, then D M = {(s j,nj , a j,nj , s j,nj ) : j = 1, 2, .., m, n j = 1, 2, ..., N j }. Note that the policy label j in the subscript is only for notation clarity and it is not revealed in the data. Then, if we randomly draw a single (s, a, s ) tuple from D M , its distribution function is the mixture of state-action-next-state tuple distributions generated by each behavior policy: With slight notation abuse, we write d M (s) = j w j d \u03c0j as the mixture state distribution. For each state-action pair (a, s), define the mixture policy \u03c0 M (a|s) as the weighted average of the behavior policies: The following result shows that, the multiple-behavior-policy data D M and the corresponding state distribution d M (s) can be viewed as if they were generated by the mixture policy \u03c0 M Proposition 1. The state-action-next-station tuples generated by \u03c0 M follows the distribution be the likelihood ratio of the state distribution generated by the target policy over the mixture state distribution. We can estimate the average reward by \u03c9 M : Proposition 2. The average award satisfies 4.2 EMP METHOD Proposition 1 shows that d M (s), the state distribution generated by multiple behavior policies, is equal to the state stationary distribution generated by \u03c0 M . If \u03c0 M is known, then, following the same argument of BCH, we can learn \u03c9 M (s) = \u03c9(\u03b7 0 ; s) via a similar min-max problem as (2), by replacing d \u03c00 with d M and \u03c0 0 with \u03c0 M , and then, estimate R \u03c0 using (4), by replacing d \u03c00 with d M , \u03c0 0 with \u03c0 M and \u03c9 with \u03c9 M . But \u03c0 M is usually unknown. Indeed, it involves not only the behavior polices \u03c0 j , but also their stationary distributions d \u03c0j , which are unknown and hard to compute. In our EMP method, we will estimate \u03c0 M directly from data, without learning \u03c0 j and d \u03c0j . In particular, we assume the mixture policy \u03c0 M belongs to some parametric family, i.e. \u03c0 M (a|s) = \u03c0(\u03b8 M ; a, s). For instance, \u03c0(\u03b8; a, s) could be a regression model or a neural network. We then estimate \u03b8 M by MLE, i.e.\u03b8 After this pre-estimation step, we replace the exact mixture policy \u03c0 M with the estimated mixture policy \u03c0(\u03b8 M ; \u00b7) and finally formulate the following min-max problem for EMP to learn \u03c9 M : Applying Theorem 1, we build the following MSE bound for EMP method. Theorem 2. Under the same conditions of Theorem 1, if \u03c9(\u03b7; s) and \u03c9(\u03b7; s) are the stationary distribution correction learned from (6) and from the same min-max problem but with exact value of \u03c0 M , then, asymptotically One important feature of EMP is that it pools the data from different policy behaviors together and treat them as if they are from a single mixture policy. Of course, pooling makes EMP applicable to settings with minimal information on the behavior policies, for instance, EMP does not even require the knowledge on the number of behavior policies. In this part, we show that, the pooling feature of EMP is not just a compromise to the lack of behavior policy information, it also leads to variance reduction in an intrinsic manner. If instead, the data are treated separately according to the behavior policies, we can still use EMP, or any OPPE method for single behavior policy, to obtain the stationary distribution correction \u03c9 j = d \u03c0 /d \u03c0j for each behavior policy. Given \u03c9 j , a common approach for variance reduction is to apply multiple importance sampling (MIS) (Tirinzoni et al., 2019; Veach & Guibas, 1995) technique and the average reward estimator is of the form where the function h is often referred to as heuristics and must be a partition of unity, i.e., j h j (s) = 1 for all s \u2208 S. It has been proved by Veach & Guibas (1995) that MIS is unbiased, and, for given w j = N j /N , there is an optimal heuristic function to minimize the variance ofR M IS . , \u2200j = 1, 2, ..., m and s \u2208 S, reaches the minimal variance. Plug the optimal heuristic h j (s) into MIS estimator (7), and we will obtain that the optimal MIS estimator coincides with the EMP estimator (4). In this light, by pooling the data together and directly learning \u03c9 M , EMP also learns the optimal MIS weight inexplicitly. In this section, we evaluate EMP on OPPE problems in three discrete-control tasks Taxi, Singlepath, Gridworld and one continuous-control task Pendulum (see Appendix D.1 for the details), in both single-behavior-policy (Section 5.1)and multiple-behavior-policy settings (Section 5.2), with following purposes: (i) to compare the performance of EMP with existing OPPE methods; (ii) to validate the theoretical properties for EMP; (iii) to explore potential improvement of EMP methods for future study. We will release the codes with the publication of this paper for relevant study. In this section, we compare the EMP method with BCH, DualDice 2 and step-wise importance sampling (IS) in the setting of single-behavior policy, i.e. the data is generated from a single behavior policy. Experiment Set-up. A single behavior policy is learned by a certain reinforcement learning algorithm 3 for evaluating BCH and IS. This single behavior policy then generates a set of trajectories consisting of s-a-s-r tuples. These tuples are used to estimate the behaviour policy in EMP method 4 as well as estimating the stationary distribution corrections for estimating the average step reward of the target policy. Stationary Distribution Learning Performance. To validate Theorem 1, we use the Taxi domain as an example to compare the stationary distributiond \u03c0true andd \u03c0esti learned by BCH (using exact behavior policy) and EMP (using estimated behavior policy). Figure 1 The results indicate that bothd \u03c0true andd \u03c0esti converge, whiled \u03c0esti converges faster and is significantly closer to d \u03c0 when the data size is small. These observations are well consistent with Theorem 1. Figure 2 reports the MSE of policy evaluation by EMP, BCH, DualDice and IS methods for the 4 different environments. We observe that, (i) EMP consistently obtains smaller MSE than the other three methods for different sample scales and different environments. (ii) The performance of EMP, BCH and DualDice improves as the number of trajectories and length of horizons increase, while the IS method suffers from growing variance. In this section, we compare the performance of EMP with a multiple-behavior version of BCH method, DualDice and MIS (Precup et al., 2000) , and explore potential improvement of EMP methods. Figure 3 shows that, in all 4 environments, we found EMP consistently obtain smaller MSE than the other three methods. Pooling is Beneficial. To validate the variance reduction result Proposition 3, we compare EMP with its variation EMP (single) in which trajectories from different behaviors are not pooled. In detail, EMP (single) applies EMP to each group of data generated by same behavior policy and return the mean average reward estimation. Figure 4 shows that EMP outperforms EMP (single), which is consistent with the theoretic variance reduction result. On the other hand, the optimality of EMP in Proposition 3 holds for fixed w j , we now explore the possibility of further variance reduction via optimizing w j , i.e. , by re-weighting the proportion of samples generated by different behavior policies. In detail, we implement a variation of EMP in which the data samples are re-weighted according to the KL-divergence between its behavior policy and the target policy 6 . Figure 4 also shows that the performance of KL-EMP has greater improvement with the increase of sample size and could outperform EMP in cases of large sample size where the KL-divergence is better estimated. In this paper, we advocate the viewpoint of partial policy-awareness and the benefits of estimating a \"virtual\" mixture policy for off-policy policy evaluation. The theoretical results of reduced variance coupled with experimental results illustrate the power of this class of methods. One key question that still remains is the following: if we are willing to estimate the individual behavior policies, can we further improve EMP by developing an efficient algorithm to compute the optimal weights? The preliminary experiment results suggest that the answer would be yes, and we will leave this for future study. We use the reproducing kernel Hilbert space to solve the mini-max problem of BCH (Liu et al. (2018) ). The key property of RKHS we leveraged is called reproducing property. The reproducing property claims, for any function f \u2208 H (H is a RKHS), the evaluation of f at point x equals its inner product with another function in RKHS: Given the objective function of BCH L(w, f ) = E (s,a,s )\u223cd\u03c0 0 [(\u03c9(s) \u03c0(a|s \u03c00(a|s)\u2212\u03c9 )f (s )]. We use the reproducing property to obtain the closed form representation of max f \u2208F L(w, f ) 2 , which is shown as follows: This equation has been proved in BCH Liu et al. (2018) . B PROOF OF THEOREM 1 In this appendix, we provide the mathematical details and proof of Theorem 1. We first introduce some notations and assumptions. We assume the behavior policy \u03c0 0 (a|s) belongs to a class of policies \u03a0 = {\u03c0(\u03b8; a, s) : \u03b8 \u2208 E \u03b8 }, where E \u03b8 is the parameter space, i.e. there exists \u03b8 0 \u2208 E \u03b8 such that \u03c0 0 (a|s) = \u03c0(\u03b8 0 ; a, s). The estimated behavior policy\u03c0 0 (a|s) = \u03c0(\u03b8; a, s) is obtained via maximum likelihood method, i.e. log(\u03c0(\u03b8; s n , a n )). We assume central limit theorem holds for\u03b8: Recall that we have assumed in Section 3.2 that the true stationary distribution correction \u03c9(s) = \u03c9(\u03b7 0 ; s). Using the kernel method introduced in Appendix A, we estimate\u03c9(s) = \u03c9(\u03b7; s) b\u0177 , The BCH method estimate\u03b8 b\u1ef9 Assumption 2. We assume the following regularity conditions on G: 1. G is second order differentiable. ] is finite and non-zero. Here we simply write E xi\u223cd\u03c0 0 ,xj \u223cd\u03c0 0 as E for the simplicity of notation. Theorem 1. Under Assumptions 1 and 2, we have, asymptotically Proof. Following the kernel method, . By definition,\u03b7 is the solution to the optimization problemmax \u03b7 1\u2264i,j\u2264N G(\u03b7, \u03b8 0 ; (x i , x j )). There- . Following the proof of Theorem 1 of (Henmin et al. 2007) , it suffices to prove that The last equality holds because ( For the simplicity of derivation, in the rest of proof, we denote On the other hand, note that Here, we denote Therefore, E[(k(s 1 , s 2 ) + k(s 2 , s 1 )) (H 1 + H 2 )] = 0. So we obtain (8). B.3 PROOF OF COROLLARY 1 Corollary 1. Let N be the number of (s, a, s ) tuples in the data. Under Assumptions 1 and 2, Proof. In the prove of Theorem 1, we see that 1 N 2 1\u2264i,j\u2264N . We assume that CLT holds for the maximum likelihood estimator\u03b8, i.e. , under Condition 4 of Assumption 2, , we can apply the central limit theorem (for stationary Markov chain) and have Propostion 1. The state-action-next-station tuples generated by \u03c0 M follows the distribution d M (s, a, s ). As a consequence, d M (s) is the stationary state distribution generated by \u03c0 M . Proof. It suffices to check that for any s , For each behavior policy \u03c0 j , we have Note that the left hand side is simple d M (s ). In the right hand side, and then we immediately obtain (9). Propostion 2. The average award satisfies Proof. Theorem 2. Under the same conditions of Theorem 1, if \u03c9(\u03b7; s) and \u03c9(\u03b7; s) are the stationary distribution correction learned from (6) and from the same min-max problem but with exact value of \u03c0 M , then, asymptotically As a result, Proof. The proof follows immediately from that of Theorem 1. In particular, assume \u03c0 M \u2208 {\u03c0(\u03b8; a, s) : \u03b8 \u2208 E \u03b8 } and the estimated\u03c0 M = \u03c0(\u03b8; \u00b7) is obtained vi\u00e2 log(\u03c0(\u03b8; s j,n , a j,n )). Gridworld (Thomas & Brunskill, 2016 ) is a 4 \u00d7 4 grid world which including one reward state, one terminate state and one fire state and thirteen normal state. Four action can be taken in this environment: up, down, left and right. A reward of -1 will be received while the agent in normal states, 1 reward is obtained in reward state, 100 reward is got in terminate state and -11 reward will got in fire state. SinglePath has 5 states, 2 actions. The agent begins in state 0 and both actions either take the agent from state n to state n + 1 or cause the agent to remain in state n. If the agent arrives at a new state, it will receive a +1 reward, otherwise it will get a 1 reward. Pendulum has a continuous state space of R 3 which describes the triangle of and a action space of [\u22122, 2]. For the discrete environments Taxi, Gridworld and SinglePath, the MLE estimate (5) coincide with count-frequency, and therefore, we directly estimat\u00ea For the continuous environment Pendulum, we use a neural network to model the policy. In detail, we train a two-layer MLP neural network to estimate the policy. The size of the two hidden layers are both 32 with the learning rate 0.001 and tanh activation function. We use MEL (5) and Adam optimizer to train the neural network with batch size 128. In EMP algorithm, the proportion of samples from policy \u03c0 j in the data buffer, w j , is fixed. In an ad-hoc way, we optimize the weights w KL j according to the KL-divergence between the behavior policy \u03c0 j and the target policy. Then, we will generate a new data buffer as follows. First, we sample j \u2208 {1, 2, ..., m} with probability w KL j . Then, given j, we sample uniformly from the subgroup of data generated by j. Note that, to implement KL-EMP, one do not need to know the exact value of \u03c0 j , but one need to know which data are generated from which policy. In the numerical experiment, we use the following formula to compute w KL for finite state space: s\u2208S 1(i = arg min 1\u2264k\u2264m D KL (\u03c0(\u00b7|s)||\u03c0 k (\u00b7|s))) = s\u2208S 1(j = arg min 1\u2264k\u2264m D KL (\u03c0(\u00b7|s)||\u03c0 k (\u00b7|s))) |S| To implement this method for infinite or continuous state space, we replace the set of all possible states S in (10) with the set of all states that has been visited in the data buffer. Besides, the behavior policy \u03c0 k is unknown, to estimate the KL-divergence D KL (\u03c0(\u00b7|s)|\u03c0 k (\u00b7|s)), we use a neural network to learn \u03c0 k from the subgroup of data that are generated from policy \u03c0 k . The numerical results show that using the KL weights {w KL j } could achieve smaller MSE compared to using {w j } as given by the data sample. We believe this approach deserves more careful analysis in future research studies. Note that EMP, EMP (single) and KL-EMP all have their policy-aware analogues. In order to test the variance reduction effect of policy estimation, we implement the policy-aware version for each EMP-type algorithm and compare their performances. The policy-aware version of EMP (single) is naive BCH, in which we first apply BCH to each behavior policy and then return the estimation average. The policy-aware version of EMP is named as BCH (pooled). In BCH (pooled), the corresponding min-max problem formation is They both pool the data from different behavior policies together and the main difference is that BCH (pooled) uses the exact behavior policies. The policy-aware version of KL-EMP is called BCH (KL-polled). The main difference between BCH (KL-polled) and BCH (polled) is that BCH (KL-polled) utilizes KL-divergence to optimize the weights. Comparison results are shown in Figure 5 . We observe that the partially policy-agnostic methods consistently outperform their policy-aware analogues."
}