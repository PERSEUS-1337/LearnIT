{
    "title": "Sy_MK3lAZ",
    "content": "Most existing deep reinforcement learning (DRL) frameworks consider action spaces that are either\ndiscrete or continuous space. Motivated by the project of design Game AI for King of Glory\n(KOG), one the world\u2019s most popular mobile game, we consider the scenario with the discrete-continuous\nhybrid action space. To directly apply existing DLR frameworks, existing approaches\neither approximate the hybrid space by a discrete set or relaxing it into a continuous set, which is\nusually less efficient and robust. In this paper, we propose a parametrized deep Q-network (P-DQN)\nfor the hybrid action space without approximation or relaxation. Our algorithm combines DQN and\nDDPG and can be viewed as an extension of the DQN to hybrid actions. The empirical study on the\ngame KOG validates the efficiency and effectiveness of our method. In recent years, the exciting field of deep reinforcement learning (DRL) have witnessed striking empirical achievements in complicated sequential decision making problems that are once believed unsolvable. One active area of the application of DRL methods is to design artificial intelligence (AI) for games. The success of DRL in the game of Go provides a promising methodology for game AI. In addition to the game of Go, DRL has been widely used in other games such as Atari BID19 , Robot Soccer BID8 BID17 , and Torcs ) to achieve super-human performances. However, most existing DRL methods only handle the environments with actions chosen from a set which is either finite and discrete (e.g., Go and Atari) or continuous (e.g. MuJoCo and Torcs) For example, the algorithms for discrete action space include deep Q-network (DQN) BID18 , Double DQN (Hasselt et al., 2016) , A3C BID20 ; the algorithms for continuous action space include deterministic policy gradients (DPG) BID29 and its deep version DDPG .Motivated by the applications in Real Time Strategic (RTS) games, we consider the reinforcement learning problem with a discrete-continuous hybrid action space. Different from completely discrete or continuous actions that are widely studied in the existing literature, in our setting, the action is defined by the following hierarchical structure. We first choose a high level action k from a discrete set {1, 2, \u00b7 \u00b7 \u00b7 , K}; upon choosing k, we further choose a low level parameter x k \u2208 X k which is associated with the k-th high level action. Here X k is a continuous set for all k \u2208 {1, . . . , K}.1 Therefore, we focus on a discrete-continuous hybrid action space A = (k, x k ) x k \u2208 X k for all 1 \u2264 k \u2264 K .To apply existing DRL approaches on this hybrid action space, two straightforward ideas include:\u2022 Approximate A by an finite discrete set. We could approximate each X k by a discrete subset, which, however, might lose the natural structure of X k . Moreover, when X k is a region in the Euclidean space, establishing a good approximation usually requires a huge number discrete actions.\u2022 Relax A into a continuous set. To apply existing DRL framework with continuous action spaces, BID8 define the following approximate space DISPLAYFORM0 where F k \u2286 R. Here f 1 , f 2 , . . . , f K is used to select the discrete action either deterministically (by picking arg max i f i ) or randomly (with probability softmax(f )). Compared with the original action space A, A might significantly increases the complexity of the action space. Furthermore, continuous relaxation can also lead to unnecessary confusion by over-parametrization. For example, (1, 0, \u00b7 \u00b7 \u00b7 , 0, x 1 , x 2 , x 3 , \u00b7 \u00b7 \u00b7 , x K ) \u2208 A and (1, 0, \u00b7 \u00b7 \u00b7 , 0, x 1 , x 2 , x 3 , \u00b7 \u00b7 \u00b7 , x K ) \u2208 A indeed represent the same action (1, x 1 ) in the original space A.In this paper, we propose a novel DRL framework, namely parametrized deep Q-network learning (P-DQN), which directly work on the discrete-continuous hybrid action space without approximation or relaxation. Our method can be viewed as an extension of the famous DQN algorithm to hybrid action spaces. Similar to deterministic policy gradient methods, to handle the continuous parameters within actions, we first define a deterministic function which maps the state and each discrete action to its corresponding continuous parameter. Then we define a action-value function which maps the state and finite hybrid actions to real values, where the continuous parameters are obtained from the deterministic function in the first step. With the merits of both DQN and DDPG, we expect our algorithm to find the optimal discrete action as well as avoid exhaustive search over continuous action parameters. To evaluate the empirical performances, we apply our algorithm to King of Glory (KOG), which is one of the most popular online games worldwide, with over 200 million active users per month. KOG is a multi-agent online battle arena (MOBA) game on mobile devices, which requires players to take hybrid actions to interact with other players in real-time. Empirical study indicates that P-DQN is more efficient and robust than BID8 's method that relaxes A into a continuous set and applies DDPG. In reinforcement learning, the environment is usually modeled by a Markov decision process (MDP) M = {S, A, p, p 0 , \u03b3, r}, where S is the state space, A is the action space, p is the Markov transition probability distribution, p 0 is the probability distribution of the initial state, r(s, a) is the reward function, and \u03b3 \u2208 [0, 1] is the discount factor. An agent interacts with the MDP sequentially as follows. At the t-th step, suppose the MDP is at state s t \u2208 S and the agent selects an action a t \u2208 A, then the agent observe an immediate reward r(s t , a t ) and the next state s t+1 \u223c p(s t+1 |s t , a t ). A stochastic policy \u03c0 maps each state to a probability distribution over A, that is, \u03c0(a|s) is defined as the probability of selecting action a at state s. Whereas a deterministic \u00b5 : S \u2192 A maps each state to a particular action in A. Let R t = j\u2265t \u03b3 j\u2212t r(s j , a j ) be the cumulative discounted reward starting from time-step t. We define the state-value function and the action-value function of policy \u03c0 as V \u03c0 = E(R t |S t = s; \u03c0) and Q \u03c0 (s, a) = E(R t |S 0 = s, A 0 = a; \u03c0), respectively. Moreover, we define the optimal state-and action-value functions as V \u03c0 = sup \u03c0 V \u03c0 and Q * = sup \u03c0 Q \u03c0 , respectively, where the supremum is taken over all possible policies. The goal of the agent is to find a policy the maximizes the expected total discounted reward J(\u03c0) = E(R 0 |\u03c0), which is can be achieved by estimating Q * . Broadly speaking, reinforcement learning algorithms can be categorized into two classes: value-based methods and policy-based methods. Value-based methods first estimate Q * and then output the greedy policy with respect to that estimate. Whereas policy-based methods directly optimizes J(\u03c0) as a functional of \u03c0. The Q-learning algorithm BID36 ) is based on the Bellman equation DISPLAYFORM0 which has Q * as the unique solution. In the tabular setting, the algorithm updates the Q-function by iteratively applying the sample counterpart of the Bellman equation DISPLAYFORM1 where \u03b1 > 0 is the stepsize and s is the next state observed given the current state s and action a. However, when the state space S is so large that it is impossible to store all the states in memory, function approximation for Q * is applied. Deep Q-Networks (DQN) BID19 approximates Q * using a neural network Q(s, a; w) \u2248 Q(s, a), where w is the network weights. In the t-th iteration, the DQN updates the parameter using the gradient of the least squares loss function DISPLAYFORM2 In practice, DQN is trained with techniques such as experience replay and asynchronous stochastic gradient descent methods BID20 ) which enjoy great empirical success. In addition to the value-based methods, the policy-based methods directly models the optimal policy. In specific, let \u03c0 be any policy. We write p t (\u00b7|s; \u03c0) as the distribution of S t given S 1 = s with actions executed according to policy \u03c0. We define the discounted probability distribution \u03c1 \u03c0 by DISPLAYFORM3 Then the objective of policy-based methods is to find a policy that maximizes the expected reward DISPLAYFORM4 Let \u03c0 \u03b8 be a stochastic policy parametrized by \u03b8 \u2208 \u0398. For example, \u03c0 \u03b8 could be a neural network in which the last layer is a softmax layer with |A| neurons. The stochastic gradient methods aims at finding a parameter \u03b8 that maximizes J(\u03c0 \u03b8 ) via gradient descent. The stochastic policy gradient theorem BID33 states that DISPLAYFORM5 3)The policy gradient algorithm iteratively updates \u03b8 using estimates of (2.3). For example, the REINFORCE algorithm BID37 updates \u03b8 using \u2207 \u03b8 log \u03c0 \u03b8 (a t |s t ) \u00b7 r t . Moreover, the actor-critic methods use another neural network Q(s, a; w) to estimate the value function Q \u03c0 \u03b8 (s, a) associated to policy \u03c0 \u03b8 . This algorithm combines the value-based and policy-based perspectives together, and is recently used to achieve superhuman performance in the game of Go BID31 . When the action space is continuous, value-based methods will no longer be computationally tractable because of taking maximum over the action space A in (2.2), which in general cannot be computed efficiently. The reason is that the neural network Q(s, a; w) is nonconvex when viewed as a function of a; max a\u2208A Q(s, a; w) is the global minima of a nonconvex function, which is NP-hard to obtain in the worst case. To resolve this issue, the continuous Q-learning BID6 rewrite the action value function as Q(s, a) = V (s) + A(s, a), where V (s) is the state value function and A(s, a) is the advantage function that encodes the relative advantage of each action. These functions are approximated by neural networks V (s; \u03b8 V ) and A(s, a; \u03b8 A ), respectively, where \u03b8 V and \u03b8 A are network weights. The action value function is given by DISPLAYFORM0 Then in the t-th iteration, the continuous Q-learning updates \u03b8 v and \u03b8 a by taking a gradient step using the least squares loss function DISPLAYFORM1 Moreover, it is also possible to adapt policy-based methods to continuous action spaces by considering deterministic policies. Let \u00b5 \u03b8 : S \u2192 A be a deterministic policy. Similar to (2.3), the deterministic policy gradient (DPG) theorem BID29 states that DISPLAYFORM2 Furthermore, this deterministic version of the policy gradient theorem can be viewed as the limit of (2.3) with the variance of \u03c0 \u03b8 going to zero. Based on (2.4), the DPG algorithm BID29 and the deep deterministic policy gradient (DDPG) algorithm are proposed. General reinforcement learning There is a huge body of literature in reinforcement learning, we refer readers to textbooks by Sutton & Barto (1998); Szepesv\u00e1ri FORMULA6 for detailed introduction. Combined with the recent advancement of deep learning BID5 , deep reinforcement learning becomes a blossoming field of research with a plethora of new algorithms which achieve surprising empirical success in a variety of applications that are previously considered extremely difficult and challenging. Finite discrete action space methods For reinforcement learning problems with finite action spaces, BID18 propose the DQN algorithm, which first combines the deep neural networks with the classical Q-learning algorithm BID36 . A variety of extensions are proposed to improve DQN, including Double DQN , dueling DQN BID35 , bootstrap DQN BID23 , asynchronous DQN BID20 , and averaged- DQN Anschel et al. (2017) .In terms of policy-based methods, BID33 propose the REINFORCE algorithm, which is the basic form of policy gradient. An important extension is the actor-critic method BID13 , whose asynchronous deep version A3C BID20 produces the stateof-the-art performances on the Arcade Learning Environment (ALE) benchmark BID1 .Continuous action space methods Moreover, for DRL on continuous action spaces, BID29 proposes the deterministic policy gradient algorithm and deterministic actor-critic algorithms. This work is further extended by , which propose the DDPG algorithm, which is an model-free actor critic algorithm using deep neural networks to parametrize the policies. A related line of work is policy optimization methods, which improve the policy gradient method using novel optimization techniques. These methods include natural gradient descent BID12 , trust region optimization BID27 , proximal gradient descent BID28 , mirror descent BID21 , and entropy regularization BID22 .Hybrid actions A related body of literature is the recent work on reinforcement learning with a structured action space, which contains finite actions each parametrized by a continuous parameter. To handle such parametrized actions, BID8 applies the DDPG algorithm on the relaxed action space directly, and BID17 proposes a learning framework updating the parameters for discrete actions and continuous parameters alternately. Game AI Recently remarkable advances have been made in building AI bots for computer games using deep reinforcement learning. These games include Atari Games, a collection of video games, Texas Hold'em, a multi-player poker game, and Doom, a first-person shooter game. See BID18 ; BID9 ; BID15 ; BID2 for details and see BID11 for a comprehensive survey. More notably, the computer Go agent AlphaGo achieves super-human performances by defeating the human world champion Lee Sedol. Two more complicated class of games are the real-time strategy (RTS) games and MOBA games. These are multi-agent games which involves searching within huge state and action spaces that are possibly continuous. Due to the difficulty of these problems, current research for these games are rather inadequate with most existing work consider specific scenarios instead of the full-fledged RTS or MOBA games. See, e.g., BID4 ; BID24 for an recent attempt on applying DRL methods to RTS games. This section introduces the proposed framework to handle the application with hybrid discretecontinuous action space. We consider a MDP with a parametrized action space A, which consists of K discrete actions each associated with a continuous parameter. In specific, we assume that any action a \u2208 A can be written as a = (k, x k ), where k \u2208 {1, . . . , K} is the discrete action, and x k \u2208 X k is a continuous parameter associated with the k-th discrete action. Thus action a is a hybrid of discrete and continuous components with the value of the continuous action determined after the discrete action is chosen. Then the parametrized action space A can be written as DISPLAYFORM0 In the sequel, we denote {1, . . . , K} by [K] for short. For the action space A in (4.1), we denote the action value function by Q(s, a) = Q(s, k, x k ) where s \u2208 S, 1 \u2264 k \u2264 K, and x k \u2208 X k . Let k t be the discrete action selected at time t and let x kt be the associated continuous parameter. Then the Bellman equation becomes DISPLAYFORM1 Here inside the conditional expectation on the right-hand side of (4.2), we first solve DISPLAYFORM2 , and then take the largest Q(s t+1 , k, x * k ). Note that taking supremum over continuous space X k is computationally intractable. However, the right-hand side of (4.2) can be evaluated efficiently providing x * k is given. To elaborate this idea, first note that, when the function Q is fixed, for any s \u2208 S and k \u2208 [K], we can view x Q k (s) = argsup DISPLAYFORM3 as a function of state s. That is, we identify (4.3) as a function x Q k : S \u2192 X k . Then we can rewrite the Bellman equation in (4.2) as DISPLAYFORM4 Note that this new Bellman equation resembles the classical Bellman equation in (2.1) with A = [K]. Similar to the deep Q-networks, we use a deep neural network Q(s, k, x k ; \u03c9) to approximate Q(s, k, x k ), where \u03c9 denotes the network weights. Moreover, for such a Q(s, k, x k ; \u03c9), we approximate x Q k (s) in (4.3) with a deterministic policy network x k (\u00b7; \u03b8) : S \u2192 X k , where \u03b8 denotes the network weights of the policy network. That is, when \u03c9 is fixed, we want to find \u03b8 such that Q s, k, x k (s; \u03b8); \u03c9 \u2248 sup DISPLAYFORM5 Remark 4.1. Readers who are familiar with the work by BID8 , that also claims to handle discrete-continuous hybrid action spaces, may be curious of its difference from the proposed P-DQN. The key differences are as follows.\u2022 In BID8 , the discrete action types are parametrized as some continuous values, say f . And the discrete action that is actually executed is chosen via k = arg max i f (i). Such a trick actually turns the hybrid action space into a continuous action space, upon which the classical DDPG algorithm can be applied. However, in our framework, the discrete action type is chosen directly by maximizing the action's Q value explicitly.\u2022 The Q network in BID8 uses the artificial parameters f as input, which makes it an action-value function estimator of current policy (Q \u03c0 ). While in our framework, the Q network is actually an approximate estimator of the optimal policy's action-value function (Q ).\u2022 We note that P-DQN is an off-policy method that can use historical data, while it is hard to use historical data in BID8 because there is only discrete action k without parameters f .(a) Network of P-DQN (b) Network of DDPG Figure 1 : Illustration of the networks of P-DQN and DDPG BID8 . P-DQN selects the discrete action type by maximizing Q values explicitly; while in DDPG, the discrete action with largest f , which can be seen as a continuous parameterization of K discrete action types, is chosen. Also in P-DQN the state and action parameters are feed into the Q-network which outputs K action values for each action type; while in DDPG, the continuous parameterization f , instead of the actual action k taken, is feed into the Q-network. Suppose that \u03b8 satisfies (4.4), then similar to DQN, we could estimate \u03c9 by minimizing the meansquared Bellman error via gradient descent. In specific, in the t-th step, let \u03c9 t and \u03b8 t be the weights of the value network and the deterministic policy network, respectively. To incorporate multi-step algorithms, for a fixed n \u2265 1, we define the n-step target y t by DISPLAYFORM0 We define the least squares loss function for \u03c9 by DISPLAYFORM1 Moreover, since we aim to find \u03b8 that minimizes Q[s, k, x k (s; \u03b8); \u03c9] with \u03c9 fixed, we define the loss function for \u03b8 by DISPLAYFORM2 Then we update \u03c9 t and \u03b8 t by gradient-based optimization methods. Moreover, the gradients are given by DISPLAYFORM3 Here \u2207 x Q(s, k, x k ; \u03c9) and \u2207 \u03c9 Q(s, k, x k ; \u03c9) are the gradients of the Q-network with respect to its third argument and fourth argument, respectively. By (5.5) and (5.4) we update the parameters using stochastic gradient methods. In addition, note that in the ideal case, we would minimize the loss function \u0398 t (\u03b8) in (5.3) when \u03c9 t is fixed. From the results in stochastic approximation methods BID14 , we could approximately achieve such a goal in an online fashion via a two-timescale update rule BID3 . In specific, we update \u03c9 with a stepsize \u03b1 t that is asymptotically negligible compared with the stepsize \u03b2 t for \u03b8. In addition, for the validity of Input: Stepsizes {\u03b1t, \u03b2t} t\u22650 , exploration parameter , minibatch size B, the replay memory D, and a probability distribution \u00b5 over the action space A for exploration. Initialize network weights \u03c91 and \u03b81. for t = 1, 2, . . . , T doCompute action parameters x k \u2190 x k (s , \u03b8t). Select action at = (kt, x k t ) according to the -greedy policy at = a sample from distribution \u00b5 with probability , (kt, x k t ) such that kt = arg max k\u2208[K] Q(s , k, x k ; \u03c9t) with probability 1 \u2212 .Take action at, observe reward rt and the next state st+1. DISPLAYFORM0 Use data {y b , s b , a b } b\u2208 [B] to compute the stochastic gradient \u2207\u03c9 Q t (\u03c9) and \u2207 \u03b8 \u0398 t (\u03b8) defined in (5.5) and (5.4). Update the parameters by \u03c9t+1 \u2190 \u03c9t \u2212 \u03b1t \u00b7 \u2207\u03c9 Q t (\u03c9t) and \u03b8t+1 \u2190 \u03b8t \u2212 \u03b2t \u00b7 \u2207 \u03b8 \u0398 t (\u03b8t). end for stochastic approximation, we require {\u03b1 t , \u03b2 t } to satisfy the Robbins-Moron condition BID25 . We present the P-DQN algorithm with experienced replay in Algorithm 1.Note that this algorithm requires a distribution \u00b5 defined on the action space A for exploration. In each step, with probability , the agent sample an random action from \u00b5; otherwise, it takes the greedy action with respect to the current value function. In practice, if each X k is a compact set in the Euclidean space (as in our case), \u00b5 could be defined as the uniform distribution over A. In addition, as in the DDPG algorithm (Lillicrap et al., 2016), we can also add additive noise to the continuous part of the actions for exploration. Moreover, we use experience replay BID18 to reduce the dependencies among the samples, which can be replaced by more sample-efficient methods such as prioritized replay .Moreover, we note that our P-DQN algorithm can easily incorporate asynchronous gradient descent to speed up the training process. Similar to the asynchronous n-step DQN in BID20 , we consider a centralized distributed training framework where each process can compute its local gradient and synchronize with a global parameter server. In specific, each local process runs an independent game environment to generate transition trajectories and use its own transitions to compute gradients with respect to \u03c9 and \u03b8. These local gradients are then aggregated across multiple processes to update the global parameters. Note that these local stochastic gradients are independent. Thus tricks such as experience replay can be avoided in the distributed setting. Moreover, aggregating independent stochastic gradient decrease the variance of gradient estimation, which yields better algorithmic stability. We present the asynchronous P-DQN algorithm in Algorithm 2. For simplicity, here we only lay out the algorithm for each local process, which fetches \u03c9 and \u03b8 from the parameter server and computes the gradient. The parameter server stores the global parameters \u03c9, \u03b8 . It updates the global parameters using the gradients sent from the local processes . In addition we use the RMSProp BID10 to update the network parameters, which is shown to be more stable in practice. The game King of Glory is a MOBA game, which is a special form of the RTS game where the players are divided into two opposing teams fighting against each other. Each team has a team base located in either the bottom-left or the top-right corner which are guarded by three towers on each of the three lanes. The towers can attack the enemies when they are within its attack range. Each player controls one hero, which is a powerful unit that is able to move, kill, perform skills, and purchase Input: exploration parameter , a probability distribution \u00b5 over the action space A for exploration, the max length of multi step return tmax, and maximum number of iterations Nstep. Initialize global shared parameter \u03c9 and \u03b8 Set global shared counter Nstep = 0 Initialize local step counter t \u2190 1. repeat Clear local gradients d\u03c9 \u2190 0, d\u03b8 \u2190 0. tstart \u2190 t Synchronize local parameters \u03c9 \u2190 \u03c9 and \u03b8 \u2190 \u03b8 from the parameter server. repeat Observe state st and let x k \u2190 x k (st, \u03b8 ) Select action at = (kt, x k t ) according to the -greedy policy at = a sample from distribution \u00b5 with probability , (kt, x k t ) such that kt = arg max k\u2208[K] Q(st, k, x k ; \u03c9 ) with probability 1 \u2212 .Take action at, observe reward rt and the next state st+1. t \u2190 t + 1 Nstep \u2190 Nstep + 1 until st is the terminal state or t \u2212 tstart = tmax Define the target y = 0 for terminal st DISPLAYFORM0 Update global \u03b8 and \u03c9 using d\u03b8 and d\u03c9 with RMSProp BID10 ). until Nstep > Nmax equipments. The goal of the heroes is to destroy the base of the opposing team. In addition, for both teams, there are computer-controlled units spawned periodically that march towards the opposing base in all the three lanes. These units can attack the enemies but cannot perform skills or purchase equipments. An illustration of the map is in FIG2 , where the blue or red circles on each lane are the towers. During game play, the heroes advance their levels and obtain gold by killing units and destroying the towers. With gold, the heros are able to purchase equipments such as weapons and armors to enhance their power. In addition, by upgrading to the new level, a hero is able to improve its unique skills. Whereas when a hero is killed by the enemy, it will wait for some time to reborn. In this game, each team contains one, three, or five players. The five-versus-five model is the most complicated mode which requires strategic collaboration among the five players. In contrast, the one-versus-one mode, which is called solo, only depends on the player's control of a single hero. In a solo game, only the middle lane is active; both the two players move along the middle lane to fight against each other. The map and a screenshot of a solo game are given in FIG2 -(b) and (c), respectively. In our experiments, we play focus on the solo mode. We emphasize that a typical solo game lasts about 10 to 20 minutes where each player must make instantaneous decisions. Moreover, the players have to make different types of actions including attack, move and purchasing. Thus, as a reinforcement learning problem, it has four main difficulties: first, the state space has huge capacity; second, since there are various kinds of actions, the action space is complicated; third, the reward function is not well defined; and fourth, heuristic search algorithms are not feasible since the game is in real-time. Therefore, although we consider the simplest mode of King of Glory, it is still a challenging game for artificial intelligence. In this section, we applied the P-DQN algorithm to the solo mode of King of Glory. In our experiments, we play against the default AI hero Lu Ban provided by the game, which is a shooter with long attack range. To evaluate the performances, we compared our algorithm with the DDPG algorithm BID8 under fair condition. In our experiment, the state of the game is represented by a 179-dimensional feature vector which is manually constructed using the output from the game engine. These features consist of two parts. The first part is the basic attributes of the two heroes, the computer-controlled units, and buildings such as the towers and the bases of the two teams. For example, the attributes of the heroes include Health Point, Magic Point, Attack Damage, Armor, Magic Power, Physical Penetration/Resistance, and Magic Penetration/Resistance, and the attributes of the towers include Health Point and Attack Damage. The second component of the features is the relative positions of other units and buildings with respect to the hero controlled by the P-DQN player as well as the attacking relations between other units. We note that these features are directly extracted from the game engine without sophisticated feature engineering. We conjecture that the overall performances could be improved with a more careful engineered set of features. We simplify the actions of a hero into K = 6 discrete action types: Move, Attack, UseSkill1, UseSkill2, UseSkill3, and Retreat. Some of the actions may have additional continuous parameters to specify the precise behavior. For example, when the action type is k = Move, the direction of movement is given by the parameter x k = \u03b1, where \u03b1 \u2208 [0, 2\u03c0]. Recall that each hero's skills are unique. For Lu Ban, the first skill is to throw a grenade at some specified location, the second skill is to launch a missile in a particular direction, and the last skill is to call an airship to fly in a specified direction. A complete list of actions as well as the associated parameters are given in TAB0 . The ultimate goal of a solo game is to destroy the opponent's base. However, the final result is only available when the game terminates. Using such kind of information as the reward for training might not be very effective, as it is very sparse and delayed. In practice, we manually design the rewards using information from each frame. Specifically, we define a variety of statistics as follows. (In the sequel, we use subscript 0 to represent the attributes of our side and 1 to represent those of the opponent.)\u2022 Gold difference GD = Gold 0 \u2212 Gold 1 . This statistic measures the difference of gold gained from killing hero, soldiers and destroying towers of the opposing team. The gold can be used to buy weapons and armors, which enhance the offending and defending attributes of the hero. Using this value as the reward encourages the hero to gain more gold.\u2022 Health Point difference (HPD = HeroRelativeHP 0 \u2212 HeroRelativeHP 1 ): This statistic measures the difference of Health Point of the two competing heroes. A hero with higher Health Point can bear more severe damages while hero with lower Health Point is more likely to be killed. Using this value as the reward encourages the hero to avoid attacks and last longer before being killed by the enemy.\u2022 Kill/Death KD = Kills 0 \u2212 Kills 1 . This statistic measures the historical performance of the two heroes. If a hero is killed multiple times, it is usually considered more likely to lose the game. Using this value as the reward can encourage the hero to kill the opponent and avoid death.\u2022 Tower/Base HP difference THP = TowerRelativeHP 0 \u2212 TowerRelativeHP 1 , BHP = BaseRelativeHP 0 \u2212 BaseRelativeHP 1 . These two statistics measures the health difference of the towers and bases of the two teams. Incorporating these two statistic in the reward encourages our hero to attack towers of the opposing team and defend its own towers.\u2022 Tower Destroyed TD = AliveTower 0 \u2212 AliveTower 1 . This counts the number of destroyed towers, which rewards the hero when it successfully destroy the opponent's towers.\u2022 Winning Game W = AliveBase 0 \u2212 AliveBase 1 . This value indicates the winning or losing of the game.\u2022 Moving forward reward: MF = x + y, where (x, y) is the coordinate of Hero 0 : This value is used as part of the reward to guide our hero to move forward and compete actively in the battle field. The overall reward is calculated as a weighted sum of the time differentiated statistics defined above. In specific, the exact formula is r t = 0.5 \u00d7 10 \u22125 (MF t \u2212 MF t\u22121 ) + 0.001(GD t \u2212 GD t\u22121 ) + 0.5(HPD t \u2212 HPD t\u22121 DISPLAYFORM0 The coefficients are set roughly inversely proportional to the scale of each statistic. We note that our algorithm is not very sensitive to the change of these coefficients in a reasonable range. In the experiments, we use the default parameters of skills provided by the game environment (usually pointing to the opponent hero's location). We found such kind of simplification does not affect to the overall performance of our agent. In addition, to deal with the periodic problem of the direction of movement, we use (cos(\u03b1), sin(\u03b1)) to represent the direction and learn a normalized two-dimensional vector instead of a degree (in practice, we add a normalize layer at the end to ensure this). In addition, the 6 discrete actions are not always usable, due to skills level up, lack of Magic Point (MP), or skills Cool Down(CD). In order to deal with this problem, we replace the max k\u2208 [K] with max k\u2208[K] and k is usable when selecting the action to perform, and calculating multi-step target as in Equation 5.1.For the network structure, recall that we use a feature vector of 179 dimensions as the state. We set both the value-network and the policy network as multi-layer fully-connected deep networks. The networks are in the same size of 256-128-64 nodes in each hidden layer, with the Relu activation function. During the training and testing processes, we set the frame skipping parameter to 2. This means that we take actions every 3 frames or equivalently, 0.2 second, which adapts to the human reaction time, 0.1 second. We set t max = 20 (4 seconds) to alleviate the delayed reward. In order to encourage exploration, we use -greedy sampling in training with = 0.255. In specific, the first 5 type actions We further smooth the original noisy curves (plotted in light colors) to their running average (plotted in dark colors). In the 3 rows, we plot the average of episode lengths, reward sum averaged for each episode in training, and reward sum averaged for each episode in validation, for the two algorithms respectively. Usually a positive reward sum indicates a winning game, and vice versa. We can see that the proposed algorithm P-DQN learns much faster than its precedent work in our setting. (a) Performance of P-DQN. (b) Performance of DDPG are sampled with probability of 0.05 each and the action \"Retreat\" with probability 0.005. For actions with additional parameters, since the parameters are in bounded sets, we draw these parameters from a uniform distribution. Moreover, if the sampled action is infeasible, we execute the greedy policy from the feasible ones, so the effective exploration rate is less than . We uses 48 parallel workers with constant learning rate 0.001 in training and 1 worker with deterministic sampling in validation. The training and validating performances are plotted in Figure 3 .We implemented the DDPG BID8 ) algorithm within our learning environment to have a fair comparison. The exact network structure is plotted in Figure 1 . Each algorithm is allowed to run for 15 million steps, which corresponds to roughly 140 minutes of wall clock time when paralleled with 48 workers. From the experiments results, we can see that our algorithm P-DQN can learn the value network and the policy network much faster comparing to the other algorithm. In (a1), we see that the average length of games increases at first, reaches its peak when the two player's strength are close, and decreases when our player can easily defeat the opponent. In addition, in (a2) and (a3), we see that the total rewards in an episode increase consistently in training as well as in test settings. The DDPG algorithm may not be suitable for hybrid actions with both a discrete part and a continuous part. The major difference is that maximization over k when we need to select a action is computed explicitly in P-DQN, instead of approximated implicitly with the policy network as in DDPG. Moreover, with a deterministic policy network, we extend the DQN algorithm to hybrid action spaces of discrete and continuous types, which makes the P-DQN algorithm more suitable for realistic scenarios. Previous deep reinforcement learning algorithms mostly can work with either discrete or continuous action space. In this work, we consider the scenario with discrete-continuous hybrid action space. In contrast of existing approaches of approximating the hybrid space by a discrete set or relaxing it into a continuous set, we propose the parameterized deep Q-network (P-DQN), which extends the classical DQN with deterministic policy for the continuous part of actions. Empirical experiments of training AI for King of Glory, one of the most popular games, demonstrate the efficiency and effectiveness of P-DQN."
}