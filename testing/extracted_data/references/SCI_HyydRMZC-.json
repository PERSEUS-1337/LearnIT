{
    "title": "HyydRMZC-",
    "content": "Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples.\n Many advanced algorithms have been proposed to generate adversarial examples by leveraging the L_p distance for penalizing perturbations.\n Different defense methods have also been explored to defend against such adversarial attacks. \n While the effectiveness of L_p distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works.\n Perturbations generated through spatial transformation could result in large L_p distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses.\n We visualize the spatial transformation based perturbation for different examples and show that our technique\ncan produce realistic adversarial examples with smooth image deformation.\n Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. Deep neural networks (DNNs) have demonstrated their outstanding performance in different domains, ranging from image processing BID18 BID10 ), text analysis BID3 to speech recognition . Though deep networks have exhibited high performance for these tasks, recently they have been shown to be particularly vulnerable to adversarial perturbations added to the input images BID34 BID7 . These perturbed instances are called adversarial examples, which can lead to undesirable consequences in many practical applications based on DNNs. For example, adversarial examples can be used to subvert malware detection, fraud detection, or even potentially mislead autonomous navigation systems BID30 BID5 BID8 and therefore pose security risks when applied to security-related applications. A comprehensive study about adversarial examples is required to motivate effective defenses. Different methods have been proposed to generate adversarial examples such as fast gradient sign methods (FGSM) BID7 , which can produce adversarial instances rapidly, and optimization-based methods (C&W) BID1 , which search for adversarial examples with smaller magnitude of perturbation. One important criterion for adversarial examples is that the perturbed images should \"look like\" the original instances. The traditional attack strategies adopt L 2 (or other L p ) norm distance as a perceptual similarity metric to evaluate the distortion BID9 . However, this is not an ideal metric BID16 BID14 , as L 2 similarity is sensitive to lighting and viewpoint change of a pictured object. For instance, an image can be shifted by one pixel, which will lead to large L 2 distance, while the translated image actually appear \"the same\" to human perception. Motivated by this example, in this paper we aim to look for other types of adversarial examples and propose to create perceptually realistic examples by changing the positions of pixels instead of directly manipulating existing pixel values. This has been shown to better preserve the identity and structure of the original image BID44 . Thus, the proposed spatially transformed adversarial example optimization method (stAdv) can keep adversarial examples less distinguishable from real instances (such examples can be found in Figure 3 ).Various defense methods have also been proposed to defend against adversarial examples. Adversarial training based methods have so far achieved the most promising results BID7 BID38 BID28 . They have demonstrated the robustness of improved deep networks under certain constraints. However, the spatially transformed adversarial examples are generated through a rather different principle, whereby what is being minimized is the local geometric distortion rather than the L p pixel error between the adversarial and original instances. Thus, the previous adversarial training based defense method may appear less effective against this new attack given the fact that these examples generated by stAdv have never been seen before. This opens a new challenge about how to defend against such attacks, as well as other attacks that are not based on direct pixel value manipulation. We visualize the spatial deformation generated by stAdv; it is seen to be locally smooth and virtually imperceptible to the human eye. In addition, to better understand the properties of deep neural networks on different adversarial examples, we provide visualizations of the attention of the DNN given adversarial examples generated by different attack algorithms. We find that the spatial transformation based attack is more resilient across different defense models, including adversarially trained robust models. Our contributions are summarized as follows:\u2022 We propose to generate adversarial examples based on spatial transformation instead of direct manipulation of the pixel values, and we show realistic and effective adversarial examples on the MNIST, CIFAR-10, and ImageNet datasets.\u2022 We provide visualizations of optimized transformations and show that such geometric changes are small and locally smooth, leading to high perceptual quality.\u2022 We empirically show that, compared to other attacks, adversarial examples generated by stAdv are more difficult to detect with current defense systems.\u2022 Finally, we visualize the attention maps of deep networks on different adversarial examples and demonstrate that adversarial examples based on stAdv can more consistently mislead the adversarial trained robust deep networks compared to other existing attack methods. Here we first briefly summarize the existing adversarial attack algorithms as well as the current defense methods. We then discuss the spatial transformation model used in our adversarial attack. Adversarial Examples Given a benign sample x, an attack instance x adv is referred to as an adversarial example, if a small magnitude of perturbation is added to x (i.e. x adv = x + ) so that x adv is misclassified by the targeted classifier g. Based on the adversarial goal, attacks can be classified into two categories: targeted and untargeted attacks. In a targeted attack, the adversary's objective is to modify an input x such that the target model g classifies the perturbed input x adv in a targeted class chosen, which differs from its ground truth. In a untargeted attack, the adversary's objective is to cause the perturbed input x adv to be misclassified in any class other than its ground truth. Based on the adversarial capabilities, these attacks can be categorized as white-box and black-box attacks, where an adversary has full knowledge of the classifier and training data in the white-box setting BID35 BID7 BID1 BID25 BID30 BID0 BID17 BID20 ; while having zero knowledge about them in the black-box setting BID29 BID24 BID26 BID27 . In this work, we will focus on the white-box setting to explore what a powerful adversary can do based on the Kerckhoffs's principle BID33 to better motivate defense methods. In computer vision and graphics literature, Two main aspects determine the appearance of a pictured object BID37 : (1) the lighting and material, which determine the brightness of a point as a function of illumination and object material properties, and (2) the geometry, which determines where the projection of a point will be located in the scene. Most previous adversarial attacks BID7 focus on changing the lighting and material aspect, while assuming the underlying geometry stays the same during the adversarial perturbation generation process. Modeling geometric transformation with neural networks was first explored by \"capsules,\" computational units that locally transform their input for modeling 2D and 3D geometric changes BID13 . Later, BID15 demonstrated that similar computational units, named spatial transformers, can benefit many visual recognition tasks. BID43 adopted the spatial transformers for synthesizing novel views of the same object and has shown that a geometric method can produce more realistic results compared to pure pixel-based methods. Inspired by these successes, we also use the spatial transformers to deform the input images, but with a different goal: to generate realistic adversarial examples. Defensive Methods Following the emergence of adversarial examples, various defense methods have been studied, including adversarial training BID7 , distillation BID31 , gradient masking BID9 and feature squeezing BID40 . However, these defenses can either be evaded by C&W attacks or only provide marginal improvements BID2 BID11 . Among these defenses, adversarial training has achieved the state-of-the-art performance. BID7 proposed to use the fast gradient sign attack as an adversary to perform adversarial training, which is much faster, followed by ensemble adversarial training BID38 and projected gradient descent (PGD) adversarial training BID28 . In this work, we explicitly analyze how effective the spatial transformation based adversarial examples are under these adversarial training based defense methods. Here we first introduce several existing attack methods and then present our formulation for producing spatially transformed adversarial examples. Given a learned classifier g : X \u2192 Y from a feature space X to a set of classification outputs Y (e.g., Y = {0, 1} for binary classification), an adversary aims to generate adversarial example x adv for an original instance x \u2208 X with its ground truth label y \u2208 Y, so that the classifier predicts g(x adv ) = y (untargeted attack) or g(x adv ) = t (targeted attack) where t is the target class. All of the current methods for generating adversarial examples are built on directly modifying the pixel values of the original image. The fast gradient sign method (FGSM) BID7 uses a first-order approximation of the loss function to construct adversarial samples for the adversary's target classifier g. The algorithm achieves untargeted attack by performing a single gradient ascent step: DISPLAYFORM0 , where g (x, y) is the loss function (e.g. cross-entropy loss) used to train the original model g, y denotes the ground truth label, and the hyper-parameter controls the magnitude of the perturbation. A targeted version of it can be done similarly. Optimization-based attack (C&W) produces an adversarial perturbation for a targeted attack based on certain constraints BID1 BID24 as formulated below: DISPLAYFORM1 where the L p norm penalty ensures that the added perturbation is small. The same optimization procedure can achieve untargeted attacks with a modified constraint g(x + \u03b4) = y. Figure 1: Generating adversarial examples with spatial transformation: the blue point denotes the coordinate of a pixel in an output adversarial image and the green point is its corresponding pixel in an input image. The flow field in red represents the displacement from the pixels in the adversarial image to the pixels in the input image. All the existing approaches directly modify pixel values, which may sometimes produce noticeable artifacts. Instead, we aim to smoothly change the geometry of the scene while keeping the original appearance, producing more perceptually realistic adversarial examples. In this section, we first introduce our spatial transformation model and then describe our objective function for generating spatially transformed adversarial examples. We use x (i) adv to denote the pixel value of the i-th pixel and 2D coordinate (u (i) adv , v (i) adv ) to denote its location in the adversarial image x adv . We assume that x (i) adv is transformed from the pixel x (i) from the original image. We use the per-pixel flow (displacement) field f to synthesize the adversarial image x adv using pixels from the input x. For the i-th pixel within x adv at the pixel location (u (i) adv , v (i) adv ), we optimize the amount of displacement in each image dimension, with the pair denoted by the flow vector f i := (\u2206u (i) , \u2206v (i) ). Note that the flow vector f i goes from a pixel x (i) adv in the adversarial image to its corresponding pixel x (i) in the input image. Thus, the location of its corresponding pixel DISPLAYFORM0 adv + \u2206v (i) ). As the (u (i) , v (i) ) can be fractional numbers and does not necessarily lie on the integer image grid, we use the differentiable bilinear interpolation BID15 to transform the input image with the flow field. We calculate x (i) adv as: DISPLAYFORM1 where N (u (i) , v (i) ) are the indices of the 4-pixel neighbors at the location (u (i) , v (i) ) (top-left, topright, bottom-left, bottom-right). We can obtain the adversarial image x adv by calculating Equation 1 for every pixel x (i) adv . Note that x adv is differentiable with respect to the flow field f BID15 BID44 . The estimated flow field essentially captures the amount of spatial transformation required to fool the classifier. Objective function Most of the previous methods constrain the added perturbation to be small regarding a L p metric. Here instead of imposing the L p norm on pixel space, we introduce a new regularization loss L flow on the local distortion f , producing higher perceptual quality for adversarial examples. Therefore, the goal of the attack is to generate adversarial examples which can mislead the classifier as well as minimizing the local distortion introduced by the flow field f . Formally, given a benign instance x, we obtain the flow field f by minimize the following objective: DISPLAYFORM2 where L adv encourages the generated adversarial examples to be misclassified by the target classifier. L flow ensures that the spatial transformation distance is minimized to preserve high perceptual quality, and \u03c4 balances these two losses. The goal of L adv is to guarantee the targeted attack g(x adv ) = t where t is the targeted class, different from the ground truth label y. Recall that we transform the input image x to x adv with the flow field f (Equation 1). In practice, directly enforcing g(x adv ) = t during optimization is highly non-linear, we adopt the objective function suggested in BID1 . DISPLAYFORM3 where g(x) represents the logit output of model g, g(x) i denotes the i-th element of the logit vector, and \u03ba is used to control the attack confidence level. To compute L flow , we calculate the sum of spatial movement distance for any two adjacent pixels. Given an arbitrary pixel p and its neighbors q \u2208 N (p), we enforce the locally smooth spatial transformation perturbation L flow based on the total variation BID32 : DISPLAYFORM4 Intuitively, minimizing the spatial transformation can help ensure the high perceptual quality for stAdv, since adjacent pixels tend to move towards close direction and distance. We solve the above optimization with L-BFGS solver BID23 . In this section, we first show adversarial examples generated by the proposed spatial transformation method and analyze the properties of these examples from different perspectives. We then visualize the estimated flows for adversarial examples and show that with small and smooth transformation, the generated adversarial examples can already achieve a high attack success rate against deep networks. We also show that stAdv can preserve a high attack success rate against current defense methods, which motivates more sophisticated defense methods in the future. Finally, we analyze the attention regions of DNNs, to better understand the attack properties of stAdv. Experiment Setup We set \u03c4 as 0.05 for all our experiments. We use confidence \u03ba = 0 for both C&W and stAdv for a fair comparison. We leverage L-BFGS BID23 as our solver with backtracking linear search. We show adversarial examples with high perceptual quality for both MNIST (LeCun & BID21 CIFAR-10 (Krizhevsky et al., 2014) datasets.stAdv on MNIST In our experiments, we generate adversarial examples againsts three target models in the white-box setting on the MNIST dataset. Model A, B, and C are derived from the prior work BID38 , which represent different architectures. See Appendix A and TAB4 for more details about their network architectures. TAB0 presents the accuracy of pristine MNIST test data on each model as well as the attack success rate of adversarial examples generated by stAdv on these models. FIG0 shows the adversarial examples against different models where the original instances appear in the diagonal. Each adversarial example achieves a targeted attack, with the target class shown on the top of the column. It is clear that the generated adversarial examples still appear to be in the same class as the original instance for humans. Another advantage for stAdv compared with traditional attacks is that examples based on stAdv seldom show noise pattern within the adversarial examples. Instead, stAdv smoothly deforms the digits and since such natural deformation also exists in the dataset digits, humans can barely notice such manipulation. stAdv on CIFAR-10 For CIFAR-10, we use ResNet-32 1 and wide ResNet-34 2 as the target classifers BID41 BID10 BID28 . We show the classification accuracy of pristine CIFAR-10 test data (p) and attack success rates of adversarial examples generated by stAdv on different models in TAB1 . Figure 3 shows the generated examples on CIFAR-10 against different models. The original images are shown in the diagonal. The other images are targeted adversarial examples, with the index of the target classes shown at the top of the column. Here we use \"0-9\" to denote the ground truth labels of images lying in the diagonal for each corresponding column. These adversarial examples based on stAdv are randomly selected from the instances that can successfully attack the corresponding classifier. Humans can hardly distinguish these adversarial examples from the original instances. Comparison of different adversarial examples In Figure 4 , we show adversarial examples that are targeted attacked to the same class (\"0\" for MNIST and \"airplane\" for CIFAR-10), which is different from their ground truth. We compare adversarial examples generated from different methods on an MNIST instance, where the digit \"0\" is misclassified as \"2.\" We can see that the adjacent flows move in a similar direction in order to generate smooth results. The flows are more focused on the edge of the digit and sometimes these flows move in different directions along the edge, which implies that the object boundary plays an important role in our stAdv optimization. Figure 6 illustrates a similar visualization on CIFAR-10. It shows that the optimized flows often focus on the area of the main object, such as the airplane. We also observe that the magnitude of flows near the edge are usually larger, which similarly indicates the importance of edges for misleading the classifiers. This observation confirms the observation that when DNNs extract edge information in the earlier layers for visual recognition tasks BID39 . In addition, we visualize the similar flow for the ImageNet dataset BID4 in Figure 7 . The top-1 label of the original image in Figure 7 (a) is \"mountain bike\". Figure 7 (b)-(d) show targeted adversarial examples generated by stAdv, which have target classes \"goldfish,\" \"Maltese dog,\" and \"tabby cat,\" respectively, and which are predicted as such as the top-1 class. An interesting observation is that, although there are other objects within the image, nearly 90% of the spatial transformation flows tend to focus on the target object bike. Different target class corresponds to different directions for these flows, which still fall into the similar area. To quantify the perceptual realism of stAdv's adversarial examples, we perform a user study with human participants on Amazon Mechanical Turk (AMT). We follow the same perceptual study protocol used in prior image synthesis work BID14 . We generate 600 images from an ImageNet-compatible dataset, described in Appendix C. In our study, the participants are asked to choose the more visually realistic image between an adversarial example generated by stAdv and its original image. During each trial, these two images appear side-by-side for 2 seconds. After the images disappear, our participants are given unlimited time to make their decision. To avoid labeling bias, we allow each user to conduct at most 50 trails. For each pair of an original image and its adversarial example, we collect about 5 annotations from different users. In total, we collected 2, 740 annotations from 93 AMT users. Examples generated by our method were chosen as the more realistic in 47.01% \u00b1 1.96% of the trails (perfectly realistic results would achieve 50%). This indicates that our adversarial examples are almost indistinguishable from natural images. Adv. 5.04% 7.61% 31.66% Ens. 4.65% 8.43% 29.56% PGD 14.9% 13.90% 31.6% Here we generate adversarial examples in the white-box setting and test different defense methods against these samples to evaluate the strength of these attacks under defenses. We mainly focus on the adversarial training defenses due to their state-of-the-art performance. We apply three defense strategies in our evaluation: the FGSM adversarial training (Adv.) BID7 , ensemble adversarial training (Ens.) BID38 , and projectile gradient descent (PGD) adversarial training BID28 methods. For adversarial training purposes, we generate adversarial examples based on L \u221e bound BID1 as 0.3 on MNIST and 8 on CIFAR-10. We test adversarial examples generated against model A, B, and C on MNIST as shown in TAB4 , and similarly adversarial examples generated against ResNet32 and wide ResNet34 on CIFAR-10.The results on the MNIST and CIFAR-10 datasets are shown in TAB2 . We observe that the three defense strategies can achieve high performance (less than 10% attack success rate) against FGSM and C&W attacks. These defense methods only achieve low defense performance on stAdv, which improve the attack success rate to more than 30% among all defense strategies. These results indicate that new type of adversarial strategy, such as our spatial transformation-based attack, may open new directions for developing better defense systems. However, for stAdv, we cannot use L p norm to bound the distance as translating an image by one pixel may introduce large L p penalty. We instead constrain the spatial transformation flow and show that our adversarial examples have high perceptual quality in FIG0 , and 4 as well as Section 4.3. We also test our adversarial examples against the 3\u00d73 average pooling restoration mechanism . TAB5 in Appendix B shows the classification accuracy of recovered images after performing 3 \u00d7 3 average filter on different models. We find that the simple 3 \u00d7 3 average pooing restoration mechanism can recover the original class from fast gradient sign examples and improve the classification accuracy up to around 70%. Carlini & Wagner have also shown that such mean blur defense strategy can defend against adversarial examples generated by their attack and improve the model accuracy to around 80% (2017b). From TAB5 , we can see that the mean blur defense method can only improve the model accuracy to around 50% on stAdv examples, which means adversarial examples generated by stAdv are more robust compared to other attacks. We also perform a perfect knowledge adaptive attack against the mean blur defense following the same attack strategy suggested in BID2 , where we add the 3 \u00d7 3 average pooling layer into the original network and apply stAdv to attack the new network again. We observe that the success rate of an adaptive attack is nearly 100%, which is consistent with Carlini & Wagner's findings (2017b) . In addition to the analyzing adversarial examples themselves, in this section, we further characterize these spatially transformed adversarial examples from the perspective of deep neural networks. Here we apply Class Activation Mapping (CAM) BID43 , an implicit attention visualization technique for localizing the discriminative regions implicitly detected by a DNN. We use it to show the attention of the target ImageNet inception_v3 model BID36 ) for both original images and generated adversarial examples. In addition, we also compare and visualize the attention regions of both naturally trained and the adversarial trained inception_v3 model 3 on adversarial images generated by different attack algorithms FIG5 ). The ground truth top-1 label is \"cinema,\" so the attention region for the original image FIG5 ) includes both tower and building regions. However, when the adversarial examples are targeted attacked into the adversarial label \"missile,\" the attention region focuses on only the tower for all the attack algorithms as shown in FIG5 (b)-(d) with slight different attention region sizes. More interestingly, we also test these adversarial examples on the public adversarial trained robust inception_v3 model. The result appears in FIG5 (f)- (h) . This time, the attention regions are drawn to the building again for both FGSM and C&W methods, which are close to the attention regions of the original image. The top-1 label for FIG5 (f) and (g) are again the ground truth \"cinema\", which means both FGSM and C&W fail to attack the robust model. However, FIG5 (h) is still misclassified as \"missile\" under the robust model and the CAM visualization shows that the attention region still focuses on the tower. This example again implies that adversarial examples generated by stAdv are challenging to defend for the current \"robust\" ImageNet models. Different from the previous works that generate adversarial examples by directly manipulating pixel values, in this work we propose a new type of perturbation based on spatial transformation, which aims to preserve high perceptual quality for adversarial examples. We have shown that adversarial examples generated by stAdv are more difficult for humans to distinguish from original instances. We also analyze the attack success rate of these examples under existing defense methods and demonstrate they are harder to defend against, which opens new directions for developing more robust defense algorithms. Finally, we visualize the attention regions of DNNs on our adversarial examples to better understand this new attack. A MODEL ARCHITECTURES Here we evaluated adversarial examples generated by stAdv against the 3 \u00d7 3 average pooling restoration mechanism suggested in . TAB5 shows the classification accuracy of recovered images after performing 3 \u00d7 3 average pooling on different models. ImageNet-compatible. We use benign images from the DEV set from the NIPS 2017 targeted adversarial attack competition. 4 This competition provided a dataset compatible with ImageNet and containing target labels for a targeted attack. We generate targeted adversarial examples for the target inception_v3 model. In Figure 10 below, we show the original images on the left with the correct label, and we show adversarial examples generated by stAdv on the right with the target label. MNIST. We generate adversarial examples for the target Model B. In Figure 11 , we show original images with ground truth classes 0-9 in the diagonal, and we show adversarial examples generated by stAdv targeting the class of the original image within that column. CIFAR-10. We generate adversarial examples for the target ResNet-32 model. In FIG0 , we show the original images in the diagonal, and we show adversarial examples generated by stAdv targeting the class of the original image within that column. Table 6 shows the magnitude of the generated flow regarding total variation (TV) and L 2 distance on the ImageNet-compatible set, MNIST, CIFAR-10, respectively. These metrics are calculated by the following equations, where n is the number of pixels: TV = 1 n all pixels p q\u2208N (p) ||\u2206u (p) \u2212 \u2206u (q)"
}