{
    "title": "H15RufWAW",
    "content": "We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.\n We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.\n Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks. Generative models for graphs have a longstanding history, with applications including data augmentation, anomaly detection and recommendation BID6 . Explicit probabilistic models such as Barab\u00e1si-Albert or stochastic blockmodels are the de-facto standard in this field BID11 . However, it has also been shown on multiple occasions that our intuitions about structure and behavior of graphs may be misleading. For instance, heavy-tailed degree distributions in real graphs were in stark disagreement with the models existing at the time of their discovery BID2 . More recent works, like BID9 , keep bringing up other surprising characteristics of real-world networks, not accounted for by the models at hand. This leads us to the question: \"How do we define a model that captures all the essential (potentially still unknown) properties of real graphs?\" An increasingly popular way to address this issue in other fields is by switching from explicit (prescribed) models to implicit ones. This transition is especially notable in Computer Vision, where Variational Autoencoder BID23 and Generative Adversarial Networks (GANs) BID13 significantly advanced the state of the art over the classic prescribed approaches like Mixtures of Gaussians BID5 . GANs achieve unparalleled results in scenarios such as image and 3D objects generation (e.g., BID32 BID4 . However, despite their massive success when dealing with real-valued data, adapting GANs to handle discrete objects like graphs or text remains an open research problem BID12 . Indeed, the combinatorial structure of the graph is only one of the obstacles when applying GANs to graphs. Second, large repositories of graphs, which all come from the same distribution, do not exist. This means that in a typical setting one has to learn from a single graph. And last, any model operating on a graph necessarily has to be permutation invariant, as the graphs remain isomorphic under node reordering. In this work we introduce GraphGAN -the first implicit generative model for graphs, that tackles all of the above challenges. We formulate the problem of learning the graph topology as learning the distribution of biased random walks over the graph. Like in the typical GAN setting, the generator G -in our case defined as a stochastic neural network with discrete output samples -learns to generate random walks that are plausible in the real graph, while the discriminator D then has to distinguish them from the true ones that are sampled from the original graph. The objective function of our model is based on the Wasserstein GAN , which allows to learn multimodal distributions and leads to more stable convergence. Our GraphGAN exhibits strong generalization properties, which we study in detail in the experimental section. The example in FIG0 shows that the graphs generated by GraphGAN possess similar properties as the input graph, as shown by the degree distributions in FIG0 . The generated graphs, however, are not simply exact replicas: as the visualized subset of nodes in Figs. 1a and 1b shows, the graphs exhibit similar structure while being not identical; in fact, the two graphs have less than 50% of edges in common. This initial insight is underlined by an extensive comparison of graphs generated by GraphGAN and the respective input networks in the experimental section of this work. And even more, when generating graphs based on specific regions of the latent space learned by GraphGAN, we can smoothly interpolate between graphs with varying properties. Our main contributions are:\u2022 We introduce GraphGAN -the first of its kind GAN that generates graphs via random walks. Our model tackles the associated challenges of staying permutation invariant, learning from a single graph and generating discrete output.\u2022 We show that our model generalizes, and is able to produce sibling graphs to the given input graph. These graphs posses similar topological characteristics, but are not exact replicas (see FIG0 ). We further demonstrate how latent space interpolation leads to generation of graphs with smoothly changing properties.\u2022 We highlight the generalization properties of GraphGAN by its link prediction performance, which is competitive with the state of the art on real-word datasets, although not trained explicitly for this task. Additionally, to give the reader a better insight about the behavior of our model, we analyze the learned weights of our model -that can be viewed as node \"embeddings\" -and by using them in a node classification task we show that they capture meaningful structural information. There are only two attempts of using GANs in the context of graphs BID34 BID27 . BID34 's approach tries to generate the full adjacency matrix of the graph directly (treating it as a binary image). To circumvent the issue of having a single graph, they apply random permutations to the adjacency matrix to generate additional training data. Since their model explicitly generates the full adjacency matrix -including zero-elements -the output (input) size of their generator (discriminator) is equal to the number of nodes squared. Such a quadratic complexity is infeasible in practice, allowing to process only small graphs. Indeed, this fundamental limitation becomes evident in their reported runtime of over 60 hours for a graph with only 154 nodes -the largest graph they processed. In contrast, our model operates on the random walks, thus only considering the non-zero elements of the adjacency matrix and efficiently exploiting the sparsity of real-wolrd graphs. While not focusing on generating full graphs, BID27 use GANs to learn graph topological features. They decompose the graph into multiple subgraphs, where each subgraph is then processed by a GAN using standard image operations (e.g. convolution, deconvolution) which contain the built-in assumption that pixels located closely (within the same receptive field) are in some way correlated. Clearly, when talking about an adjacency matrix such assumption is not sensible, as permutations of rows/columns correspond to the exact same graph but very different receptive fields. In contrast to their approach our model does not make any spatial dependence assumptions about the adjacency matrix. Moreover, since their GAN only generates edges within the subgraphs, but not between them, all inter-subgraph edges from the original graph have to be stored and copied manually to a potential new graph. In contrast, our model is learned end-to-end, it allows to generate the whole graph, and it handles arbitrary permutations of the given input graph. Due to the challenging nature of the problem, only few approaches able to generate discrete data using GANs exist. Most approaches focus on generating discrete sequences such as text, with some of them using reinforcement learning techniques to address the difficulty of backpropagation through sampling discrete random variables BID24 BID26 . Other approaches modify the GAN objective to tackle the same challenge . Focusing on non-sequential discrete data, BID8 generate high-dimensional discrete features (e.g. binary indicators, counts) in patient records. None of these methods has considered graph structured data. Apart from GANs, prescribed generative models for graphs have a long history and are well-studied. For a comprehensive survey see BID6 ; BID11 . Based on the different modeling assumptions, the generative power of these models varies significantly. A substantial portion cannot even handle power-law degree distributions as found in most real-world networks. One of the simplest model is the configuration model BID3 BID29 that rewires edges at random, but preserves the node degrees. A stronger representative able to capture power-law degree distributions, as well as diverse network topologies and community structure is the well-established degree-corrected stochastic blockmodel (DC-SBM) BID21 . Another broad family of models are exponential random graph models (ERGM) BID19 ) that explicitly preserve some manually specified graph statistics (e.g. edge count, degrees, node attribute statistics, etc.). ERGMs represent a probability distribution over all possible networks of a given fixed size, by specifying one parameter per statistic. We compare against the configuration model, DC-SBM and ERGM as baselines. We will see in Sec. 4.1 that as expected, all properties which these approaches explicitly model are preserved, while the rest deviate significantly from the input graph. This highlights the need for implicit models such as ours, that capture the properties of real-world graphs without having to manually specify them. In this section we introduce GraphGAN -a Generative Adversarial Network model for graphs. Its core idea lies in learning the topology of a graph by learning the distribution over the random walks. Given is an input graph of N nodes, defined by an unweighted adjacency matrix A \u2208 {0, 1} N \u00d7N . First, we sample a set of random walks of length T from A. This collection of random walks serves as a training set for our model. We use the biased second-order random walk sampling strategy described in BID14 , as it better captures both local and global graph structure. An important advantage of using random walks is their invariance under node reordering. Additionally, random walks only include the nonzero entries of A, thus efficiently exploiting the sparsity of real-world graphs. Like any typical GAN architecture, GraphGAN consists of two main components -a generator G and a discriminator D. The goal of the generator is to generate synthetic random walks that are plausible in the input graph. At the same time, the discriminator learns to distinguish the synthetic random walks from the real ones that come from the training set. Both G and D are trained end-toend using backpropagation. At any point of the training process it is possible to use G to generate a set of random walks, which can then be used to produce an adjacency matrix of a new generated graph. In the rest of this section we describe each stage of this process and our design choices in more detail. An overview of our model's complete architecture can be seen in Fig. 2 . DISPLAYFORM0 Generator. The generator G defines an implicit probabilistic model for generating the random walks: DISPLAYFORM1 We model G as a sequential process based on a neural network f \u03b8 parametrized by \u03b8. At each step t, f \u03b8 produces two values: the probability distribution over the next node to be sampled, denoted as p t , and the current memory state of the model, denoted as m t . The new node v t (represented as a one-hot vector) is sampled from Cat(p t ), and together with m t passed into f \u03b8 at the next step t + 1. Similarly to the classic GAN setting, a latent code z drawn from a multivariate standard normal distribution is passed through a parametric function g \u03b8 to initialize m 0 . The generative process of G is summarized in the box above. In this work we focus our attention on the Long short-term memory (LSTM) architecture for f \u03b8 , introduced by BID18 . The memory state m t of an LSTM is represented by the cell state C t , and the hidden state h t . The latent code z goes through two separate streams, each consisting of two fully connected layers with tanh activation, and then used to initialize (C 0 , h 0 ).A natural question might arise: \"Why use a model with memory and temporal dependencies, when the random walks are Markov processes?\" (2nd order Markov for biased RWs). Or put differently, what's the benefit of using random walks of length greater than 2. In theory, a model with large enough capacity could simply memorize all existing edges in the graph and recreate them. However, for large graphs achieving this in practice is not feasible. More importantly, pure memorization is not the goal of GraphGAN, rather we want to have generalization and generate similar sibling graphs, not exact replicas. Having longer random walks combined with memory helps the model to learn the topology and general patterns in the data (e.g. community structure). Our experiments in Sec. 4.2 confirm this, showing that longer random walks are indeed beneficial. After each time step, in order to generate the next node in the random walk, the network f \u03b8 needs to output the vector of probabilities p t of length N . Operating in such high dimensional space within the LSTM cell is infeasible, and leads to unnecessary computational overhead. For this reason, we do the following: the model outputs o t \u2208 R H , with H N , which is then up-projected to R N using W up \u2208 R N \u00d7H . One can view it as similar to context embeddings in representation learning. Given the probability distribution over the next node in the random walk, p t \u2208 \u2206 N \u22121 , from which v t is to be drawn, we are faced with another challenge: Sampling from a categorical distribution is a non-differentiable operation -thus, it blocks the flow of gradients and precludes backpropagation. We circumvent this problem by using the Straight-Through Gumbel estimator by BID20 . More specifically, we perform the following transformation: First, we let Generator architecture DISPLAYFORM2 Random walk Figure 2: The GraphGAN architecture proposed in this work (b) and the generator architecture (a). DISPLAYFORM0 where \u03c4 is a temperature parameter, and g i 's are i.i.d. samples from a Gumbel distribution with zero mean and unit scale. Then, the next sample is computed as v t = onehot(arg max v * t ). While the one-hot sample v t is passed as input to the next time step, during the backward pass the gradients will flow through the differentiable v * t . The choice of \u03c4 allows to trade-off between better flow of gradients (large \u03c4 , more uniform v * t ) and more exact calculations (small \u03c4 , v * t \u2248 v t ). Now that a new node v t is sampled, it needs to be projected back to a lower-dimensional representation before feeding into the LSTM. This is done by means of down-projection matrix W down \u2208 R H\u00d7N . Together with the up-projection matrix W up , the matrix W down deserves a special mention, as it happens to learn -as a byproduct -meaningful information about the nodes. We can view these matrices as a form of node \"embeddings\" and to gain a better understanding of our model we study their properties in Sec. 4.2 and 4.3.Discriminator. The discriminator D is based on the standard LSTM architecture. At every time step t, a one-hot vector v t , denoting the node at the current position, is fed as input. After processing the entire sequence, the discriminator outputs a single score that represents the probability of the random walk being real. Wasserstein GAN. We train our model based on the Wasserstein GAN (WGAN) framework . To enforce the Lipschitz constraint of the discriminator, we use the gradient penalty as in BID15 . We observe that in our setting using the WGAN objective leads to noticeable improvements over the vanilla GAN: it prevents mode collapse, as well as leads to a more stable learning procedure overall. The model parameters {\u03b8, \u03b8 } are trained using stochastic gradient descent with Adam BID22 . Weights are regularized with an L 2 penalty. Early stopping. Because we are interested in generalizing the input graph, the \"trivial\" solution where the generator has memorized all existing edges is of no interest to us. This means that we need to control overfitting of our model. To achieve this, we employ two early stopping strategies. The first strategy, named VAL-CRITERION is concerned with the generalization properties of GraphGAN. During training, we keep a sliding window of the random walks generated in the last 1,000 iterations and use them to construct a matrix of transition counts. This matrix is then used to evaluate the link prediction performance on a validation set (i.e. ROC and AP scores, for more details see Sec. 4.2). We stop with training when the validation performance stops improving. The second strategy, named EO-CRITERION makes GraphGAN very flexible and gives the user control over the graph generation. We stop training when we achieve a user specified edge overlap between the generated graphs (see next section) and the original one at a given iteration. Based on her end task the user can choose to generate graphs with either small or large edge overlap with the original, while maintaining structural similarity. This will lead to generated graphs that either generalize better or are closer replicas respectively, yet still capture the properties of the original. After finishing the training, we use the generator G to construct a score matrix S of transition counts, i.e. we count how often an edge appears in the set of generated random walks (typically, using a much larger number of random walks than for early stopping, e.g., 500K). While the raw counts matrix S is sufficient for link prediction purposes, we need to convert it to a binary adjacency matrix\u00c3, if we wish to reason about the synthetic graph. First, S is symmetrized by setting s ij = s ji = max{s ij , s ji }. Because we cannot explicitly control the starting node of the random walks generated by G, some high-degree nodes will likely be overrepresented. Thus, a simple binarization strategy like thresholding or choosing top-k entries might lead to leaving out the low-degree nodes and producing singletons. To address this issue, we use the following approach. (i) We ensure that every node i has at least one edge by sampling a neighbor j with probability p ij = sij v siv . If an edge was already sampled before, we repeat the procedure. (ii) We continue sampling edges without replacement, using for each edge (i, j) the probability p ij = sij u,v suv , until we reach the desired amount of edges (e.g., as many edges as in the original graph). Note that this procedure is not guaranteed to produce a fully connected graph. Besides using GraphGAN to generate graphs, we also evaluate its output and learned representations on other typical graph mining tasks, most prominently link prediction and node classification. We evaluate GraphGAN on these tasks and several real-world datasets and compare it with state-of-theart methods. Furthermore, we demonstrate how we can generate graphs with smoothly changing properties via latent space interpolation. Datasets. For our evaluation, we use several well-know citation datasets, as well as the Political Blogs dataset. TAB0 show the dataset statistics. Cora-ML is the subset of machine learning papers from the original Cora dataset typically considered in other works. For all our experiments, we consider only the largest connected component in each network and treat them as undirected. In this task, we use GraphGAN to generate sibling graphs to a given input graph, and compare its performance to the baselines. The goal is to generate graphs that are similar in their properties to the input graph -while not trivially copying the input network. We randomly hide 15% of the edges (which are used for the stopping criterion; see Sec. 3.2) and train GraphGAN and DC-SBM on the remaining graph. We then sample graphs from the trained models and compare their properties with the input graph. We report the results for CORA-ML here, and for CITESEER in the appendix. Evaluation. FIG1 shows that GraphGAN and DC-SBM are able to generate graphs whose degree distributions nicely match the input graph's. For DC-SBM, this is not surprising, given that it explicitly encodes the degree distribution in the model. GraphGAN, however, does not have access to the degree distribution of the input graph -yet is still able to model it to a high accuracy. Going beyond the degree distribution, in TAB1 we show seven other important graph statistics and see that for most of them GraphGAN closely matches the original graph. We do not expect that GraphGAN is superior to every existing explicit model in every possible regard. Rather, our goal is to lay a foundation for the study of implicit models for graph generation. We report the results for both early stopping strategies: VAL-CRITERION and EO-CRITERION. In line with our intuition, we can see that higher EO leads to generated graphs with statistics closer to the original. Results for additional statistics, respective definitions, as well as details about the baselines can be found in the appendix. Figs. 3b and 3c show how the graph statistics evolve as we train GraphGAN on CORA-ML. In FIG1 we see that after 40K training iterations we are able to reach the assortativity value of the original graph. FIG1 shows that the edge overlap smoothly increasing with the number of epochs. 9.8e\u22124 \u00b11e\u22125 9.9e\u22124 \u00b12e\u22125 node2vec na\u00efve (1% EO) 14 \u00b1 1.4 -0.007 \u00b1 0.011 16 \u00b1 4.4 1.68 \u00b1 0.001 2,810 \u00b1 0.1 1.4e\u22123 \u00b11e\u22125 3.8e\u22124 \u00b12e\u22125 DC-SBM (11% EO) 165 \u00b1 9.0 -0.052 \u00b1 0.004 1,403 \u00b1 67 1.814 \u00b1 0.008 2,474 \u00b1 18.9 6.7e\u22124 \u00b12e\u22125 1.2e\u22123 \u00b14e\u22125 ERGM (56% EO) 243 \u00b1 1.94 -0.077 \u00b1 0.000 2,293 \u00b1 23 1.786 \u00b1 0.003 2,489 \u00b1 11.0 6.9e\u22124 \u00b12e\u22125 1.2e\u22123 \u00b11e\u22125 GraphGAN VAL (39% EO) 199 \u00b1 6.7 -0.060 \u00b1 0.004 1,410 \u00b1 30 1.773 \u00b1 0.002 2,809 \u00b1 1.6 6 .5e\u22124 \u00b11e\u22125 1 .3e\u22123 \u00b12e\u22125 GraphGAN EO (52% EO) 233 \u00b1 3.6 -0.066 \u00b1 0.003 1,588 \u00b1 59 1.793 \u00b1 0.003 2,807 \u00b1 1.6 6.0e\u22124 \u00b11e\u22125 1.4e\u22123 \u00b11e\u22125Note: EO is a suitable measure of closeness since we generate graphs with same node ordering and same number of edges as the input. We provide similar plots for the other graph statistics and for CITESEER in the appendix. Link prediction is a classical task in graph mining, where the goal is to predict new links in a given graph. We use it to evaluate the generalization properties of GraphGAN. We hold out 10% of edges from the graph for validation, and 5% as the test set, along with the same amount of randomly selected non-edges. We also ensure that the training network remains connected and does not contain any singletons. We measure the performance with the commonly used metrics area under the ROC curve (AUC) score and precision-recall AUC score, known as average precision (AP).To evaluate GraphGAN's link prediction performance, we sample a specific number of random walks (500K/100M) from the trained generator. We use the observed transition counts between any two nodes as a measure of how likely there is an edge between them. Additionally, we concatenate W down and W up and use the dot product as an alternative way to perform link prediction. We compare with Adamic/Adar (Adamic & Adar, 2003), the degree-corrected stochastic blockmodel (DC-SBM) BID21 , and node2vec BID14 .Evaluation. The results are listed in TAB2 . There is no overall dominating method, with different methods achieving best results on different datasets. GraphGAN shows competitive performance for all datasets, for both the transition count based and the dot product based link prediction, even achieving state-of-the-art results for some of them, despite not being explicitly trained for this task. Interestingly, for the transition count based link prediction, the GraphGAN performance increases when increasing the number of random walks sampled from the generator. This is especially true for the larger networks (CORA, DBLP, PUBMED), since given their size we need more random walks to cover the entire graph. This suggests that for an additional computational cost, one can get significant gains in performance. Note that while 100M may seem like an large number, the sampling process is trivially parallelizable. Sensitivity analysis. Although GraphGAN has many hyperparameters -typical for a GAN model, in practice most of them are not critical for performance. The two important exceptions are the length of the random walks T , and the discriminator type. FIG2 empirically confirms the choice of a neural network that generates random walks of length T as opposed to just edges; the model does not have the capacity to fit the model by just considering edges (i.e. random walks of length 2). In the experiment we sample 500K random walks from the trained models for each T , averaged over five runs. The performance gain for random walk length 20 over 16 is marginal and does not outweigh the additional computational cost; therefore, we use random walks of length 16 for all experiments. In the appendix we show that our choice of a recurrent discriminator achieves better link prediction performance than a variant based on convolutions, hence we use it for all experiments in this work. We perform node classification using W up and W down , which can be viewed as low-dimensional feature representations of the nodes, to show the generalization properties of GraphGAN. We evaluate both W down , W up , as well as their concatenation, comparing with node2vec as a strong baseline. Note that unlike in the link prediction task we cannot use the generated graphs to perform node classification. Similar to BID14 BID31 we train a logistic regression model on a small randomly selected subset (< 20% of nodes) using the ground-truth labels and evaluate the classification performance on the remaining samples. Additionally, we visualize W up using t-SNE to demonstrate that a community structure has emerged in the embedding space. Evaluation. In FIG3 , we visualize the weighted macro F1 score for node classification. The results are averaged over five trials, and the shaded areas indicate the standard deviation of the respective curves. The embedding-like weights in W up produced by GraphGAN as a byproduct show comparable performance with node2vec -an algorithm specifically designed to learn node embeddings. This indicates that during the process of learning to generate random walks GraphGAN also captured meaningful structural information. The emergent community structure seen in FIG3 further emphasizes this. In this t-SNE visualization of W up , we can clearly see a grouping of nodes from the same community, which means that they lie in similar regions of the embedding space. Although GraphGAN's learned node representations clearly show they they capture useful information about the nodes, they are only a byproduct of the training procedure. Recall that the main goal of GraphGAN is graph generation; if node classification is the user goal, dedicated node embedding algorithms are the preferred option. Latent space interpolation is a good way to gain insight into what kind of structure the generator was able to capture. To be able to visualize the properties of the generated graphs we train our model using noise z drawn from a bivariate standard normal distribution, which corresponds to a 2-dimensional latent space \u2126 = R 2 . Then, instead of sampling z from the entire latent space \u2126, we now sample from subregions of \u2126 and visualize the results. More specifically, we divide \u2126 into 20\u00d720 subregions (bins) of equal probability mass using the cumulative distribution function \u03a6. For each bin we generate 62.5K random walks. We evaluate properties of both the generated random walks themselves, as well as properties of the resulting graph when sampling a binary adjacency matrix for each bin, visualizing them as heatmaps. Evaluation. In Fig. 6a and 6b we see properties of the generated random walks; in Fig. 6c and 6d , we visualize properties of graphs sampled from the random walks in the respective bins. In all four heatmaps, we see distinct patterns, e.g. higher average degree of starting nodes for the bottom right region of Fig. 6a , or higher degree distribution inequality in the top-right area of Fig. 6c . While Fig. 6c and 6d show that certain regions of z correspond to generated graphs with very different degree distributions, recall that sampling from the entire latent space (\u2126) yields sibling graphs with degree distribution similar to the original graph (see FIG0 . The model was trained on CORA-ML. We provide further heatmaps for other metrics (16 in total) as well as visualizations for CITESEER in the appendix. This experiment clearly demonstrates that by interpolating in the latent space we can obtain graphs with smoothly changing properties. The smooth transitions in the heatmaps provide evidence that our model learns to map specific parts of the latent space to specific properties of the graph. (\u2126) is the community distribution when sampling from the entire latent space, and (*) is the community distribution of the CORA-ML network. Also available as an animation https://goo.gl/BGDX4o.We can also see this mapping from latent space to the generated graph properties in the community distribution histograms on a 10 \u00d7 10 grid in Fig. 7 . Marked by (*) and (\u2126) we see the community distributions for the input graph and the graph obtained by sampling on the complete latent space, respectively. In Fig. 7b and 7c , we see the evolution of selected community shares when following a trajectory from top to bottom, and left to right, respectively. The community histograms resulting from sampling random walks from opposing regions of the latent space are very different; again the transitions between these histograms are smooth, as can be seen in the trajectories in Fig. 7b and 7c. When evaluating different graph generative models in Sec. 4.1, we observed a major limitation of explicit models. While the prescribed approaches excel at recovering the properties that are directly included in their definition, they perform significantly worse with respect to the rest of the metrics. This phenomenon clearly indicates the need for implicit graph generators, such as GraphGAN. Indeed, we notice that our model is able to consistently capture all the important graph characteristics (see TAB1 ). Moreover, GraphGAN generalizes beyond the input graph, as can be seen by its strong link prediction performance in Sec. 4.2.Still, being the first model of its kind, GraphGAN possesses certain limitations, and a number of related questions could be addressed in follow-up works:Scalability. We have observed in Sec. 4.2 that it takes a large number of generated random walks to get representative transition counts for large graphs. While sampling random walks from GraphGAN is trivially parallelizable, a possible extension of our model is to use a conditional generator, i.e. the generator can be provided a desired starting node, thus ensuring a more even coverage. On the other hand, the sampling procedure itself can be sped up by incorporating a hierarchical softmax output layer -a method commonly used in natural language processing. Evaluation. It is nearly impossible to judge whether a graph is realistic by visually inspecting it (unlike images, for example). In this work we already quantitatively evaluate the performance of GraphGAN on a large number of standard graph statistics. However, developing new measures applicable to (implicit) graph generative models will deepen our understanding of their behavior. Experimental scope. In the current work we focused on the setting of a single connected graph. Other scenarios, such as dealing with a collection of smaller i.i.d. graphs, that frequently occur in other fields (e.g., chemistry, biology), would be an important application area for the proposed model. Studying the influence of the graph topology (e.g., sparsity, diameter) on performance of GraphGAN will shed more light on its properties. Other types of graphs. While plain graphs are ubiquitous, many of real-world applications deal with attributed, k-partite or heterogeneous networks. Adapting the GraphGAN model to handle these other modalities of the data is a promising direction for future research. Especially important would be an adaptation to the dynamic / inductive setting, when new nodes are added over time. GraphGAN is the first work to successfully bridge the worlds of implicit modeling and graphs. Our work enables future researchers to gain better insight into the properties of real networks and opens new and exciting lines of research. We are able to generate realistic graphs by learning to generate (biased) random walks from the same distribution as the random walks from an input graph. We employ the GAN framework to learn our implicit generative model, overcoming key challenges such as permutation invariance, working in the discrete domain and having a single graph as input. Our generator is able to generate sibling graphs that maintain structural similarity with the original graph without being exact replicas. Better yet, using our defined stopping criteria, we can control how close are the generated graphs to the original. We further show that GraphGAN learns a semantic mapping from the latent space to the properties of the generated graph, which is evidenced by the smooth transitions of the output. GraphGAN shows strong generalization properties, as demonstrated by the competitive performance on the link prediction and the promising results on the node classification task, without being explicitly trained with these tasks in mind. We denote as V the set of all nodes in a graph, E as the set of edges, and as C i \u2286 V , i \u2208 {1, . . . , k} communities in a graph. Every node in the graphs we consider is assigned to exactly one of these communities. N (v) = {v |(v, v ) \u2208 E} denotes the set of neighbors of a node v, and d(v) = |N (v)| is the degree of node v. Given two graphs G 1 and G 2 with the same number of nodes and edges, i.e. |V 1 | = |V 2 |, |E 1 | = |E 2 |, we define their edge overlap as DISPLAYFORM0 Graph generation from node embeddings. As a na\u00efve baseline for generating a graph from node embeddings, we propose the following strategy. As suggested by BID14 , we train a logistic regression model on node embeddings learned by node2vec to get edge probabilities. Using these link prediction scores, we generate first-order random walks. The starting nodes are sampled from a categorical distribution where the probability for each node is proportional to its degree in the input graph. The subsequent nodes are sampled using the logistic regression model, i.e. proportional to the log probabilities of a link between the previous node and all other nodes. We repeat this for T=16 time steps and for 500K random walks. We use our procedure described in Section 3.3 to assemble an adjacency matrix from the transition counts. Configuration model. In addition to randomly rewiring all edges in the input graph, we also generate random graphs with similar overlap as graphs generated by GraphGAN using the configuration model. For this, we randomly select a share of edges (e.g. 39%) and keep them fixed, and shuffle the remaining edges. This leads to a graph with the specified edge overlap; in TAB1 we show that with the same edge overlap, GraphGAN's generated graphs in general match the input graph better w.r.t the statistics we measure. Exponential random graph model. The ERGM we used takes as parameters the edge count, density, degree correlation, deg1.5, and gwesp. Here, deg1.5 is the sum of all degrees to the power of 1.5, and gwesp refers to the geometrically weighted edgewise shared partner distribution (see BID16 for details). Table 4 : Graph statistics used to measure graph properties in this work. Maximum degree max DISPLAYFORM0 Maximum degree of all nodes in a graph. DISPLAYFORM1 Share of in-and outgoing edges of community C i , normalized by the number of edges in the graph. DISPLAYFORM2 |f | Size of largest connected component, where F are all connected components. DISPLAYFORM3 Exponent of the power law distribution, where d min denotes the minimum degree in a network. Gini coefficient DISPLAYFORM4 Common measure for inequality in a distribution, whered is the sorted list of degrees in the graph. Triangle count |{{u,v,w}|{(u,v),(v,w) ,(u,w)}\u2286E}| 6Number of triangles in the graph, where u \u223c v denotes that u and v are connected. Wedge count DISPLAYFORM5 Number of wedges, i.e. two-hop paths in an undirected graph. Rel. edge distr. entropy DISPLAYFORM6 Entropy of degree distribution, 1 means uniform, 0 means a single node is connected to all others. DISPLAYFORM7 Pearson correlation of degrees of connected nodes, where the (x i , y i ) pairs are the degrees of connected nodes. Power law exp. Table 5 : GraphGAN with recurrent vs convolutional discriminator. We train GraphGAN with the recurrent and convolutional discriminator variants five times each and measure their link prediction scores on the CORA-ML dataset to evaluate which variant is suited better for our task. Avg. Prec. Mean Std. Mean Std Recurrent 92.07 \u00b1 0.005 94.88 \u00b1 0.002 Conv.89.70 \u00b1 0.017 93.02 \u00b1 0.011 Table 6 : Comparison of graph statistics between the CITESEER/CORA-ML graph and graphs generated by GraphGAN and DC-SBM, averaged after 5 trials. Marked in bold and italic are the results that are closest and second-closest to the ground truth graph, respectively, except for edge overlap, where lower can be considered better."
}