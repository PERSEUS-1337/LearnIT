{
    "title": "R42827",
    "content": "In considering budget issues, Congress has long been interested in the relative efficiency and effectiveness of federal programs, including foreign assistance. Foreign assistance evaluation is one aspect of a government-wide effort to link program effectiveness to budgeting decisions. It is also an element of broader foreign aid reforms implemented in recent years. The 2010 Quadrennial Diplomacy and Development Review (QDDR), the basis of many aid policy initiatives, called for the State Department and the U.S. Agency for International Development (USAID) to plan foreign aid budgets and programs \"based not on dollars spent, but on outcomes achieved,\" and for USAID to become \"the world leader in monitoring and evaluation.\" The 2015 QDDR continued the emphasis on evaluation, emphasizing the strategic use of data and the need to build agency evaluation capacity. Rigorous evaluation is also a cornerstone of the Millennium Challenge Corporation (MCC), established in 2004 to promote a new model of development assistance. According to former USAID Administrator Rajiv Shah, global development policies and practices are experiencing a \"transformation based on absolute demand for results.\" That demand comes, in part, from some Members of Congress as they scrutinize the Administration's international affairs budget request and consider foreign aid spending priorities. It also comes from aid beneficiaries and American taxpayers who want to know what impact, if any, foreign aid dollars are having and whether foreign aid programs are achieving their intended objectives.  The current emphasis on evaluation is not new. The importance, purpose and methodologies of foreign aid evaluation have varied over the decades since USAID was established in 1961, responding to political and fiscal circumstances, as well as evolving development theories. There are a number of reasons that this issue has again gained prominence in recent years. For one, foreign aid funding levels increased significantly in the first decade of the 21 st century, while evaluations decreased, raising questions about the knowledge basis for aid policy. Analysts have noted that after decades of aid agencies spending billions of dollars on assistance programs, very little is known about the impact of these programs. Some wonder how policymakers can develop effective foreign aid strategies without a clear understanding of how and why prior assistance has succeeded or failed. This report focuses primarily on U.S. bilateral assistance, not on the work of multilateral aid entities, such as the World Bank, to which the United States contributes. While a wide range of federal agencies provide foreign assistance in some form, this report focuses on the three agencies that have primary policy authority and implementation responsibility for U.S. foreign assistance\u2014USAID, the State Department, and the Millennium Challenge Corporation (MCC). It discusses past efforts to improve aid evaluation, as well as ongoing issues that make evaluation challenging in the foreign assistance context. The report also provides an overview of the current evaluation policies of the primary implementing agencies, and discusses related issues for Congress, including recent legislation. To know whether aid is successful, one must understand its purpose. The Foreign Assistance Act (FAA) of 1961 (P.L.87-195), as amended, is the authorizing legislation for most modern foreign aid programs. The FAA declared that the principal objective of the foreign policy of the United States is the encouragement and sustained support of the people of developing countries in their efforts to acquire the knowledge and resources essential to development, and to build the economic, political, and social institutions that will improve the quality of their lives.  The original legislation lists five principal goals for foreign aid: (1) the alleviation of the worst physical manifestations of poverty among the world's poor majority; (2) the promotion of conditions enabling developing countries to achieve self-sustaining economic growth and equitable distribution of benefits; (3) the encouragement of development processes in which individual civil and economic rights are respected and enhanced; (4) the integration of the developing countries into an open and equitable international economic system; and (5) the promotion of good governance through combating corruption and improving transparency and accountability. Amending legislation over the years added dozens of new, though often overlapping, aid objectives. For example, \"the suppression of the illicit manufacturing of and trafficking in narcotic and psychotropic drugs\" was added in 1971, \"to alleviate human suffering caused by natural and manmade disasters\" was added in 1975, and \"to enhance the antiterrorism skills of friendly countries by providing training and equipment\" and \"to strengthen the bilateral ties of the United States with friendly governments by offering concrete [antiterrorism] assistance\" were added in 1983. In short, U.S. foreign aid is intended to be a tool for fighting poverty, enhancing bilateral relationships, and/or protecting U.S. security and commercial interests. In this broad view, some instances of specific development assistance projects and programs are widely viewed as successful. The largest aid program of the last century, the Marshall Plan (1948-1952), for example, is acclaimed as a key factor in the post-World War II reconstruction of European states that have gone on to become major strategic and trade partners of the United States. In the late 1960s and 1970s, aid associated with the \"green revolution\" was credited with greatly improving agricultural productivity and addressing hunger and malnutrition in parts of Asia, and global health programs were credited with virtually eradicating smallpox. Korea, Taiwan, and Botswana are often cited as aid success stories as a result of remarkable economic progress following significant aid infusions. More recently, unquestionable progress in battling public health crises, such as HIV/AIDS, across the globe can be largely attributed to massive foreign assistance programs, both bilateral and multilateral. Recent studies have also shown a positive but modest impact of aid on economic growth rates. Even in these instances, however, close analysis often reveals many caveats.  In other specific instances foreign aid programs and projects have been considered to be conspicuously unsuccessful, or even harmful to intended beneficiaries. Critics of foreign assistance cite decades of aid to corrupt governments in Africa, which enriched corrupt leaders and did little to improve the lives of the poor. In Latin America, U.S. aid to anti-communist rebels and regimes during the Cold War was associated with brutal violence and believed by many to have damaged U.S. credibility as a champion of democracy. Numerous examples exist of hospitals, schools, and other facilities that were built with donor funds and left to rot, unused in developing countries that did not have the resources or will to maintain them. In some instances, critics assert that foreign aid may do more harm than good, by reducing recipient government accountability, fueling corruption, damaging export competitiveness, creating dependence, and undermining incentives for adequate taxation. The most notable successes and conspicuous failures of foreign aid give fodder to both aid advocates and detractors, but in all likelihood represent just a small segment of assistance activities. In most cases, clear evidence of the success or failure of U.S. assistance programs is lacking, both at the program level and in aggregate. One reason for this is that aid provided for development objectives is often conflated with aid provided for political and security purposes. Another reason is that historically, most foreign assistance programs are never evaluated for the purpose of determining their impact, either at the time of implementation or retrospectively. Furthermore, evaluation practices are not consistent enough to allow for the use of project level data as the basis for broader, strategic evaluations. A 2009 review of monitoring and evaluation of U.S. foreign assistance described the evaluation effort at that time as \"uneven across agencies, rarely assesses impact, lacks sufficient rigor, and does not produce the necessary analysis to inform strategic decision making.\" In recent years, however, aid-implementing agencies have taken steps to improve both the quantity and quality of aid evaluations, and to make better use of the information gleaned from those efforts. A 2016 USAID review identified notable improvements in evaluation practices at USAID since implementation of a new evaluation policy in 2011. The Department of State, USAID, and other U.S. agencies implementing foreign assistance programs consistently monitor the performance of their own personnel and contractors in meeting discrete objectives, tracking project inputs and outputs. Depending on the nature of the project or program, staff and contractors might monitor the miles of road built, number of police officers trained, or changes in the use of fertilizers by farmers. These results can be compared to the initial program goals and expectations to determine whether the project or contract has been performed successfully. This type of oversight is called performance monitoring . Financial audits by agency Inspectors General, which examine whether funds are being used as intended, are also a common form of performance monitoring, particularly at the State Department. These audits are in addition to regular financial audits required by agencies of contractors, aid-implementing partners, and host government entities. If the data gathered through performance monitoring are analyzed in an effort to explain how and why a program meets or fails to meet strategic objectives, this is called performance evaluation . Performance evaluations have typically been carried out sporadically, to address questions of efficiency, effectiveness, and sustainability, among other things. Performance evaluations represent the vast majority of foreign aid evaluations.  Performance monitoring and evaluation play an important part in project management but do little to answer questions about foreign aid effectiveness. Addressing this question, some argue, requires impact evaluations . Impact evaluations look not at the output of an activity, but rather at its impact on a development objective. For example, while performance monitoring of an education program may involve tracking the number of textbooks provided and teachers trained, an impact evaluation may determine how or if literacy or math skills had improved for the target group as compared to a similar group that did not receive the textbooks or teacher training. A performance evaluation of an HIV prevention project may report the number of public awareness events held or condoms distributed, and analyze this data in the context of program goals, while an impact evaluation of the same program would monitor changes in the HIV/AIDS infection rate of the targeted population relative to a control group. An impact evaluation of a police training program would look at the program's impact on civil order and public safety rather than simply report how many officers were trained or the value of equipment supplied. Impact evaluation can take many forms, ideally using a defined counterfactual, or control group, and baseline data to measure change that can be attributed to an aid intervention. Randomized controlled trials, in which beneficiaries are randomly selected from a prequalified group and compared before and after the program to those not selected, are widely viewed as best practice for impact evaluation, but less rigorous methods are used as well. For example, \"before and after\" data analysis, case studies, and mixed method designs using both qualitative and quantitative data may be used for impact evaluation.  Impact evaluations can be a key to determining whether a foreign assistance program \"works.\" However, impact evaluations are generally far more complex and resource-intensive than performance monitoring and evaluation, and usually must be planned before an activity begins. Agencies implementing foreign assistance must balance the potential knowledge to be gained from impact evaluation with the additional resources necessary to carry out such evaluations. As a result, while the potential learning benefits of impact evaluation have long been recognized by aid officials, the use of rigorous impact evaluation has been, and continues to be, very limited. More typically, agencies aim for evaluation practices that are, as one expert has put it, \"cost-effectively rigorous,\" and, at minimum, \"independent, transparent, and consistent, thus persuasive.\" The practice of foreign assistance evaluation has changed over time to reflect evolving, or some might say cyclical, attitudes about the purpose and relative importance of evaluation. This is evident both in the United States and internationally. Aid evaluation practices and policies have variously focused on different evaluation objectives, including meeting program management needs, institutional learning, accountability for resources, informing policymakers, and building local oversight and project design capacity.  The history of U.S. foreign assistance evaluation begins with USAID, which implemented the vast majority of U.S. foreign assistance prior to the last decade. In its early years, USAID was primarily involved in large capital and infrastructure projects, for which evaluations focused on financial and economic rates of return were appropriate. However, the agency soon shifted focus towards smaller and more diverse projects to address basic human needs, and found that the rate of return evaluation model was no longer sufficient. The agency established its first Office of Evaluation in 1968, and used a Logical Framework (LogFrame) model as its primary system for monitoring and evaluation. The LogFrame approach, subsequently adopted by many international development agencies, employed a matrix to identify project goals, purposes, results, and activities, with corresponding indicators, verification methods, and important assumptions. Baseline data were to be used for each indicator, and results were reported at quarterly points during the life of a project. However, these data were not analyzed to look for competing explanations of the results or unintended consequences of activities. In many respects, the LogFrame approach was quite similar to the current GPRA requirements (discussed in the \"Program Evaluation Government-Wide\" text box above.)  While the LogFrame approach established USAID as a thought leader with respect to evaluation policy, in practice, evaluation quality varied significantly from project to project. A 1970 evaluation handbook included a diagram of the \"ideal\" program evaluation design, which resembles a randomized controlled trial, but notes that \"there are a great many reasons why it may not be possible to reach the ideal.\" Reviews of foreign assistance evaluation over decades revealed shortcomings. For one, the system had become decentralized over time, suitable to meet the information needs of project managers in the field but not contribute to broader learning or policy making. A 1982 report by the General Accounting Office (now the Government Accountability Office, GAO) found that \"AID staff does not apply lessons learned in the development of new projects,\" and that \"lessons learned are neither systematically nor comprehensively identified or recorded by those who are directly involved.\" In response to the GAO report's recommendation that USAID build an \"information analysis capability,\" the agency created the Center for Development Information and Evaluation (CDIE) in 1983, with a mandate to \"foster the use of development information in support of AID's assistance efforts.\" CDIE carried out meta-evaluations to reveal broader trends in aid impact, provided information and training on evaluation best practices to mission staff, and made a wide range of evaluation reports accessible to implementers in the field. Aid officials suggest that CDIE's evaluation work played a significant role in shaping USAID strategies and priorities in many sectors over decades. An internal USAID review in 1988 found that CDIE had greatly increased the use of aid evaluation information by implementers, but also identified a need to improve the quality and timeliness of evaluation reports. While the evaluation policy at the time still called for rigorous, statistical methods of evaluation, it was found that this approach was never actually widely used at USAID because the required skills, time, and expense made implementation difficult. As one internal review noted, \"statistical rigor in evaluation methods was deemphasized in favor of 'reasonably' valid evidence about project performance.\" Guidance to missions encouraged the use of low-cost and timely qualitative evaluation methodologies, including the use of key informant interviews, focus group discussions, community meetings, and informal surveys.  In the early 1990s, accountability for funds became a primary focus of aid evaluation. After a 1990 GAO review concluded that USAID evaluation practices made it difficult or impossible to account for use of aid funds, attention turned to tracking where aid money was going, not measuring what it was accomplishing. At the same time, USAID was facing increasing budgetary pressure and increasing congressional and public concern about what was being achieved through foreign assistance. In response, USAID carried out an Evaluation Initiative from 1990 to 1992, greatly expanding the staff and budget of CDIE and making significant investments in rigorous evaluation designs and innovative methods to evaluate sector-wide results. However, by the mid-1990s the priorities changed once again. A 1993 agency reorganization led to the 1994 elimination of an Office of Evaluation within CDIE, a reduction of overall CDIE staff, and a new emphasis on \"rapid appraisal techniques,\" which guidance documents describe as a compromise between slow, costly, and credible formal evaluation methods and cheap, quick, informal methods (focus group, etc.) that may be less reliable.  In 1995, USAID replaced the requirement to conduct mid-term and final evaluations of all projects with a policy calling for evaluation only when necessary to address a specific management question. The rationale was that the required evaluations had become pro forma, as GAO reviews had suggested, and that fewer, more comprehensive evaluations would be a better use of time and resources. As a result, the number of completed evaluations dropped from 425 in 1993 to an estimated 138 in 1999, but the depth and scope of new evaluations reportedly did not change. One study suggests that inconsistent guidance on evaluation in these years allowed many already overburdened mission staff to ignore agency-wide requirements, but noted that the Global Health, Africa, and Europe & Eurasia bureaus, which had their own evaluation procedures, continued to carry out quality evaluation work.  Foreign assistance levels grew rapidly starting in 2003 to support military activities in Afghanistan and Iraq, as well as the President's Emergency Plan for AIDS Relief (PEPFAR) and the creation in 2004 of the Millennium Challenge Corporation (MCC). Accountability to Congress became a major evaluation priority. In 2005, inspired by remarks made by then House Foreign Operations Appropriations Subcommittee Chairman Jim Kolbe regarding the importance of being able to clearly demonstrate results of aid expenditures, USAID Administrator Andrew Natsios sought to revitalize evaluation within the agency. He sent a cable to all mission directors calling for the inclusion of evaluation plans, and higher quality evaluations, in all program designs; designated monitoring and evaluation officers at each post; and set aside funding for evaluations and incentives for employees who do evaluations; among other things.  In 2006, in further pursuit of accountability, as well as a desire to rationalize the bilateral assistance efforts of multiple U.S. agencies, Secretary of State Condoleezza Rice created the Office of the Director of Foreign Assistance (F Bureau) at the State Department. In addition to consolidating many USAID and State policy and planning functions for foreign assistance, the F Bureau established an extensive set of standard performance indicators \"to measure both what is being accomplished with U.S. Government foreign assistance funds and the collective impact of foreign and host-government efforts to advance country development.\" Prior to this initiative, the State Department, which traditionally had managed a much smaller aid portfolio than USAID, is said to have made a de facto decision not to evaluate its assistance programs on a systematic basis. The data collected through the \"F process,\" which remains in place today, allow for a marked improvement in aid transparency, demonstrating comprehensively where and for what purpose aid funds are allocated by State and USAID as of FY2006. However, the demands of F process reporting were believed by some to have interfered with more results-oriented evaluation work at USAID, and a 2008 assessment of State's evaluation capacity found that several bureaus, including those that manage State's security assistance programs, still had little or no evaluation capacity.  The structural reforms of the F Bureau came at a time of heightened congressional scrutiny of foreign aid. In 2004, Congress established the Helping to Enhance the Livelihood of People (HELP) Around the Globe Commission, through a provision in P.L. 108-199 , to independently review foreign assistance policy decisions, delivery challenges, methodology, and measurement of results. After nearly two years of work, the HELP Commission released its report in late 2007. On the subject of evaluation, the report noted that \"everyone to whom members of the Commission spoke about monitoring and evaluation expressed concern about the inadequacy of the existing process\" and concluded that \"unless our government better evaluates projects based on the outcomes they achieve, it will not improve the effectiveness of taxpayer dollars.\" The commission recommended creation of a unified foreign assistance policy, budgeting, and evaluation system within State, quite similar to the F process, which was established before the report was released. Other HELP Commission recommendations included ensuring that evaluation strategies use control groups and randomization as much as possible; considering new evaluation methods, such as the use of professional associations or accreditation agencies; and building, in collaboration with other donors, the capacities of recipient governments to provide reliable baseline data. At the same time the F Bureau was established, and the HELP Commission was active, the international donor community began to prioritize aid effectiveness, sparking renewed interest in rigorous impact evaluation (see the \"A Global Perspective on Aid Evaluation\" text box below). Some aid professionals viewed the F process as an opportunity to build a cross-agency aid evaluation practice focused on impact, and were disappointed that the common indicators used by the F Bureau, while an improvement with respect to comparability, measured outputs rather than impact. Furthermore, the use of more rigorous evaluation methodologies was not a focus of the reform.  These issues were revisited by the Obama Administration when it embarked in 2009 on a Quadrennial Diplomacy and Development Review (QDDR) to examine how State and USAID could be better prepared for current and future challenges. As a result of that review, the Administration committed itself in December 2010 to several principles of foreign assistance effectiveness, including \"focusing on outcomes and impact rather than inputs and outputs, and ensuring that the best available evidence informs program design and execution.\" The first QDDR became the basis of many changes at State and USAID, including the creation of a new Office of Learning, Evaluation and Research at USAID and a new USAID evaluation policy, which took effect in January 2011. A second QDDR, in 2015, called for training to deepen evaluation expertise at both USAID and State, and for adding \"rigor\" to evaluations through better use of diagnostics and data analysis. The State Department adopted an evaluation policy similar to that of USAID in February 2012, requiring all large projects and programs to be evaluated at least once in their lifetime or five-year period, all State bureaus to complete two to four evaluations before the end of 2012-2013, and posts to do the same in 2013-2014. The 2012 policy also called for 3%-5% of program resources to be identified for evaluation purposes. It appears, however, that some of these requirements were not met, and in January 2015, State revised its policy, paring it down to a less directive form that was thought to be more appropriate for the wide range of State activities, from diplomatic engagement to foreign assistance, and to reflect ongoing challenges in evaluating particularly sensitive activities such as security assistance (see the \"Evaluating Security Assistance\" text box below). The new policy removed the requirement that all large projects be evaluated, requires one evaluation per bureau per year, and does not require any evaluations at the post level. Further details of the new policy are provided in the Appendix . The Millennium Challenge Corporation, established in 2004, has been regarded by many as a leader in aid evaluation, largely as a result of its demanding evaluation policy. MCC provides funding and technical assistance to support five-year development plans, called \"compacts,\" created and submitted by partner countries. Since its inception, MCC policy has required that every project in a compact be evaluated by independent evaluators, using pre-intervention baseline data. MCC has also put a stronger emphasis on impact evaluation than State and USAID; of the 48 completed evaluations as of April 2016, 13 are described as impact evaluations (as are about 40 of the 101 planned evaluations), a much high proportion than at other aid agencies. Despite this emphasis, the overall impact of MCC assistance remains unclear. Individual project evaluations have demonstrated successful project implementation, but often little evidence of progress toward the overarching objective of raising household incomes in targeted areas . Such evidence, however, may only be apparent many years after compact completion. The current evaluation emphasis on measuring impact and broader learning about what works is not new; as discussed above, it was the basis of USAID evaluation policy in the 1970s and at various times since. Nevertheless, a 2009 meta-evaluation of U.S foreign aid programs indicated that rigorous impact evaluation\u2014the kind that could determine with credibility whether a specific aid intervention or broader sector strategy worked to produce a specific development outcome\u2014was rarely attempted. Of the 296 evaluations posted between 2005 and 2008 to USAID's Development Experience Clearinghouse website, an independent reviewer found only 9% reported on a comparison group and only one used an experimental design involving randomized assignment, the method most likely to produce accurate data. A 2005 review of USAID evaluations (focused on democracy and governance programs) found that \"as a group, they lacked information that is critical to demonstrating the results of USAID projects, let alone whether the projects were the real cause of whatever change the evaluation reported.\" A meta-evaluation covering the period 2009-2012 found a notable increase in evaluation following the new evaluation policy and found improvements in 68% of quality factors examined, including the inclusion of recommendations. For most factors, however, the improvements were less than 15%, and most evaluations met USAID quality standards in only a few of the 37 criteria reviewed. USAID anticipates completing a second meta-evaluation, covering the period 2012-2016, in 2017. The gap between evaluation goals and actual practices has been documented repeatedly over the history of U.S. foreign assistance. So, too, have the challenges that make it difficult for implementers to achieve ideal evaluation practices in the field. Some of these challenges are discussed below. Mixed Objectives . The U.S. foreign assistance program has dozens of official objectives written into statute, and many aid programs are designed to meet multiple objectives. Often there are both strategic objectives and development objectives attached to an aid intervention, which may or may not be acknowledged in budget and planning documents. For example, assistance to Uzbekistan may have been requested and appropriated for specific agriculture sector activities, but may have been motivated primarily by a desire to secure U.S. overflight privileges for military aircraft bringing troops and supplies to Afghanistan. An evaluation of the agricultural impact may be of no use to policymakers who are more interested in the strategic goal, nor to aid professionals who are unlikely to view any lessons learned in these circumstances as applicable to agricultural development projects if political needs overrode the development rationale for the program.  Another example is the Food for Peace program, which provides U.S. agricultural commodities to countries facing food insecurity. One objective of the program is to feed hungry people, but long-standing requirements that most of the food be provided by U.S. agribusiness and be shipped by U.S.-flagged vessels make clear that supporting the U.S. agriculture and shipping industries is a program objective as well, and a potentially conflicting one. Studies have shown that the buy and ship America provisions, as they are known, may lessen the hunger-alleviation impact of food aid by up to 40%.  Despite the political and diplomatic considerations that arguably underlie the majority of foreign aid, evaluations that examine those strategic objectives are rare (or at least not publicly available). This may be understandable, as such evaluations would often be politically and diplomatically sensitive. Nevertheless, evaluation that focuses only on the development or humanitarian impact of a particular program or project, when broader strategic objectives are drivers of the aid, may largely miss the point. For example, a 2015 Mercy Corp evaluation of youth employment programs in Afghanistan (funded by the United Kingdom, not the United States) tested the assumption that a program to create economic opportunities for youth would promote stability by lessening participants' support for political violence. Contrary to expectation, the evaluation found that the employment, economic confidence, and business connections fostered by the program made participants more likely to express support for political violence. Funding and Personnel C onstraints . The more rigorous and extensive an evaluation, the costlier it tends to be, both in funds and staff time. Impact evaluations are particularly costly and require specially trained implementers. Absent a directive from agency leadership, aid implementers are unlikely to make resources available for evaluation at the expense of other program components. As one internal USAID review explained, \"since USAID's development professionals have limited staff, limited budget, and copious priorities, unfortunately, due to lack of training on the crucial role of evaluation in the development process, most have chosen to eliminate evaluation from their programs.\" Competitive contracting plays a role as well. At a time when most program implementation is contracted out, and cost is a key factor in winning contract bids, some argue that there is little incentive to invest in the up-front costs, such as baseline surveys, of a well-designed evaluation plan in the absence of an enforced requirement. As a result, ad hoc evaluations of limited scope and learning value\u2014as one report describes it, the \"do the best you can in three weeks\" approach\u2014often prevail by default. \"It is rare,\" according to one report, \"that the resources provided for an evaluation are sufficient to develop and apply more rigorous research methods that would produce valid empirical evidence regarding outcomes and attributable impact.\" While MCC has the benefit of compacts being fully funded up front, which may account in part for its more comprehensive evaluation practices, State and USAID cannot count on receiving requested project funding from year to year, creating a challenge for all aspect of program implementation, including evaluation.  Sometimes the limited resource is personnel, rather than funding. Past reviews of assistance evaluation repeatedly cite lack of trained evaluation personnel as a problem. USAID has tried to address this problem by training 1,600 staff in evaluation design and implementation since 2011 and producing a number of evaluation tools, publications, and webinars available to staff. USAID has also recently recruited monitoring and evaluation fellows, who are placed for six months to two years in offices that need additional expertise. Another part of this effort is building strong relationships with other entities focused on aid evaluation, including aid agencies of other donor countries and the International Initiative for Impact Evaluation (3ie). Some experts have suggested that greater emphasis on collective evaluations\u2014donor countries and foundations contributing to an independent organization that conducts evaluations of aid crossing many donor portfolios\u2014could address resource and expertise limitations as well as allow for generalization of evaluation findings and policy relevance. Emphasis on Accountability of F unds . Aid monitoring and evaluation efforts over the past decade have primarily focused on accountability of funds because that is what stakeholders, including Congress, generally ask about. Concerned about corruption and waste, bound by allocation limits, and required by law to report on various aspects of aid administration, implementing agencies have developed monitoring, evaluation, and data collection practices that are geared toward tracking where funds go and what they have purchased rather than the impact of funds on development or strategic objectives. For example, the F Bureau's Foreign Assistance Framework, launched in 2006, was created largely to address the information demands of stakeholders, who wanted more data on how aid funds are being spent. It worked, to the extent that it is now easier to find information on how much aid is being spent in a given year on counterterrorism activities in Kenya, for example, or on agricultural growth programs in Guatemala. But little if any of the resulting data addresses the impact of aid programs.  M ethodological C hallenges . In the complex environment in which many aid projects are carried out, it can be challenging to employ high quality evaluation methods. U.S. agency policies allow for a variety of evaluation methods (see Appendix ), acknowledging that the most rigorous methods are not always practical. Sometimes it is impossible to identify a comparable control group for an impact evaluation, or unethical to exclude people from a humanitarian intervention for the purpose of comparison. Sometimes the goals are intangible and cannot be accurately documented through metrics. For example, it may be much harder to measure the impact of programs such as the Middle East Partnership Initiative, designed to strengthen relationships, than to measure more concrete objectives, such as reducing malaria prevalence. This may be one reason why reviews have found that global health assistance has a stronger evaluation history than other aid sectors; disease prevalence and mortality rates lend themselves to quantification better than military personnel attitudes towards human rights or the strength of civil society. Rigorous methodology can also limit program flexibility, as making program changes mid-course, in response to changed circumstances or early results, can compromise the evaluation design. Some MCC evaluation reports note that information gleaned from early project implementation resulted in mid-course changes that improved program logic but undermined impact evaluation plans.  Even when metrics and baselines are well established, it can still be very difficult to attribute impact to a specific U.S. aid intervention when such programs are often carried out in the context of a broader trade, investment, political, and multi-donor environment. A 2016 SIGAR report, for example, notes that while USAID frequently cites improvements in Afghanistan's education sector among the highlights of U.S. reconstruction efforts, the agency is unable to establish a link between U.S. assistance and trends in the sector, in which many donors are active. Also, some aid professionals see broader drawbacks to rigorous impact evaluation methods. Some assert that the use of randomized control groups, which generally require the use of independent evaluators, limits the participation of affected individuals and communities in project design. They argue that community participation in project planning and evaluation, which can lead to greater buy-in and local capacity building, is more valuable in the development context than high-quality evaluation findings. Others counter that more participatory methodologies are often weakened by bias, and that it is unwise and even unethical to replicate programs, which may profoundly affect participants, without having properly evaluated them.  Compressed Timelines . While development assistance, in particular, is recognized as a long-term endeavor, aid strategies can be trumped by political pressures, which can influence evaluation. In 2001, a USAID survey report stated that \"the pattern found was that evaluation work responds to the more immediate pressures of the day.\" Policymakers facing relatively short budget and election cycles do not always allow adequate time for programs to demonstrate their potential impact. Such pressures have only increased over the past 15 years, particularly in the politically charged environments of Iraq, Afghanistan, and Pakistan. As a Senate Foreign Relations Committee majority-staff report on aid to Afghanistan found, \"the U.S. Government has strived for quick results to demonstrate to Afghans and Americans alike that we are making progress. Indeed, the constant demand for immediate results prevented the implementation of programs that could have met long-term goals and would now be bearing fruit.\"  The type of evaluation necessary to determine whether aid has real impact is both hard to do and of limited use in a short-term context. Timelines are particularly restrictive for MCC, which originally intended to complete evaluations during the compact implementation period. This goal, which reflects broad support for limited timeframes on foreign assistance, was found not to be feasible during implementation of MCC's first compacts in Cape Verde and Honduras. Baseline data and evaluation models can be rendered worthless if program timelines change. For example, an MCC evaluation of a farmer training program in Armenia found that the planned impact evaluation model\u2014a phased roll-out\u2014was compromised by a delay in implementing one component of the program and the five-year compact timeline. The long-term impacts of aid may be the most significant in judging effectiveness, but are least likely to be evaluated.  Country Ownership and Donor Coordination . The United States and other aid donor countries have made pledges to both coordinate their efforts and increase recipient country control, or \"ownership,\" over the planning of aid projects and the management of aid funds. Country ownership is believed by many to increase the odds that positive results will be sustained over time both by ensuring aid projects are consistent with recipient priorities and by helping to build the budget and project management capacity of recipient country governments and nongovernmental organizations (NGOs) that administer the assistance. Donor coordination of assistance efforts is supposed to promote efficiency, ease administrative burdens on aid recipients, and avoid duplication, among other things. USAID, as part of its ongoing procurement reform process, aims to channel an increasing portion of contract and grant aid directly to governments and local organizations. However, greater country ownership, and the pooled funds that may result from donor coordination, generally means diminished donor control, and a lesser ability to evaluate how U.S. funds contributed to a particular outcome. Accountability concerns often greatly overshadow the learning aspects of evaluation in such a context, as Congress has expressed concern about the heightened potential for corruption and mismanagement when funds flow directly to recipient country institutions. A 2016 report of the Special Inspector General for Afghanistan Reconstruction (SIGAR), for example, notes that while an increasing portion of U.S. aid to Afghanistan is being provided through Afghan government ministries, these ministries struggle with staffing, technical skills, management, and accountability. Security . Over the past 15 years, a significant percentage of foreign aid has been allocated to countries where security concerns have presented major obstacles to implementing, monitoring and evaluating foreign aid. A 2012 evaluation of a USAID agricultural development program in rural Pakistan, for example, states \"the operating environment for development projects has been especially testing in recent years in the presence of an insurgency and frequent targeted killings and kidnappings.\" Development staff in Afghanistan and Iraq in particular have not always been able to safely visit project sites to verify that a structure has been built or supplies delivered, much less be out on the streets conducting the types of surveys that certain evaluations would normally call for. A 2011 USAID Inspector General report noted that more than half of performance audits in Iraq at that time indicated security concerns, and a 2016 SIGAR report noted that the drawdown of U.S. and coalition military personnel in Afghanistan, and the deteriorating security situation, made it difficult or impossible for civilian agency personnel to oversee projects first-hand. Even in less hostile environments, security concerns can undermine evaluation quality. For example, a 2011 evaluation of Office of Transition Initiatives governance activities in Colombia noted that \"security considerations limited to some degree the evaluation team's freedom to interview community members in project sites at will. This fact made it difficult to be certain that field research did not suffer from a form of sampling bias.\" While security challenges may weigh against the use of aid in certain regions, the most insecure places are sometimes where the U.S. foreign policy interests are greatest, and policymakers must consider whether the risk of being unable to evaluate even the performance of an aid intervention is worth taking for other reasons. Agency and Personal Incentives. Given discretion in the use and conduct of evaluations, observers have noted the inclination of foreign assistance officials to avoid formal evaluation for fear of drawing attention to the shortcomings of the programs on which they work. While agency staff are clearly interested in learning about program results, many are reportedly defensive about evaluation, concerned that evaluations identifying poor program results may have personal career implications, such as loss of control over a project, damage to professional reputation, budget cuts, or other potential career repercussions. As explained by one USAID direct-hire in response to a 2001 survey, \"if you don't ask [about results], you don't fail, and your budget isn't cut.\" That same study revealed that staff felt more pressure to produce success stories than to produce balanced and rigorous evaluations, and that \"professional staff do not see any Agency-wide incentive to advance learning through evaluations.\" Few observers consider risk taking and accepting failure as a necessary component of learning to be hallmarks of USAID or State Department culture, but a shift in this attitude may be in progress. According to USAID Administrator Gayle Smith, there has been \"a cultural shift from checking the box that everything is fine to here's what we're learning and here's what happened.\" Other experts have suggested that there remains a reluctance within USAID to hold staff responsible for poor evaluation practices.  A consistent theme in past reviews of foreign aid evaluation practices is that even when quality evaluation takes place, the resulting information and analysis are often not considered and applied beyond the immediate project management team. Evaluations are rarely designed or used to inform policy. Lack of faith in the quality of the evaluation, irregular dissemination practices, and resistance to criticism may all contribute to this problem, as does lack of time on the part of aid implementers and policymakers alike to read and digest evaluation reports. A 2009 survey of U.S. aid agencies found that \"bureaucratic incentives do not support rigorous evaluation or use of findings,\" \"evaluation reports are often too long or technical to be accessible to policymakers and agency leaders with limited time,\" and learning that takes place, if any, is \"largely confined to the immediate operational unit that commissioned the evaluation.\" The shift in recent decades towards the use of contractors and implementing partners for most project implementation, and most project evaluation, may also impact the learning process. As one report notes, \"partner organizations are learning from the experience, but USAID is not,\" and most evaluation work does not circulate beyond the partner.  Congress expressed some interest in this issue with the Initiating Foreign Assistance Reform Act of 2009 ( H.R. 2139 in the 111 th Congress, introduced by Representative Howard Berman), which called for \"a process for applying the lessons learned and results from evaluation activities, including the use and results of impact evaluation research, into future budgeting, planning, programming, design and implementation of such United States foreign assistance programs.\" The government-wide GPRA performance planning and assessment requirements mentioned earlier (see \"Program Evaluation Government-Wide\" text box above) also attempted to mandate better use of evaluation data in policymaking government-wide. Aid agencies have addressed this issue with renewed focus and mixed results. USAID reviewed the utilization of evaluation data over the first several years under its new policy and found that 90% of surveyed evaluation findings and recommendations had some impact on program-level decisionmaking, mostly for project design and modification. USAID requires that its five-year Country Development Cooperation Strategies (CDCS) cite evidence as the basis of their development hypothesis, and 60% of the CDCS in 2015 cited evaluation reports as evidence. However, there is no USAID requirement that new policies draw on evaluation findings, and the study found little evidence linking evaluations to higher-level policy decisions. The learning aspect of evaluation relies heavily on agency culture, which may be shaped more by leadership than policy. The effective application of evaluation information depends also on the details of implementation, such as evaluation questions being based on the information needs of policymakers and program managers, and information being presented in a format and to a scale that is useful. Policymakers, for example, may be much better able to make actionable use of a meta-evaluation of microfinance programs, presented in a short report highlighting key findings, than a whole database of detailed analysis of single projects, the results of which may or may not be more broadly applicable. Experts have pointed out that individual project evaluations, even when well done, do not roll up nicely into a document showing what works and what does not. They contend that for maximum learning, an effort must be made at the cross-agency or even whole-of-government level to develop evaluation meta-data that is responsive not only to the needs of a project manager interested in the impact of a particular activity, but also to agency leadership and policymakers who want to know, more broadly, what foreign assistance is most effective.  This view has been reflected in legislation introduced in recent Congresses. The Foreign Assistance Revitalization and Accountability Act of 2009 ( S. 1524 in the 111 th Congress, introduced by then Senator Kerry) called for the creation of a Council on Research and Evaluation of Foreign Policy to do cross-agency evaluation of aid programs. The Foreign Aid Transparency and Accountability Act (introduced in successive congresses by Senator Marco Rubio and Representative Ted Poe before being enacted and signed into law in July 2016), directs the President to establish guidelines for the consistent evaluation of foreign assistance across federal agencies.  As important as evaluation can be to improving aid effectiveness, not every aid project has broad learning potential. Knowing which potential evaluations could have the greatest policy implications may be key to maximizing evaluation resources. Many USAID projects, for example, are designed with no intention that they be scaled up or replicated elsewhere. In other situations, an approach may have already been well proven. In such instances, a basic performance evaluation for accountability may be appropriate, but rigorous evaluation may be a poor use of resources. A 2012 USAID \"Decision Tree for Selecting the Evaluation Design\" asks staff to first consider whether an evaluation is needed, and decline to evaluate if the timing is not right, if there are no unanswered questions for the evaluation to address, or if there is no demand from stakeholders.  The primary U.S. government agencies managing foreign assistance each have their own distinct evaluation policies, with varying degrees of specificity. The Quadrennial Diplomacy and Development Review (QDDR) report of December 2010 stated the intent that USAID would reclaim its leadership role with respect to international development evaluation and learning, and referenced a new USAID evaluation policy in the works to reflect the growing demand for results data and attempt to address some persistent evaluation challenges. That policy took effect January 2011. The State Department followed suit in February 2012 with a new evaluation policy that was similar in many respects to the USAID policy, and MCC updated its policy in May 2012. State then updated its policy again in early 2015, apparently paring down several requirements in the 2012 policy, though the 2015 QDDR reaffirmed the State Department's commitment to building evaluation capacity. The Appendix table compares key provisions of the current evaluation policies of USAID, State, and MCC.  The State and USAID policies share much in common, balancing the costs and expected gains from evaluation. For example, both require performance evaluations of all larger-than-average projects and experimental/pilot projects, but not all projects. The policies share an emphasis on accessibility of information, with provisions to promote consistent and timely dissemination of evaluation reports, though State only requires public dissemination of foreign assistance evaluations, and summaries rather than full reports. In their introductory language, both policies emphasize the learning benefits of evaluation, in addition to accountability. The USAID policy is notably more detailed than State's on many of the issues. The USAID policy establishes required features for evaluation reports, and specifies that evaluation questions be identified in the design phase of projects, issues which the State policy does not address. USAID states that most evaluations will be conducted by third party contractors or grantees, to promote independence, while State's policy does not require independent evaluators. While USAID suggests a target allocation of 3% of program funds for program evaluation, the State policy provides no such target and the guidance suggests that such a target may not be realistic. Perhaps most significantly, USAID's policy calls for impact evaluation whenever feasible, while the State policy sets a clear expectation that impact evaluation will be rare.  MCC's evaluation policy shares many elements of the State and USAID policies, but goes farther in many respects. MCC requires independent evaluations of all compact projects, using indicators and baselines established prior to project implementation. The agency has also made a practice of including a \"lessons learned\" section in its evaluation reports. It may be, however, that first-hand experience with the challenges of evaluation is bringing MCC policy and practice closer to that of USAID over time. MCC's 2012 policy revision adopts definitions from USAID's 2011 evaluation policy and includes a section on institutional learning. The update also appears to move closer to the USAID model with respect to impact evaluation, calling for impact evaluations \"when their costs are warranted,\" whereas the previous iteration referred to independent impact evaluations as an \"integral part\" of MCC's focus on results. The MCC policy still appears to have the strongest enforcement mechanism among the three agency policies, conditioning the release of quarterly disbursements on substantial compliance with the policy. USAID's policy, in contrast, calls only for occasional compliance audits, and State's policy does not address compliance at all. While some experts have called for greater uniformity of evaluation practices across agencies to allow for comparative analysis, others view the differences in State, USAID, and MCC evaluation polices as reflecting the different experience, scope of work, and priorities of the agencies. USAID, with the largest and most diverse assistance portfolio among the agencies, and numerous small projects, may require a more flexible approach to evaluation than MCC, which is narrowly focused on economic growth and recipient government ownership. At State, foreign assistance is just one part of a broader portfolio (including diplomatic activities), potentially impacting what type and scope of evaluation is useful or possible. State is also responsible for many military and security assistance programs, which present unique challenges, as discussed in the \" Evaluation Challenges \" section above. These current evaluation policies may represent a step towards improving knowledge of foreign assistance measures of effectiveness at the program or project level, and increasing transparency of the evaluation process. They do not, however, attempt to establish a systemic approach to aid evaluation that would make country-wide, sector-wide, or cross-agency evaluation or aid more feasible. They look similar to earlier initiatives to improve aid evaluation. Many aspects of the 2011 USAID policy, for example, are strikingly similar to the required actions called for in the 2005 cable to USAID missions (e.g., evaluation planning as part of all program designs, designated evaluation officers at each post, and set-aside evaluation funds). It may be too early to know whether this new multiagency initiative will have more real or lasting impact than its predecessors. A meta-evaluation examining USAID evaluations from 2009 to 2012 indicates that both the number and quality of evaluations increased significantly in that period, but most evaluations in 2012 still failed to meet evaluation standards.  While some momentum on foreign aid evaluation reform has originated within the Administration, Congress may have significant influence on this process. Not only can Congress mandate or promote a certain approach to evaluation directly through legislation, as has been proposed, it can modulate Administration policies by controlling the appropriations necessary to implement the policies. Congress may also influence how, or if, the information resulting from evaluations will impact foreign assistance policy priorities. These issues are discussed in greater detail below.  Re form Authorization Legislation. In the 112 th and 113 th Congresses, legislation was introduced that focused specifically on foreign aid evaluation. The Foreign Aid Transparency and Accountability Act ( H.R. 3159 / S. 3310 in the 112 th , S. 1271 / H.R. 2638 in the 113 th Congress) sought to evaluate the performance of U.S. foreign assistance programs and improve program effectiveness by requiring the President to establish guidelines on measurable goals, performance metrics, and monitoring and evaluation plans for foreign assistance programs that can be applied on a consistent basis across implementing agencies. The legislation also called for the creation of a website that would make detailed, program-level information on foreign assistance, including country strategies, budget documents, budget justifications, actual expenditures, and program reports and evaluations available to the public. The legislation was reintroduced in the 114 th Congress ( H.R. 3766 / S. 2184 ) with some modifications, including the exclusion of most security assistance. It was enacted and signed into law in July 2016 as P.L. 114-191 , potentially shaping aid evaluation practices in the years to come. The general focus of these proposals is on codifying evaluation requirements and extending them across the various federal and agencies that administer aid programs. The benefit of such broad uniformity, arguably, is that it could enable policymakers, the public, and other stakeholders to better compare the activities of various agencies and get a more comprehensive picture of total U.S. foreign assistance. A potential drawback is the effort and expense required to impose such uniformity on agencies with different objectives, management structures, and information technology systems. These proposals also focus on transparency and accountability rather than effectiveness, and do not explicitly promote the use of impact evaluation, though they call for the use of rigorous methodologies, including impact evaluation. If performance evaluation continues to comprise the vast majority of aid evaluations, such a cross-agency requirement may provide comparable information on aid management from agency to agency, but is not likely to facilitate comparative analysis of what aid channels are most effective.  Appropriations for Enhanced Evaluation . Increasing the number and quality of foreign aid evaluations, while potentially cost effective in the long run, requires an investment of resources. For the most part, evaluation costs are integrated into program accounts at the various implementing agency budgets and are not scrutinized specifically by Congress. Annual funding levels established by Congress, together with any related legislative directives that limit the use of funds, may play a role in determining the extent of the Administration's efforts and capacity to strengthen evaluation practice. Congress may also wish to specify in appropriations legislation a portion of funds to be used for evaluation purposes.  Impact of Evidence- Based Approach on Congressional Priorities . Congress has long exerted control over foreign assistance not only through appropriated funds and restrictions, but also by directing foreign assistance funds to certain sectors, countries, or even specific projects through bill or report language. For example, the committee reports accompanying the annual State-Foreign Operations appropriation proposals provide specific funding levels for microfinance, basic education, water and sanitation, women's leadership training, people-to-people reconciliation programs in the Middle East, and other sectors of particular interest to Members of Congress. Should credible information about the relative effectiveness of these programs be made available as a result of improved evaluation practices, Congress can weigh the importance of the data, among other considerations, in establishing aid priorities. Some congressional directives on aid are less likely than others to be affected by evaluation results. The availability of actionable evaluation data may not result in a maximization of aid effectiveness, but may allow Congress to make more deliberate trade-offs between effectiveness and other objectives.  The primary U.S. agencies charged with implementing foreign assistance have made significant steps in the last several years to address ongoing deficiencies in evaluation practices that make it difficult to judge whether foreign assistance is achieving its various objectives. There is widespread agreement on the need for more consistent performance evaluation of aid programs. The value of rigorous impact evaluation is broadly recognized as well, though the agencies differ in their capabilities and aspirations in this respect. Past policies and evaluation reform efforts, however, have been similarly focused but not sustained in the face of persistent challenges, many of which remain today. Other reforms, such as the establishment of centralized evaluation processes or the creation of an independent evaluation entity, have been proposed in legislation but not yet enacted. Growing emphasis in Congress and the Administration on results-based budgeting, as well as movement within the international aid donor community toward more rigorous aid evaluation practices, may provide the context for sustained progress. The 114 th Congress continues to have opportunities to influence how U.S. foreign assistance is evaluated through legislative proposals, appropriations, and oversight activities. "
}