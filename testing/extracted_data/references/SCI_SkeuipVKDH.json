{
    "title": "SkeuipVKDH",
    "content": "In the problem of unsupervised learning of disentangled representations, one of the promising methods is to penalize the total correlation of sampled latent vari-ables.   Unfortunately, this well-motivated strategy often fail to achieve disentanglement due to a problematic difference between the sampled latent representation and its corresponding mean representation.   We provide a theoretical explanation that low total correlation of sample distribution cannot guarantee low total correlation of the  mean representation. We prove that for the mean representation of arbitrarily high total correlation, there exist distributions of latent variables of abounded total correlation.   However, we still believe that total correlation could be a key to the disentanglement of unsupervised representative learning, and we propose a remedy,  RTC-VAE, which rectifies the total correlation penalty.    Experiments show that our model has a more reasonable distribution of the mean representation compared with baseline models, e.g.,\u03b2-TCVAE and FactorVAE. VAEs (Variational AutoEncoders) Kingma & Welling (2013) ; Bengio et al. (2007) follow the common assumption that the high-dimensional real world observations x can be re-generated by a lowerdimension latent variable z which is semantically meaningful. Recent works Kim & Mnih (2018) ; Chen et al. (2018) ; Kumar et al. (2017) suggest that decomposing the ELBO (Evidence Lower Bound) could lead to distinguishing the factor of disentanglement. In particular, recent works Kim & Mnih (2018) ; Chen et al. (2018) focused on a term called total correlation (TC). The popular belief Chen et al. (2018) is that by adding weights to this term in objective function, a VAE model can learn a disentangled representation. This approach appears to be promising since the total correlation of a sampled representation should describe the level of factorising since total correlation is defined to be the KL-divergence between the joint distribution z \u223c q(z) and the product of marginal distributions j q(z j ). In this case, a low value suggests a less entangled joint distribution. However, Locatello et al. (2018) pointed out that the total correlation of sampled distribution T C sample being low does not necessarily give rise to a low total correlation of the corresponding mean representation T C mean . Conventionally, the mean representation is used as the encoded latent variables, an unnoticed high T C mean is usually the culprit behind the undesirable entanglement. Moreover, Locatello et al. (2018) found that as regularization strength increases, the total correlation of sampled representation T C sample and mean representation T C mean are actually negatively correlated. Locatello et al. (2018) put doubts on most methods of disentanglement including penalizing the total correlation term Kim & Mnih (2018) ; Chen et al. (2018) , and they concluded that \"the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases\". Acknowledging the difficulty in learning disentangled representation, we provide a detailed explanation of the seemingly contradictory behaviors of the total correlations of sampled and mean representation in previous works on TC penalizing strategy. Moreover, we find that this problem described above can be remedied simply with an additional penalty term on the variance of a sampled representation. Our contributions: \u2022 In Theorem 1, we prove that for all mean representations, there exists a large class of sample distributions with bounded total correlation. Particularly, a mean representation with arbitrarily large total correlation can have a corresponding sample distribution with low total correlation. This implies that a low total correlation of sample distribution cannot guarantee a low total correlation of the mean representation. (Section. 2) \u2022 Acknowledging the issue above, we further delve into total correlation, and provide a simple remedy by adding an additional penalty term on the variance of sample distribution. The penalty term forces a sampled representation to behave similar to the corresponding mean representation. Such penalty term is necessary for the strategy of penalizing T C mean in the view of Theorem 1. (Section. 4) \u2022 We study several different methods of estimating total correlation. They are compared and benchmarked against the ground truth value on the multivariate Gaussian distribution Locatello et al. (2018) . We point out that the method of (minibatch) estimators suffers from the curse of dimensionality and other drawbacks, making their estimation accuracy decay significantly with the increase of the dimension of the latent space, and some strong correlated distributions can be falsely estimated to have low total correlation. (Section. 5) In information theory, total correlation is one of the generalizations of mutual information, which measures the difference between the joint distribution of multiple random variables and the product of their marginal distributions. A high value means the joint distribution is far from an independent distribution, and hence it suggests high entanglement among these random variables. Definition 1. Total correlation of random variable x, . Naturally, people seek the solution of disentanglement in the form of low total correlation of the latent variables, e.g. Kim & Mnih (2018); Chen et al. (2018) . However, there can be large difference between the total correlations of sample representation and mean representation. Forcing the former to be small does not guarantee the latter being small. In fact, given a mean representation of arbitrarily large total correlation, we can construct a family of distribution of sample representation that have a bounded total correlation, where the bound does not rely on the total correlation of the mean. Theorem 1. Let \u00b5 \u223c N (0, \u03a3) and \u03c3 j be the standard deviation of \u00b5 j , j = 1, \u00b7 \u00b7 \u00b7 , D, and , where \u03a3 (\u00b5) is diagonal and satisfies that for some R > 0, (1) The details of the proof are presented in Appendix A.3. Here's another way to interpret Theorem 1: with C and parameters R, c 0 , \u00b7 \u00b7 \u00b7 , c 4 , l fixed, one can make T C(\u00b5) arbitrarily large, since T C(\u00b5) depends only on the correlation matrix of \u00b5 (see Proposition 1). Theorem 1 provides an explanation to the contradiction observed by Locatello et al. (2018) that TC(z) is low does not mean TC(\u00b5) is low (actually much higher than TC(z)). Since there exist such a large class of distributions of z that all have bounded T C(z). When the objective function only penalizes T C(\u00b5), neural networks are so flexible to easily find a distribution with low T C(z), and total correlation estimators like MSS can encourage shutting down latent dimensions (see Section 5.3), which together cause the disparity of T C(\u00b5) and T C(z). This fact was not noticed until Locatello et al. (2018) , and our investigation gives an explanation of the peculiar property of total correlation. Hence, Theorem 1 leads to the necessity of a regularizer of the difference between the distributions of \u00b5 and z when penalizing T C sample . In Section. 4, we propose a simple regularizer that serves this goal. It is an interesting question whether there exists a distribution of z N (\u00b5, \u03a3 (\u00b5)) with arbitrarily small T C(z) given \u00b5. If not, what's the lower bound of T C(z)? These questions remain open to us for now, and we leave them to future work. In the study of disentanglement, proposed a modification of the VAE framework and introduced an adjustable hyperparameter \u03b2 that balances latent channel capacity and independence constraints with reconstruction accuracy. One drawback of \u03b2-VAE is the trade-off between the reconstruction quality and disentanglement. Motivated to alleviate this trade-off of \u03b2-VAE, Kim & Mnih (2018) proposed FactorVAE which decomposes the evidence lower bound and penalize a term measuring the total correlation between latent variables. Around the same time, Chen et al. (2018) proposed a similar ELBO decomposition method called \u03b2-TCVAE. The major difference between FactorVAE and \u03b2-TCVAE lies in their different strategies of estimating total correlation. Chen et al. (2018) used formulated estimators while Kim & Mnih (2018) utilized the density-ratio trick which requires an auxiliary discriminator network and an inner optimization loop. We will discuss these two strategies more in details in Section. 5. The works above belong to representative learning without inductive biases. There are also works about representative learning with inductive biases, see Rolinek et al. (2019) and references therein. As for the disentanglement metric, this will be discussed in Section. 6. To simplify notation, let p(n) = p(x n ), q(z|x n ) = q(z|n). Recall the average evidence lower bound (ELBO), Chen et al. (2018) and independently by Kim & Mnih (2018) introduced objective function that penalizes total correlation, which can be formulated as This approach unfortunately has a drawback. It turns out that instead of being able to obtain disentangled representation, we often find a sample representation appears to be disentangled while the mean representation is still entangled. In fact, when we are maximizing L \u03b2\u2212TC , we could totally end up learning a distribution of z that makes TC(z) goes low, while the total correlation of its mean \u00b5 is still high. To resolve this, we define RTC-VAE, where Our penalty originates from the first term of the law of total covariance Factorized representation 1 indicates a diagonal covariance matrix Cov q(z) [z] . Motivated by this, Kumar et al. (2017) penalizes the off-diagonal terms in the second term, while ignores E p(n) Cov q(z|n) [z] since it is diagonal. Their penalty term leads to a vanishing \u00b5, which is the mean representation. The remedy to this is to add another penalty term on the distance between \u00b5's and 1. DIP-VAE Kumar et al. (2017) employs this remedy, however, DIP-VAE does not outperform other VAE's when measured by various disentanglement metrics, e.g., FactorVAE score, see Fig. 3 & 14 in Locatello et al. (2018) . This is actually not surprising since the two penalty terms in DIP-VAE contribute in opposite directions, with one leading to vanishing \u00b5's and another fighting against it. This formulation can easily get the model stuck in saddle points. Our objective, on the other hand, does not penalize directly on \u00b5. Instead, it penalizes on \u03c3, the standard deviation of the distribution q(z|n). This may seem little counter-intuitive at first sight, since penalizing a diagonal component of covariance Cov[z] = Cov q(z) [z] seems not helpful to factorising. However, in the view of Theorem 1, our objective will force the distribution of z to be similar to the distribution of \u00b5. Hence, it pushes us away from the situation of large TC(\u00b5) and low TC(z). Consequently, by minimizing TC(z) we get a model that has low TC(\u00b5), a disentangled mean representation. The naive Monte Carlo method comes with an intrinsic issue of underestimating total correlation. To avoid or resolve this, Kim & Mnih (2018) proposed a discriminator network with the help of density-ratio trick (see equation (3) and Appendix D. of Kim & Mnih (2018) ). In Chen et al. (2018), two kinds of estimator of total correlation are proposed, Minibatch Weighted Sampling (MWS) and Minibatch Stratified Sampling (MSS) (see Appendix C.1 and C.2 in Chen et al. (2018)). For instance, MSS can be described as followed. For a minibatch of sample, For the convenience of readers, MWS is listed in Appendix A.1. Density-ratio trick Nguyen et al. (2010) ; Sugiyama et al. (2012) can be used to estimate KLdivergence, where D is discriminator that classifies z being sampled from q(z) or j q(z j ). For detail implementation, please refer to section 3 in Kim & Mnih (2018) . For multivariate normal distribution, the total correlation can be explicitly calculated which can be used as a ground truth for our comparison. To be specific, Proposition 1. Let x \u223c N (0, \u03a3), then 2 There is a small part of the implementation of MSS in Chen et al.'s code that is not quite clear to us, specifically, the computation of log importance weight matrix in equation 6. In our experiment, we implement MSS with our understanding and denote it as MSS1, and we denote Chen et al.'s implementation MSS0. See Appendix A.2 It's difficult to track the exact reference of Proposition 1 since it is a fundamental property in information theory. Locatello et al. (2018) used this proposition to approximate the total correlation of the mean representation in latent space. In appendix, we provide a simple proof for the convenience of the readers. We compared the performance of each method, MWS, MSS 0 and MSS 1 on the estimation of total correlation. For \u00b5 \u223c N (0, I), and z|\u00b5 \u223c N (\u00b5, \u03a3) where \u03a3 = diag(\u03c3 2 ) and \u03c3 = 0.1. We choose \u03c3 small so that the distribution of z can be approximated by normal distribution. Results are presented in Figure 1 . From Figure. 1, we can summarize the following observations: 1. MWS tends to underestimate total correlation in general; 2. For latent space of dimension \u2264 4, MSS 0 and MSS 1 are quite accurate; 3. For latent space of high dimension, both MSS 0 and MSS 1 tend to overestimate total correlation when the actual value of total correlation is small; 4. Overall MSS 1 estimates closer to ground truth than MMS 0 does. In the following analysis of the above observations, we use a less formal way of analyzing, which can be formalized to be rigorous, in order to convey our idea directly. To interpret the third observation, let \u00b5 \u223c N (0, Id) and z|\u00b5 \u223c N (0, \u03a3), where Id is identity matrix and \u03a3 = diag(\u03c3 2 ). Then T C(\u00b5) = 0 and T C(z) small if \u03c3 small. Consider q(z is a sample drawn in a minibatch and z (i) = z(n (i) ). We claim this: when the ground truth total correlation of z is low (the off-diagnal values of correlation matrix is small), only the elements on the diagonal surface of the cube, namely those elements of index (i, i, k), take some bounded values O(1), and all the other elements are very small o(1) (since \u03c3 = 0.1). To see the claim, let's first consider 1-D case, where \u00b5 \u223c N (0, 1), z|\u00b5 \u223c N (0, \u03c3 2 ). When \u03c3 is small, z can be approximately treated as N (0, 1). z (i) and \u00b5 (j) are independent for i = j, hence z (i) \u2212 \u00b5 (j) \u223c N (0, 2), and See a proof in Appendix A.5. Then for D-dimension, the probability P (|z . Now, only if t takes value as small as \u03c3, q(z is not small. For example, \u03c3 = 0.1 and D = 10 and the chance of such case to happen is O(10 \u221210 ). Compared to batchsize, usually O(10 3 ), the amount of such cases can be ignored. Hence, Assigning weights to elements q(z ) such as MSS does not make essential change to the analysis above. In addition, \u03b2-TCVAE (trained with MSS in our experiments and with MWS in Locatello et al. (2018) ) seems to have an increasing total correlation of mean representation as regularization strength increases (higher \u03b2's), as observed by Locatello et al. (2018) . Here, we provide an explanation to the cause of this problem: Now, consider any strongly correlated z's (e.g., (z 1 , z 2 ) \u223c N (0, \u03a3), where \u03a3 = 0.01 \u22120.1 \u22120.1 1 , see Figure 2 ). Then the Gaussian (ground truth) total correlation is arbitrarily large (TC(z 1 , z 2 ) = \u221e). This kind of distribution can score a relatively low TC value (for instance lower than z) with estimators such as MSS and MWS by the analysis above. Hence, as \u03b2 increases, VAE trained with these estimators will be encouraged to obtain some dimensions of very low variance, and these dimensions are easily trapped in a strong correlation with other dimensions (like z 1 and z 2 ). Figure 2: One dimension of mean has low variance (shutting down), and the distribution is strongly correlated (It appears to be almost flat due to small scale of the shutdown dimension). A sampled distribution (e.g. z|\u00b5 \u223c N (\u00b5, 1)) has a very low TC. Shutting down dimension is not preferable because latent dimensions should not be fewer than ground truth. Moreover, considering datasets such as dSprites, though shape is labelled as a single dimension, models can learn to represent complex geometry with multiple dimensions, hence more active dimensions are learned than the number of ground-truth dimensions. Based on the reasons above, we opt for the method of discriminators (density-ratio trick) in our implementation. Experiment shows that density-ratio trick provides a more stable estimation of total correlation when training VAE. The datasets we use include dSprites , Shapes3D Burgess & Kim (2018) and 3D faces Paysan et al. (2009) . At the time of writing, the scale of our experiments is limited, but there are already some evidence to deliver our arguments. We have scheduled further experiments and tests on larger scale in future works. For every model, we trained with 10 different initialization. Hyperparameter \u03b2, also the regularization strength, takes 2, 4, \u00b7 \u00b7 \u00b7 , 10. For hyperparameter \u03b7, we fix \u03b7 = 10 for a simple reason. Since the variance term in equation 4 becomes small (close to 0) shortly after training begins, this term will not contribute much compared with L beta\u2212T C term. So, \u03b7 = 10 is enough to strengthen the penalty at the beginning of training. In our experiments, higher value of \u03b7 cannot bring further improvement since z is close to \u00b5 already. And lower values may not guarantee z being close to \u00b5. From experiments, we observe that RTC-VAE has much lower T C mean with different regularization strength than FactorVAE does (Figure 4 ). And on different datasets, this is also the case (see Appendix). The T C mean behaves almost identically as T C sample in RTC-VAE (see Figure 3 (b) ). The problem of contradictory behaviors of T C mean and T C sample is evidently remedied by RTC-VAE. In addition, the ELBO of RTC-VAE seems to converge faster than FactorVAE as a byproduct (see Figure 5 and Figure 6 ). Examining the distributions of latent dimensions (mean representation), FactorVAE tends to have some strongly correlated latent dimensions (see Figure 8) , and RTC-VAE shows well factorized latent distributions (see Figure 7 ). We wish there were a widely accepted metric of disentanglement to compare our model RTCVAE with other models. Unfortunately, it is still an open question, how we can measure disentanglement. Various attempts have been made so far, but Locatello et al. (2018) challenged most of them, indicating that the score under any metric varies due to different initializations and datasets. Here, we analyse several important metrics and attempt to point out some blind spots that have not being considered by these metrics. proposed using a classifier to measure each dimension of latent space and each ground truth factor, e.g. (x, y) coordinates, scale, rotation, etc. Kim & Mnih (2018) glement metric that considers modularity, compactness and explicitness. Then Ridgeway & Mozer (2018) made analysis on compactness, and compactness mean that each ground truth factor associates with only one or a few latent dimensions. They pointed out that in some situation a perfectly disentangled representation may not be compact (see Section. 3 in Ridgeway & Mozer (2018) ). Here we argue that modularity also should be reconsidered. A modular representation means that each dimension of latent space conveys information of at most one ground truth factor. This is exactly the goal attempted by ; Kim & Mnih (2018); Chen et al. (2018) , etc. However, multiple latent dimensions can work together to represent multiple ground truth factors meanwhile these latent dimensions are disentangled. For instance, x and y coordinates can be represented by r and \u03b8 in polar coordinate system (or any coordinate system under rotation, i.e., (x , y ) T = A(x, y) T where A is any orthogonal matrix). These coordinate systems are perfectly disentangled but r (or x ) conveys information of both x and y. In this work, we demonstrated that our RTC-VAE, which rectifies the total correlation penalty can remedy its peculiar properties (disparity between total correlation of the samples and the mean representations). Our experiments show that our model has a more reasonable distribution of the mean representation compared with baseline models including \u03b2-TCVAE and FactorVAE. We also provide several theoretical proofs which could help diagnose several specific symptoms of entangle-ment. Hopefully, our contributions could add to the explainability of the unsupervised learning of disentangled representations. See Chen et al. (2018) , A.2 MMS 0 AND MMS 1 There is no mathematical difference between MSS 0 and MSS 1 (same formulation as equation 5), only a difference in implementation. We replace this chunk of code (https://github.com/ rtqichen/beta-tcvae/blob/master/vae_quant.py#L199-L201) to f o r i i n r a n g e ( b a t c h s In the following proof, we use the convention of mathematical analysis that the meaning of C can change through lines to eliminate some redundant work of tracking. Theorem (Theorem 1 restated). Let \u00b5 \u223c N (0, \u03a3) and \u03c3 j be the standard deviation of \u00b5 j , j = 1, \u00b7 \u00b7 \u00b7 , D, and max j \u03c3 j = c 0 . For a fixed \u00b5, let z \u223c N (\u00b5, \u03a3 (\u00b5)), where \u03a3 (\u00b5) is diagonal and satisfies that for some R > 0, for some constants c 1 , c 2 , c 3 , c 4 . Then TC(z) \u2264 C for some C = C(R, c 0 , \u00b7 \u00b7 \u00b7 , c 4 , l) > 0. Proof. Let Since KL-divergence is non-negative, if TC(z) + is bounded, then TC(z) must be bounded. In the following, we work on S + , i.e., we assume p(z) \u2265 j p(z j ). For |z| < R, For |z| > 2R, where r 0 = max(c 0 , c 2 ) and r 1 = min(c 1 , c 4 ), and since l \u2265 1 and , it is easy to see that p(z) < C. And for |z| > R, Hence, Proof. First, recall that the KL-divergence between two distributions P and Q is defined as Also, the density function for a multivariate Gaussian distribution N (\u00b5, \u03a3) is p(x) = 1 (2\u03c0) n/2 det(\u03a3) 1/2 exp(\u2212 1 2 (x \u2212 \u00b5) T \u03a3 \u22121 (x \u2212 \u00b5))."
}