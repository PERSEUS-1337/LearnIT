{
    "title": "H1xipsA5K7",
    "content": "We give a new algorithm for learning a two-layer neural network under a very general class of input distributions. Assuming there is a ground-truth two-layer network \ny = A \\sigma(Wx) + \\xi,\nwhere A, W are weight matrices, \\xi represents noise, and the number of neurons in the hidden layer is no larger than the input or output,  our algorithm is guaranteed to recover the parameters A, W of the ground-truth network. The only requirement on the input x is that it is symmetric, which still allows highly complicated and structured input. \n\n Our algorithm is based on the method-of-moments framework and extends several results in tensor decompositions. We use spectral algorithms to avoid the complicated non-convex optimization in learning neural networks. Experiments show that our algorithm can robustly learn the ground-truth neural network with a small number of samples for many symmetric input distributions. Deep neural networks have been extremely successful in many tasks related to images, videos and reinforcement learning. However, the success of deep learning is still far from being understood in theory. In particular, learning a neural network is a complicated non-convex optimization problem, which is hard in the worst-case. The question of whether we can efficiently learn a neural network still remains generally open, even when the data is drawn from a neural network. Despite a lot of recent effort, the class of neural networks that we know how to provably learn in polynomial time is still very limited, and many results require strong assumptions on the input distribution. In this paper we design a new algorithm that is capable of learning a two-layer 1 neural network for a general class of input distributions. Following standard models for learning neural networks, we assume there is a ground truth neural network. The input data (x, y) is generated by first sampling the input x from an input distribution D, then computing y according to the ground truth network that is unknown to the learner. The learning algorithm will try to find a neural network f such that f (x) is as close to y as possible over the input distribution D. Learning a neural network is known to be a hard problem even in some simple settings (Goel et al., 2016; Brutzkus & Globerson, 2017) , so we need to make assumptions on the network structure or the input distribution D, or both. Many works have worked with a simple input distribution (such as Gaussians) and try to learn more and more complex networks (Tian, 2017; Brutzkus & Globerson, 2017; Li & Yuan, 2017; Soltanolkotabi, 2017; Zhong et al., 2017) . However, the input distributions in real life are distributions of very complicated objects such as texts, images or videos. These inputs are highly structured, clearly not Gaussian and do not even have a simple generative model. We consider a type of two-layer neural network, where the output y is generated as y = A\u03c3(W x) + \u03be. Here x \u2208 R d is the input, W \u2208 R k\u00d7d and A \u2208 R k\u00d7k are two weight matrices 2 . The function \u03c3 is the standard ReLU activation function \u03c3(x) = max{x, 0} applied entry-wise to the vector W x, and \u03be is a noise vector that has E[\u03be] = 0 and is independent of x. Although the network only has two layers, learning similar networks is far from trivial: even when the input distribution is Gaussian, Ge et al. (2017b) and Safran & Shamir (2018) showed that standard optimization objective can have bad local optimal solutions. Ge et al. (2017b) gave a new and more complicated objective function that does not have bad local minima. For the input distribution D, our only requirement is that D is symmetric. That is, for any x \u2208 R d , the probability of observing x \u223c D is the same as the probability of observing \u2212x \u223c D. A symmetric distribution can still be very complicated and cannot be represented by a finite number of parameters. In practice, one can often think of the symmetry requirement as a \"factor-2\" approximation to an arbitrary input distribution: if we have arbitrary training samples, it is possible to augment the input data with their negations to make the input distribution symmetric, and it should take at most twice the effort in labeling both the original and augmented data. In many cases (such as images) the augmented data can be interpreted (for images it will just be negated colors) so reasonable labels can be obtained. When the input distribution is symmetric, we give the first algorithm that can learn a two-layer neural network. Our algorithm is based on the method-of-moments approach: first estimate some correlations between x and y, then use these information to recover the model parameters. More precisely we have Theorem 1 (informal) . If the data is generated according to Equation (1), and the input distribution x \u223c D is symmetric. Given exact correlations between x, y of order at most 4, as long as A, W and input distribution are not degenerate, there is an algorithm that runs in poly(d) time and outputs a network\u00c2,\u0174 of the same size that is effectively the same as the ground-truth network: for any input x,\u00c2\u03c3(\u0174 x) = A\u03c3(W x).Of course, in practice we only have samples of (x, y) and cannot get the exact correlations. However, our algorithm is robust to perturbations, and in particular can work with polynomially many samples. Theorem 2 (informal). If the data is generated according to Equation (1), and the input distribution x \u223c D is symmetric. As long as the weight matrices A, W and input distributions are not degenerate, there is an algorithm that uses poly(d, 1/ ) time and number of samples and outputs a network A,\u0174 of the same size that computes an -approximation function to the ground-truth network: for any input x, \u00c2 \u03c3(\u0174 x) \u2212 A\u03c3(W x) 2 \u2264 .In fact, the algorithm recovers the original parameters A, W up to scaling and permutations. Here when we say weight matrices are not degenerate, we mean that the matrices A, W should be full rank, and in addition a certain distinguishing matrix that we define later in Section 2 is also full rank. We justify these assumptions using the smoothed analysis framework (Spielman & Teng, 2004) .In smoothed analysis, the input is not purely controlled by an adversary. Instead, the adversary can first generate an arbitrary instance (in our case, arbitrary weight matrices W, A and symmetric input distribution D), and the parameters for this instance will be randomly perturbed to yield a perturbed instance. The algorithm only needs to work with high probability on the perturbed instance. This limits the power of the adversary and prevents it from creating highly degenerate cases (e.g. choosing the weight matrices to be much lower rank than k). Roughly speaking, we show Published as a conference paper at ICLR 2019Theorem 3 (informal). There is a simple way to perturb the input distribution, W and A such that with high probability, the distance between the perturbed instance and original instance is at most \u03bb, and our algorithm outputs an -approximation to the perturbed network with poly(d, 1/\u03bb, 1/ ) time and number of samples. In the rest of the paper, we will first review related works. Then in Section 2 we formally define the network and introduce some notations. Our algorithm is given in Section 3. Finally in Section 4 we run experiments to show that the algorithm can indeed learn the two-layer network efficiently and robustly. The experiments show that our algorithm works robustly with reasonable number of samples for different (symmetric) input distributions and weight matrices. Due to space constraints, the proof for polynomial number of samples (Theorem 2) and smoothed analysis (Theorem 3) are deferred to the appendix. There are many works in learning neural networks, and they come in many different styles. Non-standard Networks Some works focus on networks that do not use standard activation functions. BID1 gave an algorithm that learns a network with discrete variables. Livni et al. (2014) and follow-up works learn neural networks with polynomial activation functions. Oymak & Soltanolkotabi (2018) used the rank-1 tensor decomposition for learning a non-overlapping convolutional neural network with differentiable and smooth activation and Gaussian input. ReLU network, Gaussian input When the input is Gaussian, Ge et al. (2017b) showed that for a two-layer neural network, although the standard objective does have bad local optimal solutions, one can construct a new objective whose local optima are all globally optimal. Several other works (Tian, 2017; Du et al., 2017b; Brutzkus & Globerson, 2017; Li & Yuan, 2017; Soltanolkotabi, 2017; Zhong et al., 2017; Zhang et al., 2018 ) extend this to different settings. General input with score functions A closely related work (Janzamin et al., 2015) does not require the input distribution to be Gaussian, but still relies on knowing the score function of the input distribution (which in general cannot be estimated efficiently from samples). Recently, Gao et al. (2018) gave a way to design loss functions with desired properties for one-hidden-layer neural networks with general input distributions based on a new proposed local likelihood score function estimator. For general distributions (including symmetric ones) their estimator can still require number of samples that is exponential in dimension d (as in Assumption 1(d)). There are several lines of work that try to extend the learning results to more general distributions. Du et al. (2017a) showed how to learn a single neuron or a single convolutional filter under some conditions for the input distribution. Daniely et al. (2016); Zhang et al. (2016; 2017) ; Goel & Klivans (2017) ; Du & Goel (2018) used kernel methods to learn neural networks when the norm of the weights and input distributions are both bounded (and in general the running time and sample complexity in this line of work depend exponentially on the norms of weights/input). Recently, Du et al. (2018) showed that gradient descent minimizes the training error in an over-parameterized two-layer neural network. They only consider training error while our results also apply to testing error. The work that is most similar to our setting is Goel et al. (2018) , where they showed how to learn a single neuron (or a single convolutional filter) for any symmetric input distribution. Our two-layer neural network model is much more complicated. Our work uses method-of-moments, which has already been applied to learn many latent variable models (see BID0 and references there). The particular algorithm that we use is inspired by an over-complete tensor decomposition algorithm FOOBI (De Lathauwer et al., 2007) . Our smoothed analysis results are inspired by BID1 and Ma et al. (2016) , although our setting is more complicated and we need several new ideas. Published as a conference paper at ICLR 2019 In this section, we first describe the neural network model that we learn, and then introduce notations related to matrices and tensors. Finally we will define distinguishing matrix, which is a central object in our analysis. We consider two-layer neural networks with d-dimensional input, k hidden units and k-dimensional output, as shown in Figure 1 . We assume that k \u2264 d. The input of the neural network is denoted by x \u2208 R d . Assume that the input x is i.i.d. drawn from a symmetric distribution D 3 . Let the two weight matrices in the neural network be W \u2208 R k\u00d7d and A \u2208 R k\u00d7k . The output y \u2208 R k is generated as follows: DISPLAYFORM0 where \u03c3(\u00b7) is the element-wise ReLU function and \u03be \u2208 R k is zero-mean random noise, which is independent with input x. Let the value of hidden units be h \u2208 R k , which is equal to \u03c3(W x). Denote i-th row of matrix W as w i (i = 1, 2, ..., k). Also, let i-th column of matrix A be a i (i = 1, 2, ..., k). By property of ReLU activations, for any constant c > 0, scaling the i-th row of W by c while scaling the i-th column of A by 1/c does not change the function computed by the network. Therefore without loss of generality, we assume every row vector of W has unit norm. DISPLAYFORM1 Figure 1: Network model. We use [n] to denote the set {1, 2, \u00b7 \u00b7 \u00b7 , n}. For two random variables X and Y , we say X d = Y if they come from the same distribution. In the vector space R n , we use \u00b7, \u00b7 to denote the inner product of two vectors, and use \u00b7 to denote the Euclidean norm. We use e i to denote the i-th standard basis vector. For a matrix A \u2208 R m\u00d7n , let A [i,:] denote its i-th row vector, and let A [:,j] denote its j-th column vector. Let A's singular values be \u03c3 1 (A) \u2265 \u03c3 2 (A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3 min(m,n) (A), and denote the smallest singular value be \u03c3 min (A) = \u03c3 min(m,n) (A). The condition number of matrix A is defined as \u03ba(A) := \u03c3 1 (A)/\u03c3 min (A). We use I n to denote the identity matrix with dimension n \u00d7 n. The spectral norm of a matrix is denoted as \u00b7 , and the Frobenius norm as \u00b7 F .We represent a d-dimensional linear subspace S by a matrix S \u2208 R n\u00d7d , whose columns form an orthonormal basis for subspace S. The projection matrix onto the subspace S is denoted by Proj S = SS , and the projection matrix onto the orthogonal subspace of S is denoted by Proj S \u22a5 = I n \u2212 SS .For matrix A \u2208 R m1\u00d7n1 , C \u2208 R m2\u00d7n2 , let the Kronecker product of A and C be A \u2297 C \u2208 R m1m2\u00d7n1n2 , which is defined as (A \u2297 C) (i1,i2),(j2,j2) = A i1,i2 C j1,j2 . For a vector x \u2208 R d , the Kronecker product x \u2297 x has dimension d 2 . We denote the p-fold Kronecker product of x as x \u2297p , which has dimension d p .We often need to convert between vectors and matrices. For a matrix A \u2208 R m\u00d7n , let vec(A) \u2208 R mn be the vector obtained by stacking all the columns of A. For a vector a \u2208 R m 2 , let mat(x) \u2208 R m\u00d7m denote the inverse mapping such that vec(mat(a)) = a. A central object in our analysis is a large matrix whose columns are closely related to pairs of hidden variables. We call this the distinguishing matrix and define it below: Definition 1. Given a weight matrix W of the first layer, and the input distribution D, the distin- DISPLAYFORM0 is a matrix whose columns are indexed by ij where 1 \u2264 i < j \u2264 k, and DISPLAYFORM1 . Another related concept is the augmented distinguishing matrix M , which is a d 2 \u00d7 (k 2 + 1) matrix whose first k 2 columns are exactly the same as distinguishing matrix N , and the last column (indexed by 0) is defined as DISPLAYFORM2 For both matrices, when the input distribution is clear from context we use N or M and omit the superscript. The exact reason for these definitions will only be clear after we explain the algorithm in Section 3. Our algorithm will require that these matrices are robustly full rank, in the sense that \u03c3 min (M ) is lowerbounded. Intuitively, every column N D ij looks at the expectation over samples that have opposite signs for weights w i , w j (w i xw j x \u2264 0, hence the name distinguishing matrix).Requiring M and N to be full rank prevents several degenerate cases. For example, if two hidden units are perfectly correlated and always share the same sign for every input, this is very unnatural and requiring the distinguishing matrix to be full rank prevents such cases. Later in Section C we will also show that requiring a lowerbound on \u03c3 min (M ) is not unreasonable: in the smoothed analysis setting where the nature can make a small perturbation on the input distribution D, we show that for any input distribution D, there exists simple perturbations D that are arbitrarily close to D such that \u03c3 min (M D ) is lowerbounded. In this section, we describe our algorithm for learning the two-layer networks defined in Section 2.1. As a warm-up, we will first consider a single-layer neural network and recover the results in Goel et al. (2018) using method-of-moments. This will also be used as a crucial step in our algorithm. Due to space constraints we will only introduce algorithm and proof ideas, the detailed proof is deferred to Section A in appendix. Throughout this section, when we use E[\u00b7] without further specification the expectation is over the randomness x \u223c D and the noise \u03be. We will first give a simple algorithm for learning a single-layer neural network. More precisely, suppose we are given samples (x 1 , y 1 ), ..., (x n , y n ) where x i \u223c D comes from a symmetric distribution, and the output y i is computed by DISPLAYFORM0 Here \u03be i 's are i.i.d. noises that satisfy E[\u03be i ] = 0. Noise \u03be i is also assumed to be independent with input x i . The goal is to learn the weight vector w. The idea of the algorithm is simple: we will estimate the correlations between x and y and the covariance of x, and then recover the hidden vector w using these two estimates. The main challenge here is that y is not a linear function on x. Goel et al. (2018) gave a crucial observation that allows us to deal with the non-linearity: Published as a conference paper at ICLR 2019Algorithm 1 Learning Single-layer Neural Networks Input: Samples (x 1 , y 1 ), ..., (x n , y n ) generated according to Equation (3). Output: Estimate of weight vector w. DISPLAYFORM0 Lemma 1. Suppose x \u223c D comes from a symmetric distribution and y is computed as in (3), then DISPLAYFORM1 Importantly, the right hand side of Lemma 1 does not contain the ReLU function \u03c3. This is true because if x comes from a symmetric distribution, averaging between x and \u2212x can get rid of nonlinearities like ReLU or leaky-ReLU. Later we will prove a more general version of this lemma (Lemma 6).Using this lemma, it is immediate to get a method-of-moments algorithm for learning w: we just need to estimate E[yx] and E[xx ], then we know DISPLAYFORM2 . This is summarized in Algorithm 1. In order to learn the weights of the network defined in Section 2.1, a crucial observation is that we have k outputs as well as k hidden-units. This gives a possible way to reduce the two-layer problem to the single-layer problem. For simplicity, we will consider the noiseless case in this section, where DISPLAYFORM0 Let u \u2208 R k be a vector and consider u y, it is clear that u y = (u A)\u03c3(W x). Let z i be the normalized version i-th row of A \u22121 , then we know z i has the property that z i A = \u03bb i e i where \u03bb i > 0 is a constant and e i is a basis vector. The key observation here is that if u = z i , then u A = \u03bb i e i . As a result, u y = \u03bb i e i \u03c3(W x) = \u03c3(\u03bb i w i x) is the output of a single-layer neural network with weight equal to \u03bb i w i . If we know all the vectors {z 1 , ..., z k }, the input/output pairs (x, z i y) correspond to single-layer networks with weight vectors {\u03bb i w i }. We can then apply the algorithm in Section 3.1 (or the algorithm in Goel et al. (2018) ) to learn the weight vectors. When u A = \u03bb i e i , we say that u y is a pure neuron. Next we will design an algorithm that can find all vectors {z i }'s that generate pure neurons, and therefore reduce the problem of learning a two-layer network to learning a single-layer network. Pure Neuron Detector In order to find the vector u that generates a pure neuron, we will try to find some property that is true if and only if the output can be represented by a single neuron. Intuitively, using ideas similar to Lemma 1 we can get a property that holds for all pure neurons: DISPLAYFORM1 As before, the ReLU activation does not appear because of the symmetric input distribution. For y = u y, we can estimate all of these moments (E[\u0177 2 ], E[\u0177x ], E[xx ]) using samples and check whether this condition is satisfied. However, the problem with this property is that even if z = u y is not pure, it may still satisfy the property. More precisely, if\u0177 = k i=1 c i \u03c3(w i x), then we have DISPLAYFORM2 Published as a conference paper at ICLR 2019The additional terms may accidentally cancel each other which leads to a false positive. To address this problem, we consider a higher order moment: DISPLAYFORM3 Here N ij 's are columns of the distinguishing matrix defined in Definition 1.The important observation here is that there are DISPLAYFORM4 considering their symmetry) dimensional objects. When the distinguishing matrix is full rank, we know its columns N ij are linearly independent. In that case, if the sum of the extra terms is 0, then the coefficient in front of each N ij must also be 0. The coefficients are c i c j which will be non-zero if and only if both c i , c j are non-zero, therefore to make all the coefficients 0 at most one of {c i } can be non-zero. This is summarized in the following Corollary: DISPLAYFORM5 . Suppose the distinguishing matrix is full rank, if f (u) = 0 for unit vector u, then u must be equal to one of \u00b1z i .We will call the function f (u) a pure neuron detector, as u y is a pure neuron if and only if f (u) = 0. Therefore, to finish the algorithm we just need to find all solutions for f (u) = 0.Linearization The main obstacle in solving the system of equations f (u) = 0 is that every entry of f (u) is a quadratic function in u. The system of equations f (u) = 0 is therefore a system of quadratic equations. Solving a generic system of quadratic equations is NP-hard. However, in our case this can be solved by a technique that is very similar to the FOOBI algorithm for tensor decomposition (De Lathauwer et al., 2007) . The key idea is to linearize the function by thinking of each degree 2 monomial u i u j as a separate variable. Now the number of variables is k 2 +k = k 2 +k and f is linear in this space. In other words, there exists a matrix DISPLAYFORM6 } are all in the nullspace of T . Later in Section A we will prove that the nullspace of T consists of exactly these vectors (and their combinations):Lemma 4. Let T be the unique R d 2 \u00d7(k2+k) matrix that satisfies T vec * (uu ) = f (u) (where f (u) is defined as in Corollary 1), suppose the distinguishing matrix is full rank, then the nullspace of T is exactly the span of {vec DISPLAYFORM7 Based on Lemma 4, we can just estimate the tensor T from the samples we are given, and its smallest singular directions would give us the span of {vec DISPLAYFORM8 Finding z i 's from span of z i z i 's In order to reduce the problem to a single-layer problem, the final step is to find z i 's from span of z i z i 's. This is also a step that has appeared in FOOBI and more generally other tensor decomposition algorithms, and can be solved by a simultaneous diagonalization. Let Z be the matrix whose rows are z i 's, which means Z = diag(\u03bb)A \u22121 . Let X = Z D X Z and Y = Z D Y Z be two random elements in the span of z i z i , where D X and D Y are two random diagonal matrices. Both matrices X and Y can be diagonalized by matrix Z. In this case, if we compute DISPLAYFORM9 That is, z i is an eigenvector of XY \u22121 ! The matrix XY \u22121 can have at most k eigenvectors and there are k z i 's, therefore the z i 's are the only eigenvectors of XY \u22121 . Published as a conference paper at ICLR 2019Lemma 5. Given the span of z i z i 's, let X, Y be two random matrices in this span, with probability 1 the z i 's are the only eigenvectors of XY \u22121 .Using this procedure we can find all the z i 's (up to permutations and sign flip). Without loss of generality we assume z i A = \u03bb i e i . The only remaining problem is that \u03bb i might be negative. However, this is easily fixable by checking E[ DISPLAYFORM0 has the same sign as \u03bb i , and we can flip z i if E[z i y] is negative. We can now give the full algorithm, see Algorithm 2. The main steps of this algorithm is as explained in the previous section. Steps 2 -5 constructs the pure neuron detector and finds the span of vec * (z i z i ) (as in Corollary 1); Steps 7 -9 performs simultaneous diagonalization to get all the z i 's; Steps 11, 12 calls Algorithm 1 to solve the single-layer problem and outputs the correct result. Algorithm 2 Learning Two-layer Neural Networks Input: Samples (x 1 , y 1 ), ..., (x n , y n ) generated according to Equation (4) Output: Weight matrices W and A. DISPLAYFORM0 where each entry is expressed as a degree-2 polynomial over u. {Reduce to 1-Layer Problem} 11: For each z i , let v i be the output of Algorithm 1 with input (x 1 , z i y 1 ), ..., (x n , z i y n ). 12: Let Z be the matrix whose rows are z i 's, V be the matrix whose rows are v i ' s. 13: return V , Z \u22121 . DISPLAYFORM1 We are now ready to state a formal version of Theorem 1:Theorem 4. Suppose A, W, E[xx ] and the distinguishing matrix N are all full rank, and Algorithm 2 has access to the exact moments, then the network returned by the algorithm computes exactly the same function as the original neural network. It is easy to prove this theorem using the lemmas we have. Proof. By Corollary 1, we know that after Step 5 of Algorithm 2, the span of columns of S is exactly equal to the span of {vec * (z i z i )}. By Lemma 5, we know the eigenvectors of XY \u22121 at Step 8 are exactly the normalized version of rows of A \u22121 . Without loss of generality, we will fix the permutation and assume z i A = \u03bb i e i . In Step 9, we use the fact that E[ DISPLAYFORM2 where E[\u03c3(w i x)] is always positive because \u03c3 is the ReLU function. Therefore, after Step 9 we can assume all the \u03bb i 's are positive. Now the output z i y = \u03bb i \u03c3(w i x) = \u03c3(\u03bb i w i x) (again by property of ReLU function \u03c3), by the design of Algorithm 1 we know v i = \u03bb i w i . We also know that Z = diag(\u03bb)A \u22121 , therefore DISPLAYFORM3 . These two scaling factors cancel each other, so the two networks compute the same function. Published as a conference paper at ICLR 2019Figure 2: Error in recovering W , A and outputs (\"MSE\") for different numbers of training samples and different dimensions of W and A. Each point is the result of averaging across five trials, where on the left W and A are both drawn as random 10 \u00d7 10 orthonormal matrices and in the center as 32 \u00d7 32 orthonormal matrices. On the right, given 10, 000 training samples we plot the square root of the algorithm's error normalized by the dimension of W and A, which are again drawn as random orthonormal matrices. The input distribution is a spherical Gaussian. In this section, we provide experimental results to validate the robustness of our algorithm for both Gaussian input distributions as well as more general symmetric distributions such as symmetric mixtures of Gaussians. There are two important ways in which our implementation differs from our description in Section 3.3. First, our description of the simultaneous diagonalization step in our algorithm is mostly for simplicity of both stating and proving the algorithm. In practice we find it is more robust to draw 10k random samples from the subspace spanned by the last k right-singular vectors of T and compute the CP decomposition of all the samples (reshaped as matrices and stacked together as a tensor) via alternating least squares (Comon et al., 2009) . As alternating least squares can also be unstable we repeat this step 10 times and select the best one. Second, once we have recovered and fixed A we use gradient descent to learn W , which compared to Algorithm 1 does a better job of ensuring the overall error will not explode even if there is significant error in recovering A. Crucially, these modifications are not necessary when the number of samples is large enough. For example, given 10,000 input samples drawn from a spherical Gaussian and A and W drawn as random 10 \u00d7 10 orthogonal matrices, our implementation of the original formulation of the algorithm was still able to recover both A and W with an average error of approximately 0.15 and achieve close to zero mean square error across 10 random trials. First we show that our algorithm does not require a large number of samples when the matrices are not degenerate. In particular, we generate random orthonormal matrices A and W as the ground truth, and use our algorithm to learn the neural network. As illustrated by Figure 2 , regardless of the size of W and A our algorithm is able to recover both weight matrices with minimal error so long as the number of samples is a few times of the number of parameters. To measure the error in recovering A and W , we first normalize the columns of A and rows of W for both our learned parameters and the ground truth, pair corresponding columns and rows together, and then compute the squared distance between learned and ground truth parameters. Note in the rightmost plot of Figure 2 , in order to compare the performance between different dimensions, we further normalize the recovering error by the dimension of W and A. It shows that the squared root of normalized error remains stable as the dimension of A and W grows from 10 to 32. In Figure 2 , we also show the overall mean square error-averaged over all output units-achieved by our learned parameters. FIG0 demonstrates the robustness of our algorithm to label noise \u03be for Gaussian and symmetric mixture of Gaussians input distributions. In this experiment, we fix the size of both A and W to be 10 \u00d7 10 and again generate both parameters as random orthonormal matrices. The overall mean square error achieved by our algorithm grows almost perfectly in step with the amount of label noise, Published as a conference paper at ICLR 2019 : Error in recovering W , A and outputs (\"MSE\"), on the left for different levels of conditioning of W and on the right for A. Each point is the result of averaging across five trials with 20,000 training samples, where for each trial one parameter is drawn as a random orthonormal matrix while the other as described in Section 4.3. The input distribution is a mixture of Gaussians with two components, one based at the all-ones vector and the other at its reflection.indicating that our algorithm recovers the globally optimal solution regardless of the choice of input distribution. We've already shown that our algorithm continues to perform well across a range of input distributions and even when A and W are high-dimensional. In all previous experiments however, we sampled A and W as random orthonormal matrices so as to control for their conditioning. In this experiment, we take the input distribution to be a random symmetric mixture of two Gaussians and vary the condition number of either A or W by sampling singular value decompositions U \u03a3V such that U and V are random orthonormal matrices and \u03a3 ii = \u03bb \u2212i , where \u03bb is chosen based on the desired condition number. FIG1 respectively demonstrate that the performance of our algorithm remains steady so long as A and W are reasonably well-conditioned before eventually fluctuating. Moreover, even with these fluctuations the algorithm still recovers A and W with sufficient accuracy to keep the overall mean square error low. Optimizing the parameters of a neural network is a difficult problem, especially since the objective function depends on the input distribution which is often unknown and can be very complicated. In this paper, we design a new algorithm using method-of-moments and spectral techniques to avoid the Published as a conference paper at ICLR 2019 complicated non-convex optimization for neural networks. Our algorithm can learn a network that is of similar complexity as the previous works, while allowing much more general input distributions. There are still many open problems. The current result requires output to have the same (or higher) dimension than the hidden layer, and the hidden layer does not have a bias term. Removing these constraints are are immediate directions for future work. Besides the obvious ones of extending our results to more general distributions and more complicated networks, we are also interested in the relations to optimization landscape for neural networks. In particular, our algorithm shows there is a way to find the global optimal network in polynomial time, does that imply anything about the optimization landscape of the standard objective functions for learning such a neural network, or does it imply there exists an alternative objective function that does not have any local minima? We hope this work can lead to new insights for optimizing a neural network. In this section, we first provide the missing proofs for the lemmas appeared in Section 3. Then we discuss how to handle the noise case (i.e. y = \u03c3(W x) + \u03be) and give the corresponding algorithm (Algorithm 3). At the end we also briefly discuss how to handle the case when the matrix A has more rows than columns (more outputs than hidden units).Again, throughout the section when we write E[\u00b7], the expectation is taken over the randomness of x \u223c D and noise \u03be. Single-layer: To get rid of the non-linearities like ReLU, we use the property of the symmetric distribution (similar to (Goel et al., 2018) ). Here we provide a more general version (Lemma 6) instead of proving the specific Lemma 1. Note that Lemma 1 is the special case when a = w and p = q = 1 (here \u03be does not affect the result since it has zero mean and is independent with x, thus DISPLAYFORM0 Lemma 6. Suppose input x comes from a symmetric distribution, for any vector a \u2208 R d and any non-negative integers p and q satisfying that p + q is an even number, we have DISPLAYFORM1 where the expectation is taken over the input distribution. Proof. Since input x comes from a symmetric distribution, we know that E \u03c3(a x) DISPLAYFORM2 There are two cases to consider: p and q are both even numbers or both odd numbers.1. For the case where p and q are even numbers, we have DISPLAYFORM3 2. For the other case where p and q are odd numbers, we have DISPLAYFORM4 14 Published as a conference paper at ICLR 2019Pure neuron detector: The first step in our algorithm is to construct a pure neuron detector based on Lemma 2 and Lemma 3. We will provide proofs for these two lemmas here. Proof of Lemma 2. This proof easily follows from Lemma 6. Setting a = w, p = 2 and q = 0 in Lemma 6, we have DISPLAYFORM5 Proof of Lemma 3. Here, we only prove the second equation, since the first equation is just a special case of the second equation. First, we rewrite\u0177 = k i=1 c i \u03c3(w i x) = u y by letting u A = c . Then we transform these two terms in the LHS as follows. Let's look at DISPLAYFORM6 (5) where the second equality holds due to Lemma 6. Now, let's look at the second term E (u y) DISPLAYFORM7 Published as a conference paper at ICLR 2019Now, we subtract (5) by (6) to obtain DISPLAYFORM0 where (7) uses FORMULA34 of the following Lemma 7, and (8) uses the definition of distinguishing matrix N (Definition 1). Lemma 7. Given input x coming from a symmetric distribution, for any vector a, b \u2208 R d , we have DISPLAYFORM1 where the expectation is taken over the input distribution. Proof. Here we just prove the first identity, because the proof of the second one is almost identical. First, we rewrite DISPLAYFORM2 Thus, we only need to show that DISPLAYFORM3 When a xb x > 0, we know a x and b x are both positive or both negative. In either case, we DISPLAYFORM4 which finished our proof. Finding span: Now, we find the span of {vec DISPLAYFORM5 Published as a conference paper at ICLR 2019It is not hard to verify that u y is a pure neuron if and only if f (u) = 0. Note that f (u) = 0 is a system of quadratic equations. So we linearize it by increasing the dimension (i.e., consider u i u j as a single variable) similar to the FOOBI algorithm. Thus the number of variable is DISPLAYFORM6 Now, we prove the Lemma 4 which shows the null space of T is exactly the span of {vec * (z i z i )}.Proof of Lemma 4. We divide the proof to the following two cases:1. For any vector vec * (U ) belongs to the null space of T , we have T vec * (U ) = 0. Note that the RHS of (10) equals to 0 if and only if A U A is a diagonal matrix since the distinguishing matrix N is full column rank and A U A is symmetric. Thus vec * (U ) belongs to the span of {vec * (z i z i )} since U = Z DZ for some diagonal matrix D.2. For any vector vec * (U ) belonging to the span of {vec DISPLAYFORM7 Note that A z i only has one non-zero entry due to the definition of z i , for any i \u2208 [k]. Thus all coefficients in the RHS of (10) are 0. We get T vec * (U ) = 0.Finding z i 's: Now, we prove the final Lemma 5 which finds all z i 's from the span of {vec * (z i z i )} by using simultaneous diagonalization. Given all z i 's, this two-layer network can be reduced to a single-layer one. Then one can use Algorithm 1 to recover the first layer parameters w i 's. Proof of Lemma 5. As we discussed before this lemma, we have Proof. First, we know there exist diagonal matrices DISPLAYFORM8 DISPLAYFORM9 } are the k-least right singular vectors of T (see Line 5 of Algorithm 2). Then, let the vector d i \u2208 R k be the diagonal elements of D i , for all i \u2208 k. Let matrix Q \u2208 R k\u00d7k be a matrix where its i-th column is DISPLAYFORM10 , where \u03b6 1 and \u03b6 2 are two random k-dimensional standard Gaussian vectors (see Line 7 of Algorithm 2). DISPLAYFORM11 Thus, Q has full rank and none of its rows are zero vectors. Let i-th row of Q be q i . Let's consider D X first. In order for i-th diagonal element of D X to be zero, we need q i \u03b6 1 = 0. Since q i is not a zero vector, we know the solution space of q i \u03b6 1 = 0 is a lower-dimension manifold in R k , which has zero measure. Since finite union of zero-measure sets still has measure zero, the event that zero valued elements exist in the diagonal of D X or D Y happens with probability zero. If i-th and j-th diagonal elements of DISPLAYFORM12 \u22121 . Again, we know the solution space is a lower-dimensional manifold in R 2k space, with measure zero. Since finite union of zero-measure sets still has measure zero, the event that duplicated diagonal elements exist in DISPLAYFORM13 Y happens with probability zero. A.2 NOISY CASE Now, we discuss how to handle the noisy case (i.e. y = \u03c3(W x) + \u03be). The corresponding algorithm is described in Algorithm 3. Note that the noise \u03be only affects the first two steps, i.e., pure neuron detector (Lemma 3) and finding span of vec * (z i z i ) (Lemma 4). It does not affect the last two steps, i.e., finding z i 's from the span (Lemma 5) and learning the reduced single-layer network. Because Published as a conference paper at ICLR 2019 Algorithm 3 Learning Two-layer Neural Networks with Noise Input: Samples (x 1 , y 1 ), ..., (x n , y n ) generated according to Equation (2) Output: Weight matrices W and A. 1: {Finding span of vec DISPLAYFORM14 where each entry is expressed as a degree-2 polynomial over u. DISPLAYFORM15 , where \u03b6 1 and \u03b6 2 are two independently sampled kdimensional standard Gaussian vectors. 8: Let z 1 , ..., z k be eigenvectors of XY \u22121 .9: For each z i , use the second half of samples {( DISPLAYFORM16 {Reduce to 1-Layer Problem} 11: For each z i , let v i be the output of Algorithm 1 with input {(x j , z i y j )} n j=n/2+1 . 12: Let Z be the matrix whose rows are z i 's, V be the matrix whose rows are v i 's. DISPLAYFORM17 Lemma 5 is independent of the model and Lemma 1 is linear wrt. noise \u03be, which has zero mean and is independent of input x. Many of the steps in Algorithm 3 are designed with the robustness of the algorithm in mind. For example, in step 5 for the exact case we just need to compute the null space of T . However if we use the empirical moments the null space might be perturbed so that it has small singular values. The separation of the input samples into two halves is also to avoid correlations between the steps, and is not necessary if we have the exact moments. Modification for pure neuron detector: Recall that in the noiseless case, our pure neuron detector contains a term E[(u y) 2 (x \u2297 x)], which causes a noise square term in the noisy case. Here, we modify our pure neuron detector to cancel the extra noise square term. In the following lemma, we state our modified pure neuron detector in Equation 11, and give it a characterization. Lemma 9. Suppose y = A\u03c3(W x) + \u03be, for any u \u2208 R k , we have DISPLAYFORM18 where N ij 's are columns of the distinguishing matrix (Definition 1), and DISPLAYFORM19 We defer the proof of this lemma to the end of this section. Recall that the augmented distinguishing matrix M consists of the distinguishing matrix N plus column E[x \u2297 x]. Now, we need to assume the augmented distinguishing matrix M is full rank. Modification for finding span: For Lemma 4, as we discussed above, here we assume the augmented distinguishing matrix M is full rank. The corresponding lemma is stated as follows (the proof is exactly the same as previous Lemma 4):Published as a conference paper at ICLR 2019 11 ), suppose the augmented distinguishing matrix is full rank, then the nullspace of T is exactly the span of {vec * (z i z i )}. DISPLAYFORM20 Similar to Theorem 4, we provide the following theorem for the noisy case. The proof is almost the same as Theorem 4 by using the noisy version lemmas (Lemmas 9 and 10).Theorem 5. Suppose E[xx ], A, W and the augmented distinguishing matrix M are all full rank, and Algorithm 3 has access to the exact moments, then the network returned by the algorithm computes exactly the same function as the original neural network. Now, we only need to prove Lemma 9 to finish this noise case. Proof of Lemma 9. Similar to (5) and FORMULA32 , we deduce these three terms in RHS of (11) one by one as follows. For the first term, it is exactly the same as (5) since the expectation is linear wrt. \u03be. Thus, we have DISPLAYFORM21 Now, let's look at the second term E (u y) 2 (x \u2297 x) which is slightly different from (6) due to the noise \u03be. Particularly, we add the third term to cancel this extra noise square term later. DISPLAYFORM22 where FORMULA0 uses (6).For the third term, we have DISPLAYFORM23 where the third equality holds due to Lemma 6 and Lemma 7, and (16) uses the definition of m ij . Published as a conference paper at ICLR 2019Algorithm 4 Learning Two-layer Neural Networks with Non-square A Input: Samples (x 1 , y 1 ), ..., (x n , y n ) generated according to Equation (2). Output: Weight matrices W \u2208 R k\u00d7d and A \u2208 l\u00d7k .1: Using half samples (i.e. {(x i , y i )} n/2 i=1 ) to estimate empirical moments\u00ca[yx ]. 2: Let P be a l \u00d7 k matrix, which columns are left singular vectors of\u00ca [yx ] . 3: Run Algorithm 3 on samples {(x i , P y i )} n i=n/2 . Let the output of Algorithm 3 be V, Z \u22121 .4: return V , P Z \u22121 .Finally, we combine these three terms (13-16) as follows: DISPLAYFORM0 where FORMULA0 uses FORMULA34 (same as (7) ). In this paper, for simplicity, we have assumed that the dimension of output equals the number of hidden units and thus A is a k \u00d7 k square matrix. Actually, our algorithm can be easily extended to the case where the dimension of output is at least the number of hidden units. In this section, we give an algorithm for this general case, by reducing it to the case where A is square. The pseudo-code is given in Algorithm 4. Theorem 6. Suppose E[xx ], W, A and the augmented distinguishing matrix M are all full rank, and Algorithm 4 has access to the exact moments, then the network returned by the algorithm computes exactly the same function as the original neural network. Proof. Let the ground truth parameters be A \u2208 R l\u00d7k and W \u2208 R k\u00d7d . The samples are generated by y = A\u03c3(W x)+\u03be, where the noise \u03be is independent with input x. We have DISPLAYFORM0 Since both W and E[xx ] are full-rank, we know the column span of E[yx ] are exactly the column span of A. Furthermore, we know the columns of P is a set of orthonormal basis for the column span of A.For a ground truth neural network with weight matrices W and P A, the generated sample will just be (x, P y). According to Theorem 5, we know for any input x, we have Z \u22121 \u03c3(V x) = P A\u03c3(W x). Thus, we have DISPLAYFORM1 where the second equality holds since P P is just the projection matrix to the column span of A. In this section we will show that even if we do not have access to the exact moments, as long as the empirical moments are estimated with enough (polynomially many) samples, Algorithm 2 and Published as a conference paper at ICLR 2019Algorithm 3 can still learn the parameters robustly. We will focus on Algorithm 3 as it is more general, the result for Algorithm 2 can be viewed as a corollary when the noise \u03be = 0. Throughout this section, we will useV ,\u1e90 \u22121 to denote the results of Algorithm 3 with empirical moments, and use V, Z \u22121 for the results when the algorithm has access to exact moments, similarly for other intermediate results. For the robustness of Algorithm 3, we prove the following theorem. Theorem 7. Assume that the norms of x, \u03be, A are bounded by x \u2264 \u0393, \u03be \u2264 P 2 , A \u2264 P 1 , the covariance matrix and the weight matrix are robustly full rank: DISPLAYFORM0 Further assume that the augmented distinguishing matrix has smallest singular values \u03c3 min (M ) \u2265 \u03b1. For any small enough , for any \u03b4 < 1, given poly \u0393, P 1 , P 2 , d, 1/ , 1/\u03b3, 1/\u03b1, 1/\u03b2, 1/\u03b4 number of i.i.d. samples, let the output of Algorithm 3 beV ,\u1e90 \u22121 , we know with probability at least 1 \u2212 \u03b4, DISPLAYFORM1 for any input x. In order to prove the above Theorem, we need to show that each step of Algorithm 3 is robust. We can divide Algorithm 3 into three steps: finding the span of vec * (z i z i )'s; finding z i 's from the span of vec * (z i z i )'s; recovering first layer using Algorithm 1. We will first state the key lemmas that prove every step is robust to noise, and finally combine them to show our main theorem. First, we show that with polynomial number of samples, we can approximate the span of vec * (z i z i )'s in arbitrary accuracy. LetT be the empirical estimate of T , which is the pure neuron detector matrix as defined in Algorithm 3. As shown in Lemma 10, the null space of T is exactly the span of vec * (z i z i )'s. We use standard matrix perturbation theory (see Section D.2) to show that the null space of T is robust to small perturbations. More precisely, in Lemma 11, we show that with polynomial number of samples, the span of k least singular vectors ofT is close to the null space of T .Lemma 11. Under the same assumptions as in Theorem 7, let S \u2208 R (k2+k)\u00d7k be the matrix whose k columns are the k least right singular vectors of T . Similarly define\u015c \u2208 R (k2+k)\u00d7k for empirical estimateT . Then for any \u2264 \u03b3/2, for any \u03b4 < 1, given O( DISPLAYFORM2 ) number of i.i.d. samples, we know with probability at least 1 \u2212 \u03b4, DISPLAYFORM3 The proof of the above lemma is in Section B.1. Basically, we need to lowerbound the spectral gap (k 2 -th singular value of T ) and to upperbound the Frobenius norm of T \u2212T . Standard matrix perturbation bound shows that if the perturbation is much smaller than the spectral gap, then the null space is preserved. Next, we show that we can robustly find z i 's from the span of vec * (z i z i )'s. Since this step of the algorithm is the same as the simultaneous diagonalization algorithm for tensor decompositions, we use the robustness of simultaneous diagonalization BID1 to show that we can find z i 's robustly. The detailed proof is in Section B.2. DISPLAYFORM4 , where \u03b6 1 and \u03b6 2 are two independent standard Gaussian vectors. Let z 1 , \u00b7 \u00b7 \u00b7 z k be the normalized row vectors of A \u22121 . Let z 1 , ...,\u1e91 k be the eigenvectors ofX\u0176 \u22121 (after sign flip). For any \u03b4 > 0 and small enough , with DISPLAYFORM5 , with probability at least 1 \u2212 \u03b4 over the randomness of \u03b6 1 , \u03b6 2 and i.i.d. samples, there exists a permutation \u03c0(i) \u2208 [k] such that DISPLAYFORM6 Finally, given\u1e91 i 's, the problem reduces to a one-layer problem. We will first give an analysis for Algorithm 1 as a warm-up. When we call Algorithm 1 from Algorithm 3, the situation is slightly different. Note we reserve fresh samples for this step, so that the samples used by Algorithm 1 are Published as a conference paper at ICLR 2019 still independent with the estimate\u1e91 i (learned using the other set of samples). However, since\u1e91 i is not equal to z i , this introduces an additional error term (\u1e91 i \u2212 z i ) y which is not independent of x and cannot be captured by \u03be. We modify the proof for Algorithm 1 to show that the algorithm is still robust as long as \u1e91 i \u2212 z i is small enough. Lemma 13. Assume that x \u2264 \u0393, A \u2264 P 1 , \u03be \u2264 P 2 and \u03c3 min (E[xx ]) \u2265 \u03b3. Suppose that for each 1 \u2264 i \u2264 k, \u1e91 i \u2212 z i \u2264 \u03c4 . Then for any \u2264 \u03b3/2 and \u03b4 < 1, given O( DISPLAYFORM7 ) number of samples for Algorithm 1, we know with probability at least 1 \u2212 \u03b4, DISPLAYFORM8 Combining the above three lemmas, we prove Theorem 7 in Section B.4. DISPLAYFORM9 We first prove that the step of finding the span of {vec * (z i z j )} is robust. The main idea is based on standard matrix perturbation bounds (see Section D.2). We first give a lowerbound on the k 2 -th singular value of T , giving a spectral gap between the smallest non-zero singular value and the null space. See the lemma below. The proof is given in Section B.1.1. Lemma 14. Suppose \u03c3 min (M ) \u2265 \u03b1, \u03c3 min (A) \u2265 \u03b2, we know that matrix T has rank k 2 and the k 2 -th singular value of T is lower bounded by \u03b1\u03b2 2 .Then we show that with enough samples the estimateT is close enough to T , so Wedin's Theorem (Lemma 25) implies the subspace found is also close to the true nullspace of T . The proof is deferred to Section B.1.2. Lemma 15. Assume that x \u2264 \u0393, A \u2264 P 1 , \u03be \u2264 P 2 and \u03c3 min (E[xx ]) \u2265 \u03b3 > 0, then for any DISPLAYFORM10 ) number of i.i.d. samples, we know T \u2212 T F \u2264 , with probability at least 1 \u2212 \u03b4. Finally we combine the above two lemmas and show that the span of the least k right singular vectors ofT is close to the null space of T .Lemma 11. Under the same assumptions as in Theorem 7, let S \u2208 R (k2+k)\u00d7k be the matrix whose k columns are the k least right singular vectors of T . Similarly define\u015c \u2208 R (k2+k)\u00d7k for empirical estimateT . Then for any \u2264 \u03b3/2, for any \u03b4 < 1, given O( DISPLAYFORM11 ) number of i.i.d. samples, we know with probability at least 1 \u2212 \u03b4, DISPLAYFORM12 Proof. According to Lemma 15, given O( DISPLAYFORM13 ) number of i.i.d. samples, we know with probability at least 1 \u2212 \u03b4, DISPLAYFORM14 According to Lemma 14, we know \u03c3 k2 (T ) \u2265 \u03b1\u03b2 2 . Then, due to Lemma 27, we have DISPLAYFORM15 Published as a conference paper at ICLR 2019 DISPLAYFORM0 Figure 5: Characterize T as the product of four matrices. In order to lowerbound the k 2 -th singular value of T , we first express T as the product of four simpler matrices, T = M BCF , as illustrated in Figure 5 . The definitions of these four matrices DISPLAYFORM0 will be introduced later as we explain their effects. From Lemma 9, we know that DISPLAYFORM1 for any symmetric k \u00d7 k matrix U . For convenience, we first use matrix F to transform vec * (U ) to vec(U ), which has k 2 dimensions. Matrix F is defined such that F vec * (U ) = vec(U ), for any k \u00d7 k symmetric matrix U . Note that this is very easy as we just need to duplicate all the non-diagonal entries. Second, we hope to get the coefficients (A U A) ij 's. Notice that DISPLAYFORM0 Since we only care about the elements of A U A at the ij-th position for 1 \u2264 i < j \u2264 k, we just pick corresponding rows of A \u2297 A to construct our matrix C, which has dimension k 2 \u00d7 k 2 .The first matrix M is the augmented distinguishing matrix (see Definition 1). In order to better understand the reason that we need matrix B, let's first re-write T vec * (U ) in the following way: DISPLAYFORM1 Thus, T vec * (U ) is just a linear combination of (M ij \u2212 m ij E[x \u2297 x])'s with coefficients equal to (A U A) ij ' s. We have already expressed coefficients (A U A) ij 's using CF vec * (U ). Now, we just need to use matrix B to transform the augmented distinguishing matrix M to a d 2 \u00d7 k 2 matrix, with each column equal to (M ij \u2212 m ij E[x \u2297 x]). In order to achieve this, the first k 2 rows of B is just the identity matrix I k2 , and the last row of DISPLAYFORM2 With above characterization of T , we are ready to show that the k 2 -th singular value of T is lower bounded. Lemma 14. Suppose \u03c3 min (M ) \u2265 \u03b1, \u03c3 min (A) \u2265 \u03b2, we know that matrix T has rank k 2 and the k 2 -th singular value of T is lower bounded by \u03b1\u03b2 2 .Proof. Since matrix C has dimension k 2 \u00d7 k 2 , it's clear that the rank of T is at most k 2 . We first prove that the rank of T is exactly k 2 .Since the first k 2 rows of B constitute the identity matrix I k2 , we know B is a full-column rank matrix with rank equal to k 2 . We also know that matrix M is a full column rank matrix with rank k 2 + 1. Thus, the product matrix M B is still a full-column rank matrix with rank k 2 . If we can prove that the product matrix CF has full-row rank equal to k 2 . It's clear that T = M BCF also has rank k 2 . Next, we prove that CF has full-row rank. Published as a conference paper at ICLR 2019 Since \u03c3 min (A) \u2265 \u03b2, we know A \u2297 A is full rank, and a subset of its rows C has full row rank. For the sake of contradiction, suppose that there exists non-zero vector a \u2208 R k2 , such that k2 l=1 a l (CF ) [l,:] DISPLAYFORM0 Since C consists of a subset of rows of A \u2297 A , we know C [l,ij] = C [l,ji] for any l and any i < j. Thus, k2 l=1 a l (CF ) [l,:] = 0 simply implies k2 l=1 a l C [l,:] = 0, which breaks the fact that C is full-row rank. Thus, the assumption is false and CF has full-row rank. Now, let's prove that the k 2 -th singular value of T is lower bounded. We first show that in the product characterization of T , the smallest singular value of each individual matrix is lower bounded. According to the assumption, we know the smallest singular value of M is lower bounded by \u03b1. Since the first k 2 rows of matrix B constitute a k 2 \u00d7 k 2 identity matrix, we know DISPLAYFORM1 where u is any k 2 -dimensional vector. Since \u03c3 min (A) \u2265 \u03b2, we know \u03c3 min (A \u2297 A ) \u2265 \u03b2 2 . According to the construction of C, we know C consists a subset of rows of A \u2297 A . Denote the indices of the row not picked as S. We have DISPLAYFORM2 where u has dimension k 2 and v has dimension k 2 .We lowerbound the smallest singular value of CF by showing that \u03c3 min (CF ) \u2265 \u03c3 min (C). For any unit vector u \u2208 R k2 , we know DISPLAYFORM3 ji for any i < j. We also know [u C] ij = [u C] ji for i < j. Thus, we know for any unit vector u, u CF \u2265 u C , which implies \u03c3 min (CF ) \u2265 \u03c3 min (C).Finally, since in the beginning we have proved that matrix T has rank k 2 , the k 2 -th singular value is exactly the smallest non-zero singular value of T . Denote the smallest non-zero singular of T as \u03c3 + min (T ), we have DISPLAYFORM4 where the first inequality holds because both M and B has full column rank. In this section, we prove that given polynomial number of samples, T \u2212 T F is small with high probability. We do this by standard matrix concentration inequalities. Note that our requirements on the norm of x is just for convenience, and the same proof works as long as x has reasonable tail-behavior (e.g. sub-Gaussian). Lemma 15. Assume that x \u2264 \u0393, A \u2264 P 1 , \u03be \u2264 P 2 and \u03c3 min (E[xx ]) \u2265 \u03b3 > 0, then for any \u2264 \u03b3/2, for any 1 > \u03b4 > 0, given O( DISPLAYFORM0 ) number of i.i.d. samples, we know T \u2212 T F \u2264 , with probability at least 1 \u2212 \u03b4. Proof. In order to get an upper bound for T \u2212 T F , we first show that T \u2212 T 2 is upper bounded. We know DISPLAYFORM0 , according to the definition of T , we know DISPLAYFORM1 wheref (u) =T vec * (uu ) and the fourth inequality uses the Cauchy-Schwarz inequality. Next, we only need to upper bound max u: u \u22642 f (u) \u2212f (u) . Recall that DISPLAYFORM2 Notice that DISPLAYFORM3 We first show that given polynomial number of samples, DISPLAYFORM4 is upper bounded with high probability. Since each row of W has unit norm, we have W \u2264 \u221a k. Due to the assumption that x \u2264 \u0393, A \u2264 P 1 , \u03be \u2264 P 2 , we have DISPLAYFORM5 According to Lemma 24, we know given O( DISPLAYFORM0 2 ) number of samples, DISPLAYFORM1 with probability at least 1 \u2212 \u03b4. Similarly, we can show that given O( DISPLAYFORM2 2 ) number of samples, DISPLAYFORM3 with probability at least 1 \u2212 \u03b4. Since xx \u2264 \u0393 2 , we know that given O( DISPLAYFORM4 2 ) number of samples, DISPLAYFORM5 with probability at least 1 \u2212 \u03b4. Suppose that \u2264 \u03b3/2 \u2264 \u03c3 min (E[xx ])/2, we know\u00ca[xx ] has full rank. According to Lemma 29, we have DISPLAYFORM6 with probability at least 1 \u2212 \u03b4. By union bound, we know for any < \u03b3/2, given O( DISPLAYFORM7 2 ) number of samples, with probability at least 1 \u2212 \u03b4, we have DISPLAYFORM8 Then, we have DISPLAYFORM9 Thus, given O( DISPLAYFORM10 ) number of samples, we know DISPLAYFORM11 Published as a conference paper at ICLR 2019 with probability at least 1 \u2212 \u03b4. 2 , according to Lemma 24, we know given DISPLAYFORM0 with probability at least 1 \u2212 \u03b4. Next, let's look at the third term DISPLAYFORM1 Again, using Lemma 24 and union bound, we know given O( DISPLAYFORM2 ) number of samples, we have DISPLAYFORM3 Thus, Define DISPLAYFORM4 Then, we have DISPLAYFORM5 Thus, we know that given O( DISPLAYFORM6 ) number of samples, we know DISPLAYFORM7 with probability at least 1 \u2212 \u03b4. Now, let's bound the last term, DISPLAYFORM8 Similar as the first term, we can show that given O( DISPLAYFORM9 ) number of samples, we have DISPLAYFORM10 with probability at least 1 \u2212 \u03b4. Now, we are ready to combine our bound for each of four terms. By union bound, we know given DISPLAYFORM11 ) number of samples, DISPLAYFORM12 Published as a conference paper at ICLR 2019hold with probability at least 1 \u2212 \u03b4. Thus, we know DISPLAYFORM13 with probability at least 1 \u2212 \u03b4. where the second inequality holds since DISPLAYFORM0 Thus, we know given O( DISPLAYFORM1 ) number of samples, DISPLAYFORM2 with probability at least 1 \u2212 \u03b4. Thus, given O( DISPLAYFORM3 ) number of samples, T \u2212 T F \u2264 with probability at least 1 \u2212 \u03b4. In this section, we will show that the simultaneous diagonalization step in our algorithm is robust. Let S and\u015c be two (k 2 + k) by k matrices, whose columns consist of the least k right singular vectors of T andT respectively. According to Lemma 11, we know with polynomial number of samples, the Frobenius norm of SS \u2212\u015c\u015c \u22a5 is well bounded. However, due to the rotation issue of subspace basis, we cannot conclude that S \u2212\u015c F is small. Only after appropriate alignment, the difference between S and\u015c becomes small. Lemma 16. Let S and\u015c be two (k 2 + k) by k matrices, whose columns consist of the least k right singular vectors of T andT respectively. If SS \u2212\u015c\u015c F \u2264 , there exists an rotation matrix R \u2208 R k\u00d7k satisfying RR = R R = I k , such that DISPLAYFORM0 Proof. Since S has orthonormal columns, we have \u03c3 k (SS ) = 1. Then, according to Lemma 35, we know there exists rotation matrix R such that DISPLAYFORM1 Let the k columns of S be vec DISPLAYFORM2 , where D i is a diagonal matrix. Let Q be a k \u00d7 k matrix, whose i-th column consists of the diagonal elements of D i , such that Q ij equals the j-th diagonal element of D i . Let vec * (X) = SR\u03b6 1 , vec * (Y ) = SR\u03b6 2 , where R is the rotation matrix in Lemma 16 and \u03b6 1 , \u03b6 2 are two independent standard Gaussian vectors. Let D X = diag(QR\u03b6 1 ) and D Y = diag(QR\u03b6 2 ). It's not hard to check that DISPLAYFORM3 Y are well separated. Lemma 17. Assume that A \u2264 P 1 , \u03c3 min (A) \u2265 \u03b2. Then for any \u03b4 > 0 we know with probability at least 1 \u2212 \u03b4, we have sep( DISPLAYFORM4 Proof. We first show that matrix Q is well-conditioned. Since DISPLAYFORM0 . Let U be a k 2 \u00d7k matrix whose columns consist of vec(U i )'s. Also defineQ as a k 2 \u00d7 k matrix whose columns are vec(D i )'s. Note that matrixQ only has k non-zero rows, which are exactly matrix Q. With the above definition, we have DISPLAYFORM1 Notice that a subset of rows of U constitute matrix S, which is an orthonormal matrix. Thus, we have \u03c3 min (U ) \u2265 \u03c3 min (S) = 1. Since we assume \u03c3 min (A) \u2265 \u03b2, we have DISPLAYFORM2 Thus, we have \u03c3 min (Q) \u2265 \u03b2 2 , which implies \u03c3 min (Q) \u2265 \u03b2 2 .We also know DISPLAYFORM3 .Since S = 1, we know U \u2264 \u221a 2. For the smallest singular value of A \u2212 \u2297 A \u2212 , we have DISPLAYFORM4 Thus, we have Q \u2264 \u221a 2P DISPLAYFORM5 By properties of Gaussians, we know q \u22a5 i,j , R\u03b6 1 is independent of q j , R\u03b6 1 , so we can first fix q j , R\u03b6 1 and apply anti-concentration of Gaussians (see Lemma 38) to q \u22a5 i,j , R\u03b6 1 . As a result we know with probability at least 1 \u2212 \u03b4/k 2 : DISPLAYFORM6 By union bound, we know with probability at least 1 \u2212 \u03b4, DISPLAYFORM7 LetX =\u015c\u03b6 1 and\u0176 =\u015c\u03b6 2 . Next, we prove that the eigenvectors ofX\u0176 \u22121 are close to the eigenvectors of XY \u22121 .Published as a conference paper at ICLR 2019 DISPLAYFORM8 , where \u03b6 1 and \u03b6 2 are two independent standard Gaussian vectors. Let z 1 , \u00b7 \u00b7 \u00b7 z k be the normalized row vectors of A \u22121 . Let z 1 , ...,\u1e91 k be the eigenvectors ofX\u0176 \u22121 (after sign flip). For any \u03b4 > 0 and small enough , with DISPLAYFORM9 , with probability at least 1 \u2212 \u03b4 over the randomness of \u03b6 1 , \u03b6 2 and i.i.d. samples, there exists a permutation DISPLAYFORM10 Proof. Let z 1 , \u00b7 \u00b7 \u00b7 , z k be the eigenvectors of XY \u22121 (before sign flip step). Similarly defin\u00ea z 1 , \u00b7 \u00b7 \u00b7 ,\u1e91 k forX\u0176 \u22121 . We first prove that the eigenvectors ofX\u0176 \u22121 are close to the eigenvectors of XY \u22121 . DISPLAYFORM11 where DISPLAYFORM12 According to Lemma 34, we have DISPLAYFORM13 . In order to bound the perturbation matrices F and G , we need to first bound E X , E Y and \u03c3 min (Y ), \u03c3 min (\u0176 ).As we know, E X =X \u2212 X = (\u015c \u2212 SR)\u03b6 1 . According to Lemma 16, we have \u015c \u2212 SR \u2264 \u015c \u2212 SR F \u2264 2 . We also know with probability at least 1 DISPLAYFORM14 Similarly, with probability at least 1 DISPLAYFORM15 Now, we lower bound the smallest singular value of X and Y . Since DISPLAYFORM16 Since D X is a diagonal matrix, its smallest singular value equals the smallest absolute value of its diagonal element. Recall each diagonal element of D X is q i , R\u03b6 1 , which follows a Gaussian distribution whose standard deviation is at least q i \u2265 \u03b2 2 . By anti-concentration property of Gaussian (see Lemma 38), we know | q i , R\u03b6 2 | \u2265 \u2126(\u03b4\u03b2 2 /k) for all i with probability 1 \u2212 \u03b4/4. Thus, we have \u03c3 min (X) \u2265 poly(1/d, 1/P 1 , \u03b2, \u03b4). Similarly we have the same conclusion for Y . DISPLAYFORM17 Thus, for small enough , we have F \u2264 poly(d, P 1 , 1/\u03b2, , 1/\u03b4) and G \u2264 poly(d, P 1 , 1/\u03b2, , 1/\u03b4). In order to apply Lemma 33, we also need to bound \u03ba(A \u2212 ) and DISPLAYFORM18 where the second inequality holds because \u03c3 min (Y ) \u2265 poly(1/d, 1/P 1 , \u03b2, \u03b4). Recall that X = mat * (SR\u03b6 1 ). It's not hard to verify that with probability at least 1 \u2212 exp(\u2212d \u2126(1) ), we have X \u2264 poly(d). Thus, we know XY \u22121 \u2264 poly(d, P 1 , 1/\u03b2, 1/\u03b4). Similarly, we can also prove that DISPLAYFORM19 Published as a conference paper at ICLR 2019According to Lemma 17, we know with probability at least 1 \u2212 \u03b4/4, sep( DISPLAYFORM0 . Thus, by union bound, we know for small enough , with probability at least DISPLAYFORM1 DISPLAYFORM2 with probability at least 1 \u2212 \u03b4. According to Lemma 5, the eigenvectors of XY \u22121 (after sign flip) are exactly the normalized rows of A \u22121 (up to permutation). Now the only issue is the sign of\u1e91 i . By the robustness of sign flip step (see Lemma 18), we know for small enough , with O( DISPLAYFORM3 , with probability at least 1 \u2212 \u03b4, the sign flip of\u1e91 i is consistent with the sign flip of z i .In the following lemma, we show that the sign flip step of\u1e91 i is robust. DISPLAYFORM4 . We know, for any \u03b4 < 1, with O( DISPLAYFORM5 Proof. We first show that E[ z i , y ] is bounded away from zero. Let Z be a k \u00d7 k matrix, whose rows are {z i }. Without loss of generality, assume that Z = diag(\u00b1\u03bb)A \u22121 . Since A \u22121 \u2264 1/\u03b2, we know \u03bb i \u2265 \u03b2, for each i. Thus, we have DISPLAYFORM6 Published as a conference paper at ICLR 2019Note that we reserve fresh samples for this step, thus\u1e91 i is independent with samples in\u00ca[ \u1e91 i , y ]]. DISPLAYFORM7 ) number of samples, we have DISPLAYFORM8 . Combined with the fact that DISPLAYFORM9 , with O( DISPLAYFORM10 We will first show that Algorithm 1 is robust. DISPLAYFORM0 ) number of i.i.d. samples, we know with probability at least 1 \u2212 \u03b4, \u0175 \u2212 w \u2264 , where\u0175 is the learned weight vector. Proof. We first show that given polynomial number of i. DISPLAYFORM1 is upper bounded with high probability, where\u00ca[yx] is the empirical estimate of E [yx] . Due to the assumption that x \u2264 \u0393, w \u2264 1, \u03be \u2264 P 2 , we have DISPLAYFORM2 According to Lemma 24, we know given O( DISPLAYFORM3 2 ) number of samples, with probability at least 1 \u2212 \u03b4, we have DISPLAYFORM4 Since xx \u2264 \u0393 2 , we know that given O( DISPLAYFORM5 \u2264 with probability at least 1 \u2212 \u03b4. Suppose that \u2264 \u03b3/2 \u2264 \u03c3 min (E[xx ])/2, we know\u00ca[xx ] has full rank. According to Lemma 29, we have DISPLAYFORM6 with probability at least 1 \u2212 \u03b4. By union bound, we know for any \u2264 \u03b3/2, given O( DISPLAYFORM7 2 ) number of samples, with probability at least 1 \u2212 \u03b4, we have DISPLAYFORM8 . Published as a conference paper at ICLR 2019 Then, we have DISPLAYFORM0 Thus, given O( DISPLAYFORM1 ) number of samples, with probability at least 1 \u2212 \u03b4, we have \u0175 \u2212 w \u2264 . Now let's go back to the call to Algorithm 1 in Algorithm 3. Let z i 's be the normalized rows of A \u22121 , and let\u1e91 i 's be the eigenvectors ofX\u0176 \u22121 (with correct sign). From Lemma 12, we know {\u1e91 i } are close to {z i } with permutation. Without loss of generality, we assume the permutation here is just an identity mapping, which means z i \u2212\u1e91 i is small for each i. For each z i , let v i be the output of Algorithm 1 given infinite number of inputs (x, z i y). For eac\u0125 z i , letv i be the output of Algorithm 1 given only finite number of samples (x,\u1e91 i y). In this section, we show that suppose z i \u2212\u1e91 i is bounded, with polynomial number of samples, v i \u2212v i is also bounded. The input for Algorithm 1 is (x,\u1e91 i y). We view\u1e91 i y as the summation of z i y and a noise term (\u1e91 i \u2212 z i ) y. Here, the issue is that the noise term (\u1e91 i \u2212 z i ) y is not independent with the sample (x,\u1e91 i y), which makes the robust analysis in Theorem 8 not applicable. On the other hand, since we reserve a separate set of samples for Algorithm 1, the estimate\u1e91 i is independent with the samples (x, y)'s used by Algorithm 1. Thus, the samples (x,\u1e91 i y)'s here are still i.i.d., which enables us to use matrix concentration bounds to show the robustness here. Lemma 13. Assume that x \u2264 \u0393, A \u2264 P 1 , \u03be \u2264 P 2 and \u03c3 min (E[xx ]) \u2265 \u03b3. Suppose that for each 1 \u2264 i \u2264 k, \u1e91 i \u2212 z i \u2264 \u03c4 . Then for any \u2264 \u03b3/2 and \u03b4 < 1, given O( DISPLAYFORM2 ) number of samples for Algorithm 1, we know with probability at least 1 \u2212 \u03b4, DISPLAYFORM3 [\u1e91 i yx]. Thus, in order to bound v i \u2212v i , we only need to show DISPLAYFORM4 are both bounded. The first term can be bounded as follows. DISPLAYFORM5 We can use standard matrix concentration bounds to upper bound the second term. By similar analysis of Theorem 1, we know given O( DISPLAYFORM6 ) number of i.i.d. samples, with probability at least 1 \u2212 \u03b4, DISPLAYFORM7 Published as a conference paper at ICLR 2019 Overall, we have DISPLAYFORM0 By union bound, we know given O( DISPLAYFORM1 ) number of i.i.d. samples, with probability at least 1 \u2212 \u03b4, DISPLAYFORM2 for any 1 \u2264 i \u2264 k. Proof of Theorem 7. Combining Lemma 11, Lemma 12 and Lemma 13, we know given poly \u0393, P 1 , P 2 , d, 1/ , 1/\u03b3, 1/\u03b1, 1/\u03b2, 1/\u03b4 number of i.i.d. samples, with probability at least 1 \u2212 \u03b4, DISPLAYFORM0 Let V be a k \u00d7 d matrix whose rows are v i 's. Similarly define matrixV forv i ' s. Since v i \u2212v i \u2264 for any i, we know every row vector of V \u2212V has norm at most , which implies V \u2212V \u2264 \u221a k .Let Z be a k \u00d7 k matrix whose rows are z i ' s. Similarly define matrix\u1e90 for\u1e91 i ' s. Again, we have Z \u2212\u1e90 \u2264 \u221a k . In order to show Z \u22121 \u2212\u1e90 \u22121 is small using standard matrix perturbation bounds (Lemma 29), we need to lower bound \u03c3 min (Z). Notice that Z is just matrix A \u22121 with normalized row vectors. As we know, \u03c3 min (A \u22121 ) \u2265 1/P 1 , and A \u22121 \u2264 1/\u03b2, which implies that every row vector of A \u22121 has norm at most 1/\u03b2. Let D z be the diagonal matrix whose i, i-th entry is the norm of i-th row of DISPLAYFORM1 Then, according to Lemma 29, as long as \u2264 \u03b2 2P1 , we have DISPLAYFORM2 In order to bound V , we can bound the norm of its row vectors. We have, DISPLAYFORM3 . Now we can bound Z \u22121 \u03c3(V x)\u2212\u1e90 \u22121 \u03c3(V x) as follows. DISPLAYFORM4 where the first inequality holds since DISPLAYFORM5 Thus, we know given poly \u0393, P 1 , P 2 , d, 1/ , 1/\u03b3, 1/\u03b1, 1/\u03b2, 1/\u03b4 number of i.i.d. samples, with probability at least 1 \u2212 \u03b4, DISPLAYFORM6 where the first equality holds because A\u03c3(W x) = Z \u22121 \u03c3(V x), as shown in Theorem 5. In smoothed analysis, it's clear that after adding small Gaussian perturbations, matrix A and W will become robustly full rank with reasonable probability (Lemma 36). In this section, we will focus on the tricky part, using smoothed analysis framework to show that it is natural to assume the distinguishing matrix is robustly full rank. We will consider two settings. In the first case, the input distribution is the Gaussian distribution N (0, I d ), and the weights for the first layer matrix W is perturbed by a small Gaussian noise. In this case we show that the augmented distinguishing matrix M has smallest singular value \u03c3 min (M ) that depends polynomially on the dimension and the amount of perturbation. This shows that for the Gaussian input distribution, \u03c3 min (M ) is lower bounded as long as W is in general position. In the second case, we will fix a full rank weight matrix W , and consider an arbitrary symmetric input distribution D. There is no standard way of perturbing a symmetric distribution, we give a simple perturbation D that can be arbitrarily close to D, and prove that \u03c3 min (M D ) is lowerbounded. Perturbing W for Gaussian Input We first consider the case when the input follows standard Gaussian distribution N (0, I d ). The weight matrix W is perturbed to W where DISPLAYFORM0 Here E \u2208 R k\u00d7d is a random matrix whose entries are i.i.d. standard Gaussians. We will use M to denote the perturbed version of the augmented distinguishing matrix M . Recall that the columns of M has the form: DISPLAYFORM1 where w i is the i-th row of W . Also, since M is the augmented distinguishing matrix it has a final column M 0 = vec(I d ). We show that the smallest singular value of M is lower bounded with high probability. Theorem 9. Suppose that k \u2264 d/5, and the input follows standard Gaussian distribution N (0, I d ).Given any weight matrix W with w i \u2264 \u03c4 for each row vector, let W be a perturbed version of W according to Equation (18) and M be the perturbed augmented distinguishing matrix. With probability at least 1 \u2212 exp(\u2212d \u2126(1) ), we have DISPLAYFORM2 We will prove this Theorem in Section C.1.Perturbing the Input Distribution Our algorithm works for a general symmetric input distribution D. However, we cannot hope to get a result like Theorem 9 for every symmetric input distribution D. As a simple example, if D is just concentrated on 0, then we do not get any information about weights and the problem is highly degenerate. Therefore, we must specify a way to perturb the input distribution. We define a perturbation that is parametrized by a random Gaussian matrix Q and a parameter \u03bb \u2208 (0, 1). The random matrix Q is used to generate a Gaussian distribution D Q with a random covariance matrix. To sample a point in D Q , first sample n \u223c N (0, I d ), and then output Qn. We show that given any input distribution, after applying (Q, \u03bb)-perturbation with a random Gaussian matrix Q, the smallest singular value of the augmented distinguishing matrix M D is lower bounded. Recall that M D is defined as DISPLAYFORM3 as the first k 2 columns and has E x\u223cD [x \u2297 x] as the last column. Theorem 10. Given weight matrix W with w i \u2264 \u03c4 for each row vector and symmetric input distribution D. Suppose that k \u2264 d/7 and \u03c3 min (W ) \u2265 \u03c1, after applying (Q, \u03bb)-perturbations to yield perturbed input distribution D , where Q is a d \u00d7 d matrix whose entries are i.i.d. Gaussians, we have with probability at least 1 \u2212 exp(\u2212d\u2126 FORMULA0 ) over the randomness of Q, DISPLAYFORM4 We will prove this later in Section C.3. In this section, we will prove Theorem 9, as restated below: Theorem 9. Suppose that k \u2264 d/5, and the input follows standard Gaussian distribution N (0, I d ).Given any weight matrix W with w i \u2264 \u03c4 for each row vector, let W be a perturbed version of W according to Equation FORMULA0 and M be the perturbed augmented distinguishing matrix. With probability at least 1 \u2212 exp(\u2212d \u2126(1) ), we have DISPLAYFORM0 To prove this theorem, recall the definition of M ij : DISPLAYFORM1 Since Gaussian distribution is highly symmetric, for every direction u that is orthogonal to both w i and w j , we have u mat(M ij )u be a constant. We can compute this constant as DISPLAYFORM2 This implies that if we consider mat(M ij ) \u2212 m ij I d , it is going to be a matrix whose rows and columns are in span of w i and w j . In fact we can compute the matrix explicitly as the following lemma: Lemma 19. Suppose input x follows standard Gaussian distribution N (0, I d ), and suppose weight matrix W has full-row rank, then for any 1 \u2264 i < j \u2264 k, we have DISPLAYFORM3 where 0 < \u03c6 ij < \u03c0 is the angle between weight vectors w i and w j .Of course, the same lemma would be applicable to W , so we have an explicit formula for M ij . We will bound the smallest singular value using the idea of leave-one-out distance (as previously used in Rudelson & Vershynin (2009) ).Leave-one-out Distance Leave-one-out distance is a metric that is closely related to the smallest singular value but often much easier to estimate. Definition 2. For a matrix A \u2208 R d\u00d7n (d \u2265 n), the leave-one-out distance d(A) is defined to be the smallest distance between a column of A to the span of other columns. More precisely, let A i be the i-th column of A and S \u2212i be the span of all the columns except for A i , then DISPLAYFORM4 Published as a conference paper at ICLR 2019 Rudelson & Vershynin (2009) showed that one can lowerbound the smallest singular value of a matrix by its leave-one-out distance. Lemma 20 (Rudelson & Vershynin (2009) DISPLAYFORM5 Therefore, to bound \u03c3 min ( M ) we just need to lowerbound d( M ). We use the ideas similar to BID1 and Ma et al. (2016) . Since every column of M (except for M 0 ) is random, we will try to show that even if we condition on all the other columns, because of the randomness in M ij , the distance between M ij to the span of other columns is large. However, there are several obstacles in this approach:1. The augmented distinguishing matrix M has a special column M 0 = vec(I d ) that does not have any randomness.2. The closed form expression for M (as in Lemma 19) has complicated coefficients that are not linear in the vectors w i and w j .3. The columns of M ij are not independent with each other, so if we condition on all the other columns, M ij is no longer random. To address the first obstacle, we will prove a stronger version of Lemma 20 that allows a special column. Lemma 21. Let A \u2208 R d\u00d7(n+1) (d \u2265 n + 1) be an arbitrary matrix whose columns are A 0 , A 1 , ..., A n . For any i = 1, 2, ..., n, let S \u2212i be the subspace spanned by all the other columns (including A 0 ) except for A i , and let d (A) := min i=1,...,n (I d \u2212Proj S\u2212i )A i . Suppose the column A 0 has norm \u221a d and A 1 , ..., A n has norm at most C, then DISPLAYFORM6 This lemma shows that if we can bound the leave-one-out distance for all but one column, then the smallest singular value of the matrix is still lowerbounded as long as the columns do not have very different norms. We defer the proof to Section C.2.For the second obstacle, we show that these coefficients are lowerbounded with high probability. Therefore we can condition on the event that all the coefficients are large enough. Lemma 22. Given weight vectors w i and w j with norm w i , w j \u2264 \u03c4 , let w i = w i + \u03c1\u03b5 i , w j = w j +\u03c1\u03b5 j where \u03b5 i , \u03b5 j are i.i.d. Gaussian random vectors. With probability at least 1\u2212exp(\u2212d \u2126(1) ), we know w i \u2264 \u03c4 + 3\u03c1 2 d/2, w j \u2264 \u03c4 + 3\u03c1 2 d/2 and DISPLAYFORM7 where \u03c6 ij is the angle between w i and w j . In particular, if W = W + \u03c1E where E is an i.i.d. Gaussian random matrix, with probability at least 1\u2212exp(\u2212d \u2126(1) ), for all i, w i \u2264 \u03c4 + 3\u03c1 2 d/2, and for all i < j, the coefficient \u03c6 ij /\u03c0 in front of the term w i w j + w j w i is at least DISPLAYFORM8 This lemma intuitively says that after the perturbation w i and w j cannot be close to co-linear. We defer the detailed proof to Section C.2.For the final obstacle, we use ideas very similar to Ma et al. (2016) which decouples the randomness of the columns. Proof of Theorem 9. Let E 1 be the event that Lemma 22 does not hold. Event E 1 will be one of the bad events (but note that we do not condition on E 1 not happening, we use a union bound at the end).Published as a conference paper at ICLR 2019 DISPLAYFORM9 That is, the columns of M are DISPLAYFORM10 for i < j, where w i,L denotes the restriction of vector w i to the subset L. Note that the restriction of vec(I d ) to the rows indexed by L 1 \u00d7 L 2 is just an all zero vector. We will focus on a column M ij with i < j and try to prove it has a large distance to the span of all the other columns. Let V ij be the span of all other columns, which is equal to DISPLAYFORM11 It's clear that V ij is correlated with M ij , which is bad for the proof. To get around this problem, we follow the idea of Ma et al. (2016) and define the following subspace that contains V ij , DISPLAYFORM12 By definition V ij \u2282V ij , and thusV DISPLAYFORM13 Note that w i,L1 \u2297 w j,L2 is independent withV ij . Moreover, subspaceV ij has dimension at most DISPLAYFORM14 . Then by Lemma 31, we know that with probability at least 1 DISPLAYFORM15 Let E 2 be the event that this inequality does not hold for some i, j. DISPLAYFORM16 }. Now we know when neither bad events E 1 or E 2 happens, for every pair i < j, DISPLAYFORM17 Currently, we have proved that for any i < j, the distance between column M ij and the span of other columns is at least inverse polynomial. To use Lemma 21 we just need to give a bound on the norms of these columns. By Lemma 22, we know when E 1 does not happen DISPLAYFORM18 where \u03c4 is the uniform upper bound of the norm of every row vector of W . Let \u03c4 = \u03c4 + DISPLAYFORM19 Published as a conference paper at ICLR 2019 Thus, we have DISPLAYFORM20 Thus, there exists C = poly(\u03c4, d, \u03c1), such that M ij \u2264 C for every i < j. Now applying Lemma 21 immediately gives the result. C.2 PROOF OF AUXILIARY LEMMAS FOR SECTION C.1We will first prove the characterization for columns in the augmented distinguishing matrix. Proof of Lemma 19. For simplicity, we start by assuming that every weight vector w i has unit norm. At the end of the proof we will discuss how to incorporate the norms of w i , w j . Also throughout the proof we will abuse notation to use M ij as its matrix form mat(M ij ).Let S ij be the subspace spanned by w i and w j . Let S \u22a5 ij be the orthogonal subspace of S ij . Let {e DISPLAYFORM21 } be a set of orthonormal basis for S ij such that e , w j > 0. We use matrix S ij \u2208 R d\u00d72 to represent subspace S ij , which matrix has e Let Proj Sij = S ij S ij , and Proj DISPLAYFORM22 which is equivalent to proving that Proj DISPLAYFORM23 DISPLAYFORM24 where the last equality holds since u \u2208 S \u22a5 ij is orthogonal to e . We also know that DISPLAYFORM25 where the third equality holds because u x is independent with w i x, w j x and v . Note since u is orthogonal with w i , w j , v, we know for standard Gaussian vector x, random variable u x is independent with w i x, w j x, v x. Published as a conference paper at ICLR 2019Since the column span and row span of Proj S \u22a5 ij M ij both belong to the subspace S \u22a5 ij , there must exist DISPLAYFORM26 . We only need to show this matrix C must be m ij I d\u22122 . In order to show this, we prove for any u, v \u2208 S DISPLAYFORM27 where the fourth equality holds because u x, v x are independent with w i x, w j x. Thus, we know DISPLAYFORM28 Let's now compute the closed form for m ij . Recall that DISPLAYFORM29 Note, we only need to consider input x within subspace S ij , which subspace has dimension two. Using the polar representation of two-dimensional Gaussian random variables (r is the radius and \u03b8 is the angle), we have ) , which means DISPLAYFORM30 DISPLAYFORM31 where c DISPLAYFORM32 and c DISPLAYFORM33 are four coefficients. Now, we only need to figure out the four coefficients of this linear combination. Similar as the computation for m ij , we use polar integration to show that, DISPLAYFORM34 where the first equality holds because e . Similarly, we can show that DISPLAYFORM35 Published as a conference paper at ICLR 2019Proof of Lemma 21. The smallest singular value of A can be defined as follows: DISPLAYFORM36 Au .Suppose u * \u2208 argmin u: u =1 Au . Let u * i be the coordinate corresponding to the column A i , for 0 \u2264 i \u2264 n. We consider two cases here. If |u * 0 | \u2265 4nC 2 4nC 2 +d , then we have DISPLAYFORM37 where the third inequality uses Cauchy-Schwarz inequality. DISPLAYFORM38 Above all, we know that the smallest singular value of A is lower bounded as follows, DISPLAYFORM39 Next we give the bound on the angle between two perturbed vectors w i and w j .Proof of Lemma 22. According to the definition of \u03c1-perturbation, we know w i = w i + \u03c1\u03b5 i , w j = w j + \u03c1\u03b5 j , where \u03b5 i , \u03b5 j are i.i.d. standard Gaussian vectors. First, we show that with high probability, the projection of w i on the orthogonal subspace of w j is lower bounded. Denote the subspace spanned by w j as S wj , and denote the subspace spanned by { w j , w i } as S wj \u222awi . Thus, we have DISPLAYFORM40 where S \u22a5 wj is the orthogonal subspace of S wj . Published as a conference paper at ICLR 2019 DISPLAYFORM0 matrix, whose columns constitute a set of orthonormal basis for the subspace S \u22a5 wj \u222awi . Thus, it's not hard to check that Proj S \u22a5 w j \u222aw i DISPLAYFORM1 , which is a chi-squared random variable with (d \u2212 2) degrees of freedom. According to the tail bound for chi-squared random variable, we have DISPLAYFORM2 , \u2200t \u2208 (0, 1).Let t = 1 2 , we know that with probability at least 1 \u2212 2 exp( DISPLAYFORM3 Thus, we have Proj S \u22a5 w j DISPLAYFORM4 . Recall that DISPLAYFORM5 We also know DISPLAYFORM6 where the last equality holds since w i \u2264 \u03c4 . Note \u03b5 i 2 is another chi-squared random variable with d degrees of freedom. Similar as above, we can show that with probability at least 1 \u2212 2 exp( DISPLAYFORM7 By union bound, we know with probability at least 1 \u2212 2 exp( DISPLAYFORM8 Combined with the fact that \u03c6 ij \u2265 sin( \u03c6 ij ) when \u03c6 ij \u2208 [0, \u03c0], we know with probability at least 1 \u2212 2 exp( DISPLAYFORM9 Given W = W + \u03c1E, where E is an i.i.d. Gaussian matrix, by union bound, we know with probability at least 1 DISPLAYFORM10 Published as a conference paper at ICLR 2019 In this section, we show that starting from any well-conditioned weight matrix W , and any symmetric input distribution D, how to perturb the distribution locally to D so that the smallest singular value of M D is at least inverse polynomial. Recall the definition of (Q, \u03bb)-perturbation: we mix the original distribution D with a distribution D Q which is just a Gaussian N (0, QQ ). To create a sample x in D , with probability 1 \u2212 \u03bb we draw a sample from D; otherwise we draw a standard Gaussian n \u223c N (0, I d ) and let x = Qn. We will prove Theorem 10 which we restate below:Theorem 10. Given weight matrix W with w i \u2264 \u03c4 for each row vector and symmetric input distribution D. Suppose that k \u2264 d/7 and \u03c3 min (W ) \u2265 \u03c1, after applying (Q, \u03bb)-perturbations to yield perturbed input distribution D , where Q is a d \u00d7 d matrix whose entries are i.i.d. Gaussians, we have with probability at least 1 \u2212 exp(\u2212d \u2126(1) ) over the randomness of Q, DISPLAYFORM0 To prove this, let us first take a look at the structure of augmented distinguishing matrix for these DISPLAYFORM1 DISPLAYFORM2 Our proof will go in two steps. First we will show that \u03c3 min (M D Q ) is large. Then we will show that even mixing with M D will not significantly reduce the smallest singular value, so \u03c3 min (M D ) is also large. In addition to the techniques that we developed in Section C.1, we need two ideas that we call noise domination and subspace decoupling to solve the new challenges here. Noise Domination First let us focus on \u03c3 min (M D Q ). This instance has weight W and input distribution N (0, QQ ). Let M W Q be the augmented distinguishing matrix for an instance with weight W Q and input distribution N (0, I d ). Our first observation shows that M D Q and M W Q are closely related, and we only need to analyze the smallest singular value of M W Q . The problem now is very similar to what we did in Theorem 9, except that the weight W Q is not an i.i.d. Gaussian matrix. However, we will still be able to use Theorem 9 as a black-box because the amount of noise in W Q is in some sense dominating the noise in a standard Gaussian. More precisely, we use the following simple claim: Claim 1. Suppose property P holds for N (\u00b5, I d ) for any \u00b5, and the property P is convex (in the sense that if P holds for two distributions it also holds for their mixture), then for any covariance matrix \u03a3 I d , we know P also holds for N (\u00b5, \u03a3).Intuitively the claim says that if the property holds for a Gaussian distribution with smaller variance regardless of the mean, then it will also hold for a Gaussian distribution with larger variance. The proof is quite simple:Proof. Let \u03a3 = \u03a3 \u2212 I d , by assumption we know \u03a3 is still a positive semidefinite matrix. Let x \u223c N (\u00b5, \u03a3), x \u223c N (\u00b5, \u03a3 ) and \u03b4 \u223c N (0, I d ), by property of Gaussians it is easy to see that That is, N (\u00b5, \u03a3) is a mixture of N (x , I). Since property P is true for all N (x , I), it is also true for N (\u00b5, \u03a3).With this claim we can immediately use the result of Theorem 9 to show \u03c3 min (M D Q ) is large. Published as a conference paper at ICLR 2019Subspace Decoupling Next we need to consider the mixture M D . The worry here is that although \u03c3 min (M D Q ) is large, mixing with D might introduce some cancellations and make \u03c3 min (M D ) much smaller. To prove that this cannot happen with high probability, the key observation is that in the first step, to prove \u03c3 min (M W Q ) is large we have only used the property of W Q. If we letQ be the projection of Q to the orthogonal space of row span of W , thenQ is still a Gaussian random matrix even if we condition on the value of W Q! Therefore in the second step we will use the additional randomness inQ to show that the cancellation cannot happen. The idea of partitioning the randomness of Gaussian matrices has been widely used in analysis of approximate message passing algorithms. The actual proof is more involved and we will need to partition the Gaussian matrix Q into more parts in order to handle the special column in the augmented distinguishing matrix . Now we are ready to give the full proof of Theorem 10Proof of Theorem 10. Let us first recall the definition of augmented distinguishing matrix: M D is a d 2 by (k 2 + 1) matrix, where the first k 2 columns consist of M D ij := E x\u223cD (w i x)(w j x)(x \u2297 x)1{w i xw j x \u2264 0} , and the last column is E x\u223cD [x \u2297 x] . According to the definition of (Q, \u03bb)-perturbation, if we let D Q be N (0, QQ ), then we have DISPLAYFORM3 In the first step, we will try to analyze M D Q . The first k 2 columns of this matrix M D Q can be written as: M D Q ij = E x\u223cD Q (w i x)(w j x)(x \u2297 x)1{w i xw j x \u2264 0} = E n\u223cN (0,I d ) (w i Qn)(w j Qn)(Qn \u2297 Qn)1{w i Qnw j Qn \u2264 0} = Q \u2297 QE n\u223cN (0,I d ) (w i Qn)(w j Qn)(n \u2297 n)1{w i Qnw j Qn \u2264 0} for any i < j, and the last column is DISPLAYFORM4 Except for the factor Q \u2297 Q, the remainder of these columns are exactly the same as the augmented distinguishing matrix of a network whose first layer weight matrix is W Q and input distribution is N (0, I d ). We use M W Q to denote the augmented distinguishing matrix of such a network, then we have DISPLAYFORM5 Therefore we can first analyze the smallest singular value of M W Q . Let W = W Q. Note that Q is a Gaussian matrix, and W is fixed, so W Q is also a Gaussian random matrix except its entries are not i.i.d. More precisely, there are only correlations within columns of W Q, and for any column of W Q, the covariance matrix is W W . Since the smallest singular value of W is at least \u03c1, we know \u03c3 min (W W ) \u2265 \u03c1 2 . Let the covariance matrix of W Q be \u03a3 W Q \u2208 R kd\u00d7kd , which has smallest singular value at least \u03c1 2 . Therefore we know \u03a3 W Q \u03c1 2 I kd . It's not hard to verify that with probability at least 1 \u2212 exp(\u2212d \u2126(1) ), the norm of every row of W Q is upper bounded by poly(\u03c4, d). By Claim 1, any convex property that holds for any N (0, \u03c1 2 I kd ) perturbation must also hold for \u03a3 W Q 4 . Thus, we know with probability at least 1 \u2212 exp(\u2212d \u2126(1) ), \u03c3 min (M W Q ) \u2265 poly(1/\u03c4, 1/d, \u03c1).To prepare for the next step, we will rewrite M W Q as the product of two matrices. According to the closed form of M W Q ij in Lemma 19, we know each column of M W Q can be expressed as a linear combination of w i \u2297 w j 's and vec(I d ). Therefore: DISPLAYFORM6 Published as a conference paper at ICLR 2019Since R has full row rank, we know that the row span of Proj W \u22a5 \u2297W \u22a5 M D belongs to the row span of R. According to the definition of U , it's also clear that the column span of Proj W \u22a5 \u2297W \u22a5 M D belongs to the column span of U \u2297 U . Thus, there exists matrix C \u2208 R DISPLAYFORM7 Thus, DISPLAYFORM8 \u2265\u03c3 min (1 \u2212 \u03bb)Proj W \u22a5 \u2297W \u22a5 M D + \u03bbU \u2297 U P 2 V W \u2297 P 2 V W , vec(P 1 P 1 + P 2 P 2 ) R =\u03c3 min \u03bbU \u2297 U 1 \u2212 \u03bb \u03bb C + P 2 V W \u2297 P 2 V W , vec(P 1 P 1 + P 2 P 2 ) R .Note that C only depends on U and R, U only depends on W , and R only depends on W Q. With W Q fixed, C is also fixed. Clearly, C is independent with P 1 and P 2 . For convenience, denote H := 1 \u2212 \u03bb \u03bb C + P 2 V W \u2297 P 2 V W , vec(P 1 P 1 + P 2 P 2 ) . Now, let's prove that the smallest singular value of matrix H \u2208 R DISPLAYFORM9 is lower bounded using leave-one-out distance. Let's first consider its submatrix\u0124 which consists of the first k 2 columns of H. Note that within random matrix P 2 V W , every row are independent with each other. Within each row, the covariance matrix is W W . Recall that W is a random matrix whose covariance \u03a3 W Q \u03c1 2 I kd , we can again apply Claim 1 with the property proved in Lemma 37. As a result, with probability at least 1 \u2212 exp(\u2212d \u2126(1) ), DISPLAYFORM10 Thus the covariance matrix of each row of P 2 V W has smallest singular value at least \u03b3 := poly(1/d, \u03c1).We can view P 2 V W as the summation of two independent Gaussian matrix, one of which has covariance matrix \u03b3I (d\u2212k)k . For this matrix, we will do something very similar to Theorem 9 in order to lowerbound its smallest singular value. Claim 2. For a random matrix K \u2208 R (d\u2212k)\u00d7k that is equal to K o + E where E is a Gaussian random matrix whose entries have variance \u03b3. If d \u2265 7k, for any subspace S C that is independent of K and has dimension at most k 2 + 1, the leave-one-out distance d(Proj S \u22a5 C K \u2297 K) is at least poly(\u03b3, 1/d).The proof idea is similar as Theorem 9, and we try to apply Lemma 31 to K \u2297 K. In the proof we should think of K := P 2 V W , and denote i-th column of K as K i . We also think of the space S C as the column span of C.Since U is an orthonormal matrix, we know \u03c3 min (U \u2297 U ) = 1. According to Eq. 19, we know with probability at least 1 \u2212 exp(\u2212d \u2126(1) ), DISPLAYFORM11 \u2265\u03bb\u03c3 min (U \u2297 U )\u03c3 min (H)\u03c3 min (R) \u2265poly(1/\u03c4, 1/d, \u03c1, \u03bb)where the second inequality holds since all of U \u2297 U , H and R have full column rank. In this section, we collect some known results on matrix perturbations and concentration bounds. Basically, we used matrix concentration bounds to do the robust analysis and used matrix perturbation bounds to do the smoothed analysis. We also proved several corollaries that are useful in our setting. Matrix concentration bounds tell us that with enough number of independent samples, the empirical mean of a random matrix can converge to the mean of this matrix. Lemma 23 (Matrix Bernstein; Theorem 1.6 in Tropp FORMULA0 ). Consider a finite sequence {Z k } of independent, random matrices with dimension d 1 \u00d7 d 2 . Assume that each random matrix satisfies E[Z k ] = 0 and Z k \u2264 R almost surely. Define DISPLAYFORM0 Then, for all t \u2265 0, DISPLAYFORM1 \u03c3 2 + Rt/3 .As a corollary, we have: Lemma 24. Consider a finite sequence {Z 1 , Z 2 \u00b7 \u00b7 \u00b7 Z m } of independent, random matrices with dimension d 1 \u00d7 d 2 . Assume that each random matrix satisfies Z k \u2264 R, 1 \u2264 k \u2264 m. Then, for all t \u2265 0, Alignment of Subspace Basis. Due to the rotation issue, we cannot conclude that S \u2212\u015c is small even we know SS \u2212\u015c\u015c is bounded. The following Lemma shows that after appropriate alignment, S is indeed close to\u015c. Lemma 35 (Lemma 6 in Ge et al. (2017a) ). Given matrices S,\u015c \u2208 R d\u00d7r , we have . Let A \u2208 R m\u00d7n and suppose that m \u2265 n. Assume that the entries of A are independent standard Gaussian variable, then for every > 0, with probability at least 1 \u2212 (C ) m\u2212n+1 + e \u2212C n , where C, C are two absolute constants, we have: DISPLAYFORM2 DISPLAYFORM3 DISPLAYFORM4 However, in our setting, we are more interested in fixed matrices perturbed by Gaussian variables. The smallest singular value of these \"perturbed rectangular matrices\" can be bounded as follows. Lemma 37 (Lemma G.16 in Ge et al. (2015) ). Let A \u2208 R m\u00d7n and suppose that m \u2265 3n. If all the entries of A are independently \u03c1-perturbed to yield A, then for any > 0, with probability at least 1 \u2212 (C ) 0.25m , for some absolute constant C, the smallest singular value of A is bounded below by: DISPLAYFORM5"
}