{
    "title": "HygJq12VtH",
    "content": "We propose that approximate Bayesian algorithms should optimize a new criterion, directly derived from the loss, to calculate their approximate posterior which we refer to as pseudo-posterior. Unlike standard variational inference which optimizes a lower bound on the log marginal likelihood, the new algorithms can be analyzed to provide loss guarantees on the predictions with the pseudo-posterior. Our criterion can be used to derive new sparse Gaussian process algorithms that have error guarantees applicable to various likelihoods. Results in learning theory show that, under some general conditions, minimizing training set loss, also known as empirical risk minimization (ERM), provides good solutions in the sense that the true loss of such procedures is bounded relative to the best loss possible in hindsight. Alternative algorithms such as structural risk minimization or regularized loss minimization (RLM) have similar guarantees under more general conditions. On the other hand, Bayesian approaches are, in a sense, prescriptive. Given prior and data, we calculate a posterior distribution that compactly captures all our knowledge about the problem. Then, given a prediction task with an associated loss for wrong predictions, we pick the best prediction given our posterior. This is optimal when the model is correct and the exact posterior is tractable. However, the algorithmic choices are less clear with misspecified models or, even if the model is correct, when exact inference is not possible and the learning algorithm can only return an approximation to the posterior. Since the choices are often heuristically motivated we call such approximations pseudo-posteriors. The question is how the pseudo-posterior should be calculated. In this paper we propose to use learning theory to guide this process. To motivate our approach consider the variational approximation which is one of the most effective methods for approximate inference in Bayesian models. In lieu of finding the exact posterior, variational inference maximizes the ELBO, a lower bound on the marginal likelihood. It is well known that this can be seen alternatively as performing regularized loss minimization. For example, in a model with parameters w, prior p(w), and data y where p(y|w, x) = i p(y i |w, x i ), we have log p(y) \u2265 ELBO E where q(w) is the variational posterior and we have suppressed the dependence on x for visual clarity. Minimizing the negative ELBO, we have a loss term i E q(w) [\u2212 log p(y i |w, x i )] and a regularization term d KL (q(w), p(w)). The RLM viewpoint is attractive from the perspective of statistical learning theory because such algorithms are known to have good generalization guarantees (under some conditions). However, the ELBO objective is not matched to the intended use of Bayesian predictors: given a posterior q(w) and test example x * , the Bayesian predictor first calculates the predictive distribution p(y * |x * ) = E q(w) [p(y * |x * , w)] and then, assuming we are interested in the log loss, suffers the loss \u2212 log p(y * |x * ). In other words, seen from the perspective of learning theory, variational inference optimizes for , which is the loss of the Bayesian predictor. These observations immediately raise several questions: Should we design empirical risk minimization (ERM) algorithms minimizing L B that produce pseudo-posteriors? Should a regularization term, e.g., d KL , be added? Can we use standard analysis, that typically handles frequentist models, to provide guarantees for such algorithms? We emphasize that this differs from standard non-Bayesian algorithms that perform ERM or RLM to find the best parameter w. Here, we propose to perform ERM or RLM to find the best pseudoposterior q(w) as given by the parameters that define it. In this paper, we show that such an analysis can indeed be performed, and provide results which are generally applicable to Bayesian predictors optimized using ERM. Then, we focus on sparse Gaussian processes (sGP) for which we develop risk bounds for a smoothed variant of log loss 1 and any observation likelihood (the non-conjugate case). The significance of this is conceptual, in that it points to a different principle for designing approximate inference algorithms where we no longer aim to optimize the marginal likelihood (or ELBO), but instead a criterion that is directly related to the loss -this diverges from current practice in the literature. The paper highlights sparse GP because it is an important model with significant recent interest and work. But the approach and results are more generally applicable. To illustrate this point the appendix shows how the results can be applied to the Correlated Topic Model (CTM) of Blei and Lafferty (2006) . It is important to distinguish this work from two previous lines of work. Our earlier work (Sheth and Khardon, 2017) made similar observations w.r.t. the mismatch between the optimization criterion and the intended objective. However, the goal there was to analyze existing algorithms where possible. More concretely we showed that optimizing a criterion related to L G does have some risk guarantees, though these are weaker than the ones in this paper. Here, we propose to explore new algorithms based on direct loss minimization with stronger associated guarantees. In Alaoui and Mahoney (2015) and Burt et al. (2019) , the goal is to show that the sparse GP approximation can be chosen to be very close to the full GP solution. Conditions on the kernel functions and on the algorithm to select inducing input locations and variational distribution are given for this to be true. This is a very strong result showing that nothing is lost by using the sparse approximation. However, in many cases, the number of inducing inputs required is too large (e.g., for Matern kernels). In contrast, our analysis aims at identifying the best sGP posterior in terms of the resulting prediction performance, whether it is close to the full GP posterior or not. In other words, we seek an \"agnostic PAC guarantee\" for the sparse GP posterior. Due to space constraints, the main paper sketches the technical results with full details given in Appendices A to E. In short, three different approaches to proving agnostic PAC guarantees for learning with a Lipschitz loss under a bounded hypothesis space are provided. The three results use slightly different variants of ERM as the optimization algorithm. All three provide bounds if, in addition, the loss itself is bounded. Approach 1 (Appendix A) uses this directly and proves bounds using a standard discretization argument. Approach 2 (Appendix B) requires a bounded loss but adapts results based on Rademacher complexity (Meir and Zhang, 2003) to provide risk bounds that do not depend on the dimension of the hypothesis space and, in this way, potentially improves on approach 1. Approach 3 (Appendix C), which we present below, is new and has the potential to provide bounds with unbounded losses, although, for the application in this paper, we will be using bounded loss functions. We stress, though, that any of these approaches can be utilized to obtain guarantees under a Lipschitz loss and bounded hypothesis space. Appendices D and E develop the details for sGP and CTM. In the following, we consider a loss : \u0398 \u00d7 (X, Y ) \u2192 R over a hypothesis space \u0398 \u2282 R M and example/label spaces X and Y . We assume that the hypothesis space is closed and bounded w.r.t. infinity norm with sup \u03b8\u2208\u0398 ||\u03b8|| \u221e \u2264 B. We further assume that is L-Lipschitz in its first argument w.r.t. the same norm, i.e., \u2200\u03b8, ), U denotes the uniform distribution, and \u03bb > 0 is a scalar. The algorithm averages 2 the ERM objective of random neighbors of the solution\u03b8. We have: where 2. Given the other approaches described in the appendix, it is reasonable to consider this an artifact of the proof. In this case, ERM may be used directly. The proof (Appendix C) uses the compression lemma, \u03b8) , but applied to the variational parameters \u03b8 in contrast with Germain et al. (2016) and Sheth and Khardon (2017) that applied it on w. This new approach is the source of jitter in the randomized ERM objective. Specifically we apply the compression lemma with q(\u03b8) = q jit (\u03b8|\u03b8 ERM ) and . This bounds the potential overfitting, expressed by 1 \u03bb f (\u03b8), by a KL term that we can compute explicitly and log E p(\u03b8) e f (\u03b8) which results in \u03a8. If the loss is bounded, i.e., | | \u2264 c, then, \u03a8(\u03bb, n) \u2264 2\u03bb 2 c 2 n (see Germain et al. (2016) ; Sheth and Khardon (2017)) implying the following corollary showing that the expected risk of Randomized ERM is bounded by the risk of any posterior in \u0398 plus a term that decays at a rate of 1/ \u221a n. \u03a8(\u03bb, n) can be bounded under some conditions even if the loss is not bounded 3 but we leave further exploration of this for future work. Corollary 2 If the loss is bounded, i.e., | | \u2264 c, then using \u03bb = \u221a n we have In the (zero-mean) sparse GP model of Titsias (2009), w represents the latent function at the M inducing inputs Here, the pseudo-posterior is given by q(w|\u03b8) = N (w|m, C C) and the parameter space \u0398 includes both the mean and the Cholesky factor of the covariance of the pseudo-posterior, i.e., \u03b8 m vec(C) . Given q(w|\u03b8), the induced distribution q(f |\u03b8) w p(f |w)q(w)dw can be calculated exactly from Gaussian identities. Then, the log loss of the Bayesian prediction is ((m, C), U U K U * , and const signifies terms that do not depend on m or C. To apply Corollary 2, we require a bounded loss function which is also Lipschitz w.r.t. \u03b8. To enable this, we define a \"smoothed\" log loss. Assume for now that p \u2264 \u03be < \u221e. We use a smoothing parameter \u03b1 \u2208 (0, 1) and define nlog is Lipschitz w.r.t. \u03b8 and infinity norm yielding that nlog and \u03bb min (K U U ) denotes the minimum eigenvalue of K U U . Therefore, to apply Corollary 2 to any non-conjugate sparse GP model with smoothed log loss, all we need is to (i) verify that \u2203\u03be s.t. E q(f * |\u03b8) [p(y * |f * )] < \u03be and (ii) calculate bounds on d df * p(y * |f * ) and is easily achieved when Y is discrete, e.g., for binary classification and count regression. For standard regression, we can guarantee this by lower bounding the noise variance \u03c3 2 Y and upper bounding the range of X, Y . Bounds on the first and second derivatives (condition (ii)) are easily derived for the same likelihoods. Corollary 3 Randomized ERM using smoothed log loss with the sparse GP predictive distribution enjoys the bounds of Corollary 2 for regression, binary classification, Poisson regression. We have shown that ERM-type algorithms performing direct minimization of log loss have strong performance guarantees for the Bayesian predictor, and we applied these results to the non-conjugate sparse GP model under a smoothed log loss. However, in some scenarios, we may want to minimize a different loss function requiring an explicit prediction. In this case, given a posterior q(w) and example x with label y true , the Bayesian predictor first identifies the optimal prediction\u0177 =\u0177 q(w) (x) = arg min y\u2208Y E q(w)p(y |x,w) [ (y, y )] and then suffers the loss (q(w), (x, y true )) = (\u0177 q(w) (x), y true ). Therefore, the natural loss term for optimization is L B = i (\u0177 q(w) (x i ), y i ). We note that L G from the introduction, which implicitly uses the Gibbs log loss, is even less directly related to the learning goal in this case. On the other hand, the results of this paper do potentially apply to this more general setting as long as the conditions for the theorem hold. Our theory does not directly apply to the square loss (\u0177 \u2212 y) 2 because of the need for smoothing. However, it is interesting to consider the use of DLM for square loss and the resulting algorithms. In this case, sGP uses the standard regression model with Gaussian noise for prediction, that is, for calculating\u0177. It is well known that for the square loss the optimal predictor is the mean of the predictive distribution. As discussed above, for sGP the mean of the predictive distribution on example i is equal to a i m where Therefore the ERM algorithm will minimize i (a i m\u2212y i ) 2 and similarly for the randomized ERM. We therefore see that, if we do not use regularization, the optimization criterion does not depend on the covariance of w and the optimization simplifies into a sparse variant of kernel least squares. The role of the posterior covariance, might become apparent with regularization, which might also be helpful to reduce some of the conditions in our theorem. We leave the derivation of DLM algorithms and performance guarantees for other loss functions to future work. The paper points out the potential of DLM to yield a new type of approximate pseudoBayesian algorithm. In this paper we focused on the analysis of ERM and application to sparse GP. There are many important questions for future work including analysis for RLM, analysis for hyperparameter selection, removing the need for bounded or smoothed loss in our theorem, and investigating empirical properties of these algorithmic variants. This straightforward proof shows that having a Lipschitz condition and bounded loss are sufficiently strong to make the problem simple by essentially learning on a grid. We include it here in order to put the other proofs and their potential improvements in context. Let \u0398 \u2282 R M and || \u00b7 || denote the infinity norm. Recall that we assume a bounded loss for the application of the discretization approach, i.e., | | \u2264 c. Since \u0398 is assumed bounded, there exists a finite \u03c1-cover of \u0398,\u0398, i.e., \u2200\u03b8 \u2208 \u0398, \u2203\u03b8 \u2208\u0398 s.t. ||\u03b8 \u2212\u03b8|| \u2264 \u03c1. Let For an arbitrary \u03b8 \u2208 \u0398, let\u03b8 denote the closest point in\u0398 to \u03b8. Since the loss is assumed to be L-Lipschitz in the hypothesis parameter, we have that In addition, by combining the union bound and Hoeffding's bound for bounded loss | | \u2264 c we have that, with probability \u2265 1 \u2212 \u03b4 over the choice of sample S, for all \u03b8 \u2208\u0398: Let \u03b8 be any competitor for the posterior parameters. With probability \u2265 1 \u2212 \u03b4 we have \u2264 E (x,y)\u223cD (\u03b8, (x, y)) + 2c 2 log(2|\u0398|/\u03b4) n + 2L\u03c1 where (e) follows because ERM minimizes training set loss. With |\u0398| \u2264 2B \u03c1 M , the terms on the RHS of (8) depending on \u03c1 are given by 2c 2 log(2/\u03b4) + 2M log( The last expression is optimized when \u03c1 = 4cM 3 \u221a nL . Hence, we have that, with probability \u2265 1 \u2212 \u03b4 over the choice of S, \u2200\u03b8 \u2208 \u0398, Appendix B. Rademacher complexity In this section, we show how the result of Meir and Zhang (2003) can be adapted to handle Bayesian predictors. Meir and Zhang (2003) assume a set of parameterized predictors h(x; w) : X \u2192 Y and, in addition, assume that predictions can be averaged so that E q(w|\u03b8) [h(x; w)] is a meaningful prediction. One can then apply the loss (y, E q(w|\u03b8) [h(x; w)]). For Bayesian predictors, we average the probabilities inp = E q(w|\u03b8) [p(y|x, w)] but not the predictions themselves. Nonetheless, the same proof technique can be adapted to yield a result for some loss functions, specifically the smoothed log loss discussed in the main paper. We next develop the details. Note that, although the results of Meir and Zhang (2003) are for unbounded losses, their conditions are complex and it is not clear how to apply these results directly for Bayesian predictors such as the sparse GP discussed in this paper. Assuming a family of distributions Q over w and an upper bound p(y|x, w) \u2264 p y|w (where p y|w is a constant), uniform convergence for the averaged predictor E q(w) [p(y|x, w) ] under the smoothed log loss nlog (\u03b1) () will be shown. From Theorem 26.5.1 of Shalev-Shwartz and Ben-David (2014) 4 , for all q \u2208 Q, the following holds with probability 1 \u2212 \u03b4 over the choice of S: where H {E q(w) [p(\u00b7; w, \u00b7)] : q \u2208 Q}, \u2022 stands for function composition, 4. See also Corollary 4 of Meir and Zhang (2003) . These results give one-sided bounds but can be easily adapted to give the two sided bound shown here. for Rademacher variables \u03c3, and c = max{| log(\u03b1)|, Next, we slightly adapt the argument outlined in Sections 5 and 6.1 of Meir and Zhang (2003) . Fix some constant \u03bb \u2208 (0, \u221e) and sample-independent distribution p(w) over w. By applying the compression lemma (Banerjee, 2006) where (11) follows from the inequality E \u03c3 i exp(\u03c3 i a i ) \u2264 exp(a 2 i /2) (Lemma A.6 of ShalevShwartz and Ben-David (2014)), and we have defined A sup q\u2208Q KL(q(w), p(w)). Opti- . Substituting this value in (12) results in Utilizing (13) in (10), we have that with probability 1 \u2212 \u03b4 over the choice of S, for all q \u2208 Q, Defining Q A {q \u2208 Q s.t. KL(q, p) \u2264 A}, and the ERM hypothesis as we can use the above with the standard argument for ERM to get that, with probability 1 \u2212 \u03b4 over the choice of S, for all q \u2208 Q A , Applications of this results to sparse GP are possible as outlined in the main paper. Comparing this result to the discretization proof and randomization proof (below), we see that the requirements for Lipschitz constants are weaker. Here, we only need L (\u03b1) whereas other proofs require a Lipschitz condition w.r.t. the parameter \u03b8. This proof can potentially yield bounds that do not depend on the dimension M . Note that, applied to Gaussian distributions, A implicitly depends on M , so a direct application does include such a dependence. But Meir and Zhang (2003) show how to use structural risk minimization to get around this dimension dependence through data-dependent bounds. Let\u0398 denote some known subset of \u0398, i.e.,\u0398 \u2282 \u0398, and let {q jit (\u03b8|\u03b8)} denote a family of distributions over \u0398 parameterized by members of the subset\u0398. The members of the family are as yet unspecified, but represent \"jitter\" distributions which will be defined shortly. Let Note, where we exchange order of expectations in the following development, we assume the conditions of Fubini's theorem are met. The following lemma is standard (see ShalevShwartz and Ben-David (2014) ): Proof It is sufficient to prove that holds for all\u03b8 \u2208\u0398: Since\u03b8 ERM is the ERM hypothesis, it follows that \u2200\u03b8 \u2208\u0398, Taking expectations of both sides w.r.t. D n yields the result. The following lemma uses a technique from Germain et al. (2016) and Sheth and Khardon (2017) . The novelty, however, is to apply the compression lemma at a level higher than previous work. Here, we use it at the level of parameters \u03b8 defining the posterior distribution, which requires us to introduce the jitter, whereas previous work applied it at the level of base parameter w. This gives a qualitatively different result. Lemma 2. Let p(\u03b8) be any sample-independent distribution over \u0398 and define Then, \u2200\u03b8 \u2208\u0398, Proof First, apply Fubini's theorem to change the order of expectations of with q(\u03b8) = q jit (\u03b8|\u03b8 ERM ) and f (\u03b8) = \u03bb E (x,y)\u223cD (\u03b8, (x, y)) \u2212 1 n n i=1 (\u03b8, (x i , y i )) to the resulting expression within the expectation w.r.t. S \u223c D n . Finally, take the expectation w.r.t. S \u223c D n and note that E S\u223cD n log(\u00b7) \u2264 log E S\u223cD n (\u00b7) by Jensen's inequality to yield the statement of the lemma. Lemma 3. Let some norm || \u00b7 || over \u0398 be given. For an L-Lipschitz function (\u03b8) w.r.t. || \u00b7 ||, we have that \u2200\u03b8 \u2208\u0398, \u2200\u03b8 \u2208 \u0398, Since this holds for all values of \u03b8 (given some \u03b8 ), it also holds in expectation over any distribution in \u03b8, specifically q jit (\u03b8|\u03b8). Lemma 4. Let p(\u03b8) be any sample-independent distribution over \u0398 and (\u03b8, (x, y)) be L-Lipschitz in its first argument. Then, \u2200\u03b8 \u2208 \u0398, where D max \u03b8 \u2208\u0398\\\u0398 min\u03b8 \u2208\u0398 ||\u03b8 \u2212 \u03b8 || and \u03a8(\u03bb, n) is defined in Lemma 2. Proof Following from the left inequality of (18) with\u03b8 = \u03b8 =\u03b8 ERM , we have Utilizing (20) in (17) yields that, \u2200\u03b8 \u2208\u0398, Following from the right inequality of (18), we have that, \u2200\u03b8 \u2208\u0398, \u2200\u03b8 \u2208 \u0398, Utilizing (22) in (21) yields that, \u2200\u03b8 \u2208\u0398, \u03b8 \u2208 \u0398, Next, we develop a uniform bound over \u03b8 for the term E q jit (\u03b8|\u03b8) ||\u03b8 \u2212 \u03b8 || . Since (23) holds for all\u03b8 \u2208\u0398, we consider how\u03b8 can be selected per \u03b8 \u2208 \u0398. First, note that Now, when \u03b8 \u2208\u0398, a uniform bound over\u03b8 for E q jit (\u03b8|\u03b8) ||\u03b8 \u2212\u03b8|| translates to a uniform bound over \u03b8 for E q jit (\u03b8|\u03b8) ||\u03b8 \u2212 \u03b8 || since it is possible to select\u03b8 = \u03b8 in (24). When \u03b8 \u2208 \u0398\\\u0398, the distance to the \"closest\" point in\u0398 to \u03b8 is min\u03b8 \u2208\u0398 ||\u03b8 \u2212 \u03b8 ||. Then, the second term of (24) is uniformly upper-bounded over \u03b8 by D max \u03b8 \u2208\u0398\\\u0398 min\u03b8 \u2208\u0398 ||\u03b8 \u2212 \u03b8 ||. Combining the two cases yields the lemma. Jitter distributions. First, we define \u0398 parametrically as a function of some \u03c1 > 0 and relative to\u0398. Assume\u0398 is a subset of some space T (equipped with norm || \u00b7 ||), and define \u0398 as the set {\u03b8 \u2208 T s.t. min\u03b8 \u2208\u0398 ||\u03b8 \u2212\u03b8|| \u2264 \u03c1}. Then,\u0398 \u2282 \u0398, and D \u2264 \u03c1. The \u03c1-ball centered at\u03b8 \u2208\u0398 is denoted B \u03c1 (\u03b8) {\u03b8 \u2208 \u0398 s.t. ||\u03b8 \u2212\u03b8|| \u2264 \u03c1}. Let the jitter distribution q jit (\u03b8|\u03b8) be defined as the following uniform density with support in \u0398: q jit (\u03b8|\u03b8) = . For this choice of jitter distribution and prior, (19) becomes If vol(B \u03c1 (\u03b8)) = \u03ba\u03c1 M for some constants \u03ba, M , then the RHS of (26) be the 1-norm. Following standard Gaussian (see e.g., Rezende et al. (2014) ) and matrix"
}