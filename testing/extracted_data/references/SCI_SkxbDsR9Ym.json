{
    "title": "SkxbDsR9Ym",
    "content": "Knowledge Graph Embedding (KGE) is the task of jointly learning entity and relation embeddings for a given knowledge graph. Existing methods for learning KGEs can be seen as a two-stage process where (a) entities and relations in the knowledge graph are represented using some linear algebraic structures (embeddings), and (b) a scoring function is defined that evaluates the strength of a relation that holds between two entities using the corresponding relation and entity embeddings. Unfortunately, prior proposals for the scoring functions in the first step have been heuristically motivated, and it is unclear as to how the scoring functions in KGEs relate to the generation process of the underlying knowledge graph. To address this issue, we propose a generative account of the KGE learning task. Specifically, given a knowledge graph represented by a set of relational triples (h, R, t), where the semantic relation R holds between the two entities h (head) and t (tail), we extend the random walk model (Arora et al., 2016a) of word embeddings to KGE. We derive a theoretical relationship between the joint probability p(h, R, t) and the embeddings of h, R and t. Moreover, we show that marginal loss minimisation, a popular objective used by much prior work in KGE, follows naturally from the log-likelihood ratio maximisation under the probabilities estimated from the KGEs according to our theoretical relationship. We propose a learning objective motivated by the theoretical analysis to learn KGEs from a given knowledge graph. The KGEs learnt by our proposed method obtain state-of-the-art performance on FB15K237 and WN18RR benchmark datasets, providing empirical evidence in support of the theory.\n Knowledge graphs such as Freebase BID2 organise information in the form of graphs, where entities are represented by vertices in the graph and the relation between two entities is represented by the edge that connects the corresponding two vertices. By embedding entities and relations that exist in a knowledge graph in some (possibly lower-dimensional and latent) space we can infer previously unseen relations between entities, thereby expanding a given knowledge graph BID11 BID20 Lin et al., 2015; BID9 BID16 BID17 BID4 .Existing KGE methods can be seen as involving two main steps. First, given a knowledge graph represented by a set of relational triples (h, R, t), where a semantic relation R holds between a head entity h and a tail entity t, entities and relations are represented using some mathematical structures such as vectors, matrices or tensors. Second, a scoring function is proposed that evaluates the relational strength of a triple (h, R, t) and entity and relation embeddings that optimise the defined scoring function are learnt using some optimisation method. Table 1 shows some of the scoring functions proposed in prior work in KGE learning. Despite the wide applications of entity and relation embeddings created via KGE methods, the existing scoring functions are motivated heuristically to capture some geometric requirements of the embedding space. For example, TransE BID4 assumes that the entity and relation embeddings co-exist in the same (possibly lower dimensional) vector space and translating (shifting) the head entity embedding by the relation embedding must make it closer to the tail entity embedding, whereas ComplEx BID16 ) models the asymmetry in relations using Score function f (h, R, t) Relation parameters Unstructured BID4 h \u2212 t 1/2 none Structured embeddings BID4 R 1 h \u2212 R 2 t 1,2 R 1 , R 2 \u2208 R d\u00d7d TransE BID4 h + R \u2212 t 1/2 R \u2208 R d DistMult BID20 h, R, t R \u2208 R d RESCAL BID9 h Rt R d\u00d7d ComplEx BID16 h, R,t R \u2208 C d Table 1 : Score functions proposed in selected prior work on KGE. Entity embeddings h, t \u2208 R d are vectors in all models, except in ComplEx where h, t \u2208 C d . Here, x 1/2 denotes either 1 or 2 norm of the vector x. In ComplEx,x is the elementwise complex conjugate, and \u00b7, \u00b7, \u00b7 denotes the component-wise multi-linear inner-product.the component-wise multi-linear inner-product among entity and relation embeddings. Relational triples extracted from a given knowledge graph are used as positive training instances, whereas pseudo-negative BID4 instances are automatically generated by randomly corrupting positive instances. Finally, KGE are learnt such that the prediction loss computed over the positive and negative instances is minimised. Despite the good empirical performances of the existing KGE methods, theoretical understanding of KGE methods is comparatively under developed. For example, it is not clear how the heuristically defined KGE objectives relate to the generative process of a knowledge graph. In this paper, we attempt to fill this void by providing a theoretical analysis of KGE. Specifically, in section 2, we propose a generative process where we explain the formation of a relation R between two entities h and t using the corresponding relation and entity embeddings. Following this generative story, we derive a relationship between the probability of R holding between h and t, p(h, t | R), and the embeddings of R, h and t. Interestingly, the derived relationship is not covered by any of the previously proposed heuristically-motivated scoring functions, providing the first-ever KGE method with a provable generative explanation. Next, in section 3, we show that the margin loss, which has been popularly used as a training objective in prior work on KGE, naturally arises as the log-likelihood ratio computed from p(h, t | R). Based on this result, we derive a training objective that we subsequently optimise for learning KGEs that satisfy our theoretical relationship. Using standard benchmark datasets proposed in prior work on KGE learning, we evaluate the learnt KGEs on a link prediction task and a triple classification task. Experimental results show that the learnt KGEs obtain state-of-the-art performance on FB15K237 and WN18RR benchmarks, thereby providing empirical evidence to support the theoretical analysis. Let us consider a knowledge graph D where the knowledge is represented by relational triples (h, R, t) \u2208 D. Here, R is a relational predicate of two arguments, where h (head) and t (tail) entities respectively filling the first and second arguments. We assume relations to be asymmetric in general. In other words, if (h, R, t) \u2208 D then it does not necessarily follow that (t, R, h) \u2208 D. The goal of KGE is to learn embeddings (representations) for the relations and entities in the knowledge graph such that the entities that participate in similar relations are embedded closely to each other in the entity embedding space, while at the same time relations that hold between similar entities are embedded closely to each other in the relational embedding space. We call the learnt entity and relation embeddings collectively as KGEs. Following prior work on KGE BID4 BID16 BID20 , we assume that entities and relations are embedded in the same vector space, allowing us to perform linear algebraic operations using the embeddings in the same vector space. Let us consider a random walk characterised by a time-dependent knowledge vector c k , where k is the current time step. The knowledge vector represents the knowledge we have about a particular group of entities and relations that express some facts about the world. For example, the knowledge that we have about people that are employed by companies can be expressed using entities of classes such as people and organisation, using relations such as CEO-of, employed-at, works-for, etc. We assume that entities h and t are represented by time-independent d-dimensional vectors, respectively h, t \u2208 R d .We assume the task of generating a relational triple (h, R, t) in a given knowledge graph to be a two-step process as described next. First, given the current knowledge vector at time k, c = c k and the relation R, we assume that the probability of an entity h satisfying the first argument of R to be given by (1). DISPLAYFORM0 Here, R 1 \u2208 R d\u00d7d is a relation-specific orthogonal matrix that evaluates the appropriateness of h for the first argument of R. For example, if R is the CEO-of relation, we would require a person as the first argument and a company as the second argument of R. However, note that the role of R 1 extends beyond simply checking the types of the entities that can fill the first argument of a relation. For our example above, not all people are CEOs and R 1 evaluates the likelihood of a person to be selected as the first argument of the CEO-of relation. Z c is a normalisation coefficient such that h\u2208V p(h | R, c) = 1, where the vocabulary V is the set of all entities in the knowledge graph. After generating h, the state of our random walker changes to c = c k+1 , and we next generate the second argument of R with the probability given by (2). DISPLAYFORM0 Here, R 2 \u2208 R d\u00d7d is a relation-specific orthogonal matrix that evaluates the appropriateness of t as the second argument of R. Z c is a normalisation coefficient such that t\u2208V p(t | R, c) = 1. Following our previous example of the CEO-of relation, R 2 evaluates the likelihood of an organisation to be a company with a CEO position. Importantly, R 1 and R 2 are representations of the relation R and independent of the entities. Therefore, we consider (R 1 and R 2 ) to collectively represent the embedding of R. Orthogonality of R 1 , R 2 is a requirement for the mathematical proof and also act as a regularisation constraint to prevent overfitting by restricting the relational embedding space. We first perform our mathematical analysis for relational embeddings represented by orthogonal matrices and discuss later how this requirement can be relaxed. We assume a slow random walk where the knowledge vectors do not change significantly between consecutive time steps (c k \u2248 c k+1 ). More specifically, we assume that c k \u2212 c k+1 \u2264 2 for some small 2 > 0. This is a realistic assumption for generating the two entity arguments in the same relational triple because, if the knowledge vectors were significantly different in the two generation steps, then it is likely that the corresponding relations are also different, which would not be coherent with the above-described generative process. Moreover, we assume that the knowledge vectors are distributed uniformly in the unit sphere and denote the distribution of knowledge vectors by C.To learn KGEs, we must estimate the probability that h and t satisfy the relation R, p(h, t | R), which can be obtained by taking the expectation of p(h, t | R, c, c ) w.r.t. c, c \u223c C given by (3). DISPLAYFORM1 Here, partition functions are given by Z c = h\u2208V c\u2208C exp h R 1 c and Z c = t\u2208V c \u2208C exp t R 2 c . (4) follows from our two-step generative process where the generation of h and t in each step is independent given the relation and the corresponding knowledge vectors. Computing the expectation in (5) is generally difficult because of the two partition functions Z c and Z c . However, Lemma 1 shows that the partition functions are narrowly distributed around a constant value for all c (or c ) values with high probability. Lemma 1 (Concentration Lemma). If the entity embedding vectors satisfy the Bayesian prior v = sv, wherev is from the spherical Gaussian distribution, and s is a scalar random variable, which is always bounded by a constant \u03ba, then the entire ensemble of entity embeddings satisfies that DISPLAYFORM2 for z = O(1/ \u221a n), and \u03b4 = exp(\u2212\u2126(log 2 n)), where n \u2265 d is the number of words and Z c is the partition function for c given by c\u2208V exp h R 1 c .proof: To prove the concentration lemma, we show that the mean E h [Z c ] of Z c is concentrated around a constant for all knowledge vectors c and its variance is bounded. Recall that DISPLAYFORM3 If P is an orthogonal matrix and x is a vector, then P x DISPLAYFORM4 2 , because P P = I. Therefore, from FORMULA4 and the orthogonality of the relational embeddings, we see that R 1 c is a simple rotation of c and does not alter the length of c. We represent h = s h\u0125 , where s h = h and\u0125 is a unit vector (i.e. \u0125 2 = 1) distributed on the spherical Gaussian with zero mean and unit covariance matrix I d \u2208 R d\u00d7d . Let s be a random variable that has the same distribution as s h . Moreover, let us assume that s is upper bounded by a constant \u03ba such that s \u2264 \u03ba. From the assumption of the knowledge vector c, it is on the unit sphere as well, which is then rotated by R 1 .We can write the partition function using the inner-product between two vectors h and R 1 c, Z c = h\u2208V exp h (R 1 c) . BID0 showed that (Lemma 2.1 in their paper) the expectation of a partition function of this form can be approximated as follows: DISPLAYFORM5 where n = |V| is the number of entities in the vocabulary. (8) follows from the expectation of a sum and the independence of h and R 1 from c. The inequality of FORMULA6 is obtained by applying the Taylor expansion of the exponential series and the final equality is due to the symmetry of the spherical Gaussian. From the law of total expectation, we can write DISPLAYFORM6 where, x = h R 1 c. Note that conditioned on s h , h is a Gaussian random variable with variance \u03c3 2 = s 2 h . Therefore, conditioned on s h , x is a random variable with variance \u03c3 2 = \u03c3 2 h . Using this distribution, we can evaluate E x|s h exp h R 1 c as follows: DISPLAYFORM7 Therefore, it follows that DISPLAYFORM8 where s is the variance of the 2 norms of the entity embeddings. Because the set of entities is given and fixed, both n and \u03c3 are constants, proving that E[Z c ] does not depend on c. Next, we calculate the variance V c [Z c ] as follows: DISPLAYFORM9 Because 2h R 1 t is a Gaussian random variable with variance 4\u03c3 2 = 4s 2 h from a similar calculation as in FORMULA0 we obtain, DISPLAYFORM10 for \u039b = exp(8\u03ba 2 ) a constant bounding s \u2264 \u03ba as stated. From above, we have bounded both the mean and variance of the partition function by constants that are independent of the knowledge vector. Note that neither exp h R 1 c nor exp t R 2 c are subGaussian nor sub-exponential. Therefore, standard concentration bounds derived for sub-Gaussian or sub-exponential random variables cannot be used in our analysis. However, the argument given in Appendix A.1 in BID1 for a partition function with bounded mean and variance can be directly applied to Z c in our case, which completes the proof of the concentration lemma. From the symmetry between h and t, Lemma 1 also applies for the partition function t\u2208V t R 2 c . Under the conditions required to satisfy Lemma 1, the following main theorem of this paper holds: Theorem 1. Suppose that the entity embeddings satisfy (1). Then, we have DISPLAYFORM11 DISPLAYFORM12 where DISPLAYFORM13 The complete proof of Theorem 1 is given in Appendix A. Below we briefly sketch the main steps. Proof sketch: Let F be the event that both c and c are within (1 \u00b1 z )Z. Then, from Lemma 1 and the union bound, event F happens with probability at least 1 \u2212 2 exp(\u2212\u2126(log 2 n)). The R.H.S. of (5) can be split into two parts T 1 and T 2 according to whether F happens or not. DISPLAYFORM14 T 1 can be approximated as given by (21). DISPLAYFORM15 On the other hand, T 2 can be shown to be a constant, independent of d, given by (22). DISPLAYFORM16 The vocabulary size n of real-world knowledge graphs is typically over 10 5 , for which T 2 becomes negligibly small. Therefore, it suffices to consider only T 1 . Because of the slowness of the random walk we have c \u2248 c Using the law of total expectation we can write T 1 as follows: DISPLAYFORM17 where A(c) := E c |c exp t R 2 c . Doing some further evaluations we show that The relationship given by (18) indicates that head and tail entity embeddings are first transformed respectively by R 1 and R 2 , and the squared 2 norm of the sum of the transformed vectors is proportional to the probability p(h, t | R). DISPLAYFORM18 In this section, we derive a training objective from Theorem 1 that we can then optimise to learn KGE. The goal is to empirically validate the theoretical result by evaluating the learnt KGEs. Knowledge graphs represent information about relations between two entities in the form of relational triples. The joint probability p(h, R, t) given by Theorem 1 is useful for determining whether a relation R exists between two given entities h and t. For example, if we know that with a high probability that R holds between h and t, then we can append (h, R, t) to the knowledge graph. The task of expanding knowledge graphs by predicting missing links between entities or relations is known as the link prediction problem BID16 . In particular, if we can automatically append such previously unknown knowledge to the knowledge graph, we can expand the knowledge graph and address the knowledge acquisition bottleneck. To derive a criteria for determining whether a link must be predicted among entities and relations, let us consider a relational triple (h, R, t) \u2208 D that exists in a given knowledge graph D. We call such relational triples as positive triples because from the assumption it is known that R holds between h and t. On the other hand, consider a negative relational triple (h , R, t ) \u2208 D formed by, for example, randomly perturbing a positive triple. A popular technique for generating such (pseudo) negative triples is to replace h or t with a randomly selected different instance of the same entity type. As an alternative for random perturbation, Cai and Wang (2018) proposed a method for generating negative instances using adversarial learning. Here, we are not concerned about the actual method used for generating the negative triples but assume a set of negative triples,D, generated using some method, to be given. Given a positive triple (h, R, t) \u2208 D and a negative triple (h , R, t ) \u2208D, we would like to learn KGEs such that a higher probability is assigned to (h, R, t) than that assigned to (h , R, t ). We can formalise this requirement using the likelihood ratio given by (25). DISPLAYFORM0 Here, \u03b7 > 1 is a threshold that determines how higher we would like to set the probabilities for the positive triples compares to that of the negative triples. By taking the logarithm of both sides in (25) we obtain DISPLAYFORM1 If a positive triple (h, R, t) is correctly assigned a higher probability than a negative triple p(h , R, t ), then the left hand side of (26) will be negative, indicating that there is no loss incurred during this classification task. Therefore, we can re-write (26) to obtain the marginal loss BID5 , L(D,D), a popular choice as a learning objective in prior work in KGE, as shown in (27) . DISPLAYFORM2 We can assume 2d log \u03b7 to be the margin for the constraint violation. Theorem 1 requires R 1 and R 2 to be orthogonal. To reflect this requirement, we add two 2 regularisation terms R 1 R 1 \u2212 I 2 2 and R 2 R 2 \u2212 I 2 2 respectively with regularisation coefficients \u03bb 1 and \u03bb 2 to the objective function given by (27). In our experiments, we compute the gradients (27) w.r.t. each of the parameters h, t, R 1 and R 2 and use stochastic gradient descent (SGD) for optimisation. This approach can be easily extended to learn from multiple negative triples as shown in Appendix B. At a high-level of abstraction, KGE methods can be seen as differing in their design choices for the following two main problems: (a) how to represent entities and relations, and (b) how to model the interaction between two entities and a relation that holds between them. Next, we briefly discuss prior proposals to those two problems (refer BID17 BID10 BID6 for an extended survey on KGE).A popular choice for representing entities is to use vectors, whereas relations have been represented by vectors, matrices or tensors. For example, TransE BID4 , TransH BID18 , TransD (Ji et al., 2015) , TransG BID19 , TransR (Lin et al., 2015) , lppTransD BID21 , DistMult BID20 , HolE BID11 and ComplEx BID16 represent relations by vectors, whereas Structured Embeddings BID4 Given entity and relation embeddings, a scoring function is defined that evaluates the strength of a relation R between two entities h and t in a triple (h, R, t). The scoring functions that encode various intuitions have been proposed such as the 1 or 2 norms of the vector formed by a translation of the head entity embedding by the relation embedding over the target embedding, or by first performing a projection from the entity embedding space to the relation embedding space BID21 As an alternative to using vector norms as scoring functions, DistMult and ComplEx use the component-wise multi-linear dot product. Once a scoring function is defined, KGEs are learnt that assign better scores to relational triples in existing knowledge graphs (positive triples) over triples where the relation does not hold (negative triples) by minimising a loss function such as the logistic loss (RESCAL, DistMult, ComplEx) or marginal loss (TransE, TransH, TransD, TransD). Because knowledge graphs record only positive triples, a popular method to generate pseudo negative triples is to perturb a positive instance by replacing its head or tail entity by an entity selected uniformly at random from the vocabulary of the entities. However, uniformly sampled negative triples are likely to be obvious examples that do not provide much information to the learning process and can be detected by simply checking for the type of the entities in a triple. Cai and Wang (2018) proposed an adversarial learning approach where a generator assigns a probability to each relation triple and negative instances are sampled according to this probability distribution to train a discriminator that discriminates between positive and negative instances. BID19 proposed TransG, a generative model based on the Chinese restaurant process, to model multiple relations that exist between a pair of entities. However, their relation embeddings are designed to satisfy vector translation similar to TransE.As an alternative to directly learning embeddings from a graph, several methods (Grover and Leskovec, 2016; BID12 BID13 have considered the vertices visited during truncated random walks over the graph as pseudo sentences, and have applied popular word embedding learning algorithms such as skip-gram with negative sampling or continuous bag-of-words model (Mikolov et al., 2013) to learn vertex embeddings. However, pseudo sentences generated this way are syntactically very different from sentences in natural languages. On the other hand, our work extends the random walk analysis by BID0 that derives a useful connection between the joint co-occurrence probability of two words and the 2 norm of the sum of the corresponding word embeddings. Specifically, they proposed a latent variable model where the words in a corpus are generated by a probabilistic model parametrised by a time-dependent discourse vector that performs a random walk. However, unlike in our work, they do not consider the relations between two co-occurring words in a corpus. BID3 extended the model proposed by BID0 to capture co-occurrences involving more than two words. They defined the co-occurrence of k unique words in a given context as a k-way co-occurrence, where BID0 's result could be seen as a special case corersponding to k = 2. Moreover, BID3 showed that it is possible to learn word embeddings that capture some types of semantic relations such as antonymy and collocation using 3-way co-occurrences more accurately than using 2-way co-occurrences. However, their model does not explicitly consider the relations between words/entities and uses only a corpus for learning the word embeddings. To empirically evaluate the theoretical result stated in Theorem 1, we learn KGEs (denoted by RelWalk) by minimising the marginal loss objective derived in section 3. We use the FB15k237, FB13 (subsets of Freebase) and WN11, WN18RR (subsets of WordNet) datasets, which are standard benchmarks for KGE. We use the standard training, validation and test splits as detailed in TAB2 . We generate negative triples by replacing a head or a tail entity in a positive triple by a randomly selected different entity and learn KGEs. We train the model until convergence or at most 1000 epochs over the training data where each epoch is divided into 100 mini-batches. The best model is selected by early stopping based on the performance of the learnt embeddings on the validation set (evaluated after each 20 epochs). The training details and hyperparameter settings are detailed in Appendix C. RelWalk is implemented in the open-source toolkit OpenKE (Han et al., 2018) . We conduct two evaluation tasks: link prediction (predict the missing head or tail entity in a given triple (h, R, ?) or (?, R, t)) BID4 and triple classification (predict whether a relation R holds between h and t in a given triple (h, R, t)) BID14 . We evaluate the performance in the link prediction task using mean reciprocal rank (MRR), mean rank (MR (the average of the rank assigned to the original head or tail entity in a corrupted triple) and hits at ranks 1, 3 and 10 (H@1,3,10), whereas in the triple classification task we use accuracy (percentage of the correctly classified test triples). We only report scores under the filtered setting BID5 , which removes all triples appeared in training, validating and testing sets from candidate triples before obtaining the rank of the ground truth triple. In link prediction, we consider all entities that appear in the corresponding argument in the entire knowledge graph as candidates. In TAB0 we compare the KGEs learnt by RelWalk against prior work using the published results. For link prediction, RelWalk reports SoTA on both WN18RR and FB15K237 in all evaluation measures, except against ConvE in WN18RR measured by MRR. WN18RR excludes triples from WN18 that are simply inverted between train and test partitions BID15 Dettmers et al., 2017 ). RelWalk's consistently good performance on both versions of this dataset shows that it is considering the global structure in the knowledge graph when learning KGEs. For triple classification, RelWalk reports the best performance on FB13, whereas TransG reports the best performance on WN11. Considering that both TransG and RelWalk are generative models, it would be interesting to further investigate generative approaches for KGE in the future. Overall, the experimental results support our theoretical claim and emphasise the importance of theoretically motivating the scoring function design process. We proposed RelWalk, a generative model of KGE and derived a theoretical relationship between the probability of a triple and entity, relation embeddings. We then proposed a learning objective based on the theoretical relationship we derived. Experimental results on a link prediction and a triple classification tasks show that RelWalk obtains strong performances in multiple benchmark datasets. A PROOF OF THEOREM 1Let us consider the probabilistic event that DISPLAYFORM0 Then from the union bound we have, DISPLAYFORM1 Moreover, let F be the probabilistic event that both F c and F c being True. Then from DISPLAYFORM2 We can decompose the expectation in the R.H.S. in FORMULA2 into two terms T 1 and T 2 depending on whether respectively F is True or False as follows: DISPLAYFORM3 Here, 1 F and 1F are indicator functions given by: DISPLAYFORM4 and DISPLAYFORM5 Let us first show that T 2 is negligibly small. For two real integrable functions \u03c8 1 (x) and \u03c8 2 (x) in [a, b], the Cauchy-Schwarz's inequality states that DISPLAYFORM6 The second term of FORMULA2 is upper bounded by DISPLAYFORM7 The first term of (35) can be bounded as follows: DISPLAYFORM8 where \u03b1 > 1. Therefore, it is sufficient to bound E c exp(\u03b1h R 1 c) DISPLAYFORM9 Let us denote by z the random variable 2h R 1 c. Moreover, let r(z) = E c |z [1F ] , which is a function of z between [0, 1]. We wish to upper bound E c [exp(z)r(z)]. The worst-case r(z) can be quantified using a continuous version of Abel's inequality (proved as Lemma A.4 in BID1 ), we can upper bound E c [exp(z)r(z)] as follows: DISPLAYFORM10 where DISPLAYFORM11 is a function that takes the value 1 when z \u2265 t and zero elsewhere. Then, we claim Pr c [z \u2265 t] \u2264 exp(\u2212\u2126(log 2 n)) implies that t \u2265 \u2126(log .9 n).If c was distributed as N (0, DISPLAYFORM12 , this would be a simple tail bound. However, as c is distributed uniformly on the sphere, this requires special care, and the claim follows by applying the tail bound for the spherical distribution given by Lemma A.1 in BID0 instead. Finally, applying Corollary A.3 in BID0 , we have: DISPLAYFORM13 From a similar argument as above we can obtain the same bound for c as well. Therefore, T 2 in (29) can be upper bounded as follows: DISPLAYFORM14 Because n = |V|, the size of the entity vocabulary, is large (ca. n > 10 5 ) in most knowledge graphs, we can ignore the T 2 term in (29). Combining this with (29) we obtain an upper bound for p(h, t | R) given by (41). DISPLAYFORM15 where |D| is the number of relational tuples (h, R, t) in the KB D and \u03b4 0 = |D| exp(\u2212\u2126(log 1.8 n)) \u2264 exp(\u2212\u2126(log 1.8 n)) by the fact that Z \u2264 exp(2\u03ba)n = O(n), where \u03ba is the upper bound on h R 1 c and t R 2 c , which is regarded as a constant. On the other hand, we can lower bound p(h, t | R) as given by (42). DISPLAYFORM16 Taking the logarithm of both sides, from (41) and (42), the multiplicative error translates to an additive error given by (43). DISPLAYFORM17 where A(c) := E c |c exp t R 2 c .We assumed that c and c are on the unit sphere and R 1 and R 2 to be orthogonal matrices. Therefore, R 1 c and R 2 c are also on the unit sphere. Moreover, if we let the upper bound of the 2 norm of the entity embeddings to be \u03ba DISPLAYFORM18 Then we can lower bound A(c) as follows: DISPLAYFORM19 For some 2 > 0. The last inequality holds because DISPLAYFORM20 To obtain a lower bound on A(c) from the first-order Taylor approximation of exp(x) \u2265 1 + x we observe that DISPLAYFORM21 Therefore, from our model assumptions we have DISPLAYFORM22 Hence, DISPLAYFORM23 Therefore, from FORMULA3 and FORMULA6 we have DISPLAYFORM24 Plugging A(c) back in (43) we obtain DISPLAYFORM25 = log E c exp h R 1 c exp t R 2 c \u00b1 \u03b4 0 \u2212 2 log Z + 2 log(1 \u00b1 z ) + log(1 \u00b1 2 )= log E c exp h R 1 c + t R 2 c \u00b1 \u03b4 0 \u2212 2 log Z + 2 log(1 \u00b1 z ) + log(1 \u00b1 2 )= log E c exp R 1 h + R 2 t c \u00b1 \u03b4 0 \u2212 2 log Z + 2 log(1 \u00b1 z ) + log(1 \u00b1 2 )Note that c has a uniform distribution over the unit sphere. In this case, from Lemma A.5 in BID1 ), (52) holds approximately. DISPLAYFORM26 where 3 =\u00d5(1/d). Plugging FORMULA1 in FORMULA0 we have that 8 n) ). Therefore, \u03b4 0 can be ignored. Note that 3 =\u00d5(1/d) and z =\u00d5(1/ \u221a n) by assumption. Therefore, we obtain that DISPLAYFORM27 DISPLAYFORM28 DISPLAYFORM29 In this section, we show how the margin loss-based learning objective derived in section 3 can be extended to learn from more than one negative triples per each positive triple. This formulation leads to rank-based loss objective used in prior work on KGE. Considering that negative triples are generated via random perturbation, it is important to consider multiple negative triples during training to better estimate the classification boundary. Let us consider that we are given a positive triple, (h, R, t) and a set of K negative triples {(h k , R, t k )} K k=1 . We would like our model to assign a probability, p(h, t | R), to the positive triple that is higher than that assigned to any of the negative triples. This requirement can be written as (55). DISPLAYFORM0 We could further require the ratio between the probability of the positive triple and maximum probability over all negative triples to be greater than a threshold \u03b7 \u2265 1 to make the requirement of (55) to be tighter. DISPLAYFORM1 By taking the logarithm of (56) we obtain log p(h, t | R) \u2212 log max k=1,...,K DISPLAYFORM2 Therefore, we can define the margin loss for a misclassification as follows:L (h, R, t), {(h k , R, t k )} K k=1 = max 0, log max k=1,...,K p(h k , t k | R) + log(\u03b7) \u2212 log p(h, t | R) FORMULA2 However, from the monotonicity of the logarithm we have \u2200x 1 , x 2 > 0, if log(x 1 ) \u2265 log(x 2 ) then x 1 \u2265 x 2 . Therefore, the logarithm of the maximum can be replaced by the maximum of the logarithms in (58) as shown in (59).L (h, R, t), {(h k , R, t k )} K k=1 = max 0, max k=1,...,K log p(h k , t k | R) + log(\u03b7) \u2212 log p(h, t | R) (59)By substituting (18) for the probabilities in (59) we obtain the rank-based loss given by (60).L (h, R, t), {(h k , R, t k )} In practice, we can use p(h k , t k | R) to select the negative triple with the highest probability for training with the positive triple. C TRAINING DETAILSThe statistics of the benchmark datasets are show in TAB2 .We selected the initial learning rate (\u03b1) for SGD in {0.01, 0.001}, the regularisation coefficients (\u03bb 1 , \u03bb 2 ) for the orthogonality constraints of relation matrices in {0, 1, 10, 100}. The number of randomly generated negative triples n neg for each positive example is varied in {1, 10, 20, 50, 100} and d \u2208 {50, 100}. Optimal hyperparameter settings were: \u03bb 1 = \u03bb 2 = 10, n neg = 100 for all the datasets, \u03b1 = 0.001 for FB15K, FB15K237 and FB13, \u03b1 = 0.01 for WN18, WN18RR and WN11. For FB15K237 and WN18RR d = 100 was the best, whereas for all other datasets d = 50 performed best."
}