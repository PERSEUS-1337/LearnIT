{
    "title": "SJfb5jCqKm",
    "content": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods. The deployment of deep learning models in applications with demanding decision-making components such as autonomous driving or medical diagnosis hinges on our ability to monitor and control their statistical uncertainties. Conceivably, the Bayesian framework offers a principled approach to infer uncertainties from a model; however, there are computational hurdles in implementing it for deep neural networks BID9 . Presently, practically feasible (say, for image classification) uncertainty estimation methods for deep learning are based on signals emerging from standard (non Bayesian) networks that were trained in a standard manner. The most common signals used for uncertainty estimation are the raw softmax response BID4 , some functions of softmax values (e.g., entropy), signals emerging from embedding layers BID20 , and the MC-dropout method BID9 ) that proxies a Bayesian inference using dropout sampling applied at test time. These methods can be quite effective, but no conclusive evidence on their relative performance has been reported. A recent NIPS paper provides documentation that an ensemble of softmax response values of several networks performs better than the other approaches BID17 .In this paper, we present a method of confidence estimation that can consistently improve all the above methods, including the ensemble approach of BID17 . Given a trained classifier and a confidence score function (e.g., generated by softmax response activations), our algorithm will learn an improved confidence score function for the same classifier. Our approach is based on the observation that confidence score functions extracted from ordinary deep classifiers tend to wrongly estimate confidence, especially for highly confident instances. Such erroneous estimates constitute a kind of artifact of the training process with an stochastic gradient descent (SGD) based optimizers. During this process, the confidence in \"easy\" instances (for which we expect prediction with high confidence) is quickly and reliably assessed during the early SGD epochs. Later on, when the optimization is focused on the \"hard\" points (whose loss is still large), the confidence estimates of the easy points become impaired. Uncertainty estimates are ultimately provided in terms of probabilities. Nevertheless, as previously suggested BID10 BID20 BID17 , in a non-Bayesian setting (as we consider here) it is productive to decouple uncertainty estimation into two separate tasks: ordinal ranking according to uncertainty, and probability calibration. Noting that calibration (of ordinal confidence ranking) already has many effective solutions BID21 BID23 BID27 BID11 , our main focus here is on the core task of ranking uncertainties. We thus adopt the setting of BID17 , and others BID10 BID20 , and consider uncertainty estimation for classification as the following problem. Given labeled data, the goal is to learn a pair (f, \u03ba), where f (x) is a classifier and \u03ba(x) is a confidence score function. Intuitively, \u03ba should assign lower confidence values to points that are misclassified by f , relative to correct classifications (see Section 2 for details).We propose two methods that can boost known confidence scoring functions for deep neural networks (DNNs). Our first method devises a selection mechanism that assigns for each instance an appropriate early stopped model, which improves that instance's uncertainty estimation. The mechanism selects the early-stopped model for each individual instance from among snapshots of the network's weights that were saved during the training process. This method requires an auxiliary training set to train the selection mechanism, and is quite computationally intensive to train. The second method approximates the first without any additional examples. Since there is no consensus on the appropriate performance measure for scoring functions, we formulate such a measure based on concepts from selective prediction BID10 BID26 . We report on extensive experiments with four baseline methods (including all those mentioned above) and four image datasets. The proposed approach consistently improves all baselines, often by a wide margin. For completeness, we also validate our results using probably-calibrated uncertainty estimates of our method that are calibrated with the well-known Platt scaling technique BID23 and measured with the negative log-likelihood and Brier score Brier (1950). In this work we consider uncertainty estimation for a standard supervised multi-class classification problem. We note that in our context uncertainty can be viewed as negative confidence and vice versa. We use these terms interchangeably. Let X be some feature space (e.g., raw image pixels) and Y = {1, 2, 3, . . . , k}, a label set for the classes. Let P (X, Y ) be an unknown source distribution over X \u00d7 Y. A classifier f is a function f : X \u2192 Y whose true risk w.r.t. DISPLAYFORM0 + is a given loss function, for example, the 0/1 error. Given a labeled set DISPLAYFORM1 We consider deep neural classification models that utilize a standard softmax (last) layer for multi-class classification. Thus, for each input x \u2208 X , the vector f (x) = (f (x) 1 , . . . , f (x) k ) \u2208 R k is the softmax activations of the last layer. The model's predicted class\u0177 =\u0177 f (x) = argmax i\u2208Y f (x) i .Consider the training process of a deep model f through T epochs using any mini-batch SGD optimization variant. For each 1 \u2264 i \u2264 T , we denote by f[i] a snapshot of the partially trained model immediately after epoch i. For a multi-class model f , we would like to define a confidence score function, \u03ba(x, i, |f ), where x \u2208 X , and i \u2208 Y. The function \u03ba should quantify confidence in predicting that x is from class i, based on signals extracted from f . A \u03ba-score function should induce a partial order over points in X , and thus is not required to distinguish between points with the same score. For example, for any softmax classifier f , the vanilla confidence score function is \u03ba(x, i|f ) = \u2206 f (x) i (i.e., the softmax response values themselves). Perhaps due to the natural probabilistic interpretation of the softmax function (all values are non-negative and sum to 1), this vanilla \u03ba has long been used as a confidence estimator. Note, however, that we are not concerned with the standard probabilistic interpretation (which needs to be calibrated to properly quantify probabilities BID11 ).An optimal \u03ba (for f ) should reflect true loss monotonicity in the sense that for every two labeled instances (x 1 , y 1 ) \u223c P (X, Y ), and ( DISPLAYFORM2 In the domain of (deep) uncertainty estimation there is currently no consensus on how to measure performance (of ordinal estimators). For example, BID17 used the Brier score and the negative-log-likelihood to asses their results, while treating \u03ba values as absolute scores. In BID20 the area under the ROC curve was used for measuring performance. In this section we propose a meaningful and unitless performance measure for \u03ba functions, which borrows elements from other known approaches. In order to define a performance measure for \u03ba functions, we require a few concepts from selective classification BID7 BID25 . As noted in (Geifman & ElYaniv, 2017) , any \u03ba function can be utilized to construct a selective classifier (i.e., a classifier with a reject option). Thus, selective classification is a natural application of confidence score functions based on which it is convenient and meaningful to assess performance. The structure of this section is as follows. We first introduce the (well known) terms selective classifier, selective risk and coverage. Then we introduce the risk-coverage curve. We propose to measure the performance of a \u03ba function as the area under the risk-coverage curve (AURC) of a selective classifier induced by \u03ba. The proposed measure is a normalization of AURC where we subtract the AURC of the best \u03ba in hindsight. The benefit of the proposed normalization is that it allows for meaningful comparisons accross problems. We term the this normalized metric \"excess AURC\" (E-ARUC) and it will be used throughout the paper for performance evaluation of \u03ba functions. A selective classifier is a pair (f, g), where f is a classifier, and g : X \u2192 {0, 1} is a selection function, which serves as a binary qualifier for f as follows, DISPLAYFORM0 The performance of a selective classifier is quantified using coverage and risk. Coverage, defined to DISPLAYFORM1 , is the probability mass of the non-rejected region in X . The selective risk of DISPLAYFORM2 These two measures can be empirically evaluated over any finite labeled set S m (not necessarily the training set) in a straightforward manner. Thus, the empirical selective risk is, DISPLAYFORM3 where\u03c6 is the empirical coverage,\u03c6(f, DISPLAYFORM4 The overall performance profile of a family of selective classifiers (optimized for various coverage rates) can be measured using the risk-coverage curve (RC-curve), defined to be the selective risk as a function of coverage. Given a classifier f and confidence score function \u03ba defined for f , we define an empirical performance measure for \u03ba using an independent set V n of n labeled points. The performance measure is defined in terms of the following selective classifier (f, g) (where f is our given classifier), and the selection functions g is defined as a threshold over \u03ba values, DISPLAYFORM5 Let \u0398 be the set of all \u03ba values of points in V n , \u0398 = \u2206 {\u03ba(x,\u0177 f (x)|f ) : (x, y) \u2208 V n }; for now we assume that \u0398 contains n unique points, and later we note how to deal with duplicate values. The performance of \u03ba is defined to be the area under the (empirical) RC-curve (AURC) of the pair DISPLAYFORM6 Intuitively, a better \u03ba will induce a better selective classifier that will tend to reject first the points that are misclassified by f . Accordingly, the associated RC-curve will decrease faster (with decreasing coverage) and the AURC will be smaller. For example, in Figure 1 we show (in blue) the RC-curve of classifier f obtained by training a DNN trained over the CIFAR-100 dataset. The \u03ba induced by f is the softmax response confidence score, \u03ba(x) = max i f (x) i . The RC-curve in the figure is calculated w.r.t. to an independent labeled set V n of n = 10, 000 points from CIFAR-100. Each point on the curve is the empirical selective risk (2) of a selective classifier (f, g \u03b8 ) such that \u03b8 \u2208 \u0398. As can be seen, the selective risk is monotonically increasing with coverage. For instance, at full coverage = 1, the risk is approximately 0.29. This risk corresponds to a standard classifier (that always predicts and does not reject anything). The risk corresponding to coverage = 0.5 is approximately 0.06 and corresponds to a selective classifier that rejects half of the points (those whose confidence is least). Not surprisingly, its selective risk is significantly lower than the risk obtained at full coverage. Figure 1: RC-curve for the CIFAR100 dataset with softmax response confidence score. Blue: the RC curve based on softmax response; black: the optimal curve that can be achieved in hindsight. An optimal in hindsight confidence score function for f , denoted by \u03ba * , will yield the optimal risk coverage curve. This optimal function rates all misclassified points (by f ) lower than all correctly classified points. The selective risk associated with \u03ba * is thus zero at all coverage rates below 1 \u2212r(f |V n ). The reason is that the optimal function rejects all misclassified points at such rates. For example, in Figure 1 we show the RC-curve (black) obtained by relying on \u03ba * , which reaches zero at coverage of 1 \u2212 0.29 = 0.71 (red dot); note that the selective risk at full coverage is 0.29.Since the AURC of all RC-curves for f induced by any confidence scoring function will be larger than the AURC of \u03ba * , we normalize by AURC(\u03ba * ) to obtain a unitless performance measure. To compute the AURC of \u03ba * , we compute the discrete integral ofr (w.r.t. \u03ba * ) from the coverage level of 1 \u2212r(f |V n ) (0 errors) to 1 (nr errors). Thus, DISPLAYFORM7 We approximate (3) using the following integral: DISPLAYFORM8 For example, the gray area in Figure 1 is the AURC of \u03ba * , which equals 0.04802 (and approximated by 0.04800 using the integral).To conclude this section, we define the Excess-AURC (E-AURC) as E-AURC(\u03ba, f |V n ) = AURC(\u03ba, f |V n ) \u2212 AURC(\u03ba * , f |V n ). E-AURC is a unitless measure in [0, 1], and the optimal \u03ba will have E-AURC = 0. E-AURC is used as our main performance measure. The area of uncertainty estimation is huge, and way beyond our scope. Here we focus only on non-Bayesian methods in the context of deep neural classification. Motivated by a Bayesian approach, BID9 proposed the Monte-Carlo dropout (MC-dropout) technique for estimating uncertainty in DNNs. MC-dropout estimates uncertainty at test time using the variance statistics extracted from several dropout-enabled forward passes. The most common, and well-known approach for obtaining confidence scores for DNNs is by measuring the classification margin. When softmax is in use at the last layer, its values correspond to the distance from the decision boundary, where large values tend to reflect high confidence levels. This concept is widely used in the context of classification with a reject option in linear models and in particular, in SVMs BID0 BID3 BID8 . In the context of neural networks, BID4 BID5 were the first to propose this approach and, for DNNs, it has been recently shown to outperform the MC-dropout on ImageNet BID10 .A K-nearest-neighbors (KNN) algorithm applied in the embedding space of a DNN was recently proposed by BID20 . The KNN-distances are used as a proxy for classconditional probabilities. To the best of our knowledge, this is the first non-Bayesian method that estimates neural network uncertainties using activations from non-final layers. A new ensemble-based uncertainty score for DNNs was proposed by BID17 . It is well known that ensemble methods can improve predictive performance BID1 . Their ensemble consists of several trained DNN models, and confidence estimates were obtained by averaging softmax responses of ensemble members. While this method exhibits a significant improvement over all known methods (and is presently state-of-the-art), it requires substantially large computing resources for training. When considering works that leverage information from the network's training process, the literature is quite sparse. BID14 proposed to construct an ensemble, composed of several snapshots during training to improve predictive performance with the cost of training only one model. However, due to the use of cyclic learning rate schedules, the snapshots that are averaged are fully converged models and produce a result that is both conceptually and quantitatively different from our use of snapshots before convergence. BID15 similarly proposed to average the weights across SGD iterations, but here again the averaging was done on fully converged models that have been only fine-tuned after full training processes. Thus both these ensemble methods are superficially similar to our averaging technique but are different than our method that utilizes \"premature\" ensemble members (in terms of their classification performance). In this section we present an example that motivates our algorithms. Consider a deep classification model f that has been trained over the set S m through T epochs. Denote by f [i] the model trained at the ith epoch; thus, f = f [T ] . Take an independent validation set V n of n labeled points. We monitor the quality of the softmax response generated from f (and its intermediate variants f [i] ), through the training process, as measured on points in V n . The use of V n allows us to make meaningful statements about the quality of softmax response values (or any other confidence estimation method) for unseen test points. We construct the example by considering two groups of instances in V n defined by confidence assessment assigned using the softmax response values f gives to points in V n . The green group contains the highest (99%-100%) percentile of most confident points in V n , and the red group contains the lowest (0%-1%) percentile of least confident points. Although the softmax response is rightfully criticized in its ability to proxy confidence BID9 , it is reasonable to assume it is quite accurate in ranking green vs. red points (i.e., a prediction by f regarding a red point is likely to be less accurate than its prediction about a green point).We observe that the prediction of the green points' labels is learned earlier during training of f , compared to a prediction of any red point. This fact is evident in FIG0 where we see the training of f over CIFAR-100. Specifically, we see that the softmax response values of green points stabilize at their maximal values around Epoch 80. We also note that the green points in this top percentile are already correctly classified very early, near Epoch 25 (not shown in the figure) . In contrast, red points continue to improve their confidence scores throughout. This observation indicates that green points can be predicted very well by an intermediate model such as f 130 . Can we say that f 130 can estimate the confidence of green points correctly? Recall from Section 3 that a useful method for assessing the quality of a confidence function is the E-AURC measure (applied over an independent validation set). We now measure the quality of the softmax response of all intermediate classifiers shows the E-AURC of the red points. We see that for the green points, the confidence estimation quality improves (almost) monotonically and then degrades (almost) monotonically. The best confidence estimation is obtained by intermediate classifiers such as f 130 . Surprisingly, the final model f [T ] is one of the worst estimators for green points! In sharp contrast, the confidence estimates for the red points monotonically improves as training continues. The best estimator for red points is the final model f [T ] . This behavior can be observed in all the datasets we considered (not reported). DISPLAYFORM0 The above dynamics indicates that the learning of uncertainty estimators for easy instances conceptually resembles overfitting in the sense that the assessment of higher confidence points in the test set degrades as training continues after a certain point. To overcome this deficiency we propose an algorithm that uses the concept of early stopping in a pointwise fashion, where for each sample (or set of samples) we find the best intermediate snapshot for uncertainty estimation. In this section, first we present a supervised algorithm that learns an improved scoring function for a given pair (f, \u03ba), where f is a trained deep neural classifier, and \u03ba is a confidence scoring function for f 's predictions. In principle, \u03ba : X \u2192 R, where \u03ba(x) can be defined as any mapping from the activations of f applied on x to R. All the confidence estimation methods we described above comply with this definition.1 Our algorithm requires a labeled training sample. The second algorithm we present is an approximated version of the first algorithm, which does not rely on additional training examples. Let f be a neural classifier that has been trained using any (mini-batch) SGD variant for T epochs, and let F = \u2206 {f [i] : 1 \u2264 i \u2264 T } be the set of intermediate models obtained during training (f [i] is the model generated at epoch i). We assume that f , the snapshots set F , and a confidence score function for f , \u03ba(\u00b7, \u00b7|f ) : X \u2192 (0, 1]), are given.2 Let V n be an independent training set. The Pointwise Early Stopping (PES) algorithm for confidence scores (see pseudo-code in Algorithm 1) operates as follows. The pseudo-code contains both the training and inference procedures. At each iteration of the training main loop (lines 3-11), we extract from V (which is initialized as a clone of the set V n ) a set of the q most uncertain points. We abbreviate this set by S (the \"layer\"). The size of the layer is determined by the hyperparameter q. We then find the best model in F using the DISPLAYFORM0 for i = 0 to n/q do 4: DISPLAYFORM1 indicates the qth order statistic of r 6: DISPLAYFORM2 10: DISPLAYFORM3 end for 12:return K,\u0398 13: end function 14: function ESTIMATE CONFIDENCE(x,f ,K,\u0398) 15: DISPLAYFORM4 DISPLAYFORM5 end function E-AURC measure with respect to S. This model, denoted f [j] , is found by solving DISPLAYFORM6 The best performing confidence score over S, and the threshold over the confidence level, \u03b8, are saved for test time (lines 8-9) and used to associate points with their layers. We iterate and remove layer after layer until V is empty. Our algorithm produces a partition of X comprising layers from least to highest confidence. For each layer we find the best performing \u03ba function based on models from F .To infer the confidence rate for given point x at test time, we search for the minimal i that satisfies DISPLAYFORM7 , where \u03ba i and \u03b8 i are the i'th elements of K and \u0398 respectively. DISPLAYFORM8 , where i is added to enforce full order on the confidence score between layers, recall that \u03ba \u2208 (0, 1] . As we saw in Section 6.1, the computational complexity of the PES algorithm is quite intensive. Moreover, the algorithm requires an additional set of labeled examples, which may not always be available. The Averaged Early Stopping (AES) is a simple approximation of the PES motivated by the observation that \"easy\" points are learned earlier during training as shown in FIG0 . By summing the area under the learning curve (a curve that is similar to 2(a)) we leverage this property and avoid some inaccurate confidence assessments generated by the last model alone. We approximate the area under the curve by averaging k evenly spaced points on that curve. Let F be a set of k intermediate models saved during the training of f , DISPLAYFORM0 where linspace(t, T, k) is a set of k evenly spaced integers between t and T (including t and T ). We define the output \u03ba as the average of all \u03bas associated with models in F , DISPLAYFORM1 As we show in Section 7, AES works surprisingly well. In fact, due to the computational burden of running the PES algorithm, we use AES in most of our experiments below. We now present results of our AES algorithm applied over the four known confidence scores: softmax response, NN-distance BID20 , MC-dropout BID9 ans Ensemble BID17 ) (see Section 4). For implementation details for these methods, see Appendix A. We evaluate the performance of these methods and our AES algorithm that uses them as its core \u03ba. In all cases we ran the AES algorithm with k \u2208 {10, 30, 50}, and t = 0.4T . We experiment with four standard image datasets: CIFAR-10, CIFAR-100, SVHN, and Imagenet (see Appendix A for details).Our results are reported in Table 4 . The table contains four blocks, one for each dataset. Within each block we have four rows, one for each baseline method. To explain the structure of this table, consider for example the 4th row, which shows the results corresponding to the softmax response for CIFAR-10. In the 2nd column we see the E-AURC (\u00d710 3 ) of the softmax response itself (4.78). In the 3rd column, the result of AES applied over the softmax response with k = 10 (reaching E-AURC of 4.81). In the 4th column we specify percent of the improvement of AES over the baseline, in this case -0.7% (i.e., in this case AES degraded performance). For the imagenet dataset, we only present results for the softmax response and ensemble. Applying the other methods on this large dataset was computationally prohibitive. Let us now analyze these results. Before considering the relative performance of the baseline methods compares to ours, it is interesting to see that the E-AURC measure nicely quantifies the difficulty level of the learning problems. Indeed, CIFAR-10 and SVHN are known as relatively easy problems and the E-AURC ranges we see in the table for most of the methods is quite small and similar. CIFAR-100 is considered harder, which is reflected by significantly larger E-AURC values recorded for the various methods. Finally, Imagenet has the largest E-AURC values and is considered to be the hardest problem. This observation supports the usefulness of E-AURC. A non-unitless measure such as AUC, the standard measure, would not be useful in such comparisons. DISPLAYFORM0 It is striking that among all 42 experiments, our method improved the baseline method in 39 cases. Moreover, when applying AES with k = 30, it always reduced the E-AURC of the baseline method. For each dataset, the ensemble estimation approach of BID17 is the best among the baselines, and is currently state-of-the-art. It follows that for all of these datasets, the application of AES improves the state-of-the-art. While the ensemble method (and its improvement by AES) achieve the best results on these datasets, these methods are computationally intensive. It is, therefore, interesting to identify top performing baselines, which are based on a single classifier. In CIFAR-10, the best (single-classifier) method is softmax response, whose E-AURC is improved 6% by AES (resulting in the best single-classifier performance). Interestingly, in this dataset, NN-distance incurs a markedly bad E-AURC (35.1), which is reduced (to 4.58) by AES, making it on par with the best methods for this dataset. Turning to CIFAR-100, we see that the (single-classifier) top method is NN-distance, with an E-AURC of 45.56, which is improved by 22% using AES. Table 2 : NLL and Brier score of AES method applied with Platt scaling on CIFAR-10, CIFAR-100, SVHN and Imagenet compared to the baseline method (calibrated as well). Next we examine AES applied together with probability calibration. We calibrate the results of the AES algorithm using the Platt scaling technique; see BID23 for details. Platt scaling is applied on the results of the AES algorithm with k = 30, and compared to the independently scaled underlying measure without AES. Performance is evaluated using both negative log-likelihood (NLL) and the Brier score BID2 . For further implementation details of this experiment see Appendix A. The results appear in Table 2 . As can easily be seen, the probability scaling results are remarkably consistent with our raw uncertainty estimates (measured with the E-AURC) over all datasets and underlying uncertainty methods. We conclude that AES also improves calibrated probabilities of the underlying uncertainty measures, and the E-AURC can serve as a reliable proxy also for calibrated probabilities. We implemented the PES algorithm only over the softmax response method (SR) for several datasets. To generate an independent training set, which is required by PES, we randomly split the original validation set (in each dataset) into two parts and took a random 70% of the set for training our algorithm, using the remaining 30% for validation. Table 3 : E-AURC and % improvement for the Pointwise Early Stopping algorithm (PES) compared to the softmax response (SR) on CIFAR-10, CIFAR-100 and SVHN. All E-AURC values are multiplied by 10 3 for clarity.applying PES over NN-distance, the time complexity is nmT k DISPLAYFORM0 , where k is the number of neighbours and C f (S m ) is the time complexity of running a forward pass of m samples using the classifier f . Similarly, the complexity of PES when the underlying scores are from MC-dropout is O(dT C f (V n ) where d is the number of dropout iterations (forward passes) of the MC-dropout algorithm. Thus, when n = 7000, T = 250 (the parameters used for applying PES over CIFAR-100), and with d = 100 (as recommended in BID9 ), this amounts to 175,000,000 forward passes. We set q = n/3 . We repeated the experiment over 10 random training-validation splits and report the average results and the standard errors in Table 3 .As seen, PES reduced the E-AURC of softmax on all datasets by a significant rate. The best improvement was achieved on CIFAR-100 (E-AURC reduced by 18%).Our difficulties when applying the PES algorithm on many of the underlying confidence methods, and the outstanding results of the AES motivate further research that should lead to improving the algorithm and making it more efficient. We presented novel uncertainty estimation algorithms, which are motivated by an observation regarding the training process of DNNs using SGD. In this process, reliable estimates generated in early epochs are later on deformed. This phenomenon somewhat resembles the well-known overfitting effect in DNNs. The PES algorithm we presented requires an additional labeled set and expensive computational resources for training. The approximated version (AES) is simple and scalable. The resulting confidence scores our methods generate systematically improve all existing estimation techniques on all the evaluated datasets. Both PES and AES overcome confidence score deformations by utilizing available snapshot models that are generated anyway during training. It would be interesting to develop a loss function that will explicitly prevent confidence deformations by design while maintaining high classification performance. In addition, the uncertainty estimation of each instance currently requires several forward passes through the network. Instead it would be interesting to consider incorporating distillation BID13 so as to reduce inference time. Another direction to mitigate the computational effort at inference time is to approximate PES using a single model per instance based on an early stopping criterion similar to the one proposed by BID19 . Softmax Response: For the softmax response method (SR) we simply take the relevant softmax value of the sample, \u03ba(x, i|f ) = f (x) i . We implemented the NN-distance method using k = 500 for the nearest neighbors parameter. We didn't implemented the two proposed extensions (embedding regularization, and adversarial training), this add-on will degrade the performance of f for better uncertainty estimation, which we are not interested in. Moreover, running the NN-distance with this add-on will require to add it to all other methods to manage a proper comparison. The MC-dropout implemented with p = 0.5 for the dropout rate, and 100 feed-forward iterations for each sample. Ensemble: The Ensemble method is implemented as an average of softmax values across ensemble of 5 DNNs. Platt scaling BID23 : The Platt scaling is applied as follows. Given a confidence measure \u03ba and a validation set V , the scaling is the solution of the logistic regression from \u03ba(x,\u0177 f (x)|f ) to \u03ba * (x,\u0177 f (x)|f ), where \u03ba * (x,\u0177 f (x)|f ) is defined as 0 when x =\u0177 f (x) and 1 otherwise. We train the logistic regression models based on all points in V . To validate the training of this calibration we randomly split (50-50) the original test set to a training and test subsets. The calibration is learned over the training subset and evaluated on the test subset. The performance of the resulting scaled probabilities has been evaluated using both negative log likelihood (NLL) and the Brier score BID2 , which is simply the average L 2 distance between the predicted and the true probabilities. We provide here the table of the experiments of AES for softmax response and NN-distance now with standard errors. Due to computational complexity the standard error for all other methods has not been computed. Table 4 : E-AURC and % improvement for AES method on CIFAR-10, CIFAR-100, SVHN and ImageNET for various k values compared to the baseline method. All E-AURC values are multiplied by 10 3 for clarity. In Section 5 we motivated our method by dividing the domain X to \"easy points\" (green) and \"hard points\" (red). We demonstrated that the \"easy points\" have a phenomenon similar to overfitting, where at some point during training the E-AURC measured for \"easy points\" start degrading. This observation strongly motivates our strategy that extracts information from early stages of the training process that helps to recover uncertainty estimates of the easy points. Here, we extend this demonstration that previously was presented done with respect to the softmax \u03ba function. In Figures 3 and 4 we show plots similar to FIG0 (b,c) for the MC-dropout and NN-distance, respectively. It is evident that the overfitting occurs in all cases. but to a much lesser extent in the case of MC-dropout. This result is consistent with the results of the AES algorithm where E-AURC improvement over the MC-dropout was smaller compared to the improvements achieved for the other two methods. In the case of NN-distance a slight overfitting also affects the easy points, but the hard instances are affected much more severely. Thus, from this perspective in all three cases the proposed correction stratgey is potentially useful.(a) (b) Figure 3 : The E-AURC of MC-dropout on CIFAR-100 along training for 5000 points with highest confidence (a), and 5000 points with lowest confidence (b).(a) (b) Figure 4 : The E-AURC of NN-distance on CIFAR-100 along training for 5000 points with highest confidence (a), and 5000 points with lowest confidence (b)."
}