{
    "title": "S1CChZ-CZ",
    "content": "We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. \n\n We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. \n\n The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. \n\n We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming. Web and social media have become primary sources of information. Users' expectations and information seeking activities co-evolve with the increasing sophistication of these resources. Beyond navigation, document retrieval, and simple factual question answering, users seek direct answers to complex and compositional questions. Such search sessions may require multiple iterations, critical assessment, and synthesis BID19 .The productivity of natural language yields a myriad of ways to formulate a question BID3 . In the face of complex information needs, humans overcome uncertainty by reformulating questions, issuing multiple searches, and aggregating responses. Inspired by humans' ability to ask the right questions, we present an agent that learns to carry out this process for the user. The agent sits between the user and a backend QA system that we refer to as 'the environment'. We call the agent AQA, as it implements an active question answering strategy. AQA aims to maximize the chance of getting the correct answer by sending a reformulated question to the environment. The agent seeks to find the best answer by asking many questions and aggregating the returned evidence. The internals of the environment are not available to the agent, so it must learn to probe a black-box optimally using only question strings. The key component of the AQA agent is a sequence-to-sequence model trained with reinforcement learning (RL) using a reward based on the answer returned by the environment. The second component to AQA combines the evidence from interacting with the environment using a convolutional neural network to select an answer. We evaluate on a dataset of Jeopardy! questions, SearchQA BID7 . These questions are hard to answer by design because they use convoluted language, e.g., Travel doesn't seem to be an issue for this sorcerer & onetime surgeon; astral projection & teleportation are no prob (answer: Doctor Strange). Thus SearchQA tests the ability of AQA to reformulate questions such that the QA system has the best chance of returning the correct answer. AQA improves over the performance of a deep network built for QA, BiDAF BID28 , which has produced state-of-the-art results on multiple tasks, by 11.4% absolute F1, a 32% relative F1 improvement. Additionally, AQA outperforms other competitive heuristic query reformulation benchmarks. AQA defines an instance of machine-machine communication. One side of the conversation, the AQA agent, is trying to adapt its language to improve the response from the other side, the QA environment. To shed some light on this process we perform a qualitative analysis of the language generated by the AQA agent. By evaluating on MSCOCO , we find that the agent's question reformulations diverge significantly from natural language paraphrases. Remarkably, though, the agent is able to learn non-trivial and transparent policies. In particular, the agent is able to discover classic IR query operations such as term re-weighting, resembling tf-idf, and morphological simplification/stemming. A possible reason being that current machine comprehension tasks involve the ranking of short textual snippets, thus incentivizing relevance, more than deep language understanding.2 RELATED WORK Lin & Pantel (2001) learned patterns of question variants by comparing dependency parsing trees. BID6 showed that MT-based paraphrases can be useful in principle by providing significant headroom in oracle-based estimations of QA performance. Recently, BID1 used paraphrasing to augment the training of a semantic parser by expanding through the paraphrases as a latent representation. Bilingual corpora and MT have been used to generate paraphrases by pivoting through a second language. Recent work uses neural translation models and multiple pivots BID18 . In contrast, our approach does not use pivoting and is, to our knowledge, the first direct neural paraphrasing system. BID27 propose phrase-based paraphrasing for query expansion. In contrast with this line of work, our goal is to generate full question reformulations while optimizing directly the end-to-end target performance metrics. Reinforcement learning is gaining traction in natural language understanding across many problems. For example, BID20 use RL to learn control policies for multi-user dungeon games where the state of the game is summarized by a textual description, and BID14 use RL for dialogue generation. Policy gradient methods have been investigated recently for MT and other sequence-to-sequence problems. They alleviate limitations inherent to the word-level optimization of the cross-entropy loss, allowing the use of sequence-level reward functions, like BLEU. Reward functions based on language models and reconstruction errors are used to bootstrap MT with fewer resources BID33 . RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training BID26 . We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include , who train a semantic parser to query a knowledge base, and BID29 who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning. The work of BID21 is most related to ours. They identify a document containing an answer to a question by following links on a graph. Evaluating on a set of questions from the game Jeopardy!, they learn to walk the Wikipedia graph until they reach the predicted answer. In a follow-up, BID22 improve document retrieval with an approach inspired by relevance feedback in combination with RL. They reformulate a query by adding terms from documents retrieved from a search engine for the original query. Our work differs in that we generate complete sequence reformulations rather than adding single terms, and we target question-answering rather than document retrieval. Active QA is also related to recent research on fact-checking: BID32 propose to perturb database queries in order to estimate the support of quantitative claims. In Active QA questions are perturbed semantically with a similar purpose, although directly at the surface natural language form. Figure 1 shows the Active Question Answering (AQA) agent-environment setup. The AQA model interacts with a black-box environment. AQA queries it with many versions of a question, and finally returns the best of the answers found. An episode starts with an original question q 0 . The agent then Figure 1 : The AQA agent-environment setup. In the downward pass the agent reformulates the question and sends variants to the QA system. In the upward pass the final answer is selected. generates a set of reformulations DISPLAYFORM0 . These are sent to the environment which returns answers DISPLAYFORM1 . The selection model then picks the best from these candidates. For the QA environment, we use a competitive neural question answering model, BiDirectional Attention Flow (BiDAF) BID28 . BiDAF is an extractive QA system, it selects answers from contiguous spans of a given document. Given a question, the environment returns an answer and, during training, a reward. The reward may be any quality metric for the returned answer, we use token-level F1 score. Note that the reward for each answer a i is computed against the original question q 0 . We assume that the environment is opaque; the agent has no access to its parameters, activations or gradients. This setting enables one, in principle, to also interact with other information sources, possibly providing feedback in different modes such as images and structured data from knowledge bases. However, without propagating gradients through the environment we lose information, feedback on the quality of the question reformulations is noisy, presenting a challenge for training. The reformulator is a sequence-to-sequence model, as is popular for neural machine translation. We build upon the implementation of BID2 . The major departure from the standard MT setting is that our model reformulates utterances in the same language. Unlike in MT, there is little high quality training data available for monolingual paraphrasing. Effective training of highly parametrized neural networks relies on an abundance of data. We address this challenge by first pre-training on a related task, multilingual translation, and then using signals produced during the interaction with the environment for adaptation. During training, we have access to the reward for the answer returned for each reformulation q i . However, at test time we must predict the best answer a * . The selection model selects the best answer from the set {a i } N i=1 observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants. We use pre-trained embeddings for the tokens of query, rewrite, and answer. For each, we add a 1-dimensional CNN followed by max-pooling. The three resulting vectors are then concatenated and passed through a feed-forward network which produces the output. We train a model on the training set for the QA task at hand, see Section 5.4 for details. Afterwards, BiDAF becomes the black-box environment and its parameters are not updated further. In principle, we could train both the agent and the environment jointly to further improve performance. However, this is not our desired task: our aim is for the agent to learn to communicate using natural language with an environment over which is has no control. For a given question q 0 , we want to return the best possible answer a * , maximizing a reward a * = argmax a R(a|q 0 ). Typically, R is the token level F1 score on the answer. The answer a = f (q) is an unknown function of a question q, computed by the environment. The reward is computed with respect to the original question q 0 while the answer is provided for q. The question is generated according to a policy \u03c0 \u03b8 where \u03b8 are the policy's parameters q \u223c \u03c0 \u03b8 ( \u00b7 |q 0 ). The policy, in this case, a sequence-to-sequence model, assigns a probability DISPLAYFORM0 to any possible question q = w 1 , . . . , w T , where T is the length of q with tokens w t \u2208 V from a fixed vocabulary V . The goal is to maximize the expected reward of the answer returned under the policy, DISPLAYFORM1 We optimize the reward directly with respect to parameters of the policy using Policy Gradient methods BID30 . The expected reward cannot be computed in closed form, so we compute an unbiased estimate with Monte Carlo sampling, DISPLAYFORM2 To compute gradients for training we use REINFORCE BID31 , DISPLAYFORM3 This estimator is often found to have high variance, leading to unstable training BID9 . We reduce the variance by subtracting the following baseline reward: DISPLAYFORM4 This expectation is also computed by sampling from the policy given q 0 . We often observed collapse onto a sub-optimal deterministic policy. To address this we use entropy regularization DISPLAYFORM5 This final objective is: DISPLAYFORM6 where \u03bb is the regularization weight. Unlike the reformulation policy, we train the answer with either beam search or sampling. We can produce many rewrites of a single question from our reformulation system. We issue each rewrite to the QA environment, yielding a set of (query, rewrite, answer) tuples from which we need to pick the best instance. We train another neural network to pick the best answer from the candidates. We frame the task as binary classification, distinguishing between above and below average performance. In training, we compute the F1 score of the answer for every instance. If the rewrite produces an answer with an F1 score greater than the average score of the other rewrites the instance is assigned a positive label. We ignore questions where all rewrites yield equally good/bad answers. We evaluated FFNNs, LSTMs, and CNNs and found that the performance of all systems was comparable. We choose a CNN which offers good computational efficiency and accuracy (cf. 3.3). We pre-train the policy by building a paraphrasing Neural MT model that can translate from English to English. While parallel corpora are available for many language pairs, English-English corpora are scarce. We first produce a multilingual translation system that translates between several languages BID11 . This allows us to use available bilingual corpora. Multilingual training requires nothing more than adding two special tokens to every line which indicate the source and target languages. The encoder-decoder architecture of the translation model remains unchanged. As BID11 show, this model can be used for zero-shot translation, i.e. to translate between language pairs for which it has seen no training examples. For example, after training English-Spanish, English-French, French-English, and Spanish-English the model has learned a single encoder that encodes English, Spanish, and French and a decoder for the same three languages. Thus, we can use the same model for French-Spanish, Spanish-French and also English-English translation by adding the respective tokens to the source. BID11 note that zero-shot translation usually performs worse than bridging, an approach that uses the model twice: first, to translate into a pivot language, then into the target language. However, the performance gap can be closed by running a few training steps for the desired language pair. Thus, we first train on multilingual data, then on a small corpus of monolingual data. We train BiDAF directly on the SearchQA training data. We join snippets to form the context from which BiDAF selects answer spans. For performance reasons, we limit the context to the top 10 snippets. This corresponds to finding the answer on the first page of Google results. The results are only mildly affected by this limitation, for 10% of the questions, there is no answer in this shorter context. These data points are all counted as losses. We trained with the Adam optimizer for 4500 steps, using learning rate 0.001, batch size 60. For the pre-training of the reformulator, we use the multilingual United Nations Parallel Corpus v1.0 BID34 . This dataset contains 11.4M sentences which are fully aligned across six UN languages: Arabic, English, Spanish, French, Russian, and Chinese. From all bilingual pairs, we produce a multilingual training corpus of 30 language pairs. This yields 340M training examples which we use to train the zero-shot neural MT system BID11 . We tokenize our data using 16k sentence pieces. 1 Following BID2 we use a bidirectional LSTM as the encoder and a 4-layer stacked LSTM with attention as the decoder. The model converged after training on 400M instances using the Adam optimizer with a learning rate of 0.001 and batch size of 128. The model trained as described above has poor quality. For example, for the question What month, day and year did Super Bowl 50 take place? , the top rewrite is What month and year goes back to the morning and year?. To improve quality, we resume training on a smaller monolingual dataset, extracted from the Paralex database of question paraphrases BID8 .2 Unfortunately, this data contains many noisy pairs. We filter many of these pairs out by keeping only those where the Jaccard coefficient between the sets of source and target terms is above 0.5. Further, since the number of paraphrases for each question can vary significantly, we keep at most 4 paraphrases for each question. After processing, we are left with about 1.5M pairs out of the original 35M. The refined model has visibly better quality than the zero-shot one; for the example question above it generates What year did superbowl take place?. We also tried training on the monolingual pairs alone. As in BID11 , the quality was in between the multilingual and refined models. After pre-training the reformulator, we switch the optimizer from Adam to SGD and train for 100k RL steps of batch size 64 with a low learning rate of 0.001. We use an entropy regularization weight of \u03bb = 0.001. For a stopping criterion, we monitor the reward from the best single rewrite, generated via greedy decoding, on the validation set. In contrast to our initial training which we ran on GPUs, this training phase is dominated by the latency of the QA system and we run inference and updates on CPU and the BiDAF environment on GPU. For the selection model we use supervised learning: first, we train the reformulator, then we generate N = 20 rewrites for each question in the SearchQA training and validation sets. After sending these to the environment we have about 2M (question, rewrite, answer) triples. We remove queries where all rewrites yield identical rewards, which removes about half of the training data. We use pre-trained 100-dimensional embeddings BID23 for the tokens. Our CNN-based selection model encodes the three strings into 100-dimensional vectors using a 1D CNN with kernel width 3 and output dimension 100 over the embedded tokens, followed by max-pooling. The vectors are then concatenated and passed through a feed-forward network which produces the binary output, indicating whether the triple performs below or above average, relative to the other reformulations and respective answers. We use the training portion of the SearchQA data thrice, first for the initial training of the BiDAF model, then for the reinforcement-learning based tuning of the reformulator, and finally for the training of the selector. We carefully monitored that this didn't cause severe overfitting. BiDAF alone has a generalization gap between the training and validation set errors of 3.4 F1. This gap remains virtually identical after training the rewriter. After training the CNN, AQA-Full has a slightly larger gap of 3.9 F1. We conclude that training AQA on BiDAF's training set causes very little additional overfitting. We use the test set only for evaluation of the final model. As a baseline, we report the results of the modified pointer network, called Attention Sum Reader (ASR), developed for SearchQA BID7 .We also report the performance of the BiDAF environment used without the reformulator to answer the original question. We evaluate against several benchmarks. First, following BID12 , we implement a system (MI-SubQuery) that generates reformulation candidates by enumerating all subqueries of the original SearchQA query and then keeps the top N ranked by mutual information.3 From this set, we pick the highest scoring one as the top hypothesis to be used as a single rewrite. We also use the whole set to train a CNN answer selector for this specific source of rewrites. In this way, we can compare systems fairly both in single prediction or ensemble prediction modes. Additionally, we evaluate against another source of reformulations: the zero-shot monolingual NMT system trained on the U.N. corpus and Paralex (Base-NMT), without reinforcement learning. As with the MI-SubQuery benchmark, we evaluate the Base-NMT system both as a single reformulation predictor and as a source of N best rewrites, for which we train a dedicated CNN answer selector. We also report human performance on SearchQA, based on a sample of the test set, from BID7 . We evaluate several variants of AQA. For each query q in the evaluation we generate a list of reformulations q i , for i = 1 . . . N , from the AQA reformulator trained as described in Section 4. We set N = 20 in these experiments, the same value is used for the benchmarks. In AQA TopHyp we use the top hypothesis generated by the sequence model, q 1 . In AQA Voting we use BiDAF scores for a heuristic weighted voting scheme to implement deterministic selection. Let a be the answer returned by BiDAF for query q, with an associated score s(a). We pick the answer according to argmax a a =a s(a ). In AQA MaxConf we select the answer with the single highest BiDAF score across question reformulations. Finally, AQA CNN identifies the complete system with the learned CNN model described in Section 3. Table 1 shows the results. We report exact match (EM) and F1 metrics, computed on token level between the predicted answer and the gold answer. We present results on the full validation and test sets (referred to as n-gram in BID7 ). Overall, SearchQA appears to be harder than other recent QA tasks such as SQuAD BID25 , for both machines and humans. BiDAF's performance drops by 40 F1 points on SearchQA compared to SQuAD. However, BiDAF is still competitive on SeachQA, improving over the Attention Sum Reader network by 13.7 F1 points. DISPLAYFORM0 Using the top hypothesis already yields an improvement of 2.2 F1 on the test set. This demonstrates that even the reformulator alone is capable to produce questions more easily answered by the environment. When generating a single prediction, both MI-SubQuery and Base-NMT benchmarks perform worse than BiDAF. Heuristic selection via both Voting and Max Conf yields a further performance boost. Both heuristics draw upon the intuition that when BiDAF is confident in its answer it is more likely to be correct, and that multiple instances of the same answer provide positive evidence (for MaxConf, the max operation implicitly rewards having an answer scored with respect to multiple questions). Finally, a trained selection function improves performance further, yielding an absolute increase of 11.4 F1 points (32% relative) over BiDAF with the original questions. In terms of exact match score, this more than closes half the gap between BiDAF and human performance. The benchmarks improve considerably when they generate N candidates, and paired with a dedicated CNN selector. This is not surprising as it provides an ensemble prediction setup. However, the AQA CNN system outperforms both MI-SubQuery and Base-NMT in all conditions by about 3%.Finally, we consider the maximum performance possible that could be achieved by picking the answer with the highest F1 score from the set of those returned for all available reformulations. Here we find that the different sources of rewrites provide comparable headroom: the oracle Exact Match is near 50, while the oracle F1 is close to 58. The AQA agent can learn several types of sub-optimal policies. For example, it can converge to deterministic policies by learning to emit the same, meaningless, reformulation for any input question. This strategy can lead to local optima because the environment has built in strong priors on what looks like a likely answer, even ignoring the input question. Hence, convergence to non-negligible performance is easy. Entropy regularization typically fixes this behavior. Too much weight on the entropy regularizer, on the other hand, might yield random policies. A more competitive sub-optimal policy is one that generates minimal changes to the input, in order to stay close to the original question. This is a successful strategy because the environment has been trained on the original questions alone, which leads to baseline performance. It seems quite remarkable then that AQA is able to learn non-trivial reformulation policies, that differ significantly from all of the above. One can think of the policy as a language for formulating questions that the agent has developed while engaging in a machine-machine communication with the environment. In this section we look deeper into the agent's language. We analyze input questions and reformulations on the development partition of SearchQA to gain insights on how the agent's language evolves during training via policy gradient. It is important to note that in the SearchQA dataset the original Jeopardy! clues have been preprocessed by lowercasing and stop word removal. The resulting preprocessed clues that form the sources (inputs) for the sequence-to-sequence reformulation model resemble more keyword-based search queries than grammatical questions. For example, the clue Gandhi was deeply influenced by this count who wrote \"War and Peace\" is simplified to gandhi deeply influenced count wrote war peace. The (preprocessed) SearchQA questions contain 9.6 words on average. They contain few repeated terms, computed as the mean term frequency (TF) per question. The average is 1.03, but for most of the queries (75%) TF is 1.0. We also compute the median document frequency (DF) per query, where the document is the context from which the answer is selected, as a measure of how informative a term is. 4 As another measure of query performance, we also compute Query Clarity (QC) BID5 .5 FIG0 summarizes statistics of the questions and rewrites. We first consider the top hypothesis generated by the pre-trained NMT reformulation system, before reinforcement learning (Base-NMT). The Base-NMT rewrites differ greatly from their sources. They are shorter, 6.3 words on average, and have even fewer repeated terms (1.01). Interestingly, these reformulations are mostly syntactically well-formed questions. For example, the clue above becomes Who influenced count wrote war?. 6 Base-NMT improves structural language quality by properly reinserting dropped function words and wh-phrases. We also verified the increased fluency by using a large language model and found that the Base-NMT rewrites are 50% more likely than the original questions. While more fluent, the Base-NMT rewrites involve lower DF terms. This is probably due to a domain mismatch between SearchQA and the NMT training corpus. The query clarity of the Base-NMT rewrites is also degraded as a result of the transduction process. We next consider the top hypothesis generated by the AQA question reformulator (AQA-QR) after the policy gradient training. The AQA-QR rewrites are those whose corresponding answers are evaluated as AQA TopHyp in Table 1 . These single rewrites alone outperform the original SearchQA queries by 2% on the test set. We analyze the top hypothesis instead of the final output of the full AQA agent to avoid confounding effects from the answer selection step. These rewrites look different from both the Base-NMT and the SearchQA ones. For the example above AQA-QR's top hypothesis is What is name gandhi gandhi influence wrote peace peace?. Surprisingly, 99.8% start with the prefix What is name. The second most frequent is What country is (81 times), followed by What is is (70) and What state (14). This is puzzling as it occurs in only 9 Base-NMT rewrites, and never in the original SearchQA questions. We speculate it might be related to the fact that virtually all answers involve names, of named entities (Micronesia) or generic concepts (pizza).AQA-QR's rewrites seem less fluent than both the SearchQA and the Base-MT counterparts. In terms of language model probability, they are less likely than both SearchQA and Base-NMT. However, they have more repeated terms (1.2 average TF), are significantly longer (11.9) than in Base-NMT and contain more informative context terms than SearchQA questions (lower DF). Also, the translation process does not affect query clarity much. Finally, we find that AQA-QR's reformulations contain morphological variants in 12.5% of cases. The number of questions that contain multiple tokens with the same stem doubles from SearchQA to AQA-QR. Singular forms are preferred over plurals. Morphological simplification is useful because it increases the chance that a word variant in the question matches the context. We also investigate the general paraphrasing abilities of our model, focusing on the relation between paraphrasing quality and QA quality. To tease apart the relationship between paraphrasing and reformulation for QA we evaluated 3 variants of the reformulator:Base-NMT This is the model used to initialize RL training of the agent. Trained first on the multilingual U.N. corpus, then on the Paralex corpus, as detailed in Section 5.2. Base-NMT-NoParalex This is the model above trained solely on the multilingual U.N. corpus, without the Paralex monolingual corpus. Base-NMT+Quora This is the same as Base-NMT, additionally trained on the Quora dataset 7 which contains 150k duplicate questions. Following BID24 , we evaluate all models on the MSCOCO 8 ) validation set (VAL2014). This dataset consists of images with 5 captions each, of which we select a random one as the source and the other four as references. We use beam search, to compute the top hypothesis and report uncased, moses-tokenized BLEU using multeval 9 BID4 . Please note, that the MSCOCO data is only used for evaluation purposes. Examples of all systems can be found in Appendix C.The Base-NMT model performs at 11.4 BLEU (see Table 1 for the QA eval numbers). In contrast, Base-NMT-NoParalex performs poorly at 5.0 BLEU. Limiting training to the multilingual data alone also degrades QA performance: the scores of the Top Hypothesis are at least 5 points lower in all metrics and CNN scores are 2-3 points lower. By training on additional monolingual data, the Base-NMT+Quora model improves BLEU score slightly to 11.6. End-to-end QA performance also improves marginally, the maximum delta with respect to Base-NMT under all conditions is +0.5 points, but the difference is not statistically significant. Thus, adding the Quora training does not have a significant effect. This might be due to the fact that most of the improvement is captured by training on the larger Paralex data set. Improving raw paraphrasing quality as well as reformulation fluency helps AQA up to a point. However, they are only partially aligned with the main task, which is QA performance. The AQA-QR reformulator has a BLEU score of 8.6, well below both Base-NMT models trained on monolingual data. Yet, AQA-QR significantly outperforms all others in the QA task. Training the agent starting from the Base-NMT+Quora model yielded comparable results as starting from Base-NMT. Recently, BID13 trained chatbots that negotiate via language utterances in order to complete a task. They report that the agent's language diverges from human language if there is no incentive for fluency in the reward function. Our findings seem related. The fact that the questions reformulated by AQA do not resemble natural language is not due to the keyword-like SearchQA input questions, because Base-NMT is capable of producing more fluent questions from the same input. AQA learns to re-weight terms by focusing on informative (lower document frequency), query-specific (high query clarity), terms while increasing term frequency (TF) via duplication. At the same time it learns to modify surface forms in ways akin to stemming and morphological analysis. Some of the techniques seem to adapt to the specific properties of current deep QA architectures such as character-based modeling and attention. Sometimes AQA learns to generate semantically nonsensical, novel, surface term variants; e.g., it might transform the adjective dense to densey. The only justification for this is that such forms can be still exploited by the character-based BiDAF question encoder. Finally, repetitions can directly increase the chances of alignment in the attention components. We hypothesize that, while there is no incentive for the model to use human language due to the nature of the task, AQA learns to ask BiDAF questions by optimizing a language that increases the likelihood of BiDAF ranking better the candidate answers. BID10 argue that reading comprehension systems are not capable of significant language understanding and fail easily in adversarial settings. We speculate that current machine comprehension tasks involve mostly pattern matching and relevance modeling. As a consequence deep QA systems might implement sophisticated ranking systems trained to sort snippets of text from the context. As such, they resemble document retrieval systems which incentivizes the (re-)discovery of IR techniques, such as tf-idf re-weighting and stemming, that have been successful for decades BID0 . We propose a new framework to improve question answering. We call it active question answering (AQA), as it aims to improve answering by systematically perturbing input questions. We investigated a first system of this kind that has three components: a question reformulator, a black box QA system, and a candidate answer aggregator. The reformulator and aggregator form a trainable agent that seeks to elicit the best answers from the QA system. Importantly, the agent may only query the environment with natural language questions. Experimental results prove that the approach is highly effective and that the agent is able to learn non-trivial and somewhat interpretable reformulation policies. For future work, we will continue developing active question answering, investigating the sequential, iterative aspects of information seeking tasks, framed as end-to-end RL problems, thus, closing the loop between the reformulator and the selector."
}