{
    "title": "H1gBhkBFDH",
    "content": "Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, however, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Fourier theory). In this paper we lift these limitations and propose a modular framework for the design and implementation of G-CNNs for arbitrary Lie groups. In our approach the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines that is defined on the Lie algebra. This leads to a flexible framework that enables localized, atrous, and deformable convolutions in G-CNNs by means of respectively localized, sparse and non-uniform B-spline expansions. The impact and potential of our approach is studied on two benchmark datasets: cancer detection in histopathology slides (PCam dataset) in which rotation equivariance plays a key role and facial landmark localization (CelebA dataset) in which scale equivariance is important. In both cases, G-CNN architectures outperform their classical 2D counterparts and the added value of atrous and localized group convolutions is studied in detail. Group convolutional neural networks (G-CNNs) are a class of neural networks that are equipped with the geometry of groups. This enables them to profit from the structure and symmetries in signal data such as images (Cohen & Welling, 2016) . A key feature of G-CNNs is that they are equivariant with respect to transformations described by the group, i.e., they guarantee predictable behavior under such transformations and are insensitive to both local and global transformations on the input data. Classical CNNs are a special case of G-CNNs that are equivariant to translations and, in contrast to unconstrained NNs, they make advantage of (and preserve) the basic structure of signal data throughout the network (LeCun et al., 1990) . By considering larger groups (i.e. considering not just translation equivariance) additional geometric structure can be utilized in order to improve performance and data efficiency (see G-CNN literature in Sec. 2). Part of the success of G-CNNs can be attributed to the lifting of feature maps to higher dimensional objects that are generated by matching kernels under a range of poses (transformations in the group). This leads to a disentanglement with respect to the pose and together with the group structure this enables a flexible way of learning high level representations in terms of low-level activated neurons observed in specific configurations, which we conceptually illustrate in Fig. 1 . From a neuro-psychological viewpoint, this resembles a hierarchical composition from low-to high-level features akin to the recognition-by-components model by Biederman (1987) , a viewpoint which is also adopted in work on capsule networks (Hinton et al., 2011; Sabour et al., 2017) . In particular in ) the group theoretical connection is made explicit with equivariant capsules that provide a sparse index/value representation of feature maps on groups (Gens & Domingos, 2014) . In G-CNNs feature maps are lifted to the high-dimensional domain of the group G in which features are disentangled with respect to pose/transformation parameters. G-convolution kernels then learn to recognize high-level features in terms of patterns of relative transformations, described by the group structure. This is conceptually illustrated for the detection of faces, which in the SE(2) case are considered as a pattern of lines in relative positions and orientations, or in the R 2 R + case as blobs/circles in relative positions and scales. Representing low-level features via features maps on groups, as is done in G-CNNs, is also motivated by the findings of Hubel & Wiesel (1959) and Bosking et al. (1997) on the organization of orientation sensitive simple cells in the primary visual cortex V1. These findings are mathematically modeled by sub-Riemannian geometry on Lie groups (Petitot, 2003; Citti & Sarti, 2006; Duits et al., 2014) and led to effective algorithms in image analysis (Franken & Duits, 2009; Bekkers et al., 2015b; Favali et al., 2016; Duits et al., 2018; Baspinar, 2018) . In recent work Montobbio et al. (2019) show that such advanced V1 modeling geometries emerge in specific CNN architectures and in Ecker et al. (2019) the relation between group structure and the organization of V1 is explicitly employed to effectively recover actual V1 neuronal activities from stimuli by means of G-CNNs. Figure 2: The Log-map allows us to map elements from curved manifolds such as the 2-sphere to a flat Euclidean tangent space. For Lie groups the Logmap is analytic, globally defined, and it provides us with a flexible tool to define group convolution kernels via Bsplines. In our Lie group context the 2-sphere is treated as a quotient group SO(3)/SO(2). Technical details are given in Sec. 3 and App. B. that are the semi-direct product of the translation group with a Lie group H. As such, only a few core definitions about the Lie group H (group product, inverse, Log, and action on R d ) need to be implemented in order to build full G-CNNs that are locally equivariant to the transformations in H. The impact and potential of our approach is studied on two datasets in which respectively rotation and scale equivariance plays a key role: cancer detection in histopathology slides (PCam dataset) and facial landmark localization (CelebA dataset). In both cases G-CNNs out-perform their classical 2D counterparts and the added value of atrous and localized G-convolutions is studied in detail. G-CNNs The introduction of G-CNNs to the machine learning community by Cohen & Welling (2016) led to a growing body of G-CNN literature that consistently demonstrates an improvement of G-CNNs over classical CNNs. It can be roughly divided into work on discrete G-CNNs (Cohen & Welling, 2016; Dieleman et al., 2016; Winkels & Cohen, 2018; Worrall & Brostow, 2018; Hoogeboom et al., 2018) , regular continuous G-CNNs (Oyallon & Mallat, 2015; Bekkers et al., 2015a; Zhou et al., 2017; Marcos et al., 2017) and steerable continuous G-CNNs (Cohen et al., 2018b; Worrall et al., 2017; Kondor & Trivedi, 2018; Thomas et al., 2018; Esteves et al., 2018a; Andrearczyk et al., 2019) . Since 3D rotations can only be sampled in very restrictive ways (without destroying the group structure) the construction of 3D roto-translation G-CNNs is limited. In order to avoid having to sample all together, steerable (G-)CNNs can be used. These are specialized G-CNNs in which the kernels are expanded in circlar/spherical harmonics and computations take place using the basis coefficients only (Chirikjian & Kyatkin, 2000; Franken, 2008; Almsick, 2007; Skibbe & Reisert, 2017) . The latter approach is however only possible for unimodular groups such as roto-translations. Scale equivariance In this paper we experiment with scale-translation G-CNNs, which is the first direct application of G-CNNs to achieve equivariance beyond roto-translations. Scale equivariance is however addressed in several settings (Henriques & Vedaldi, 2017; Esteves et al., 2018b; Marcos et al., 2018; Tai et al., 2019; Worrall & Welling, 2019; Jaderberg et al., 2015; Li et al., 2019) , of which (Worrall & Welling, 2019 ) is most related. There, scale-space theory and semi-group theory is used to construct scale equivariant layers that elegantly take care of moving band-limits due to rescaling. Although our work differs in several ways (e.g. non-learned lifting layer, discrete group convolutions via atrous kernels, semi-group theory), the first two layers of deep scale-space networks relate to our lifting layer by treating our B-splines as a superposition of dirac deltas transformed under the semi-group action of (Worrall & Welling, 2019) , as we show in App. C.1. Li et al. (2019) achieve scale invariance by sharing weights among kernels with different dilation rates. Their approach can be considered a special case of our proposed B-spline G-CNNs with kernels that do not encode scale interactions. Related work by Tai et al. (2019) and Henriques & Vedaldi (2017) relies on the same Lie group principles as we do in this paper (the Log map) to construct convenient coordinate systems, such as log-polar coordinates Esteves et al. (2018b) , to handle equivariance. Such methods are however generally not translation equivariant and do not deal with local symmetries as they act globally on feature maps, much like spatial transformer networks (Jaderberg et al., 2015) . The current work can be seen as a generalization of the B-spline based SE(2) CNNs of Bekkers et al. (2015a; , see Sec. 3.3. Closely related is also the work of in which B-splines are used to generalize CNNs to non-Euclidean data (graphs). There it is proposed to perform convolution via B-spline kernels on R d that take as inputs vectors u(i, j) \u2208 R d that relate any two points i, j \u2208 G in the graph to each other. How u(i, j) is constructed is left as a design choice, however, in this is typically done by embedding the graph in an Euclidean space where points relate via offset vectors. In our work on Lie G-CNNs, two points g, g \u2208 G in the Lie group G relate via the logarithmic map u(g, g ) = Log g \u22121 g . Another related approach in which convolutions take place on manifolds in terms of \"offset vectors\" is the work by Cohen et al. (2019) . There, points relate via the exponential map with respect to gauge frames rather than the left-invariant vector fields as in this paper, see App. C.2. The following describes the essential tools required for deriving a generic framework G-CNNs. Although we treat groups mostly in an abstract setting, we here provide examples for the rototranslation group and refer to App. B for more details, explicit formula's, and figures for several groups. Implementations are available at https://github.com/ebekkers/gsplinets. Group A group is defined by a set G together with a binary operator \u00b7, the group product, that satisfies the following axioms: Closure: For all h, g \u2208 G we have h \u00b7 g \u2208 G; Identiy: There exists an identity element e; Inverse: for each g \u2208 G there exists an inverse element g Lie group and Lie algebra If furthermore the group has the structure of a differential manifold and the group product and inverse are smooth, it is called a Lie group. The differentiability of the group induces a notion of infinitesimal generators (see also the exponential map below), which are elements of the Lie algebra g. The Lie algebra consists of a vector space (of generators), that is typically identified with the tangent space T e (G) at the identity e, together with a bilinear operator called the Lie bracket. In this work the Lie bracket is not of interest and we simply say g = T e (G). Exponential and logarithmic map We expand vectors in g in a basis {A i } n i=1 and write A = n i a i A i \u2208 g, with components a = (a 1 , . . . , a n ) \u2208 R n . This allows us to identify the Lie algebra with R n . Lie groups come with a logarithmic map that maps elements from the typically non-flat manifolds G to the flat Euclidean vector space g via A = Log g (conversely g = Exp A), see Fig. 2 . Semi-direct product groups In this paper we specifically consider (affine) Lie groups of type G = R d H that are the semi-direct product of the translation group R d with a Lie group H that acts on For example the special Euclidean motion group SE(2) is constructed by choosing H = SO(2), the group of 2 \u00d7 2 rotation matrices with matrix multiplication as the group product. The group product of G is then given by , with x 1 , x 2 \u2208 R 2 and R \u03b81 , R \u03b82 \u2208 SO(2) rotation matrices parameterized by a rotation angle \u03b8 i , and in which rotations act on vectors in R 2 simply by matrix vector multiplication. X) that transform functions (or feature maps) f \u2208 L 2 (X) on some space X as representations of a group G if they share the group structure via f )(x), with \u2022 denoting function composition. Thus, a concatenation of two such transformations, parameterized by g 1 and g 2 , can be described by a single transformation parameterized by Equivariance An operator \u03a6 : is said to be equivariant to G when it satisfies Equivariance of artificial neural networks (NNs) with respect to a group G is a desirable property as it guarantees that no information is lost when applying transformations to the input, the information is merely shifted to different locations in the network. It turns out that if we want NNs to be equivariant, then our only option is to use layers whose linear operator is defined by group convolutions. We summarize this in Thm. 1. To get there, we start with the traditional definition of NN layers via with x \u2208 X the input vector, K w : X \u2192 Y a linear map parameterized by a weight vector w, with b \u2208 Y a bias term, and \u03c6 a point-wise non-linearity. In classical neural networks X = R Nx and Y = R Ny are Euclidean vector spaces and the linear map K w = R Ny\u00d7Nx is a weight matrix. In this work we instead focus on structured data and consider feature maps on some domain X as functions f : X \u2192 R N , the space of which we denote with (L 2 (X)) Ny are the spaces of multi-channel feature maps, b \u2208 R Ny , and K w : X \u2192 Y is a kernel operator. Now when we constrain the linear operator K w to be equivariant under transformations in some group G we arrive at group convolutional neural networks. This is formalized in the following theorem on equivariant maps between homogeneous spaces (see (Duits & Burgeth, 2007; Kondor & Trivedi, 2018; for related statements). be linear and bounded, let X, Y be homogeneous spaces on which Lie group G act transitively, and d\u00b5 X a Radon measure on X, then 2. under the equivariance constraint of Eq. (3) the map is defined by a one-argument kernel for any g y \u2208 G such that y = g y y 0 for some fixed origin y 0 \u2208 Y , 3. if Y \u2261 G/H is the quotient of G with H = Stab G (y 0 ) = {g \u2208 G|g y 0 = y 0 } then the kernel is constrained via Proof. See App. A Corollary 1. If X = R d is a homogeneous space of an affine Lie group G = R d H and d\u00b5 X (x) = dx is the Lebesgue measure on R d then the kernel front-factor simplifies to with | det h| denoting the determinant of the matrix representation of h, for any g = (x, h) \u2208 G. If X = G and d\u00b5 X (x) is a Haar measure on G then In view of Thm. 1 we see that standard CNNs are a special case of G-CNNs that are equivariant to translations. In this case the domain of the feature maps X = R d coincides with the space of translation vectors in the translation group G = (R d , +). It is well known that if we want the networks to be translation and rotation equivariant (G = R d SO(d)), but stick to planar feature maps, then the kernels should be rotation invariant, which of course limits representation power. This constraint is due Thm. 1 item 3, since the domain of such features maps is a quotient group Thus, in order to maximize representation power (without constrains on k) the feature maps should be lifted to the higher dimensional domain of the group itself (i.e. Y = G). We therefore propose to build G-CNNs with the following 3 types of layers (illustrated in Fig. 1 ): In this layer K is defined by lifting correlations , with g = (x, h), which by splitting of the representation (Eq. (2)) can be written as k (x) the transformed kernel. Lifting correlations thus match a kernel with the input feature map under all possible transformations in G. \u2022 Group correlation layer (X = G, Y = G): In this case K is defined by group correlations with d\u00b5(g) a Haar measure on G. We can again split this cross-correlation into a transformation of K followed by a spatial cross-correlation via with x , h \u22121 \u00b7h ) the convolution kernel transformed by h \u2208 H and in which we overload R d to indicate cross-correlation on the \u2022 In this case K is a linear projection defined by where we simply integrate over H instead of using a kernel that would otherwise be constant over H and spatially isotropic with respect to H. App. B provides explicit formula's for these layers for several groups. E.g., in the SE(2) we get: Central in our formulation of G-CNNs is the transformation of convolution kernels under the action of H as described above in Eqs. (6) and (7) in the continuous setting. For the implementation of G-CNNs the kernels and their transformations need to be sampled on a discrete grid. We expand on the idea's in (Bekkers et al., 2015a; Weiler et al., 2018b) to express the kernels in an analytic form which we can then sample under arbitrary transformations in G to perform the actual computations. In particular we generalize the approach of Bekkers et al. (2015a; to expand group correlation kernels in a basis of shifted cardinal B-splines, which are localized polynomial functions on R d with finite support. In (Bekkers et al., 2015a; , B-splines on R d could be used to construct kernels on SE(2) by identifying the group with the space of positions and orientations and simply using periodic splines on the 1D orientation axis S 1 . However, in order to construct B-splines on arbitrary Lie groups, we need a generalization. In the following we propose a new definition of B-splines on Lie groups H which enables us to construct the kernels on R d H that are required in the G-correlations (Eq. (7)). Definition 1 (Cardinal B-spline on R n ). The 1D cardinal B-Spline of degree n be is defined as where Cardinal B-splines are piece-wise polynomials and are localized on support . Functions can be expanded in a basis of shifted cardinal B-splines, which we simply refer to as B-splines. Definition 2 (B-splines on R n ). A B-spline is a function f : R d \u2192 R expanded in a basis that consists of shifted and scaled copies of the cardinal B-spline and is fully characterized by spline degree n, scale s x , set of centers forms a uniform grid on R d , in which the distance x j \u2212 x i between neighbouring centers x i , x j \u2208 R d is constant along each axis and equal to s x . Definition 3 (B-splines on Lie group H). A B-spline on H is a function f : H \u2192 R expanded in a basis that consists of shifted (by left multiplication) and scaled copies of the cardinal B-spline with h \u2208 H and Log : H \u2192 h the logarithmic map on H. The B-spline is fully characterized by the spline degree n, scale s h , set of centers with h i \u2208 H and corresponding coefficients c = (c 1 , c 2 , . . . , c N ) T \u2208 R N . The spline is called uniform if the distance Log h Examples of B-splines on Lie groups H are given in Fig. 3 . In this paper we choose to expand convolution kernels on G = R d H as the tensor product of B-splines on R d and H respectively and obtain functions f : Note that that one could also directly define B-splines on G via (12), however, this splitting ensures we can use a regular Cartesion grid on the R d part. In our experiments we use B-splines as in (13) and consider the coefficients c as trainable parameters and the centers (x i and/or h i ) and scales (s x and/or s h ) are fixed by design. Some design choices are the following (and illustrated in Fig. 4 ). Global vs localized uniform B-splines The notion of a uniform B-spline globally covering H exists only for a small set of Lie groups, e.g. for any 1D group and abelian groups, and it is not possible to construct uniform B-splines on Lie groups in general due to non-zero commutators. Nevertheless, we find that it is possible to construct approximately uniform B-splines on compact groups either by constructing a grid of centers {h i } N i=1 on H that approximately uniformly covers H, e.g. by using a repulsion model in which Log h \u22121 i \u00b7 h j between any two grid points h i , h j \u2208 H is maximized (as is done in Fig. 3 ), or by specifying a uniform localized grid on the lie algebra h and obtaining the centers via the exponential map. The latter approach is in fact possible for any Lie group and leads to a notion of localized convolution kernels that have a finite support on H, see Fig. 4 . Atrous B-splines Atrous convolutions, i.e. convolutions with sparse kernels defined by weights interleaved with zeros (Holschneider et al., 1990) , are commonly used to increase the effective receptive field size and add a notion of scale to deep CNNs (Yu & Koltun, 2016; Chen et al., 2018) . Atrous convolution kernels can be constructed with B-splines by fixing the scale factors s x and s h , e.g. to the grid size, and increasing the distance between the center points x i and h i . Non-uniform/deformable B-splines In non-uniform B-splines the centers x i and h i do not necessarily need to lie on a regular grid. Then, deformable CNNs, first proposed by Dai et al. (2017) , are obtained by treating the centers as trainable parameters. For B-spline CNNs on R d of order n = 1 this in fact leads to the deformable convolution layers as defined in (Dai et al., 2017) . The design of G-correlation layers (Eqs. (6-8)) using B-spline kernels (Eqs. (11-13)) results in a generic and modular construction of G-CNNs that are equivariant to Lie groups G and only requires a few group specific definitions (see examples in App. B): The group structure of H (group product \u00b7 and inverse), the action of H on R d (together with the group structure of H this automatically defines the structure of G = R d H), and the logarithmic map Log : H \u2192 h. The sum of all B-spline basis functions add up to one, illustrating partition of unity on the 2D rotation group SO(2) (row 1), scaling/dilation group (R + , \u00b7) (row 2), and the sphere S 2 treated as the quotient group SO(3)/SO(2), with B-spline centers indicated with green dots (row 3-5). Right: A random B-Spline on SO(2) (row 1) and (R + , \u00b7) (row 2) and reconstruction of a color texture on the sphere S 2 at several scales (row 3-5) to illustrate multi-scale properties. Experiments G-CNNs ensure roto-translation equivariance both locally (low-level) and globally (high-level) and invariance is achieved by means of pooling. In our experiments we test the performance of roto-translation G-CNNs (with G = R 2 SO(2)) against a 2D baseline and investigate the effect of different choices (local, global, atrous) for defining the kernels on the SO(2)-part of the network, cf. Eq. (13) We generally observe that a finer sampling of SO(2) leads to better results up until N h = 12 after which results slightly degrade. This is line with findings in (Bekkers et al., 2018a) . The degradation after this point could be explained by overfitting; there is a limit on the resolution of the signal generated by rotating 5x5 convolution kernels; at some point the splines are described in more detail than the data and thus an unnecessary amount of coefficients are trained. One could still benefit from sampling coarse kernels (low N k ) on a fine grid (high N h ), e.g. compare the cases N h > N k for fixed N k . This is in line with findings in (Weiler et al., 2018b) where a fixed circular harmonic basis is used. Generally, atrous kernels tend to outperform dense kernels as do the localized kernels in the low N k regime. Finally, comparing the models with and without 90 \u2022 augmentation show that such augmentations are crucial for the 2D model but hardly affect the SE(2) model. Moreover, the SE(2) model without outperforms the 2D model with augmentation. This confirms the theory: G-CNNs guarantee both local and global equivariance by construction, whereas with augmentations valuable network capacity is spend on learning (only) global invariance. The very modest drop in the SE(2) case may be due to the discretization of the network on a grid after which it is no longer purely equivariant but rather approximately, which may be compensated for via augmentations. Data The CelebA dataset (Liu et al., 2015) contains 202,599 RGB images of varying size of celebrities together with labels for attributes (hair color, glasses, hat, etc) and 5 annotated facial landmarks (2 eyes, 1 nose, 2 corners of the mouth). We reformatted the data as follows. All images are isotropically scaled to a maximum width or height of 128 and if necessary padded in the other dimension with zeros to obtain a size of 128x128. For each image we took the distance between the eyes as a reference for the size of the face and categorized each image into above and below average size. For each unique celebrity with at least 1 image per class, we randomly sampled 1 image per class. The final dataset consists of 17,548 images of 128x128 of 8,774 celebrities with faces at varying scales. Each image is labeled with 5 heatmaps constructed by sampling a Gaussian with standard deviation 1.5 centered around each landmark. Experiments We train a scale-translation G-CNN (with G = R 2 R + ) with different choices for kernels. The \"dense\" networks have kernels defined over the whole discretization of H = R + and thus consider interactions between features at all scales. The \"local\" networks consider only \u25b2 \u25b2 \u25b2 \u25b2 \u25b2 \u25b2 interaction between neighbouring scales via localized kernels (N k = 3) or no scale interaction at all (N k = 1). Either way, each G-CNN is a multi-scale network in which kernels are applied at a range of scales. We compared against a 2D baseline with fixed-scale kernels which we tested for several scales separately. In the G-CNNs, H is uniformly sampled (w.r.t. to the metric on H) on a fixed scale range, generating the discrete sets i=1 with s h = 1 2 ln 2. Each G-CNN is sampled with the same resolution in H with s h , and each B-spline basis function is centered on the discrete grid (i.e. h i \u2208 H d ). We note that the discretization of H is formally no longer a group as it is not closed, however, the group structure still applies locally. The result is that information may leak out of the domain in a similar way as happens spatially in standard zero-padded 2D CNNs (translational G-CNNs), in which the discretized domain of translations is also no longer (locally) compact. This information leak can be avoided by using localized kernels of size N k = 1 along the H axis, as is also done in (Worrall & Welling, 2019; Li et al., 2019) . The networks are trained without dataaugmentation, except for the 2D network, which for reference we train with and without random scale augmentations at train-time. Augmentations were done with a random scale-factor between 1 and 1.4. We found that scale augmentations beyond 1.4 did not improve results. Results Fig. 5 summarizes the results. By testing our 2D baseline at several scales we observe that there is an optimal scale (h = 2) that gives a best trade off between the scale variations in the data. This set of experiments is also used to rule out the idea that G-CNNs outperform the 2D baseline simply because they have a larger effective receptive field size. For large scale ranges the G-CNNs start to outperform 2D CNNs as these networks consider both small and large scale features simultaneously (multi-scale behavior). Comparing different G-CNN kernel specifications we observe that enabling interactions between neighbouring scales, via localized kernels on H (\"local\", N h = 3), outperforms both all-scale interactions (\"dense\") and no-scale interaction at all (N h = 1). This finding is in line with those in (Worrall & Welling, 2019) . Finally, although 2D CNNs moderately benefit from scale augmentations, it is not possible to achieve the performance of G-CNNs. It seems that a multi-scale approach (G-CNNs) is essential. This paper presents a flexible framework for building G-CNNs for arbitrary Lie groups. The proposed B-spline basis functions, which are used to represent convolution kernels, have unique properties that cannot be achieved by classical Fourier based basis functions. Such properties include the construction of localized, atrous, and deformable convolution kernels. We experimentally demonstrated the added value of localized and atrous group convolutions on two different applications, considering two different groups. In particular in experiments with scale-translation G-CNNs, kernel localization was important. The B-spline basis functions can be considered as smooth pixels on Lie groups and they enable us to design G-CNNs using familiar notions from classical CNN design (localized, atrous, and deformable convolutions). Future work will focus on exploring these options further in new applications that could benefit from equivariance constraints, for which the tools now are available for a large class of transformation groups via the proposed Lie group B-splines. (Kantorovich & Akilov, 1982 , Ch 9, Thm 5), or (Duits, 2005, Thm 1) , that if K is linear and bounded it is an integral operator. 2. The left-equivariance constraint then imposes bi-left-invariance of the kernelk as follows, where \u2200 g\u2208G and \u2200 f \u2208L2(X) : Since (14) should hold for all f \u2208 L 2 (X) we obtain Furthermore, since G acts transitively on Y we have that \u2200 y,y0\u2208Y \u2203 gy\u2208G such that y = g y y 0 and thusk for every g y \u2208 G such that y = g y y 0 with arbitrary fixed origin y 0 \u2208 Y . 3. Every homogeneous space Y of G can be identified with a quotient group G/H. Choose an origin y 0 \u2208 Y s.t. \u2200 h\u2208H : h y 0 = y 0 , i.e., H = Stab G y 0 , the\u00f1 We further remark that When Y \u2261 G = G/{e}, with e \u2208 G the identify element of G, the symmetry constraint of Eq. (5) vanishes. Thus, in order to construct equivariant maps without constraints on the kernel the functions should be lifted to the group G. In the following sub-sections some explicit examples of Lie groups H are given, together with their actions on R d and the Log operators. The required tools for building B-spline based G-CNNs for Lie groups of the form G = R d H are then automatically derived from these core definitions. E.g., the action of H on a space X defines a left-regular representation on functions on X via When X = G is the group itself, the action equals the group product. The group structure of semidirect product groups G = R d H is automatically derived from the action of H on R d , see Eq. (1) and is in turn used to define the representations (see Eq. (2)). Some examples are given below. The group of translations is given by the space of translation vectors R d with the group product and inverse given by Now, since the space of translations can be identified with R d , the lifting (Eq. (6)) and group correlations (Eq. (7)) coincide with the standard definition of cross-correlation. I.e., The logarithmic map is simply given by so the convolution kernels can be defined by regular splines on R d , cf. Eq. (11). The special orthogonal group SO(2) consists of all orthogonal 2 \u00d7 2 matrix with determinant 1, i.e., rotation matrices of the form and the group product and inverse is given by the matrix product and matrix inverse: (2) with \u03b8, \u03b8 \u2208 S 1 . The identity element is e = (R 0 ) = (I). The action of H on R 2 is given by matrix vector multiplication: Combining the group structure of 2D translations with rotations in SO(2) as a semi-direct product group gives us the roto-translation group SE(2) = R 2 SO(2), also known as the special Euclidean motion group. The group structure of SE(2) is automatically derived from that of SO(2), see Sec. 3.1 for details. The left-regular representation of SO(2) are given by \u03b8 .x , R \u03b8 \u2212\u03b8 ). Note that the latter two representations in terms of the rotation parameters \u03b8, \u03b8 \u2208 S 1 represents the periodic shift \u03b8 \u2212 \u03b8 mod 2\u03c0 along the rotation axis. See Fig. 6 for an illustration for the transformation of SE(2) convolution kernels via the representation L SO(2)\u2192L2(SE (2)) h . The determinant of the Jacobian of the action of H on R d , see corollary 1, is | det h| = 1. Using the above group structure we can write out the explicit forms for the lifting (Eq. (6)) and group correlations (Eq. (7)) as follows: Log R \u03b8 = 0 \u2212\u03b8 mod 2\u03c0 \u03b8 mod 2\u03c0 0 \u2208 T e (SO(2)) = span {A 1 } Figure 6 : Top row: In group theoretical terms we can describe a smiley face via a collection of group elements S \u2282 SE (2), which e.g. represents the locations and orientations of low-level features such as edges/lines. Such a collection of points transforms via left multiplication, e.g. with h \u2208 SO(2). Bottom row: In a group convolutional setting we can work with convolution kernels which are functions on SE(2) that assign weights to locations in which locally oriented features are expected. Such kernels transform via the group representation. Which in terms of the Lie algebra basis {A 1 } with A 1 = 0 \u22121 1 0 gives a vector with coefficient a 1 = \u03b8 mod 2\u03c0. The B-spline basis, centered around each h i = (R \u03b8i ) \u2208 H with scale s h \u2208 R, as depicted in Fig. 3 , is thus computed via We call the positive real line R + , together with multiplication, the scaling group. The group product and inverse are given by The identity element is e = (1). The action of H on R d is given by scalar multiplication h x = s x, with x \u2208 R d . By combining 2D translations with scaling as a semi-direct product group we obtain the scale-translation group, which we denote with R 2 R + . The left-regular representation of the scaling group are given by In group theoretical terms we can describe a smiley face via a collection of group elements S \u2282 R 2 R + , which e.g. represents the locations and scale of low-level features such as blobs/circles. Such a collection of points transforms via left multiplication, e.g. h \u00b7 S := {(0, h) \u00b7 g | g \u2208 S}, with h \u2208 R + . Bottom row: In a group convolutional setting we can work with convolution kernels which are functions on R 2 \u00d7 R + that assign weights to locations in which locally scaled features are expected. Such kernels transform via the group representation. The scaling of a R 2 R + group convolution kernel is thus achieved by a planar scaling and a logarithmic shift along the scale axis. This is illustrated in Fig. 7 . The determinant of the Jacobian of this action is | det h| = s d . Using the above group structure we can write out the explicit forms for the lifting (Eq. (6)) and group correlations (Eq. (7)) as follows: Log h = ln s. The B-spline basis, centered around each h i = (s i ) \u2208 H with scale s i \u2208 R, as depicted in Fig. 3 , is thus computed via The 3D rotation group is given by space of 3 \u00d7 3 orthogonal matrices with determinant 1, with the group product and inverse given by matrix product and matrix inverse: The action of SO(3) on R 3 is given by matrix-vector multiplication h x = R.x, with x \u2208 R 3 . Combining the group structure of 3D translations with rotations in SO(3) as a semidirect product group gives us the roto-translation group SE(3) = R 3 SO(3), also known as the 3D special Euclidean motion group. The left-regular representation are then The determinant of the Jacobian of the action of SO(3) on R 3 is | det h| = 1. Using the above group structure we can write out the explicit forms for the lifting (Eq. (6)) and group correlations (Eq. (7)) as follows: with dR denoting the Haar measure on SO (3), which depends on the parameterization, see details below. The logarithmic map from the group SO(3) to the Lie algebra so (3) is given by the matrix logarithm and the resulting matrix can be expanded in a basis {A 1 , A 2 , A 3 } for the Lie algebra In practice it is often convenient to rely on a parameterization of the group and define the group structure in terms of these parameters. A common choice is to do this via ZYZ Euler angles via with R e,\u03b8 a rotation of \u03b8 around a reference axis e, and \u03b1 \u2208 [0, 2\u03c0], \u03b2 \u2208 [0, \u03c0], \u03b3 \u2208 [0, 2\u03c0]. A Haar measure in terms of this parameterization is then given by d\u00b5(R) = sin \u03b2d\u03b1d\u03b2d\u03b3. We will use this parameterization in construction of the quotient group SO(3)/SO(2) next. Here we define the 2-sphere as a group quotient of SO(3) and remark that the same equations as in the SO(3) case (App. B.4) are used for the lifting and group correlations. This section describes how to construct convolution kernels on the sphere using the logarithmic map of SO(3), which can then be used to build G-CNNs on The 2-sphere is defined as S 2 = {x \u2208 R 3 | x = 1}. Any point on the sphere can be obtained by rotating a reference vector z = (0, 0, 1) T with elements of SO (3), i.e., \u2200 n\u2208S 2 , \u2203 R\u2208SO(3) : n = R.z. In other words, the group SO(3) acts transitively on S 2 . In ZYZ Euler angle parameterization of SO(3) all angles \u03b1 leave the reference vector z in place, meaning that for each n \u2208 S 2 we have several R \u2208 SO(3) that map z to the same n. As such, we can treat S 2 as the quotient group SO(3)/SO(2), where SO(2) refers to the sub-group of rotations around the z-axis. In order to define B-splines on the 2-sphere we need a logarithmic map from a point in S 2 to the (Euclidean) tangent vector space T e (S 2 ) at the origin. We will construct this logarithmic map using the Log define for SO(3). Let us parameterize the sphere with n(\u03b2, \u03b3) = R ez,\u03b3 .R ey,\u03b2 .z. Any rotation R \u03b1,\u03b2,\u03b3 with arbitrary \u03b1 maps to the same n(\u03b2, \u03b3) \u2208 S 2 . As such, there are also many vectors A = Log R \u03b1,\u03b2,\u03b3 \u2208 T e (SO(3)) that map to a suitable rotation matrix via the exponential map R = exp A. We aim to find the vector in T e (SO(3)) for which c 3 = 0, which via the exponential map generate torsion free exponential curves. The Log of any R \u03b1,\u03b2,\u03b3 with \u03b1 = \u2212\u03b3 results in such a vector (Portegies et al., 2015) . As such we define Log S 2 n(\u03b2, \u03b3) := Log SO(3) R \u2212\u03b3,\u03b2,\u03b3 , which maps any point in S 2 to a 2-dimensional vector space T e (S 2 ) \u2282 T e (SO(3)). A B-spline on S 2 can then be defined via in which individual splines basis functions are centered around points n(\u03b2 i , \u03b3 i ). We remark that the group product R \u22121 \u03b1i,\u03b2i,\u03b3i .R 0,\u03b2,\u03b3 generates different rotations when varying \u03b1 i , that however still map to the same n. The vectors obtained by taking Log S 2 of the rotation matrices rotate with the choice for \u03b1 i . Since the B-splines are approximately isotropic we neglect this effect and simply set \u03b1 i = 0 in Eq. (16). Finally, we remark that the superposition of shifted splines (as in Eq. (16)) is not isotropic by construction, which is desirable when using the spline as a convolution kernel to lift functions to SO(3). When constraining G-CNNs to generate feature maps on S 2 , the kernels are constrained to be isotropic. Alternatively on could stay on S 2 entirely and resort to gauge-equivariant networks (Cohen et al., 2019) , for which the proposed splines are highly suited to move from the discrete setting (as in (Cohen et al., 2019) In (Worrall & Welling, 2019) images f \u2208 L 2 (R d ) are lifted to a space of positions and scale parameters by constructing a scale space via 4s . The kernels and images are sampled on a discrete grid. Let \u2126 \u2282 Z d be the support of the kernel. Then the discrete scale space correlation is given by (Worrall & Welling, 2019, Eq. (19) ) with H d the discretized set of scales, e.g., , where we remark that here we use the convention of scaling of a function by s \u2265 1 instead of using the dilation parameter a = 1 s in (Worrall & Welling, 2019 ). Next we remark that the scale space correlation without any scale interaction (H d = {1}) is defined by a 2D correlation kernel via which can be regarded as a discrete atrous/dilated correlation on each of the scale slices of f \u2191 (x, s) with kernels dilated by a factor s. Let our lifting correlation kernel k be given in a B-spline basis via Eq. (11) and let c : \u2126 \u2282 Z d \u2192 R be the map that assigns the weights to each B-spline center x i \u2208 \u2126 with \u2126 the set of spline centers (i.e. c(x i ) = c i ). Let the Gaussian kernel G s (x) be approximated by a scaled B-spline (up to a factor) and define B With such an approximation (see also (Bouma et al., 2007) ) our lifting correlation via Eq. (6) coincides with the lifting of (Worrall & Welling, 2019) followed by their non-scale interacting scale-space correlation, i.e., with c : \u2126 \u2192 R the spline coefficients. We show this by rewriting The following highlights commonalities between this paper and the work by Cohen et al. (2019) with respect to use of left-invariant vector fields in equivariant neural networks. Consider some Lie group G with Lie algebra g = T e (G), the exponential map Exp : g \u2192 G, and logarithmic map Log : G \u2192 g. Consider the group correlation between a kernel and function K, F : G \u2192 R, given in Eq. (7), which for B-spline kernels K with finite support \u2126 := supp(K) \u2282 G reads as with \u00b5 G (h) the Haar measure on G, and where write K for the convolution kernel on G, andK for the corresponding kernel on g: Let \u2126 := supp( K) \u2282 g be the support of K. Finally let \u2126 be localized such that Exp is a diffeomorphism (i.e., Exp( \u2126) = \u2126 and Log(\u2126) = \u2126). Now consider the definition of gauge equivariant correlation on manifolds as given by Eq. (3) of Cohen et al. (2019) for the case of scalar functions (in which case the trivial representation \u03c1(g) = 1 is to be used). In this case integration takes place over the Lie algebra, and gauge equivariant correlation is defined by with dx the Lebesgue measure on R d , and where Exp g denotes the exponential map from T g (G) \u2192 G. In our Lie group setting all tangent spaces can be identified with the tangent space at the origin (via the push-forward of left-multiplication) and we are able to write Exp g := g \u00b7 Exp x. In the setting of gauge equivariant CNNs as in (Cohen et al., 2019 ) the exponential maps are generally dependent on g, using a separate reference/gauge frame (basis for the tangent space) at each g. For Lie groups the following identity holds between group correlations with localized B-splines on the one hand, in which integration takes place over the group G and elements are mapped to the algebra via Log, and gauge equivariant correlation on the other hand, in which integration takes place on the tangent spaces and vectors in these tangent spaces are mapped to the manifolds via Exp. In other words, given the definition of G cross-correlation in (7), denoted with , and gauge correlation in (19) or (Cohen et al., 2019, Eq. (1)), denoted with , the two operators relate via We show this by deriving In the above dg is a Haar measure on G. At 1 = the substitutiong \u2192 g \u00b7g is made and left-invariance of the Haar measure is used (d(g \u00b7g) = dg). At 2 = we switch from integration over the region \u2126 in the Lie group to integration over region \u2126 = Log(\u2126) in the Lie algebra. This section describes the G-CNN architectures used in the experiments of Sec. 4 using the layers as defined in Sec. 3.2 and illustrated in Fig. 1 . Two slightly different architectures are in the two different tasks (metastasis classifiation and landmark detection), but both are regular sequential GCNNs that start with a lifting layer (6), followed by a several group correlation layers (7), possibly alternated with spatial max-pooling, followed by a projection over H via (8), and end with a 1 \u00d7 1 convolution or fully connected layers. The architectures are summarized in Table. 1 and 2. Note that the output of the PCam architecture is two probabilities (1 for each class), and the output of the CelebA is five heatmaps (1 for each landmark). The architecture for metastasis classification in the PCam dataset is given in Tab. 1. The input (64 \u00d7 64) is first cropped to 88 \u00d7 88 and is then used as input for the first layer (the lifting layer). None of the layers use spatial padding such that the image is eventually cropped to size 1 \u00d7 1. Each layer is followed by batch normalization 1 and a ReLU activation function, except for the last layer (layer 7) which is followed by adding a bias vector of length 2 and a softmax. Note that the first five layers, including max pooling over rotations, encode the image into a 64-dimensional rotation invariant feature vector. The final two layers (6 and 7) can be regarded as a classical neural network classifier. To reduce a possible orientation bias we aim to approximate the support of the kernels with a disk, rather than a rectangle. We do this by only considering splines with basis function centers {x i \u2208 Z d | x i \u2264 r, with radius r. For 5 \u00d7 5 kernels we set r = \u221a 5 by which we discard the basis functions at the corners of the 5 \u00d7 5 grid. The grid on H is uniformly sampled with N h samples, giving the discretized grid Finally, we follow the same data-augmentations at train time as proposed in (Liu et al., 2015) . These include geometric augmentations such as 90 \u2022 rotations and horizontal flips, as well as color augmentations such as brightness, saturation, hue and contrast variations. Table 1 : PCam SE(2) G-CNN settings and the number of free parameters. Here N k denotes the number basis functions used on the H = SO(2) part of the group correlation kernels. Basis size: Nr of output feature maps (# weights) 1: Lifting (5 \u00d7 5) 40 (2, 520) 23 (1, 449) 20 (1, 260) 18 (1, 134) 14 (882) 11 (693) 10 (630) Spatial max pooling (2 \u00d7 2) by a factor 2 2: G-corr (5 \u00d7 5 \u00d7 N h ) 40 (33, 600) 23 (33, 327) 20 (33, 600) 18 (34, 020) 14 (32, 928) 11 (30, 492) 10 (33,600) Spatial max pooling (2 \u00d7 2) by a factor 2 3: G-corr (5 \u00d7 5 \u00d7 N h ) 40 (33, 600) 23 (33, 327) 20 (33, 600) 18 (34, 020) 14 (32, 928) 11 (30, 492) 10 (33,600) Spatial max pooling (3 \u00d7 3) by a factor 3 4: G-corr (5 \u00d7 5 \u00d7 N h ) 40 (33, 600) 23 (33, 327) 20 (33, 600) 18 (34, 020) 14 (32, 928) 11 (30, 492) 10 (33,600) 5: G-corr (1 \u00d7 1 \u00d7 N h ) 64 (2, 560) 64 (4, 416) 64 (5, 120) 64 (5, 760) 64 (7, 178) 64 (8, 448) 64 (10,240) Max pooling over H (Projection layer) 6: 2D-corr (1 \u00d7 1) 16 (1, 024) 16 (1, 024) 16 (1, 024) 16 (1, 024) 16 (1, 024) 16 (1, 024) The architecture for landmark detecion in the CelebA dataset is biven in Tab. 2. The input is formatted according to the details in Sec. 4. In each layer zero padding is used in order to map the 128\u00d7128 input images to a 128 \u00d7 128 output heatmaps. Each layer is followed by batch normalization and a ReLU activation function, except for the last layer (layer 10) which is followed by adding a bias vector and a logistic sigmoid activation function. Note that the result of the first 6 layers, including average pooling over scale, assign locally scaleinvariant feature vectors to each pixel. The final layers convert these feature maps into heatmaps via regular 2D convolutions. Landmarks are localized via the argmax on each heatmap. The results in Fig. 5 show the success rate for localizing a landmark correctly. The success rate is computed as the average fraction of successful detections for all five landmarks in all images. A detection is considered successful if the distance to the actual landmark is less then 10 pixels. The H axis is uniformly sampled (w.r.t. to the metric on H) on a fixed scale range, generating the discrete sets i=1 with s h = 1 2 ln 2. The global kernels have their centers on this grid, i.e., h i \u2208 H d with the scale parameter the same as that of the grid. The local kernels also have their centers equidistant (with scale s h ) to eachother, but are localized and given by h i \u2208 {e is h } N k /2 i=\u2212 N k /2 . Table 2 : CelebA scale-translation G-CNN settings and the number of free parameters. Here N k denotes the number basis functions used on the H = (R + , \u00d7) part of the group correlation kernels. Basis size:"
}