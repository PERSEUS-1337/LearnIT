{
    "title": "SygD-hCcF7",
    "content": "Most deep learning models rely on expressive high-dimensional representations to achieve good performance on tasks such as classification. However, the high dimensionality of these representations makes them difficult to interpret and prone to over-fitting. We propose a simple, intuitive and scalable dimension reduction framework that takes into account the soft probabilistic interpretation of standard deep models for classification. When applying our framework to visualization, our representations more accurately reflect inter-class distances than standard visualization techniques such as t-SNE. We show experimentally that our framework improves generalization performance to unseen categories in zero-shot learning. We also provide a finite sample error upper bound guarantee for the method. Dimensionality reduction is an important problem in machine learning tasks to increase classification performance of learned models, improve computational efficiency, or perform visualization. In the context of visualization, high-dimensional representations are typically converted to two or threedimensional representations so that the underlying relations between data points can be observed and interpreted from a scatterplot. Currently, a major source of high-dimensional representations that machine learning practitioners have trouble understanding are those generated by deep neural networks. Techniques such as PCA or t-SNE ( BID11 are typically used to visualize them, e.g., in Law et al. (2017) ; BID5 . Moreover, BID4 proposed a visualization technique that represents examples based on their predicted category only. However, none of these techniques exploit the fact that deep models have soft probabilistic interpretations. For instance, the output of deep classifiers typically employs softmax regression, which optimizes classification scores across categories by minimizing cross entropy. This results in soft probabilistic representations that reflect the confidence of the model in assigning examples to the different categories. Many other deep learning tasks such as semantic segmentation (Long et al., 2015) or boundary/skeleton detection BID16 also optimize for probability distributions. In this paper, we experimentally demonstrate that the soft probability representations learned by a neural network reveal key structure about the learned model. To this end, we propose a dimensionality reduction framework that transforms probability representations into a low-dimensional space for easy visualization. Furthermore, our approach improves generalization. In the context of zero-shot learning where novel categories are added at test time, deep learning approaches often learn high-dimensional representations that over-fit to training categories. By learning low-dimensional representations that match the classification scores of a high-dimensional pre-trained model, our approach takes into account inter-class similarities and generalizes better to unseen categories than standard approaches. Proposed approach: We propose to exploit as input representations the probability scores generated by a high-dimensional pre-trained model, called the teacher model or target, in order to train a lower-dimensional representation, called the student. In detail, our approach learns low-dimensional student representations of examples such that, when applying a specific soft clustering algorithm on the student representations, the predicted clustering scores are similar to the target probability scores. Contributions: This paper makes the following contributions: (1) We propose the first dimensionality reduction approach optimized to consider some soft target probabilistic representations as input. (2) By exploiting the probability representations generated by a pre-trained model, our approach reflects the learned semantic structure better than standard visualization approaches. (3) We experimentally show that our approach improves generalization performance in zero-shot learning.(4) We theoretically analyze the statistical properties of the approach and provide a finite sample error upper bound guarantee for it. Our method, called Dimensionality Reduction of Probabilistic Representations (DRPR, pronounced \"Derper\"), is given probability distribution representations generated from high-dimensional data as target. Its goal is to learn a low-dimensional representation such that the soft clustering scores predicted by a soft clustering algorithm are similar to the target. If the targets are probability distributions generated by a pre-trained classifier, we want the low-dimensional space to reflect the relationships between categories interpreted by the classifier. The position of each example in the low-dimensional space should then reflect the ambiguity of the classifier for the example (see FIG1 ). We summarize in Section 2.1 the soft clustering algorithm that is used by DRPR in the low-dimensional space, the algorithm is detailed in Banerjee et al. (2005) . The general learning algorithm of DRPR is introduced in Section 2.2. Probability density: We consider that we are given a set of n vectors f 1 , \u00b7 \u00b7 \u00b7 , f n \u2208 V concatenated into a single matrix F = [f 1 , \u00b7 \u00b7 \u00b7 , f n ] \u2208 V n . In the following, we consider V = R d , and V n = R n\u00d7d . The goal is to partition n examples into k soft clusters. Each cluster C c with c \u2208 {1, \u00b7 \u00b7 \u00b7 , k} has a center\u03bc c \u2208 V and its corresponding probability density is p c (f i ) = exp(\u2212d(f i ,\u03bc c )) b(f i ), where d is a regular Bregman divergence (Banerjee et al., 2005; Bregman, 1967) and b : V \u2192 R + is a uniquely determined function that depends on d and ensures that the integral of the density over V is 1 (e.g., b(f i ) = 1/(2\u03c0) d if d is the squared Euclidean distance). For simplicity, we consider that d is the squared Euclidean distance. The density p c (f i ) decreases as the divergence between the example f i and the center\u03bc c increases. The BSCP (Banerjee et al., 2005) is defined as that of learning the maximum likelihood parameters \u0393 = {\u03bc c ,\u03c0 c } k c=1 of a mixture model p(f i |\u0393) = k c=1\u03c0 c exp(\u2212d(f i ,\u03bc c )) b(f i ) where\u03c0 c is the prior probability that f i is generated by C c . To partition the n examples into k clusters, Banerjee et al. (2005) apply the EM algorithm to maximize the likelihood parameter estimation problem for mixture models formulated as: max \u0393 n i=1 p(f i |\u0393) Assignment matrix: Partitioning the n observations in F into k soft clusters is equivalent to determining some soft assignment matrix\u0176 \u2208 (0, 1) n\u00d7k in the set Y n\u00d7k of matrices whose rows are positive and sum to 1. Formally, Y n\u00d7k is written Y n\u00d7k := {\u0176 \u2208 (0, 1) n\u00d7k :\u0176 1 k = 1 n } where 1 k \u2208 {1} k is the k-dimensional vector containing only 1. For a given value of \u0393, the element Y ic = p(C c |f i ) \u2208 (0, 1) is the posterior probability, or responsibility of C c for f i . The higher the value of\u0176 ic , the more likely f i belongs to cluster C c .Local maximum condition: Once the BSCP has converged to a local maximum of max \u0393 n i=1 p(f i |\u0393), the following equations are all satisfied:(E-step) \u2200i, c, p(C c |f i ) =\u0176 ic =\u03c0 c exp(\u2212d(f i ,\u03bc c )) DISPLAYFORM0 Eq.(1) corresponds to the E-step of the EM algorithm that computes p(C c |f i ) =\u0176 ic when the parameters F and \u0393 are given. Eq. (2) corresponds to the M-step, which has a simple form since the likelihood is a regular exponential family function (Banerjee et al., 2005) . The M-step may be computationally expensive for other types of exponential family distributions (Banerjee et al., 2005, Section 5.2) . It is worth noting that these optimality conditions do not depend on the function b used to define p c (f i ), so b can be ignored. Section 2.1 explains how to perform soft clustering on some fixed representation F . We now describe how to learn F so that the soft clustering scores predicted by the BSCP match those of the target. We assume that we are given the probability representation y i \u2208 [0, 1] k as target for each training example f i . These representations are concatenated into a single ma- DISPLAYFORM0 n\u00d7k which is the target of our method for F . DRPR learns the representation of F \u2208 V n such that the soft assignment matrix obtained from applying the BSCP on F is close to Y . We first give the formulation of our prediction function in Eq. (3). Our dimensionality reduction problem is given in Eq. (4).Prediction function: Let us assume that we are given the dataset matrix DISPLAYFORM1 (1), we define our prediction function \u03c8(F, M, \u03c0) = \u03a8 \u2208 Y n\u00d7k as the soft assignment matrix predicted by the BSCP given F , M and \u03c0. The elements of the matrix \u03a8 are then computed as follows: DISPLAYFORM2 Optimization problem: DRPR learns the representation of F so that the predicted assignment matrix \u03c8(F, M, \u03c0) = \u03a8 is similar to Y . Given the optimal condition properties of the BSCP stated in Section 2.1, the optimal values of M and \u03c0 also depend on \u03a8 and are therefore variables of our dimensionality reduction problem that we formulate: DISPLAYFORM3 The function \u2206 n (\u03a8, Y ) is an empirical discrepancy loss between the predicted assignment matrix \u03a8 and the target assignment matrix Y . Since the rows of \u03a8 and Y represent probability distributions, we formulate \u2206 n as the average KL-divergence between the rows of Y and the rows of \u03a8. Let \u03c8 i and y i be the i-th rows of \u03a8 and Y , respectively, we formulate DISPLAYFORM4 Note that the choice of the discrepancy loss \u2206 n is independent of the chosen Bregman divergence d. Moreover, the number of classes k has an impact on the number of clusters in the low-dimensional space but is not related to the dimensionality d of the model. DRPR considers that each class c \u2208 {1, \u00b7 \u00b7 \u00b7 , k} is represented by one cluster prototype \u00b5 c \u2208 R d . In our experiments, the target (or teacher) is the assignment matrix Y \u2208 Y n\u00d7k that contains the probability scores generated by a pre-trained neural network. It corresponds to the output of a classifier trained with softmax regression in the visualization experiments, and to the matrices Y 1 and Y 2 described in Section 4.2. The goal is then to learn F , M and \u03c0 so that (1) F , M and \u03c0 reach the BSCP optimality conditions given in Section 2.1 and (2) \u03a8 = \u03c8(F, M, \u03c0) is similar to Y .Visualization: DRPR can be used for visualization since many models (e.g., usual neural networks) have probabilistic interpretations w.r.t. the k training categories. In our visualization task, the matrices M and \u03c0 are not provided, whereas the target matrix Y is given. By using the optimality conditions input : Set of training examples (e.g., images) in X and their target probability scores (e.g., classification scores w.r.t. k training categories), nonlinear mapping g \u03b8 parameterized by parameters \u03b8, number of iterations t 1: for iteration 1 to t do 2: Randomly sample n training examples x1, \u00b7 \u00b7 \u00b7 , xn \u2208 X and create target assignment matrix Y \u2208 Y n\u00d7k containing the target probability scores y1, DISPLAYFORM0 \u22121 Y F and prior vector \u03c0 \u2190 1 n Y 1n 5: Update the parameters \u03b8 by performing a gradient descent iteration of \u2206n (\u03c8(F, M, \u03c0), Y ) (i.e., Eq. (4)) 6: end for output : nonlinear mapping g \u03b8 in Eq. (2), we can write the desired values of M and \u03c0 as a function of F and Y : at each iteration, for some current value of F , the optimal values M = diag(Y 1 n ) \u22121 Y F and \u03c0 = 1 n Y 1 n are computed and F is updated via gradient descent. DRPR is illustrated in Algorithm 1 in the case where F is the output of a model g \u03b8 parameterized by \u03b8, e.g., g \u03b8 can be a neural network. However, we represent F as non-parametric embeddings in our visualization experiments to have an equitable comparison to non-parametric baselines. The learning algorithm then modifies the matrix F at each iteration. If the priors \u03c0 c are all equal, then the priors are updated in step 4 as follows: \u03c0 \u2190 1 k 1 k . Zero-shot learning: DRPR can be used to improve zero-shot learning generalization since highdimensional models may overfit to training categories and the goal of zero-shot learning is to generalize to novel categories. In the considered zero-shot learning task, the variable F concatenates image representations (outputs of a neural network) in the same way as step 3 of Algorithm 1, and the variable M concatenates category representations extracted from text (outputs of another neural network). Both F and M are of different nature and are therefore computed as concatenating the outputs of two distinct neural networks taking different sources as input. To optimize Eq. (4), both neural networks are trained jointly by fixing the other neural network during backpropagation. In our experiments, we consider the squared Euclidean distance d(f i ,\u03bc c ) = f i \u2212\u03bc c 2 2 . However, DRPR can be used with any regular Bregman divergence (Banerjee et al., 2005) . The algorithm is then identical, with the exception of the chosen divergence d to compute the prediction in Eq. (3).Convergence and scalability: Although our problem has 3 variables (F , M and \u03c0), we use the optimal properties of the BSCP to write them as a function of each other (see step 4 of Algo 1). Eq. (4) is then an optimization problem w.r.t. only one variable F . Since the problem is differentiable wrt F and unconstrained, it is easy to optimize by gradient descent (e.g., back-propagation when training neural networks). Moreover, our loss is nonnegative, hence lower-bounded. It is worth noting that we do not apply multiple iterations of the EM algorithm at each gradient descent iteration, as we first use the optimal properties of BSCP to obtain closed-form formulations of M and \u03c0, and then compute \u03c8(F, M, \u03c0) = \u03a8 to minimize our problem in Eq. (4). Unlike t-SNE and many iterative DR problems, the complexity of DRPR is linear in n (instead of quadratic) and linear in k, which makes it efficient and scalable. Our visualization experiments take less than 5 minutes to do 10 5 iterations while t-SNE takes 1 hour to do 1000 iterations. PCA, which has an efficient closed-form solution, is still much faster. Our approach is thus simple to optimize, hence scalable, and it generalizes to a large family of Bregman divergences. We now interpret the gradient of our optimization problem w.r.t. examples. To simplify its formulation, we consider that all the priors \u03c0 1 , \u00b7 \u00b7 \u00b7 , \u03c0 k are equal and the matrix M does not depend on F , which is the case in our zero-shot learning task. When d is the squared Euclidean distance, all the priors are equal and M does not depend on F , the gradient of Eq. (4) w.r.t. f i is: DISPLAYFORM0 One can observe that its magnitude depends on both the target scores Y ic \u2208 (0, 1) and the predicted responsibilities \u03a8 ic . The gradient tries to make the vector f i closer to each centroid \u00b5 c while separating it from all the centroids \u00b5 m depending on their predicted scores \u03a8 im \u2208 (0, 1). We analyze the statistical property of the algorithm in Appendix A. Theorem 1 provides a finite sample upper bound guarantee on the quality of the minimizer of the empirical discrepancy of Eq. (4). We show that it is upper bounded by the minimum of the true expected discrepancy, and an estimation error term of O(n \u22121/2 ) (under certain conditions and with high probability). We defer the detail to the appendix. This paper introduces a dimensionality reduction method that represents the relations in a dataset that has probabilistic interpretation. It can be seen as a metric learning approach. Metric Learning: The most related approaches try to solve the \"supervised\" hard clustering problem (Law et al., 2016) . During training, they are given the target hard assignment matrix Y \u2208 {0, 1} n\u00d7k where Y ic = 1 if the training example f i has to belong to C c , and 0 otherwise. The goal is to learn a representation such that applying a hard clustering algorithm (e.g., kmeans) on the training dataset will return the desired assignment matrix Y . These approaches can be decomposed in 2 groups: (1) the methods that optimize some regression problem (Lajugie et al., 2014; Law et al., 2016; BID5 and exploit orthogonal projection properties that hold only in the hard clustering context, and (2) the methods that use exponential families (Mensink et al., 2012; BID5 to describe some probability score. In the latter case, the learning problem is written as a multiclass logistic regression problem where the probability of a category C c given an example DISPLAYFORM0 and \u03b4 W is the learned dissimilarity function. DRPR generalizes those approaches and can also be used for hard clustering learning. DISPLAYFORM1 n\u00d7k is a hard assignment matrix and \u2206 n is the same \u2206 n as ours (i.e., \u2206 n (\u03a8, Y ) = 1 n n i=1 D KL (y i \u03c8 i ) by using the convention 0 log 0 = 0). Moreover, the optimal value of M is not implicitly written as a function of F and Y in BID5 . The approach in Mensink et al. (2012) is similar to BID5 but only considers linear models. In summary, both BID5 and Mensink et al. (2012) do not exploit the BSCP formulation to its full potential as they consider some restricted hard clustering case. DRPR generalizes these approaches to the soft clustering context at no additional algorithmic complexity. Dimensionality reduction: Learning models by exploiting soft probability scores predicted by another pre-trained model as supervision was proposed in Ba & Caruana (2014) for classification. It was experimentally observed in Ba & Caruana (2014) that using the output of a large pre-trained neural network as supervision, instead of ground truth labels, improves classification performance of small neural networks. However, in Ba & Caruana (2014) , each dimension of the student representation describes the confidence score of the model for one training category, which is problematic in contexts such as zero-shot learning where categories can be added or removed at test time. Our approach can learn a representation with dimensionality different from the number of training categories. It can therefore be used in zero-shot learning. Dimensionality reduction with neural networks has been proposed in unsupervised contexts (e.g., to maximize variance in the latent space (Hinton & Salakhutdinov, 2006) ) and in supervised contexts (e.g., using ground truth labels as supervision BID3 ). Instead, our approach exploits probability scores generated by a teacher pre-trained model. These scores may be very different from ground truth labels if the pretrained model does not generalize well on the dataset. Our approach can then help understand what was learned by the teacher by visualizing groups of examples the teacher has trouble distinguishing. representation obtained with t-SNE by exploiting soft probability scores w.r.t. the 6 clusters; (d) 2D representation obtained by our method by exploiting using the same supervision as (c). The relative inter-cluster distances of the original dataset are preserved with our approach, unlike t-SNE. We evaluate the relevance of our method in two types of experiments. The first learns low-dimensional representations for visualization to better interpret pre-trained deep models. The second experiment exploits the probability scores generated by a pre-trained classifier in the zero-shot learning context; these probability scores are used as supervision to improve performance on novel categories. Interpreting deep models is a difficult task, and one of the most common tools to solve that task is visualization. Representations are most often visualized with t-SNE which does not account for the probabilistic interpretation of the learned models. We propose to exploit probability classification scores as input of our dimensionality reduction framework. In the visualization experiments of this section, DRPR learns non-parametric low-dimensional embeddings (i.e. our representations are not outputs of neural networks but vectors) as done by non-parametric baselines. Nonetheless, DRPR can also be learned with neural networks (e.g., as done in Section 4.2).Toy dataset: As an illustrative toy experiment, we compare the behavior of t-SNE and DRPR when applied to reduce a simple artificial 3-dimensional dataset as 2D representations. The 3D dataset illustrated in FIG3 (a) contains k = 6 clusters, each generated by a Gaussian model and containing 1,000 artificial points. To generate target soft clustering probability scores in the 3D space, we compute the relative distances of the examples to the different cluster centers and normalize them to obtain (probability) responsibilities as done in Eq. (3). In detail, let us note the original DISPLAYFORM0 (where n is the number of examples) plotted in FIG3 n\u00d7k where k = 6 is constructed by computing DISPLAYFORM0 where c c \u2208 R 3 is the center of the c-th Gaussian model (defined by its color) in the original space, and priors are equal. We plot in FIG3 the 2D representation of our model when using Y as input/target, and the t-SNE representations obtained when using the original dataset X as input in Fig (1) the global structure of the original dataset is better preserved with DRPR than with t-SNE; (2) DRPR satisfies the relative distances between the different clusters better than t-SNE since DRPR tries to preserve the relative responsibilities of the different clusters. We quantitatively evaluate these two observations in the following. It is also worth noting that it is known that distances between clusters obtained with t-SNE may not be meaningful BID13 as t-SNE preserves local neighborhood instead of global similarities. In the following (i.e. in FIG6 and tables of results), we only consider the case where t-SNE takes as input the logits (i.e., classification scores before the softmax operation) instead of probability scores since the latter case returns bad artifacts such as the one in FIG3 (c). Other examples of bad artifacts obtained with t-SNE exploiting probability scores are provided in the supplementary material. We also provide in the supplementary material the visualizations obtained by t-SNE when replacing the 2 -norm in the input space by the KL-divergence and the Jensen-Shannon divergence to compare probabilistic representations that DRPR uses as input. These lead to worse visualizations than Quantitative evaluation metrics: We quantify the relevance of our approach with the two following evaluation metrics: FORMULA0 an adaptation of the Neighborhood Preservation Ratio (NPR) (Van der Maaten & Hinton, 2012): for each image i, it counts the ratio of \u03ba nearest neighbors of i (i.e. that have the closest probability scores w.r.t. the KL divergence) that are also in the set of \u03ba nearest neighbors in the learned low-dimensional space (w.r.t. the Euclidean distance). It is averaged over all images i. This metric evaluates how much images that have similar probability scores are close to each other with the student representation. (2) Clustering Distance Preservation Ratio (CDPR): we randomly sample 10 5 triplets of images (i, i + , i \u2212 ) such that the 3 images all belong to different categories and i has closer probability score to i + than to i \u2212 w.r.t. the KL divergence. The metric counts the percentage of times that the learned representation of i is closer to i + than to i \u2212 w.r.t. the Euclidean distance in the low-dimensional representation. This evaluates how well inter-cluster distances are preserved. We evaluate our approach on the test sets of the MNIST (LeCun et al., 1998), STL (Coates et al., 2011), CIFAR 10 and CIFAR 100 (Krizhevsky & Hinton, 2009) datasets with pre-trained models that are publicly available and optimized for cross entropy.2 The dimensionality of the high-dimensional representations is equal to the number of categories in the respective datasets (i.e. 10 except for CIFAR 100 that contains 100 categories). Our goal is to visualize the teacher representations with 2-dimensional representations by using their probability scores as target, not the ground truth labels. Quantitative comparisons with standard visualization techniques such as t-SNE, ISOMAP BID9 , Locally Linear Embedding (LLE) BID2 and PCA using the 2 leading dimensions are provided in TAB0 . We also report the scores obtained with the logit representations which are not dimensionality reduction representations but provide an estimate of the behavior of the original dataset. DRPR outperforms the dimensionality reduction baselines w.r.t. both evaluation metrics and is competitive with the logit representation. Examples that have similar probability-based representations are closer with our approach than with other dimensionality reduction baselines. DRPR also better preserves inter-cluster distances. It is worth noting that DRPR exploits as much supervision as the \"unsupervised\" visualization baselines. Indeed, all the compared methods use as input the same source of supervision which is included in the (classifier output) representations given as input. Qualitative results: Visualizations of pre-trained models obtained with DRPR and t-SNE are illustrated in FIG6 for MNIST and STL. The visualizations for CIFAR 10 and 100 are in the supplementary material. DRPR representations contain spiked groups at the corners to better reflect examples that have high confidence scores for one category. Indeed, an example in a spike at the corner of a figure has a soft assignment score w.r.t. its closest center close to 1. This means that the pre-trained model has very high confidence to assign the example to the corresponding category (see One can observe that representations obtained with DRPR reflect the semantic structure between categories. On MNIST, categories that contain a curve at the bottom of the digit (i.e., 0, 3, 5, 6, 8 and 9) are in the bottom of FIG6 (left); some pairs of digits that are often hard to differentiate by classifiers (i.e., 4 and 9, 1 and 7, 3 and 8) are also adjacent. On STL and CIFAR 10, animal categories are illustrated on the right whereas machines are on the left. Semantically close categories such as airplane and bird, or car and truck are also adjacent in the figures. One main difference between the DRPR and t-SNE representations for STL is the distance between the clusters ship and airplane. These categories are actually hard for the model to differentiate since they contain blue backgrounds and relatively similar objects. In particular, the STL airplane category contains many images of seaplanes lying on the sea and can then be mistaken for ships. This ambiguity between both categories is not observed on the t-SNE representation. Due to lack of space, a detailed analysis for the CIFAR 100 and STL datasets is available in the supplementary material. A summary of the results is that categories that belong to the same superclass (e.g., categories hamster, mouse, rabbit, shrew, squirrel are part of the superclass small mammals) are grouped together with DRPR. The DRPR visualization also reflects some semantical structure: plants and insects are on the top left; animals are on the bottom left and categories on the right are outdoor categories. Medium mammals are also represented between small mammals and large carnivores. In conclusion, the quantitative results show that the representations of DRPR are meaningful since they better preserve the cluster structure and allow observation of ambiguities between categories. We consider the same zero-shot learning scenario as BID1 and BID5 . In particular, we test our approach on the same datasets and splits as them. The main goal is to learn two mappings, one for image representations and one for category representations, in a common space V. The latter mapping takes some category descriptions as input (e.g., from text description or visual attributes). Image representations are then learned so that they are closer to the representative of their category than to the representative of any other category. At test time, categories that were unseen during training are considered, and their representative is obtained by using the second mapping. An image is assigned to the category with closest representative. Training datasets: We use the medium-scaled Caltech-UCSD Birds (CUB) dataset BID14 and Oxford Flowers-102 (Flowers) dataset (Nilsback & Zisserman, 2008) . CUB contains 11,788 bird images from 200 different species categories split into disjoint sets: 100 categories for training, 50 for validation and 50 for test. Flowers contains 8,189 flower images from 102 different species categories: 62 categories are used for training, 20 for validation and 20 for test. To represent images, BID1 train a GoogLeNet BID8 model whose output dimensionality is 1,024. For each category, BID1 extract some text annotations from BID0 50.1% DS-SJE BID1 (Bag-of-words) 44.1 % DS-SJE BID1 (Char CNN-RNN) 54.0 % Ziming & Saligrama BID19 55.3 % DS-SJE BID1 (Word CNN-RNN) 56.8 % Prototypical Networks BID5 58.3 % Ours -using DS-SJE (Char CNN-RNN) as supervision 57.7 % Ours -using Prototypical Networks as supervision 60.3 % 59.6 % DS-SJE BID1 (Word CNN-RNN) 65.6 % Prototypical Networks BID5 63.9 % Ours -using DS-SJE (Char CNN-RNN) [publicly available] 62.4 % Ours -using Prototypical Networks as supervision 68.2 % which they learn a representative vector (e.g., based on Char CNN-RNN BID18 ). The image representations of examples and the text representations of categories are learned jointly so that each image is more similar to the representative vector of its own category than to any other. We now describe how we generate the supervision/target of our model from the models pre-trained by BID1 BID5 and provided by their respective authors. Once its training is over, Prototypical Network BID5 represents each image i by some vectorf i and each category c by some vector\u03bc c . By concatenating the different vectors into matrices DISPLAYFORM0 In the case of BID1 , we consider the same preprocessing as BID5 . Each image i is represented by some vectorf i , each category c is represented by some vector\u03bc c (that is 2 -normalized in BID5 . The target soft assignment matrix of DRPR is then DISPLAYFORM1 In the model that BID5 provide and that obtains 58.3% accuracy on CUB, ProtoNet trains two models that take as arguments the representations learned by BID1 . They train one modelg\u03b8 1 for images such that \u2200i,f i =g\u03b8 1 (f i ), and one modelg\u03b8 2 for text representative vectors such that \u2200c,\u03bc c =g\u03b8 2 (\u03bc c ). Following BID5 , we train two (neural network) models: g \u03b81 for images, and g \u03b82 for categories. Both of them take as input the image and category representations used to create the target soft assignment matrix (i.e., we take the representations learned by BID1 when its probability scores Y 2 are used as supervision, and the representations learned by Snell et al. (2017) otherwise) . In this context, we alternately optimize g \u03b81 by fixing M (which depends on g \u03b82 ) and optimize g \u03b82 by fixing F (which depends on g \u03b81 ).Implementation details: We consider that the learned models g \u03b81 and g \u03b82 have the same architecture and are multilayer perceptrons (MLP) with tanh activation functions. The number of hidden layers \u03bb \u2208 {0, 1, 2} and output dimensionality d are hyperparameters cross-validated from the accuracy on the validation set. More details on their architecture can be found in the supplementary material. Results: We report the performance of our approach on the test categories of the CUB and Flowers datasets in TAB2 , respectively. The performance is measured as the average classification accuracy across all unseen classes. We use DS-SJE (Char CNN-RNN) and Prototypical Networks as supervision for our model because they are the only approaches whose pre-trained models are publicly available. Our approach obtains state-of-the-art results on both CUB and Flowers datasets by significantly improving the classification performance of the different classifiers. For instance, it improves the scores of 63.9% obtained by ProtoNet of Flowers up to 68.2%. In general, it improves zero-shot learning performance of the different classifiers by 2% to 4.3%. We report in the supplementary material the performance of our model on both the validation and test sets using different numbers of hidden layers, and ranging the output dimensionality d from 16 to the dimensionality e of the input representations. Except for linear models (i.e. \u03bb = 0), reducing the dimensionality improves generalization. This shows that the zeroshot learning performance of a given model can be significantly improved by taking its prediction scores as supervision of our model. To study the impact of the dimensionality reduction generated DRPR, we also ran the codes of BID1 ; BID5 by learning representations with dimensionality smaller than those provided (using the same ranges as those in the tables of the supp. material). This decreased their generalization performance. Therefore, directly learning a low-dimensional representation is not a sufficient condition to generalize well. Our framework that learns representations so that examples with similar ambiguities (i.e. similar teacher predictions) are close to each other acts as a semantic regularizer. This is suggested by the fact that test accuracy is improved with DRPR even when e = d (as long as the MLPs contain hidden layers).It is worth mentioning that one test category of the CUB dataset (Indigo bunting) belongs to the ImageNet dataset (Deng et al., 2009 ) that was used to pretrain GoogLeNet. By using the train/val/test category splits proposed by Xian et al., we did not observe a change of performance of the different models on CUB. We have proposed a dimensionality reduction approach such that the soft clustering scores obtained in the low-dimensional space are similar to those given as input. We experimentally show that our approach improves generalization performance in zero-shot learning on challenging datasets. It can also be used to complement t-SNE, as a visualization tool to better understand learned models. In particular, we show that we can give a soft clustering interpretation to models that have probabilistic interpretations. Real-world applications that can be used with DRPR include distillation. For instance, when the teacher model is too large to store on a device with small memory (e.g., mobile phone), the student model which has a smaller memory footprint is used instead. Low-dimensional representations can also speed up retrieval tasks. Future work includes applying our approach to the task of distillation in the standard classification task where training categories are also test categories. We thank Fartash Faghri and the anonymous reviewers for their helpful comments on early versions of this manuscript. This work was supported by Samsung and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. AMF acknowledges funding from the Canada CIFAR AI Chairs Program. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. This section provides a statistical guarantee for the algorithm presented in Section 2.2. We show that the minimizer of the empirical discrepancy (4), which uses a finite number of data points x 1 , \u00b7 \u00b7 \u00b7 , x n \u2208 X , is close to the minimizer of the true expected discrepancy measure, to be defined shortly. Let us define the setup here. We are given data points D n = {X 1 , . . . , X n }.3 We suppose that each X i \u2208 X is independent and identically distributed (i.i.d.) with the distribution \u03bd \u2208 M(X ), where M(X ) is the space of all probability distributions defined over X . The teacher is a fixed function \u03c6 = [\u03c6(\u00b7; 1), . . . , \u03c6(\u00b7; k)] that maps points in X to a k-dimensional simplex, and provides the target probability distributions. That is, the target y i for X i is computed as y i = \u03c6(X i ).Consider a function space G whose domain is X and its range is a subset of R d . This function space might be represented by a DNN, but we do not make such an assumption in our statistical analysis. Given a function g \u2208 G (called g \u03b8 in the main article) and the number of clusters k, we define \u03c8 g (x) = [\u03c8 g (x; 1), . . . , \u03c8 g (x; k)] as DISPLAYFORM0 , with the cluster centres DISPLAYFORM1 and the priors DISPLAYFORM2 for c \u2208 {1, . . . , k} (cf. Section 2.1). Note that \u03c8 g (x) defines a k-dimensional probability distribution, and \u00b5 c (g) is a mapping of the function g to a point in R d .Similarly, given D n , we define the empirical cluster centre\u015d DISPLAYFORM3 .Note that here for simplicity of analysis, we assume that the priors \u03c0 c are exact, and not estimated from data. The student's goal is to find a g such that \u03c8 g is close to \u03c6. The closeness of the student to the teacher is defined based on their KL divergence. Specifically, Algorithm 1 minimizes the distorted empirical discrepancy (4), which can be written as DISPLAYFORM4 where X i s are from dataset D n . Notice that the distorted empirical discrepancy \u2206 n (\u03c8, \u03c6) is defined based on\u03c8, which uses the empirical centres\u03bc c , instead of the expected centres \u00b5 c . We also define an empirical discrepancy w.r.t. the true \u00b5 c as \u2206 n (\u03c8, \u03c6). This quantity is not accessible to the algorithm. We evaluate the quality of g, and its corresponding \u03c8 g , based on how well, in average, it performs on new points x \u2208 X . We consider the expected KL divergence between \u03c8 and \u03c6 w.r.t. distribution \u03bd as the measure of performance. Therefore, the discrepancy is DISPLAYFORM5 The output of Algorithm 1 is the minimizer 5 of \u2206 n (\u03c8, \u03c6), which we denote by\u011d DISPLAYFORM6 We also define the minimizer of the discrepancy by g * : DISPLAYFORM7 We would like to compare the performance of\u011d when evaluated according to discrepancy, that is \u2206(\u03c8\u011d, \u03c6), and compare it with \u2206(\u03c8 g * , \u03c6).Before stating our results, we enlist our assumptions. We shall remark on them as we introduce. DISPLAYFORM8 consists of i.i.d. samples drawn from \u03bd(X ).The i.i.d. assumption simplifies the analysis. With extra effort, one can provide similar results for some classes of dependent processes too. For example, if the dependent process comes from a time series and it gradually \"forgets\" its past, one may still obtain similar statistical guarantees. Forgetting can be formalized through the notion of \"mixing\" of the underlying stochastic process (Doukhan, 1994) . One can then provide statistical guarantees for learning algorithms under various mixing conditions BID17 Meir, 2000; BID7 Mohri & Rostamizadeh, 2009; BID14 Farahmand & Szepesv\u00e1ri, 2012) . DISPLAYFORM9 This is a mild and realistic assumption on the function space G, and is mainly here to simplify some steps of the analysis. Assumption A3 (Teacher) Part I) The output of the teacher \u03c6 is a probability distribution, i.e., for any x \u2208 X and c = {1, . . . , k}, we have \u03c6(x; c) \u2265 0 and k c=1 \u03c6(x; c) = 1. Part II) We assume that \u03c0 c = E [\u03c6 c (X)] is bounded away from zero for all c. We set \u03c0 min = min c \u03c0 c . This first part of the assumption explicitly expresses the fact that the algorithm expects to receive a probability distribution from the teacher. If it does not, for example if \u03c6(x; c) is negative for some x and c but we treat it as a probability in the calculation of the KL divergence, the algorithm would not be well-defined. The second part of this assumption requires that prior probability for each cluster is bounded away from zero, and has the probability at least \u03c0 min . This is a technical assumption used by the proof technique; it might be possible that one can relax this assumption. We need to make some assumptions about the function space G and its complexity, i.e., capacity. We use covering number (and its logarithm, i.e., metric entropy) as the characterizer of the complexity. The covering number at resolution \u03b5 is the minimum number of balls with radius \u03b5 required to cover the space M according to a particular metric. We use N (\u03b5, G, \u00b7 ) to denote the covering number of G w.r.t. the norm \u00b7 , which we shall explicitly specify. As \u03b5 decreases, the covering number increases (or more accurately, the covering number is non-decreasing). For example, the covering number for a p-dimensional linear function approximator with constraint on the magnitude of its Let us define a norm for the function space G: DISPLAYFORM10 This is a mixed-norm where we compute the 2 -norm for each g(x) \u2208 R d , and then take the supremum norm over the resulting 2 -norm. We use this norm to characterize the covering number of G.Assumption A4 (Metric Entropy) There exists constants B > 0 and 0 < \u03b1 < 1 such that for any \u03b5, the following covering number (i.e., metric entropy) condition is satisfied: DISPLAYFORM11 The logarithm of the covering number of G is O( 1 \u03b5 2\u03b1 ). It grows much faster than the metric entropy of linear models, which is O(p log( 1 \u03b5 )). This behaviour is suitable to capture the complexity of large function spaces such as the Sobolev space W k (R d ) and many reproducing kernel Hilbert spaces (RKHS).6 Note that we use a mixed-norm to define the covering number. The use of supremum norm in the definition might be considered conservative. Using a more relaxed norm, for example based on the empirical L p (P X1:n )-norm for some 1 \u2264 p < \u221e, is an open technical question for future work. Finally let us define the pointwise loss function DISPLAYFORM12 Notice that \u2206 n (\u03c8 g , \u03c6) = DISPLAYFORM13 We define the following function space: DISPLAYFORM14 We also define the entropy integral (Dudley integral) DISPLAYFORM15 We are now ready to state the main theoretical result of this section. Theorem 1. Suppose that Assumptions A1, A2, and A3 hold. Consider\u011d obtained by solving (6).There exists a finite c 1 > 0 such that for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, we have DISPLAYFORM16 Furthermore, if the metric entropy satisfies Assumption A4, there exist constants c 2 , c 4 > 0 and a function c 3 (\u03b1) > 0, which depends only on \u03b1, such that for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, we have DISPLAYFORM17 , the Sobolev space defined w.r.t. the L2-norm of the weak derivatives, we can DISPLAYFORM18 This theorem provides a finite sample error upper bound on the true discrepancy of\u011d, and relates it to the expressiveness and complexity of the function space G, the number of samples n, and some other properties. The term min g\u2208G \u2206(\u03c8 g , \u03c6) = \u2206(\u03c8 g * , \u03c6) is the function approximation error, and reflects the expressiveness of G. This is the minimum achievable error given the function space G. The other terms in the upper bound correspond to the estimation error caused by having a finite number of data points. Let us focus on the second part of the theorem, which is under the particular choice of covering number according to Assumption A4. In that case, the estimation error shows n \u22121/2 dependence on the number of samples, and hence decreases as we have more training data. We observe that the upper bound increases as the range L of the function space G, the dimension d of the low-dimensional space, and the number of clusters k increases. The effect of using the distorted empirical discrepancy \u2206 n (\u03c8 g , \u03c6) instead of \u2206 n (\u03c8 g , \u03c6) shows itself in the last term, i.e., the term with the constant multiplier of (equality under the uniform distribution over classes), so we have at least a linear dependence on k in the upper bound. This dependence might be due to our proof technique; it remains to be seen whether this can be improved. Proof. To simplify the notation, we denote DISPLAYFORM19 , and \u2206(g) = \u2206(\u03c8 g , \u03c6). We want to relate \u2206(\u011d) to \u2206(g * ), the supremum of the empirical process \u2206(g) \u2212 \u2206 n (g) and the supremum of the distortion of the empirical loss \u2206 n (g) \u2212\u2206 n (g). We have the following relations: DISPLAYFORM20 The first inequality is because of the the optimizer property of\u011d, i.e.,\u2206 n (\u011d) \u2264\u2206 n (g) for any g \u2208 G, including g * .We need to upper bound the supremum of the empirical process, that is sup g\u2208G |\u2206(g) \u2212 \u2206 n (g)|, and the supremum of the distortion caused by using\u03c8 in minimizing \u2206 n instead of \u03c8, that is sup g\u2208G |\u2206 n (g) \u2212\u2206 n (g)|, cf (6).Upper Bounding sup g\u2208G |\u2206(g) \u2212 \u2206 n (g)|. We use Lemma 7 in Appendix B.3 in order to upper bound the supremum of the empirical process, sup g\u2208G |\u2206(g) \u2212 \u2206 n (g)|, which is equivalent to sup l\u2208L DISPLAYFORM21 . That lemma, which is originally Theorem 2.1 of Bartlett et al. (2005) , relates the supremum of the empirical process to the Rademacher complexity of L, defined in the same appendix. To apply the lemma, we first provide upper bound on l(x) and DISPLAYFORM22 for example, see the proof leading to (19). So DISPLAYFORM23 We evoke Proposition 4 with the choice of f (x; c) = g(x) \u2212 \u00b5 c 2 2 and L = 4dL 2 to obtain that l(x; g) \u2264 8dL 2 + log 2 k. Since l(x; g) is bounded, we have DISPLAYFORM24 By the choice of \u03b2 = 1, B = (8dL 2 + log 2 k), and r = (8dL 2 + log 2 k) 2 in Lemma 7, we get that for any \u03b4 1 > 0, DISPLAYFORM25 with probability at least 1 \u2212 \u03b4 1 . . This can be done by using Dudley's integral to relate the Rademacher complexity of L to the covering number of L. Afterwards, we use Lemma 3 in Appendix A.1 to relate the covering number of L to the covering number of G. We have DISPLAYFORM0 In the second inequality, we benefit from two observations: first, we use l(x) \u2264 8dL 2 + log 2 k for any l \u2208 L (11) to upper bound diam(L); second, the covering number w.r.t. L 2 (P X1:n ) can be upper bounded by the covering number w.r.t. the supremum norm. Upper Bounding sup g\u2208G \u2206 n (g) \u2212\u2206 n (g) . We use Proposition 5 in Appendix A.2, which states that for any \u03b4 2 > 0, DISPLAYFORM0 with probability at least 1 \u2212 \u03b4 2 .Plugging FORMULA0 and FORMULA0 in FORMULA0 and using the entropy integral upper bound (13) lead to the desired result of the first part. To prove the second part of the theorem, we use log N \u03b5, G, \u00b7 \u221e,2 \u2264 B \u03b5 2\u03b1 to calculate J (\u03b5), which results in DISPLAYFORM1 By plugging in \u03b5 = 16dL 2 + 2 log 2 k, we get that DISPLAYFORM2 We upper bound J (2L) in FORMULA0 to obtain: DISPLAYFORM3 After some simplifications these lead to the desired result of the second part. A.1 SOME TECHNICAL TOOLSWe develop some technical tools required in the proof of Theorem 1. Proposition 2 provides Lipschitz constant for some functions that are used in the proof of Lemma 3 to relate the covering number of the function space L, defined in (8), to that of G. This was a key step of the proof of the theorem. Proposition 4 provides an upper bound on the magnitude of l(x; f ), shortly defined in (16).We introduce a few more notations to reduce the clutter. Let DISPLAYFORM4 , so we can write DISPLAYFORM5 For a function f : X \u00d7 {1, . . . , k} \u2192 R, we define DISPLAYFORM6 We overload the pointwise loss function l(x; g), defined in FORMULA27 , and define a similar definition for l(x; f ) as follows DISPLAYFORM7 It is clear that with the choice of f = d g , the probability distribution p f is the same as \u03c8 g .The following proposition specifies the Lipschitz properties of d g and l(x; f ). Proposition 2. Suppose that Assumption A3 hold. Part I) Consider g 1 , g 2 \u2208 G and let Assumption A2 hold. For any x \u2208 X and c \u2208 {1, . . . , k}, we have DISPLAYFORM8 Part II) Consider two functions f 1 , f 2 : X \u00d7 {1, . . . , k} \u2192 R. For any x \u2208 X we have DISPLAYFORM9 Proof. Part I) First notice that for any two vectors u and v, by the Cauchy-Schwarz inequality we have DISPLAYFORM10 Consider g 1 , g 2 \u2208 G, and their corresponding d g1 and d g2 . We have DISPLAYFORM11 Note that DISPLAYFORM12 where we used Jensen's inequality and the fact that \u03c6 c (x) \u2265 0. As \u00b5 c (0) = 0, we also obtain that DISPLAYFORM13 As DISPLAYFORM14 Therefore by FORMULA0 , FORMULA0 , and (19), DISPLAYFORM15 Part II) For functions f 1 , f 2 , using the definition of l(x; f ) (16), we get that DISPLAYFORM16 By substituting the definition (15) and some simplifications, we get DISPLAYFORM17 with DISPLAYFORM18 We study the Lipschitz property of \u03c1(u) as a function of u in order to upper bound the second term on the right-hand side (RHS).We take the derivative of \u03c1(u) w.r.t. the c-th component of u to obtain that DISPLAYFORM19 Notice that q c (u) is a probability distribution. We denote (q 1 (u), . . . , q k (c)) by q(u).By Taylor's theorem, for u, u \u2208 R k , we have DISPLAYFORM20 for some\u0169 = (1 \u2212 \u03bb)u + \u03bbu with 0 \u2264 \u03bb \u2264 1. By H\u00f6lder inequality, for any H\u00f6lder conjugate DISPLAYFORM21 where max over u \u2264\u0169 \u2264 u should be understood as the maximum over the line segment between u and u .In particular, DISPLAYFORM22 Here we used the fact that q c (u) is a probability distribution and its sum is equal to 1, for any choice of\u0169. We substitute FORMULA0 in FORMULA1 and use the upper bound |\u03c1(\u2212f 2 (x; \u00b7)) \u2212 \u03c1(\u2212f 1 (x; \u00b7))| \u2264 f 1 (x; \u00b7) \u2212 f 2 (x; \u00b7) \u221e , which is just shown, to get that DISPLAYFORM23 as desired. The following lemma relates the covering number of the function space L to the covering number of the function space G. Lemma 3. Consider the function space G and its induced function space L (8). Let Assumptions A2 and A3 hold. For any \u03b5 > 0, we have DISPLAYFORM24 and using both parts of Proposition 2, we get that DISPLAYFORM25 If we have an \u03b5-cover of G w.r.t. g \u221e,2 = sup x\u2208X g(x) 2 , it induces a 16L \u221a d\u03b5-cover on L w.r.t. the supremum norm. The following proposition upper bounds the magnitude of l(x; f ). Proposition 4. Suppose that Assumption A3 hold and |f (x; c)| \u2264 L for any x \u2208 X and c \u2208 {1, . . . , k}. Consider p f defined in (15). It holds that DISPLAYFORM26 Proof. For simplicity, we ignore the dependence on x. We use the definition of p f (15) to get DISPLAYFORM27 Let us consider each term on the RHS.\u2022 As log \u03c6(c) \u2264 0, we have c \u03c6(c) log \u03c6(c) \u2264 0.\u2022 The priors (\u03c0 1 , . . . , \u03c0 k ) indeed defines a probability distribution, as each \u03c0 c is nonnegative and c \u03c0 c = c \u03c6 c (x)d\u03bd(x) = c \u03c6 c (x)d\u03bd(x) = 1 \u00d7 d\u03bd(x) = 1. So \u2212 c \u03c6(c) log \u03c0 c is the entropy of a probability distribution over an alphabet with size k, which is at most log 2 k.\u2022 The summation c \u03c6(c)f (c) is upper bounded by L because of the boundedness of f (c) \u2264 L and the fact that \u03c6 is a probability distribution and sums to one.\u2022 Consider the term c \u03c6(c) log ( b \u03c0 b exp(\u2212f (b))). By the boundedness of f (b), we have DISPLAYFORM28 Collecting all these terms leads to the upper bound of 2L + log 2 k. This section provides an upper bound on the distortion of the empirical discrepancy, i.e., sup g\u2208G |\u2206 n (g) \u2212\u2206 n (g)|.Proposition 5. Suppose that Assumptions A1, A2, and A3 hold. For any \u03b4 > 0, there exists a constant c 1 > 0 such that with probability at least 1 \u2212 \u03b4, we have DISPLAYFORM0 . Also from the definition of l(x, f ) (16), we can write DISPLAYFORM1 Therefore, for any g \u2208 G and by the application of Proposition 2 (Part II), we have DISPLAYFORM2 where the supremum norm is taken over the centres c = {1, . . . , k}. For any x \u2208 X and any c = {1, . . . , k}, we have DISPLAYFORM3 It was shown in (19) that \u00b5 c (x) 2 is upper bounded by sup x\u2208X g(x) 2 . One may similarly show the same for \u03bc c (x) 2 : DISPLAYFORM4 This together with FORMULA1 and FORMULA1 show that DISPLAYFORM5 Proposition 6, which we prove soon, upper bounds sup g\u2208G \u00b5 c (g) \u2212\u03bc c (g) 2 . By a union bound argument over c \u2208 {1, . . . , k}, we get that for any fixed \u03b4 > 0, there exists a constant c 1 > 0 such that DISPLAYFORM6 with probability at least 1 \u2212 \u03b4. This proposition upper bounds the supremum of the 2 distance between cluster centres. Proposition 6. Suppose that Assumptions A1, A2, and A3 hold. Consider a fixed c \u2208 {1, . . . , k}. For any \u03b4 > 0, there exists a constant c 1 > 0 such that with probability at least 1 \u2212 \u03b4, we have DISPLAYFORM7 Proof. To shorten the formulae, we use the notation DISPLAYFORM8 f (X i ) to denote the empirical expectation. We can decompose \u00b5 c (g) \u2212\u03bc c (g) as follows DISPLAYFORM9 .In the rest of the proof, we provide an upper bound for Term (I) and Term (II).Term (I): Fix \u03b4 1 > 0. We denote 1 d as the d-dimensional vector with all components equal to 1. As DISPLAYFORM10 where the comparison is dimension-wise. Therefore, DISPLAYFORM11 We provide a probabilistic upper bound on DISPLAYFORM12 Since \u03c6 c is a fixed 1-bounded function, we use Hoeffding's inequality to upper bound it. After some manipulations, we obtain that DISPLAYFORM13 with probability at least 1 \u2212 \u03b4 1 . We use this along (25) and the assumption that DISPLAYFORM14 with probability at least 1 \u2212 \u03b4 1 . We want to upper bound the supremum of the norm of second term (II). Fix \u03b4 2 > 0. To simplify the notation, let us first define function f (x) = \u03c6 c (x)g(x) corresponding to a function g. Functions f are mapping from X to R d . Furthermore, we define the function space DISPLAYFORM0 , where g s is the s-th dimension of g. With this notation, we can write DISPLAYFORM1 Let us focus on the supremum over F term. We have DISPLAYFORM2 We take the square root of both sides and use the fact that a 2 1 + . . . DISPLAYFORM3 Notice that as \u03c6 c (x) is bounded by 1 and g is L-bounded dimension-wise, each f s (x) is also Lbounded. We use Lemma 7 (Appendix B.3) on each term to obtain a high probability upper bound. We choose the parameters of that lemma as \u03b2 = 1, B = L, r = L 2 , and \u03b4 = \u03b4 2 /d. This leads to DISPLAYFORM4 with probability at least 1 \u2212 \u03b4 2 .In order to control E [R n (F s )], we relate it to the covering number of F s . We see that the covering number of F s can be upper bounded by the covering number of F, which in turn can be upper bounded by the covering number of G. First, we use Dudley's integral to get DISPLAYFORM5 The covering number argument is as follows. Choose any functions f s , f s \u2208 F s . For any sequence x 1:n , the squared of the empirical 2 -norm is DISPLAYFORM6 where in the last step we used the fact that \u03c6 c (x) \u2264 1.An \u03b5-cover of G w.r.t. \u00b7 \u221e,2 induces an \u03b5-cover of G w.r.t. L 2 (P x1:n ), which in turn induces an \u03b5-cover on F s w.r.t. L 2 (P x1:n ), as the inequality above shows. Notice that this holds for any choice of x 1:n , including X 1:n appearing in the Dudley's integral (28). Therefore, by (28) and setting diam(F s ) = 2L, we have DISPLAYFORM7 where we used the definition of the entropy integral of G (9).After plugging this upper bound in (27) and use the upper bound (26), which was obtained for Term (I), we see that DISPLAYFORM8 with probability at least 1 \u2212 (\u03b4 1 + \u03b4 2 ). We set \u03b4 1 = \u03b4 2 = \u03b4/2 and simplify the upper bound to obtain the desired result. For the convenience of the reader, we collect some auxiliary definitions and results that are used in the our proofs. Here we briefly define some of the notations that we use throughout the paper. Consider the domain X and a function space F : X \u2192 R. We do not deal with measure theoretic considerations, so we just assume that all functions involved are measurable w.r.t. an appropriate \u03c3-algebra for that space. We use M(X ) to refer to the set of all probability distributions defined over that space. We use symbols such as \u03bd \u2208 M(X ) to refer to probability distributions defined over that space. We use f p,\u03bd to denote the L p (\u03bd)-norm (1 \u2264 p < \u221e) of a measurable function f : X \u2192 R, i.e., DISPLAYFORM0 The supremum norm is defined as DISPLAYFORM1 . . , x n be a sequence of points in X . We use x 1:n to refer to this sequence. The empirical measure P n is the probability measure that puts a mass of 1 n at each x i , i.e., DISPLAYFORM2 where \u03b4 x is the Dirac's delta function. For DISPLAYFORM3 We can also define other L p (P n )-norms similarly. 8 When there is no chance of confusion about D n , we may denote the empirical norm simply by f n . We quote the definition of the covering number from Gy\u00f6rfi et al. (2002) . Definition 1 (Definition 9.3 of Gy\u00f6rfi et al. 2002) . Let \u03b5 > 0, F be a set of real-valued functions defined on X , and \u03bd X be a probability measure on X . Every finite collection of N \u03b5 = {f 1 , . . . , f N\u03b5 } defined on X with the property that for every f \u2208 F, there is a function f \u2208 N \u03b5 such that f \u2212 f p,\u03bd X < \u03b5 is called an \u03b5-cover of F w.r.t. \u00b7 p,\u03bd X .Let N (\u03b5, F, \u00b7 p,\u03bd X ) be the size of the smallest \u03b5-cover of F w.r.t. \u00b7 p,\u03bd X . If no finite \u03b5-cover exists, take N (\u03b5, F, \u00b7 p,\u03bd X ) = \u221e. Then N (\u03b5, F, \u00b7 p,\u03bd X ) is called an \u03b5-covering number of F and log N (\u03b5, F, \u00b7 p,\u03bd X ) is called the metric entropy of F w.r.t. the same norm. Given a x 1:n = (x 1 , . . . , x n ) \u2282 X and its corresponding empirical measure P n = P x1:n , we can define the empirical covering number of F w.r.t. the empirical norm \u00b7 p,x1:n and is denoted by DISPLAYFORM0 We define Rademacher complexity and quote a result from Bartlett & Mendelson (2002) . For more information about Rademacher complexity, we refer the reader to Bartlett & Mendelson (2002); Bartlett et al. (2005) .Let \u03c3 1 , . . . , \u03c3 n be independent random variables with P {\u03c3 i = 1} = P {\u03c3 i = \u22121} = 1/2. For a function space F : DISPLAYFORM0 , in which the expectation is w.r.t. both \u03c3 and X i . Rademacher complexity appears in the analysis of the supremum of an empirical process right after the application of the symmetrization technique. As such, its behaviour is closely related to the 8 Or maybe more clearly, Pn(A) = 1 n n i=1 I{xi \u2208 A} for any measurable subset A \u2282 X . behaviour of the empirical process. One may interpret the Rademacher complexity as a complexity measure that quantifies the extent that a function from F can fit a noise sequence of length n (Bartlett & Mendelson, 2002) .The following is a simplified (and slightly reworded) version of Theorem 2.1 of Bartlett et al. (2005) .Lemma 7. Let F : X \u2192 R be a measurable function space with B-bounded functions. Let X 1 , . . . , X n \u2208 X be independent random variables. Assume that for some r > 0, Var [f (X i )] \u2264 r for every f \u2208 F. Then for every \u03b4 > 0, with probability at least 1 \u2212 \u03b4, DISPLAYFORM1 We give implementation details about the experiments of our submitted paper in Section C.1. We report the detailed performance of our model as a function of the output dimensionality and the number of hidden layers in the zero-shot learning context in Section C.2. We show in Section C.3 that our method can be generalized to hard clustering by using implicit centers. We give additional visualization results in Section C.4. We coded our method in PyTorch and ran all our experiments on a single Nvidia GeForce GTX 1060 which has 6GB of RAM.PyTorch automatically calculates the gradient w.r.t. the mini-batch representation F . Nonetheless, it is worth mentioning that both the first and second arguments of our prediction function \u03c8(F, M, \u03c0) depend on F in the case where the centers are implicit (i.e., when we write M = diag(Y 1 n ) \u22121 Y F ). In this case, the gradient of our loss function w.r.t. F depends on both the first and second arguments of \u03c8. We now give details specific to the zero-shot experiments. In the zero-shot learning experiment where F and M are computed from different sources (i.e., images and text) and are the output of two different networks, the optimization is performed by alternately optimizing one variable while fixing the other. Mini-batch size: The training datasets of CUB and Flowers contain 5894 and 5878 images, respectively. In order to fit into memory, we set our mini-batch sizes as 421 (= 5894/14) and 735 (\u2248 5878/8) for CUB and Flowers, respectively. . In this case, we formulate: DISPLAYFORM0 Using a temperature of 10 made the optimization more stable as it avoided gradients with high values. We use a temperature of 2 when using the representations provided by BID1 .Initial temperature of our model: To make our optimization framework stable, we start with a temperature of 50. We then formulate our Bregman divergence as: DISPLAYFORM1 where f i and \u00b5 c are the representations learned by our model. We decrease our temperature by 10% (i.e., temp t+1 = 0.9temp t ) every 3000 epochs until the algorithm stops training. We stop training at 10k epochs on CUB and 1k epochs on Flowers. We now give details specific to the visualization experiments. Dataset size: To be comparable to t-SNE, we directly learn two-dimensional embeddings instead of neural networks. Our mini-batch size is the size of the test set (i.e., the number of examples is n = 10 4 for most datasets except STL that contains n = 8000 test examples).Optimizer: We use the RMSprop optimizer with a learning rate of 10 \u22123 , \u03b1 = 0.99, = 10 \u22126 , the weight decay and momentum are both 0, and the data is not centered. We also we formulate the empirical discrepancy loss DISPLAYFORM0 k the vector containing the logits of the learned representation of the i-th test example. We formulate y i = [y i,1 , \u00b7 \u00b7 \u00b7 , y i,k ] \u2208 R k our target assignment vector for the i-th test example as follows: DISPLAYFORM1 where \u03c4 = 5 for all the dataset except CIFAR-100 for which \u03c4 = 4.We report the quantitative scores for \u03c4 = 1.Initial temperature of our model: We learned our representation by using a fixed temperature of 1 (i.e., using the standard squared Euclidean distance).We stop the algorithm after 8000 iterations. Tuning t-SNE: we tested different ranges of scaling (1/1, 1/10, 1/100) and perplexity (i.e., 1, 10, 30 (default) and 100) and reported the representations that obtained the best quantitative results. Let e \u2208 N be the dimensionality of the representations taken as input and d the output dimensionality of the models g \u03b81 and g \u03b82 , the architecture of the models is e-d and e-e-d in the 1 and 2 hidden layer cases, respectively. The hyperparameter d \u2208 {16, 32, 64, \u00b7 \u00b7 \u00b7 , e} is also a hyperparameter cross-validated on the validation set. We give the detailed accuracy performance of our model in TAB5 .\u2022 TAB5 reports the test performance of our model on CUB when using the features provided by BID1 as supervision for different numbers of hidden layers and values of output dimensionality of our model.\u2022 TAB6 (resp. Table 7) reports the validation (resp. test) performance of our model on CUB when using the features provided by BID5 as supervision.\u2022 Table 8 (resp. Table 9 ) reports the validation (resp. test) performance of our model on Flowers when using the features provided by BID5 as supervision. Dimensionality reduction improves performance, though the optimal dimensionality is dataset specific. In general, increasing the number of hidden layers also helps. Table 8 : Validation accuracy (in %) as a function of the output dimensionality when using ProtoNet BID5 Table 9 : Test accuracy (in %) as a function of the output dimensionality when using ProtoNet BID5 as supervision on Flowers C.3 GENERALIZATION TO HARD CLUSTERING We validate that DRPR can be used to perform hard clustering as in BID5 but with implicit centers. To this end, we train a neural network with 2 convolutional layers on MNIST (LeCun et al., 1998) followed by a fully connected layer. Its output dimensionality is d = 2 or d = 3, the mini-batch size is n = 1000, the number of categories is k = 10 and the target hard assignment matrix Y \u2208 {0, 1} n\u00d7k contains category membership information (i.e., Y ic is 1 if the example f i belongs to category c, 0 otherwise). We train the model on the training set of MNIST and plot in Fig. 6 the representations of the test set. By assigning each test example to the category with closest centroid (obtained from the training set), the model obtains 98% (resp. 99%) accuracy when d = 2 (resp. d = 3). DRPR can then be learned for hard clustering when the centers are implicitly written as a function of the mini-batch matrix representation F and the target hard assignment matrix Y .Published as a conference paper at ICLR 2019 Figure 6: Visualization of the representation learned on MNIST by our approach in the supervised hard clustering setup. The left (resp. right) figure is the representation learned by our model when its output dimensionality is d = 2 (resp. d = 3). We now present visualization results. C.4.1 ARTIFACTS WITH T-SNE FIG11 illustrates the CIFAR 100 representation learned by t-SNE when its input data is the target probability distribution that we give as supervision/input of our algorithm. Following the recommendations mentioned in https://lvdmaaten.github.io/tsne/ when the representation form a strange ball with uniformly distributed points and obtaining very low error, we decreased the perplexity from 30 (which is the value by default) to 10 and 1 and divided our data by 10, 100 and 1000. Nonetheless, we still obtained the same type of representation as in FIG11 .This kind of artifact is the reason why we only report results obtained with logits. We plot in FIG12 the visualization obtained by t-SNE when using the KL or JS divergences to compare pairs of probability distribution representations. The representations obtained in this case are still worse than using the original 3-dimensional representations as the cluster structures are not preserved, nor the inter-cluster distances. This suggests that comparing pairs of examples, as done by t-SNE, is less appropriate than our method that considers similarities between examples and the different k = 6 clusters. C.4.3 ADDITIONAL RESULTS FIG13 illustrates the DRPR and t-SNE representations of CIFAR 10. Animal categories are illustrated on the right whereas machines are on the left. FIG14 illustrates the DRPR and t-SNE representations of CIFAR 100. We learned the representations by exploiting 100 clusters but plot only 20 colors (one for each superclass of CIFAR 100), which is why multiple spikes have the same color. Groups with same colors are better defined with our approach than with t-SNE, this means that different categories from the same superclass (e.g., hamster, mouse, rabbit, shrew, squirrel which are small mammals) are grouped together with DRPR. One can observe a semantic structure in the 2D representation of DRPR: plants and insects are on the top left; animals are on the bottom left and categories on the right are outdoor categories. Medium mammals are also represented between small mammals and large carnivores. Figures 10, 11, 12 and 13 illustrate the representations learned by our model for the STL, MNIST, CIFAR 100 and CIFAR 10 datasets, respectively. Instead of using colors that represent the categories of the embeddings as done in the submitted paper, we directly plot the images. In general, we observe that images towards the end of spikes consist of a clearly visible object in a standard viewpoint on a simple background. Those closer to the center often have objects with a non-standard viewpoint or have a complex textured background. At a high-level, the classes appear to be organized by their backgrounds. Taking the STL-10 visualization as an example, deer and horses are close together since they both tend to be found in the presence of green vegetation. These classes are far from boats and planes, which often have solid blue backgrounds. Looking more closely, the ordering of classes is sensible. Planes are neighbors to both boats (similar background) and birds (similar silhouette). And trucks neighbor both cars (similar background) and horses, which appear visually similar, particularly for images in which the horse is pulling a cart. Taking the MNIST visualization as another example, one can observe that written characters in spikes are easy to recognize as they correspond to examples for which the learned model has high confidence in its scores. On the other hand, ambiguous examples are between multiple spikes (e.g., the characters 0 and 6 between spikes are more ambiguous than their neighbors in spikes)."
}