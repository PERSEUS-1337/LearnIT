{
    "title": "ByeNra4FDB",
    "content": " Conventional out-of-distribution (OOD) detection schemes based on variational autoencoder or Random Network Distillation (RND) are known to assign lower uncertainty to the OOD data than the target distribution. In this work, we discover that such conventional novelty detection schemes are also vulnerable to the blurred images. Based on the observation, we construct a novel RND-based OOD detector, SVD-RND, that utilizes blurred images during training. Our detector is simple, efficient in test time, and outperforms baseline OOD detectors in various domains. Further results show that SVD-RND learns a better target distribution representation than the baselines. Finally, SVD-RND combined with geometric transform achieves near-perfect detection accuracy in CelebA domain. Out-of distribution (OOD), or novelty detection aims to distinguish samples in unseen distribution from the training distribution. A majority of novelty detection methods focus on noise filtering or representation learning. For example, we train an autoencoder to learn a mapping from the data to the bottleneck layer and use the bottleneck representation or reconstruction error to detect an OOD (Sakruada et al., 2014; Pidhorskyi et al., 2018) . Recently, deep generative models (Kingma et al., 2014; Dinh et al., 2017; Kingma et al., 2018; Schlegl et al., 2017) are widely used for novelty detection due to their ability to model high dimensional data. However, OOD detection performance of deep generative models has been called into question since they have been observed to assign a higher likelihood to the OOD data than the training data (Nalisnick et al., 2019; Choi et al., 2018) . On the other hand, adversarial examples are widely employed to fool the classifier, and training classifiers against adversarial attacks has shown effectiveness in detecting unknown adversarial attacks (Tramer et al., 2018) . In this work, we propose blurred data as the adversarial example. When we test novelty detection models on the blurred data generated by Singular Value Decomposition (SVD), we found that the novelty detection models assign higher confidence to the blurred data than the original data. Motivated by this observation, we employ blurring to prevent the OOD detector from overfitting to low resolution. We propose a new OOD detection model, SVD-RND, which is trained using the idea of Random Network Distillation (RND) (Burda et al., 2019) to discriminate the training data from the blurred image. SVD-RND is evaluated in the hard target to OOD domain where vanilla generative models show nearly 50% detection accuracy, such as CIFAR-10 to SVHN and ImageNet to CIFAR-10 (Nalisnick et al., 2019) . Compared to conventional baselines, SVD-RND shows a significant performance gain from 50% to over 90% in these domains. Moreover, SVD-RND shows improvements over baselines on domains where conventional OOD detection schemes show moderate results, such as CIFAR-10 to LSUN. The goal of OOD detection is to determine whether the data is sampled from the target distribution D. Therefore, based on the training data D train \u2282 D, we train a scalar function that expresses the confidence, or uncertainty of the data. The performance of the OOD detector is tested on the D test \u2282 D against the OOD dataset D OOD . We denote a target and OOD pair as target : OOD in this paper, e.g., CIFAR-10 : SVHN. In this section, we mention only closely related fields to our research. For a broader survey on deep OOD detection, we recommend the paper from Chalapathy et al. (2019) . OOD Detection: A majority of OOD detection methods rely on a reconstruction error and representation learning. Ruff et al. (2018) trained a deep neural network to map data into a minimum volume hypersphere. Generative probabilistic novelty detection (GPND) (Pidhorskyi et al., 2018) employed the distance to the latent data manifold as the confidence measure and trained the adversarial autoencoder (AAE) to model the manifold. Deep generative models are widely employed for latent space modeling in OOD detection (Zenati et al., 2018; Sabokrou et al., 2018) . However, a recent paper by Nalisnick et al. (2019) discovered that popular deep generative models, such as variational autoencoder (VAE) (Kingma et al., 2014) or GLOW (Kingma et al., 2018) , fail to detect simple OOD from the training distribution. While adversarially trained generative models, such as generative adversarial networks (GAN) or AAE, are not discussed in Nalisnick et al. (2019) , our experiments in GPND show that such models can also struggle to detect such simple OODs. OOD Detection with Additional Data: Some methods try to solve OOD detection by appending additional data or labels for training. Hendrycks et al. (2019) trained the detector to additional outlier data independent of the test OOD data. Ruff et al. (2019) employed semi-supervised learning for anomaly detection in the scenario where we have ground truth information on few training data. Golan et al. (2018) designed geometrically transformed data and trained the classifier to distinguish geometric transforms, such as translation, flipping, and rotation. Shalev et al. (2018) fine-tuned the image classifier to predict word embedding. However, the intuition behind these methods is to benefit from potential side information, while our self-generated blurred image focuses on compensating the deep model's vulnerability to OOD data with a lower effective rank. Adversarial Examples and OOD Detection on Labeled Data: Some methods combine OOD detection with classification, resulting in OOD detection on labeled data. Adversarial examples can be viewed as generated OOD data that attacks the confidence of a pretrained classifier. Therefore, two fields share similar methodologies. For example, Hendrycks et al. (2017) set the confidence as the maximum value of the probability output, which is vulnerable against the adversarial examples generated by the Fast Sign Gradient Method (FSGM) . On the other hand, Liang et al. (2018) employed FSGM counterintuitively to shift the OOD data from the target further, therefore improving OOD detection. Lee et al. (2018) employed Mahalanobis distance to measure uncertainty in the hidden features of the network, which also proved efficient in adversarial defense. Bayesian Uncertainty Calibration: Bayesian formulation is widely applied for better calibration of the model uncertainty. Recent works employed bayesian neural networks (Sun et al., 2017) or interpreted a neural network's architecture in the bayesian formulation, such as dropout (Gal et al., 2016) , and Adam optimizer (Khan et al., 2018) . Our baseline, RND (Burda et al., 2019) , can be viewed as the bayesian uncertainty of the model weight under randomly initialized prior (Osband et al., 2018) . In this section, we motivate our use of blurred data as adversarial examples to conventional deep OOD detection methods. Motivatied by the observation, we present our proposed algorithm, SVD-RND, and provide intuitions why SVD-RND help OOD detection. In this work, blurred images function as adversarial examples. We directly employ the SVD on the data matrix of the single image in the training data and force the bottom non-zero singular values to zero to construct a blurred image. Suppose that data image d \u2208 D consists of i channels, where the j-th channel has N j nonzero singular values \u03c3 j1 \u2265 \u03c3 j2 \u2265 . . . \u03c3 jNj > 0. Then, the j-th channel can be represented as the weighted sum of orthonormal vectors. We prune the bottom K non-zero singular values of each channel to reconstruct the blurred image. We test conventional novelty detection methods on blurred images. We first train the VQ-VAE Oord et al. (2017) in the CIFAR-10 ( Krizhevsky et al., 2009 ) dataset. Figure 1 shows the loss of VQ-VAE on the test data and blurred test data (K = 28). We follow the settings of the original paper. VQ-VAE assigns higher likelihood to the blurred data than the original data. We note that this phenomenon is not constrained to the generative models. We trained the RND on the CIFAR-10 dataset and plot the l 2 loss in the test data and blurred test data in Figure 1 . We refer Appendix B for detailed explanation and employed architecture for the RND in the experiment. We plot the l 2 loss on SVHN (Netzer et al., 2011 ) data for relevance. Multiple skip connections in residual blocks don't resolve the information leakage on their own. Furthermore, we plot the average loss on the blurred test data and original test data during the training procedure. Throughout the training phase, the model assigns lower uncertainty to the blurred data. This trend is similar to the CIFAR-10 : SVHN phenomenon observed by Nalisnick et al. (2019) , where the generative model assigns more confidence to the OOD data from the beginning. While we employ SVD for our main blurring technique, conventional techniques in image processing can be applied for blurring, such as Discrete Cosine Transform (DCT) or Gaussian Blurring. However, DCT squares the size of the hyperparameter search space, therefore much harder to optimize than SVD. We further compare the performance between SVD and other blurring techniques in Section 4. We now present our proposed algorithm, SVD-RND. SVD-RND trains the predictor network f to discriminate between original and blurred datasets. We first generate blurred datasets D Ki from D train by zeroing the bottom K i non-zero singular values of each data channel (i = 1, . . . , b train , where b train is the number of generated blurred datasets used for training). Then, we assign a different randomly initialized target network g i to each D Ki . Finally, we assign g 0 as the target network for the original dataset. Predictor network f is trained to minimize the l 2 loss against the corresponding target network on each dataset. We do not update the target network during the training procedure. When the new sample x is given, SVD-RND outputs f (x) \u2212 g 0 (x) 2 2 as the uncertainty to the sample. Figure 2 shows the training process of SVD-RND. While the original RND paper employs a single target network to train the predictor network, SVD-RND employs multiple target networks to discriminate the original data from the blurred images. No other regularization techniques or explicit metrics are employed in SVD-RND. While SVD-RND directly regularizes only on the blurred images, we expect such regularization generally improve OOD detection for the following two reasons. First, while RND fails on OODs generated by blurring, it performs moderately on OODs generated by the orthogonal direction to the dataset. For the evidence, we show in Appendix D on CIFAR-10 dataset that RND is able to detect OODs generated by adding noise orthogonal to the data. RND outputs higher uncertainty to every OOD dataset generated from 20 independent runs. Second, Equation 2 forces the predictor network f to output g 0 (x) for the original data x \u2208 D train , and g i (x) for the blurred data x \u2208 D Ki . Therefore, f naturally learns to discriminate between the data and its low-rank projection. From such regularization, we expect f to learn the target distributionspecific information from the projection vector, which is previously neglected in conventional deep OOD detection methods. We will verify our reasoning in further experiments. In this section, we examine the performance of SVD-RND on the target : OOD domain, where deep generative models have been observed to assign a higher likelihood to the OOD data. SVD-RND is examined in the cases in Table 1 . CIFAR-10 : SVHN, CelebA (Liu et al., 2015) : SVHN, and TinyImageNet (Deng et al., 2009 ) : (SVHN, CIFAR-10, CIFAR-100) are the cases reported by Nalisnick et al. (2019) . We expect SVD-RND outperform conventional OOD detection methods by a large margin. We also studied CIFAR-10 : (LSUN (Yu et al., 2015) , TinyImageNet), LSUN : (SVHN, CIFAR-10, CIFAR-100) and CelebA: (CIFAR-10, CIFAR-100) target : OOD pairs to examine potential tradeoffs of our method. We implement the baselines and SVD-RND in the PyTorch framework. 1 For a unified treatment, we resized all images in all datasets to 32\u00d732. We refer to a detailed setting in Appendix C. For SVD-RND, we optimize the number of blurred non-zero singular values over different datasets. We choose the detector with the best performance across the validation data. We refer to all the parameter settings in Appendix C. We also examine the case where the image is blurred by DCT and Gaussian blurring. For DCT, we apply the DCT to the image, discard low magnitude signals, and generate the blurred image by inverse DCT. In DCT-RND, we optimize the number of unpruned signals in the frequency domain. For gaussian blurring, we optimize the shape of the Gaussian kernel. We denote this method as GB-RND. We compare the performance of SVD-RND, DCT-RND, and GB-RND to the following baselines. Generative Probabilistic Novelty Detector: GPND (Pidhorskyi et al., 2018) is the conventional generative model based novelty detection method that models uncertainty as a deviation of data to the latent representation, which is modeled by the adversarial autoencoder. We trained GPND with further parameter optimization. Geometric Transforms: We compare the effectiveness of the blurred image against geometric transforms (Golan et al., 2018) . The authors use four types of geometric transforms: flip, rotation, vertical translation, and horizontal translation. We compute the independent effects of each transformation by setting them as OOD proxies in the RND framework. Moreover, we also investigate the effect of pixel inversion, contrast reduction, and shearing. We refer Cubuk et al. (2019) for detailed explanation of the augmentation strategies. We employ RND (Burda et al., 2019) to show the effectiveness of our regularizer directly. Typicality Test: NalisNick et al. (2019) set the OOD metric of the generative model as the distance between the mean log likelihood of the model on the training data and the log likelihood of the model on the incoming data. We experiment typicality test on the RND framework by employing the test error of RND as the role of negative log likelihood in the generative models. Five metrics on binary hypothesis testing are used to evaluate the OOD detectors: area under the Receiver Operating Characteristic curve (AUROC), area of the region under the Precision-Recall curve (AUPR), detection accuracy, and TNR (True negative rate) at 95% TPR (True positive rate). All criterions are bounded between 0 and 1, and the result close to 1 implies better OOD detection. We summarize our results on the TNR in 95% TPR in Table 2 . We refer to full results in appendix A. In all target : OOD domains except the CelebA : (CIFAR-10, CIFAR-100) domain, SVD-RND outperforms all other baselines in every metric. Furthermore, all the proposed techniques outperform GPND and RND on all target : OOD domains, especially in CIFAR-10 : (LSUN, TinyImageNet) domains and CelebA: (CIFAR-10, CIFAR-100) domains where even GPND and RND show moderate results. For better understanding, we visualize the CIFAR-10 data before and after blurring in Appendix E. We plot the performance of SVD-RND in 50 epochs over different K 1 in Figure 3 . We increase the number of seeds to 4 to check the stability of our result. In the best performing parameter for each OOD data, SVD-RND shows narrow confidence intervals. Furthermore, we extend the Figure 3 to small K 1 for comparison. We refer the results in Appendix F. data increases (150, 38400, 1516, 2102)% over its test loss of RND. Therefore, SVD-RND further discriminates OOD from the target distribution. GPND and RND fail to discriminate OOD from the targets in CIFAR-10 : SVHN, LSUN : (SVHN, CIFAR-10, CIFAR-100), TinyImageNet : (SVHN, CIFAR-10, CIFAR-100), and CelebA : SVHN domains. Moreover, GPND performs the SVD of the jacobian matrix in test time, which makes GPND slower than SVD-RND. Furthermore, we visualize the uncertainty prediction of RND and SVD-RND. Figure 4 shows the top-9 examples on CIFAR-10 test data, where SVD-RND and RND assign the highest uncertainty. We observe that SVD-RND tends to assign higher uncertainty to blurry or hardly recognizable image compared to RND. On the other hand, OOD detection schemes based on geometric transformations (Golan et al., 2018) show generally improved results against GPND and RND on detecting OOD data compared to RND and GPND. Especially in CelebA : (SVHN, CIFAR-10, CIFAR-100) domain, rotation and translation based methods show prominent performance. However, in the CIFAR-10 target domain, OOD detection schemes based on geometric transformations show degraded performance against RND or GPND on LSUN and TinyImageNet OOD data. Furthermore, typicality test shows mixed results compared to the baseline algorithms. Finally, we also investigate the case where limited training data is available. We examined the performance of SVD-RND and RND in CIFAR-10 : (LSUN, TinyImageNet) domains. Figure 4 shows the TNR at 95% TPR metric of each method when the number of training data is reduced. For each OOD data, we denote result on SVD-RND as OOD SVD, and denote result on RND as OOD RND. Compared to RND, SVD-RND shows consistent performance when only 20% of training data is available. Furthermore, SVD-RND outperforms RND until 16% of training data is available. In this section, we further analyze and apply SVD-RND for specific scenarios. In Section 5.1, we examine whether SVD-RND learns better representation compared to the baseline. Furthermore, we propose a novel heuristic for training SVD-RND in Section 5.2, where no validation OOD data is available. Finally, we show that we can further improve the performance of SVD-RND by incorporating geometric transformations. While SVD-RND outperforms RND on every target : OOD domains in Section 4, we provide further evidence that SVD-RND learns superior target distribution representation compared to RND. For the evidence, we fine-tune a classifier over the fixed activation map of SVD-RND and RND. We set the activation map as the output of the first 15 or 27 layers of RND and SVD-RND predictor network trained in CIFAR-10 datasets. For the fine-tuning, we either appended three residual blocks and a linear output layer with softmax activation (denoted as 7-layer in Table 3 ) or a linear layer (denoted as linear in Table 3 ). Then, we fine-tune the appended network for the CIFAR-10 classification task. The SGD optimizer with learning rate 0.1 is used for fine-tuning, and the learning rate is annealed to 0.01 and 0.001 after 30 and 60 epochs over 100 epochs of training, respectively. We average the result across three fixed random seeds. We show our results in Table 3 . SVD-RND consistently outperforms RND and the randomly initialized network on the fine-tuning task. Therefore, the result supports that SVD-RND learns better target distribution-specific knowledge. Surprisingly, when we fine-tune over 7-layer neural network, RND consistently underperforms over randomly initialized weights. In our main experiments in Section 4, we used the OOD validation data for tuning the novelty detection methods. However, in realistic scenarios, OOD data are generally unknown to the detector. We propose an effective rank (Roy et al., 2007) based design of SVD-RND that does not use the OOD validation dataset and compare its performance against the results in Section 4. Log effective rank of the single image matrix D is defined as the entropy of the normalized singular values (\u03c3 1 , . . . , \u03c3 N ) of the image matrix. In equation 3, H is defined as the entropy function. Then, effective rank is set to two to the power of log effective rank. We set the effective rank of image data as the averaged effective rank of each channel. In SVD-RND, selecting each K 1 , . . . , K btrain corresponds to regularization against OOD with similar effective rank. We propose selecting each K i such that average of log effective rank on each blurred dataset is equally spaced to each other. Specifically, suppose the log effective rank of the Then, we select K i such that the average of the log effective rank in the blurred dataset with K i discarded singular values is closest to LER i . We test our criterion in CIFAR-10 and TinyImageNet data with different b train . We train SVD-RND for 25 epochs for b train = 3, and 20 epochs for b train = 4. We show the performance of SVD-RND based on uniform spacing of log effective rank in equation 4 in Table 4 , which is denoted as SVD-RND (uniform). We also show results of SVD-RND optimized with the validation OOD data from Table 2 and denote them as SVD-RND (optimized) in Table 4 . Uniform SVD-RND already outperforms the second-best methods in Table 2 . Furthermore, as b train increases, uniform SVD-RND approaches the performance of the optimized SVD-RND. While SVD-RND achieves reasonable OOD detection performance, combining SVD-RND with other baseline algorithms may further enhance the performance. For example, as shown in Table 2 , training against rotated data benefits OOD detection in CelebA dataset. Therefore, we unify SVD-RND and geometric transform-based method to further improve SVD-RND. We treat both blurred data and geometrically transformed data as OOD and train the target network to discriminate the original data from the OOD. We combine rotation and vertical translation with SVD-RND and denote them as SVD-ROT-RND and SVD-VER-RND, respectively. We compare the performance of SVD-ROT-RND and SVD-VER-RND against rotation and vertical translation in CelebA : (SVHN, CIFAR-10, CIFAR-100) domain. We refer readers to the results in Table 5 . We observe that SVD-ROT-RND and SVD-VER-RND outperform their counterparts and SVD-RND. Especially, SVD-ROT-RND and SVD-VER-RND show significant performance gain in CelebA : (CIFAR-10, CIFAR-100) domains. In this work, a blurred image is introduced as an adversarial example to the deep OOD detection method. SVD-RND is employed for adversarial defense against blurred images. SVD-RND achieves significant performance gain in all target : anomaly domains. Even without the validation OOD data, we can design SVD-RND to outperform conventional OOD detection models. We stress that such performance gain is achieved without external data or additional regularization techniques. Furthermore, experiments on SVD-RND and RND show that the neural network can potentially learn to perform OOD detection, however overfits to blurred data. Understanding this phenomenon will be beneficial to performance of the image-based models. We use RND (Burda et al., 2019) as the base model of our OOD detector. RND consists of the trainable predictor network f , and randomly initialized target network g. The predictor network is trained to minimize the l 2 distance against the target network on training data. We do not update the target network g throughout the training phase. Then, for the newly encountered data x, RND outputs f (x) \u2212 g(x) 2 2 as an uncertainty to the data. The main intuition of the RND is to reduce the distance between f and g only on the target distribution, hence naturally threshold between the target and the OOD distribution. We employ RND for our base OOD detector due to its simplicity over generative models. Also, RND has already shown to be effective in novelty detection on MNIST dataset (Burda et al., 2019) . In RND, f is generated by appending two fully connected layers in the network of g, where g consists of 3 convolution layers and a fully connected layer. In our experiments, we set g as the first 33 layers of ResNet34 without ReLU activation in the end. f is constructed by appending two sequential residual blocks. The output size of each residual block is 1024 and 512. We also discard ReLU activation in the second residual block to match the form of g. To make the OOD detection task harder, we reduce CelebA, TinyImageNet, and LSUN data into 50000 training data (for test dataset, we reduce the CelebA test data to 26032 examples). For TinyImageNet data, we discard half of the images in each class, resulting in 250 training samples for each 200 class. Reduction in LSUN dataset results in 5000 data for each 10 class. Also, the first 1000 images of the test OOD data are used for validation. For SVD-RND, and all other RND based detectors, we use the same structure for f and g defined in Appendix B. The number of parameter updates is fixed across the experiments. The Adam optimizer, with a learning rate of 10 \u22124 , is used for RND based OOD detection methods. The learning rate is annealed to 10 \u22125 in half of the training process. For our main experiment, we average the result across two fixed random seeds. In SVD-RND, DCT-RND, and GB-RND, we used one blurred data for CIFAR-10 and CelebA dataset, and two blurred data for TinyImageNet and LSUN dataset. For SVD-RND, We optimize across {18, 20, 22, 24, 25, 26, 27 , 28} in the CIFAR-10 and CelebA datasets. For TinyImageNet and LSUN datasets, we optimize over K 1 \u2208 {8, 10, 12, 14} and K 2 \u2208 {22, 24, 26, 28}. In DCT-RND, we define K i as the number of unpruned signals in the frequency domain. For CIFAR-10 and CelebA datasets, we optimize K 1 across {4, 8, 12, 14, 16, 20, 24, 28} . For TinyImageNet and LSUN datasets, we optimize over K 1 \u2208 {20, 24, 28, 32} and K 2 \u2208 {40, 44, 48, 52}. For gaussian blurring, we optimize over the shape (x i , y i ) of the Gaussian kernel. We optimized the parameter over x i \u2208 {1, 3, 5} , y i \u2208 {1, 3, 5} for each blurred data. To fix the number of updates, we train SVD-RND, DCT-RND, and GB-RND for 50 epochs in the CIFAR-10 and CelebA datasets, and 34 epochs for the rest. For GPND, the settings for the original paper are followed. Furthermore, we optimize the reconstruction loss \u03bb 1 and adversarial loss \u03bb 2 for discriminator D z across \u03bb 1 \u2208 {8, 9, 10, 11, 12} and \u03bb 2 \u2208 {1, 2, 3}. We choose the parameters with the best validation performance in 100 epochs, For RND, we trained over 100 epochs. For geometric transforms, we optimize the magnitude of the shift of shear, horizontal translation and vertical translation methods. We optimize the magnitude of translation across {4, 8, 12, 16} and choose the parameter with the best validation performance. Detector is trained for 100 |T |+1 epochs, where |T | is the number of transformations. The number of transformations is 1 in flipping and invert, 2 for horizontal translation, vertical translation, and shear, and 3 for rotation and contrast. Finally, for typicality test, we estimated the average test loss of the RND for 50000 training data as the mean. Then, when every time a sample is given, we set the distance of the test loss of the sample Table 7 : Test uncertainty of RND on OOD CIFAR-10 data generated by adding orthogonal noise to the CIFAR-10 data. Original Blurred \u03b1 = 5 \u03b1 = 10 \u03b1 = 15 \u03b1 = 20 to the mean as the OOD metric. For fair comparison, we assumed that one sample is given in a test time. In Section 3.2, we proposed that data in the blurred direction is the main weakness of the conventional novelty detection methods. For the evidence, we present the results on OODs generated by adding vectors orthogonal to the data. Precisely, we sample a Gaussian vector z and compute the component of the random vector z orth,x that is orthogonal to the data x. We scaled the l 2 norm of the orthogonal vector z orth,x on each data to be \u03b1% of the l 2 norm of the signal. We plot the average uncertainty of RND on the original data, blurred data, and the perturbed data in Table 7 . From the 20 independent runs on the perturbed data, we report the case with smallest test uncertainty in Table 7 . We varied \u03b1 from 5 to 20. While blurring reduces average test uncertainty of RND, adding orthogonal vector to the data incerases the test uncertainty of RND. For the visualization, we plot the CIFAR-10 image and blurred version processed by SVD-RND, DCT-RND, and GB-RND in Figure 5 . Images in the same column are processed with the same technique. Furthermore, column (b), (d), (e) is the best performing parameter of SVD-RND, DCT-RND, and GB-RND on SVHN OOD data. Likewise, (c), (e), (f) is the best performing parameter of SVD-RND, DCT-RND, and GB-RND on TinyImageNet OOD data. We further extend Figure 3 to analyze the behavior of SVD-RND when small number of singular values are discarded. Therefore, we experiment SVD-RND where K 1 = 0, 5, 10, 15 and plot the result in Figure 6 . When K 1 = 0, this corresponds to training a target network to follow between g 0 and g 1 . Therefore, we use the test metric for sample x as min( f (x) \u2212 g 0 (x) 2 2 , f (x) \u2212 g 1 (x) 2 2 ). We also set the test metric as f (x) \u2212 g0(x)+g1(x) 2 2 2 and this shows TNR(at 95% TPR) of 0.07/0.04/0.06."
}