{
    "title": "SkgpGgrYPH",
    "content": "Energy-based models (EBMs), a.k.a. un-normalized models, have had recent successes in continuous spaces. However, they have not been successfully applied to model text sequences.   While decreasing the energy at training samples is straightforward, mining (negative) samples where the energy should be increased is difficult.    In part, this is because standard gradient-based methods are not readily applicable when the input is high-dimensional and discrete.   Here, we side-step this issue by generating negatives using pre-trained auto-regressive language models.   The EBM then works\nin the {\\em residual} of the language model; and is trained to discriminate real text from text generated by the auto-regressive models.\n We  investigate the generalization ability of residual EBMs, a pre-requisite for using them in other applications.   We extensively analyze generalization for the task of classifying whether an input is machine or human generated, a natural task given the training loss and how we mine negatives. Overall, we observe that EBMs can generalize remarkably well to changes in the architecture of the generators producing negatives. However, EBMs exhibit more sensitivity to the training set used by such generators. Energy-based models (EBMs) have a long history in machine learning (Hopfield, 1982; Hinton, 2002; LeCun et al., 2006) . Their appeal stems from the minimal assumptions they make about the generative process of the data. Unlike directed or auto-regressive models which are defined in terms of a sequence of conditional distributions, EBMs are defined in terms of a single scalar energy function, representing the joint compatibility between all input variables. EBMs are a strict generalization of probability models, as the energy function need not be normalized or even have convergent integral. Training an EBM consists of decreasing the energy function at the observed training data points (a.k.a. positives), while increasing it at other data points (a.k.a. negatives) (LeCun et al., 2006) . Different learning strategies mainly differ in how negatives are mined (Ranzato et al., 2007) . Some find negatives by gradient descent, or using Monte Carlo methods like Gibbs sampling (Welling et al., 2005) and hybrid Monte Carlo (Teh et al., 2003) , which enable the loss to approximate maximum likelihood training (Hinton, 2002) . Other approaches instead use implicit negatives, by enforcing global constraints on the energy function, like sparsity of the internal representation (Ranzato et al., 2007) , for instance. GANs (Goodfellow et al., 2014) can be interpreted as a particular form of EBM where the negatives are generated by a learned model. While there are works exploring the use of EBMs for modeling images (Teh et al., 2003; Ranzato et al., 2013; Du & Mordatch, 2019) , they have not been successfully applied to text. One reason is that text consists of sequences of discrete variables, which makes the energy function not differentiable with respect to its inputs. Therefore, it is not possible to mine negatives using gradient-based methods. Other approaches to mine negatives are also not immediately applicable or may be too inefficient to work at scale. In this work, we start from the observation that current large auto-regressive locally-normalized language models are already strong , and therefore, it may be beneficial to use them to constrain the search space of negatives. We propose to learn in the residual space of a pre-trained language model (LM), which we accomplish by using such LM to generate negatives for the EBM. Given a dataset of positives and pre-generated negatives, the EBM can be trained using either a binary cross-entropy loss or a ranking loss, to teach the model to assign a lower energy to true human generated text than to the text generated by the pre-trained LM. The question we ask in this work is whether such an EBM can generalize well. Understanding this is important for two reason. First, this generalization is a prerequisite for using residual EBMs for modeling text. Second, in our setting, this generalization question is equivalent to the question of whether it is possible for a learned model (the energy function) to discriminate real text from text generated by an auto-regressive model. Discriminating real vs. machine-generated text is an important task on its own that has recently gained a lot of attention (Gehrmann et al., 2019; Zellers et al., 2019) . Our contribution is an extensive study of the generalization ability of such residual EBMs, or in other words, the generalization ability of models trained to detect real text from machine generated text. In particular, we assess how well the energy function is robust to changes in the architecture of the generator and to changes in the data used to train the generator. The overall finding is that the energy function is remarkably robust, and the bigger the model and the longer the generation the better its performance. Moreover, the energy function is robust to changes in the architecture of the LM producing negatives at test time. However, it is sensitive to the training dataset of the test generator. Our work can be interpreted as a particular instance of EBMs (LeCun et al., 2006) where negatives are produced by a pre-trained language model as opposed to the energy function itself. Learning a generator and a discriminator relates also to Generative Adversarial Networks (Goodfellow et al., 2014) , except that in our case the generator is trained beforehand. Since the discriminator is learned after the generator has been trained, it learns from the residual error of the generator, and therefore, our training procedure is a particular instance of a \"cascade\" model (Viola & Jones, 2001 ) and \"boosting\" (Freund & Schapire, 1997) . Using a separately trained scoring function to evaluate and rank candidate outputs has a long history which dates back to work on parsing and machine translation (Shen et al., 2004) . In that work however, the goal was to improve a weak generator by employing a linear reranker taking as input relatively few hand-design features. The approach has been recently re-discovered in the context of dialogue modeling by Kulikov et al. (2018) , but here negatives are randomly chosen next utterances from the training dataset. Several recent works have studied whether machine generations can be detected automatically, but they do not study how these findings generalize to settings where generator architectures and corpora are different between training and test time. For example, Zellers et al. (2019) (GROVER) assume that the generator is known and apply only slight fine-tuning in order to train the energy function. Similarly, Gehrmann et al. (2019) (GLTR) assume knowledge of the generator; these Authors say \"We further hypothesize that these methods generalize to black-box scenarios, as long as the fake text follows a similar sampling assumption and is generated by a large language model\"; our work answers precisely this question, provides a rigorous experimental protocol and quantitative results. Finally, there has been a release of a training dataset of the GPT-2 language model generations for the purpose of training discriminators capable of detecting machine generated text. While we share the same motivation, our work is a much broader investigation on the topic. We assess generalization of several discriminator architectures to not just one but several kinds of generators and corpora used for training (including GPT-2). In this section, we describe how we train the energy based model and how we mine negatives. Our goal is to learn an energy function E(w 1 , . . . , w n |c; \u03b8) \u2208 R that scores the joint compatibility of an input sequence of tokens (w 1 , . . . , w n ) given some context c and a set of parameters \u03b8. The context depends on the application, it could be the preceding text, some keywords, a bag of words, a title, etc. In this work for simplicity, c is an affix from which we condition the generation. The goal of training is to assign to golden sequences, i.e. sequences taken from a dataset of human generated text, lower energy than other sequences. We parameterize the energy function as a neural network, using the architectures described in \u00a74.3. At training time, the energy function can be trained using a variety of different losses. In this work, we consider two choices: the binary cross-entropy loss and the ranking loss (Collobert et al., 2011) . As the findings are similar, unless otherwise specified we will refer to the binary cross-entropy loss, and report results with the ranking loss in Appendix C. + be a positive sample taken from the training set, and consisting of a sequence of n tokens given some context c. Let (x \u2212 1 , ..., x \u2212 k ) be a set of k negative samples each derived from the same context c as above, all containing at least some machine generated tokens. We train our energy function using the (per-sample) binary cross-entropy loss: wherex \u2212 is the most offending negative (LeCun et al., 2006) , i.e. its index is the solution of arg min , and \u03c3 is the sigmoid function: The most critical component of training an energy based model is the method used to generate negatives, i.e. inputs where the energy should score high (unlikely inputs). In settings with continuous variables, researchers have suggested MCMC (Teh et al., 2003) or Langevin dynamics (Du & Mordatch, 2019) . In this work instead, we use the fact that modern auto-regressive models for text are already quite good, and we use them for negative sampling. We train two auto-regressive language models, a left-to-right one which will be used to produce suffixes assuming the prefix is the context, and a right-to-left one which will be used to generate prefixes assuming the suffix is the context. The negatives are generated by top-k sampling (Fan et al., 2018) setting k equal to 10. Given a trained language model (for instance, a left-to-right autoregressive model) and given a positive example x + = (w i+1 , . . . , w i+n ), for a given context: c = (w 1 , . . . , w i ), a negative can be written as: x \u2212 = (\u0175 i+1 , . . . ,\u0175 i+n ), where w j for j \u2208 [1, i + n] are ground truth words, the first i of them belonging to the common context, and\u0175 j for j \u2208 [i + 1, i + n] are words generated by the language model conditioned on c. In the same way, we can sample a negative with a right-to-left model yielding x \u2212 = (\u0175 1 , . . . ,\u0175 n ), for a given context c = (w n+1 , . . . , w n+i ). In this section we first describe the datasets and preprocessing used, provide architecture details for both generators and scoring functions, and finally introduce the evaluation settings. We train models on three corpora coming from different domains. We report more detailed statistics about the sizes of these corpora in Appendix Table 8 : Books: The Toronto books corpus described in ; , which consists of fiction books in 16 different genres, totaling about half a billion words. CCNews: We collect a de-duplicated subset of the English portion of the CommonCrawl news dataset (Nagel, 2016) , which totals around 16 Billion words. Wikitext: The wikitext103 dataset from Merity et al. (2016) , which consists of 103 million words from English Wikipedia articles. While Wikitext and CCNews are factual, Books is fiction and comprises a wide variety of writing styles. The CCNews corpus has the narrowest domain and it is two orders of magnitude larger than Wikipedia. Overall, these datasets are interesting because they enable us to assess the ability of the energy function to fit and generalize across various axes, from the amount of data available at training time to the richness of style and relatedness among the different data sources. On Wikitext and Books, we extract positive sequences from windows of text that are 160 tokens long with a stride of 40. On the larger CCNews we do the same except that we stride by 160 tokens. This protocol to mine positives is used both at training and test time, although at test time we limit the evaluation to 60,000 randomly chosen positive samples. We use a Byte Pair Encoding (Sennrich et al., 2015) in order to represent all the dataset with a common vocabulary. In particular, our vocabulary contains 50k tokens that was constructed from a byte level UTF-8 encoding of the CC-News corpus following . We mainly use a transformer based network (Vaswani et al., 2017) to generate negatives. We have a medium, large and huge transformer model based on the architecture used in , yielding three language models in total: TransfSmall, TransfBig and TransfHuge; see details also in Appendix B. The small sized models use 6 blocks each containing a multi-head attention module with 8 heads. The large models use 12 blocks each containing a multi-head attention module with 16 heads. The huge models use 48 blocks each containing a multi-head attention module with 25 heads. Transformer models are also implemented in as \"transformer_lm\", \"transformer_lm_big\", and \"transformer_lm_gpt2_big\". The TransfHuge has 10x the number of parameters than TransfBig and it is trained on CCNews only. For each architecture except for TransfHuge we train two models on each each dataset: left to right and right to left. In addition to the transformer generator, we also consider a 12-layer convolutional architecture (Conv) (Dauphin et al., 2017) , and we also use a the third-party trained GPT2 models as described in \u00a75.3. As described in \u00a73.2, we use these language models to generate either a prefix or a suffix. Unless otherwise specified, the context is long either 120 or 140 tokens (with equal probability). Positive and negative examples have 40 or 20 tokens depending on the context size, for an overall length of 160 tokens in all cases. In preliminary experiments, we found that increasing the size of the generations and reducing the size of the context makes the learning task significantly easier. We analyze the effect of the context size in \u00a75.5. We consider three architectures for the energy function: Linear which computes an energy value via a bag of tokens: , where u i is a learnt scalar parameter corresponding to the i-th token in the vocabulary. BiLSTM (Schuster & Kuldip, 1997; Graves & Schmidhuber, 2005) which computes an energy value through L bidirectional layers using LSTM recurrent units (Hochreiter & Schmidhuber, 1997) , as in Linear(AvgPool(h L,1 , . . . , h L,n )), where h L,i is the hidden state at position i and layer L which is the concatenation of the forward and backward hidden states, AvgPool averages hidden states over positions and Linear is a vector of parameters projecting the hidden state down to a scalar value. We consider two versions, referred to as \"BiLSTMsmall\" and \"BiLSTMbig\". Both have 4 layers, but BiLSTMsmall has 512 units in both the embedding layer and the hidden layers, while BiLSTMbig has 758 units in the embedding layer and 2014 units in the hidden states. Transformer (Vaswani et al., 2017; Devlin et al., 2018) which computes an energy value similarly to the BiLSTM's, except that each bi-LSTM layer is replaced by a either a bidirectional Transformer layer (BiTransf), or a Transformer with causal self-attention (UniTransf). For unidirectional models we use the same averaging technique as with BiLSTM models. For bidirectional models the energy is computed via: f (w 1 , ..., w n ) = u h L,1 + b, where h L,1 is the top layer hidden state at the first position (as common practice also in prior work (Devlin et al., 2018) ). BiTransf uses the BERT-Large CORPUS: GENERATOR ARCHITECTURE: in-domain cross-architecture cross-corpus unseen Table 1 : Four evaluation settings considered in this work, described in \u00a74.4. architecture (Devlin et al., 2018) initialized from Liu et al. (2019) . It uses 24 self-attention layers with 1024 units and 16-head attention each. UniTransf has instead 12 layers with 1024 units and 16 attention heads per layer and it is initialized from a language modeling task as in . For all models, we use Adam (Kingma & Ba, 2014) optimizer with warmup. Training is stopped after processing 2.5M samples without any improvement on the validation set. We use data-parallel synchronous multi-GPU training with up to 8 nodes, each with 8 Nvidia V100 GPUs. To improve training speed, we use mixed precision training 1 . Following common practice we clip the norm of the gradient vector (Pascanu et al., 2013) . More details about hyper-parameter setting can be found in Appendix Table 11, while Table 10 in Appendix reports the number of parameters of each energy function. We evaluate the generalization of a residual EBM in four settings: in-domain, cross-architecture, cross-corpus, and unseen. These settings are determined by the corpora C train used to train the training generator G train with architecture A train and the corpora C test used to train the testing generator G test with architecture A test . Note that G train = G test even if A test = A train as we use different training seeds. In all cases, C train is used for fitting the G train and also for the positives for the EBM. In the in-domain setting, C test is C train (but any affixes used as conditioning during testing are from the test-set of the corpus), and A test = A train . In the cross-architecture setting, again C test is C train , but A test is different from A train . In the cross-corpus setting, A test = A train but C test is different than C train , and G test is trained on the training split of C test , while G train trained on the train split of C train . In the unseen setting, both C test is different than C train and A test is different from A train . In all settings, we report performance in terms of average classification accuracy balancing the positive and negative classes. We now present the main results of this work and extensively investigate the generalization ability of the energy functions we have considered. In Table 2 we report the results of the in-domain generalization experiment using our large language model, TransfBig. We observe that when the EBMs have similar representational power compared with the generator (UniTransf, see Table 10 ), they are able to distinguish real from fake completions fairly accurately, reaching an accuracy of more than 90% on the Books dataset (which is easier since it exhibits the larger variety of style and topics), and attaining above 88% on the more challenging CCNews dataset (for which generation is easier and hence discrimination harder). The Wikipedia dataset has lower accuracy because the EBM overfits to this smaller dataset. TransfBig (log-likelihood) 57.1 50.8 50.5 Table 2 : \"In domain\" generalization accuracy of EBMs (each row) on various text corpora. A column corresponds to the corpus used to get positives and to fit the train and test language models, which are TransfBig ( \u00a74.2) with different initial seeds. The last row is the accuracy when using as energy the log-probability of the training language model over the whole sequence. Conv TransfSmall Conv 92.9 81.2 TransfSmall 86.5 87.9 Table 3 : Cross-architecture generalization accuracy using the Wikitext dataset for both training and testing (Ctrain = Ctest). Each row is a model architecture used for generating the training negatives (Atrain), and each column is a model architecture for generating the testing negatives (Atest). The energy function is UniTransf. Weaker energy models are able to do comparably or better at discriminating real from fake than the training generator used as a discriminator by taking the log probability of the sequence as energy. In Table 3 , we assess how well the UniTransf energy function generalizes to different generator architectures at test time, namely Conv and TransfSmall. As a reference on the Wikitext dataset, the test perplexity of Conv and TransfSmall are 35.4 and 33.5, respectively. Therefore, these two generators attain roughly the same perplexity, despite Conv having about 4 times more parameters, see Table 9 . Surprisingly, UniTransf has significantly harder time discriminating TransfSmall negatives with an in-domain rate of 87.9%, compared to 92.9% of Conv. Also, UniTransf trained with TransfSmall negatives is more robust to the (weaker) Conv generations, than vice versa, with a mild 1.4% accuracy drop. However, if we average values across rows, we see that UniTransf tested with mixed negatives is just slightly more accurate when training with the harder negatives produced by TransfSmall. In Table 4 we show the results of generalizing across corpora using UniTransf as an energy function and TransfBig as generator both at training and test time. We observe that models generalize less well across corpora; for instance, when testing on Wikitext an energy function trained with either Books or CCNews, the accuracy is 59.1% and 65.5%, respectively. However, training on the union of two of the corpora gives a large benefit over training on just one or the other when testing on the third. Finally, training on the union of all the three corpora (last two rows) yields an energy function that is very robust to the testing conditions, with an accuracy which is on par if not better than training on in-domain data, even for the largest CC-News dataset (second column). We also tested the bidirectional transformer energy function BiTransf with 355M parameters (almost twice as UniTransf), and found that on CC-News it improves accuracy by more than 5% when it is trained on the union of all corpora, confirming the finding that bigger models trained on more data can achieve substantially better discrimination. As BiTransf was pre-trained using the whole Wikipedia rather than the training part of Wikitext103, we do not report its accuracy on Wiki test set. Test time negatives are generated by models specified in the rows, with their training set in parenthesis and model size in millions of parameters. Note that both the training corpus and GPT2 generator are \"unseen\" by the energy function during training. In Table 5 we test the generalization of the energy functions to GPT-2 generators 2 that were trained on a completely different dataset, namely WebText (Radford et al., 2019) a dataset of 8 million web pages. This is an instance of unseen generalization since C train = C test , and A train = A test . We also consider generations from TransfHuge (last row) whose configuration is similar to the unreleased biggest GPT2 model with 1.4 billion parameters, 7 times bigger than TransfBig, the generator used at training time. Expectedly as the generator gets bigger the discrimination tasks gets harder. When the energy function is confronted with generations from the GPT2 small model, which is smaller than the training generator, the accuracy is close to the in-domain setting, however. For instance the BiTransf accuracy increases by 0.4% on the Book corpus and decreases by 5.4% on the CCNews corpus compared to the fully in-domain results of Table 4 . That suggests that for a known domain, a big enough energy model trained with a single big generator can efficiently discriminate a block-box generator. Of course, accuracy decreases as the black-box generator is made bigger (GPT-2 medium). Finally, we investigate generalization of the energy function to a new domain, such as samples from the dataset of GPT-2 generations . For each model the dataset has a 250k generated texts with either top-k sampling or random sampling. Also, the dataset provides samples from the WebText corpus that was used to train the generator models, and that we use to discriminate against. To adapt our models to this task we split the text segments into sets of intersecting blocks of 160 tokens. During training we treat all blocks in a set as either positives or negatives. During evaluation we take the mean prediction over all blocks in a segment as a prediction for the whole segment. Table 6 : Generalization of the energy function to unconditional generation from various GPT2 models (model size in parantheses, followed by sampling method used). Each row contains the accuracy on the corresponding test set. TF-IDF results are taken from . In Table 6 we report results of the BiTransf energy function compared to the TF-IDF baseline provided with the dataset. We consider two cases. In the in-domain setting, we finetune the energy function on the train set of each of the datasets, following the same protocol used by the provided TF-IDF baseline. In generalization mode, we finetune only on the generations from the small GPT2 model (both top-k and random sampling), and apply the model to the other datasets. Unsurprisingly, in-domain BiTransf beats TF-IDF baseline getting almost 100% across the board. However in generalization mode, we can outperform the TF-IDF baseline only when the generator is less than three times bigger than what was used at training time. Interestingly, our energy function was trained using a fixed length input with a prefix. These generalization results are significantly higher than the in-domain experiment of Table 2 because the unconditional task is significantly easier, a topic further discussed next. First, we investigate the dependency between performance of the energy functions and length of the prefix. We trained BiLSTMSmall and UniTransf models on examples with varying prefix length from the Wikitext corpus, and computed the accuracy for each prefix length independently. Figure 1 shows that as the prefix length increases (and the generation gets shorter), the discrimination task gets harder and the difference between the models more prominent. The unconditional case, i.e. zero prefix length, is the easiest, while prefixes of length 120 and 140 that are the main experimental setup in this work, are the hardest. Finally, in Table 7 we study the impact of the number of negatives and using the most offending negative in the loss of Eq. (1). Using more negatives and harder negatives improves accuracy. In the previous sections we have seen that the energy function is less robust to negatives generated from a model trained on a different corpus. However, even in that case, a negative is still a sample from an auto-regressive neural network. In Appendix F, we show examples where changing a few entities can cause large jumps in the energy (from negative to positive or vice versa), and so fool the EBM. More generally, we see that the energy function is not robust to truly out-of-domain samples. For example, the energy will score blocks of randomly generated text lower than real text. These behaviors are evidence that the energy functions have learned the regularities of generated text, as opposed to learning the regularities of real text. We surmise that it does so because modeling the latter would be much more difficult than the former. By modeling generated text, the energy function assigns low score to anything that is not generated by its training generator. While not surprising, this might be considered a liability of such energy functions. However, as a model of text, the energy functions should be considered as working on the residuals of the language models used to generate negatives. For the examples in Appendix F, the language model records a large decrease in likelihood after the change in entity; and language models of course give much lower likelihood to random text than gold or generated text. Therefore, the energy function needs not to be accurate on examples that are already very unlikely according to these language models. In Figure 2 we show the average effects of applying various perturbations to sequences from Wikitext103 on an in-domain energy and language model at each location (from 1 to 160) in the sequence. We see that for all perturbations, the energy decreases its value, but the language model increases its negative log likelihood. We also see that the energy function is more sensitive to the ends of the text, which is where the negatives were different from real text at training time. The EBM framework could potentially unlock more expressive models of text, as they are not limited to scoring a single word at a time as current locally normalized auto-regressive models do. Unfortunately, training EBMs is challenging because generating negatives using the energy function itself is still an open research problem, and does not scale well in practice. In this work, we propose a simple solution, which is to leverage generations produced by pre-trained language models as negative samples. As a preliminary yet necessary step in this direction we have investigated the generalization ability of such EBMs. We found that EBMs, when trained on large datasets, achieve good generalization. For instance, they behave nicely when tested with negatives produced by generators that have rather different architectures. The generalization is less good when generators are trained on other corpora, but EBMs re-gain robustness once we train them on even bigger composite datasets. In the future, we can improve EBMs for text by simply making their architectures bigger and increasing the diversity and size of their training datasets. Of course, further scaling up of EBMs will pose formidable engineering challenges. On the application side, a natural application of the current formulation of EBMs is real/fake text discrimination. We believe that this is important application in its own right, and that EBMs can be very powerful, as demonstrated by their superior performance compared to discriminating using the original language model log-likelihood. Table 9 : Number of parameters (in millions) for the generator language models. The computational cost is directly related to the number of parameters in other layers than the input embedding layer (second row). a We use models from HuggingFace repository (https://github.com/huggingface/ pytorch-transformers) and report here the sizes of these models as they were used to generate data for table 5. Note that the OpenAI GPT2 repository (https://github.com/openai/gpt-2) defines models sizes as 124M and 355M for small and medium model correspondingly. b As reported in . Table 10 : Number of parameters in millions for the scoring functions. The computational cost is directly related to the number of parameters in other layers than the input embedding layer (second row). The (per-sample) ranking loss is: In this case we also refer to the negative energy as the model score. The ranking loss makes the energy values local, as the loss takes as input the difference of energies for a pairs of positive and negative that share the same context. Instead, the binary cross entropy loss of Eq. 1 encourages a more global and absolute scoring as the loss forces all positive examples to have negative energy, and all negative samples to have positive energy, regardless of the context. Therefore, the binary cross entropy loss is perhaps more interpretable as it is not context dependent, but the task is also harder to learn. Empirically, we found similar findings with both losses. When the energy function is trained using the ranking loss of eq. 2, we evaluate the model using precision at 1 (P@1), which is the ratio between the number of times the ground truth sequence scores the lowest over its set of negatives, averaged over the number of sequences in the test set. All models are implemented using the PyTorch framework (Paszke et al., 2017) and are optimized using Adam (Kingma & Ba, 2014) . To train our biggest models (UniTransf and BiTransf) we used 8 machines each with 8 GPUs in synchronous mode using data parallelism. The resulting large batch size speeds up training when combined with float16 reduced precision and cosine scheduling of the learning rate without any restarts (Loshchilov & Hutter, 2016) , i.e. we decay the learning rate to zero over the course of \"max steps\" updates and then stop training. Using these methods, we reduced training time by five times compared to a single node training. For all other configurations we used a single node with up to 8 GPUs and inverse square root decay. In this section we show that we can change a few words to make a negative example become a \"positive\" one as judged by the energy function, and vice versa, by using gradient information. Below here, we show an example of a ground truth sentence from the Wikitext dataset. Here the block has 160 BPE tokens, where the first 120 tokens (black font) are used as context and the remaining 40 are the ground truth completion. Next, we use a language model to generate 10 negatives: . . . In this example, using the big transformer model, UniTransf, as the energy function, we are able to separate real from fake examples as shown (Figure 4) . We want to perturb these negatives to violate the margin. To do so, we make use of the gradient information from the energy function \u2207 x E \u03b8 (x) and use a first order Taylor expansion to approximate the effect of a token replacement (we abuse our notations and use x to denote embeddings in this analysis). Given the original sample x, we change one word x i to x i to arrive at x . The score of x is approximately: Using this approximation, we can search for those token replacements that increase/decrease the energy the most. We can easily change a negative sample to a positive one by replacing the 5 words highlighted below. In paratheses, we report both score and language model perplexity."
}