{
    "title": "HJlF3h4FvB",
    "content": "Distillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is \"early stopping\". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, Anisotropic In- formation Retrieval (AIR), which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparame- terized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation al- gorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoret- ically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of l2 distance, while the previous result was on convergence in 0-1 loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.\n Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing LeCun et al. (2015) . Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In Hinton et al. (2015) , the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, Furlanello et al. (2018) ; Bagherinezhad et al. (2018) proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by Zagoruyko & Komodakis (2016a) ; Huang & Wang (2017) ; Czarnecki et al. (2017) to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs Hinton et al. (2015) ; Furlanello et al. (2018) ; Yang et al. (2018a) ; Bagherinezhad et al. (2018) ; Yang et al. (2018b) . To explain the effectiveness of distillation, Hinton et al. (2015) suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In Hinton et al. (2015) ; Furlanello et al. (2018) ; Yang et al. (2018a) , the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and Bagherinezhad et al. (2018) observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. In this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training Du et al. (2018b; a) ; Oymak & Soltanolkotabi (2018) ; Allen-Zhu et al. (2018) . In this case, if we train the overparameterized teacher network until convergence, the network's output coincides exactly with the ground truth hard labels. This is because the logits corresponding to the incorrect classes are all zero, and hence no \"Dark knowledge\" can be extracted. Thus, we claim that the core factor that enables an overparameterized network to learn \"Dark knowledge\" is early stopping. What's more, Arpit et al. (2017) ; Rahaman et al. (2018) ; Xu et al. (2019) observed that \"Dark knowledge\" represents the discrepancy of convergence speed of different types of information during the training of the neural network. Neural network tends to fit informative information, such as simple pattern, faster than non-informative and unwanted information such as noise. Similar phenomenon was observed in the inverse scale space theory for image restoration Scherzer & Groetsch (2001) ; Burger et al. (2006) ; Xu & Osher (2007) ; Shi & Osher (2008) . In our paper, we call this effect Anisotropic Information Retrieval (AIR). With the aforementioned interpretation of distillation, We further utilize AIR to refine noisy labels by introducing a new self-distillation algorithm. To extract anisotropic information, we sequentially extract knowledge from the output of the network in the previous epoch to supervise the training in the next epoch. By dynamically adjusting the strength of the supervision, we can theoretically prove that the proposed self-distillation algorithm can recover the correct labels, and empirically the algorithm achieves the state-of-the-art results on Fashion MNIST and CIFAR10. The benefit brought by our theoretical study is twofold. Firstly, the existing approach using large networks ; Zhang & Sabuncu (2018) often requires a validation set to early terminate the network training. However, our analysis shows that our algorithm can sustain long training without overfitting the noise which makes the proposed algorithm more user-friendly. Secondly, our analysis is based on an 2 -loss of the clean labels which enables the algorithm to generate a trained network with a bigger margin and hence generalize better. We summarize our contributions as follows \u2022 This paper aims to understand distillation theoretically (i.e. understand the regularization effect of distillation). Distillation works due to the soft targets generated by the teacher network. Based on the observation that the overparameterized network can exactly fit the one-hot labels which contain no dark knowledge, we theoretically justify that early stopping is essential for an overparameterized teacher network to extract dark knowledge from the hard labels. This provides a new understanding of the regularization effect of distillation. \u2022 This is the first attempt to theoretically understand the role of distillation in noisy label refinery using overparameterized neural networks. Inspired by , we utilize distillation to propose a self-distillation algorithm to train a neural network under label corruption. The algorithm is theoretically guaranteed to recover the unknown correct labels in terms of the 2 -loss rather than the previous 0-1-loss. This enables the algorithm to generate a trained network whose output has a bigger margin and hence generalizes better. Furthermore, our algorithm does not need a validation set to early stop during training, which makes it more hyperparameter friendly. The theoretical understanding of the overparameterized networks encourages us to use large models which empirically produce better results. 2.1 NO EARLY STOPPING, NO DARK KNOWLEDGE As mentioned in the introduction, an overparameterized teacher network is able to extract dark knowledge from the on-hot hard labels because of early stopping. In this section we present an experiment to verify this effect of early stopping, where we use a big model as a teacher to teach a smaller model as the student. In this experiment, we train a WRN-28 Zagoruyko & Komodakis (2016b) Figure 1: A good teacher may not be able to produce a good student. To maximize the effectiveness of distillation, you should early stop your epoch at a proper time. As we can see, the teacher model does not suffer from overfitting during the training, while it does not always educate a good student model either. As suggested by Yang et al. (2018a) , a more tolerant teacher educates better students. The teacher model trained with 80 epochs produces the best result which indicates that early stopping of the bigger model can extract more informative information for distillation. In this paper, we introduce a new concept called Anisotropic Information Retrieval (AIR), which means to exploit the discrepancy of the convergence speed of different types of information during the training of an overparameterized neural network. An important observation of AIR is that informative information tends to converge faster than non-informative and unwanted information such as noise. Selective bias of an iterative algorithm to approximate a function has long been discovered in different areas. For example, Xu (1992) observed that iterative linear equation solver fits the low frequency component first, and they proposed a multigrid algorithm to exploit this property. In image processing, Scherzer & Groetsch (2001); Burger et al. (2006) ; Xu & Osher (2007); Shi & Osher (2008) proposed inverse scale space methods that recover image features earlier in the iteration and noise comes back later. In kernel learning, early stopping of gradient descent is equivalent to the ridge regression Smale & Zhou (2007); Yao et al. (2007); Vito et al. (2005) , which means that bias from the eigenspace corresponding to the larger eigenvalues is reduced quicker by the gradient descent. Under the neural network setting, Rahaman et al. (2018); Xu et al. (2019) observed that neural networks find low frequency patterns more easily. Cicek et al. (2018) discovered that noisy labels can slow down the training. Arpit et al. (2017); Rolnick et al. (2017) studied the memorization effects of the deep networks, and revealed that, during training, neural networks first memorize the data with clean labels and later with the wrong labels. In the next subsection, we will characterize AIR of overparametrized neural networks using the Neural Tangent Kernel Jacot et al. (2018) . In Jacot et al. (2018) , the authors introduced the Neural Tangent Kernel to characterize the trajectory of gradient descent algorithm learning infinitely wide neural networks. Denote f (\u03b8, x) \u2208 R as the output of neural network with \u03b8 \u2208 R n being the trainable parameter and x \u2208 R p the input. Consider training the neural network using the 2 -loss on the dataset {( We use the gradient descent \u03b8 t+1 = \u03b8 t \u2212 \u03b7\u2207l(\u03b8 t ) to train the neural network. Let u t = (f (\u03b8 t , x i )) i\u2208[n] \u2208 R n be the network outputs on all the data {x i } at iteration t and y = (y i ) i\u2208 [n] . It was shown by Du et al. (2018b); Oymak & Soltanolkotabi (2018); that the evolution of the error u t \u2212 y can be formulated in a quasi-linear form, It is known that\u0124 t \u2212 H t = o(1) with respect to d, where Jacot et al. (2018) . Note that H * is a symmetric positive semi-definite matrix. Assume that \u03bb 1 > \u00b7 \u00b7 \u00b7 > \u03bb n \u2265 0 are its n eigenvalues and e 1 , e 2 , \u00b7 \u00b7 \u00b7 , e n are the corresponding eigenvectors. The eigenvectors are orthogonal < e i , e j >= 0 and . Consider the evolution of the projection of the loss function in different eigenspaces We can see that the component lies in the eigenspace with a larger eigenvalue converges faster. Therefore, AIR describes the phenomenon that the gradient descent algorithm searches for information components corresponding to different eigenspaces at different rates. Rahaman et al. (2018) ; Arpit et al. (2017) ; Arora et al. (2019); Zhang & Sabuncu (2018) has shown that that one possible reason of neural network's good generalization property is that neural network fits useful information faster. Thus in our paper, we regard informative information as the eigenspaces associated with the largest few eigenvalues of NTK. Figure 2: Components of label noise in the largest five eigensapces of NTK are decreasing. We denote the projection of supervision signal to the eigenspace with a larger eigenvalue as useful information. In Figure 2 , We calculate the ratio of the172norm of the label vector provided by the self-distillation algorithm lies in the top-5 eigenspace as a representative of informative information. We can see that the informative information decreases when the noise level increases. This motivates us to further explore how \"Dark Knowledge\" helps with label refinery. Supervised learning requires high quality labels. However, due to noisy crowd-sourcing platforms Khetan et al. (2017) and data augmentation pipeline Bagherinezhad et al. (2018) , it is hard to acquire entirely clean labels for training. On the other hand, successive deep models often have huge capacities with millions or even billions of parameters. Such huge capacity enables the network to memorize all the labels, right or wrong Zhang et al. (2016) , which makes learning deep neural networks with noisy labels a challenging task. Arpit et al. (2017) ; Rolnick et al. (2017); Han et al. (2018) pointed out that neural networks often fit the clean labels before the noisy ones during training. Recently, Li et al. (2019) theoretically showed that early stopping can clean up label noise with overparametrized neural networks. This, together with our understanding of distillation with AIR, inspired us to use distillation for noisy label refinery. We shall introduce a new self-distillation algorithm with theoretically guaranteed recovery of the clean labels under suitable assumptions. Training deep models on datasets with label corruption is an important and challenging problem that has attracted much attention lately. In Ma et al. (2018) , the authors proposed to regularize the Local Intrinsic Dimensionality of deep representations to detect label noise. Tanaka et al. (2018) proposed a joint optimization framework to simultaneously optimize the network parameters and output labels. Zhang & Sabuncu (2018) In the literature of distillation, Bagherinezhad et al. (2018) first utilized distillation to refine noisy labels of ImageNet, while Yang et al. (2018b) introduced a distillation method to complete teacherstudent training in one generation. The latter is most related to our proposed self-distillation algorithm. However, the difference is that their model aims to ensemble diverse models in one training trajectory but ours aims to utilize AIR to refine noisy labels during the training. It is known that the label noise lies in the eigenspaces associated to small eigenvalues Arora et al. (2019); . Thus, Li et al. (2019) used early stopping to remove label noise. However, early stopping is hard to tune and sometimes leads to unsatisfactory results. In this section, we proposed a self-distillation algorithm with an excellent empirical performance and a theoretical guarantee to recover the correct labels under certain conditions but without the requirement of early stopping. From the perspective of AIR, we observe that the knowledge learned in early epochs is informative information (i.e. the eigenspaces associated with the largest few eigenvalues of NTK) and can be used to refine the training for later epochs. In other words, the algorithm distills knowledge sequentially to guide the training of the model in later epochs by the knowledge distilled by the model from earlier epochs. The informative information learned during early epochs is, in some sense, \"low frequency information\", which is the core factor to enable the model to generalize well. A nice property of the self-distillation algorithm is that it generates the final model in one generation (i.e. single-round training), which has almost no additional computational cost compared to normal training. The proposed self-distillation algorithm is given by Algorithm 1. Here, the function h(\u00b7) in the algorithm is the label function. It can either be a hard label function such as hardmax, or a soft label function such as softmax with a certain temperature. The choice of h(\u00b7) and interpolation coefficient \u03b1 t depends on the usage of the self-distillation algorithm. If Randomly initialize the network. we want to clean up label noise, we normally choose h(\u00b7) to be hardmax or softmax with a low temperature. The weight \u03b1 t is chosen to be adaptively decreasing corresponding to the increase of our confidence on the learned model at current epoch. The introduction of h(\u00b7) helps to boost AIR and the information gained from the previous epoch. In this section, we provide a theoretical justification of the performance of the self-distillation algorithm with overparameterized neural networks. Here, we only consider binary classification task with label \u2208 {\u22121, +1}. \u2022 We consider a dataset with n data: . (x i , y i ) are the input data and its associated label seen by the model while\u1ef9 i is the unobserved ground truth label. The pair (x i , y i ) with y i =\u1ef9 i is called a clean data, otherwise it is called a corrupted data. \u2022 We assume that {x i } i\u2208 [n] contains points with unit Euclidean norm and has K clusters. The data is randomly i.i.d sampled from a distribution P . Let n l be the number of points in the lth cluster. In our analysis, we ssume that number of data in each cluster is balanced in the sense that n l \u2265 c low n K for constant c low > 0 and all of the data is bounded, i.e. \u2022 For each of the K clusters, we assume that all the input data lie within the Euclidean ball B(c l , ), where c l is the center with unit Euclidean norm and > 0 is the radius. \u2022 Assume that the data in the same cluster has the same ground truth label\u1ef9. For the lth cluster, we denote \u03c1 l the proportion of the data with wrong labels. Let \u03c1 = max{\u03c1 i : i \u2208 [K]} and assume that \u03c1 < 1 2 . \u2022 A dataset satisfying the above assumptions is called an ( , \u03c1) dataset. The above definition of dataset follows that of the previous work Li et al. (2019); Li & Liang (2018) . It is reasonable to assume that \u03c1 < 1 2 in order to ensure the correct labels dominate each cluster. In this work, we consider two-layers neural networks. For input data x \u2208 R d , the output of the neural network f is: where W \u2208 R k\u00d7d is the weight matrix and \u03c6 is the activation function applied to W x entry-wise. We suppose k is even and fix the output layer by assigning half of the entries T , we simply denote the output vector on the data matrix as Following the previous work on training overparameterized neural network Du et al. (2018b), we consider the MSE loss Definition 2. For a data matrix D \u2208 m\u00d7d , we denote \u03bb(D) the small eigenvalue of the neural network covariance matrix The above definition reveals the matching score of the model and data. We denote C = [c 1 , c 2 , . . . , c K ] T the matrix composed by the center of the cluster. We denote \u039b = min(\u03bb(C), \u03bb(X)) for simplification. We are now ready to present our main theorem that establishes the convergence of the proposed self-distillation algorithm to the ground truth labels under certain conditions. Theorem 1. Assume that |\u03c6(0)|, |\u03c6 (\u00b7)| and |\u03c6 (\u00b7)| are bounded with upper bound \u0393 \u2265 1. We fix a learning rate \u03b7 = 1 2\u0393 2 n for the gradient descent. Assume that the sequence \u03b1 t monotonically decreases to 0. Furthermore, we have two slow-decay conditions on \u03b1 t where ) and T 2 = inf{t : For the self-distillation algorithm, if the following two conditions for the radius and the width k are satisfied then for random initialization W 0 \u223c N (0, 1) k\u00d7d , with probability 1 \u2212 \u03b4, we have: where W t is the parameter generated by the full-batch self-distillation algorithm at iteration t. Theorem 2. Combining Theorem 1 and Neyshabur et al. (2018), with failure probability \u03b4 \u2208 (0, 1), using the self-distillation algorithm and neural network described in Theorem 1 and under the same condition, we have Compared to previous result on noisy label Li et al. (2019), our results made the following improvements. Firstly, our algorithm fits the ground truth labels without the help of early stopping as long as a mild condition on \u03b1 t is satisfied. Secondly, while previous work only ensures the algorithm to yield correct class labels on training set, our results state the 2 convergence of the outputs to the ground truth labels. As a result, the solution that our algorithm finds tends to have larger margin which leads to better generalization Mohri et al. (2018) . This is shown in Theorem 2. and will be supported by our empirical studies in the next subsection. 3.4 NOISY LABEL REFINERY Figure 4 : Training on CIFAR10 with 40% noise injection. The normal training suffers from over-fitting, while selfdistillation does not. (Note that we are conducting cosine learning rate scheduling. The learning rate is extremely small at the end of learning.) In this section, we conduct experiments on the self-distillation algorithm. In the experiments, we applied our algorithm on corrupted Fashion MNIST and CI-FAR10. At noise level p, every data in the original training dataset is chosen and assigned a symmetric noisy label with probability p. We test our algorithm for p = 0.2, 0.4, 0.6 and 0.8. The test accuracy is calculated with respect to the ground truth labels. We adopted the shake-shake network of Gastaldi (2017) with 32 channels and cross entropy loss. We trained the network by momentum stochastic gradient descent (SGD) with batch size 128, momentum 0.9 and a weight decay of 1e-4. We schedule the learning rate following cosine learning rate Loshchilov & Hutter (2017) with a maximum learning rate 0.2 and minimum learning rate 0. In order to ensure convergence, we trained the models for 600 epochs. Following Lee et al. (2015) , mean subtraction, horizontal random flip, 32 \u00d7 32 random crops after padding with 4 pixels on each side from the padded image is performed as the data augmentation process. For testing, We only do evaluation on the original 32 \u00d7 32 image. In self-distillation, we adaptively adjust \u03b1 t by setting 1 \u2212 \u03b1 t = \u03bb * accuracy, where accuracy is the accuracy calculated on the current batch. The direct ratio \u03bb is the only tuning parameter. For CIFAR10, We simply set \u03bb as 1 when the noisy level is low, such as p = 0, 0.2 and 0.4. When the noisy level p = 0.6 and 0.8, we take \u03bb = 1.5. For Fashion MNIST, We simply set \u03bb as 0.6, 1, 1, 1.4, 1.6 respectively when p = 0, 0.2, 0.4, 0.6, 0.8. We report the average final test accuracy of 3 runs for Fashion MNIST and CIFAR10 in Figure 5: Self-distillation always gains information. Distillation can benefit from more than just early stopping. Distillation has the ability to enhance the AIR and thus can extract more dark knowledge from the data via early stopping. For example, Hinton et al. (2015) enhanced AIR by adjusting the temperature in the softmax layer. In self-distillation, the label function h(\u00b7) is proposed to amplify the information gained in the earlier epochs so that the knowledge gained in the earlier epochs is preserved. Thus, self-distillation does not require early stopping which makes the algorithm more user-friendly. On the other hand, the selfdistillation algorithm dynamically enhances AIR which enables it to achieve 2 convergence and thus better generalization. Again, we use the ratio of the norm of the label vector which lies in the top-5 eigenspace as a representative of informative information. We call the subtraction of informative information corresponding to the label vector provided by self-distillation algorithm and original label vector as information gain. From 5 we can see that the information gain is mostly larger than zero during the training of self-distillation algorithm. This phenomenon indicates that the supervision signal of self-distillation algorithm gains more information than directly using the noisy label. To sum up, a well-designed distillation algorithm can enjoy a regularization effect beyond early stopping and is able to gain more knowledge from the data. This paper provided an understanding of distillation using overparameterized neural networks. We observed that such neural networks posses the property of Anisotropic Information Retrieval (AIR), which means the neural network tends to fit the infomrative information (i.e. the eigenspaces associated with the largest few eigenvalues of NTK) first and the non-informative information later. Through AIR, we further observed that distillation of the Dark Knowledge is mainly due to early stopping. Based on this new understanding, we proposed a new self-distillation algorithm for noisy label refinery. Both theoretical and empirical justifications of the performance of the new algorithm were provided. Our analysis is based on the assumption that the teacher neural network is overparameterized. When the teacher network is not overparameterized, the network will be biased towards the label even without early stopping. It is still an interesting and unclear problem that whether the bias can provide us with more information. For label refinery, our analysis is mostly based on the symmetric noise setting. We are interested in extending our analysis to the asymmetric setting. A PROOF DETAILS A.1 NEURAL NETWORK PROPERTIES As preliminaries, we first discuss some properties of the neural network. We begin with the jacobian of the one layer neural network x \u2192 v \u03c6(W x), the Jacobian matrix with respect to W takes the form First we borrow Lemma 6.6, 6.7, 6.8 from Oymak & Soltanolkotabi (2018) and Theorem 6.7, 6.8 from . T be a data matrix made up of data with unit Euclidean norm. Assuming that \u03bb(X) > 0, the following properties hold. , at random Gaussian initialization W 0 \u223c N (0, 1) k\u00d7d , with probability at least 1 \u2212 \u03b4, we have T in whichx i corresponds to the center of cluster including x i . What's more, we define the matrix of cluster center C = [c 1 , c 2 , . . . , c K ] T . Assuming that \u03bb(C) > 0, the following properties hold. \u2022 , at random Gaussian initialization W 0 \u223c N (0, 1) k\u00d7d , with probability at least 1 \u2212 \u03b4, we have \u2022 range(J(W,X)) \u2282 S + for any parameter matrix W . Then, we gives out the perturbation analysis of the Jacobian matrix. Lemma 3. Let X be a -clusterable data matrix with its center matrixX. For parameter matrices W,W , we have Proof. We bound J(W, X) \u2212 J(W ,X) by The first term is bounded by Lemma 1. As to the second term, we bound it by Combining the inequality above, we get Lemma 4. Let X be a -clusterable data matrix with its center matrixX. We assume W 1 , W 2 have a upper bound c \u221a k. Then for parameter matrices W 1 , W 2 ,W 1 ,W 2 , we have Proof. By the definition of average Jacobian, we have A.2 PROVE OF THE THEOREM First, we introduce the proof idea of our theorem. Our proof of the theorem divides the learning process into two stages. During the first stage, we aim to prove that the neural network will give out the right classification, i.e. the 0-1-loss converges to 0. The proof in this part is modified from . Furthermore, we proved that training 0-1-loss will keep 0 until the second stage starts and the margin at the first stage will larger than 1\u22122\u03c1 2 . During the second stage, we prove that the neural networks start to further enlarge the margin and finally the 2 loss starts to converge to zero. (2019) has shown that this dynamic can be illustrated by the average Jacobian. Definition 3. We define the average Jacobian for two parameters W 1 and W 2 and data matrix X as The residualr = f (\u03b8) \u2212 y, r = f (\u03b8) \u2212 y obey the following equation r = (I \u2212 \u03b7C(\u03b8))r In our proof, we project the residual to the following subspace Definition 4. Let {x i } n i=1 be a -clusterable dataset and {x i } n i=1 be the associated cluster centers, that is,x i = c l iff x i is from lth cluster. We define the support subspace S + as a subspace of dimension K, dictated by the cluster membership as follows. Let \u039b l \u2282 {1, 2, \u00b7 \u00b7 \u00b7 , n} be the set of coordinates i such that = c l . Then S + is characterized by Definition 5. We define the minimum eigenvalue of a matrix B on a subspace S \u03c3 min (B, S) = min where P S is the projection to the space S. Recall the generation process of the dataset Definition 6. (Clusterable Dataset Descriptions) \u2022 We assume that {x i } i\u2208[n] contains points with unit Euclidean norm and has K clusters. Let n l be the number of points in the lth cluster. Assume that number of data in each cluster is balanced in the sense that n l \u2265 c low n K for constant c low > 0. \u2022 For each of the K clusters, we assume that all the input data lie within the Euclidean ball B(c l , ), where c l is the center with unit Euclidean norm and > 0 is the radius. \u2022 A dataset satisfying the above assumptions is called an -clusterable dataset. First, we reduct the dataset to its cluster center, i.e. = 0 for -clusterable dataset. Lemma 6. We fix the label function be a -clusterable dataset and {x i } n i=1 be the associated cluster centers, that is,x i = c l iff x i is from lth cluster. We denote the data matrix X andX. We denote \u03b1 = . We set the learning rate \u03b7 = min( where \u0398 is maximum of the residual norm during the optimization. We suppose along the optimization path we have \u03b1 \u2264 J(W,X)v \u2264 \u03b2 for all v \u2208 S + . We set T 1 = log 1\u2212 \u03b7\u03b1 2 4 1\u22122\u03c1 8 r0 2 , wherer 0 = P S+ (f (W 0 ,X) \u2212 y 0 ) is the projected residual on the space S + . Then \u2200t \u2265 T 1 , we have \u2022 The neural network can learn the true label sgn (f ) (W t ,X) =\u1ef9. \u2022 The weight vector will be close to its initialization for all iterations Proof. We denote C 1 max t\u22650 2 \u221a n(\u03b1 t \u2212 \u03b1 t+1 ). We denote J t = J(W t ,X). Follow the step of gradient descent, we have We denote G t = J(W t+1 , W t ,X)J(W t ,X) T . Then the dynamic of gradient descent can be written by We consider the dynamic of the residual r t r t+1 = (I \u2212 \u03b7G t )r t + y t \u2212 y t+1 . We project the residual on S +r t+1 = (I \u2212 \u03b7G t )r t +\u0233 t \u2212\u0233 t+1 . Thus, the norm of the residual can be bounded by Utilizing the following lemma Lemma 7. (Claim2. Li et al. (2019)) Let P S+ be the projection matrix to S + , then the following inequality holds Therefore, as long as \u03b7 \u2264 \u03b1 L\u03b2 rt 2 holds, we have When By simple calculation, we have After T 1 = log 1\u2212 As a result, all the data have been classified correctly at iteration T 1 and the following inequality holds For the next step, we use induction to prove that For t = T 1 , it has already been proved. We assume the claim is correct for arbitrary t, we establish the induction for t + 1. By applying projection to S + on equation 7, we have By applying lemma 7, we have Given that h(f t ) =\u1ef9, we have\u0233 Thus, for f t+1 we have By the definition of h(\u00b7), we deduce that h(f t+1 ) =\u1ef9. Back to equation 8, we have Finally, we estimate the total variation of the parameter. Combining equation 11 and 12, we can conclude that inequality holds for all t \u2265 0. After taking sum on both sides for t = 0, 1, 2, . . . , we have By simple calculation, we get Combining equation 6, we have Then we further to adapt the above theorem to the -clusterable dataset by a pertubation analysis. Since we have a simple inequality \u03b1 a \u2265 \u03b1 b , in the following discussion, we simply replace the \u03b1 a in the previous conclusion by \u03b1 b , the conclusion also holds. be a -clusterable dataset and {x i } n i=1 be the associated cluster centers, that is,x i = c l iff x i is from lth cluster. We denote the data matrix X andX. For the same initialization W 0 =W 0 , we run the self-distillation algorithm on X and tX respectively. We denote the parameter matrix W t andW t for t \u2265 0. We denote \u03b1 = c low n\u039b . We set the learning rate \u03b7 = min( where \u0398 is maximum of the residual norm during the optimization. We denote c \u221a k the upper bound of the Frobenius norm of the parameter matrxi and we set M = c + 1. We set T 2 = inf{t : \u03b1 t < 1 24 \u221a n }. Then if the following conditions hold Proof. We introduce the following notations. We can conclude the following inequalities from lemma 3 and lemma 4 Thus the parameters are updated by gradient descent, we have Also, we have To sum up, if we can guarantee that p t \u2264 1\u22122\u03c1 4 holds for t \u2264 T 2 , we have For r t 2 , we have r t 2 \u2264 Tr t 2 + r t \u2212 Tr t 2 Thus we have the following inequality for p t We claim that if the following conditions for and k hold It is obviously when t = 0. We further suppose the inequalities hold for an arbitrary t satisfying t < T 2 , we have because of the condition on k ensures that L\u0398d t \u2264 M \u0393\u0398 \u221a n . When it comes to p t+1 , we have because of the conditions on and k ensure that Ld T2 \u2264 M \u0393 \u221a n and p t \u2264 \u0398. Now we are ready to finalize the proof of our main theorem. Proof. We denote C 2 max s\u2265T2 2 \u221a n(\u03b1 s \u2212 \u03b1 s+1 ). We take \u0398 = (C 3 \u0393 log 8 \u03b4 + 1) \u221a n. C 3 is the constant in Hoeffding's inequality. By lemma 6, we have \u0398 \u2265 max t\u22650 T r t 2 with probability 1 \u2212 \u03b4 4 . Combining lemma 6 and 8, we have f (W t , X) =\u1ef9 for T 1 \u2264 t \u2264 T 2 and Similar to the proof in 6, we consider the gradient descent on original dataset X after T 2 . We proof the following claim by induction For s = T 2 , it has already been proved. We assume the claim is correct for arbitrary s, we establish the induction for s + 1. By equation 7, we have By applying lemma 7, we have f (W s+1 ,X) \u2212\u0233 s 2 \u2264 r s 2 \u2264 5 8 (1 \u2212 2\u03c1) + 1 24 . Given that h(f (W s , X)) =\u1ef9, we have y s (i)\u1ef9(i) \u2265 1 \u2212 2\u03b1 T2 , i = 1, 2, . . . , n. Thus, for f (W s+1 , x i ) we have As a result, we have f (W s+1 , X) =\u1ef9. Furthermore, we can bound r s+1 2 by r s+1 2 \u2264 (I \u2212 \u03b7G s )r s 2 + (1 \u2212 \u03b1 s ) h(f (W s , X)) \u2212 h(f (W s+1 , X)) 2 + (\u03b1 s \u2212 \u03b1 s+1 ) \u0233 \u2212 h(f (W s+1 , X)) 2 \u2264(1 \u2212 \u03b7\u03b1 To sum up, we have the following inequality holds for all s \u2265 T 2 r s+1 2 \u2264 (1 \u2212 \u03b7\u03b1 2 2 ) r s 2 + 2 \u221a n(\u03b1 s \u2212 \u03b1 s+1 ). After taking sum on both sides for s \u2265 T 2 , we have By lemma 1 and lemma 2, as long as k \u2265 To ensure \u03b1 lower bounding the eigenvalue of the gram matrix, we need to verify that That is to say Another condition related to R is the condition on M . We require By Bernstein's inequality, we have with probability 1 \u2212 \u03b4/4. On the condition that k \u2265 R 2 (acutually one can show that we can choose M = d + C 4 log 8 \u03b4 + 2. We take these constants to the lemma 8. Firstly, for we have ---512 bn fc. ---10 dropout Table 2 : Architecture of the student network. After each convolution layer, there is a Rectified Linear Unit(ReLU) layer. Experiment In Section2.3 and Section3.5 For these two experiments, we modify CIFAR10 to a binary classification task. We choose class 2, 7 to be the positive class and the others to be negative. We train resnet56 with MSE loss. We set batch size 128, momentum 0.9, weight decay 5e \u2212 4 and learning rate 0.1. In the experiment in section2.3, we fetch a batch of data (batch size=128) from the testset and randomly corrupted the label by noise level 0, 0.1, 0.2, 0.3, 0.4, 0.5. We plot the ratio of the norm of the label vector which lies in the subspaces corresponding to top-5 eigenvalues of NTK. In the experiment in section2.3, we calculate the ratio of the norm of the label vector which lies in the subspaces corresponding to top-5 eigenvalues of NTK firstly. We calculate the ratio of the norm of the label vector provided by the self-distillation algorithm lies in the top-5 eigenspace. We calculate the difference of the latter ratio and the former ratio and called it information gain. We plot the information gain of the first 1500 iterations."
}