{
    "title": "SkgiX2Aqtm",
    "content": "We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible trans- formations, we emphasize the importance of invertible compression. We introduce new class of likelihood-based auto encoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST, where our model outperform WAE and VAE in sharpness of the generated images. We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible transformations, we emphasize the importance of invertible compression as there are many cases where one cannot or will not decide a priori what part of the information is important and what part is not. Compression of images for person ID in a small company requires less resolution then person ID at an airport. To loose part of the information without harm to the future purpose of viewing the picture requires knowing the purpose upfront. Therefore, the fundamental advantage of invertible information compression is that compression can be undone if a future purpose so requires. Recent advances of classification models have demonstrated that deep learning architectures of proper design do not lead to information loss while still being able to achieve state-of-the-art in classification performance. These i-RevNet models BID5 implement a small but essential modification of the popular RevNet models while achieving invertibility and a performance similar to the standard RevNet BID2 . This is of great interest as it contradicts the intuition that information loss is essential to achieve good performance in classification BID13 . Despite the requirement of the invertibility, flow-based generating models BID0 ; BID11 ; BID6 demonstrate that the combination of bijective mappings allows one to transform the raw distribution of the input data to any desired distribution and perform the manipulation of the data. On the other hand, Auto-Encoders have provided the ideal mechanism to reduce the data to the bare minimum while retaining all essential information for a specific task, the one implemented in the loss function. Variational Auto Encoders (VAE) BID7 and Wasserstein Auto Encoders (WAE) BID14 are performing best. They provide an approach for stable training of autoencoders, which demonstrate good results at reconstruction and generation. However, both of these methods involve the optimization of the objective defined on the pixel level. We would emphasise the importance of avoiding the separate decoder part and training the model without relying on the reconstuction quality directly. Combining the best of Invertible mappings and Auto-Encoders, we introduce Pseudo Invertible Encoder. Our model combines bijectives with restriction and extension of the mappings to the dependent sub-manifolds FIG0 . The main contributions of this paper are the following:\u2022 We introduce new class of likelihood-based Auto-Encoders, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles.\u2022 We demonstrate the properties of Gaussian Pseudo Invertible Encoder in manifold learning.\u2022 We compare our model with WAE and VAE on MNIST, and report that the sharpness of the images, generated by our models is better. 2 RELATED WORK ResNets BID3 enable Networks to grow even more and thus memory consumption becomes a bottleneck. BID2 propose a Reversible Residual Network (RevNet) where each layer's activations can be reconstructed from the activations of the next layer. By replacing the residual blocks with coupling layers, they mimic the behaviour of residual blocks while being able to retrieve the original input of the layer. RevNet replaces the residual blocks of ResNets, but also accommodates non-invertible components to train more efficiently. By adding a downsampling operator to the coupling layer, i-RevNet circumvents these non-invertible modules BID5 . With this they show that losing information is not a necessary condition to learn representations that generalize well on complicated problems. Although i-RevNet circumvents non-invertible modules, data is not compressed and the model is only invertible up to the last layer. All their methods do not allow dimensionality reduction. In current research we build a pseudo invertible model which performs dimensionality reduction. Auto-Encoders were first introduced by BID12 as an unsupervised learning algorithm. They are now widely used as a technique for dimension reduction by compressing input data. By training an encoder and a decoder network, and measuring the distance between original and reconstructed data, data can be represented in a latent space. This latent space can then be used for supervised learning algorithms. Instead of learning a compressed representation of the input data BID7 propose to learn the parameters of a probability distribution that represent the data. tol introduced new class of models -Wasserstein Auto Encoders, which use Optimal Transport to be trained. These methods require the optimization of the objective function which includes the terms defined on pixel level. Our model does not require such optimization. Moreover, it only perform encoding at training time. Here we introduce the approach for obtaining dimensionality reduction invertible mappings. Our method is based on the restriction of the mappings to low-dimensional manifolds, and extension of the inverse mappings with certain constraints (Fig. 2 ). Given data DISPLAYFORM0 In other words, we are looking for a pair of Figure 2 : The schematic representation of the Restriction-Extension approach. The invertible mapping X \u2194 Z is preformed by using the dependent sub-manifold R = g(Z) and a pair extended functionsG,G \u22121 .associated functions G and G \u22121 such that DISPLAYFORM1 We use this residual manifold in order to match the dimensionalities of the hidden and initial spaces. Here we introduce the function g : DISPLAYFORM2 With no loss of generality we can say that R = g(Z). We use the pair of extended functionsG : DISPLAYFORM3 Rather than searching for the invertible dimensionality reduction mapping directly, we seek to find G, the invertible transformation with certain constraints, expressed by R.In search forG, we focus on DISPLAYFORM4 where F is a parametric family of functions invertible on R D . We select the function F \u03b8 with parameters \u03b8 which satisfy the constraint: DISPLAYFORM5 where DISPLAYFORM6 Taking into account constraint 3, we derive F \u03b8 (x) = [z, r], where z \u2208 Z and r \u2208 R. By combining this with Eq. 2 we have the desired pair of functions: DISPLAYFORM7 The obtained function G is Pseudo Invertible Endocer, or shortly PIE. As we are interested in high dimensional data such as images, the explicit choice of parameters \u03b8 is impossible. We choose \u03b8 * as a maximizer of the log likelihood of the observed data given the prior p \u03b8 (x): DISPLAYFORM0 After a change of variables according to Eq. 4 we obtain Taking into account the constraint 3 we derive the joint distribution for DISPLAYFORM1 DISPLAYFORM2 Dirac's delta function can be viewed as a limit of sequence of Gaussians: DISPLAYFORM3 Let us fix 2 = 2 0 DISPLAYFORM4 Finally, for the log likelihood we have: DISPLAYFORM5 We choose prior distribution p(z) as Standard Gaussian. We search for the parameters by using Gradient Descent. The method relies on the function F \u03b8 . This choice is challenging by itself. The currently known classes of real-value bijectives are limited. To overcome this issue, we approximate F \u03b8 with a composition of basic bijectives from certain classes: DISPLAYFORM0 where DISPLAYFORM1 Taking into account that a composition of PIE is also PIE, we create a final dimensionality reduction mapping from a sequence of PIEs: such that DISPLAYFORM2 DISPLAYFORM3 where DISPLAYFORM4 Then the log likelihood is represented as DISPLAYFORM5 where J kl is the Jacobian of the k-th function of the l-th PIE. The approximation error here depends only on , according to the Eq. 10. For the simplicity we will now refer to the whole model as PIE. The building blocks of this model are PIE blocks. If we choose the distribution p(z) in Eq. 17 as Standard Gaussian, g l (\u00b7) = 0, \u2200l and 0 = 1, then the model can be viewed as Normalizing Flow with multi-scale architecture BID1 FIG1 . It was demonstrated in BID1 that the model with such architecture achieves semantic compression. This section introduces the basic bijectives for the Pseudo-Invertible Encoder (PIE). We explain what each building bijective consists of and how it fits in the global architecture as shown in FIG2 . PIE is composed of a series of convolutional blocks followed by linear blocks, as depicted in FIG2 . The convolutional PIE blocks consist of series of coupling layers and 1\u00d71 convolutions. We perform invertible downsampling of the image at the beginning of the convolutional block, by reducing the spatial resolution and increasing the number of channels, keeping the overall number of the variables the same. At the end of the convolutional PIE block, the split of variables is performed. One part of the variables is projected to the residual manifold R while others is feed to the next block. The linear PIE blocks are constructed in the same manner. However, the downsampling is not performed and 1 \u00d7 1 convolutions are replaced invertible linear mappings. DISPLAYFORM0 Figure 5: Structure of a coupling block. P partitions the input into two groups of equal length. U unites these group together. In the inverse P \u22121 and U \u22121 are the reverse of these operations respectively. In order to enhance the flexibility of the model, we utilize affine coupling layers Fig. 5 . We modify the version, introduced in BID1 .Given input data x, the output y is obtained by using the mapping: DISPLAYFORM0 Here multiplication and division are performed element-wise. The scalings s 1 , s 2 and the biases b 1 , b 2 are the functions, parametrized with neural networks. The invertibility is not required for this functions. x 1 , x 2 are the non-intersecting partitions of x. For convolutional blocks we partition the tensors by splitting them into halves along the channels. In case of the linear blocks, we just split the features into halves. The log determinant of the Jacobian of coupling layer is given by: DISPLAYFORM1 where log | \u00b7 | is calculated element-wise. The affine couplings operate on non-intersecting parts of the tensor. In order to capture the various correlations between channels and features, the different mechanism of channel permutations were proposed. BID6 demonstrated that invertible 1\u00d71 convolutions perform better than fixed permutations and reversing of the order of channels BID1 .We parametrize Invertible 1 \u00d7 1 Convolutions and invertible linear mappings with Householder Matrices Householder (1958) . Given the vector v, the Householder Matrix is computed as: DISPLAYFORM0 (b) Inverse Figure 6 : Structure of the split method. P partitions the input into two sub samples. P \u22121 unites these sub samples together. The obtained matrix is orthogonal. Therefore, its inverse is just its transpose, which makes the computation of the inverse easier comparing to BID6 . The log determinant of the Jacobian of such transformation is equal to 0. We use invertible downsampling to progressively reduce the spatial size of the tensor and increase the number of its channels. The downsampling with the checkerboard patterns Jacobsen et al. FORMULA3 ; BID1 transforms the tensor of size C \u00d7H \u00d7W into a tensor of size 4C \u00d7 DISPLAYFORM0 where H, W are the height and the width of the image, and C is the number of the channels. The log determinant of the Jacobian of Downsampling is 0 as it just performs permutation. All the discussed blocks transform the data while preserving its dimensionality. Here we introduce Split block Fig. 6 , which is responsible for the projection, restrictions and extension, described in Section 3. It reduces the dimensionality of the data by splitting the variables into two nonintersecting parts z, r of dimensionalities d and D \u2212 d, respectively. z is kept and is to be processed by the subsequent blocks. r is constrained to match N (r|g(z), 2 0 I). The mappings is defined as DISPLAYFORM0 For this experiment we trained a Gaussian PIE on the MNIST digits dataset. We build PIE with 2 convolutional blocks, each splitting the data in the last layer to 50% of the input size. Next we add three linear blocks to PIE, reducing the dimensions to 64, 10 and the last block does not reduce the dimensions any further. For each affine transformation we use the three biggest possible Householder reflections. For this experiment we set K l equal to 3. Optimization is done with the Adam optimizer BID8 . The model diminishes the number of dimensions from R 784 to R 10 .This experiment shows the ability of PIE to learn a manifold with three different constraints; 2 = 0.01, 2 = 0.1 and 2 = 1.0. The results are shown in FIG3 . As the constraint gets to loose, as shown in the right column, the model is not able to reconstruct anymore FIG3 . Lower values for 2 perform better in terms of reconstruction. Too low values, however, sample fuzzy images FIG3 . Narrowing down the distribution to sample from increases the models probability to produce accurate images. This is shown in FIG3 where samples are taken from N (0, 0.5). For both 2 = 0.01 and 2 = 0.1 reconstructed images are more accurate. This experiment shows that tightening the constraint by decreasing 2 increases the power of the manifold learned by the model. This is shown again in FIG3 where we diminished the number of dimensions even further from R 10 to R 2 utilizing UMAP BID9 . With 2 = 1.0 UMAP created a manifold with a good Gaussian distribution. However, from the manifold created by PIE it was not able to separate distinct digits from each other. Tightening the constraint with a lower 2 moves the manifold created by UMAP further away from a Gaussian distribution, while it is better able to separate classes from each other. It is a well-known problem in VAEs that generated images are smoothened. WAE Tolstikhin et al. (2018) improves over VAEs by utilizing Wasserstein distance function. To test the sharpness of generated images we convolve the grey-scaled images with the Laplace filter. This filter acts as an edge detector. We compute the variance of the activations and average them over 10000 sampled images. If an image is blurry, it means there are less edges and thus more activations will be close to zero, leading to a smaller variance. In this experiment we compare the sharpness of images generated by PIE with WAE, VAE and the sharpeness of the original images. For VAE and WAE we take the architecture as described in BID10 . For PIE we take the architecture as described in section 5.1. Table 1 shows the results for this experiment. PIE outperforms both VAE and WAE in terms of sharpeness of generated images. Images generated by PIE are even more sharp then original images from the MNIST dataset. An explanation for this is the use of a checkerboard pattern in the downsampling layer of the PIE convolutional block. With this technique we capture intrinsic properties of the data and are thus able to reconstruct sharper images. In this paper we have proposed the new class of Auto Encoders, which we call Pseudo Invertible Encoder. We provided a theory which bridges the gap between Auto Encoders and Normalizing Flows. The experiments demonstrate that the proposed model learns the manifold structure and generates sharp images."
}