{
    "title": "H1ERcs09KQ",
    "content": "The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations. To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model. Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components. This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy. In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines. Clustering is one of the most traditional and frequently used machine learning tasks. Clustering models are designed to represent intrinsic data structures, such as latent Dirichlet allocation BID2 . The recent development of representation learning has contributed to generalizing model feature engineering, which also enhances data representation BID1 . Therefore, representation learning has been merged into the clustering models, e.g., variational deep embedding (VaDE) (Jiang et al., 2017) . Besides merging representation learning and clustering, another critical line of research is structuring the clustering result, e.g., hierarchical clustering. This paper introduces a unified model enabling nonparametric Bayesian hierarchical clustering with neural-network-based representation learning. Autoencoder (Rumelhart et al., 1985) is a typical neural network for unsupervised representation learning and achieves a non-linear mapping from a high-dimensional input space to a lowdimensional embedding space by minimizing reconstruction errors. To turn the low-dimensional embeddings into random variables, a variational autoencoder (VAE) (Kingma & Welling, 2014) places a Gaussian prior on the embeddings. The autoencoder, whether it is probabilistic or not, has a limitation in reflecting the intrinsic hierarchical structure of data. For instance, VAE assuming a single Gaussian prior needs to be expanded to suggest an elaborate clustering structure. Due to the limitations of modeling the cluster structure with autoencoders, prior works combine the autoencoder and the clustering algorithm. While some early cases pipeline just two models, e.g., Huang et al. (2014) , a typical merging approach is to model an additional loss, such as a clustering loss, in the autoencoders (Xie et al., 2016; Guo et al., 2017; Yang et al., 2017; Nalisnick et al., 2016; BID4 Jiang et al., 2017) . These suggestions exhibit gains from unifying the encoding and the clustering, yet they remain at the parametric and flat-structured clustering. A more recent development releases the previous constraints by using the nonparametric Bayesian approach. Figure 1: Example of hierarchically clustered embeddings on MNIST with three levels of hierarchy, the reconstructed digits from the hierarchical Gaussian mixture components, and the extracted level proportion features. We marked the mean of a Gaussian mixture component with the colored square, and the digit written inside the square refers to the unique index of the mixture component. For example, the infinite mixture of VAEs (IMVAE) BID0 explores the infinite space for VAE mixtures by looking for an adequate embedding space through sampling, such as the Chinese restaurant process (CRP). Whereas IMVAE remains at the flat-structured clustering, VAEnested CRP (VAE-nCRP) (Goyal et al., 2017) captures a more complex structure, i.e., a hierarchical structure of the data, by adopting the nested Chinese restaurant process (nCRP) prior (Griffiths et al., 2004) into the cluster assignment of the Gaussian mixture model. This paper proposes hierarchically clustered representation learning (HCRL) that is a joint model of 1) nonparametric Bayesian hierarchical clustering, and 2) representation learning with neural networks. HCRL extends a previous work on merging flat clustering and representation learning, i.e., VaDE, by incorporating inter-cluster relation modelings. Unlike a previous work of VAE-nCRP, HCRL learns the full spectrum of hierarchical clusterings, such as the level assignment and the level proportion of generating a component hierarchy. These level assignments and proportions were not modeled in VAE-nCRP, so each data instance cannot be analyzed from the perspective of generalization and specialization in a hierarchy. On the contrary, by adding level assignment and proportion modeling, a data instance can be generated from an internal component of the hierarchy, which is limited to the leaf component in VAE-nCRP. Hierarchical mixture density estimation (Vasconcelos & Lippman, 1999) , where all internal and leaf components are directly modeled to generate data, is a flexible framework for hierarchical mixture modeling, such as hierarchical topic modeling (Mimno et al., 2007; Griffiths et al., 2004) , with regard to the learning of the internal components. Specifically, HCRL jointly optimizes soft-divisive hierarchical clustering in an embedding space from VAE via two mechanisms. First, HCRL includes a hierarchical-versioned Gaussian mixture model (HGMM) with a mixture of hierarchically organized Gaussian distributions. Then, HCRL sets the prior of embeddings by adopting the generative processes of HGMM. Second, to handle a dynamic hierarchy structure dealing with the clusters of unequal sizes, we explore the infinite hierarchy space by exploiting an nCRP prior. These mechanisms are fused as a unified objective function; this is done rather than concatenating the two distinct models of clustering and autoencoding. The quantitative evaluations focus on density estimation quality and hierarchical clustering accuracy, which shows that HCRL has competent likelihoods and the best accuracies compared with the baselines. When we observe our results qualitatively, we visualize 1) the hierarchical clusterings, 2) the embeddings under the hierarchy modeling, and 3) the reconstructed images from each Gaussian mixture component, as shown in FIG3 . These experiments were conducted by crossing the data domains of texts and images, so our benchmark datasets include MNIST, CIFAR-100, RCV1 v2, and 20Newsgroups. 2.1 VARIATIONAL DEEP EMBEDDING mixture components, respectively, are declared outside of the neural network 1 . VaDE trains model parameters to maximize the lower bound of marginal log likelihoods via the mean-field variational inference (Jordan et al., 1999) . VaDE uses the Gaussian mixture model (GMM) as the prior, whereas VAE assumes a single standard Gaussian distribution on embeddings. Following the generative process of GMM, VaDE assumes that 1) the embedding draws a cluster assignment, and 2) the embedding is generated from the selected Gaussian mixture component. VaDE uses an amortized inference as VAE, with a generative and inference networks; L(x) in Equation 1 denotes the evidence lower bound (ELBO), which is the lower bound on the log likelihood. It should be noted that VaDE merges the ELBO of VAE with the likelihood of GMM. DISPLAYFORM0 2.2 VARIATIONAL AUTOENCODER NESTED CHINESE RESTAURANT PROCESS VAE-nCRP uses the nonparametric Bayesian prior for learning tree-based hierarchies, the nCRP (Griffiths et al., 2004) , so the representation could be hierarchically organized. The nCRP prior defines the distributions over children components for each parent component, recursively in a topdown way. The variational inference of the nCRP can be formalized by the nested stick-breaking construction (Wang & Blei, 2009) , which is also kept in the VAE setting. The distribution over paths on the hierarchy is defined as being proportional to the product of weights corresponding to the nodes lying in each path. The weight, \u03c0 i , for the i-th node follows the Griffiths-Engen-McCloskey (GEM) distribution (Pitman et al., 2002) , where \u03c0 i is constructed as DISPLAYFORM1 by a stick-breaking process. Since the nCRP provides the ELBO with the nested stick-breaking process, VAE-nCRP has a unified ELBO of VAE and the nCRP in Equation 2. DISPLAYFORM2 Given the ELBO of VAE-nCRP, we recognized a number of potential improvements. First, term (3.1) is for modeling the hierarchical relationship among clusters, i.e., each child is generated from its parent. VAE-nCRP trade-off is the direct dependency modeling among clusters against the meanfield variational approach. This modeling may reveal that the higher clusters in the hierarchy are more difficult to train. Second, in term (3.2), leaf mixture components generate embeddings, which implies that only leaf clusters have direct summarization ability for sub-populations. Additionally, in term (3.2), variance parameter \u03c3 2 D is modeled as the hyperparameter shared by all clusters. In other words, only with J-dimensional parameters, \u03b1, for the leaf mixture components, the local density modeling without variance parameters has a critical disadvantage. For all of these weaknesses, we were able to compensate with the level proportion modeling and HGMM prior. The level assignment generated from the level proportion allows a data instance to select among all mixture components. We do not need direct dependency modeling between the parents and their children because all internal mixture components also generate embeddings. The generative process of HCRL resembles the generative process of hierarchical clusterings, such as the hierarchical latent Dirichlet allocation (Griffiths et al., 2004) . In detail, the generative process departs from selecting a path \u03b6, from the nCRP prior (phase 1). Then, we sample a level proportion (phase 2) and a level, l (phase 3), from the sampled level proportion to find the mixture component in the path, and this component of \u03b6 l provides the Gaussian distribution for the latent representation (phase 4). Finally, the latent representation is exploited to generate an observed datapoint (phase 5). The below formulas are the generative process with its density functions. In addition, FIG2 illustrates a graphical representation corresponding to the described generative process. The generative process also presents our formalization of corresponding prior distributions, denoted as p(\u00b7), and variational distributions, denoted as q(\u00b7), by generation phases. The variational distributions are used in our inference methods called mean-field variational inference (MFVI) (Jordan et al., 1999) as detailed in Section 3.3. where DISPLAYFORM0 The neural architecture of HCRL consists of two probabilistic encoders on z and \u03b7, and one probabilistic decoder on z as shown in the right part of FIG2 . This unbalanced architecture originates from our modeling assumption of p(x|z), not p(x|z, \u03b7). The reconstruction design of x depending on the two stochastic variables of z and \u03b7 may lead to a large variance of the reconstruction on x. Additionally, we cannot guarantee that both z and \u03b7 contribute to the the reconstruction on x BID3 . Although the decoding structure of \u03b7 is not included explicitly in the neural network architecture of HCRL, we provide the formalization of p(\u03b7|z) in TAB0 according to our generative assumptions. We call this reconstruction process, which is inherently a generative process of the traditional probabilistic graphical model (PGM), PGM reconstruction (see the decoding neural network part of FIG2 ). DISPLAYFORM1 The formal specification can be a factorized probabilistic model as Equation 3, where \u03a6 = {v, \u03b6, \u03b7, l, z} denotes the set of latent variables, and M T denotes the set of all nodes in tree T . DISPLAYFORM2 The proportion and assignment on the mixture components for the n-th data instance are modeled by \u03b6 n as a path assignment; \u03b7 n as a level proportion; and l n as a level assignment. v is a Beta draw used in the stick-breaking construction. The latent variables are inferred through MFVI, and therefore we assume the variational distributions are as Equation 4 : DISPLAYFORM3 where q \u03c6\u03b7 (\u03b7 n |x n ) and q \u03c6z (z n |x n ) should be noted because these two variational distributions follow the amortized inference of VAE. q(\u03b6|x) \u221d S \u03b6 \u03b6\u2208child(\u03b6) S \u03b6 is the variational distribution over path \u03b6, where child(\u03b6) means the set of all full paths that are not in T but include \u03b6 as a sub path. Because we specified both generative and variational distributions, we define the ELBO of HCRL, L = E q log p(\u03a6,x) q(\u03a6|x) , in Equation 5. Appendix F enumerates the full derivation in detail. We report that the Laplace approximation with the logistic normal distribution is applied to model the prior, \u03b1, of the level proportion, \u03b7. We choose a conjugate prior of a multinomial, so p(\u03b7 n |\u03b1) follows the Dirichlet distribution. To configure the inference network on the Dirichlet prior, the Laplace approximation is used (MacKay, 1998; Srivastava & Sutton, 2017; Hennig et al., 2012) . DISPLAYFORM4 This model is formalized according to the stick-breaking process scheme. Unlike the CRP, the stickbreaking process does not represent the direct sampling of the mixture component at the data instance level. Therefore, it is necessary to devise a heuristic algorithm for operations, such as GROW, PRUNE, and MERGE, to refine the hierarchy structure. Appendix C provides details about each operation, together with the overall training algorithm of HCRL. In the below description, an inner path and a full path refer to the path ending with an internal node and a leaf node, respectively.\u2022 GROW expands the hierarchy by creating a new branch under the heavily weighted internal node. Compared with the work of Wang & Blei (2009), we modified GROW to first sample a path, \u03b6 * , proportional to n q(\u03b6 n = \u03b6 * ), and then to grow the path if the sampled path is an inner path.\u2022 PRUNE cuts a randomly sampled minor full path, \u03b6 * , satisfying DISPLAYFORM0 n,\u03b6 q(\u03b6n=\u03b6) < \u03b4, where \u03b4 is the pre-defined threshold. If the removed leaf node of the full path is the last child of the parent node, we also recursively remove the parent node.\u2022 MERGE combines two full paths, \u03b6 (i) and \u03b6 (j) , with similar posterior probabilities, measured DISPLAYFORM1 Datasets: We used various hierarchically organized benchmark datasets as well as MNIST.\u2022 MNIST (LeCun et al., 1998) : 28x28x1 handwritten image data, with 60,000 train images and 10,000 test images. We reshaped the data to 784-d in one dimension.\u2022 CIFAR-100 (Krizhevsky & Hinton, 2009): 32x32x3 colored images with 20 coarse and 100 fine classes. We used 3,072-d flattened data with 50,000 training and 10,000 testing. DISPLAYFORM0 The preprocessed text of the Reuters Corpus Volume. We preprocessed the text by selecting the top 2,000 tf-idf words. We used the hierarchical labels up to the 4-level, and the multi-labeled documents were removed. The final preprocessed corpus consists of 11,370 training and 10,000 testing documents randomly sampled from the original test corpus.\u2022 20Newsgroups (Lang, 1995) : The benchmark text data extracted from 20 newsgroups, consisting 11,314 training and 7,532 testing documents. We also labeled by 4-level following the annotated hierarchical structure. We preprocessed the data through the same process as that of RCV1 v2.Baselines: We completed our evaluation in two aspects: 1) optimizing the density estimation, and 2) clustering the hierarchical categories. First, we evaluated HCRL from the density estimation perspective by comparing it with diverse flat clustered representation learning models, and VAE-nCRP. Second, we tested HCRL from the accuracy perspective by comparing it with multiple divisive hierarchical clusterings. The below is the list of baselines. We also added the two-stage pipeline approaches, where we trained features from VaDE first and then applied the hierarchical clusterings. We reused the open source codes 3 provided by the authors for several baselines, such as IDEC, DCN, VAE-nCRP, and SSC-OMP. We used two measures to evaluate the learned representations in terms of the density estimations: 1) negative log likelihood (NLL), and 2) reconstruction errors (REs). Autoencoder models, such as IDEC and DCN, were tested only for the REs. The NLL is estimated with 100 samples. TAB1 indicates that HCRL is best in the NLL and is competent in the REs which means that the hierarchically clustered embeddings preserve the intrinsic raw data structure. VaDE generally performed better than VAE did, whereas other flat clustered representation learning models tended to be slightly different for each dataset. HCRL showed overall competent performance and better results with a deeper hierarchy of level four than of level three, which implies that capturing the deeper hierarchical structure is likely to be useful for the density estimation. Additionally, we evaluated hierarchical clustering accuracies by following Xie et al. FORMULA33 , except for MNIST that is flat structured. TAB2 points out that HCRL has significantly better microaveraged F-scores compared with every baseline. HCRL is able to reproduce the ground truth hierarchical structure of the data, and this trend is consistent when HCRL compared with the pipelined model, such as VaDE with a clustering model. The result of the comparisons with the clustering models, such as HKM, MOHG, RGMM, and RSSCOMP, is interesting because it experimentally proves that the joint optimization of hierarchical clustering in the embedding space improves hierarchical clustering accuracies. HCRL also presented better hierarchical accuracies than VAE-nCRP. We conjecture the reasons for the modeling aspect of VAE-nCRP: 1) the simplified prior modeling on the variance of the mixture component as just constants, and 2) the non-flexible learning of the internal components. MNIST: In FIG3 , the digits {4, 7, 9} and the digits {3, 8} are grouped together with a clear hierarchy, which was consistent between HCRL and VaDE. Also, some digits {0, 4, 2} in a round form are grouped, together, in HCRL. In addition, among the reconstructed digits from the hierarchical mixture components, the digits generated from the root have blended shapes from 0 to 9, which is natural considering the root position. FIG4 shows the hierarchical clustering results on CIFAR-100. Given that there were no semantic inputs from the data, the color was dominantly reflected in the clustering criteria. However, if one observes the second hierarchy, the scene images of the same sub-hierarchy are semantically consistent, although the background colors are slightly different. RCV1 v2: FIG5 shows the embedding of RCV1 v2. VAE and VaDE show no hierarchy, and close sub-hierarchies are distantly embedded. VAE-nCRP guides the internal mixture components to be agglomerated at the center, and the cause of agglomeration is the generative process of VAEnCRP, where the parameter of the internal components are inferred without direct information from data. HCRL shows a clear separation between the sub-hierarchy without the agglomeration. 20Newsgroups: FIG6 shows the example sub-hierarchies on 20Newsgroups. We enumerated topic words from documents with top-five likelihoods for each cluster, and we filtered the words by tf-idf values. We observe relatively more general contents in the internal clusters than in the leaf clusters of each internal cluster. In this paper, we have introduced a hierarchically clustered representation learning framework for the hierarchical mixture density estimation on deep embeddings. HCRL aims at encoding the relations among clusters as well as among instances to preserve the internal hierarchical structure of data. The main differentiated features of HCRL are 1) the crucial assumption regarding the internal mixture components for having the ability to generate data directly, and 2) the unbalanced autoencoding neural architecture for the level proportion modeling as the encoding structure, and the probabilistic model as the decoding structure. From the modeling and the evaluation, we found that HCRL enables the improvements due to the high flexibility modeling compared with the baselines. We created a synthetic dataset that has a hierarchical structure and is sampled from the 50-dimensional Gaussian distributions, presented in FIG7 . The hierarchy, which has a branch factor of two and a depth of four, has a total of eight leaf clusters. FIG7 shows the raw synthetic dataset in the input space of R 50 , and after running HCRL, we plot the hierarchically clustered embeddings in the latent space in FIG7 . In addition to the embeddings, we also present a confidence ellipse with dashed lines for each learned Gaussian mixture component. Because the root component is involved in generating all of the data, it forms a large ellipse, while the leaf component summarizes the local density, so the small ellipse is learned. We show how the above embeddings learned to be hierarchically clustered in the latent space during training in FIG8 . In the learning mechanism of HCRL, we can observe the hierarchically clustered embeddings from a major deviation to a minor deviation in the data over iterations. We conducted experiments for all autoencoder-based models with a neural architecture whose encoder network was set as fully connected layers with dimensions D-2000-2000-500-J for z, and D-10-10-L for \u03b7, and the decoder network is a mirror of the encoder network for z. The hyperparameters of HCRL given by users, \u03b3 and \u03b1, was set to 1.0, and a vector of all entries 1 sized of L, respectively. We used the Adam optimizer (Kingma & Ba, 2014) with an init learning rate of 0.001 for MNIST dataset and 0.0001 for other datasets. Meanwhile, VAE-nCRP is targeted for grouped data. For experiments with our non-grouped datasets, we treated the group instance as a group instance having a single data instance. For parametric hierarchical clustering models, we gave the branch factor as the input parameter, [1, 20, 5] , [1, 4, 7, 9] , and [1, 6, 4, 3], for CIFAR-100, RCV1 v2, and 20Newsgroups, respectively. For VaDE, we set the number of clusters to the number of leaf clusters; 100 for CIFAR-100, 252 for RCV1 v2, and 72 for 20Newsgroups. Algorithm 1 summarizes the overall algorithm for HCRL. The tree-based hierarhcy T is defined as (N, P), where N and P denote a set of nodes and paths, respectively. We refer to the node at level l lying on path \u03b6, as N(\u03b6 1:l ) \u2208 N. The defined paths, P, consist of full paths (ending at a leaf node), P full , and inner paths (ending at an internal node), P inner , as a union set. Algorithm 1 selects an operation out of three operations: GROW, PRUNE, and MERGE. The GROW algorithm is executed for every specific iteration period, t G . After ellapsing t b iterations since performing the GROW operation, we begin to check whether the PRUNE or MERGE operation should be performed. We prioritize the PRUNE operation first, and if the condition of performing PRUNE is not satisfied, we check for the MERGE operation next. After performing any operation, we initialize n b to 0, which is for locking the changed hierarchy during minimum t b iterations to be fitted to the training data. Input: Training examples x; the tree-based hierarchy depth, L; period of performing GROW, t grow ; minimum number of epochs locking the hierarchy, t lock ; operation-related thresholds \u03b4 prune , \u03b4 merge ; a queue whose element is the set of changed paths, Q; the number of training epochs, E; maximum length of Q, Q max ; grow scale, s grow Output: \u03c6 z , \u03c6 \u03b7 , \u03b8 \u2190 Update the network weight parameters using gradients \u2207 \u03c6z,\u03c6\u03b7,\u03b8 L(x) 6: DISPLAYFORM0 DISPLAYFORM1 Update other variational parameters using gradients \u2207L(x) DISPLAYFORM2 if mod(e, t grow ) = 0 then 9: if T (e) = T (e\u22121) then n lock \u2190 0 else n lock \u2190 n lock + 1 16: end for DISPLAYFORM3 DISPLAYFORM4 Sample a path \u03b6 * with probability DISPLAYFORM5 Q \u2190 \u03c6 // Temporary set of changed paths in this epoch 5:if \u03b6 * \u2208 P inner and \u03b6 * / \u2208 Q s.t. Q \u2208 Q then 6: DISPLAYFORM6 j 0 \u2190 Maximum index for the child node whose parent path is \u03b6 * 1:l 9: DISPLAYFORM7 11: DISPLAYFORM8 if l < L \u2212 1 then 13: DISPLAYFORM9 else 15: enqueue Q to Q 20: DISPLAYFORM10 DISPLAYFORM11 T \u2190 (N, P) The PRUNE operation cuts a minor path, which is sampled according to N n=1 q(\u03b6 n = \u03b6) among the full paths satisfying N n=1 q(\u03b6 n = \u03b6) < \u03b4, where \u03b4 is the pre-defined threshold parameter. If the removed leaf node of the full path is the last child of the parent node, we also recursively remove the parent node as shown in the upper case of FIG3 . DISPLAYFORM12 Randomly sample a full path \u03b6 * \u223c \u2126 if |P full | > 1 and \u03b6 * / \u2208 Q s.t. Q \u2208 Q then 6: DISPLAYFORM0 for l = L \u2212 1, \u00b7 \u00b7 \u00b7 , 1 do 8: DISPLAYFORM1 if l < L \u2212 1 then 10: DISPLAYFORM2 end if n c \u2190 Number of the children nodes whose parent path is \u03b6 DISPLAYFORM0 T \u2190 (N, P) The MERGE operation combines two full paths with similar posterior probabilities, measured by DISPLAYFORM1 Gaussian components by following Ueda et al. (1999) . The specific meaning of combining the two paths is merging the paired two Gaussian distributions lying on the two paths by level, if the two Gaussian distribtions are different. The estimation of merged Gaussian parameters, \u00b5 and \u03c3, is the weighted summation of two subject Gaussian parameters. The propbability of the node at level l lying on a path \u03b6 given x, p(\u03b6 l |x), is proportional to n {q(l n = l) \u00b7 \u03b6\u2208\u039b q(\u03b6 n = \u03b6)}, where \u039b = {\u03b6 |\u03b6 l = \u03b6 l and \u03b6 \u2208 P full }. DISPLAYFORM2 Randomly sample a pair of paths (\u03b6 DISPLAYFORM3 Q \u2190 \u03c6 // Temporary set of changed paths in this epoch DISPLAYFORM4 l \u2190 Maximum level of nodes shared by \u03b6 (1) , \u03b68: DISPLAYFORM5 for l = l, \u00b7 \u00b7 \u00b7 , L \u2212 1 do 10: DISPLAYFORM6 11: DISPLAYFORM7 w (1) +w FORMULA33 13: j 0 \u2190 Maximum index for the child node whose parent path is \u03b6 * 1:l 14: DISPLAYFORM8 if l < L \u2212 1 then 18: DISPLAYFORM9 1:l +1 } else 20: while Q max < |Q| do dequeue Q DISPLAYFORM0 P \u2190 P full \u222a P inner 28:T \u2190 (N, P) return T, Q 30: end function The following TAB8 lists the notations used throughout this paper. A encoder network parametrized by * , whose input is x f \u03b8 (z)A decoder network parametrized by \u03b8, whose input is z \u03b8The variational parameters and weights of the decoder network f \u03b8 \u00b5z,\u03c3 2 zThe variational mean and variance for Gaussian distribution q \u03c6z (z|x) \u00b5x, \u03c3 2 xThe prior parameters, mean and variance, for Gaussian distribution DISPLAYFORM0 The variational parameters and weights of the encoder network g \u03c6 \u00b5,\u03c3 2The variational mean and variance for Gaussian distribution q \u03c6 (z|x) The number of datapoints xn=1,\u00b7\u00b7\u00b7 ,N n-th observed datapoint zn=1,\u00b7\u00b7\u00b7 ,N n-th latent representation corresponding to xn VAE-nCRP & HCRL The height of the tree-based hierarchy VaDE K The number of (finite) clusters cn=1,\u00b7\u00b7\u00b7 ,N The cluster assignment of zn, \u2208 {1, ..., K} \u03baThe prior parameter for multinomial distribution p(c) \u00b5c, \u03c3 2 cThe prior parameters, mean and variance, for Gaussian distribution of c-th cluster, p(z) The number of sequences Nm=1,\u00b7\u00b7\u00b7 ,MThe number datapoints in m-th sequence xm,n=1,\u00b7\u00b7\u00b7 ,N n-th observed datapoint in m-th sequence zm,n=1,\u00b7\u00b7\u00b7 ,N n-th latent representation corresponding to xmn vmp The Beta draws of m-th sequence on node p, for the tree-based stickbreaking construction \u03b3 * The prior parameter for Beta distribution p(vmp) \u03b3 DISPLAYFORM0 The variational parameters, for Beta distribution q(vmp|xm) \u03b6mnThe path assignment of zmn S * mnThe variational parameter for multinomial distribution q(\u03b6mn|xmn) \u03b1 par(p)The J-dimensional parameter vector for the parent node of p \u03b1 * The prior parameter for Gaussian distribution p(\u03b1p) for the root node \u00b5 par(p) , \u03c3 2 par(p)The variational mean and variance for Gaussian distribution q(\u03b1 par(p) |x) \u03b1pThe J-dimensional parameter vector for node p \u03c3 The prior parameter, variance, for Gaussian distribution p(zmn|\u03b6mn, \u03b1p) The variational parameters and weights of the encoder network g \u03c6z \u03c6\u03b7The variational parameters and weights of the encoder network g \u03c6\u03b7 \u00b5z,\u03c3 2 zThe variational mean and variance for Gaussian distribution q \u03c6z (z|x) \u00b5\u03b7,\u03c3 2 \u03b7The variational mean and variance for logistic normal distribution q \u03c6\u03b7 (\u03b7|x) \u03b1The variational parameter for Dirichlet distribution q \u03c6\u03b7 (\u03b7|x) viThe Beta draws for the tree-based stick-breaking construction of node i \u03b3The prior parameter for Beta distribution p(vi) ai, biThe variational parameters, for Beta distribution q(vi|x) \u03b6nThe path assignment of zn SnThe variational parameter for multinomial distribution q(\u03b6n|xn) \u03b7nThe level proportion of zn \u03b1The prior parameter for Dirichlet distribution p(\u03b7n) lnThe level assignment of zn, \u2208 {1, ..., L} \u03c9nThe variational parameter for multinomial distribution q(ln|xn) \u00b5i, \u03c3 E GENERATIVE AND INFERENCE MODEL FOR HCRL HCRL assumes the generative process as described in Section 3.1. Section E.1 describes the joint probability distribution, and Section E.2 presents the corresponding variational distributions. We adopt the much notation-related conventions from Wang & Blei (2009), especially on paths. DISPLAYFORM0 \u2022 p \u03b8 (x n |z n ) : Probabilistic decoding of x n parametrized by \u03b8, whose input is z n \u2022 Tree-based stick-breaking construction -We will denote all Beta draws as v, each of which is an independent draw from Beta(v|1, \u03b3) (except for root v 1 = 1) DISPLAYFORM1 As VAE, we infer the random variables via the mean-field approximation, where the variational distribution, q \u03c6\u03b7,\u03c6z (v, \u03b6, \u03b7, l, z|x) , approximates the intractable posterior. We model the variational distributions as follows: DISPLAYFORM2 The level of the mixture component i\u2022 q(\u03b6 n |x n ) \u221d S n\u03b6 \u03b6\u2208child(\u03b6) S n\u03b6 -\u03b6: a path in the truncated tree T , either an inner path (a path ending at an internal node) or a full path (a path ending at a leaf node)-child(\u03b6): the set of all full paths that are not in T but include \u03b6 as a sub path * As a special case, if \u03b6 is a full path, child(\u03b6) just contains itself -In the case of a full path, DISPLAYFORM3 : maximum index for the child node whose parent path is \u03b6 DISPLAYFORM4 In this section, we present the detailed derivation of the ELBO in Equation 6, which is the objective function for learning HCRL. v, \u03b6, \u03b7, l, z|x) = E q log i\u2208M T p(v i |\u03b3) N n=1 p(\u03b6 n |v)p(\u03b7 n |\u03b1)p(l n |\u03b7 n )p(z n |\u03b6 n , l n )p \u03b8 (x n |z n ) i\u2208M T q(v i |a i , b i ) N n=1 q(\u03b6 n |x n )q \u03c6\u03b7 (\u03b7 n |x n )q(l n |\u03c9 n , x n )q \u03c6z (z n |x n ) DISPLAYFORM0 DISPLAYFORM1 E q [log p(\u03b6 n |v) + log p(\u03b7 n |\u03b1) + log p(l n |\u03b7 n ) + log p(z n |\u03b6 n , l n ) DISPLAYFORM2 E q [log q(\u03b6 n |x n ) + log q \u03c6\u03b7 (\u03b7 n |x n ) + log q(l n |\u03c9 n , x n ) + log q \u03c6z (z n |x n )] The followings are additional notations used for the detailed derivation:\u2022 \u03c8 : The digamma function DISPLAYFORM0 Under review as a conference paper at ICLR 2019 DISPLAYFORM1 DISPLAYFORM2 q(l n = l )q(z n = z) log p(\u03b7 n = \u03b7 |\u03b1)d\u03b7 dz dv = \u03b7 q(\u03b7 n = \u03b7 ) log p(\u03b7 n = \u03b7 |\u03b1)d\u03b7 = \u03b7 Dir(\u03b7 | \u03b1 n ) \u00b7 log Dir(\u03b7 |\u03b1)d\u03b7 DISPLAYFORM3 q(l n = l )q(z n = z ) log p(l n = l |\u03b7 n )d\u03b7dz dv = \u03b7 l q(\u03b7 n = \u03b7 )q(l n = l) log p(l n = l|\u03b7 n = \u03b7 )d\u03b7 = l q(l n = l ) \u03b7 q(\u03b7 n = \u03b7 ) log Mult(l |\u03b7 )d\u03b7 DISPLAYFORM4 q(l n = l )q(z n = z) log p \u03b8 (x n |z n = z )d\u03b7 dz dv = z q(z n = z) log p \u03b8 (x n |z n = z )dz DISPLAYFORM5 log p \u03b8 (x n |z DISPLAYFORM6 q(l n = l )q(z n = z) log q(\u03b6 n = \u03b6)d\u03b7 dz dv = \u03b6 q(\u03b6 n = \u03b6) log q(\u03b6 n = \u03b6) DISPLAYFORM7 q(l n = l )q(z n = z) log q(\u03b7 n = \u03b7 )d\u03b7 dz dv = \u03b7 q(\u03b7 n = \u03b7 ) log q(\u03b7 n = \u03b7 )d\u03b7 = \u03b7 Dir(\u03b7 | \u03b1 n ) \u00b7 log Dir(\u03b7 | \u03b1 n )d\u03b7 DISPLAYFORM8 q(l n = l )q(z n = z) log q(l n = l )d\u03b7 dz dv DISPLAYFORM9 q(l n = l )q(z n = z) log q(z n = z)d\u03b7 dz dv (1 + log \u03c3 2 znj )"
}