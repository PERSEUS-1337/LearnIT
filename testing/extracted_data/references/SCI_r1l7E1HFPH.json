{
    "title": "r1l7E1HFPH",
    "content": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems. The field of Reinforcement learning (RL) span a wide variety of algorithms for solving decisionmaking problems through repeated interaction with the environment. By incorporating deep neural networks into RL algorithms, the field of RL has recently witnessed remarkable empirical success (e.g., Mnih et al. 2015; Lillicrap et al. 2015; Silver et al. 2017 ). Much of this success had been achieved by model-free RL algorithms, such as Q-learning and policy gradient. These algorithms are known to suffer from high variance in their estimations (Greensmith et al., 2004) and to have difficulties handling function approximation (e.g., Thrun & Schwartz 1993; Baird 1995; Van Hasselt et al. 2016; Lu et al. 2018 ). These problems are intensified in decision problems with long horizon, i.e., when the discount factor, \u03b3, is large. Although using smaller values of \u03b3 addresses the \u03b3-dependent issues and leads to more stable algorithms (Petrik & Scherrer, 2009; Jiang et al., 2015) , it comes with a cost, as the algorithm may return a biased solution, i.e., it may not converge to an optimal solution of the original decision problem (the one with large value of \u03b3). Efroni et al. (2018a) recently proposed another approach to mitigate the \u03b3-dependant instabilities in RL in which they study a multi-step greedy versions of the well-known dynamic programming (DP) algorithms policy iteration (PI) and value iteration (VI) (Bertsekas & Tsitsiklis, 1996) . Efroni et al. (2018a) also proposed an alternative formulation of the multi-step greedy policy, called \u03ba-greedy policy, and studied the convergence of the resulted PI and VI algorithms: \u03ba-PI and \u03ba-VI. These two algorithms iteratively solve \u03b3\u03ba-discounted decision problems, whose reward has been shaped by the solution of the decision problem at the previous iteration. Unlike the biased solution obtained by solving the decision problem with a smaller value of \u03b3, by iteratively solving decision problems with a smaller \u03b3\u03ba horizon, the \u03ba-PI and \u03ba-VI algorithms could converge to an optimal policy of the original decision problem. In this work, we derive and empirically validate model-free deep RL (DRL) implementations of \u03ba-PI and \u03ba-VI. In these implementations, we use DQN (Mnih et al., 2015) and TRPO (Schulman et al., 2015) for (approximately) solving \u03b3\u03ba-discounted decision problems (with shaped reward), which is the main component of the \u03ba-PI and \u03ba-VI algorithms. The experiments illustrate the performance of model-free algorithms can be improved by using them as solvers of multi-step greedy PI and VI schemes, as well as emphasize important implementation details while doing so. In this paper, we assume that the agent's interaction with the environment is modeled as a discrete time \u03b3-discounted Markov Decision Process (MDP), defined by M \u03b3 = (S, A, P, R, \u03b3, \u00b5), where S and A are the state and action spaces; P \u2261 P (s |s, a) is the transition kernel; R \u2261 r(s, a) is the reward function with the maximum value of R max ; \u03b3 \u2208 (0, 1) is the discount factor; and \u00b5 is the initial state distribution. Let \u03c0 : S \u2192 P(A) be a stationary Markovian policy, where P(A) is a probability distribution on the set A. The value of \u03c0 in any state s \u2208 S is defined as V \u03c0 (s) \u2261 E[ t\u22650 \u03b3 t r(s t , \u03c0(s t ))|s 0 = s, \u03c0], where the expectation is over all the randomness in policy, dynamics, and rewards. Similarly, the action-value function of \u03c0 is defined as Q \u03c0 (s, a) = E[ t\u22650 \u03b3 t r(s t , \u03c0(s t ))|s 0 = s, a 0 = a, \u03c0]. Since the rewards have the maximum value of R max , both V and Q functions have the maximum value of V max = R max /(1 \u2212 \u03b3). An optimal policy \u03c0 * is the policy with maximum value at every state. We call the value of \u03c0 * the optimal value, and define it as V * (s) = max \u03c0 E[ t\u22650 \u03b3 t r(s t , \u03c0(s t ))|s 0 = s, \u03c0], \u2200s \u2208 S. Furthermore, we denote the stateaction value of \u03c0 * as Q * (s, a) and remind the following relation holds V * (s) = max a Q * (s, a) for all s. The algorithms by which an is be solved (obtain an optimal policy) are mainly based on two popular DP algorithms: Policy Iteration (PI) and Value Iteration (VI). While VI relies on iteratively computing the optimal Bellman operator T applied to the current value function V (Eq. 1), PI relies on (iteratively) calculating a 1-step greedy policy \u03c0 1-step w.r.t. to the value function of the current policy V (Eq. 2): It is known that T is a \u03b3-contraction w.r.t. the max norm and its unique fixed point is V * , and the 1-step greedy policy w.r.t. V * is an optimal policy \u03c0 * . In practice, the state space is often large, and thus, we can only approximately compute Eqs. 1 and 2, which results in approximate PI (API) and VI (AVI) algorithms. These approximation errors then propagate through the iterations of the API and AVI algorithms. However, it has been shown that this (propagated) error can be controlled (Munos, 2003; 2005; Farahmand et al., 2010) and after N steps, the algorithms approximately converge to a solution \u03c0 N whose difference with the optimal value is bounded (see e.g., Scherrer 2014 for API): is the expected value function at the initial state, 1 \u03b4 represents the per-iteration error, and C upper-bounds the mismatch between the sampling distribution and the distribution according to which the final value function is evaluated (\u00b5 in Eq. 3), and depends heavily on the dynamics. Finally, the second term on the RHS of Eq. 3 is the error due to initial values of policy/value, and decays with the number of iterations N . The optimal Bellman operator T (Eq. 1) and 1-step greedy policy \u03c0 1-step (Eq. 2) can be generalized to multi-step. The most straightforward form of this generalization is by replacing T and \u03c0 1-step with h-optimal Bellman operator and h-step greedy policy (i.e., a lookahead of horizon h) that are defined by substituting the 1-step return in Eqs. 1 and 2, r(s 0 , a) + \u03b3V (s 1 ), with h-step return, h\u22121 t=0 r(s t , a t ) + \u03b3 h V (s h ), and computing the maximum over actions a 0 , . . . , a h\u22121 , instead of just a 0 (Bertsekas & Tsitsiklis, 1996) . Efroni et al. (2018a) proposed an alternative form of multi-step optimal Bellman operator and multi-step greedy policy, called \u03ba-optimal Bellman operator, T \u03ba , and \u03ba-greedy policy, \u03c0 \u03ba , for \u03ba \u2208 [0, 1], i.e., 1 Note that the LHS of Eq. 3 is the 1-norm of (V where the shaped reward r t (\u03ba, V ) w.r.t. the value function V is defined as It can be shown that the \u03ba-greedy policy w.r.t. the value function V is the optimal policy w.r.t. a \u03ba-weighted geometric average of all future h-step returns (from h = 0 to \u221e). This can be interpreted as TD(\u03bb) (Sutton & Barto, 2018) for policy improvement (see Efroni et al., 2018a, Sec. 6) . The important difference is that TD(\u03bb) is used for policy evaluation and not for policy improvement. From Eqs. 4 and 5, it is easy to see that solving these equations is equivalent to solving a surrogate \u03b3\u03ba-discounted MDP with the shaped reward r t (\u03ba, V ), which we denote by M \u03b3\u03ba (V ) throughout the paper. The optimal value of M \u03b3\u03ba (V ) (the surrogate MDP) is T \u03ba V and its optimal policy is the \u03ba-greedy policy, \u03c0 \u03ba . Using the notions of \u03ba-optimal Bellman operator, T \u03ba , and \u03ba-greedy policy, \u03c0 \u03ba , Efroni et al. (2018a) derived \u03ba-PI and \u03ba-VI algorithms, whose pseudocode is shown in Algorithms 1 and 2. \u03ba-PI iteratively (i) evaluates the value of the current policy \u03c0 i , and (ii) set the new policy, \u03c0 i+1 , to the \u03ba-greedy policy w.r.t. the value of the current policy V \u03c0i , by solving Eq. 5. On the other hand, \u03ba-VI repeatedly applies the T \u03ba operator to the current value function V i (solves Eq. 4) to obtain the next value function, V i+1 , and returns the \u03ba-greedy policy w.r.t. the final value V N (\u03ba) . Note that for \u03ba = 0, the \u03ba-greedy policy and \u03ba-optimal Bellman operator are equivalent to their 1-step counterparts, defined by Eqs. 1 and 2, which indicates that \u03ba-PI and \u03ba-VI are generalizations of the seminal PI and VI algorithms. It has been shown that both PI and VI converge to the optimal value with an exponential rate that depends on the discount factor \u03b3, i.e., g., Bertsekas & Tsitsiklis, 1996; Scherrer, 2013) . Analogously, Efroni et al. (2018a) showed that \u03ba-PI and \u03ba-VI converge with faster exponential rate of \u03be(\u03ba) = , with the cost that each iteration of these algorithms is computationally more expensive than that of PI and VI. Finally, we state the following two properties of \u03ba-PI and \u03ba-greedy policies that we use in our RL implementations of \u03ba-PI and \u03ba-VI algorithms in Sections 4 and 5: 1) Asymptotic performance depends on \u03ba. The following bound that is similar to the one reported in Eq. 3 was proved by Efroni et al. (2018b, Thm. 5) for the performance of \u03ba-PI: where \u03b4(\u03ba) and C(\u03ba) are quantities similar to \u03b4 and C in Eq. 3. Note that the first term on the RHS of Eq. 7 is independent of N (\u03ba), while the second one decays with N (\u03ba). 2) Soft updates w.r.t. a \u03ba-greedy policy does not necessarily improve the performance. Let \u03c0 \u03ba be the \u03ba-greedy policy w.r.t. V \u03c0 . Then, unlike for 1-step greedy policies, the performance of (1\u2212\u03b1)\u03c0+\u03b1\u03c0 \u03ba (soft update) is not necessarily better than that of \u03c0 (Efroni et al., 2018b, Thm. 1) . This hints that it would be advantages to use \u03ba-greedy policies with 'hard' updates (using \u03c0 \u03ba as the new policy). 4 RL IMPLEMENTATIONS OF \u03ba-PI AND \u03ba-VI As described in Sec. 3, implementing \u03ba-PI and \u03ba-VI requires iteratively solving a \u03b3\u03ba-discounted surrogate MDP with a shaped reward. If a model of the environment is given, the surrogate MDP can be solved using a DP algorithm (see Efroni et al., 2018a, Sec. 7) . When the model is not available, it can be approximately solved by any model-free RL algorithm. In this paper, we focus on the case that the model is not available and propose RL implementations of \u03ba-PI and \u03ba-VI. The main question we investigate in this work is how model-free RL algorithms should be implemented to efficiently solve the surrogate MDP in \u03ba-PI and \u03ba-VI. In this paper, we use DQN (Mnih et al., 2015) and TRPO (Schulman et al., 2015) as subroutines for estimating a \u03ba-greedy policy (Line 4 in \u03ba-PI, Alg. 1 and Line 5 in \u03ba-VI, Alg. 2) or for estimating an optimal value of the surrogate MDP (Line 3 in \u03ba-VI, Alg. 2). For estimating the value of the current policy (Line 3, in \u03ba-PI, Alg. 1), we use standard policy evaluation deep RL (DRL) algorithms. To implement \u03ba-PI and \u03ba-VI, we shall set the value of N (\u03ba) \u2208 N, i.e., the total number of iterations of these algorithms, and determine the number of samples for each iteration. Since N (\u03ba) only appears in the second term of Eq. 7, an appropriate choice of Note that setting N (\u03ba) to a higher value would not dramatically improve the 1: Initialize replay buffer D, Q-networks Q \u03b8 , Q \u03c6 , and target networks Q \u03b8 , Q \u03c6 ; 2: for i = 0, . . . , N (\u03ba) \u2212 1 do 3: # Policy Improvement 4: Act by an -greedy policy w.r.t. Q \u03b8 (st, a), observe rt, st+1, and store (st, at, rt, st+1) in D; 6: Sample a batch {(sj, aj, rj, sj+1)} N j=1 from D; 7: Update \u03b8 by DQN rule with {(sj, aj, rj(\u03ba, V \u03c6 ), sj+1)} N j=1 , where 8: Copy \u03b8 to \u03b8 occasionally (\u03b8 \u2190 \u03b8); 10: end for 11: # Policy Evaluation of \u03c0i(s) \u2208 arg maxa Q \u03b8 (s, a) 12: Update \u03c6 by TD(0) off-policy rule with {(sj, aj, rj, sj+1)} N j=1 , and \u03c0i(s) \u2208 arg maxa Q \u03b8 (s, a); 15: Copy \u03c6 to \u03c6 occasionally (\u03c6 \u2190 \u03c6); 16: end for 17: end for performance, because the asymptotic term in Eq. 7 is independent of N (\u03ba). In practice, since \u03b4(\u03ba) and C(\u03ba) are unknown, we set N (\u03ba) to satisfy the following equality: where C F A is a hyper-parameter that depends on the final-accuracy we are aiming for. For example, if we expect the final accuracy being 90%, we would set C F A = 0.1. Our results suggest that this approach leads to a reasonable choice for N (\u03ba), e.g., N (\u03ba = 0.99) 4 and N (\u03ba = 0.5) 115, for C F A = 0.1 and \u03b3 = 0.99. As we increase \u03ba, we expect less iterations are needed for \u03ba-PI and \u03ba-VI to converge to a good policy. Another important observation is that since the discount factor of the surrogate MDP that \u03ba-PI and \u03ba-VI solve at each iteration is \u03b3\u03ba, the effective horizon (the effective horizon of a \u03b3\u03ba-discounted MDP is 1/(1 \u2212 \u03b3\u03ba)) of the surrogate MDP increases with \u03ba. Lastly, we need to determine the number of samples for each iteration of \u03ba-PI and \u03ba-VI. We allocate equal number of samples per iteration, denoted by T (\u03ba). Since the total number of samples, T , is known beforehand, we set the number of samples per iteration to 5 DQN AND TRPO IMPLEMENTATIONS OF \u03ba-PI AND \u03ba-VI In this section, we study the use of DQN (Mnih et al., 2015) and TRPO (Schulman et al., 2015) in \u03ba-PI and \u03ba-VI algorithms. We first derive our DQN and TRPO implementations of \u03ba-PI and \u03ba-VI in Sections 5.1 and 5.2. We refer to the resulting algorithms as \u03ba-PI-DQN, \u03ba-VI-DQN, \u03ba-PI-TRPO, and \u03ba-VI-TRPO. It is important to note that for \u03ba = 1, \u03ba-PI-DQN and \u03ba-VI-DQN are reduced to DQN, and \u03ba-PI-TRPO and \u03ba-VI-TRPO are reduced to TRPO. We then conduct a set of experiments with these algorithms, in Sections 5.1.1 and 5.2.1, in which we carefully study the effect of \u03ba and N (\u03ba) (or equivalently the hyper-parameter C F A , defined by Eq. 8) on their performance. In these experiments, we specifically focus on answering the following questions: 1. Is the performance of DQN and TRPO improve when using them as \u03ba-greedy solvers in \u03ba-PI and \u03ba-VI? Is there a performance tradeoff w.r.t. to \u03ba? 2. Following \u03ba-PI and \u03ba-VI, our DQN and TRPO implementations of these algorithms devote a significant number of sample T (\u03ba) to each iteration. Is this needed or a 'naive' choice of T (\u03ba) = 1, or equivalently N (\u03ba) = T , works just well, for all values of \u03ba? Algorithm 3 contains the pseudo-code of \u03ba-PI-DQN. Due to space constraints, we report its detailed pseudo-code in Appendix A.1 (Alg. 5). In the policy improvement stage of \u03ba-PI-DQN, we use DQN to solve the \u03b3\u03ba-discounted surrogate MDP with the shaped reward r t (\u03ba, V \u03c6 V \u03c0i\u22121 ), i.e., at the end of this stage M \u03b3\u03ba (V \u03c6 ). The output of the DQN is approximately the optimal Qfunction of M \u03b3\u03ba (V \u03c6 ), and thus, the \u03ba-greedy policy w.r.t. V \u03c6 is equal to arg max a Q \u03b8 (\u00b7, a). At the policy evaluation stage, we use off-policy TD(0) to evaluate the Q-function of the current policy Although what is needed on Line 8 is an estimate of the value function of the current policy, V \u03c6 V \u03c0i\u22121 , we chose to evaluate the Q-function of \u03c0 i : the data in our disposal (the transitions stored in the replay buffer) is an off-policy data and the Q-function of a fixed policy can be easily evaluated with this type of a data using off-policy TD(0), unlike the value function. Remark 1 In order for V \u03c6 to be an accurate estimate of the value function of \u03c0 i\u22121 on Line 8, we should use an additional target network, Q \u03b8 , that remains unchanged during the policy improvement stage. This network should be used in \u03c0 i\u22121 (\u00b7) = arg max a Q \u03b8 (\u00b7, a) on Line 8, and be only updated right after the improvement stage on Line 11. However, to reduce the space complexity of the algorithm, we do not use this additional target network and compute \u03c0 i\u22121 on Line 8 as arg max Q \u03b8 , despite the fact that Q \u03b8 changes during the improvement stage. We report the pseudo-code of \u03ba-VI-DQN in Appendix A.1 (Alg. 6). Note that \u03ba-VI simply repeats V \u2190 T \u03ba V and computes T \u03ba V , which is the optimal value of the surrogate MDP M \u03b3\u03ba (V ). In \u03ba-VI-DQN, we repeatedly solve M \u03b3\u03ba (V ) by DQN, and use its optimal Q-function to shape the reward of the next iteration. Let Q * \u03b3\u03ba,V and V * \u03b3\u03ba,V be the optimal Q and V functions of M \u03b3\u03ba (V ). , where the first equality is by definition (Sec. 2) and the second one holds since T \u03ba V is the optimal value of M \u03b3\u03ba (V ) (Sec. 3). Therefore, in \u03ba-VI-DQN, we shape the reward of each iteration by max a Q \u03c6 (s, a), where Q \u03c6 is the output of the DQN from the previous iteration, i.e., max a Q \u03c6 (s, a) T \u03ba V i\u22121 . In this section, we empirically analyze the performance of the \u03ba-PI-DQN and \u03ba-VI-DQN algorithms on the Atari domains: Breakout, Seaquest, SpaceInvaders, and Enduro (Bellemare et al., 2013) . We start by performing an ablation test on three values of parameter C F A = {0.001, 0.05, 0.2} on the Breakout domain. The value of C F A sets the number of samples per iteration T (\u03ba) (Eq. 8) and the total number of iterations N (\u03ba) (Eq. 9). Aside from C F A , we set the total number of samples to T 10 6 . This value represents the number of samples after which our DQN-based algorithms approximately converge. For each value of C F A , we test \u03ba-PI-DQN and \u03ba-VI-DQN for several \u03ba values. In both algorithms, the best performance was obtained with C F A = 0.05, thus, we set C F A = 0.05 in our experiments with other Atari domains. Alg. Table 1 shows the final training performance of \u03ba-PI-DQN and \u03ba-VI-DQN on the Atari domains with C F A = 0.05. Note that the scores reported in Table 1 are the actual returns of the Atari domains, while the vertical axis in the plots of Figure 1 corresponds to a scaled return. We plot the scaled return, since this way it would be easier to reproduce our results using the OpenAI Baselines codebase (Hill et al., 2018) . The results of Fig. 1 and Table 1 , as well as those in Appendix A.2, exhibit that both \u03ba-PI-DQN and \u03ba-VI-DQN improve the performance of DQN (\u03ba = 1). Moreover, they show that setting N (\u03ba) = T leads to a clear degradation of the final training performance on all of the domains expect Enduro, which attains better performance for N (\u03ba) = T . Although the performance degrades, the results for N (\u03ba) = T are still better than for DQN. Algorithm 4 contains the pseudo-code of \u03ba-PI-TRPO (detailed pseudo-code in Appendix A.1). TRPO iteratively updates the current policy using its return and an estimate of its value function. In our \u03ba-PI-TRPO, at each iteration i: 1) we use the estimate of the current policy V \u03c6 V \u03c0i\u22121 (computed in the previous iteration) to calculate the return R(\u03ba, V \u03c6 ) and an estimate of the value function V \u03b8 of the surrogate MDP M \u03b3\u03ba (V \u03c6 ), 2) we use the return R(\u03ba, V \u03c6 ) and V \u03b8 to compute the new policy \u03c0 i , and 3) we estimate the value of the new policy V \u03c6 V \u03c0i on the original, \u03b3 discounted, MDP. In Appendix B.1 we provide the pseudocode of \u03ba-VI-TRPO derived by the \u03ba-VI meta algorithm. As previously noted, \u03ba-VI iteratively solves the \u03b3\u03ba discounted surrogate MDP and uses its optimal value T \u03ba V i\u22121 to shape the reward of the surrogated MDP in the i'th iteration. With that in mind, consider \u03ba-PI-TRPO. Notice that as \u03c0 \u03b8 converges to the optimal policy of the surrogate \u03b3\u03ba discounted MDP, V\u03b8 converges to the optimal value of the surrogate MDP, i.e., it converges to Thus, \u03ba-PI-TRPO can be turn to \u03ba-VI-TRPO by eliminating the policy evaluation stage, and simply copy \u03c6 \u2190\u03b8, meaning, V \u03c6 \u2190 V\u03b8 = T \u03ba V \u03c6 . In this section, we empirically analyze the performance of the \u03ba-PI-TRPO and \u03ba-VI-TRPO algorithms on the MuJoCo domains: Walker2d-v2, Ant-v2, HalfCheetah-v2, HumanoidStandup-v2, and Swimmer-v2, (Todorov et al., 2012) . As in Section 5.1.1, we start by performing an ablation test on the parameter C F A = {0.001, 0.05, 0.2} on the Walker domain. We set the total number of iterations to 2000, with each iteration consisting 1000 samples. Thus, the total number of samples is T 2 \u00d7 10 6 . This is the number of samples after which our TRPO-based algorithms approximately converge. For each value of C F A , we test \u03ba-PI-TRPO and \u03ba-VI-TRPO for several \u03ba values. In both algorithms, the best performance was obtained with C F A = 0.2, thus, we set C F A = 0.2 in our experiments with other MuJoCo domains. 1: Initialize V -networks V \u03b8 and V \u03c6 , policy network \u03c0 \u03c8 , and target network V \u03c6 ; 2: for i = 0, . . . , N (\u03ba) \u2212 1 do 3: for t = 1, . . . , T (\u03ba) do 4: Simulate the current policy \u03c0 \u03c8 for M steps and calculate the following two returns for all steps j: 5: Rj(\u03ba, V \u03c6 ) = M t=j (\u03b3\u03ba) t\u2212j rt(\u03ba, V \u03c6 ) and \u03c1j = M t=j \u03b3 t\u2212j rt; Update \u03b8 by minimizing the batch loss function: # Policy Improvement 8: Update \u03c8 using TRPO by the batch {(Rj(\u03ba, V \u03c6 ), V \u03b8 (sj))} N j=1 ; 9: # Policy Evaluation 10: Update \u03c6 by minimizing the batch loss function: end for 12: Copy \u03c6 to \u03c6 (\u03c6 \u2190 \u03c6); 13: end for Table 2 shows the final training performance of \u03ba-PI-TRPO and \u03ba-VI-TRPO on the MuJoCo domains with C F A = 0.2. The results of Figure 2 and Table 2 , as well as those in Appendix B.3, exhibit that both \u03ba-PI-TRPO and \u03ba-VI-TRPO yield better performance than TRPO (\u03ba = 1). Furthermore, they show that the algorithms with C F A = 0.2 perform better than with N (\u03ba) = T . However, the improvement is less significant relative to the DQN-based results in Section 5.1.1. There is an intimate relation between \u03ba-PI and the GAE algorithm Schulman et al. (2016) which we elaborate on in this section. In GAE the policy is updated by the gradient: which can be interpreted as a gradient step in a \u03b3\u03bb discounted MDP with rewards \u03b4(V ), which we refer here as M \u03b4(V ) \u03b3\u03bb . As noted in Efroni et al. (2018a) , Section 6, the optimal policy of the MDP M \u03b4(V ) \u03b3\u03bb is the optimal policy of M \u03b3\u03ba (V ) with \u03ba = \u03bb, i.e., the \u03ba-greedy policy w.r.t. V : thus, the Domain Alg. is the \u03ba-greedy policy w.r.t. V . GAE, instead of solving the \u03ba-greedy policy while keeping V fixed, changes the policy and updates V by the return concurrently. Thus, this approach is conceptually similar to \u03ba-PI-TRPO with N (\u03ba) = T . There, the value and policy are concurrently updated as well, without clear separation between the update of the policy and the value. In Figure 2 and Table 2 the performance of GAE is compared to the one of \u03ba-PI-TRPO and \u03ba-VI-TRPO. The performance of the latter two is slightly better than the one of GAE. Remark 2 (Implementation of GAE) We used the OpenAI baseline implementation of GAE with a small modification. In the baseline code, the value network is updated w.r.t. to the target t (\u03b3\u03bb) t r t , whereas in Schulman et al. (2016) the authors used the target t \u03b3 t r t (see Schulman et al. (2016) , Eq.28). We chose the latter form in our implementation to be in accord with Schulman et al. (2016) . To supply with a more complete view on our experiments, we tested the performance of the \"vanilla\" DQN and TRPO when trained with different \u03b3 values than the previously used one (\u03b3 = 0.99). As evident in Figure 3 , only for the Ant domain this approach resulted in improved performance when for TRPO trained with \u03b3 = 0.68. It is interesting to observe that for the Ant domain the performance of \u03ba-PI-TRPO and especially of \u03ba-VI-TRPO (Table 2 ) significantly surpassed the one of TRPO trained with \u03b3 = 0.68. The performance of DQN and TRPO on the Breakout, SpaceInvaders and Walker domains decreased or remained unchanged in the tested \u03b3 values. Thus, on these domains, changing the discount factor does not improve the DQN and TRPO algorithms, as using \u03ba-PI or \u03ba-VI with smaller \u03ba value do. It is interesting to observe that the performance on the Mujoco domains for small \u03b3, e.g., \u03b3 = 0.68, achieved good performance, whereas for the Atari domains the performance degraded with lowering \u03b3. This fits the nature of these domains: in the Mujoco domains the decision problem inherently has much shorter horizon than in the Atari domains. Furthermore, it is important to stress that \u03b3 and \u03ba are two different parameters an algorithm designer may use. For example, one can perform a scan of \u03b3 value, fix \u03b3 to the one with optimal performance, and then test the performance of different \u03ba values. In this work we formulated and empirically tested simple generalizations of DQN and TRPO derived by the theory of multi-step DP and, specifically, of \u03ba-PI and \u03ba-VI algorithms. The empirical investigation reveals several points worth emphasizing. 1. \u03ba-PI is better than \u03ba-VI for the Atari domains.. In most of the experiments on the Atari domains \u03ba-PI-DQN has better performance than \u03ba-VI-DQN. This might be expected as the former uses extra information not used by the latter: \u03ba-PI estimates the value of current policy whereas \u03ba-VI ignores this information. 2. For the Gym domains \u03ba-VI performs slightly better than \u03ba-PI. For the Gym domains \u03ba-VI-TRPO performs slightly better than \u03ba-PI-TRPO. We conjecture that the reason for the discrepancy relatively to the Atari domains lies in the inherent structure of the tasks of the Gym domains: they are inherently short horizon decision problems. For this reason, the problems can be solved with smaller discount factor (as empirically demonstrated in Section 5.3) and information on the policy's value is not needed. 3. Non trivial \u03ba value improves the performance. In the vast majority of our experiments both \u03ba-PI and \u03ba-VI improves over the performance of their vanilla counterparts (i.e., \u03ba = 1), except for the Swimmer and BeamRider domains from Mujoco and Atari suites. Importantly, the performance of the algorithms was shown to be 'smooth' in the parameter \u03ba. This suggests careful hyperparameter tuning of \u03ba is not of great necessity. 4. Using the 'naive' choice of N (\u03ba) = T deteriorates the performance. Choosing the number of iteration by Eq. 8 improves the performance on the tested domains. An interesting future work would be to test model-free algorithms which use other variants of greedy policies (Bertsekas & Tsitsiklis, 1996; Bertsekas, 2018; Efroni et al., 2018a; Sun et al., 2018; Shani et al., 2019) . Furthermore, and although in this work we focused on model-free DRL, it is arguably more natural to use multi-step DP in model-based DRL (e.g., Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019) . Taking this approach, the multi-step greedy policy would be solved with an approximate model. We conjecture that in this case one may set \u03ba -or more generally, the planning horizon -as a function of the approximate model's 'quality': as the approximate model gets closer to the real model larger \u03ba can be used. We leave investigating such relation in theory and practice to future work. Lastly, an important next step in continuation to our work is to study algorithms with an adaptive \u03ba parameter. This, we believe, would greatly improve the resulting methods, and possibly be done by studying the relation between the different approximation errors (i.e., errors in gradient and value estimation, Ilyas et al., 2018) , the performance and the \u03ba value that should be used by the algorithm. A DQN IMPLEMENTATION OF \u03ba-PI AND \u03ba-VI In this section, we report the detailed pseudo-codes of the \u03ba-PI-DQN and \u03ba-VI-DQN algorithms, described in Section 5.1, side-by-side. Algorithm 5 \u03ba-PI-DQN 1: Initialize replay buffer D, and Q-networks Q \u03b8 and Q \u03c6 with random weights \u03b8 and \u03c6; 2: Initialize target networks Q \u03b8 and Q \u03c6 with weights \u03b8 \u2190 \u03b8 and \u03c6 \u2190 \u03c6; # Policy Improvement 5: Select a t as an -greedy action w.r.t. Q \u03b8 (s t , a); Execute a t , observe r t and s t+1 , and store the tuple (s t , a t , r t , s t+1 ) in D; 8: Sample a random mini-batch {(s j , a j , r j , s j+1 )} N j=1 from D; Update \u03b8 by minimizing the following loss function: 10: 11: Copy \u03b8 to \u03b8 occasionally (\u03b8 \u2190 \u03b8); Set \u03c0 i (s) \u2208 arg max a Q \u03b8 (s, a); 16: Update \u03c6 by minimizing the following loss function: 19: Copy \u03c6 to \u03c6 occasionally (\u03c6 \u2190 \u03c6); end for 22: end for Algorithm 6 \u03ba-VI-DQN 1: Initialize replay buffer D, and Q-networks Q \u03b8 and Q \u03c6 with random weights \u03b8 and \u03c6; 2: Initialize target network Q \u03b8 with weights \u03b8 \u2190 \u03b8; # Evaluate T \u03ba V \u03c6 and the \u03ba-greedy policy w.r.t. V \u03c6 5: Select a t as an -greedy action w.r.t. Q \u03b8 (s t , a); Execute a t , observe r t and s t+1 , and store the tuple (s t , a t , r t , s t+1 ) in D; 8: Update \u03b8 by minimizing the following loss function: 10: Copy \u03b8 to \u03b8 occasionally (\u03b8 \u2190 \u03b8); In this section, we report additional results of the application of \u03ba-PI-DQN and \u03ba-VI-DQN on the Atari domains. A summary of these results has been reported in Table 1 in the main paper. B TRPO IMPLEMENTATION OF \u03ba-PI AND \u03ba-VI In this section, we report the detailed pseudo-codes of the \u03ba-PI-TRPO and \u03ba-VI-TRPO algorithms, described in Section 5.2, side-by-side. Algorithm 7 \u03ba-PI-TRPO 1: Initialize V -networks V \u03b8 and V \u03c6 , and policy network \u03c0 \u03c8 with random weights \u03b8, \u03c6, and \u03c8 2: Initialize target network V \u03c6 with weights \u03c6 \u2190 \u03c6 3: for i = 0, . . . , N (\u03ba) \u2212 1 do 4: Simulate the current policy \u03c0 \u03c8 for M time-steps; 6: end for Sample a random mini-batch {(s j , a j , r j , s j+1 )} N j=1 from the simulated M time-steps; 10: Update \u03b8 by minimizing the loss function: 11: # Policy Improvement 12: Sample a random mini-batch {(s j , a j , r j , s j+1 )} N j=1 from the simulated M time-steps; 13: Update \u03c8 using TRPO with advantage function computed by Update \u03c6 by minimizing the loss function: end for # Evaluate T \u03ba V \u03c6 and the \u03ba-greedy policy w.r.t. V \u03c6 5: Simulate the current policy \u03c0 \u03c8 for M time-steps; 7: end for 10: Sample a random mini-batch {(s j , a j , r j , s j+1 )} N j=1 from the simulated M time-steps 11: Update \u03b8 by minimizing the loss function: Sample a random mini-batch {(s j , a j , r j , s j+1 )} N j=1 from the simulated M time-steps 13: Update \u03c8 using TRPO with advantage function computed by In this section, we report additional results of the application of \u03ba-PI-TRPO and \u03ba-VI-TRPO on the MuJoCo domains. A summary of these results has been reported in Table 2 in the main paper. C REBUTTAL RESULTS In this section, we analyze the role \u03ba plays in the proposed methods by reporting results on the simple CartPole environment for \u03ba-PI TRPO. For all experiments, we use a single layered value function network and a linear policy network. Each hyperparameter configuration is run for 10 different random seeds and plots are shown for a 50% confidence interval. Note that since the CartPole is extremely simple, we do not see a clear difference between the \u03ba values that are closer to 1.0 (see Figure 16 ). Below, we observe the performance when the discount factor \u03b3 is lowered (see Figure 17) . Since, there is a ceiling of R = 200 on the maximum achievable return, it makes intuitive sense that observing the \u03ba effect for a lower gamma value such as \u03b3 = 0.36 will allow us to see a clearer trade-off between \u03ba values. To this end, we also plot the results for when the discount factor is set to 0.36. The intuitive idea behind \u03ba-PI, and \u03ba-VI similarly, is that at every time step, we wish to solve a simpler sub-problem, i.e. the \u03b3\u03ba discounted MDP. Although, we are solving an easier/shorter horizon problem, in doing so, the bias induced is taken care of by the modified reward in this new MDP. Therefore, it becomes interesting to look at how \u03ba affects its two contributions, one being the discounting, the other being the weighting of the shaped reward (see eq. 11). Below we look at what happens when each of these terms are made \u03ba independent, one at a time, while varying \u03ba for the other term. To make this clear, we introduce different notations for both such \u03ba instances, one being \u03ba d (responsible for discounting) and the other being \u03ba s (responsible for shaping). We see something interesting here. For the CartPole domain, the shaping term does not seem to have any effect on the performance (Figure 18(b) ), while the discounting term does. This implies that the problem does not suffer from any bias issues. Thus, the correction provided by the shaped term is not needed. However, this is not true for other more complex problems. This is also why we see a similar result when lowering \u03b3 in this case, but not for more complex problems. In this section, we report results for the Mountain Car environment. Contrary to the CartPole results, where lowering the \u03ba values degraded the performance, we observe that performance deteriorates when \u03ba is increased. We also plot a bar graph, with the cumulative score on the y axis and different \u03ba values on the x axis. We use the continuous Mountain Car domain here, which has been shown to create exploration issues. Therefore, without receiving any positive reward, using a \u03ba value of 0 in the case of discounting (solving the 1 step problem has the least negative reward) and of 1 in the case of shaping results in the best performance. In this section, we move to the Pendulum environment, a domain where we see a non-trivial best \u03ba value. This is due to there not being a ceiling on the maximum possible return, which is the case in CartPole. Under review as a conference paper at ICLR 2020 Choosing the best \u03b3 value and running \u03ba-PI on it results in an improved performance for all \u03ba values (see Figure 23 ). To summarize, we believe that in inherently short horizon domains (dense, per time step reward), such as the Mujoco continuous control tasks, the discounting produced by \u03ba-PI and VI is shown to cause major improvement in performance over the TRPO baselines. This is reinforced by the results of lowering the discount factor experiments. On the other hand, in inherently long horizon domains (sparse, end of trajectory reward), such as in Atari, the shaping produced by \u03ba-PI and VI is supposed to cause the major improvement over the DQN baselines. Again, this is supported by the fact that lowering the discount factor experiments actually result in deterioration in performance. Figure 25: Cumulative training performance of \u03ba-PI-TRPO on HalfCheetah (Left, corresponds to Figure 12 ) and Ant (Right, corresponds to Figure 11 ) environments."
}