{
    "title": "SkxwBgpmDE",
    "content": "Training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing. This creates a fundamental quality-versus-quantity trade-off in the learning process.   Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data? We argue that if the learner could somehow know and take the label-quality into account, we could get the best of both worlds.    To this end, we introduce \u201cfidelity-weighted learning\u201d (FWL), a semi-supervised student-teacher approach for training deep neural networks using weakly-labeled data. FWL modulates the parameter updates to a student network, trained on the task we care about on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher, who has access to limited samples with high-quality labels. \"All samples are equal, but some samples are more equal than others.\" -Inspired by George Orwell quote, Animal Farm, 1945 The success of deep neural networks to date depends strongly on the availability of labeled data and usually it is much easier and cheaper to obtain small quantities of high-quality labeled data and large quantities of unlabeled data. For a large class of tasks, it is also easy to define one or more so-called \"weak annotators\" BID10 , additional (albeit noisy) sources of weak supervision based on heuristics or weaker, biased classifiers trained on e.g. non-expert crowd-sourced data or data from different domains that are related. While easy and cheap to generate, it is not immediately clear if and how these additional weakly-labeled data can be used to train a stronger classifier for the task we care about. Assuming we can obtain a large set of weakly-labeled data in addition to a much smaller training set of \"strong\" labels, the simplest approach is to expand the training set simply by including the weakly-supervised samples (all samples are equal). Alternatively, one may pretrain on the weak data and then fine-tune on strong data, which is one of the common practices in semi-supervised learning. We argue that treating weakly-labeled samples uniformly (i.e. each weak sample contributes equally to the final classifier) ignores potentially valuable information of the label quality. Instead, we introduce Fidelity-Weighted Learning (FWL), a Bayesian semi-supervised approach that leverages a small amount of data with true labels to generate a larger training set with confidence-weighted weakly-labeled samples, which can then be used to modulate the fine-tuning process based on the fidelity (or quality) of each weak sample. By directly modeling the inaccuracies introduced by the weak annotator in this way, we can control the extent to which we make use of this additional source of weak supervision: more for confidently-labeled weak samples close to the true observed data, and less for uncertain samples further away from the observed data. In this section, we describe FWL. We assume we are given a large set of unlabeled data samples, a heuristic labeling function called the weak annotator, and a small set of high-quality samples labeled by experts, called the strong dataset, consisting of tuples of training samples x i and their true labels y i , i.e. D s = {(x i ,y i )}. We consider the latter to be observations from the true target function that we are trying to learn. We use the weak annotator to generate labels for the unlabeled samples. Generated labels are noisy due to the limited accuracy of the weak annotator. This gives us the weak dataset consisting of tuples of training samples x i and their weak labels\u1ef9 i , i.e. D w = {(x i ,\u1ef9 i )}. Note that we can generate a large amount of weak training data D w at almost no cost using the weak annotator. In contrast, we have only a limited amount of observations from the true function, i.e. |D s | |D w |. Step 1: Pre-train student on weak data, Step 2: Fit teacher to observations from the true function, and Step 3: Fine-tune student on labels generated by teacher, taking the confidence into account. Red dotted borders and blue solid borders depict components with trainable and non-trainable parameters, respectively. Our proposed setup comprises a neural network called the student and a Bayesian function approximator called the teacher. The training process consists of three phases which we summarize in FIG0 .Step 1 Pre-train the student on D w using weak labels generated by the weak annotator. The main goal of this step is to learn a task dependent representation of the data as well as pretraining the student. The student function is a neural network consisting of two parts. The first part \u03c8(.) learns the data representation and the second part \u03c6(.) performs the prediction task (e.g. classification). Therefore the overall function is\u0177 = \u03c6(\u03c8(x i )). The student is trained on all samples of the weak dataset D w = {(x i ,\u1ef9 i )}. For brevity, in the following, we will refer to both data sample x i and its representation \u03c8(x i ) by x i when it is obvious from the context. Step 2 Train the teacher on the strong data (\u03c8(x j ), y j ) \u2208 D s represented in terms of the student representation \u03c8(.) and then use the teacher to generate a soft dataset D sw consisting of sample,predicted label, confidence for all data samples. We use a Gaussian process as the teacher to capture the label uncertainty in terms of the student representation, estimated w.r.t the strong data. A prior mean and covariance function is chosen for GP. The learned embedding function \u03c8(\u00b7) in Step 1 is then used to map the data samples to dense vectors as input to the GP. We use the learned representation by the student in the previous step to compensate lack of data in D s and the teacher can enjoy the learned knowledge from the large quantity of the weakly annotated data. This way, we also let the teacher see the data through the lens of the student. The GP is trained on the samples from D s to learn the posterior mean m post (used to generate soft labels) and posterior co-variance K post (., .) (which represents label uncertainty) BID17 . We then create the soft dataset D sw = {(x t ,\u0233 t )} using the posterior GP, input samples x t from D w \u222a D s , and predicted labels\u0233 t with their associated uncertainties as computed T (x t ) = g(m post (x t )) and \u03a3(x t ) = h(K post (x t ,x t )). The generated labels are called soft labels. Therefore, we refer to D sw as a soft dataset. g(.) transforms the output of GP to the suitable output space. For example in classification tasks, g(.) would be the softmax function to produce probabilities that sum up to one. For multidimensional-output tasks where a vector of variances is provided by the GP, the vector K post (x t ,x t ) is passed through an aggregating function h(.) to generate a scalar value for the uncertainty of each sample. Note that we train GP only on the strong dataset D s but then use it to generate soft labels\u0233 t = T (x t ) and uncertainty \u03a3(x t ) for samples belonging to D sw = D w \u222aD s .Step 3 Fine-tune the weights of the student network on the soft dataset, while modulating the magnitude of each parameter update by the corresponding teacher-confidence in its label. The student network of Step 1 is fine-tuned using samples from the soft dataset D sw = {(x t ,\u0233 t )} wher\u0113 y t = T (x t ). The corresponding uncertainty \u03a3(x t ) of each sample is mapped to a confidence value, and this is then used to determine the step size for each iteration of the stochastic gradient descent (SGD). So, intuitively, for data points where we have true labels, the uncertainty of the teacher is almost zero, which means we have high confidence and a large step-size for updating the parameters. However, for data points where the teacher is not confident, we down-weight the training steps of the student. This means that at these points, we keep the student function as it was trained on the weak data in Step 1. The weak annotator, i.e. the unsupervised method used for annotating the unlabeled data. Full Supervision Only, i.e. the student trained only on strong labeled data (Ds). Weak Supervision Only, i.e. the or the student trained only on weakly labeled data (Dw).NN W/S + Weak Supervision + Oversampled Strong Supervision, i.e. thestudent trained on samples that are alternately drawn from Dw without replacement, and Ds with replacement. Since |Ds| |Dw|, it oversamples the strong data. Weak Supervision + Fine Tuning, i.e. the student trained on weak dataset Dw and fine-tuned on strong dataset Ds. NN W \u03c9 \u2192NN S The student trained on the weak data, but the step-size of each weak sample is weighted by a fixed value 0 \u2264 \u03c9 \u2264 1, and fine-tuned on strong data. As an approximation for the optimal value for \u03c9, we have used the mean of \u03b72 of our model (below). The student trained on the weakly labeled data and fine-tuned on examples labeled by the teacher without taking the confidence into account. This baseline is similar to BID16 .More specifically, we update the parameters of the student by training on D sw using SGD: DISPLAYFORM0 where l(\u00b7) is the per-example loss, \u03b7 t is the total learning rate, N is the size of the soft dataset D sw , w w w is the parameters of the student network, and R(.) is the regularization term. We define the total learning rate as \u03b7 t = \u03b7 1 (t)\u03b7 2 (x t ), where \u03b7 1 (t) is the usual learning rate of our chosen optimization algorithm that anneals over training iterations, and \u03b7 2 (x t ) is a function of the label uncertainty \u03a3(x t ) that is computed by the teacher for each data point. Multiplying these two terms gives us the total learning rate. In other words, \u03b7 2 represents the fidelity (quality) of the current sample, and is used to multiplicatively modulate \u03b7 1 . Note that the first term does not necessarily depend on each data point, whereas the second term does. We propose \u03b7 2 (x t ) = exp[\u2212\u03b2\u03a3(x t )] to exponentially decrease the learning rate for data point x t if its corresponding soft label\u0233 t is unreliable (far from a true sample). In practice, when using mini-batches, we implement this by multiplying the loss of each example in the batch by its fidelity score and average over these fidelity-weighted losses in the batch when calculating the batch gradient based on that loss. \u03b2 is a positive scalar hyper-parameter that controls the contribution of weak and strong data to the training procedure. A small \u03b2 results in a student which listens more carefully to the teacher and copies its knowledge, while a large \u03b2 makes the student pay less attention to the teacher, staying with its initial weak knowledge. Hence, \u03b2 gives a handle to control the bias-variance trade-off. In Appendix A, we apply FWL to a one-dimensional toy problem to illustrate its various steps. In this section, we apply FWL to document ranking task and evaluate its performance compared to the baselines presented in TAB1 . Document Ranking is the core information retrieval problem and is challenging as the ranking model needs to learn a representation for long documents and capture the complex notion of relevance between queries and documents. Furthermore, the size of publicly available datasets with query-document relevance judgments is unfortunately quite small (\u223c 250 queries). We employ a state-of-the-art pairwise neural ranker architecture as the student BID3 in which the ranking is cast as a regression task. Given each training sample x as a triple of query q, and two documents d + and d \u2212 , the goal is to learn a function F : {< q,d + ,d \u2212 >} \u2192 R, which maps each data sample x to a scalar output value y indicating the probability of d + being ranked higher than d \u2212 with respect to q. The student follows the architecture proposed in BID3 . The first layer of the network, i.e. representation learning layer \u03c8 : {< q,d DISPLAYFORM0 \u2212 >} \u2192 R m maps each input sample to an mdimensional real-valued vector. In general, besides learning embeddings for words, function \u03c8 learns to compose word embedding based on their global importance in order to generate query/document embeddings. The representation layer is followed by a simple fully-connected feed-forward network with a sigmoidal output unit to predict the probability of ranking d + higher than d \u2212 . The general schema of the student is illustrated in FIG1 . More details are provided in Appendix C.1.The teacher is implemented by clustered GP algorithm. See Appendix C.2 for more details. The weak annotator is BM25 BID11 ), a well-known unsupervised method for scoring query-document pairs based on statistics of the matched terms. More details are provided in Appendix C.3. Description of the data with weak labels and data with true labels as well as the setup of the document-ranking experiments is presented in Appendix C.4 in more details. Results and Discussions. We conducted k-fold cross-validation on D s (the strong data) and report two standard evaluation metrics for ranking: mean average precision (MAP) of the top-ranked 1,000 documents and normalized discounted cumulative gain calculated for the top 20 retrieved documents (nDCG@20). TAB2 shows the performance on both datasets. As can be seen, FWL provides a significant boost on the performance over all datasets. In the ranking task, the student is designed in particular to be trained on weak annotations BID3 , hence training the network only on weak supervision, i.e. NN W performs better than NN S . This can be due to the fact that ranking is a complex task requiring many training samples, while relatively few data with true labels are available. Alternating between strong and weak data during training, i.e. NN S + /W seems to bring little (but statistically significant) improvement. However, we can gain better results by the typical fine-tuning strategy, NN W\u2192S . We can gain improvement by fine-tuning the NN W using labels generated by the teacher without considering their confidence score, i.e. FWL \\\u03a3. This means we just augmented the fine-tuning process by generating a fine-tuning set using teacher which is better than D s in terms of quantity and D w in terms of quality. This baseline is equivalent to setting \u03b2 = 0. However, we see a big jump in performance when we use FWL to include the estimated label quality from the teacher, leading to the best overall results. Sensitivity of the FWL to the Quality of the Weak Annotator. Our proposed setup in FWL requires defining a socalled \"weak annotator\" to provide a source of weak supervision for unlabelled data. In this section, we study how the quality of the weak annotator may affect the performance of the FWL on the Robust04 dataset. To do so, besides BM25 BID11 ), we use three other weak annotators: vector space model BID12 with binary term occurrence (BTO) weighting schema and vector space model with TF-IDF weighting schema, which are both weaker than BM25, and BM25+RM3 (Abduljaleel et al., 2004 ) that uses RM3 as the pseudo-relevance feedback method on top of BM25, leading to better labels. FIG2 illustrates the performance of these four weak annotators in terms of their mean average precision (MAP) on the test data, versus the performance of FWL given the corresponding weak annotator. As it is expected, the performance of FWL depends on the quality of the employed weak annotator. The percentage of improvement of FWL over its corresponding weak annotator on the test data is also presented in FIG2 . As can be seen, the better the performance of the weak annotator is, the less the improvement of the FWL would be. Training neural networks using large amounts of weakly annotated data is an attractive approach in scenarios where an adequate amount of data with true labels is not available, a situation which often arises in practice. In this paper, we introduced fidelity-weighted learning (FWL), a new student-teacher framework for semi-supervised learning in the presence of weakly labeled data. We applied FWL to document ranking and empirically verified that FWL speeds up the training process and improves over state-of-the-art semi-supervised alternatives. To better understand FWL, we apply FWL to a one-dimensional toy problem to illustrate the various steps. Let ft(x) = sin(x) be the true function (red dotted line in FIG4 ) from which a small set of observations Ds = {xj,yj} is provided (red points in FIG4 ). These observation might be noisy, in the same way that labels obtained from a human labeler could be noisy. A weak annotator function fw(x) = 2sinc(x) (magenta line in FIG4 ) is provided, as an approximation to ft(.).The task is to obtain a good estimate of ft(.) given the set Ds of strong observations and the weak annotator function fw(.). We can easily obtain a large set of observations Dw = {xi,\u1ef9i} from fw(.) with almost no cost (magenta points in FIG4 ).As the teacher, we use standard Gaussian process regression 2 with this kernel: DISPLAYFORM0 where, kRBF(xi,xj) = exp xi \u2212xj 2 2 2 k White (xi,xj) = constant value, \u2200x1 = x2 and 0 otherwise We fit only one GP on all the data points (i.e. no clustering). Also during fine-tuning, we set \u03b2 = 1. The student is a simple feed-forward network with the depth of 3 layers and width of 128 neurons per layer. We have used tanh as the nonlinearity for the intermediate layers and a linear output layer. As the optimizer, we used Adam (Kingma & Ba, 2015) and the initial learning rate has been set to 0.001. We randomly sample 100 data points from the weak annotator and 10 data points from the true function. We introduce a small amount of noise to the observation of the true function to model the noise in the human labeled data. We consider two experiments:1. A neural network trained on weak data and then fine-tuned on strong data from the true function, which is the most common semi-supervised approach FIG4 ). 2. A teacher-student framework working by the proposed FWL approach. As can be seen in FIG4 , FWL by taking into account label confidence, gives a better approximation of the true hidden function. We repeated the above experiment 10 times. The average RMSE with respect to the true function on a set of test points over those 10 experiments for the student, were as follows: Algorithm 1 Clustered Gaussian processes.1: Let N be the sample size, n the sample size of each cluster, K the number of clusters, and ci the center of cluster i. 2: Run K-means with K clusters over all samples with true labels Ds = {xi,yi}.K-means(xi) \u2192 c1,c2,...,cK where ci represents the center of cluster Ci containing samples D c i s = {xi,1,xi,2,...xi,n}. 3: Assign each of K clusters a Gaussian process and train them in parallel to approximate the label of each sample. DISPLAYFORM1 where GP c i is trained on D c i s containing samples belonging to the cluster ci. Other elements are defined in Section 2 4: Use trained teacher Tc i (.) to evaluate the soft label and uncertainty for samples from Dsw to compute \u03b72(xt) required for step 3 of FWL. We use T (.) as a wrapper for all teachers {Tc i }. We suggest using several GP = {GPc i } to explore the entire data space more effectively. Even though inducing points and stochastic methods make GPs more scalable we still observed poor performance when the entire dataset was modeled by a single GP. Therefore, the reason for using multiple GPs is mainly empirically inspired by BID13 which is explained in the following: We used the Sparse Gaussian Process implemented in GPflow. The algorithm is scalable in the sense that it is not O(N 3 ) as original GP is. It introduces inducing points in the data space and defines a variational lower bound for the marginal likelihood. The variational bound can now be optimized by stochastic methods which make the algorithm applicable in large datasets. However, the tightness of the bound depends on the location of inducing points which are found through the optimization process. The pseudo-code of the clustered GP is presented in Algorithm 1. When the main issue is computational resources (when the number of inducing points for each GP is large), we can first choose the number n which is the maximum size of the dataset on which our resources allow to train a GP, then find the number of clusters K = N/n accordingly. The rest of the algorithm remains unchanged. The employed student is proposed in BID3 . The first layer of the network models function \u03c8 that learns the representation of the input data samples, i.e. (q,d+ ,d \u2212 ), and consists of three components: (1) an embedding function \u03b5 : V \u2192 R m (where V denotes the vocabulary set and m is the number of embedding dimensions), (2) a weighting function \u03c9 : V \u2192 R, and (3) a compositionality function : (R m ,R) n \u2192 R m . More formally, the function \u03c8 is defined as: DISPLAYFORM0 where t q i and t d i denote the i th term in query q respectively document d. The embedding function \u03b5 maps each term to a dense m-dimensional real value vector, which is learned during the training phase. The weighting function \u03c9 assigns a weight to each term in the vocabulary. It has been shown that \u03c9 simulates the effect of inverse document frequency (IDF), which is an important feature in information retrieval BID3 .The compositionality function projects a set of n embedding-weighting pairs to an m-dimensional representation, independent from the value of n: DISPLAYFORM1 which is in fact the normalized weighted element-wise summation of the terms' embedding vectors. Again, it has been shown that having global term weighting function along with embedding function improves the performance of ranking as it simulates the effect of inverse document frequency (IDF). In our experiments, we initialize the embedding function \u03b5 with word2vec embeddings BID6 pre-trained on Google News and the weighting function \u03c9 with IDF.The representation layer is followed by a simple fully connected feed-forward network with l hidden layers followed by a softmax which receives the vector representation of the inputs processed by the representation learning layer and outputs a prediction\u1ef9. Each hidden layer z k in this network computes DISPLAYFORM2 where W k and b k denote the weight matrix and the bias term corresponding to the k th hidden layer and \u03b1(.) is the non-linearity. These layers follow a sigmoid output. We employ the cross entropy loss: DISPLAYFORM3 where B is a batch of data samples. We use Gaussian Process as the teacher and pass the mean of GP through the same function g(.) that is applied on the output of the student network. h(.) is an aggregation function that takes variance over several dimensions and outputs a single measure of variance. As a reasonable choice, the aggregating function h(.) in our sentiment classification task (three classes) is mean of variances over dimensions. In the teacher, linear combinations of different kernels are used in our experiments. We use sparse variational GP regression 3 (Titsias, 2009) with this kernel: DISPLAYFORM0 where, DISPLAYFORM1 +xi.xj k White (xi,xj) = constant value, \u2200x1 = x2 and 0 otherwise We empirically found l = 1 satisfying value for the length scale of Matern3/2 kernels. We also set \u03c30 = 0 to obtain a homogeneous linear kernel. The constant value of K W hite (.,.) determines the level of noise in the labels. This is different from the noise in weak labels. This term explains the fact that even in true labels there might be a trace of noise due to the inaccuracy of human labelers. We set the number of clusters in the clustered GP algorithm for the ranking task to 50. The weak annotator is BM25 BID11 ), a well-known unsupervised retrieval method. This method heuristically scores a given pair of query-document based on the statistics of their matched terms. In the pairwise document ranking setup,\u1ef9i for a given sample xj = (q,d DISPLAYFORM0 is the score obtained from the weak annotator. Collections We use two standard TREC collections for the task of ad-hoc retrieval: The first collection (Robust04) consists of 500k news articles from different news agencies as a homogeneous collection. The second collection (ClueWeb) is ClueWeb09 Category B, a large-scale web collection with over 50 million English documents, which is considered as a heterogeneous collection. Spam documents were filtered out using the Waterloo spam scorer 4 BID1 with the default threshold 70%.Data with true labels We take query sets that contain human-labeled judgments: a set of 250 queries (TREC topics 301-450 and 601-700) for the Robust04 collection and a set of 200 queries (topics for the experiments on the ClueWeb collection. For each query, we take all documents judged as relevant plus the same number of documents judged as non-relevant and form pairwise combinations among them. Data with weak labels We create a query set Q using the unique queries appearing in the AOL query logs BID9 . This query set contains web queries initiated by real users in the AOL search engine that were sampled from a three-month period from March 2006 to May 2006. We applied standard pre-processing BID3 a) on the queries: We filtered out a large volume of navigational queries containing URL substrings (\"http\", \"www.\", \".com\", \".net\", \".org\", \".edu\"). We also removed all non-alphanumeric characters from the queries. For each dataset, we took queries that have at least ten hits in the target corpus using our weak annotator method. Applying all these steps, We collect 6.15 million queries to train on in Robust04 and 6.87 million queries for ClueWeb. To prepare the weakly labeled training set Dw, we take the top 1,000 retrieved documents using BM25 for each query from training query set Q, which in total leads to \u223c |Q|\u00d710 6 training samples. Setup For the evaluation of the whole model, we conducted 3-fold cross-validation. However, for each dataset, we first tuned all the hyper-parameters of the student in the first step on the set with true labels using batched GP bandits with an expected improvement acquisition function BID4 and kept the optimal parameters of the student fixed for all the other experiments. The size and number of hidden layers for the student is selected from {64,128,256,512}. The initial learning rate and the dropout parameter were selected from {10 \u22123 ,10 \u22125 } and {0.0,0.2,0.5}, respectively. We considered embedding sizes of {300,500}. The batch size in our experiments was set to 128. We use ReLU BID7 as a non-linear activation function \u03b1 in student. We use the Adam optimizer (Kingma & Ba, 2015) for training, and dropout BID14 as a regularization technique. At inference time, for each query, we take the top 2,000 retrieved documents using BM25 as candidate documents and re-rank them using the trained models. We use the Indri 5 implementation of BM25 with default parameters (i.e., k1 = 1.2, b = 0.75, and k3 = 1,000)."
}