{
    "title": "HyxlHsActm",
    "content": "Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. Many central problems in machine learning and signal processing are most naturally formulated as optimization problems. These problems are often both nonconvex and highdimensional. High dimensionality makes the evaluation of second-order information prohibitively expensive, and thus randomly initialized first-order methods are usually employed instead. This has prompted great interest in recent years in understanding the behavior of gradient descent on nonconvex objectives (18; 14; 17; 11) . General analysis of first-and second-order methods on such problems can provide guarantees for convergence to critical points but these may be highly suboptimal, since nonconvex optimization is in general an NP-hard probem BID3 . Outside of a convex setting (28) one must assume additional structure in order to make statements about convergence to optimal or high quality solutions. It is a curious fact that for certain classes of problems such as ones that involve sparsification (25; 6) or matrix/tensor recovery (21; 19; 1) first-order methods can be used effectively. Even for some highly nonconvex problems where there is no ground truth available such as the training of neural networks first-order methods converge to high-quality solutions (40).Dictionary learning is a problem of inferring a sparse representation of data that was originally developed in the neuroscience literature (30), and has since seen a number of important applications including image denoising, compressive signal acquisition and signal classification (13; 26) . In this work we study a formulation of the dictionary learning problem that can be solved efficiently using randomly initialized gradient descent despite possessing a number of saddle points exponential in the dimension. A feature that appears to enable efficient optimization is the existence of sufficient negative curvature in the directions normal to the stable manifolds of all critical points that are not global minima BID0 . This property ensures that the regions of the space that feed into small gradient regions under gradient flow do not dominate the parameter space. FIG0 illustrates the value of this property: negative curvature prevents measure from concentrating about the stable manifold. As a consequence randomly initialized gradient methods avoid the \"slow region\" of around the saddle point. Negative curvature helps gradient descent. Red: \"slow region\" of small gradient around a saddle point. Green: stable manifold associated with the saddle point. Black: points that flow to the slow region. Left: global negative curvature normal to the stable manifold. Right: positive curvature normal to the stable manifold -randomly initialized gradient descent is more likely to encounter the slow region. The main results of this work is a convergence rate for randomly initialized gradient descent for complete orthogonal dictionary learning to the neighborhood of a global minimum of the objective. Our results are probabilistic since they rely on initialization in certain regions of the parameter space, yet they allow one to flexibly trade off between the maximal number of iterations in the bound and the probability of the bound holding. While our focus is on dictionary learning, it has been recently shown that for other important nonconvex problems such as phase retrieval BID7 performance guarantees for randomly initialized gradient descent can be obtained as well. In fact, in Appendix C we show that negative curvature normal to the stable manifolds of saddle points (illustrated in FIG0 ) is also a feature of the population objective of generalized phase retrieval, and can be used to obtain an efficient convergence rate. Easy nonconvex problems. There are two basic impediments to solving nonconvex problems globally: (i) spurious local minimizers, and (ii) flat saddle points, which can cause methods to stagnate in the vicinity of critical points that are not minimizers. The latter difficulty has motivated the study of strict saddle functions (36; BID13 , which have the property that at every point in the domain of optimization, there is a large gradient, a direction of strict negative curvature, or the function is strongly convex. By leveraging this curvature information, it is possible to escape saddle points and obtain a local minimizer in polynomial time.2 Perhaps more surprisingly, many known strict saddle functions also have the property that every local minimizer is global; for these problems, this implies that efficient methods find global solutions. Examples of problems with this property include variants of sparse dictionary learning (38), phase retrieval (37), tensor decomposition BID13 , community detection (3) and phase synchronization BID4 .Minimizing strict saddle functions. Strict saddle functions have the property that at every saddle point there is a direction of strict negative curvature. A natural approach to escape such saddle points is to use second order methods (e.g., trust region BID8 or curvilinear search BID14 ) that explicitly leverage curvature information. Alternatively, one can attempt to escape saddle points using first order information only. However, some care is needed: canonical first order methods such as gradient descent will not obtain minimizers if initialized at a saddle point (or at a point that flows to one) -at any critical point, gradient descent simply stops. A natural remedy is to randomly perturb the iterate whenever needed. A line of recent works shows that noisy gradient methods of this form efficiently optimize strict saddle functions (24; 12; 20) . For example, (20) obtains rates on strict saddle functions that match the optimal rates for smooth convex programs up to a polylogarithmic dependence on dimension. Randomly initialized gradient descent? The aforementioned results are broad, and nearly optimal. Nevertheless, important questions about the behavior of first order methods for nonconvex optimization remain unanswered. For example: in every one of the aforemented benign nonconvex optimization problems, randomly initialized gradient descent rapidly obtains a minimizer. This may seem unsurprising: general considerations indicate that the stable manifolds associated with non-minimizing critical points have measure zero (29), this implies that a variety of small-stepping first order methods converge to minimizers in the large-time limit (23). However, it is not difficult to construct strict saddle problems that are not amenable to efficient optimization by randomly initialized gradient descent -see BID11 for an example. This contrast between the excellent empirical performance of randomly initialized first order methods and worst case examples suggests that there are important geometric and/or topological properties of \"easy nonconvex problems\" that are not captured by the strict saddle hypothesis. Hence, the motivation of this paper is twofold: (i) to provide theoretical corroboration (in certain specific situations) for what is arguably the simplest, most natural, and most widely used first order method, and (ii) to contribute to the ongoing effort to identify conditions which make nonconvex problems amenable to efficient optimization. Suppose we are given data matrix Y = y 1 , . . . y p \u2208 R n\u00d7p . The dictionary learning problem asks us to find a concise representation of the data BID12 , of the form Y \u2248 AX, where X is a sparse matrix. In the complete, orthogonal dictionary learning problem, we restrict the matrix A to have orthonormal columns (A \u2208 O(n)). This variation of dictionary learning is useful for finding concise representations of small datasets (e.g., patches from a single image, in MRI FORMULA2 ).To analyze the behavior of dictionary learning algorithms theoretically, it useful to posit that Y = A 0 X 0 for some true dictionary A 0 \u2208 O(n) and sparse coefficient matrix X 0 \u2208 R n\u00d7p , and ask whether a given algorithm recovers the pair (A 0 , X 0 ). BID3 In this work, we further assume that the sparse matrix X 0 is random, with entries i.i.d. Bernoulli-Gaussian 5 . For simplicity, we will let A 0 = I; our arguments extend directly to general A 0 via the simple change of variables q \u2192 A * 0 q. (34) showed that under mild conditions, the complete dictionary recovery problem can be reduced to the geometric problem of finding a sparse vector in a linear subspace (31). Notice that because A 0 is orthogonal, row(Y ) = row(X 0 ). Because X 0 is a sparse random matrix, the rows of X 0 are sparse vectors. Under mild conditions (34), they are the sparsest vectors in the row space of Y , and hence can be recovered by solving the conceptual optimization problem min q DISPLAYFORM0 This is not a well-structured optimization problem: the objective is discontinuous, and the constraint set is open. A natural remedy is to replace the 0 norm with a smooth sparsity surrogate, and to break the scale ambiguity by constraining q to the sphere, giving DISPLAYFORM1 Here, we choose h \u00b5 (t) = \u00b5 log(cosh(t/\u00b5)) as a smooth sparsity surrogate. This objective was analyzed in (35), which showed that (i) although this optimization problem is nonconvex, when the data are sufficiently large, with high probability every local optimizer is near a signed column of the true dictionary A 0 , (ii) every other critical point has a direction of strict negative curvature, and (iii) as a consequence, a second-order Riemannian trust region method efficiently recovers a column of A 0 . BID5 The Riemannian trust region method is of mostly theoretical interest: it solves complicated (albeit polynomial time) subproblems that involve the Hessian of f DL . Note the similarity to the dictionary learning objective. Right: The objective for complete orthogonal dictionary learning (discussed in section 6) for n = 3.In practice, simple iterative methods, including randomly initialized gradient descent are also observed to rapidly obtain high-quality solutions. In the sequel, we will give a geometric explanation for this phenomenon, and bound the rate of convergence of randomly initialized gradient descent to the neighborhood of a column of A 0 . Our analysis of f DL is probabilistic in nature: it argues that with high probability in the sparse matrix X 0 , randomly initialized gradient descent rapidly produces a minimizer. To isolate more clearly the key intuitions behind this analysis, we first analyze the simpler separable objective DISPLAYFORM2 Figure 2 plots both f Sep and f DL as functions over the sphere. Notice that many of the key geometric features in f DL are present in f Sep ; indeed, f Sep can be seen as an \"ultrasparse\" version of f DL in which the columns of the true sparse matrix X 0 are taken to have only one nonzero entry. A virtue of this model function is that its critical points and their stable manifolds have simple closed form expressions (see Lemma 1). Our problems of interest have the form DISPLAYFORM0 where f : R n \u2192 R is a smooth function. We let \u2207f (q) and \u2207 2 f (q) denote the Euclidean gradient and hessian (over R n ), and let grad [f ] (q) and Hess [f ] (q) denote their Riemannian counterparts (over S n\u22121 ). We will obtain results for Riemannian gradient descent defined by the update DISPLAYFORM1 for some step size \u03b7 > 0, where exp q : T q S n\u22121 \u2192 S n\u22121 is the exponential map. The Riemannian gradient on the sphere is given by grad[f ] (q) = (I \u2212 qq * )\u2207f (q).We let A denote the set of critical points of f over S n\u22121 -these are the pointsq s.t. grad [f ] (q) = 0. We let\u0202 denote the set of local minimizers, and \"A its complement. Both f Sep and f DL are Morse functions on S n\u22121 , 7 we can assign an index \u03b1 to everyq \u2208 A, which is the number of negative eigenvalues of Hess [f ] (q).Our goal is to understand when gradient descent efficiently converges to a local minimizer. In the small-step limit, gradient descent follows gradient flow lines \u03b3 : R \u2192 M, which are solution curves of the ordinary differential equatio\u1e45 DISPLAYFORM2 To each critical point \u03b1 \u2208 A of index \u03bb, there is an associated stable manifold of dimension dim(M) \u2212 \u03bb, which is roughly speaking, the set of points that flow to \u03b1 under gradient flow: DISPLAYFORM3 7 Strictly speaking, fDL is Morse with high probability, due to results of (38). Negative curvature and efficient gradient descent. The union of the light blue, orange and yellow sets is the set C. In the light blue region, there is negative curvature normal to \u2202C, while in the orange region the gradient norm is large, as illustrated by the arrows. There is a single global minimizer in the yellow region. For the separable objective, the stable manifolds of the saddles and maximizers all lie on \u2202C (the black circles denote the critical points, which are either maximizers \" \", saddles \" \", or minimizers \" \"). The red dots denote \u2202C \u03b6 with \u03b6 = 0.2.Our analysis uses the following convenient coordinate chart DISPLAYFORM0 where w \u2208 B 1 (0). We also define two useful sets: DISPLAYFORM1 Since the problems considered here are symmetric with respect to a signed permutation of the coordinates we can consider a certain C and the results will hold for the other symmetric sections as well. We will show that at every point in C aside from a neighborhood of a global minimizer for the separable objective (or a solution to the dictionary problem that may only be a local minimizer), there is either a large gradient component in the direction of the minimizer or negative curvature in a direction normal to \u2202C. For the case of the separable objective, one can show that the stable manifolds of the saddles lie on this boundary, and hence this curvature is normal to the stable manifolds of the saddles and allows rapid progress away from small gradient regions and towards a global minimizer BID7 . These regions are depicted in Figure 3 .In the sequel, we will make the above ideas precise for the two specific nonconvex optimization problems discussed in Section 3 and use this to obtain a convergence rate to a neighborhood of a global minimizer. Our analysis are specific to these problems. However, as we will describe in more detail later, they hinge on important geometric characteristics of these problems which make them amenable to efficient optimization, which may obtain in much broader classes of problems. In this section, we study the behavior of randomly initialized gradient descent on the separable function f Sep . We begin by characterizing the critical points:Lemma 1 (Critical points of f Sep ). The critical points of the separable problem (2) are DISPLAYFORM0 For every \u03b1 \u2208 A and corresponding a(\u03b1), for \u00b5 < c \u221a n log n the stable manifold of \u03b1 takes the form DISPLAYFORM1 where c > 0 is a numerical constant. Proof. Please see Appendix ABy inspecting the dimension of the stable manifolds, it is easy to verify that that there are 2n global minimizers at the 1-sparse vectors on the sphere \u00b1 e i , 2 n maximizers at the least sparse vectors and an exponential number of saddle points of intermediate sparsity. This is because the dimension of W s (\u03b1) is simply the dimension of b in 6, and it follows directly from the stable manifold theorem that only minimizers will have a stable manifold of dimension n \u2212 1. The objective thus possesses no spurious local minimizers. When referring to critical points and stable manifolds from now on we refer only to those that are contained in C or on its boundary. It is evident from Lemma 1 that the critical points in \"A all lie on \u2202C and that \u03b1\u2208 \" A W s (\u03b1) = \u2202C , and there is a minimizer at its center given by q(0) = e n . We now turn to making precise the notion that negative curvature normal to stable manifolds of saddle points enables gradient descent to rapidly exit small gradient regions. We do this by defining vector fields u (i) (q), i \u2208 [n \u2212 1] such that each field is normal to a continuous piece of \u2202C \u03b6 and points outwards relative to C \u03b6 defined in 4. By showing that the Riemannian gradient projected in this direction is positive and proportional to \u03b6, we are then able to show that gradient descent acts to increase \u03b6(q(w)) = qn w \u221e \u2212 1 geometrically. This corresponds to the behavior illustrated in the light blue region in Figure 3 . DISPLAYFORM0 DISPLAYFORM1 where c > 0 is a numerical constant. Proof. Please see Appendix A.Since we will use this property of the gradient in C \u03b6 to derive a convergence rate, we will be interested in bounding the probability that gradient descent initialized randomly with respect to a uniform measure on the sphere is initialized in C \u03b6 . This will require bounding the volume of this set, which is done in the following lemma: DISPLAYFORM2 Proof. Please see Appendix D.3. Using the results above, one can obtain the following convergence rate:Theorem 1 (Gradient descent convergence rate for separable function). For any 0 < \u03b6 0 < 1, r > \u00b5 log on the separable objective (2) with \u00b5 < c2 \u221a n log n , enters an L \u221e ball of radius r around a global minimizer in T < C \u03b7 \u221a n r 2 + log 1 \u03b6 0 iterations with probability DISPLAYFORM0 Proof. Please see Appendix A.We have thus obtained a convergence rate for gradient descent that relies on the negative curvature around the stable manifolds of the saddles to rapidly move from these regions of the space towards the vicinity of a global minimizer. This is evinced by the logarithmic dependence of the rate on \u03b6. As was shown for orthogonal dictionary learning in (38), we also expect a linear convergence rate due to strong convexity in the neighborhood of a minimizer, but do not take this into account in the current analysis. The proofs in this section will be along the same lines as those of Section 5. While we will not describe the positions of the critical points explicitly, the similarity between this objective and the separable function motivates a similar argument. It will be shown that initialization in some C \u03b6 will guarantee that Riemannian gradient descent makes uniform progress in function value until reaching the neighborhood of a global minimizer. We will first consider the population objective which corresponds to the infinite data limit DISPLAYFORM0 and then bounding the finite sample size fluctuations of the relevant quantities. We begin with a lemma analogous to Lemma 2:Lemma 4 (Dictionary learning population gradient). For w \u2208 C \u03b6 , r < |w i |, \u00b5 < c 1 r DISPLAYFORM1 \u221a \u03b6 the dictionary learning population objective 8 obeys DISPLAYFORM2 where c \u03b8 depends only on \u03b8, c 1 is a positive numerical constant and u (i) is defined in 7.Proof. Please see Appendix BUsing this result, we obtain the desired convergence rate for the population objective, presented in Lemma 11 in Appendix B. After accounting for finite sample size fluctuations in the gradient, one obtains a rate of convergence to the neighborhood of a solution (which is some signed basis vector due to our choice A 0 = I) Theorem 2 (Gradient descent convergence rate for dictionary learning). DISPLAYFORM3 , Riemannian gradient descent with step size \u03b7 < c5\u03b8s n log np on the dictionary learning objective 1 with \u00b5 < c6 \u221a \u03b60 DISPLAYFORM4 , enters a ball of radius c 3 s from a target solution in DISPLAYFORM5 iterations with probability DISPLAYFORM6 where y = DISPLAYFORM7 , P y is given in Lemma 10 and c i , C i are positive constants. Proof. Please see Appendix BThe two terms in the rate correspond to an initial geometric increase in the distance from the set containing the small gradient regions around saddle points, followed by convergence to the vicinity of a minimizer in a region where the gradient norm is large. The latter is based on results on the geometry of this objective provided in (38). The above analysis suggests that second-order properties -namely negative curvature normal to the stable manifolds of saddle points -play an important role in the success of randomly initialized gradient descent in the solution of complete orthogonal dictionary learning. This was done by furnishing a convergence rate guarantee that holds when the random initialization is not in regions that feed into small gradient regions around saddle points, and bounding the probability of such an initialization. In Appendix C we provide an additional example of a nonconvex problem that for which an efficient rate can be obtained based on an analysis that relies on negative curvature normal to stable manifolds of saddles -generalized phase retrieval. An interesting direction of further work is to more precisely characterize the class of functions that share this feature. The effect of curvature can be seen in the dependence of the maximal number of iterations T on the parameter \u03b6 0 . This parameter controlled the volume of regions where initialization would lead to slow progress and the failure probability of the bound 1 \u2212 P was linear in \u03b6 0 , while T depended logarithmically on \u03b6 0 . This logarithmic dependence is due to a geometric increase in the distance from the stable manifolds of the saddles during gradient descent, which is a consequence of negative curvature. Note that the choice of \u03b6 0 allows one to flexibly trade off between T and 1 \u2212 P. By decreasing \u03b6 0 , the bound holds with higher probability, at the price of an increase in T . This is because the volume of acceptable initializations now contains regions of smaller minimal gradient norm. In a sense, the result is an extrapolation of works such as (23) that analyze the \u03b6 0 = 0 case to finite \u03b6 0 .Our analysis uses precise knowledge of the location of the stable manifolds of saddle points. For less symmetric problems, including variants of sparse blind deconvolution (41) and overcomplete tensor decomposition, there is no closed form expression for the stable manifolds. However, it is still possible to coarsely localize them in regions containing negative curvature. Understanding the implications of this geometric structure for randomly initialized first-order methods is an important direction for future work. One may hope that studying simple model problems and identifying structures (here, negative curvature orthogonal to the stable manifold) that enable efficient optimization will inspire approaches to broader classes of problems. One problem of obvious interest is the training of deep neural networks for classification, which shares certain high-level features with the problems discussed in this paper. The objective is also highly nonconvex and is conjectured to contain a proliferation of saddle points BID10 , yet these appear to be avoided by first-order methods BID15 for reasons that are still quite poorly understood beyond the two-layer case (39).[19] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. DISPLAYFORM0 . Thus critical points are ones where either tanh( q \u00b5 ) = 0 (which cannot happen on S n\u22121 ) or tanh( q \u00b5 ) is in the nullspace of (I \u2212 qq * ), which implies tanh( q \u00b5 ) = cq for some constant b. The equation tanh( x \u00b5 ) = bx has either a single solution at the origin or 3 solutions at {0, \u00b1r(b)} for some r(b). Since this equation must be solves simultaneously for every element of q, we obtain \u2200i \u2208 [n] : q i \u2208 {0, \u00b1r(b)}. To obtain solutions on the sphere, one then uses the freedom we have in choosing b (and thus r(b)) such that q = 1. The resulting set of critical points is thus DISPLAYFORM1 To prove the form of the stable manifolds, we first show that for q i such that |q i | = q \u221e and any q j such that |q j | + \u2206 = |q i | and sufficiently small \u2206 > 0, we have DISPLAYFORM2 For ease of notation we now assume q i , q j > 0 and hence \u2206 = q i \u2212 q j , otherwise the argument can be repeated exactly with absolute values instead. The above inequality can then be written as DISPLAYFORM3 If we now define DISPLAYFORM4 where the O(\u2206 2 ) term is bounded. Defining a vector r \u2208 R n by DISPLAYFORM5 we have r 2 = 1. Since tanh(x) is concave for x > 0, and |r i | \u2264 1, we find DISPLAYFORM6 From DISPLAYFORM7 and thus q j \u2265 1 \u221a n \u2212 \u2206. Using this inequality and properties of the hyperbolic secant we obtain DISPLAYFORM8 and plugging in \u00b5 = c \u221a n log n for some c < 1 DISPLAYFORM9 log n + log log n + log 4).We can bound this quantity by a constant, say h 2 \u2264 1 2 , by requiring DISPLAYFORM10 ) log n + log log n \u2264 \u2212 log 8and for and c < 1, using \u2212 log n + log log n < 0 we have DISPLAYFORM11 Since \u2206 can be taken arbitrarily small, it is clear that c can be chosen in an n-independent manner such that A \u2264 \u2212 log 8. We then find DISPLAYFORM12 since this inequality is strict, \u2206 can be chosen small enough such that O(\u2206 2 ) < \u2206(h 1 \u2212 h 2 ) and hence h > 0, proving 9.It follows that under negative gradient flow, a point with |q j | < ||q|| \u221e cannot flow to a point q such that |q j | = ||q || \u221e . From the form of the critical points, for every such j, q must thus flow to a point such that q j = 0 (the value of the j coordinate cannot pass through 0 to a point where |q j | = ||q || \u221e since from smoothness of the objective this would require passing some q with q j = 0, at which point grad [f Sep ] (q ) j = 0).As for the maximal magnitude coordinates, if there is more than one coordinate satisfying |q i1 | = |q i2 | = q \u221e , it is clear from symmetry that at any subsequent point q along the gradient flow line q i1 = q i2 . These coordinates cannot change sign since from the smoothness of the objective this would require that they pass through a point where they have magnitude smaller than 1/ \u221a n, at which point some other coordinate must have a larger magnitude (in order not to violate the spherical constraint), contradicting the above result for non-maximal elements. It follows that the sign pattern of these elements is preserved during the flow. Thus there is a single critical point to which any q can flow, and this is given by setting all the coordinates with |q j | < q \u221e to 0 and multiplying the remaining coordinates by a positive constant to ensure the resulting vector is on S n . Denoting this critical point by \u03b1, there is a vector b such that q = P S n\u22121 [a(\u03b1) + b] and supp(a(\u03b1)) \u2229 supp(b) = \u2205, b \u221e < 1 with the form of a(\u03b1) given by 5 . The collection of all such points defines the stable manifold of \u03b1. Proof of Lemma 2: (Separable objective gradient projection). i) We consider the sign(w i ) = 1 case; the sign(w i ) = \u22121 case follows directly. Recalling that DISPLAYFORM13 qn , we first prove DISPLAYFORM14 for some c > 0 whose form will be determined later. The inequality clearly holds for w i = q n .To DISPLAYFORM15 verify that it holds for smaller values of w i as well, we now show that \u2202 \u2202w i tanh w i \u00b5 \u2212 tanh q n \u00b5 w i q n \u2212 c(q n \u2212 w i ) < 0 which will ensure that it holds for all w i . We define s 2 = 1 \u2212 ||w|| 2 + w 2 i and denote q n = s 2 \u2212 w 2 i to extract the w i dependence, givingWhere in the last inequality we used properties of the sech function and q n \u2265 w i . We thus want to show DISPLAYFORM16 and it follows that 10 holds. For \u00b5 < 1 BID15 we are guaranteed that c > 0.From examining the RHS of 10 (and plugging in q n = s 2 \u2212 w 2 i ) we see that any lower bound on the gradient of an element w j applies also to any element |w i | \u2264 |w j |. Since for |w j | = ||w|| \u221e we have q n \u2212 w j = w j \u03b6, for every log( 1 \u00b5 )\u00b5 \u2264 w i we obtain the bound DISPLAYFORM17 Proof of Theorem 1: (Gradient descent convergence rate for separable function).We obtain a convergence rate by first bounding the number of iterations of Riemannian gradient descent in C \u03b60 \\C 1 , and then considering DISPLAYFORM18 . Choosing c 2 so that \u00b5 < 1 2 , we can apply Lemma 2, and for u defined in 7, we thus have DISPLAYFORM19 Since from Lemma 7 the Riemannian gradient norm is bounded by \u221a n, we can choose c 1 , c 2 such that \u00b5 log( DISPLAYFORM20 . This choice of \u03b7 then satisfies the conditions of Lemma 17 with r = \u00b5 log( DISPLAYFORM21 , M = \u221a n, which gives that after a gradient step DISPLAYFORM22 for some suitably chosenc > 0. If we now define by w (t) the t-th iterate of Riemannian gradient descent and DISPLAYFORM23 and the number of iterations required to exit C \u03b60 \\C 1 is DISPLAYFORM24 To bound the remaining iterations, we use Lemma 2 to obtain that for every w \u2208 C \u03b60 \\B \u221e r , DISPLAYFORM25 where we have used ||u DISPLAYFORM26 We thus have DISPLAYFORM27 Choosing DISPLAYFORM28 where L is the gradient Lipschitz constant of f s , from Lemma 5 we obtain DISPLAYFORM29 According to Lemma B, L = 1/\u00b5 and thus the above holds if we demand \u03b7 < \u00b5 2 . Combining 12 and 13 gives DISPLAYFORM30 .To obtain the final rate, we use in g(w 0 ) \u2212 g * \u2264 \u221a n andc\u03b7 < 1 \u21d2 1 log(1+c\u03b7) <C c\u03b7 for som\u1ebd C > 0. Thus one can choose C > 0 such that DISPLAYFORM31 From Lemma 1 the ball B \u221e r contains a global minimizer of the objective, located at the origin. The probability of initializing in \u0202 C \u03b60 is simply given from Lemma 3 and by summing over the 2n possible choices of C \u03b60 , one for each global minimizer (corresponding to a single signed basis vector). , where L is a lipschitz constant for \u2207f (q), one has DISPLAYFORM0 Proof. Just as in the euclidean setting, we can obtain a lower bound on progress in function values of iterates of the Riemannian gradient descent algorithm from a lower bound on the Riemannian gradient. Consider f : S n\u22121 \u2192 R, which has L-lipschitz gradient. Let q k denote the current iterate of Riemannian gradient descent, and let t k > 0 denote the step size. Then we can form the Taylor approximation to f \u2022 Exp q k (v) at 0 q k : DISPLAYFORM1 where the matrix norm is the operator norm on R n\u00d7n . Using the gradient-lipschitz property of f , we readily compute DISPLAYFORM0 since \u2207f (0) = 0 and q k \u2208 S n\u22121 . We thus have DISPLAYFORM1 If we put v = \u2212t k grad[f ](q k ) and write q k+1 = Exp q k (\u2212t k grad [f ] (q k )), the previous expression becomes DISPLAYFORM2 . Thus progress in objective value is guaranteed by lower-bounding the Riemannian gradient. As in the euclidean setting, summing the previous expression over iterations k now yields DISPLAYFORM3 Plugging in a constant step size gives the desired result. Lemma 6 (Lipschitz constant of \u2207f ). For any x 1 , x 2 \u2208 R n , it holds DISPLAYFORM4 Proof. It will be enough to study a single coordinate function of \u2207f . Using a derivative given in section D.1, we have for DISPLAYFORM5 A bound on the magnitude of the derivative of this smooth function implies a lipschitz constant for x \u2192 tanh(x/\u00b5). To find the bound, we differentiate again and find the critical points of the function. We have, using the chain rule, d dx DISPLAYFORM6 (e x/\u00b5 + e \u2212x/\u00b5 ) 3 . The denominator of this final expression vanishes nowhere. Hence, the only critical point satisfies x/\u00b5 = \u2212x/\u00b5, which implies x = 0. Therefore it holds DISPLAYFORM7 which shows that tanh(x/\u00b5) is (1/\u00b5)-lipschitz. Now let x 1 and x 2 be any two points of R n . Then one has DISPLAYFORM8 completing the proof. Proof of Lemma 4:(Dictionary learning population gradient). For simplicity we consider the case sign(w i ) = 1. The converse follows by a similar argument. We have DISPLAYFORM0 Following the notation of (38), we write x j = b j v j where b j \u223c Bern(\u03b8), v j \u223c N (0, 1) and denote the vectors of these variables by J , v respectively. Defining DISPLAYFORM1 and similarly the second term in 15 is, with DISPLAYFORM0 We already have a lower bound in Lemma 20 of (38) that we can use for the second term, so we need an upper bound for the first term. Following from p. 865, we define DISPLAYFORM0 , and defining DISPLAYFORM1 Where b k = (\u2212\u03b2) k (k + 1). Using B.3 from Lemma 40 in (38) we have DISPLAYFORM2 Where \u03a6 c (x) is the complementary Gaussian CDF (The exchange of summation and expectation is justified since Y > 0 implies Z \u2208 [0, 1], see proof of Lemma 18 in (38) for details). Using the following bounds DISPLAYFORM3 2 /2 by applying the upper (lower) bound to the even (odd) terms in the sum, and then adding a non-negative quantity, we obtain DISPLAYFORM4 and using Lemma 17 in (38) ) and taking T \u2192 \u221e so that \u03b2 \u2192 1 we have DISPLAYFORM5 DISPLAYFORM6 giving the upper bound DISPLAYFORM7 while the lower bound (Lemma 20 in (38)) is DISPLAYFORM8 After conditioning on J \\{n, i} the variables X + q n v n , X + q i v i are Gaussian. We can thus plug the bounds into 16 to obtain DISPLAYFORM0 the term in the expectation is positive since q n > ||w|| \u221e (1 + \u03b6) > w i giving DISPLAYFORM1 To extract the \u03b6 dependence we plug in q n > w i (1 + \u03b6) and develop to first order in \u03b6 (since the resulting function of \u03b6 is convex) giving DISPLAYFORM2 Given some \u03b6 and r such that w i > r, if we now choose \u00b5 such that \u00b5 < Lemma 8 (Point-wise concentration of projected gradient). For u (i) defined in 7, the gradient of the objective 1 obeys DISPLAYFORM3 Proof of Lemma 8: (Point-wise concentration of projected gradient). If we denote by x i a column of the data matrix with entries x i j \u223c BG(\u03b8), we have DISPLAYFORM4 . Since tanh(x) is bounded by 1, DISPLAYFORM5 Invoking Lemma 21 from (38) and u 2 = 1 + DISPLAYFORM6 and using Lemma 36 in (38) with R = \u221a 2, \u03c3 = \u221a 2 we have DISPLAYFORM7 Lemma 9 (Projection Lipschitz Constant). The Lipschitz constant for DISPLAYFORM8 Proof of Lemma 9: (Projection Lipschitz Constant). We have DISPLAYFORM9 where we have defined DISPLAYFORM10 We also use the fact that tanh is bounded by 1 and s(w) is bounded by X \u221e . We can then use Lemma 23 in (38) to obtain DISPLAYFORM11 Lemma 10 (Uniformized gradient fluctuations). For all w \u2208 C \u03b6 , i \u2208 [n], with probability P > P y we have DISPLAYFORM12 where DISPLAYFORM13 Proof of Lemma 10:(Uniformized gradient fluctuations). For X \u2208 R n\u00d7p with i.i.d. BG(\u03b8) entries, we define the event E \u221e \u2261 {1 \u2264 X \u221e \u2264 4 log(np)}. We have DISPLAYFORM14 For any \u03b5 \u2208 (0, 1) we can construct an \u03b5-net N for C \u03b6 \\B 2 1/20 DISPLAYFORM15 . If we choose \u03b5 = y(\u03b8,\u03b6) DISPLAYFORM16 We then denote by E g the event DISPLAYFORM17 2 in the result of Lemma 8 gives that for all w \u2208 C \u03b6 , i \u2208 [n], DISPLAYFORM18 py(\u03b8, \u03b6) Proof of Lemma 11: (Gradient descent convergence rate for dictionary learning -population). The rate will be obtained by splitting C \u03b60 into three regions. We consider convergence to B 2 s (0) since this set contains a global minimizer. Note that the balls in the proof are defined with respect to w. DISPLAYFORM19 The analysis in this region is completely analogous to that in the first part of the proof of Lemma 1. For every point in this set we have DISPLAYFORM20 . From Lemma 16 we know that , since for every point in this region r 3 \u03b6 < 1, we have DISPLAYFORM21 DISPLAYFORM22 r = z(r, \u03b6) and we thus demand \u00b5 < \u221a \u03b60 DISPLAYFORM23 and obtain from Lemma 4 that for |w i | > r DISPLAYFORM24 . We now require \u03b7 < , M = \u221a \u03b8n (since the maximal norm of the Riemannian gradient is \u221a \u03b8n from Lemma 12), obtaining that at every iteration in this region \u03b6 \u2265 \u03b6 1 + \u221a nc DL 2(8000(n \u2212 1)) 3/2 \u03b7 and the maximal number of iterations required to obtain \u03b6 > 8 and exit this region is given by DISPLAYFORM25 According to Proposition 7 in (38), which we can apply since s \u2265 DISPLAYFORM26 . Defining h(q) = w 2 2 , and denoting by q an update of Riemannian gradient descent with step size \u03b7, we have (using a Lagrange remainder term) DISPLAYFORM27 where in the last line we used q = cos(g\u03b7)q \u2212 sin(g\u03b7) DISPLAYFORM28 we obtain (using 18) DISPLAYFORM29 and thus choosing \u03b7 < DISPLAYFORM30 we find DISPLAYFORM31 Under review as a conference paper at ICLR 2019 and in our region of interest w 2 < w 2 \u2212cs\u03b8\u03b7 for somec > 0 and thus summing over iterations, we obtain for someC 2 > 0 DISPLAYFORM32 From Lemma 12, M = \u221a \u03b8n and thus with a suitably chosen c 2 > 0, \u03b7 < c2s n satisfies the above requirement on \u03b7 as well as the previous requirements, since \u03b8 < 1. Combining these results gives, we find that when initializing in C \u03b60 , the maximal number of iterations required for Riemannian gradient descent to enter B 2 s (0) is DISPLAYFORM0 for some suitably chosen C 1 , where t 1 , t 2 are given in 17,19. The probability of such an initialization is given by the probability of initializing in one of the 2n possible choices of C \u03b6 , which is bounded in Lemma 3.Once w \u2208 B 2 s (0), the distance in R n\u22121 between w and a solution to the problem (which is a signed basis vector, given by the point w = 0 or an analog on a different symmetric section of the sphere) is no larger than s, which in turn implies that the Riemannian distance between \u03d5(w) and a solution is no larger than c 3 s for some c 3 > 0. We note that the conditions on \u00b5 can be satisfied by requiring \u00b5 < DISPLAYFORM1 where X is the data matrix with i.i.d. BG(\u03b8) entries. Proof. Denoting x \u2261 (x, x n ) we have DISPLAYFORM2 and using Jensen's inequality, convexity of the L 2 norm and the triangle inequality to obtain DISPLAYFORM3 Similarly, in the finite sample size case one obtains DISPLAYFORM4 Proof of Theorem 2: (Gradient descent convergence rate for dictionary learning). The proof will follow exactly that of Lemma 11, with the finite sample size fluctuations decreasing the guaranteed change in \u03b6 or ||w|| at every iteration (for the initial and final stages respectively) which will adversely affect the bounds. DISPLAYFORM5 To control the fluctuations in the gradient projection, we choose DISPLAYFORM6 which can be satisfied by choosing y(\u03b8, \u03b6 0 ) = DISPLAYFORM7 for an appropriate c 7 > 0 . According to Lemma 10, with probability greater than P y we then have DISPLAYFORM8 With the same condition on \u00b5 as in Lemma 11, combined with the uniformized bound on finite sample fluctuations, we have that at every point in this set DISPLAYFORM9 . According to Lemma 12 the Riemannian gradient norm is bounded by M = \u221a n X \u221e . Choosing r, b as in Lemma 11, we require \u03b7 < for some chosenc > 0. We then obtain DISPLAYFORM10 for a suitably chosen C 2 > 0. The final bound on the rate is obtained by summing over the terms for the three regions as in the population case, and convergence is again to a distance of less than c 3 s from a local minimizer. The probability of achieving this rate is obtained by taking a union bound over the probability of initialization in C \u03b60 (given in Lemma 3) and the probabilities of the bounds on the gradient fluctuations holding (from Lemma 10 and FORMULA7 ). Note that the fluctuation bound events imply by construction the event E \u221e = {1 \u2264 X \u221e \u2264 4 log(np)} hence we can replace X \u221e in the conditions on \u03b7 above by 4 log(np). The conditions on \u03b7, \u00b5 can be satisfied by requiring \u03b7 < c5\u03b8s n log np , \u00b5 < c6 \u221a \u03b60 n 5/4 for suitably chosen c 5 , c 6 > 0. The bound on the number of iterations can be simplified to the form in the theorem statement as in the population case. We show below that negative curvature normal to stable manifolds of saddle points in strict saddle functions is a feature that is found not only in dictionary learning, and can be used to obtain efficient convergence rates for other nonconvex problems as well, by presenting an analysis of generalized phase retrieval that is along similar lines to the dictionary learning analysis. We stress that this contribution is not novel since a more thorough analysis was carried out by BID7 . The resulting rates are also suboptimal, and pertain only to the population objective. Generalized phase retrieval is the problem of recovering a vector x \u2208 C n given a set of magnitudes of projections y k = |x * a k | onto a known set of vectors a k \u2208 C n . It arises in numerous domains including microscopy (27), acoustics BID1 , and quantum mechanics (10) (see (33) for a review). Clearly x can only be recovered up to a global phase. We consider the setting where the elements of every a k are i. DISPLAYFORM0 We analyze the least squares formulation of the problem (7) given by DISPLAYFORM1 Taking the expectation (large p limit) of the above objective and organizing its derivatives using Wirtinger calculus FORMULA2 , we obtain DISPLAYFORM2 For the remainder of this section, we analyze this objective, leaving the consideration of finite sample size effects to future work. In (37) it was shown that aside from the manifold of minim\u0203 DISPLAYFORM0 the only critical points of E[f ] are a maximum at z = 0 and a manifold of saddle points given by DISPLAYFORM1 where W \u2261 {z|z * x = 0}. We decompose z as DISPLAYFORM2 where \u03b6 > 0, w \u2208 W . This gives z 2 = w 2 + \u03b6 2 . The choice of w, \u03b6, \u03c6 is unique up to factors of 2\u03c0 in \u03c6, as can be seen by taking an inner product with x. Since the gradient decomposes as follows: DISPLAYFORM3 the directions e i\u03c6 x x , w w are unaffected by gradient descent and thus the problem reduces to a two-dimensional one in the space (\u03b6, w ). Note also that the objective for this twodimensional problem is a Morse function, despite the fact that in the original space there was a manifold of saddle points. It is also clear from this decomposition of the gradient that the stable manifolds of the saddles are precisely the set W .It is evident from 24 that the dispersive property does not hold globally in this case. For z / \u2208 B ||x|| we see that gradient descent will cause \u03b6 to decrease, implying positive curvature normal to the stable manifolds of the saddles. This is a consequence of the global geometry of the objective. Despite this, in the region of the space that is more \"interesting\", namely B ||x|| , we do observe the dispersive property, and can use it to obtain a convergence rate for gradient descent. We define a set that contains the regions that feeds into small gradient regions around saddle points within B ||x|| by DISPLAYFORM4 We will show that, as in the case of orthogonal dictionary learning, we can both bound the probability of initializing in (a subset of) the complement of Q \u03b60 and obtain a rate for convergence of gradient descent in the case of such an initialization. plane. The full red curves are the boundaries between the sets S 1 , S 2 , S 3 , S 4 used in the analysis. The dashed red line is the boundary of the set Q \u03b60 that contains small gradient regions around critical points that are not minima. The maximizer and saddle point are shown in dark green, while the minimizer is in pink. These are used to find the change in \u03b6, w at every iteration in each region: DISPLAYFORM5 We now show that gradient descent initialized in S 1 \\Q \u03b60 cannot exit \u222a 2 we are guaranteed from Lemma 13 that at every iteration \u03b6 \u2265 \u03b6 0 . Thus the region with \u03b6 < \u03b6 0 can only be entered if gradient descent is initialized in it. It follows that initialization in S 1 \\Q \u03b60 rules out entering Q \u03b60 at any future iteration of gradient descent. Since this guarantees that regions that feed into small gradient regions are avoided, an efficient convergence rate can again be obtained. Theorem 3 (Gradient descent convergence rate for generalized phase retrieval). Gradient descent on 22 with step size \u03b7 < DISPLAYFORM0 iterations with probability DISPLAYFORM1 ii) Since only a step from S 4 can decrease \u03b6, we have that for the initial point z 2 > x 2 .Combined with DISPLAYFORM2 this gives DISPLAYFORM3 and using the lower bound (1 \u2212 2\u03b7 x 2 c)\u03b6 \u2264 \u03b6 we obtain DISPLAYFORM4 where in the last inequality we used c < DISPLAYFORM5 Proof of Lemma 14. We use the fact that for the next iterate we have DISPLAYFORM6 We will also repeatedly use \u03b7 < DISPLAYFORM7 which is a shown in Lemma 13. DISPLAYFORM8 We want to show DISPLAYFORM9 (1 + c) x 2 .1) We have z \u2208 S 3 \u21d2 z 2 = (1 \u2212 \u03b5) x 2 for some \u03b5 \u2264 c and using 28 we must show DISPLAYFORM10 Proof of Theorem 3: (Gradient descent convergence rate for generalized phase retrieval). We now bound the number of iterations that gradient descent, after random initialization in S 1 , requires to reach a point where one of the convergence criteria detailed in Lemma 15 is fulfilled. From Lemma 14, we know that after initialization in S 1 we need to consider only the set DISPLAYFORM11 S i . The number of iterations in each set will be determined by the bounds on the change in \u03b6, ||w|| detailed in 27. Assuming we initialize with some \u03b6 = \u03b6 0 . Then the maximal number of iterations in this region is DISPLAYFORM0 since after this many iterations DISPLAYFORM1 The only concern is that after an iteration in S 3 \u222a S 4 the next iteration might be in S 2 .To account for this situation, we find the maximal number of iterations required to reach S 3 \u222a S 4 again. This is obtained from the bound on \u03b6 in Lemma 13.Using this result, and the fact that for every iteration in S 2 we are guaranteed \u03b6 \u2265 (1 + 2\u03b7 x 2 c)\u03b6 the number of iterations required to reach S 3 \u222a S 4 again is given by DISPLAYFORM2 The final rate to convergence is DISPLAYFORM0 C.9 Probability of the bound holdingThe bound applies to an initialization with \u03b6 \u2265 \u03b6 0 , hence in S 1 \\Q \u03b60 . Assuming uniform initialization in S 1 , the set Q \u03b60 is simply a band of width 2\u03b6 0 around the equator of the ball B x / \u221a 2 (in R 2n , using the natural identification of C n with R 2n ). This volume can be calculated by integrating over 2n \u2212 1 dimensional balls of varying radius. DISPLAYFORM1 and by V (n) = \u03c0 n/2 n 2 \u0393( n 2 ) the hypersphere volume, the probability of initializing in S 1 \u2229 Q \u03b60 (and thus in a region that feeds into small gradient regions around saddle points) is DISPLAYFORM2 . For small \u03b6 we again find that P(fail) scales linearly with \u03b6, as was the case for the previous problems considered. Proof of Lemma 3: (Volume of C \u03b6 ). We are interested in the relative volume DISPLAYFORM0 Vol(S n\u22121 ) \u2261 V \u03b6 . Using the standard solid angle formula, it is given by DISPLAYFORM1 This integral admits no closed form solution but one can construct a linear approximation around small \u03b6 and show that it is convex. Thus the approximation provides a lower bound for V \u03b6 and an upper bound on the failure probability. From symmetry considerations the zero-order term is V 0 = 1 2n . The first-order term is given by DISPLAYFORM2 We now require an upper bound for the second integral since we are interested in a lower bound for V \u03b6 . We can express it in terms of the second moment of the L \u221e norm of a Gaussian vector as follows: where \u00b5(X) is the Gaussian measure on the vector X \u2208 R n . We can bound the first term using Combining these bounds, the leading order behavior of the gradient is DISPLAYFORM3 This linear approximation is indeed a lower bound, since using integration by parts twice we have (0) and this is the smallest L \u221e ball containing C \u03b6 .Proof. Given the surface of some L \u221e ball for w , we can ask what is the minimal \u03b6 such that \u2202C \u03b6m intersects this surface. This amounts to finding the minimal q n given some w \u221e . Yet this is clearly obtained by setting all the coordinates of w to be equal to w \u221e (this is possible since we are guaranteed q n \u2265 w \u221e \u21d2 w \u221e \u2264 where one instead maximizes q n with some fixed w \u221e .Given some surface of an L 2 ball, we can ask what is the minimal C \u03b6 such that C \u03b6 \u2286 B 2 r (0). This is equivalent to finding the maximal \u03b6 M such that \u2202C \u03b6 M intersects the surface of the L 2 ball. Since q n is fixed, maximizing \u03b6 is equivalent to minimizing w \u221e . This is done by setting w \u221e = w \u221a n\u22121, which gives DISPLAYFORM4 The statement in the lemma follows from combining these results. . If we now combine this with the fact that after a Riemannian gradient step cos(g\u03b7)q i \u2212 sin(g\u03b7) \u2264 q i \u2264 cos(g\u03b7)q i + sin(g\u03b7), the above condition on \u03b7 implies the inequality ( * ), which in turn ensures that |w i | < r \u21d2 |w i | < w \u221e : |w i | < |w i | + sin(g\u03b7) < r + g\u03b7 < ( * )(1 \u2212 g 2 \u03b7 2 )b \u2212 g\u03b7 < cos(g\u03b7) w \u221e \u2212 sin(g\u03b7) \u2264 w \u221e Due to the above analysis, it is evident that any w i such that |w i | = w \u221e obeys |w i | > r, from which it follows that we can use 31 to obtain q n w \u221e \u2212 1 = \u03b6 \u2265 \u03b6 1 + \u221a n 2 \u03b7c(w)"
}