{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) (Burda et al., 2016) is a popular variational-inference method which achieves a tighter evidence bound (and hence a lower bias) than standard variational autoencoders by optimising a multi-sample objective, i.e. an objective that is expressible as an integral over $K > 1$ Monte Carlo samples. Unfortunately, IWAE crucially relies on the availability of reparametrisations and even if these exist, the multi-sample objective leads to inference-network gradients which break down as $K$ is increased (Rainforth et al., 2018). This breakdown can only be circumvented by removing high-variance score-function terms, either by heuristically ignoring them (which yields the 'sticking-the-landing' IWAE (IWAE-STL) gradient from Roeder et al. (2017)) or through an identity from Tucker et al. (2019) (which yields the 'doubly-reparametrised' IWAE (IWAE-DREG) gradient). In this work, we argue that directly optimising the proposal distribution in importance sampling as in the reweighted wake-sleep (RWS) algorithm from Bornschein & Bengio (2015) is preferable to optimising IWAE-type multi-sample objectives. To formalise this argument, we introduce an adaptive-importance sampling framework termed adaptive importance sampling for learning (AISLE) which slightly generalises the RWS algorithm. We then show that AISLE admits IWAE-STL and IWAE-DREG (i.e. the IWAE-gradients which avoid breakdown) as special cases. Let x be some observation and let z be some latent variable taking values in some space Z. These are modeled via the generative model p \u03b8 (z, x) = p \u03b8 (z)p \u03b8 (x|z) which gives rise to the marginal likelihood p \u03b8 (x) = Z p \u03b8 (z, x) dz of the model parameters \u03b8. In this work, we analyse algorithms for variational inference, i.e. algorithms which aim to 1. learn the generative model, i.e. find a value \u03b8 which is approximately equal to the maximum-likelihood estimate (MLE) \u03b8 ml := arg max \u03b8 p \u03b8 (x); 2. construct a tractable variational approximation q \u03c6,x (z) of p \u03b8 (z|x) = p \u03b8 (z, x)/p \u03b8 (x), i.e. find the value \u03c6 such that q \u03c6 ,x (z) is as close as possible to p \u03b8 (z|x) in some suitable sense. A few comments about this setting are in order. Firstly, as is common in the literature, we restrict our presentation to a single latent representation-observation pair (z, x) to avoid notational clutter -the extension to multiple independent observations is straightforward. Secondly, we assume that no parameters are shared between the generative model p \u03b8 (z, x) and the variational approximation q \u03c6,x (z). This is common in neural-network applications but could be relaxed. Thirdly, our setting is general enough to cover amortised inference. For this reason, we often refer to \u03c6 as the parameters of an inference network. Two main classes of stochastic gradient-ascent algorithms for optimising \u03c8 := (\u03b8, \u03c6) which employ K \u2265 1 Monte Carlo samples ('particles') to reduce errors have been proposed. (Roeder et al., 2017) heuristically drops the problematic score-function terms from the IWAE \u03c6-gradient. This induces bias for the IWAE objective. -IWAE-DREG. The 'doubly-reparametrised' IWAE (IWAE-DREG) \u03c6-gradient (Tucker et al., 2019) unbiasedly removes the problematic score-function terms from the IWAE \u03c6-gradient using a formal identity. \u2022 RWS. The reweighted wake-sleep (RWS) algorithm (Bornschein & Bengio, 2015) optimises two separate objectives for \u03b8 and \u03c6. Its gradients are approximated by self-normalised importance sampling with K particles: this induces a bias which vanishes as K \u2192 \u221e. RWS can be viewed as an adaptive importance-sampling approach which iteratively improves its proposal distribution while simultaneously optimising \u03b8 via stochastic approximation. Crucially, the RWS \u03c6-gradients do not degenerate as K \u2192 \u221e. Of these two methods, the IWAE is the most popular and Tucker et al. (2019) demonstrated empirically that RWS can break down, conjecturing that this is due to the fact that RWS does not optimise a joint objective (for \u03b8 and \u03c6). Meanwhile, the IWAE-STL gradient performed consistently well despite lacking a firm theoretical footing. Yet, IWAE suffers from the above-mentioned \u03c6-gradient breakdown and exhibited inferior empirical performance to RWS (Le et al., 2019) . Thus, it is not clear whether the multi-sample objective approach of IWAE or the adaptive importance-sampling approach of RWS is preferable. In this work, we show that directly optimising the proposal distribution, e.g. as done by RWS, is preferable to optimising the IWAE multi-sample objective because (a) the multi-sample objective typically relies on reparametrisations and, even if these are available, leads to the \u03c6-gradient breakdown, (b) modifications of the IWAE \u03c6-gradient which avoid this breakdown (i.e. IWAE-STL and IWAE-DREG) can be justified in a more principled manner by taking an RWS-type adaptive importance-sampling view. This conclusion was already reached by Le et al. (2019) based on numerical experiments. They demonstrated that the need for reparametrisations can make IWAE inferrior to RWS e.g. for discrete latent variables. Our work complements theirs by formalising this argument. To this end, we slightly generalise the RWS algorithm to obtain a generic adaptive importance-sampling framework for variational inference which we term adaptive importance sampling for learning (AISLE) for ease of reference. We then show that AISLE admits not only RWS but also the IWAE-DREG and IWAE-STL gradients as special cases. Novel material is presented in Section 3, where we introduce the AISLEframework. From this, most of the previously proposed gradient estimators can be naturally derived in a principled manner. Importantly, the derived gradient estimators are guaranteed to not degenerate as K \u2192 \u221e. Specifically, we establish the following connections. \u2022 We prove that the IWAE-STL gradient can be recovered as a special case of AISLE via a principled and novel application of the 'double-reparametrisation' identity from Tucker et al. (2019) . This indicates that the breakdown of RWS observed in Tucker et al. (2019) may not be due to its lack of a joint objective as previously conjectured (since IWAE-STL avoided this breakdown despite having the same idealised objective as RWS). Our work also provides a theoretical foundation for IWAE-STL which was hitherto only heuristically justified as a biased IWAE-gradient. \u2022 We prove that AISLE also admits the IWAE-DREG gradient as a special case. Our derivation also makes it clear that the learning rate should be scaled as O(K) for the IWAE \u03c6-gradient (and its modified version IWAE-DREG) unless the gradients are normalised as implicitly done by popular optimisers such as ADAM (Kingma & Ba, 2015) . In contrast, the learning rate for AISLE need not be scaled up with of K. \u2022 When applied to the family of \u03b1-divergences, AISLE leads to a new family of gradient estimators that generalises some previously derived in the literature. \u2022 In the supplementary materials, we provide insights into the impact of the selfnormalisation bias on some of the importance-sampling based gradient approxima-tions (Appendix A) and empirically compare the main algorithms discussed in this work (Appendix B). We stress that the focus of our work is not necessarily to derive new algorithms nor to establish which of the various special cases of AISLE is preferable. Indeed, while we compare all algorithms discussed in this work empirically on Gaussian models in the supplementary materials, we refer the reader to Tucker et al. (2019) ; Le et al. (2019) for an extensive empirical comparisons of all the algorithms discussed in this work. Notation. We repeatedly employ the shorthand p(f ) : To keep the notation concise, we hereafter suppress dependence on the observation x, i.e. we write q \u03c6 (z) := q \u03c6,x (z) as well as where \u03b3 \u03b8 (z) := p \u03b8 (z, x) and where 2 Background The expectation q \u03c6 (f ) of a test function f : Z \u2192 R can be unbiasedly estimated by the \u03c6 , which are independent and identically distributed (IID) according to q \u03c6 . Similarly, expectations of the type \u03c0 \u03b8 (f ) can be approximated by the self-normalised importance sampling estimat\u00ea The notation \u03c6, z stresses the dependence of this estimator on \u03c6 and z. The quantity w \u03c8 (z k ) are called the kth importance weight and s w k \u03c8 is its self-normalised version. For readability, we have dropped the dependence of s w k \u03c8 on z \u2208 Z K from the notation. Remark 2. The self-normalised estimate\u03c0 \u03b8 \u03c6, z (f ) is typically not unbiased. Under mild assumptions (e.g. if sup w \u03c8 < \u221e), its bias vanishes at rate O(K \u22121 ), its standard deviation vanishes at Monte-Carlo rate Objective. The importance weighted autoencoder (IWAE), introduced by Burda et al. (2016) , seeks to find a value \u03b8 of the generative-model parameters \u03b8 which maximises a lower bound L K \u03c8 on the log-marginal likelihood ('evidence'). This bound depends on the inference-network parameters \u03c6 and the number of samples, K \u2265 1: where the expectation is w.r.t. z \u223c q \u2297K \u03c6 . For any finite K, optimisation of the inferencenetwork parameters \u03c6 tightens the evidence bound. Burda et al. (2016) prove that for any \u03c6 we have that L K \u03c8 \u2191 log Z \u03b8 as K \u2192 \u221e. If K = 1, the IWAE reduces to the variational autoencoder (VAE) from Kingma & Welling (2014) . However, for K > 1, as pointed out in Cremer et al. (2017) ; Domke & Sheldon (2018) , the IWAE also constitutes another VAE on an extended space based on an auxiliary-variable construction developed in Andrieu & Roberts (2009) ; Andrieu et al. (2010) ; Lee (2011 ) (see, e.g. Finke, 2015 , for a review). The gradient of the IWAE objective from (1): The intractable quantity E G \u03c8 (z) can be approximated unbiasedly via a vanilla Monte Carlo approach using a single ( Unfortunately, this approximation typically has such a large variance that it becomes impracticably noisy (Paisley et al., 2012) . To remove this high-variance term, the well known reparametrisation trick (Kingma & Welling, 2014 ) is usually employed. It requires the following assumption. (R1) There exists a distribution q on some space E and a family of differentiable mappings , the gradient can be expressed as Here, the notation \u03c8 indicates that one does not differentiate w \u03c8 w.r.t. \u03c8. The IWAE then uses a vanilla Monte Carlo estimate of (2), Before proceeding, we state the following lemma, proved in Tucker et al. (2019, Section 8.1), which generalises of the well-known identity q \u03c6 (\u2207 \u03c6 log q \u03c6 ) = 0. Lemma 1 (Tucker et al. (2019)). Under R1, for suitably integrable f \u03c8 : Z \u2192 R, we have We now exclusively focus on the \u03c6-portion of the IWAE gradient, \u2207 iwae \u03c6 \u03b8, z . Remark 3 (drawbacks of the IWAE \u03c6-gradient). The gradient \u2207 iwae \u03c6 \u03b8, z has three drawbacks. The last two of these are attributable to the 'score-function' terms \u2207 \u03c6 log q \u03c6 (z) in the \u03c6-gradient portion of (3). \u2022 Reliance on reparametrisations. A reparametrisation \u00e0 la R1 is necessary to remove the high-variance term G \u03c8 (z). For, e.g. discrete, models that violate R1, control-variate approaches (Mnih & Rezende, 2016) or continuous relaxations have been proposed but these incur additional implementation, tuning and computation costs whilst not necessarily reducing the variance (Le et al., 2019). \u2022 Vanishing signal-to-noise ratio. The \u03c6-gradient breaks down in the sense that its signal-to-noise ratio vanishes as Rainforth et al., 2018) . This is because \u2207 iwae \u03c6 \u03b8, z constitutes a self-normalised importance-sampling approximation of \u03c0 \u03b8 ( \u03c8 \u2212 \u2207 \u03c6 log q \u03c6 ) = 0, an identity which directly follows from Lemma 1 with f \u03c8 = w \u03c8 . \u2022 Inability to achieve zero variance. As pointed out in Roeder et al. (2017) , Two modifications of \u2207 iwae \u03c6 \u03b8, z have been proposed which (under R1) avoid the scorefunction terms in (3) and hence (a) exhibit a stable signal-to-noise ratio as K \u2192 \u221e and (b) can achieve zero variance if q \u03c6 = \u03c0 \u03b8 (because then \u03c8 \u2261 0 since w \u03c8 is constant). \u2022 IWAE-STL. The 'sticking-the-landing' IWAE (IWAE-STL) gradient proposed by Roeder et al. (2017) heuristically ignores the score function terms, As shown in Tucker et al. (2019)), this introduces an additional bias whenever K > 1. \u2022 IWAE-DREG. The 'doubly-reparametrised' IWAE (IWAE-DREG) gradient proposed by Tucker et al. (2019) removes the score-function terms through Lemma 1, The quantities \u2207 iwae-dreg \u03c6 \u03b8, z and \u2207 iwae \u03c6 \u03c6, z are equal in expectation. The reweighted wake-sleep (RWS) algorithm was proposed in Bornschein & Bengio (2015) . The \u03b8-and \u03c6-gradients read These quantities are usually intractable and therefore approximated by replacing \u03c0 \u03b8 by the self-normalised importance sampling approximation\u03c0 \u03b8 \u03c6, z (this does not require R1): Since (7) relies on self-normalised importance sampling, Remark 2 shows that its bias relative to (6) is of order O(1/K). Appendix A discusses the impact of this bias on the \u03c6-gradient in more detail. The optimisation of both \u03b8 and \u03c6 is carried out simultaneously, allowing both gradients to share the same particles and weights. Nonetheless, the lack of a joint objective (for both \u03b8 and \u03c6) is often viewed as the main drawback of RWS. rws \u03c6 \u03b8, z in expectation and is derived by applying Lemma 1 to the latter. It reads where the function F(w) := w(1 \u2212 w) is used to transform the self-normalised importance weights s w k \u03c8 . In high-dimensional settings, it is typically the case that the ordered selfnormalised importance weights s w are then mainly supported on the two particles with the largest self-normalised weights. and \u03c6 simultaneously is that (a) Monte Carlo samples used to approximate the \u03b8-gradient can be re-used to approximate the \u03c6-gradient and (b) optimising \u03c6 typically reduces the error (both in terms of bias and variance) of the \u03b8-gradient approximation. However, adapting the proposal distribution q \u03c6 in importance-sampling schemes need not necessarily be based on minimising the (inclusive) KL-divergence. Numerous other techniques exist in the literature (e.g. Geweke, 1989; Evans, 1991; Oh & Berger, 1992; Richard & Zhang, 2007; Cornebise et al., 2008) and may sometimes be preferable. Indeed, another popular approach with strong theoretical support is based on minimising the \u03c7 2 -divergence (see, e.g., Deniz Akyildiz & M\u00edguez, 2019). Based on this insight, we slightly generalise the RWS-objective as \u03b8 := arg max \u03b8 log Z \u03b8 , \u03c6 := arg min \u03c6 D\u0192(\u03c0\u03b8 q \u03c6 ). Here, D\u0192(p q) := Z \u0192(p(z)/q(z))q(z) dz is some \u0192-divergence from p to q. We reiterate that alternative approaches for optimising \u03c6 (which do not minimise \u0192-divergences) could be used. However, we state (9) for concreteness as it suffices for the remainder of this work; we call the resulting algorithm adaptive importance sampling for learning (AISLE). As will become clear below, this unified framework permits a straightforward and principled derivation of robust \u03c6-gradient estimators that do not degenerate as K \u2192 \u221e. Optimisation is again performed via a stochastic gradient-ascent. The intractable \u03b8-gradient The \u03b8-gradient is thus the same for all algorithms discussed in this work although the IWAEparadigm views it as an unbiased gradient of a (biased) lower-bound to the evidence, while AISLE (and RWS) interpret it as a self-normalised importance-sampling (and consequently biased) approximation of the gradient \u2207 \u03b8 log Z \u03b8 for the 'exact' objective. In the derivations to follow, integrals of the form \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ) naturally appear. These can also be expressed as Z Approximating the expectation as well as the normalising constant Z \u03b8 on the r.h.s. with the vanilla Monte Carlo method with Remark 2 shows that this approximation has a bias of order O(K \u22121 ) and a standarddeviation of order O(K \u22121/2 ). Now, most of the \u0192-divergences used for variational inference in intractable models are such that there exists a functionf : for an exponent \u03ba \u2208 R and constant C(\u03b8) independent of \u03c6. In other words, for a given value of \u03b8, the optimization of the \u0192-divergence as a function of \u03c6 can be carried out without relying on the knowledge of Z \u03b8 . Writing g(y) :=f (y) \u2212f (y)/y, simple algebra then directly shows that Since the integral in (11) is an expectation with respect to \u03c0 \u03b8 , it can be approximated with selfimportance sampling, possibly multiplied an additional importance-sampling approximation Z \u03b8 \u03c6, z of Z \u03b8 raised to some power. This leads to, Indeed, Equation (10) applies to (11), leading to the reparametrised estimator where h(y) = g(y)y and g : R \u2192 R given immediately above (11). We now describe several particular cases. We have KL( In that case, with the notations of Section 3.3.1, we have g(y) = 1 and h (y) = 1. \u2022 AISLE-KL-NOREP/RWS. Without relying on any reparametrisation, Equation (12) yields the following gradient, which clearly equals \u2207 rws \u03c6 \u03b8, z : \u2022 AISLE-KL. Using reparametrisation, Equation (13) yields the gradient: We thus arrive at the following result which demonstrates that IWAE-STL can be derived in a principled manner from AISLE, i.e. without the need for a multi-sample objective. \u03b8, z . Proposition 1 is notable because it shows that IWAE-STL (which avoids the breakdown highlighted in Rainforth et al. (2018) and which can also achieve zero variance) can be derived in a principled manner from AISLE, i.e. without relying on a multi-sample objective. Proposition 1 thus provides a theoretical basis for IWAE-STL which was previously viewed as an alternative gradient for IWAE for which it is biased and only heuristically justified. Furthermore, the fact that IWAE-STL exhibited good empirical performance in Tucker et al. (2019) even in an example in which RWS broke down, suggests that this breakdown may not be due to RWS' lack of optimising a joint objective as previously conjectured. (8) by first replacing the exact (but intractable) \u03c6-gradient by the self-normalised importance-sampling approximation \u2207 rws \u03c6 \u03b8, z and then applying the identity from Lemma 1. Note that this may result in a variance reduction but does not change the bias of the gradient estimator. In contrast, AISLE-KL is derived by first applying Lemma 1 to the exact (RWS) \u03c6-gradient and then approximating the resulting expression. This can potentially reduce both bias and variance. Up to some irrelevant additive constant, the \u03b1-divergence between two distributions p and q is given by Z (p(z)/q(z)) \u03b1 q(z) dz for some \u03b1 > 1. This can also be expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz with \u03ba = \u2212\u03b1 andf (y) = y \u03b1 . In this case, with the notation from Section 3.3.1, we have g(y) = (\u03b1 \u2212 1)y \u03b1\u22121 and h (y) = \u03b1(\u03b1 \u2212 1) y \u03b1\u22121 . Note that the case \u03b1 = 2 is equivalent, up to an irrelevant additive constant, to a standard \u03c7 2 -divergence. Minimising this divergence is natural in importance sampling since \u03c7 2 (\u03c0 \u03b8 q \u03c6 ) = var z\u223cq \u03c6 [w \u03c8 /Z \u03b8 ] is the variance of the importance weights. \u2022 AISLE-\u03b1-NOREP. Without relying on any reparametrisation, Equation (13) yields with the following special case which is also proportional to the 'score gradient' from Dieng et al. (2017, Appendix G): \u2022 AISLE-\u03b1. Using reparametrisation, Equation (12) becomes again with the special case \u2207 This demonstrates that IWAE-DREG can be derived (up to the proportionality factor 2K) in a principled manner from AISLE, i.e. without the need for a multi-sample objective. \u03b8, z . Note that if the implementation normalises the gradients, e.g. as effectively done by ADAM (Kingma & Ba, 2015) , the constant factor cancels out and AISLE-\u03c7 2 becomes equivalent to IWAE-DREG. Otherwise (e.g. in plain stochastic gradient-ascent) this shows that the learning rate needs to be scaled as O(K) for the IWAE or IWAE-DREG \u03c6-gradients. For the 'exclusive' KL-divergence, we have KL(q \u03c6 \u03c0 \u03b8 ) = f (w \u03c8 (z))q \u03c6 (z) dz + C(\u03b8) with f (y) = log(y). In that case, with the notation from Section 3.3.1, we have h (y) = 1/y. This directly leads to the following approximation, This can be recognised as a simple average over K independent replicates of the 'stickingthe-landing' estimator for VAEs proposed in Roeder et al. (2017, Equation 8 ). As we discuss in Appendix A, optimising this 'exclusive' KL-divergence can sometimes lead to faster convergence of \u03c6 than optimising the 'inclusive' KL-divergence KL(\u03c0 \u03b8 q \u03c6 ). However, care must be taken because minimising the exclusive divergence does not necessarily lead to well behaved or even well-defined importance weights and thus can negatively affect learning of \u03b8 (whose gradient is a self-normalised importance-sampling approximation which makes use of those weights). We have shown that the adaptive-importance sampling paradigm of the reweighted wake-sleep (RWS) (Bornschein & Bengio, 2015) is preferable to the multi-sample objective paradigm of importance weighted autoencoders (IWAEs) (Burda et al., 2016) because the former achieves all the goals of the latter whilst avoiding its drawbacks. A On the r\u00f4le of the self-normalisation bias within RWS/AISLE Within the self-normalised importance-sampling approximation, the number of particles, K, interpolates between two extremes: \u2022 As K \u2191 \u221e,\u03c0 \u03b8 \u03c6, z (f ) becomes an increasingly accurate approximation of \u03c0 \u03b8 (f ). \u2022 For K = 1, however,\u03c0 \u03b8 \u03c6, z (f ) = f (z 1 ) reduces to a vanilla Monte Carlo approximation of q \u03c6 (f ) (because the single self-normalised importance weight is always equal to 1). This leads to the following insight about the estimators \u2207 aisle-kl \u03c6 \u03b8, z and \u2207 aisle-\u03c7 2 \u03c6 \u03b8, z . \u2022 As K \u2191 \u221e, these two estimators become increasingly accurate approxi- , respectively. \u2022 For K = 1, however, these two estimators reduce to vanilla Monte Carlo ap- This is similar to the standard IWAE \u03c6-gradient which also represents a vanilla Monte Carlo approximation of \u2212\u2207 \u03c6 KL(q \u03c6 \u03c0 \u03b8 ) if K = 1 as IWAE reduces to a VAE in this case. Characterising the small-K self-normalisation bias of the reparametrisation-free AISLE \u03c6 gradients, AISLE-KL-NOREP and AISLE-\u03c7 2 -NOREP, is more difficult because if K = 1, they constitute vanilla Monte Carlo approximations of q \u03c6 (\u2207 \u03c6 log q \u03c6 ) = 0. Nonetheless, Le et al. (2019, Figure 5 ) lends some support to the hypothesis that the small-K self-normalisation bias of these gradients also favours a minimisation of the exclusive KL-divergence. Recall that the main motivation for use of IWAEs (instead of VAEs) was the idea that we could use self-normalised importance-sampling approximations with K > 1 particles to reduce the bias of the \u03b8-gradient relative to \u2207 \u03b8 log Z \u03b8 . The error of such (self-normalised) importance-sampling approximations can be controlled by ensuring that q \u03c6 is close to \u03c0 \u03b8 (in some suitable sense) in any part of the space Z in which \u03c0 \u03b8 has positive probability mass. For instance, it is well known that the error will be small if the 'inclusive' KL-divergence KL(\u03c0 \u03b8 q \u03c6 ) is small as this implies well-behaved importance weights. In contrast, a small 'exclusive' KL-divergence KL(q \u03c6 \u03c0 \u03b8 ) is not sufficient for well-behaved importance weights because the latter only ensures that q \u03c6 is close to \u03c0 \u03b8 in those parts of the space Z in which q \u03c6 has positive probability mass. Let Q := {q \u03c6 } (which is indexed by \u03c6) be the family of proposal distributions/the variational family. Then we can distinguish two scenarios. 1. Sufficiently expressive Q. For the moment, assume that the family Q is flexible ('expressive') enough in the sense that it contains a distribution q \u03c6 which is (at least approximately) equal to \u03c0 \u03b8 and that our optimiser can reach the value \u03c6 of \u03c6. In this case, minimising the exclusive KL-divergence can still yield well-behaved importance weights because in this case, \u03c6 := arg min \u03c6 KL(\u03c0 \u03b8 q \u03c6 ) is (at least approximately) equal to arg min \u03c6 KL(q \u03c6 \u03c0 \u03b8 ). 2. Insufficiently expressive Q. In general, the family Q is not flexible enough in the sense that all of its members are 'far away' from \u03c0 \u03b8 , e.g. if the is fully factorised. In this case, minimising the exclusive KL-divergence could lead to poorly-behaved importance weights and we should optimise \u03c6 := arg min \u03c6 KL(\u03c0 \u03b8 q \u03c6 ) as discussed above. Remark 4. In Scenario 1 above, i.e. for a sufficiently flexible Q, using a gradient-descent algorithm which seeks to minimise the exclusive divergence can sometimes be preferable to a gradient-descent algorithm which seeks to minimise the inclusive divergence. This is because both find (approximately) the same optimum but the latter may exhibit faster convergence in some applications. In such scenarios, the discussion in Subsection A.1 indicates that a smaller number of particles, K, could then be preferable for some of the \u03c6-gradients because (a) the O(K \u22121 ) self-normalisation bias outweighs the O(K \u22121/2 ) standard deviation and (b) the direction of this bias may favour faster convergence. Unfortunately, simply setting K = 1 for the approximation of the \u03c6-gradients 2 is not necessarily optimal because \u2022 even in the somewhat idealised scenario 1 above and even if the direction of the self-normalisation bias encourages faster convergence, increasing K is still desirable to reduce the variance of the gradient approximations and furthermore, even in this scenario, seeking to optimise the exclusive KL-divergence could lead to poorly behaved importance-sampling approximations of the \u03b8-gradient whenever \u03c6 is still far away from optimal; \u2022 not using the information contained in all K particles and weights (which have already been sampled/calculated to approximate the \u03b8-gradient) seems wasteful; \u2022 if K = 1, the reparametrisation-free AISLE \u03c6-gradients, AISLE-KL-NOREP and AISLE-\u03c7 2 -NOREP are simply vanilla Monte Carlo estimates of 0 and the RWS-DREG \u03c6-gradient is then equal to 0. In these supplementary materials, we illustrate the different \u03c6-gradient estimators (recall that all algorithms discussed in this work share the same \u03b8-gradient estimator). Specifically, we compare the following approximations. \u2022 AISLE-KL-NOREP. The gradient for AISLE based on the KL-divergence without any further reparametrisation from (14) i.e. this coincides with the standard RWSgradient from (7). This gradient does not require R1 but does not achieve zero variance even if q \u03c6 = \u03c0 \u03b8 . \u2022 AISLE-KL. The gradient for AISLE based on the KL-divergence after reparametrising and exploiting the identity from Lemma 1; it is given by (15) and coincides with the IWAE-STL-gradient from (4). \u2022 AISLE-\u03c7 2 -NOREP. The gradient for AISLE based on the \u03c7 2 -divergence without any reparametrisation given in (16). This gradient again does not require R1 but does not achieve zero variance even if q \u03c6 = \u03c0 \u03b8 . \u2022 AISLE-\u03c7 2 . The gradient for AISLE based on the \u03c7 2 -divergence after reparametrising and exploiting the identity from Lemma 1; it is given by (17) and is alsow proportional to IWAE-DREG from Tucker et al. (2019) which was stated in (5). When normalising the gradients (as, e.g. implicitly done by optimisers such as ADAM Kingma & Ba, 2015) the proportionality constant cancels out so that both these gradient approximations lead to computationally the same algorithm. \u2022 IWAE. The gradient for IWAE employing the reparametrisation trick from Kingma & Welling (2014) . Its sampling approximation is given in (3). Recall that this is the \u03c6-gradient whose signal-to-noise ratio degenerates with K as pointed out in Rainforth et al. (2018) (and which also cannot achieve zero variance even if q \u03c6 = \u03c0 \u03b8 ). \u2022 IWAE-DREG. The 'doubly-reparametrised' IWAE gradient from (5) which was proposed in Tucker et al. (2019) . It is proportional to AISLE-\u03c7 2 . \u2022 RWS-DREG. The 'doubly-reparametrised' RWS \u03c6-gradient from (8) Hereafter, wherever necessary, we add an additional subscript to make the dependence on the observations explicit. The joint law (the 'generative model'), parametrised by \u03b8, of the observations and latent variables then factorises as We model each latent variable-observation pair (z, x) as ..,D} \u2208 R D\u00d7D is assumed to be known and where I denotes the D \u00d7 D-identity matrix. For any \u03b8, with P := (\u03a3 \u22121 + I) \u22121 and \u03bd \u03b8,x := P (\u03a3 \u22121 \u00b5 + x). In particular, (18) implies that Proposal/variational approximation. We take the proposal distributions as a fullyfactored Gaussian: where . The parameters to optimise are thus where denotes the column vector formed by the elements in the dth row of A. Furthermore, for the reparametrisation trick, we take q( ) := N( ; 0, I), where 0 \u2208 R D is a vector whose elements are all 0, so that Note that the mean of the proposal in (20) coincides with the mean of the posterior in (19) if A = P and b This model is similar to the one used as a benchmark in Rainforth et al. (2018, Section 4) and also in Tucker et al. (2019, Section 6.1) who specified both the generative model and the variational approximation to be isotropic Gaussians. Specifically, their setting can be recovered by taking \u03a3 := I and fixing c d = log(2/3)/2 so that C = 2 3 I throughout. Here, in order to investigate a slightly more realistic scenario, we also allow for the components of the latent vectors z to be correlated/dependent under the generative model. However, as the variational approximation remains restricted to being fully factored, it may fail to fully capture the uncertainty about the latent variables. \u03c6,x (z), we then have Note that the only source of randomness in this expression is the multivariate normal random variable . Thus, by (23) and (24), for any values of A and b and any K \u2265 1, the variance of the A-and b-gradient portion of AISLE-KL/IWAE-STL and AISLE-\u03c7 2 /IWAE-DREG goes to zero as C \u2192 C = 1 2 I. In other words, in this model, these 'score-function free' \u03c6-gradients achieve (near) zero variance for the parameters governing the proposal mean as soon as the variance-parameters fall within a neighbourhood of their optimal values. Furthermore, (25) combined with (26) shows that for any K \u2265 1, the variance of the C-gradient portion also goes to zero as (A, b, C) \u2192 (A , b , C ) . A more thorough analysis of the benefits of reparametrisation-trick gradients in Gaussian settings is carried out in Xu et al. (2019) . Setup. We end this section by empirically comparing the algorithms from Subsection B.1. We run each of these algorithms for a varying number of particles, K \u2208 {1, 10, 100}, and varying model dimensions, D \u2208 {2, 5, 10}. Each of these configurations is repeated independently 100 times. Each time using a new synthetic data set consisting of N = 25 observations sampled from the generative model after generating a new 'true' prior mean vector as \u00b5 \u223c N(0, I). Since all the algorithms share the same \u03b8-gradient, we focus only on the optimisation of \u03c6 and thus simply fix \u03b8 := \u03b8 ml throughout. We show results for the following model settings. \u2022 Figure 1 . The generative model is specified via \u03a3 = I. In this case, there exists a value \u03c6 of \u03c6 such that q \u03c6,x (z) = \u03c0 \u03b8,x (z). Note that this corresponds to Scenario 1 in Subsection A.2. \u2022 Figure 2 . The generative model is specified via \u03a3 = (0.95 |d\u2212d |+1 ) (d,d )\u2208{1,...,D} 2 . Note that in this case, the fully-factored variational approximation cannot fully mimic the dependence structure of the latent variables under the generative model. That is, in this case, q \u03c6,x (z) = \u03c0 \u03b8,x (z) for any values of \u03c6. Note that this corresponds to Scenario 2 in Subsection A.2. To initialise the gradient-ascent algorithm, we draw each component of the initial values \u03c6 0 of \u03c6 IID according to a standard normal distribution. We use both plain stochastic gradient-ascent with the gradients normalised to have unit L 1 -norm (Figures 1a, 2a) and ADAM (Kingma & Ba, 2015) with default parameter values (Figures 1b, The total number of iterations is 10, 000; in each case, the learning-rate parameters at the ith step are i \u22121/2 . Figure 1 except that here, the covariance matrix \u03a3 = (0.95 |d\u2212e|+1 ) (d,e)\u2208{1,...,D} 2 is not a diagonal matrix. Again, note the logarithmic scaling on the second axis."
}