{
    "title": "SJahqJZAW",
    "content": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.   Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator. Generative adversarial networks (GANs), introduced by BID12 , endow neural networks with the ability to express distributional outputs. The framework includes a generator network that is tasked with producing samples from some target distribution, given as input a (typically low dimensional) noise vector drawn from a simple known distribution, and possibly conditional side information. The generator learns to generate such samples, not by directly looking at the data, but through adversarial training with a discriminator network that seeks to differentiate real data from those generated by the generator. To satisfy the objective of \"fooling\" the discriminator, the generator eventually learns to produce samples with statistics that match those of real data. In regression tasks where the true output is ambiguous, GANs provide a means to simply produce an output that is plausible (with a single sample), or to explicitly model that ambiguity (through multiple samples). In the latter case, they provide an attractive alternative to fitting distributions to parametric forms during training, and employing expensive sampling techniques at the test time. In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting BID7 , and super-resolution BID16 . Recently, BID13 demonstrated that GANs can be used to produce plausible mappings between a variety of domains-including sketches and photographs, maps and aerial views, segmentation masks and images, etc. GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks BID7 .Despite their success, training GANs to generate high-dimensional data (such as large images) is challenging. Adversarial training between the generator and discriminator involves optimizing a min-max objective. This is typically carried out by gradient-based updates to both networks, and the generator is prone to divergence and mode-collapse as the discriminator begins to successfully distinguish real data from generated samples with high confidence. Researchers have tried to address this instability and train better generators Figure 1 : Overview of our approach. We train a single generator against an array of discriminators, each of which receives lower-dimensional projections-chosen randomly prior to training-as input. Individually, these discriminators are unable to perfectly separate real and generated samples, and thus provide stable gradients to the generator throughout training. In turn, by trying to fool all the discriminators simultaneously, the generator learns to match the true full data distribution. through several techniques. BID8 proposed generating an image by explicitly factorizing the task into a sequence of conditional generations of levels of a Laplacian pyramid, while demonstrated that specific architecture choices and parameter settings led to higher-quality samples. Other techniques include providing additional supervision BID20 , adding noise to the discriminator input BID0 , as well as modifying or adding regularization to the training objective functions BID18 BID1 BID23 .We propose a different approach to address this instability, where the generator is trained against an array of discriminators, each of which looks at a different, randomly-chosen, low-dimensional projection of the data. Each discriminator is unable to perfectly separate real and generated samples since it only gets a partial view of these samples. At the same time, to satisfy its objective of fooling all discriminators, the generator learns to match the true full data distribution. We describe a realization of this approach for training image generators, and discuss the intuition behind why we expect this training to be stable-i.e., the gradients to the generator to be meaningful throughout training-and consistent-i.e., for the generator to learn to match the full real data distribution. Despite its simplicity, we find this approach to be surprisingly effective in practice. We demonstrate this efficacy by using it to train generators on standard image datasets, and find that these produce higher-quality samples than generators trained against a single discriminator. Researchers have explored several approaches to improve the stability of GANs for training on higher dimensional images. Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN BID23 and Wasserstein GAN BID1 show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity. BID18 further extended GAN training to any choice of f -divergence as objective. BID20 proposed several heuristics to improve the stability of training. These include modifying the objective function, virtual batch-normalization, historical averaging of parameters, semi-supervised learning, etc.. All of these methods are designed to improve the quality of gradients, and provide additional supervision to the generator. They are therefore complementary to, and can likely be used in combination with, our approach. Note that prior methods have involved training ensembles of GANs , or ensembles of discriminators BID10 . However, their goal is different from ours. In our framework, each discriminator is shown only a low-dimensional projection of the data, with the goal of preventing it from being able to perfectly reject generated samples. Crucially, we do not combine the outputs of discriminators directly, but rather compute losses on individual discriminators and average these losses. Indeed, the idea of combining multiple simple classifiers, e.g., with boosting BID11 , or mixture-of-experts Jacobs (1995), has a rich history in machine learning, including recent work where the simple classifiers act on random lower-dimensional projections of their inputs BID3 ) (additionally benefiting from the regularization effect of such projections BID9 ). While these combinations have been aimed at achieving high classification accuracy (i.e., achieve accurate discrimination), our objective is to maintain a flow of gradients from individual discriminators to the generator. It is also worth noting here the work of BID2 , who also use low-dimensional projections to deal with high-dimensional probability distributions. They use such projections to efficiently compute Wasserstein distances and optimal transport between two distributions, while we seek to enable stable GAN training in high dimensions. GANs BID12 traditionally comprise of a generator G that learns to generate samples from a data distribution P x , through adversarial training against a single discriminator D as: DISPLAYFORM0 DISPLAYFORM1 , and with P z a fixed distribution (typically uniform or Gaussian) for noise vectors DISPLAYFORM2 The optimization in (1) is carried out using stochastic gradient descent (SGD), with alternating updates to the generator and the discriminator. While this approach works surprisingly well, instability is common during training, especially when generating high-dimensional data. Theoretically, the optimal stationary point for (1) is one where the generator matches the data distribution perfectly, and the discriminator is forced to always output 0.5 BID12 . But usually in practice, the discriminator tends to \"win\" and the cost in (1) saturates at a high value. While the loss keeps increasing, the gradients received by the generator are informative during the early stages of training. However, once the loss reaches a high enough value, the gradients are dominated by noise, at which point generator quality stops improving and in fact begins to deteriorate. Training must therefore be stopped early by manually inspecting sample quality, and this caps the number of iterations over which the generator is able to improve. BID0 discuss one possible source of this instability, suggesting that it is because natural data distributions P x often have very limited support in the ambient domain of x and G(z). Then, the generator G is unable to learn quickly enough to generate samples from distributions that have significant overlap with this support. This in turn makes it easier for the discriminator D to perfectly separate the generator's samples, at which point the latter is left without useful gradients for further training. We propose to ameliorate this problem by training a single generator against an array of discriminators. Each discriminator operates on a different low-dimensional linear projection, that is set randomly prior to training. Formally, we train a generator G against multiple discriminators {D k } K k=1 as: DISPLAYFORM3 where W k , k \u2208 {1, \u00b7 \u00b7 \u00b7 , K} is a randomly chosen matrix in R d\u00d7m with m < d. Therefore, instead of a single discriminator looking at the full input (be it real or fake) in (1), each of the K discriminators in (2) sees a different low-dimensional projection. Each discriminator tries to maximize its accuracy at detecting generated samples from its own projected version, while the generator tries to ensure that each sample it generates simultaneously fools all discriminators for all projected versions of that sample. When the true data X and generated samples G(z) are images, prior work has shown that it is key that both the generator and discriminator have convolutional architectures. While individual discriminators in our framework see projected inputs that are lower-dimensional than the full image, their dimensionality is still large enough (a very small m would require a large number of discriminators) to make it hard to train discriminators with only fully-connected layers. Therefore, it is desirable to employ convolutional architectures for each discriminator. To do so, the projection matrices W T k must be chosen to produce \"image-like\" data. Accordingly, we use strided convolutions with random filters to embody the projections W T k (as illustrated in Fig. 1 ). The elements of these convolution filters are drawn i.i.d. from a Gaussian distribution, and the filters are then scaled to have unit \u2113 2 norm. To promote more \"mixing\" of the input coordinates, we choose filter sizes that are larger than the stride (e.g., we use 8 \u00d7 8 filters when using a stride of 2). While still random, this essentially imposes a block-Toeplitz structure on W T k . In this section, we motivate our approach, and provide the reader with some intuition for why we expect our approach to improve the stability of training while maintaining consistency. Stability. Each discriminators D k in our framework in (2) works with low dimensional projections of the data and can do no better than the traditional full discriminator D in (1). Let y \u2208 R d and l \u2208 {0, 1} denote the input and ideal output of the original discriminator D, where l = 1 if y is real, and 0 if generated. However, each D k is provided only a lower dimensional projection W When the discriminators D k are not perfectly saturated, D k (W T k G(z)) will have some variation in the neighborhood of a generated sample G(z), which can provide meaningful gradients to the generator. But note that gradients from each individual discriminator D k to G(z) will lie entirely in the lower dimensional sub-space spanned by W T k . Consistency. While impeding the discriminator's ability benefits stability, it could also have trivially been achieved by other means-e.g., by severely limiting its architecture or excessive regularization. However, these would also limit the ability of the discriminator to encode the true data distribution and pass it on to the generator. We begin by considering the following modified version of Theorem 1 in BID12 : Theorem 3.1. Let P g denote the distribution of the generator outputs G(z), where z \u223c P z , and let P W T k g be the marginals of P g along W k . For fixed G, the optimal {D k } are given by DISPLAYFORM0 DISPLAYFORM1 Figure 2: Training Stability. We plot the evolution of the \"generator loss\" across trainingagainst a traditional single discriminator (DC-GAN), and the average and individual losses against multiple discriminators (K = 48) in our setting. For the traditional single discriminator, this loss rises quickly to high value, indicating that the discriminator saturates to rejecting generated samples with very high confidence. In contrast, the loss in our case remains lower, allowing our discriminators to provide meaningful gradients to the generator. Figure 3: Evolution of sample quality across training iterations. With our approach, the generator improves the visual quality of its samples quickly and throughout training. Meanwhile, the generator trained in the traditional setting with a single discriminator shows slower improvement, and indeed quality begins to deteriorate after a point. This result, proved in the supplementary, implies that under ideal conditions the generator will produce samples from a distribution whose marginals along each of the projections W k match those of the true distribution. Thus, each discriminator adds an additional constraint on the generator, forcing it to match P x along a different marginal. As we show in the supplementary, matching along a sufficiently high number of such marginals-under smoothness assumptions on the true and generator distributions P x and P g -guarantees that the full joint distributions will be close (note it isn't sufficient for the set of projection matrices {W k } to simply span R d for this). Therefore, even though viewing the data through random projections limits the ability of individual discriminators, with enough discriminators acting in concert, the generator learns to match the full joint distribution of real data in R d . We now evaluate our approach with experiments comparing it to generators trained against a single traditional discriminator, and demonstrate that it leads to higher stability during training, and ultimately yields generators that produce higher quality samples. Dataset and Architectures. For evaluation, we primarily use the dataset of celebrity faces collected by BID17 -we use the cropped and aligned 64 \u00d7 64-size version of the images-and the DC-GAN architectures for the generator and discriminator. We make two minor modifications to the DC-GAN implementation that we find empirically to yield improved results (for both the standard single discriminator setting, as well as our multiple discriminator setting). First, we use different batches of generated Figure 4 : Random sets of generated samples from traditional DC-GAN and the proposed framework. For DC-GAN, we show results from the model both at 40k iterations (when the samples are qualitatively the best) and at the end of training (100k iterations). For our setting, we show samples from the end of training for generator models trained with K = 12, 24, 48 projections. Our generator produces qualitatively better samples, with finer detail and fewer distortions. Quality is worse with subtle high-frequency noise when K is smaller, but these decrease with increasing K to 24 and 48.images to update the discriminator and generator-this increases computation per iteration, but yields better quality results. Second, we employ batch normalization in the generator but not in the discriminator (the original implementation normalized real and generated batches separately, which we found yielded poorer generators).For our approach, we train a generator against K discriminators, each operating on a different single-channel 32 \u00d7 32 projected version of the input, i.e., d/m = 12. The projected images are generated through convolution with 8 \u00d7 8 filters and a stride of two. The filters are generated randomly and kept constant throughout training. We compare this to the standard DC-GAN setting of a single discriminator that looks at the full-resolution 64 \u00d7 64 color image. We use identical generator architectures in both settings-that map a 100 dimensional uniformly distributed noise vector to a full resolution image. The discriminators also have similar architectures-but each of the discriminator in our setting has one less layer as it operates on a lower resolution input (we map the number of channels in the first layer in our setting to those of the second layer of the full-resolution single-discriminator, thus matching the size of the final feature vector used for classification). As is standard practice, we compute generator gradients in both settings with respect to minimizing the \"incorrect\" classification loss of the discriminator-in our setting, this is given by \u2212 1 K k log D k (G(z)). As suggested in , we use Adam BID15 with learning rate 2 \u00d7 10 \u22124 , \u03b2 1 = 0.5, and a batch size of 64.Stability. We begin by analyzing the evolution of generators in both settings through training. Figure 2 shows the generator training loss for traditional DC-GAN with a single discriminator, and compares it the proposed framework with K = 48 discriminators. In both Figure 5 : Interpolating in latent space. For selected pairs of generated faces (with K = 48), we generate samples using different convex combinations of their corresponding noise vectors. Every combination generates a plausible face, and these appear to smoothly interpolate between various facial attributes-age, gender, expression, hair, etc. Note that the \u03b1 = 0.5 sample always corresponds to an individual clearly distinct from the original pair.settings, the generator losses increase through much of training after decreasing in the initial iterations (i.e., the discriminators eventually \"win\"). However, DC-GAN's generator loss rises quickly and remains higher than ours in absolute terms throughout training. Figure 3 includes examples of generated samples from both generators across iterations (from the same noise vectors). We observe that DC-GAN's samples improve mostly in the initial iterations while the training loss is still low, in line with our intuition that generator gradients become less informative as discriminators get stronger. Indeed, the quality of samples from traditional DC-GAN actually begins to deteriorate after around 40k iterations. In contrast, the generator trained in our framework improves continually throughout training. Consistency. Beyond stability, Fig. 3 also demonstrates the consistency of our framework. While the average loss in our framework is lower, we see this does not impede our generator's ability to learn the data distribution quickly as it collates feedback from multiple discriminators. Indeed, our generator produces higher-quality samples than traditional DC-GAN even in early iterations. Figure 4 includes a larger number of (random) samples from generators trained with traditional DC-GAN and our setting. For DC-GAN, we include samples from both roughly the end (100k iterations) of training, as well as from roughly the middle (40k iterations) where the sample quality are approximately the best. For our approach, we show results from training with different numbers of discriminators with K = 12, 24, and 48-selecting the generator models from the end of training for all. We see that our generators produce face images with higher-quality detail and far fewer distortions than traditional DC-GAN. We also note the effect of the number of discriminators on sample quality. Specifically, we find that setting K to be equal only to the projection ratio d/m = 12 leads to subtle high-frequency noise in the generator samples, suggesting these many projections do not sufficiently constrain the generator to learn the full data distribution. Increasing K diminishes these artifacts, and K = 24 and 48 both yield similar, high-quality samples. Training Time. Note that the improved generator comes at the expense of increased computation during training. Traditional DC-GAN with a single discriminator takes only 0.6s per training iteration (on an NVIDIA Titan X), but this goes up to 3.2s for K = 12, 5.8s for K = 24, and 11.2s for K = 48 in our framework. Note that once trained, all generators produce samples at the same speed as they have identical architectures. Latent Embedding. Next, we explore the quality of the embedding induced by our generator (K = 48) in the latent space of noise vectors z. We consider selected pairs of randomly generated faces, and show samples generated by linear interpolation between their corresponding noise vectors in Fig. 5 . Each of these generated samples is also a plausible face image, which confirms that our generator is not simply memorizing training samples, and that it is densely packing the latent space with face images. We also find that the generated samples smoothly interpolate between semantically meaningful face attributesgender, age, hair-style, expression, and so on. Note that in all rows, the sample for \u03b1 = 0.5 appears to be a clearly different individual than the ones represented by \u03b1 = 0 and \u03b1 = 1.Results on Imagenet-Canines. Finally, we show results on training a generator on a subset of the Imagenet-1K database BID6 ). We use 128 \u00d7 128 crops (formed by scaling the smaller side to 128, and taking a random crop along the other dimension) of 160k images from a subset of Imagenet classes (ids 152 to 281) of different canines. We use similar settings as for faces, but feed a higher 200-dimensional noise vector to the generator, which also begins by mapping this to a feature vector that is twice as large FORMULA9 , and which has an extra transpose-convolution layer to go upto the 128 \u00d7 128 resolution. We again use 8 \u00d7 8 convolutional filters with stride two to form {W T k }-in this case, these produce 64\u00d764 single channel images. We use only K = 12 discriminators, each of which has an additional layer because of the higher-resolution-beginning with fewer channels in the first layer so that the final feature vector used for classification is the same length as for faces. FIG0 shows manually selected samples after 100k iterations of training (see supplementary material for a larger random set). We see that since it is trained on a more diverse and unaligned image content, the generated images are not globally plausible photographs. Nevertheless, we find that the produced images are sharp, and that generator learns to reproduce realistic low-level textures as well as some high-level composition. In this paper, we proposed a new framework to training GANs for high-dimensional outputs. Our approach employs multiple discriminators on random low-dimensional projections of the data to stabilize training, with enough projections to ensure that the generator learns the true data distribution. Experimental results demonstrate that this approaches leads to more stable training, with generators continuing to improve for longer to ultimately produce higher-quality samples. Source code and trained models for our implementation is available at the project page [anonymized for review].In our current framework, the number of discriminators is limited by computational cost. In future work, we plan to investigate training with a much larger set of discriminators, employing only a small subset of them at each iteration, or every set of iterations. We are also interested in using multiple discriminators with modified and regularized objectives (e.g., BID18 BID1 BID23 ). Such modifications are complementary to our approach, and deploying them together will likely be beneficial. In this section we provide additional theoretical results showing the benefits of random projections, for some particular choice of distributions, for both improving stability and guaranteeing consistency of GAN training. Here, we consider several simplifying assumptions on the distribution of x, to gain intuition as to how a random low-dimensional projection affects the \"relative\" volume of the support of P x . Let us assume that the range of x is the d-dimensional ball B d of radius 1 centered at 0. We define the support supp(P x ) \u2282 B d of a distribution P x to be the set where the density is greater than some small threshold \u01eb. Assume that the projection W \u2208 R d\u00d7m is entry-wise random Gaussian (rather than corresponding to a random convolution). Denote as B d W the projection of the range on W , and P W T x as the marginal of P x along W . Theorem A.1. Assume P x = j \u03c4 j N (x|\u00b5 j , \u03a3 j ) is a mixture of Gaussians, such that the individual components are sufficiently well separated (in the sense that there is no overlap between their supports or the projections thereof, see BID5 DISPLAYFORM0 This result implies that the projection of the support of P x , under this simplified assumptions, occupies a higher fraction of the volume of the projection of the range of x. This aids in stability because it makes it more likely that a larger fraction of the generator's samples (which also lie within the range of x) will, after projection, overlap with the projected support supp(P x ), and can not be rejected absolutely by the discriminator. Proof of Theorem A.1. We first show that we can assume that the columns of the projection W are orthonormal. Since W \u2208 R d\u00d7m is entry-wise Gaussian distributed, it has rank m with high probability. Then, there exists a square invertible matrix A such that W \u2032 = AW where W \u2032 is orthonormal. In that case, Vol(supp( DISPLAYFORM1 because the numerator and denominator terms for both can be related by det(A) for the change of variables, which cancels out. Note that under this orthonormal assumption, DISPLAYFORM2 Next, we consider the case of an individual Gaussian distribution P x = N (x|\u00b5, \u03a3), and prove that the ratio of supports (defined with respect to a threshold \u01eb) does not decrease with the projection. The expression for these ratios is given by: DISPLAYFORM3 DISPLAYFORM4 For sufficiently small \u01eb, the volume ratio of a single Gaussian will increase with projection if det(W T \u03a3W ) > det(\u03a3). Note that all eigenvalues of \u03a3 \u2264 1, with at-least one eigenvalue strictly < 1 (since supp(P x ) \u2282 B d ). First, we consider the case when \u03a3 is not strictly positive definite and one of the eigenvalues is 0. Then, Vol(supp(P x )) = 0 and Vol(supp(P W T x )) \u2265 0, i.e., the volume ratio either stays the same or increases. For the case when all eigenvalues are strictly positive, consider a co-ordinate transform where the first m co-ordinates of x correspond to the column vectors of W , such that DISPLAYFORM5 where DISPLAYFORM6 Note that det(\u03a3 W \u2032 ) \u2264 1, since all eigenvalues of \u03a3 are \u2264 1, with equality only when W is completely orthogonal to the single eigenvector whose eigenvalue is strictly < 1, which has probability zero under the distribution for W . So, we have that det(\u03a3 W \u2032 ) < 1, and DISPLAYFORM7 The above result shows that the volume ratio of individual components never decrease, and always increase when their co-variance matrices are full rank (no zero eigenvalue). Now, we consider the case of the Gaussian mixture. Note that the volume ratio of the mixture equals the sum of the ratios of individual components, since the denominator Vol(B m ) is the same, where the support volume in these ratios for component j is defined with respect to a threshold \u01eb/\u03c4 j . Also, note that since mixture distribution has non-zero volume, at least one of the Gaussian components must have all non-zero eigenvalues. So, the volume ratios of P x and P W T x are both sums of individual Gaussian component terms, and each term for P W T x is greater than or equal to the corresponding term for P x , and at least one term is strictly greater. Thus, the support volume ratio of P W T x is strictly greater than that of P x . Proof of Theorem 3.1. The proof follows along the same steps as that of Theorem 1 in BID12 . DISPLAYFORM0 For any point y \u2208 supp( DISPLAYFORM1 . D k and setting to 0 gives us: DISPLAYFORM2 .Notice we can rewrite V (D k , G) as DISPLAYFORM3 Here KL is the Kullback Leibler divergence, and it is easy to see that the above expression achieves the minimum value when DISPLAYFORM4 Next, we present a result that shows that given enough random projections K, the full distribution P g of generated samples will closely match the full true distribution P x .Def: A function f : DISPLAYFORM5 Theorem A.2. Let P x and P g be two compact distributions with support of radius B, such that P W T k x = P W T k g , \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 , K}. Let {W k } be entrywise random Gaussian matrices in R d\u00d7m . Let R = P x \u2212 P g be the residual function of the difference of densities. Then, with high probability, for any DISPLAYFORM6 where L R is the Lipschitz constant of R.This theorem captures how much two probability densities can differ if they match along K marginals of dimension m, and how the error decays with increasing number of discriminators K. In particular, if we have a smooth residual function R (with small Lipschitz constant L R ), then at any point \u2208 R d , it is constrained to have small values-smaller with increasing number of discriminators K, and higher dimension m. The dependence on L R suggests that the residual can take larger values if it is not smooth-which can result from either the true density P x or the generator density P g , or both, being not smooth. Again, this result gives us rough intuition about how we may expect the error between the true and generator distribution to change with changes to the values of K and m. In practice, the empirical performance will depend on the nature of the true data distribution, the parametric form / network architecture for the generator, and the ability of the optimization algorithm to get near the true optimum in Theorem 3.1.Proof of Theorem A.2. Let R = P x \u2212 P g , be the residual function that captures the difference between the two distributions. R satisfies the following properties: DISPLAYFORM7 and for any set S, x\u2208S R(x)dx \u2264 x\u2208S P x (x)dx \u2264 1.Further, since both the distributions have same marginals along k different directions {W k }, we have, for any x, R W T k y (x) = y|x=W T k y R(y) = 0.We first prove this result for discrete distributions supported on a compact set S with \u03b3 points along each dimension. LetP denote such a discretization of a distribution P and R =P x \u2212P g . , if R is L R Lipschitz we know that, Finally, notice that the marginal constraints do not guarantee that the distributionsP W T k x andP W T k g match exactly on the \u01eb-net (or that R is 0), but only that they are equal upto an additive factor of \u01eb. Hence, combining this with equation 14 we get, |R(x)| = |P x (x) \u2212 P g (x)| \u2264 O(\u01eb), for any x with probability \u2265 1 \u2212 c \u00b7 m \u00b7 K \u00b7 e Face Images: Proposed Method (K = 48)"
}