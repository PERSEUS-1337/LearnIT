{
    "title": "rk3b2qxCW",
    "content": "In recent years deep reinforcement learning has been shown to be adept at solving sequential decision processes with high-dimensional state spaces such as in the Atari games. Many reinforcement learning problems, however, involve high-dimensional discrete action spaces as well as high-dimensional state spaces. In this paper, we develop a novel policy gradient methodology for the case of large multidimensional discrete action spaces. We propose two approaches for creating parameterized policies: LSTM parameterization and a Modified MDP (MMDP) giving rise to Feed-Forward Network (FFN) parameterization. Both of these approaches provide expressive models to which backpropagation can be applied for training. We then consider entropy bonus, which is typically added to the reward function to enhance exploration. In the case of high-dimensional action spaces, calculating the entropy and the gradient of the entropy requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. We develop several novel unbiased estimators for the entropy bonus and its gradient. Finally, we test our algorithms on two environments: a multi-hunter multi-rabbit grid game and a multi-agent multi-arm bandit problem. In recent years deep reinforcement learning has been shown to be adept at solving sequential decision processes with high-dimensional state spaces such as in the Go game ) and Atari games BID6 , BID7 , BID9 , BID11 , BID12 , BID22 , BID1 ). In all of these success stories, the size of the action space was relatively small. Many reinforcement learning problems, however, involve high-dimensional action spaces as well as high-dimensional state spaces. Examples include StarCraft BID21 , BID4 ), where there are many agents each of which can take a finite number of actions; and coordinating self-driving cars at an intersection, where each car can take a finite set of actions BID17 ).In this paper, we develop a novel policy gradient methodology for the case of large multidimensional action spaces. There are two major challenges in developing such a methodology:\u2022 For large multidimensional action spaces, how can we design expressive and differentiable parameterized policies which can be efficiently sampled?\u2022 In policy gradient, in order to encourage sufficient exploration, an entropy bonus term is typically added to the objective function. However, in the case of high-dimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. How can we efficiently approximate the entropy and its gradient while maintaining desirable exploration?In this paper, we first propose two approaches for parameterizing the policy: a LSTM model and a Modified MDP (MMDP) giving rise to Feed-Forward Network (FFN) model. For both of these parameterizations, actions can be efficiently sampled from the policy distribution, and backpropagation can be employed for training. We then develop several novel unbiased estimators for the entropy bonus and its gradient. These estimators can be combined with stochastic gradient descent giving a new a class of policy gradient algorithms with desirable exploration. Finally, we test our algorithms on two environments: a multi-agent multi-arm bandit problem and a multi-agent hunter-rabbit grid game. Consider an MDP with a d-dimensional action space A = A 1 \u00d7 A 2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 A d . Denote a = (a 1 , . . . , a d ) for an action in A. A policy \u03c0(\u00b7|s) specifies for each state s a distribution over the action space A. In the standard RL setting, an agent interacts with an environment over a number of discrete timesteps BID19 BID15 ). At timestep t, the agent is in state s t and samples an action a t from the policy distribution \u03c0(\u00b7|s t ). The agent then receives a scalar reward r t and the environment enters the next state s t+1 . The agent then samples a t+1 from \u03c0(\u00b7|s t+1 ) and so on. The process continues until the end of the episode, denoted by T . The return R t = T \u2212t k=0 \u03b3 k r t+k is the discounted accumulated return from time step t until the end of the episode. In the policy gradient formulation, we consider a set of parameterized policies \u03c0 \u03b8 (\u00b7|s), \u03b8 \u2208 \u0398, and attempt to find a good \u03b8 within a parameter set \u0398. Typically the policy \u03c0 \u03b8 (\u00b7|s) is generated by a neural network with \u03b8 denoting the weights and biases in the network. The parameters \u03b8 are updated by performing stochastic gradient ascent on the expected reward. One example of such an algorithm is REINFORCE, proposed by BID23 , where in a given episode at timestep t the parameters \u03b8 are updated as follows: DISPLAYFORM0 where b t (s t ) is a baseline. It is well known that the policy gradient algorithm often converges to a local optimum. To discourage convergence to a highly suboptimal policy, the policy entropy is typically added to the update rule: DISPLAYFORM1 where DISPLAYFORM2 This approach is often referred to as adding entropy bonus or entropy regularization BID23 ) and is widely used in different applications of neural networks, such as optimal control in Atari games ), multi-agent games BID5 ) and optimizer search for supervised machine learning with RL BID0 ). \u03b2 is referred to as the entropy weight. In applying policy gradient to MDP with large multidimensional action spaces, there are two challenges. First, how do we design an expressive and differentiable parameterized policy which can be efficiently sampled? Second, for the case of large multidimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space, which may be infeasible. How do we then enhance exploration in a principled way? To abbreviate the notation, we write p \u03b8 (a) for \u03c0 \u03b8 (a|s t ), with the conditioning on s t being implicit. We consider schemes whereby the sample components a i , i = 1, . . . , d, are sequentially generated. In particular, after obtaining a 1 , a 2 , . . . , a i\u22121 , we will generate a i \u2208 A i from some parameterized distribution p \u03b8 (\u00b7|a 1 , a 2 , . . . , a i\u22121 ) defined over the one-dimensional set A i . After generating the distribution p \u03b8 (\u00b7|a 1 , a 2 , . . . , a i\u22121 ), i = 1, . . . , d and the action components a 1 , . . . , a d sequentially, we can then define DISPLAYFORM0 . . , a i\u22121 ). We now propose two methods for creating the parameterized distributions p \u03b8 (a|a 1 , a 2 , . . . , a i\u22121 ), a \u2208 A i . To our knowledge, these models are novel and have not been studied in multidimensional action space literature. We assume that the size of the one-dimensional action sets are equal, that is, DISPLAYFORM1 To handle action sets of different sizes, we include inconsequential actions if needed. The policy p \u03b8 (a) can be learned with a recurrent neural network (RNN). Long Short-Term Memory (LSTM), a special flavor of RNN, has recently been used with great success to represent conditional probabilities in language translation tasks BID18 ). Here, as shown in FIG0 (a), we use an LSTM to generate a parameterized multidimensional distribution p \u03b8 (\u00b7) and to sample a = (a 1 , . . . , a d ) from that distribution. Specifically, p \u03b8 (a|a 1 , a 2 , . . . , a i\u22121 ), a \u2208 A i is given by the output of the LSTM. To generate a i , we run a forward pass through the LSTM with the input being a i\u22121 and the current state s t (and implicitly on a 1 , . . . , a i\u22121 which influences h i\u22121 ). This produces a hidden state h i , which is then passed through a linear layer, producing a K dimensional vector. The softmax of this vector is taken to produce the one-dimensional conditional distribution p \u03b8 (a|a 1 , a 2 , ..., a i\u22121 ), a \u2208 A i . Finally, a i is sampled from this one-dimensional distribution, and is then fed into the next stage of the LSTM to produce a i+1 .After generating the action a = (a 1 , . . . , a d ), and the conditional probabilities p \u03b8 (\u00b7|a 1 , a 2 , . . . , a i\u22121 ), i = 1, . . . , d, we can evaluate p \u03b8 (a) as the product of the conditional probabilities. During training, we can also use backpropagation to efficiently calculate the first term on the RHS of the update rule in (1). As an alternative to using a LSTM to create parameterized multidimensional policies, we can modify the underlying MDP to create an equivalent MDP for which the action space is one dimensional at each time step. We refer to this MDP as the Modified MDP (MMDP). In the original MDP, we have state space S and action space A = A 1 \u00d7 A 2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 A d where A i = {1, 2, . . . , K}. In MMDP, the state is modified to encapsulate the original state and all the action dimensions selected for state s so far, i.e., (s, a 1 , a 2 , . . . , a i , 0, . . . , 0) with a 1 , . . . , a i being selected values for action dimensions 1 to i, and 0 being the placeholder for d \u2212 i \u2212 1 dimensions. The new action space is A = {0, 1, . . . , K} and the new state space is S \u00d7 {0, 1, . . . , K} d\u22121 . The state transition probabilities for the MMDP are given by DISPLAYFORM0 where P (s |s, a 1 , . . . , a d ) is the transition probabiliy of the original MDP. The reward is only generated after all d component actions are taken. It is easily seen that the MMDP is equivalent to the original MDP.Since the MMDP has an one-dimensional action space, we can use a feed-forward network (FFN) to generate each action component as shown in FIG0 ). Note that the FFN input layer size is always |S| + K \u2212 1 and the output layer size is K. As shown in (1), an entropy bonus is typically included to enhance exploration. However, for large multidimensional action spaces, calculating the entropy and the gradient of the entropy requires enumerating all the actions in the action space and running forward and backpropagation for each action. In this section we develop computationally efficient unbiased estimates for the entropy and its gradient. Let A = (A 1 , . . . , A d ) denote a random variable with distribution p \u03b8 (\u00b7). Let H \u03b8 denote the exact entropy of the distribution p \u03b8 (a): DISPLAYFORM0 (a) The RNN architecture. To generate ai, we input st and ai\u22121 into the RNN and then pass the resulting hidden state hi through a linear layer and a softmax to generate a distribution, from which we sample ai. DISPLAYFORM1 . . . DISPLAYFORM2 The MMDP architecture. To generate ai, we input st and a1, a2, . . . , ai\u22121 into a FFN. The output is passed through a softmax layer, providing a distribution from which we sample ai. Since the input size of the FFN is fixed, when generating ai, constants 0 serve as placeholders for ai+1, . . . , a d\u22121 in the input to the FFN. During training within an episode, for each state s t , the policy (using, for example, LSTM or MMDP) generates an action a = (a 1 , a 2 , . . . , a d ). A crude approximation of the entropy bonus is: DISPLAYFORM0 This approximation is an unbiased estimate of H \u03b8 but its variance is likely to be large. To reduce the variance, we can generate M action samples a(1) , a (2) , . . . , a (M ) when in s t and average the log action probabilities over the samples. However, generating a large number of samples is costly, especially when each sample is generated from a neural network, since each sample requires one additional forward pass. In this section, we develop an alternative unbiased estimator for entropy which only requires the one episodic sample. In the course of an episode, an action a = (a 1 , a 2 , . . . , a d ) is generated for each s t . The alternative estimator accounts for the entropy along each dimension of the action space. DISPLAYFORM0 where DISPLAYFORM1 which is the entropy of A i conditioned on a i\u22121 . This approximation of entropy bonus is computationally efficient since for each dimension i, we need to obtain p \u03b8 (\u00b7|a i\u22121 ), its log and gradient anyway during training. We refer to this approximation as the smoothed entropy. The smoothed entropy H \u03b8 (A) has several appealing properties. The proofs of Theorem 1 and Theorem 3 are straightforward and omitted. DISPLAYFORM2 is an unbiased estimator of the exact entropy H \u03b8 .Theorem 2. If p \u03b8 (a) has a multivariable normal distribution with mean and variance depending on \u03b8, then: DISPLAYFORM3 Thus, the smoothed entropy equals the exact entropy for a multi-variate normal parameterization of the policy (Proof in Appendix B). Theorem 3. (i) If there is a sequence of weights \u03b8 1 , \u03b8 2 , . . . such that p \u03b8n (\u00b7) converges to the uniform distribution over A, then sup DISPLAYFORM4 Thus, the smoothed entropy H \u03b8 (a) mimics the exact entropy in that it has the same supremum and infinum values as the exact entropy. The above theorems indicate that H \u03b8 (a) may serve as a good proxy for H \u03b8 : it is an unbiased estimator for H \u03b8 , it has the same minimum and maximum values when varying \u03b8; and in the special case when p \u03b8 (a) has a multivariate normal distribution, it is actually equal to H \u03b8 for all a \u2208 A. Our numerical experiments have shown that the smoothed estimator H \u03b8 (a) typically has lower variance than the crude estimator H crude \u03b8 (a). However, it is not generally true that the smoothed estimator always has lower variance as counterexamples can be found. So far we have been looking at estimates of entropy. But the policy gradient algorithm (1) uses the gradient of the entropy rather than just simply the entropy. As it turns out, the gradient of estimators H crude \u03b8 (a) and H \u03b8 (a) are not unbiased estimates of the gradient of the entropy. In this subsection, we provide unbiased estimators for the gradient of the entropy. For simplicity, in this section, we assume an one-step decision setting, such as in a multi-armed bandit problem. A straightforward calculation shows: DISPLAYFORM0 Suppose a is one sample from p \u03b8 (\u00b7). A crude unbiased estimator for the gradient of the entropy therefore is: \u2212 log p \u03b8 (a)\u2207 \u03b8 log p \u03b8 (a) = log p \u03b8 (a)\u2207 \u03b8 H crude \u03b8 (a). Note that this estimator is equal to the gradient of the crude estimator multiplied by a correction factor. Analogous to the smoothed estimator for entropy, we can also derive a smoothed estimator for the gradient of the entropy. Theorem 4. If a is a sample from p \u03b8 (\u00b7), then DISPLAYFORM1 is an unbiased estimator for the gradient of the entropy (Proof in Appendix C).Note that this estimate for the gradient of the entropy is equal to the gradient of the smoothed estimate H \u03b8 (a) plus a correction term. We refer to this estimate of the entropy gradient as the unbiased gradient estimate. We designed experiments to compare the LSTM and MMDP models, and to also compare how the different entropy approximations perform for both. For each entropy approximation, the entropy weight as described in (1) was tuned to give the highest episode reward. For MMDP, the number of hidden layers was also tuned from 1 to 7. The rest of the hyperparameters are listed in Appendix A. In this environment, there is a n \u00d7 n grid. At the beginning of each episode d hunters and d rabbits are randomly placed in the grid. The rabbits remain fixed in the episode, and each hunter can move to a neighboring square (including diagonal neighbors) or stay at the current square. So each hunter has nine possible actions, and altogether there are |A| = 9 d actions at each time step. When a hunter enters a square with a rabbit, the hunter captures the rabbit and remains there until the end of the game. In each episode, the goal is for the hunters to capture the rabbits as quickly as possible. Each episode is allowed to run for at most 10,000 time steps. To provide a dense reward signal, we formalize the goal with the following modification: capturing a rabbit gives a reward of 1, which is discounted by the number of time steps taken since the beginning of the episode. The discount factor is 0.8. The goal is to maximize the episode's total discounted reward. After a hunter captures a rabbit, they both become inactive. The representation of an active hunter or rabbit is (1, y position, x position). The representation of an inactive hunter or rabbit is (0, -1, -1). TAB0 shows the performance of the LSTM and MMDP models with different entropy estimates. (smoothed mode entropy is explained in Appendix D). The evaluation was performed in a square grid of 5 by 5 with 5 hunters and 5 rabbits. Training was run for 1 million episodes for each of the seeds. All evaluations are averaged over 1,000 episodes per seed for a total of 5,000 episodes. First, we observe that the LSTM model always does better than the MMDP model, particularly for the episode length. Second, we note that policies obtained with the entropy approximations all perform better than policies obtained without entropy or with crude entropy. For the LSTM model, the best performing approximation is smoothed entropy, reducing the mean episode length by 45% and increasing the mean episode reward by 10% compared to without entropy. We also note that there is not a significant difference in performance between the smoothed entropy estimate, smoothed mode estimate, and the unbiased gradient estimate. As shown in TAB1 , smoothed entropy is also more robust to the initial seed than without entropy. For example, for the LSTM model, in the case of without entropy, seed 0 leads to significantly worse results than the seeds 1-4. This does not happen to smoothed entropy. We now consider how policies trained with entropy approximations compare with polices trained with exact entropy. In order to calculate exact entropy in an acceptable amount of time, we reduced the number of hunters and rabbits to 4 hunters and 4 rabbits. Training was run for 50,000 episodes. TAB2 shows the performance differences between policies trained with entropy approximations and exact entropy. We see that the best entropy approximations perform only slightly worse than exact entropy for both LSTM and MMDP. Once again we see that the LSTM model performs better than the MMDP model. We examine a multi-agent version of the standard multi-armed bandit problem, where there are d agents each pulling one of K arms, with d \u2264 K. The k th arm generates a reward r k . The total reward in a round is generated as follows. In each round, each agent chooses an arm. All of the chosen arms are then pulled, with each pulled arm generating a reward. Note that the total number of arms chosen, c, may be less than d since some agents may choose the same arm. The total reward is the sum of rewards from the c chosen arms. The optimal policy is for the d agents to collectively pull the d arms with the highest rewards. Additionally, among all the optimal assignments of d agents to the d arms that yield the highest reward, we add a bonus reward with probability p * if one particular agent-to-arms assignment is chosen. We performed experiments with 4 agents and 10 arms, with the k th arm providing a reward of k. The exceptional assignment gets a bonus of 200 with probability 0.01, and no bonus with probability 0.99. Thus the maximum expected reward is 36. Training was run for 100,000 rounds for each of the seeds. TAB3 shows average results for the last 500 of the 100,000 rounds. The results for the multi-agent bandit problem are consistent with those for the hunter-rabbit problem. Policies obtained with the entropy approximations all perform better than policies obtained without entropy or with crude entropy, particularly for the percentage of optimal arms pulled. We again note that using the unbiased gradient estimate does not perform significantly better than using the smoothed entropy estimate. There has been limited attention in the RL literature with regards to large discrete action spaces. BID13 proposes generalized value functions in the form of H-value functions, and also propose approximate linear programming as a solution technique. Their methodology is not suited for deep RL, and approximate linear programming may lead to highly sub-optimal solutions. Dulac-Arnold et al. FORMULA1 embeds discrete actions in a continuous space, picks actions in the continuous space and map these actions back into the discrete space. However, their algorithm introduces a new hyper-parameter that requires tuning for every new task. Our approach involves no new hyper-parameter other than those normally used in deep learning. In BID17 , each action dimension is treated as an agent and backpropagation is used to learn coordination between the agents. The approach is particularly adept for problems where agents leave and enter the system. However, the approach requires homogenous agents, and has not been shown to solve large-scale problems. Furthermore, the decentralized approach will potentially lead to highly suboptimal polices even though communication is optimized among the agents. To our knowledge, we are the first to propose using LSTMs and a modified MDP to create policies for RL problems with large multidimensional action spaces. Although this leads to algorithms that are straightforward, the approaches are natural and well-suited to multidimensional action spaces. We also propose novel estimators for the entropy regularization term that is often used in policy gradient. To the best of our knowledge, no prior work has dealt with approximating the policy entropy for MDP with large multidimensional discrete action space. On the other hand, there has been many attempts to devise methods to encourage beneficial exploration for policy gradient. BID10 modifies the entropy term by adding weights to the log action probabilities, leading to a new optimization objective termed under-appreciated reward exploration. While entropy regularization has been mostly used in algorithms that explicitly parameterize the policies, BID14 applies entropy regularization to Q-learning methods. They make an important observation about the equivalence between policy gradient and entropy regularized Q-learning, which they term soft Q-learning. In this paper, we developed a novel policy gradient methodology for the case of large multidimensional discrete action spaces. We proposed two approaches for creating parameterized policies: LSTM parameterization and a Modified MDP (MMDP) giving rise to Feed-Forward Network (FFN) parameterization. Both of these approaches provide expressive models to which backpropagation can be applied for training. We then developed several novel unbiased estimators for entropy bonus and its gradient. We did experimental work for two environments with large multidimensional action space. For these environments, we found that both the LSTM and MMDP approach could successfully solve large multidimensional action space problems, with the LSTM approach generally performing better. We also found that the smoothed estimates of the entropy and the unbiased gradient estimate of the entropy gradient can help reduce computational cost while not sacrificing significant loss in performance. Hyperparameters for hunter-rabbit gameThe LSTM policy has 128 hidden nodes. For the MMDP policy, the number of hidden layers for smoothed entropy, smoothed mode entropy, unbiased gradient estimate, crude entropy and without entropy are 5, 3, 3, 4 3 and 3 respectively. Each MMDP layer has 128 nodes. We parameterize the baseline in (1) with a feed forward neural network with one hidden layer of size 64. This network was trained using first visit Monte Carlo return to minimize the L1 loss between actual and predicted values of states visited during the epidode. Both the policies and baseline are optimized after each episode with RMSprop BID20 ). The RHS of FORMULA1 To obtain the results in TAB2 , the entropy weights for LSTM smoothed entropy, LSTM exact entropy, MMDP unbiased gradient estimate and MMDP exact entropy are 0.03, 0.01, 0.03 and 0.01 respectively. The MMDP networks have three layers with 128 nodes in each layer. Experimental results are averaged over five seeds. The experiments were run with 4 agents and 10 arms. For the 10 arms, their rewards are i for i = 1, . . . , 10. The LSTM policy has 32 hidden nodes. The baseline in (1) is a truncated average of the reward of the last 100 rounds. The entropy weight for crude entropy, smoothed entropy and unbiased gradient estimate are 0.005, 0.001 and 0.003 respectively. The learning rates for without entropy, crude entropy, smoothed entropy and unbiased gradient estimate are 0.006, 0.008, 0.002 and 0.005 respectively. Experimental results are averaged over ten seeds. We first note that for DISPLAYFORM0 where X 1 and X 2 are random vectors, we have X 2 | X 1 = x 1 \u223c N (\u03bc,\u03a3) wher\u0113 DISPLAYFORM1 Observe that the covariance matrix of the conditional distribution does not depend on the value of x 1 (Johnson & Wichern (1988) ).Also note that for X \u223c N (\u00b5, \u03a3), the entropy of X takes the form DISPLAYFORM2 where k is the dimension of X and | \u00b7 | denotes the determinant. Therefore, the entropy of a multivariate normal random variable depends only on the variance and not on the mean. Because A is multivariate normal, the distribution of A i given A 1 = a 1 , . . . , A i\u22121 = a i\u22121 has a normal distribution with a variance \u03c3 2 i that does not depend on a 1 , . . . , a i\u22121 . Therefore DISPLAYFORM3 does not depend on a 1 , . . . , a i\u22121 and hence H \u03b8 (a) does not depend on a. Combining this with the fact that H \u03b8 (a) is an unbiased estimator for H \u03b8 gives H \u03b8 (a) = H \u03b8 for all a \u2208 A. From (3), we have: DISPLAYFORM0 We will now use conditional expectation to calculate the terms in the double sum. For i < j: DISPLAYFORM1 For i > j: Combining these three conditional expectations with (4), we obtain: DISPLAYFORM2 DISPLAYFORM3 Depending on the episodic action a at a given time step in the episode, the smoothed entropy H \u03b8 (a) may give unsatisfactory results. For example, suppose for a particular episodic action a, H \u03b8 (a) H \u03b8 . In this case, the policy gradient may ignore the entropy bonus term, thinking that the policy already has enough entropy when it perhaps does not. We therefore consider alternative approximations which may improve performance at modest additional computational cost. First consider Thus in this case, instead of calculating the entropy over a sample action a, we calculate it over the most likely action a * . The problem here is that it is not easy to find a * when the given conditional probabilities p \u03b8 (a|a 1 , . . . , a i\u22121 ) are not in closed form but only available algorithmically as outputs of neural networks. DISPLAYFORM0 A more computationally efficient approach would be to choose the action greedily: a 1 = argmax The action\u00e2 is an approximation for the mode of the distribution p \u03b8 (\u00b7). As often done in NLP, we can use beam search to determine an action a that is a better approximation, that is, p \u03b8 (a ) \u2265 p \u03b8 (\u00e2). Indeed, the above H \u03b8 definition is beam search with beam size equal to 1. We refer to H \u03b8 as smoothed mode entropy. H \u03b8 with an appropriate beam size may be a better approximation for the entropy H \u03b8 than H \u03b8 (a). However, calculating H \u03b8 and its gradient comes with some computational cost. For example, with a beam size equal to one, we would have to make two passes through the neural network at each time step: one to obtain the episodic sample a and the other to obtain the greedy action\u00e2. For beam size n we would need to make n + 1 passes. We note that H \u03b8 is a biased estimator for H \u03b8 but with no variance. Thus there is a bias-variance tradeoff between H \u03b8 (a) and H \u03b8 . Note that H \u03b8 also satisfies Theorems 2 and 3 in subsection 4.2."
}