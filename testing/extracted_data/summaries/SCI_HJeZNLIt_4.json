{
    "title": "HJeZNLIt_4",
    "content": "Flow based models such as Real NVP are an extremely powerful approach to density estimation. However, existing flow based models are restricted to transforming continuous densities over a continuous input space into similarly continuous distributions over continuous latent variables. This makes them poorly suited for modeling and representing discrete structures in data distributions, for example class membership or discrete symmetries. To address this difficulty, we present a normalizing flow architecture which relies on domain partitioning using locally invertible functions, and possesses both real and discrete valued latent variables.   This Real and Discrete (RAD) approach retains the desirable normalizing flow properties of exact sampling, exact inference, and analytically computable probabilities, while at the same time allowing simultaneous modeling of both continuous and discrete structure in a data distribution. Latent generative models are one of the prevailing approaches for building expressive and tractable generative models. The generative process for a sample x can be expressed as DISPLAYFORM0 where z is a noise vector, and g a parametric generator network (typically a deep neural network). This paradigm has several incarnations, including variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014) , generative adversarial networks (Goodfellow et al., 2014) , and flow based models BID0 BID9 BID5 Kingma & Dhariwal, 2018; BID3 Grathwohl et al., 2019) .The training process and model architecture for many existing latent generative models, and for all published flow based models, assumes a unimodal smooth distribution over latent variables z. Given the parametrization of g as a neural network, the mapping to x is a continuous function. This imposed structure makes it challenging to model data distributions with discrete structure -for instance, multi-modal distributions, distributions with holes, distributions with discrete symmetries, or distributions that lie on a union of manifolds (as may approximately be true for natural images, see BID11 . Indeed , such cases require the model to learn a generator whose input Jacobian has highly varying or infinite magnitude to separate the initial noise source into different clusters. Such variations imply a challenging optimization problem due to large changes in curvature. This shortcoming can be critical as several problems of interest are hypothesized to follow a clustering structure, i.e. the distributions is concentrated along several disjoint connected sets (Eghbal-zadeh et al., 2018) .A standard way to address this issue has been to use mixture models BID16 Richardson & Weiss, 2018; Eghbal-zadeh et al., 2018) or structured priors (Johnson et al., 2016) . In order to efficiently parametrize the model, mixture models are often formulated as a discrete latent variable models (Hinton & Salakhutdinov, 2006; BID4 Mnih & Gregor, 2014 ; van den Oord model (1c, 1d) . Note the dependency of K on Z in 1d. While this is not necessary , we will exploit this structure as highlighted later in the main text and in Figure 4 . et al., 2017) , some of which can be expressed as a deep mixture model BID10 BID14 BID13 . Although the resulting exponential number of mixture components with depth in deep mixture models is an advantage in terms of expressivity, it is an impediment to inference, evaluation, and training of such models, often requiring as a result the use of approximate methods like hard-EM or variational inference (Neal & Hinton, 1998) .In this paper we combine piecewise invertible functions with discrete auxiliary variables, selecting which invertible function applies, to describe a deep mixture model. This framework enables a probabilistic model's latent space to have both real and discrete valued units, and to capture both continuous and discrete structure in the data distribution. It achieves this added capability while preserving the exact inference, exact sampling, exact evaluation of log-likelihood, and efficient training that make standard flow based models desirable. We introduced an approach to tractably evaluate and train deep mixture models using piecewise invertible maps as a folding mechanism. This allows exact inference, exact generation, and exact evaluation of log-likelihood, avoiding many issues in previous discrete variables models. This method can easily be combined with other flow based architectural components, allowing flow based models to better model datasets with discrete as well as continuous structure. Figure 11: RAD and REAL NVP inference processes on the ring Gaussian mixture problem. Each column correspond to a RAD or affine coupling layer. RAD effectively uses foldings in order to bridge the multiple modes of the distribution into a single mode, primarily in the last layers of the transformation, whereas REAL NVP struggles to bring together these modes under the standard Gaussian distribution using continuous bijections. A CONTINUITYThe standard approach in learning a deep probabilistic model has been stochastic gradient descent on the negative log-likelihood. Although the model enables the computation of a gradient almost everywhere, the log-likelihood is unfortunately discontinuous. Let's decompose the log-likelihood DISPLAYFORM0 There are two sources of discontinuity in this expression: f K is a function with discrete values (therefore discontinuous) and \u2202f Z \u2202x T is discontinuous because of the transition between the subsets A k , leading to the expression of interest DISPLAYFORM1 which takes a role similar to the log-Jacobian determinant, a pseudo log-Jacobian determinant.Let's build from now on the simple scalar case and a piecewise linear function DISPLAYFORM2 In this case, s(z) = log p K|Z k | z k\u2264N + C1 |K| can be seen as a vector valued function.We can attempt at parametrizing the model such that the pseudo log-Jacobian determinant becomes continuous with respect to \u03b2 by expressing the boundary condition at x = \u03b2 DISPLAYFORM3 \u21d2s(\u2212\u03b1 2 \u03b2) 2 + log(\u03b1 2 ) = s(\u2212\u03b1 2 \u03b2) 3 + log(\u03b1 3 ). DISPLAYFORM4 \u2212 log(\u03b1 1 ), log(\u03b1 2 ), log(\u03b1 3 ) + \u03b2 2 1 + cos (z\u03b1 DISPLAYFORM5 Another type of boundary condition can be found at between the non-invertible area and the invertible area z = \u03b1 2 \u03b2, as \u2200z > \u03b1 2 \u03b2, p 3|Z (3 | z) = 1, therefore DISPLAYFORM6 Since the condition \u2200k < 3, p K|Z k | z) \u2192 0 when z \u2192 (\u03b1 2 \u03b2) \u2212 will lead to an infinite loss barrier at x = \u2212\u03b2, another way to enforce this boundary condition is by adding linear pieces FIG1 ): DISPLAYFORM7 The inverse is defined as DISPLAYFORM8 In order to know the values of s at the boundaries \u00b1\u03b1 2 \u03b2, we can use the logit function DISPLAYFORM9 Given those constraints, the model can then be reliably learned through gradient descent methods. Note that the resulting tractability of the model results from the fact that the discrete variables k is only interfaced during inference with the distribution p K|Z , unlike discrete variational autoencoders approaches (Mnih & Gregor, 2014; BID15 where it is fed to a deep neural network. Similar to BID7 , the learning of discrete variables is achieved by relying on the the continuous component of the model, and, as opposed as other approaches (Jang et al., 2017; BID12 Grathwohl et al., 2018; BID12 , this gradient signal extracted is exact and closed form."
}