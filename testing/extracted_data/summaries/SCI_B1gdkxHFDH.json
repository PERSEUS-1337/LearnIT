{
    "title": "B1gdkxHFDH",
    "content": "We propose an approach to training machine learning models that are fair in the sense that their performance is invariant under certain perturbations to the features. For example, the performance of a resume screening system should be invariant under changes to the name of the applicant. We formalize this intuitive notion of fairness by connecting it to the original  notion of individual fairness put forth by Dwork et al and show that the proposed approach achieves this notion of fairness. We also demonstrate the effectiveness of the approach on two machine learning tasks that are susceptible to gender and racial biases. As AI systems permeate our world, the problem of implicit biases in these systems have become more serious. AI systems are routinely used to make decisions or support the decision-making process in credit, hiring, criminal justice, and education, all of which are domains protected by anti-discrimination law. Although AI systems appear to eliminate the biases of a human decision maker, they may perpetuate or even exacerbate biases in the training data (Barocas & Selbst, 2016) . Such biases are especially objectionable when it adversely affects underprivileged groups of users (Barocas & Selbst, 2016) . In response, the scientific community has proposed many formal definitions of algorithmic fairness and approaches to ensure AI systems remain fair. Unfortunately, this abundance of definitions, many of which are incompatible (Kleinberg et al., 2016; Chouldechova, 2017) , has hindered the adoption of this work by practitioners (Corbett-Davies & Goel, 2018) . There are two types of formal definitions of algorithmic fairness: group fairness and individual fairness. Most recent work on algorithmic fairness considers group fairness because it is more amenable to statistical analysis (Jiang et al., 2019) . Despite their prevalence, group notions of algorithmic fairness suffer from certain shortcomings. One of the most troubling is there are many scenarios in which an algorithm satisfies group fairness, but its output is blatantly unfair from the point of view of individual users (Dwork et al., 2011) . In this paper, we consider individual fairness instead of group fairness. At a high-level, an individually fair ML model treats similar users similarly. Formally, we consider an ML model as a map h : X \u2192 Y, where X and Y are the input and output spaces. The leading notion of individual fairness is metric fairness (Dwork et al., 2011) ; it requires d y (h(x 1 ), h(x 2 )) \u2264 Ld x (x 1 , x 2 ) for all x 1 , x 2 \u2208 X , (1.1) where d x and d y are metrics on the input and output spaces and L \u2208 R + . The fair metric d x encodes our intuition of which samples should be treated similarly by the ML model. We emphasize that d x (x 1 , x 2 ) being small does NOT imply x 1 and x 2 are similar in all respects. Even if d x (x 1 , x 2 ) is small, x 1 and x 2 may differ in certain attributes that are irrelevant to the ML task at hand, e.g. protected attributes. This is why we refer to pairs of samples x 1 and x 2 such that d x (x 1 , x 2 ) is small as comparable instead of similar. Despite its benefits, individual fairness is considered impractical because the choices of d x and d y are ambiguous. Unfortunately, in application areas where there is disagreement over the choice of d x and/or d y , this ambiguity negates most of the benefits of a formal definition of fairness. Dwork et al. (2011) consider randomized ML algorithms, so h(x) is generally a random variable. They suggest probability metrics (e.g. total variation distance) as d y and defer the choice of d x to regulatory bodies or civil rights organizations, but we are unaware of commonly accepted choices of d x . In this paper, we consider two data-driven choices of the fair metric: one for problems in which the sensitive attribute is reliably observed, and another for problems in which the sensitive attribute is unobserved. Due to space constraints, we defer the details to the supplement (see Appendix B). In this paper, we consider an adversarial approach to training individually fair ML models: we show that individual fairness is a restricted form of robustness: robustness to certain sensitive perturbations to the inputs of an ML model. This connection allows us to leverage recent advances in adversarial training (Madry et al., 2017) to train individually fair ML models. The paper is organized into four main sections. In Section 2 we develop a method to investigate algorithmic bias/unfairness in ML models. This leads to a training method that trains ML models to pass such investigations. Our algorithmic developments are followed by a theoretical investigation (see Section 3) and an empirical study (see Section 3) of the efficacy of the proposed approach on two ML tasks."
}