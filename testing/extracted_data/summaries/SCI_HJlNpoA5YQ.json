{
    "title": "HJlNpoA5YQ",
    "content": "The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons:  First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent. The performance of machine learning methods generally depends on the choice of data representation BID2 . In reinforcement learning (RL), the choice of state representation may affect generalization (Rafols et al., 2005) , exploration (Tang et al., 2017; Pathak et al., 2017) , and speed of learning BID7 . As a motivating example, consider goal-achieving tasks, a class of RL tasks which has recently received significant attention BID1 Pong et al., 2018) . In such tasks, the agent's task is to achieve a certain configuration in state space; e.g. in FIG0 the environment is a two-room gridworld and the agent's task is to reach the red cell. A natural reward choice is the negative Euclidean (L2) distance from the goal (e.g., as used in Nachum et al. FORMULA4 ). The ability of an RL agent to quickly and successfully solve the task is thus heavily dependent on the representation of the states used to compute the L2 distance. Computing the distance on one-hot (i.e. tabular) representations of the states (equivalent to a sparse reward) is most closely aligned with the task's directive. However, such a representation can be disadvantageous for learning speed, as the agent receives the same reward signal for all non-goal cells. One may instead choose to compute the L2 distance on (x, y) representations of the grid cells. This allows the agent to receive a clear signal which encourages it to move to cells closer to the goal. Unfortunately, this representation is agnostic to the environment dynamics, and in cases where the agent's movement is obstructed (e.g. by a wall as in FIG0 ), this choice of reward is likely to cause premature convergence to sub-optimal policies unless sophisticated exploration strategies are used. The ideal reward structure would be defined on state representations whose distances roughly correspond to the ability of the agent to reach one state from another. Although there are many suitable such representations, in this paper, we focus on a specific approach based on the graph Laplacian, which is notable for this and several other desirable properties. For a symmetric weighted graph, the Laplacian is a symmetric matrix with a row and column for each vertex. The d smallest eigenvectors of the Laplacian provide an embedding of each vertex in R d which has been found to be especially useful in a variety of applications, such as graph visualization BID9 , clustering (Ng et al., 2002) , and more BID6 .Naturally , the use of the Laplacian in RL has also attracted attention. In an RL setting, the vertices of the graph are given by the states of the environment. For a specific behavior policy, edges between states are weighted by the probability of transitioning from one state to the other (and vice-versa) . Several previous works have proposed that approximating the eigenvectors of the graph Laplacian can be useful in RL. For example, Mahadevan (2005) shows that using the eigenvectors as basis functions can accelerate learning with policy iteration. Machado et al. (2017a; b) show that the eigenvectors can be used to construct options with exploratory behavior. The Laplacian eigenvectors are also a natural solution to the aforementioned reward-shaping problem. If we use a uniformly random behavior policy, the Laplacian state representations will be appropriately aware of the walls present in the gridworld and will induce an L2 distance as shown in FIG0 (right). This choice of representation accurately reflects the geometry of the problem, not only providing a strong learning signal at every state, but also avoiding spurious local optima.While the potential benefits of using Laplacian-based representations in RL are clear, current techniques for approximating or learning the representations are ill-suited for model-free RL. For one, current methods mostly require an eigendecomposition of a matrix. When this matrix is the actual Laplacian (Mahadevan, 2005) , the eigendecomposition can easily become prohibitively expensive. Even for methods which perform the eigendecomposition on a reduced matrix (Machado et al., 2017a; b) , the eigendecomposition step may be computationally expensive, and furthermore precludes the applicability of the method to stochastic or online settings, which are common in RL. Perhaps more crucially, the justification for many of these methods is made in the tabular setting. The applicability of these methods to more general settings is unclear.To resolve these limitations, we propose a computationally efficient approach to approximate the eigenvectors of the Laplacian with function approximation based on the spectral graph drawing objective, an objective whose optimum yields the desired eigenvector representations. We present the objective in a fully general RL setting and show how it may be stochastically optimized over minibatches of sampled experience. We empirically show that our method provides a better approximation to the Laplacian eigenvectors than previous proposals, especially when the raw representation is not tabular. We then apply our representation learning procedure to reward shaping in goal-achieving tasks, and show that our approach outperforms both sparse rewards and rewards based on L2 distance in the raw feature space. Results are shown under a set of gridworld maze environments and difficult continuous control navigation environments. We have presented an approach to learning a Laplacian-based state representation in RL settings. Our approach is both general -being applicable to any state space regardless of cardinality -and scalable -relying only on the ability to sample mini-batches of states and pairs of states. We have further provided an application of our method to reward shaping in both discrete spaces and continuous-control settings. With our scalable and general approach, many more potential applications of Laplacian-based representations are now within reach, and we encourage future work to continue investigating this promising direction. A EXISTENCE OF SMALLEST EIGENVALUES OF THE LAPLACIAN.Since the Hilbert space H may have infinitely many dimensions we need to make sure that the smallest d eigenvalues of the Laplacian operator is well defined. Since L = I \u2212 D if \u03bb is an eigenvalue of D then 1 \u2212 \u03bb is an eigenvalue of L. So we turn to discuss the existence of the largest d eigenvalues of D. According to our definition D is a compact self-adjoint linear operator on H. So it has the following properties according to the spectral theorem:\u2022 D has either (i) a finite set of eigenvalues or (ii) countably many eigenvalues {\u03bb 1 , \u03bb 2 , ...} and \u03bb n \u2192 0 if there are infinitely many. All eigenvalues are real.\u2022 Any eigenvalue \u03bb satisfies \u2212 D \u2264 \u03bb \u2264 D where \u00b7 is the operator norm.If the operator D has a finite set of n eigenvalues its largest d eigenvalues exist when d is smaller than n.If D has a infinite but countable set of eigenvalues we first characterize what the eigenvalues look like: DISPLAYFORM0 Recall that the operator norm is defined as DISPLAYFORM1 Define q u be the probability measure such that DISPLAYFORM2 and DISPLAYFORM3 which hold for any f \u2208 H. Hence D \u2264 1.So the absolute values of the eigenvalues of D can be written as a non-increasing sequence which converges to 0 with the largest eigenvalue to be 1. If d is smaller than the number of positive eigenvalues of D then the largest d eigenvalues are guaranteed to exist. Note that this condition for d is stricter than the condition when D has finitely many eigenvalues. We conjecture that this restriction is due to an artifact of the analysis and in practice using any value of d would be valid when H has infinite dimensions."
}