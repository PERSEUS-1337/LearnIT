{
    "title": "SyxTQ7K88S",
    "content": "We introduce bio-inspired artificial neural networks consisting of neurons that are additionally characterized by spatial positions. To simulate properties of biological systems we add the costs penalizing long connections and the proximity of neurons in a two-dimensional space. Our experiments show that in the case where the network performs two different tasks, the neurons naturally split into clusters, where each cluster is responsible for processing a different task. This behavior not only corresponds to the biological systems, but also allows for further insight into interpretability or continual learning. Neurons in the human brain naturally group into interconnected regions, forming the full neural system [1] . In this paper, we would like to construct an analogical mechanism in the case of artificial neural networks. To put this idea into practice, we supply each neuron with spatial coordinates. Motivated by biological neural systems, we impose the cost of signal transmission between connected neurons, which grows linearly with the distance between them. In consequence, we obtain artificial groups specialized in different tasks, each group containing neurons that are placed close to each other. The proposed model is examined in a double classification task, where a single network has to classify examples from two different datasets (MNIST and Fashion-MNIST). At test time, we split the network into two subnetworks based on the structure of weights, where each subnetwork represents one task. The resulting models perform their respective tasks only slightly worse than the original network, in contrast to the large performance drop observed after splitting a standard fully connected network. Our model offers a natural interpretation of neurons' responsibilities and is analogous to biological neural systems. The idea of adding spatial coordinates to each neuron and penalizing long connections was previously introduced by [2] . Although our work is based on a similar premise, the resulting models differ significantly. We use a different, simpler spatial loss function, we investigate networks with multiple hidden layers as opposed to a single hidden layer, and we focus on the cluster-forming properties of spatial networks. There have been multiple approaches to finding clusters of neurons in artificial neural networks, although most of them consider the functional aspects of the network. For instance, [3] compare variance of neuron's activations in different tasks to decide which cluster does it belong to. In another approach, [4] cluster the network by using feature vectors calculated based on correlation between the neuron and the output. In comparison, our approach uses the structure of the network -the spatial placement of the neurons and the strength of connections between the neurons. Our model is also related to continual and multi-task learning [5] and parameter reduction models for deep neural networks [6] . In particular, the effect is slightly similar to the one obtained in [7] . Authors focus on splitting network weights into a set of groups, where each is associated with a class (task). In contrast to our biologically inspired mechanism, [7] use a specialized weight regularization technique and strive towards a different goal. In [8] , a nested sparse network is constructed and different forms of knowledge are learned at each level, enabling solving multiple tasks with a single neural network. Checking the response of different groups in our proposed spatial network could be considered useful for interpreting the network predictions, which is also an open problem [9] . We have presented a connection between neuroscience and machine learning, which to our best knowledge has not yet been explored. Experiments show that our proposed spatial artificial neural network manifests behavior similar to the region-forming processes in the brain. For future work we plan to test our model on a continual learning task. We hypothesize that, since the model is able to create disjoint clusters of neurons responsible for different tasks, learning a new task could be possible without disturbing the previously created clusters. Additional constraints could be added to achieve this, such as restricting the movement of neurons with high potential, while allowing neurons with low potential to move freely. Spatial networks could also be investigated in relation to spiking neural networks. The motivation is that spiking neural networks operate in the temporal dimension, which in the brain is dictated mainly by the spatial structure of neurons. Thus, it is possible that the two types of networks have similar properties, making the spatial network a more interesting model to investigate from the neuroscientific standpoint."
}