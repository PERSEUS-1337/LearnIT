{
    "title": "S1GkToR5tm",
    "content": "We propose a rejection sampling scheme using the discriminator of a GAN to\n approximately correct errors in the GAN generator distribution. We show that\n under quite strict assumptions, this will allow us to recover the data distribution\n exactly. We then examine where those strict assumptions break down and design a\n practical algorithm\u2014called Discriminator Rejection Sampling (DRS)\u2014that can be\n used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of\n Gaussians and on the state of the art SAGAN model. On ImageNet, we train an\n improved baseline that increases the best published Inception Score from 52.52 to\n 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use\n DRS to further improve on this baseline, improving the Inception Score to 76.08\n and the FID to 13.75. Generative Adversarial Networks (GANs) BID5 are a powerful tool for image synthesis. They have also been applied successfully to semi-supervised and unsupervised learning BID25 BID20 BID11 , image editing BID31 BID12 , and image style transfer BID2 . Informally, the GAN training procedure pits two neural networks against each other, a generator and a discriminator. The discriminator is trained to distinguish between samples from the target distribution and samples from the generator. The generator is trained to fool the discriminator into thinking its outputs are real. The GAN training procedure is thus a two-player differentiable game, and the game dynamics are largely what distinguishes the study of GANs from the study of other generative models. These game dynamics have well-known and heavily studied stability issues. Addressing these issues is an active area of research BID17 BID7 .However , we are interested in studying something different: Instead of trying to improve the training procedure, we (temporarily) accept its flaws and attempt to improve the quality of trained generators by post-processing their samples using information from the trained discriminator. It's well known that (under certain very strict assumptions) the equilibrium of this training procedure is reached when sampling from the generator is identical to sampling from the target distribution and the discriminator always outputs 1/2. However, these assumptions don't hold in practice. In particular , GANs as presently trained don't learn to reproduce the target distribution BID1 . Moreover, trained GAN discriminators aren't just identically 1/2 -they can even be used to perform chess-type skill ratings of other trained generators .We ask if the information retained in the weights of the discriminator at the end of the training procedure can be used to \"improve\" the generator. At face value, this might seem unlikely. After all, if there is useful information left in the discriminator, why doesn't it find its way into the generator via the training procedure? Further reflection reveals that there are many possible reasons. First, the assumptions made in various analyses of the training procedure surely don't hold in practice (e.g. the discriminator and generator have finite capacity and are optimized in parameter space rather than density-space). Second, due to the concrete realization of the discriminator and the generator as neural networks, it may be that it is harder for the generator to model a given distribution than it is for the discriminator to tell that this distribution is not being modeled precisely. Finally, we may simply not train GANs long enough in practice for computational reasons.In this paper, we focus on using the discriminator as part of a probabilistic rejection sampling scheme. In particular, this paper makes the following contributions:\u2022 We propose a rejection sampling scheme using the GAN discriminator to approximately correct errors in the GAN generator distribution.\u2022 We show that under quite strict assumptions, this scheme allows us to recover the data distribution exactly.\u2022 We then examine where those strict assumptions break down and design a practical algorithm -called DRS -that takes this into account.\u2022 We conduct experiments demonstrating the effectiveness of DRS. First, as a baseline, we train an improved version of the Self-Attention GAN, improving its performance from the best published Inception Score of 52.52 up to 62.36, and from a Fr\u00e9chet Inception Distance of 18.65 down to 14.79. We then show that DRS yields further improvement over this baseline, increasing the Inception Score to 76.08 and decreasing the Fr\u00e9chet Inception Distance to 13.75. We have proposed a rejection sampling scheme using the GAN discriminator to approximately correct errors in the GAN generator distribution. We've shown that under strict assumptions, we can recover the data distribution exactly. We've also examined where those assumptions break down and Each row shows images synthesized by interpolating in latent space. The color bar above each row represents the acceptance probabilities for each sample: red for high and white for low. Subjective visual quality of samples with high acceptance probability is considerably better: objects are more coherent and more recognizable as belonging to a specific class. There are fewer indistinct textures, and fewer scenes without recognizable objects. \u2022 There's no reason that our scheme can only be applied to GAN generators. It seems worth investigating whether rejection sampling can improve e.g. VAE decoders. This seems like it might help, because VAEs may have trouble with \"spreading mass around\" too much.\u2022 In one ideal case, the critic used for rejection sampling would be a human. Can we use better proxies for the human visual system to improve rejection sampling's effect on image synthesis models?\u2022 It would be interesting to theoretically characterize the efficacy of rejection sampling under the breakdown-of-assumptions that we have described earlier. In addition , we represent Inception score as a function of acceptance rate in FIG5 -left. Different acceptance rates are achieved by changing \u03b3 from the 0 th percentile of F (x) (acceptance rate = 100%) to its 90 th percentile (acceptance rate = 14%). Decreasing the acceptance rate filters more non-realistic samples and increases the final Inception score. After an specific rate, rejecting more samples does not gain any benefit in collecting a better pool of samples.Moreover, FIG5 -right shows the correlation between the acceptance probabilities that DRS assigns to the synthesized samples and the recognizability of those samples from the view-point of a pre-trained Inception network. The latter is measured by computing max j p(y j |x i ) which is the probability of sample x i belonging to the category y j from the 1,000 ImageNet classes. As expected, there is a large mass of the recognizable images accepted with high acceptance probabilities on the top right corner. The small mass of images which cannot be easily classified into one of the 1,000 categories while having high acceptance probability scores (the top left corner of the graph) can be due to the non-optimal GAN discriminator in practice. Therefore, we expect that improving the discriminator performance boosts the final inception score even more substantially. , and the acceptance probability assigned to each sample x i by DRS versus the maximum probability of belonging to one of the 1K categories based on a pre-trained Inception network, max j p(y j |x i ) (right)."
}