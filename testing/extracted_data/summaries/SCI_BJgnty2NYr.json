{
    "title": "BJgnty2NYr",
    "content": "Parameters are one of the most critical components of machine learning models. As datasets and learning domains change, it is often necessary and time-consuming to re-learn entire models. Rather than re-learning the parameters from scratch, replacing learning with optimization, we propose a framework building upon the theory of \\emph{optimal transport} to adapt model parameters by discovering correspondences between models and data, significantly amortizing the training cost. We demonstrate our idea on the challenging problem of creating probabilistic spatial representations for autonomous robots. Although recent mapping techniques have facilitated robust occupancy mapping, learning all spatially-diverse parameters in such approximate Bayesian models demand considerable computational time, discouraging them to be used in real-world robotic mapping. Considering the fact that the geometric features a robot would observe with its sensors are similar across various environments, in this paper, we demonstrate how to re-use parameters and hyperparameters learned in different domains. This adaptation is computationally more efficient than variational inference and Monte Carlo techniques. A series of experiments conducted on realistic settings verified the possibility of transferring thousands of such parameters with a negligible time and memory cost, enabling large-scale mapping in urban environments. The quintessential paradigm in the machine learning pipeline consists of the stages of data acquisition and inference of the given data. As data become plentiful, or as ones problem set become more diverse over time, it is common to learn new models tailored to the new data or problem. Contrasting this conventional modeling archetype, we argue that it is often redundant to perform inference and re-learn parameters from scratch. Such model adaptation procedures are indispensable in application domains such as robotics in which the operating environments change continuously. For instance, if the model is represented as a Bayesian model, its distribution should be redetermined regularly to adjust for changes in new data. In this paper, we focus on significantly improving the training time of building Bayesian occupancy maps such as automorphing Bayesian Hilbert maps (ABHMs) Senanayake et al. (2018) by transferring model parameters associated with a set of source datasets to a target dataset in a zero-shot fashion Isele et al. (2016) . Despite having attractive theoretical properties and being robust, the main reason that hinders models such as ABHM being used in real-world settings is the run-time cost of learning thousands of parameters (main parameters and hyperparameters). Moreover, these parameters not only vary across different places in the same environment, but also change over time. We demonstrate domain adaptation of \"geometry-dependent spatial features\" of the ABHM model from a pool of source domains to the current target domain. This is efficiently done using the theory of Optimal Transport Arjovsky et al. (2017) . Since the proposed approach completely bypasses explicitly learning parameters of the Bayesian model using domain adaptation, this process can be thought of as \"replacing parameter learning with domain adapatation.\" The notation given in Table 1 will be used throughout the rest of the paper."
}