{
    "title": "rygunsAqYQ",
    "content": "Implicit probabilistic models are models defined naturally in terms of a sampling procedure and often induces a likelihood function that cannot be expressed explicitly. We develop a simple method for estimating parameters in implicit models that does not require knowledge of the form of the likelihood function or any derived quantities, but can be shown to be equivalent to maximizing likelihood under some conditions. Our result holds in the non-asymptotic parametric setting, where both the capacity of the model and the number of data examples are finite. We also demonstrate encouraging experimental results. Generative modelling is a cornerstone of machine learning and has received increasing attention. Recent models like variational autoencoders (VAEs) BID32 BID45 and generative adversarial nets (GANs) BID21 BID25 , have delivered impressive advances in performance and generated a lot of excitement.Generative models can be classified into two categories: prescribed models and implicit models BID12 BID40 . Prescribed models are defined by an explicit specification of the density, and so their unnormalized complete likelihood can be usually expressed in closed form. Examples include models whose complete likelihoods lie in the exponential family, such as mixture of Gaussians BID18 , hidden Markov models BID5 , Boltzmann machines BID27 . Because computing the normalization constant, also known as the partition function, is generally intractable, sampling from these models is challenging.On the other hand, implicit models are defined most naturally in terms of a (simple) sampling procedure. Most models take the form of a deterministic parameterized transformation T \u03b8 (\u00b7) of an analytic distribution, like an isotropic Gaussian. This can be naturally viewed as the distribution induced by the following sampling procedure:1. Sample z \u223c N (0, I) 2. Return x := T \u03b8 (z)The transformation T \u03b8 (\u00b7) often takes the form of a highly expressive function approximator, like a neural net. Examples include generative adversarial nets (GANs) BID21 BID25 and generative moment matching nets (GMMNs) BID36 BID16 . The marginal likelihood of such models can be characterized as follows: DISPLAYFORM0 where \u03c6(\u00b7) denotes the probability density function (PDF) of N (0, I).In general, attempting to reduce this to a closed-form expression is hopeless. Evaluating it numerically is also challenging, since the domain of integration could consist of an exponential number of disjoint regions and numerical differentiation is ill-conditioned.These two categories of generative models are not mutually exclusive. Some models admit both an explicit specification of the density and a simple sampling procedure and so can be considered as both prescribed and implicit. Examples include variational autoencoders BID32 BID45 , their predecessors BID38 BID10 and extensions BID11 , and directed/autoregressive models, e.g., BID42 BID6 BID33 van den Oord et al., 2016 ). In this section, we consider and address some possible concerns about our method. We presented a simple and versatile method for parameter estimation when the form of the likelihood is unknown. The method works by drawing samples from the model, finding the nearest sample to every data example and adjusting the parameters of the model so that it is closer to the data example. We showed that performing this procedure is equivalent to maximizing likelihood under some conditions. The proposed method can capture the full diversity of the data and avoids common issues like mode collapse, vanishing gradients and training instability. The method combined with vanilla model architectures is able to achieve encouraging results on MNIST, TFD and CIFAR-10."
}