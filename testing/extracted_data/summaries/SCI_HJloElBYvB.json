{
    "title": "HJloElBYvB",
    "content": "In the Information Bottleneck (IB), when tuning the relative strength between compression and prediction terms, how do the two terms behave, and what's their relationship with the dataset and the learned representation? In this paper, we set out to answer these questions by studying multiple phase transitions in the IB objective: IB_\u03b2[p(z|x)] = I(X; Z) \u2212 \u03b2I(Y; Z) defined on the encoding distribution p(z|x) for input X, target Y and representation Z, where sudden jumps of dI(Y; Z)/d\u03b2 and prediction accuracy are observed with increasing \u03b2. We introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. Using second-order calculus of variations, we derive a formula that provides a practical condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. We provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, we present an algorithm for discovering phase transition points. Finally, we verify that our theory and algorithm accurately predict phase transitions in categorical datasets, predict the onset of learning new classes and class difficulty in MNIST, and predict prominent phase transitions in CIFAR10.\n The Information Bottleneck (IB) objective (Tishby et al., 2000) : explicitly trades off model compression (I(X; Z), I(\u00b7; \u00b7) denoting mutual information) with predictive performance (I(Y ; Z)) using the Lagrange multiplier \u03b2, where X, Y are observed random variables, and Z is a learned representation of X. The IB method has proved effective in a variety of scenarios, including improving the robustness against adversarial attacks (Alemi et al., 2016; Fischer, 2018) , learning invariant and disentangled representations (Achille & Soatto, 2018a; b) , underlying information-based geometric clustering (Strouse & Schwab, 2017b) , improving the training and performance in adversarial learning (Peng et al., 2018) , and facilitating skill discovery (Sharma et al., 2019) and learning goal-conditioned policy (Goyal et al., 2019) in reinforcement learning. From Eq. (1) we see that when \u03b2 \u2192 0 it will encourage I(X; Z) = 0 which leads to a trivial representation Z that is independent of X, while when \u03b2 \u2192 +\u221e, it reduces to a maximum likelihood objective 1 that does not constrain the information flow. Between these two extremes, how will the IB objective behave? Will prediction and compression performance change smoothly, or do there exist interesting transitions in between? In Wu et al. (2019) , the authors observe and study the learnability transition, i.e. the \u03b2 value such that the IB objective transitions from a trivial global minimum to learning a nontrivial representation. They also show how this first phase transition relates to the structure of the dataset. However, to answer the full question, we need to consider the full range of \u03b2. Motivation. To get a sense of how I(Y ; Z) and I(X; Z) vary with \u03b2, we train Variational Information Bottleneck (VIB) models (Alemi et al., 2016) on the CIFAR10 dataset (Krizhevsky & Hinton, 2009) , where each experiment is at a different \u03b2 and random initialization of the model. Fig. 1 shows the I(X; Z), I(Y ; Z) and accuracy vs. \u03b2, as well as I(Y ; Z) vs. I(X; Z) for CIFAR10 with 20% label noise (see Appendix I for details). are discontinuous and the accuracy has discrete jumps. The observation lets us refine our question: When do the phase transitions occur, and how do they depend on the structure of the dataset? These questions are important, since answering them will help us gain a better understanding of the IB objective and its close interplay with the dataset and the learned representation. Moreover, the IB objective belongs to a general form of two-term trade-offs in many machine learning objectives: L = Prediction-loss + \u03b2 \u00b7 Complexity, where the complexity term generally takes the form of regularization. Usually, learning is set at a specific \u03b2. Many more insights can be gained if we understand the behavior of the prediction loss and model complexity with varying \u03b2, and how they depend on the dataset. The techniques developed to address the question in the IB setting may also help us understand the two-term tradeoff in other learning objectives. Contributions. In this work, we begin to address the above question in IB settings. Specifically: \u2022 We identify a qualitative change of the IB loss landscape w.r.t. p(z|x) for varying \u03b2 as IB phase transitions (Section 3). \u2022 Based on the definition, we introduce a quantity G[p(z|x)] and use it to prove a theorem giving a practical condition for IB phase transitions. We further reveal the connection between G[p(z|x)] and the Fisher information matrix when p(z|x) is parameterized by \u03b8 (Section 3). \u2022 We reveal the close interplay between the IB objective, the dataset and the learned representation, by showing that in IB, each phase transition corresponds to learning a new nonlinear component of maximum correlation between X and Y , orthogonal to the previously-learned Z, and each with decreasing strength (Section 4). To the best of our knowledge, our work provides the first theoretical formula to address IB phase transitions in the most general setting. In addition, we present an algorithm for iteratively finding the IB phase transition points (Section 5). We show that our theory and algorithm give tight matches with the observed phase transitions in categorical datasets, predict the onset of learning new classes and class difficulty in MNIST, and predict prominent transitions in CIFAR10 experiments (Section 6). In this work, we observe and study the phase transitions in IB as we vary \u03b2. We introduce the definition for IB phase transitions, and based on it derive a formula that gives a practical condition for IB phase transitions. We further understand the formula via Jensen's inequality and representational maximum correlation. We reveal the close interplay between the IB objective, the dataset and the learned representation, as each phase transition is learning a nonlinear maximum correlation component in the orthogonal space of the learned representation. We present an algorithm for finding the phase transitions, and show that it gives tight matches with observed phase transitions in categorical datasets, predicts onset of learning new classes and class difficulty in MNIST, and predicts prominent transitions in CIFAR10 experiments. This work is a first theoretical step towards a deeper understanding of the phenomenon of phase transitions in the Information Bottleneck. We believe our approach will be applicable to other \"trade-off\" objectives, like \u03b2-VAE (Higgins et al., 2017) and InfoDropout (Achille & Soatto, 2018a) , where the model's ability to predict is balanced against a measure of complexity."
}