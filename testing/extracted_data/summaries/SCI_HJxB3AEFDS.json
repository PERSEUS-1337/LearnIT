{
    "title": "HJxB3AEFDS",
    "content": "We introduce a novel method for converting text data into abstract image representations, which allows image-based processing techniques (e.g. image classification networks) to be applied to text-based comparison problems. We apply the technique to entity disambiguation of inventor names in US patents. The method involves converting text from each pairwise comparison between two inventor name records into a 2D RGB (stacked) image representation. We then train an image classification neural network to discriminate between such pairwise comparison images, and use the trained network to label each pair of records as either matched (same inventor) or non-matched (different inventors), obtaining highly accurate results (F1: 99.09%, precision: 99.41%, recall: 98.76%). Our new text-to-image representation method could potentially be used more broadly for other NLP comparison problems, such as disambiguation of academic publications, or for problems that require simultaneous classification of both text and images. Databases of patent applications and academic publications can be used to investigate the process of research and innovation. For example, patent data can be used to identify prolific inventors (Gay et al., 2008) or to investigate whether mobility increases inventor productivity (Hoisl, 2009 ). However, the names of individuals in large databases are rarely distinct, hence individuals in such databases are not uniquely identifiable. For example, an individual named \"Chris Jean Smith\" may have patents under slightly different names such as \"Chris Jean Smith\", \"Chris J. Smith\", \"C J Smith\", etc. . . There may also be different inventors with patents under the same or similar names, such as \"Chris Jean Smith\", \"Chris J. Smith\", \"Chris Smith\", etc. . . Thus it is ambiguous which names (and hence patents) should be assigned to which individuals. Resolving this ambiguity and assigning unique identifiers to individuals -a process often referred to as named entity disambiguation -is important for research that relies on such databases. Machine learning algorithms have been used increasingly in recent years to perform automated disambiguation of inventor names in large databases (e.g. Li et al. (2014) ; Ventura et al. (2015) ; Kim et al. (2016) ). See Ventura et al. (2015) for a review of supervised, semi-supervised, and unsupervised machine learning approaches to disambiguation. These more recent machine learning approaches have often out-performed more traditional rule-and threshold-based methods, but they have generally used feature vectors containing several pre-selected measures of string similarity as input for their machine learning algorithms. That is, the researcher generally pre-selects a number of string similarity measures which they believe may be useful as input for the machine learning algorithm to make discrimination decisions. Here we introduce a novel approach of representing text-based data, which enables image classifiers to perform text classification. This new representation enables a supervised machine learning algorithm to learn its own features from the data, rather than selecting from a number of pre-defined string similarity measures chosen by the researcher. To do this, we treat the name disambiguation problem primarily as a classification problem -i.e. we assess pairwise comparisons between records as either matched (same inventor) or non-matched (different inventors) (Trajtenberg et al., 2006; Migu\u00e9lez & G\u00f3mez-Migu\u00e9lez, 2011; Li et al., 2014; Ventura et al., 2015; Kim et al., 2016) . Then, for a given pairwise comparison between two inventor records, our text-to-image representa-tion method converts the associated text strings into a stacked 2D colour image (or, equivalently, a 3D tensor) which represents the underlying text data. We describe our text-to-image representation method in detail in Section 4.1 (see Figure 1 in that section for an example of text-to-image conversion). We also test a number of alternative representations in Section 5.4. Our novel method of representing text-based records as abstract images enables image processing algorithms (e.g. image classification networks), to be applied to textbased natural language processing (NLP) problems involving pairwise comparisons (e.g. named entity disambiguation). We demonstrate this by combining our text-to-image conversion method with a commonly used convolutional neural network (CNN) (Krizhevsky et al., 2012) , obtaining highly accurate results (F1: 99.09%, precision: 99.41%, recall: 98.76%). Our name disambiguation algorithm provides a novel way of combining image processing with NLP, allowing image classifiers to perform text classification. We demonstrated this with the AlexNet CNN, producing highly accurate results (F1 score: 99.09%). We also analysed several variants of alternative string-maps, and found that the accuracy of the disambiguation algorithm was quite robust to such variation. Our disambiguation algorithm could easily be adapted to other NLP problems requiring text matching of multiple strings (e.g. academic author name disambiguation or record linkage problems). The algorithm could also potentially be modified to process records that contain both text and image data, by combining each record's associated image with the abstract image representation of the record's text, in a single comparison-map."
}