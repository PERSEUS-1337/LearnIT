{
    "title": "BJlSQnR5t7",
    "content": "Recent work has focused on combining kernel methods and deep learning. With this in mind, we introduce Deepstr\u00f6m networks -- a new architecture of neural networks which we use to replace top dense layers of standard convolutional architectures with an approximation of a kernel function by relying on the Nystr\u00f6m approximation. \n Our approach is easy highly flexible. It is compatible with any kernel function and it allows exploiting multiple kernels. \n We show that Deepstr\u00f6m networks reach state-of-the-art performance on standard datasets like SVHN and CIFAR100. One benefit of the method lies in its limited number of learnable parameters which make it particularly suited for small training set sizes, e.g. from 5 to 20 samples per class. Finally we illustrate two ways of using multiple kernels, including a multiple Deepstr\u00f6m  setting, that exploits a kernel on each feature map output by the convolutional part of the model.     Kernel machines and deep learning have mostly been investigated separately. Both have strengths and weaknesses and appear as complementary family of methods with respect to the settings where they are most relevant. Deep learning methods may learn from scratch relevant features from data and may work with huge quantities of data. Yet they actually require large amount of data to fully exploit their potential and may not perform well with limited training datasets. Moreover deep networks are complex and difficult to design and require lots of computing and memory resources both for training and for inference. Kernel machines are powerful tools for learning nonlinear relations in data and are well suited for problems with limited training sets. Their power comes from their ability to extend linear methods to nonlinear ones with theoretical guarantees. However, they do not scale well to the size of the training datasets and do not learn features from the data. They usually require a prior choice of a relevant kernel amongst the well known ones, or even require defining an appropriate kernel for the data at hand.Although most research in the field of deep learning seems to have evolved as a \"parallel learning strategy\" to the field of kernel methods, there are a number of studies at the interface of the two domains which investigated how some concepts can be transferred from one field to another. Mainly, there are two types of approaches that have been investigated to mix deep learning and kernels. Few works explored the design of deep kernels that would allow working with a hierarchy of representations as the one that has been popularized with deep learning (2; 14; 7; 6; 20; 23) . Other studies focused on various ways to plug kernels into deep networks (13; 24; 5; 12; 25) . This paper follows this latter line of research, it focuses on convolutional networks. Specifically, we propose Deepstr\u00f6m networks which are built by replacing dense layers of a convolutional neural network by an adaptive approximation of a kernel function. Our work is inspired from Deep Fried Convnets (24) which brings together convolutional neural networks and kernels via Fastfood (9), a kernel approximation technique based on random feature maps. We revisit this concept in the context of Nystr\u00f6m kernel approximation BID21 . One key advantage of our method is its flexibility that enables the use of any kernel function. Indeed, since the Nystr\u00f6m approximation uses an explicit feature map from the data kernel matrix, it is not restricted to a specific kernel function and not limited only to RBF kernels, as in Fastfood approximation. This is particularly useful when one wants to use or learn multiple different kernels instead of a single kernel function, as we demonstrate here. In particular we investigate two different ways of using multiple kernels, one is a straightforward extension to using multiple kernels while the second is a multiple Deepstr\u00f6m variant that exploits a Nystr\u00f6m kernel approximation for each of the feature map output by the convolutional part of the neural network.Furthermore the specific nature of our architecture makes it use only a limited number of parameters, which favours learning with small training sets as we demonstrate on targeted experiments.Our experiments on four datasets (MNIST, SVHN, CIFAR10 and CIFAR100) highlight three important features of our method. First our approach compares well to standard approaches in standard settings (using ful training sets) while requiring a reduced number of parameters compared to full deep networks and of the same order of magnitude as Deep Fried Convnets. This specific feature of our proposal makes it suitable for dealing with limited training set sizes as we show by considering experiments with tens or even fewer training samples per class. Finally the method may exploit multiple kernels, providing a new tool with which to approach the problem of multiple kernel learning (MKL) (4), and enabling taking into account the rich information in multiple feature maps of convolution networks through multiple Deepstr\u00f6m layers.The rest of the paper is organized as follows. We provide background on kernel approximation via the Nystr\u00f6m and the random Fourier features methods and describe the Deep Fried Convnet architecture in Section 2. The detailed configuration of the proposed Deepstr\u00f6m network is described in Section 3. We also show in Section 3 how Deepstr\u00f6m networks can be used with multiple kernels. Section 4 reports experimental results on MNIST, SVHN, CIFAR10 and CIFAR100 datasets to first provide a deeper understanding of the behaviour of our method with respect to the choice of the kernels and the combination of these, and second to compare it to state of the art baselines on classification tasks with respect to accuracy and to complexity issues, in particular in the small training set size setting. We proposed Deepstr\u00f6m, a new hybrid architecture that mixes deep networks and kernel methods. It is based on the Nystr\u00f6m approximation that allow considering any kind of kernel function in contrast to Deep Fried Convnets. Our proposal allows reaching state of the art results while significantly reducing the number of parameters on various datasets, enabling in particular learning from few samples. Moreover the method allows to easily deal with multiple kernels and with multiple Deepstr\u00f6m architectures. FIG5 plots the 2-dimensional \u03c6 nys representations of some CIFAR10 test samples obtained with a subsample of size equal to 2 (while the number of classes is 10) and two different kernels. One may see here that the 10 classes are already significantly well separated in this low dimensional representation space, illustrating that a very small sized subsammple is already powerfull. Beside, we experienced that designing Deepstr\u00f6m Convnets on lower level features output by lower level convolution blocks may yield state-of-the-art performance as well while requiring larger subsamples."
}