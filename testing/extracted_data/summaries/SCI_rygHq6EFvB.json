{
    "title": "rygHq6EFvB",
    "content": "Most domain adaptation methods consider the problem of transferring knowledge to the target domain from a single source dataset. However, in practical applications, we typically have access to multiple sources. In this paper we propose the first approach for Multi-Source Domain Adaptation (MSDA) based on Generative Adversarial Networks. Our method is inspired by the observation that the appearance of a given image depends on three factors: the domain, the style (characterized in terms of low-level features variations) and the content. For this reason we propose to project the image features onto a space where only the dependence from the content is kept, and then re-project this invariant representation onto the pixel space using the target domain and style. In this way, new labeled images can be generated which are used to train a final target classifier. We test our  approach using common MSDA benchmarks, showing that it outperforms state-of-the-art methods. A well known problem in computer vision is the need to adapt a classifier trained on a given source domain in order to work on another domain, i.e. the target. Since the two domains typically have different marginal feature distributions, the adaptation process needs to align the one to the other in order to reduce the domain shift (Torralba & Efros (2011) ). In many practical scenarios, the target data are not annotated and Unsupervised Domain Adaptation (UDA) methods are required. While most previous adaptation approaches consider a single source domain, in real world applications we may have access to multiple datasets. In this case, Multi-Source Domain Adaptation (MSDA) (Yao & Doretto (2010) ; Mansour et al. (2009) ; Xu et al. (2018) ; Peng et al. (2019) ) methods may be adopted, in which more than one source dataset is considered in order to make the adaptation process more robust. However, despite more data can be used, MSDA is challenging as multiple domain shift problems need to be simultaneously and coherently solved. In this paper we tackle MSDA (unsupervised) problem and we propose a novel Generative Adversarial Network (GAN) for addressing the domain shift when multiple source domains are available. Our solution is based on generating artificial target samples by transforming images from all the source domains. Then the synthetically generated images are used for training the target classifier. While this strategy has been recently adopted in single-source UDA scenarios (Russo et al. (2018) ; ; Liu & Tuzel (2016) ; Murez et al. (2018) ; Sankaranarayanan et al. (2018) ), we are the first to show how it can be effectively exploited in a MSDA setting. The holy grail of any domain adaptation method is to obtain domain invariant representations. Similarly, in multi-domain image-to-image translation tasks it is very crucial to obtain domain invariant representations in order to reduce the number of learned translations from O(N 2 ) to O(N ), where N is the number of domains. Several domain adaptation methods (Roy et al. (2019) ; Carlucci et al. (2017) ; ; Tzeng et al. (2014) ) achieve domain-invariant representations by aligning only domain specific distributions. However, we postulate that style is the most important latent factor that describe a domain and need to be modelled separately for obtaining optimal domain invariant representation. More precisely, in our work we assume that the appearance of an image depends on three factors: i.e. the content, the domain and the style. The domain models properties that are shared by the elements of a dataset but which may not be shared by other datasets, whereas, the factor style represents a property that is shared among different parts of a single image and describes low-level features which concern a specific image. Our generator obtains the do-main invariant representation in a two-step process, by first obtaining style invariant representations followed by achieving domain invariant representation. In more detail, the proposed translation is implemented using a style-and-domain translation generator. This generator is composed of two main components, an encoder and a decoder. Inspired by (Roy et al. (2019) ) in the encoder we embed whitening layers that progressively align the styleand-domain feature distributions in order to obtain a representation of the image content which is invariant to these factors. Then, in the decoder, we project this invariant representation onto a new domain-and-style specific distribution with Whitening and Coloring (W C) ) batch transformations, according to the target data. Importantly, the use of an intermediate, explicit invariant representation, obtained through W C, makes the number of domain transformations which need to be learned linear with the number of domains. In other words, this design choice ensures scalability when the number of domains increases, which is a crucial aspect for an effective MSDA method. Contributions. Our main contributions can be summarized as follows. (i) We propose the first generative model dealing with MSDA. We call our approach TriGAN because it is based on three different factors of the images: the style, the domain and the content. (ii) The proposed style-anddomain translation generator is based on style and domain specific statistics which are first removed from and then added to the source images by means of modified W C layers: Instance Whitening Transform (IW T ), Domain Whitening Transform (DW T ) (Roy et al. (2019) ), conditional Domain Whitening Transform (cDW T ) and Adaptive Instance Whitening Transform (AdaIW T ). Notably, the IW T and AdaIW T are novel layers introduced with this paper. (iii) We test our method on two MSDA datasets, Digits-Five (Xu et al. (2018) ) and Office-Caltech10 (Gong et al. (2012) ), outperforming state-of-the-art methods."
}