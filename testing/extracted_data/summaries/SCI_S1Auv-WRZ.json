{
    "title": "S1Auv-WRZ",
    "content": "Effective training of neural networks requires much data. In the low-data regime,\n parameters are underdetermined, and learnt networks generalise poorly. Data\n Augmentation (Krizhevsky et al., 2012) alleviates this by using existing data\n more effectively. However standard data augmentation produces only limited\n plausible alternative data. Given there is potential to generate a much broader set\n of augmentations, we design and train a generative model to do data augmentation.\n The model, based on image conditional Generative Adversarial Networks, takes\n data from a source domain and learns to take any data item and generalise it\n to generate other within-class data items. As this generative process does not\n depend on the classes themselves, it can be applied to novel unseen classes of data.\n We show that a Data Augmentation Generative Adversarial Network (DAGAN)\n augments standard vanilla classifiers well. We also show a DAGAN can enhance\n few-shot learning systems such as Matching Networks. We demonstrate these\n approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and\n VGG-Face data. In our experiments we can see over 13% increase in accuracy in\n the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9%\n to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we\n observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in\n EMNIST (from 59.5% to 61.3%). Over the last decade Deep Neural Networks have produced unprecedented performance on a number of tasks, given sufficient data. They have been demonstrated in variety of domains BID9 spanning from image classification BID23 BID14 , machine translation BID42 , natural language processing BID9 , speech recognition BID17 , and synthesis BID41 , learning from human play BID3 and reinforcement learning BID26 BID6 BID39 BID10 among others. In all cases, very large datasets have been utilized, or in the case of reinforcement learning, extensive play. In many realistic settings we need to achieve goals with limited datasets; in those case deep neural networks seem to fall short, overfitting on the training set and producing poor generalisation on the test set. Techniques have been developed over the years to help combat overfitting such as dropout BID18 , batch normalization BID21 , batch renormalisation BID20 or layer normalization BID2 . However in low data regimes, even these techniques fall short, since the the flexibility of the network is so high. These methods are not able to capitalise on known input invariances that might form good prior knowledge for informing the parameter learning. It is also possible to generate more data from existing data by applying various transformations BID23 to the original dataset. These transformations include random translations, rotations and flips as well as addition of Gaussian noise. Such methods capitalize on transformations that we know should not affect the class. This technique seems to be vital, not only for the low-data cases but for any size of dataset, in fact even models trained on some of the largest datasets such as Imagenet BID4 can benefit from this practice. Typical data augmentation techniques use a very limited set of known invariances that are easy to invoke. In this paper we recognize that we can learn a model of a much larger invariance space, through training a form of conditional generative adversarial network (GAN) in a different domain, typically called the source domain. This can then be applied in the low-data domain of interest, the ? Figure 1 : Learning a generative manifold for the classes in the source domain (left) can help learn better classifiers for the one shot target domain (right): The test point (pentagon) is nearer to the orange point but is actually closer to the learnt grey data manifold. If we generate extra examples on the grey manifold the nearest neighbour measure will better match the nearest manifold measure. Data augmentation is a widely applicable approach to improving performance in low-data setting, and a DAGAN is a flexible model to automatic learn to augment data. However beyond that, We demonstrate that DAGANS improve performance of classifiers even after standard data-augmentation. Furthermore by meta-learning the best choice of augmentation in a one-shot setting it leads to better performance than other state of the art meta learning methods. The generality of data augmentation across all models and methods means that a DAGAN could be a valuable addition to any low data setting. Table 2 : Omniglot One Shot Results: All results are averages over 3 independent runs. Note that our own local implementation of matching networks substantially outperform the matching network results presented in the original paper, However DAGAN augmentation takes matching networks up to the level of Conv-ARC BID32 , which explicitly use knowledge that the data has the structure of handwritten characters: the Conv-ARC model uses the stroke structure of the characters to perform well. Note DAGAN augmentation can even increase a simple pixel distance nearest neighbour model up to non-negligible levels."
}