{
    "title": "H1l7bnR5Ym",
    "content": "Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN). In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference. Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution. Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs. Generative Adversarial Networks (GAN) BID9 is notoriously hard to train and suffers from mode collapse. There has been a series of works attempting to address these issues. One noticeable thread focuses on objective design, which improves the original JensenShannon divergence with more stable pseudo-metrics such as f -divergence (Nowozin et al., 2016) , \u03c7 2 -divergence (Mao et al., 2017) , and Wasserstein distance BID0 . However, such treatment is inherently limited when a single generator does not include enough model capacity to capture the granularity in data distribution in practice. Clearly, such a generator can hardly produce accurate samples regardless of the choice of objectives.An alternative remedy is to learn multiple generators instead of a single one. This type of methods (Hoang et al., 2018; Tolstikhin et al., 2017; Wang et al., 2016b ) is motivated by a straightforward intuition that multiple generators can better model multi-modal distributions since each generator only needs to capture a subset of the modes. To entail model aggregation, probabilistic modelling is a natural and principled framework to articulate the aggregation process.Recently, Saatci & Wilson (2017) propose Bayesian GAN, a probabilistic framework for GAN under Bayesian inference. It shows that modelling the distribution of generator helps alleviate mode collapse and motivates the interpretability of the learned generators. This probabilistic framework is built upon Bayesian models for generator and discriminator, whose maximum likelihood estimation can be realized as a metaphor of typical GAN objectives.While empirical study on semi-supervised image classification tasks shows the effectiveness of Bayesian GAN, a critical theoretical question on this framework remains unanswered: Does it really converge to the generator distribution that produces the real data distribution? Indeed, our theoretical analysis and experimental results on a simple toy dataset reveal that the current Bayesian GAN falls short of convergence guarantee.With this observation, we follow the prior work to exploit probabilistic modelling as a principled way to realize model aggregation, but approach this problem from a theoretical perspective. We analyze the developed treatment, including the choice of priors, approximate inference, as well as its convergence property, and simultaneously propose a new probabilistic framework with the desirable convergence guarantee and consequently superior empirical performance.Our main contributions are:\u2022 We theoretically establish, to our best knowledge, the first probabilistic treatment of GANs such that any generator distribution faithful to the data distribution is an equilibrium.\u2022 We prove the previous Bayesian method (Saatci & Wilson, 2017) for any minimax GAN objective induces incompatibility of its defined conditional distributions.\u2022 We propose two special Monte Carlo inference algorithms for our probabilistic model which efficiently approximate the gradient of a non-differentiable criterion.\u2022 Empirical studies on synthetic high-dimensional multi-modal data and benchmark image datasets, CIFAR-10, STL-10, and ImageNet, demonstrate the superiority of the proposed framework over the state-of-the-art GAN methods. In this paper, we propose ProbGAN, a novel probabilistic modelling framework for GAN. From the perspective of Bayesian Modelling, it contributes a novel likelihood function establishing a connection to existing GAN models and a novel prior stabilizing the inference process. We also design scalable and asymptotically correct inference algorithms for ProbGAN. In the future work, we plan to extend the proposed framework to non-parametric Bayesian modelling and investigate more theoretical properties of GANs in the probabilistic modelling context.Developing Bayesian generalization for deep learning models is not a recent idea and happens in many fields other than generative models such as adversarial training (Ye & Zhu, 2018) and Bayesian neural networks (Wang et al., 2016a) . By this work, we emphasize the importance of going beyond the intuition and understanding the theoretical behavior of the Bayesian model. We hope that our work helps inspire continued exploration into Bayesian deep learning (Wang & Yeung, 2016 ) from a more rigorous perspective. A OMITTED PROOFS Theorem 1. This theorem is general and holds when the GAN objective and the discriminator space have symmetry. The symmetry of GAN objective means its functions \u03c6 1 and \u03c6 2 satisfy that \u2203c \u2208 R, \u2200x \u2208 R, \u03c6 1 (x) \u2261 \u03c6 2 (c \u2212 x). While the symmetry of discriminator space DISPLAYFORM0 Note that the symmetry condition are very weak, first it holds for all the common choices of GAN objectives such as those listed in TAB0 . Second, it holds for neural network which is the most common parameterization for discriminator in practice. DISPLAYFORM1 DISPLAYFORM2 Eqn. 11 and Eqn. 12 prove that q * DISPLAYFORM3 Thus the generator distribution will not change based on the dynamics in Eqn. 2 since q * DISPLAYFORM4 )."
}