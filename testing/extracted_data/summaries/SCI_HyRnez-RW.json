{
    "title": "HyRnez-RW",
    "content": "Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures. Reading comprehension, the task of answering questions based on a set of one more documents, is a key challenge in natural language understanding. While data-driven approaches for the task date back to BID11 , much of the recent progress can be attributed to new largescale datasets such as the CNN/Daily Mail Corpus BID8 , the Children's Book Test Corpus BID9 and the Stanford Question Answering Dataset (SQuAD) BID21 . These datasets have driven a large body of neural approaches BID24 BID16 BID22 BID27 , inter alia) that build complex deep models typically driven by long short-term memory networks BID12 . These models have given impressive results on SQuAD where the document consists of a single paragraph and the correct answer span is typically only present once. However, they are computationally intensive and cannot scale to large evidence texts. Such is the case in the recently released TriviaQA dataset BID14 , which provides as evidence, entire webpages or Wikipedia articles, for answering independently collected trivia-style questions.So far, progress on the TriviaQA dataset has leveraged existing approaches on the SQuAD dataset by truncating documents and focusing on the first 800 words BID14 BID18 . This has the obvious limitation that the truncated document may not contain the evidence required to answer the question 1 . Furthermore, in TriviaQA there is often useful evidence spread throughout the supporting documents. This cannot be harnessed by approaches such as that greedily search for the best 1-2 sentences in a document. For example, in Fig.1 the answer does not appear in the first 800 words. The first occurrence of the answer string is not sufficient to answer the question. The passage starting at token 4089 does contain all of the information required to infer the answer, but this inference requires us to resolve the two complex co-referential phrases in 'In the summer of that year they got married in a church'. Access to other mentions of Krasner and Pollock and the year 1945 is important to answer this question. Figure 1: Example from TriviaQA in which multiple mentions contain information that is useful in inferring the answer. Only the italicized phrase completely answers the question (Krasner could have married multiple times) but contains complex coreference that is beyond the scope of current natural language processing. The last phrase is more easy to interpret but it misses the clue provided by the year 1945.In this paper we present a novel cascaded approach to extractive question answering ( \u00a73) that can accumulate evidence from an order of magnitude more text than previous approaches, and which achieves state-of-the-art performance on all tasks and metrics in the TriviaQA evaluation. The model is split into three levels that consist of feed-forward networks applied to an embedding of the input. The first level submodels use simple bag-of-embeddings representations of the question, a candidate answer span in the document, and the words surrounding the span (the context). The second level submodel uses the representation built by the first level, along with an attention mechanism BID2 that aligns question words with words in the sentence that contains the candidate span. Finally, for answer candidates that are mentioned multiple times in the evidence document, the third level submodel aggregates the mention level representations from the second level to build a single answer representation. At inference time, predictions are made using the output of the third level classifier only. However, in training, as opposed to using a single loss, all the classifiers are trained using the multi-loss framework of BID1 , with gradients flowing down from higher to lower submodels. This separation into submodels and the multi-loss objective prevents adaptation between features BID10 . This is particularly important in our case where the higher level, more complex submodels could subsume the weaker, lower level models c.f. BID1 .To summarize, our novel contributions are\u2022 a non-recurrent architecture enabling processing of longer evidence texts consisting of simple submodels \u2022 the aggregation of evidence from multiple mentions of answer candidates at the representation level \u2022 the use of a multi-loss objective.Our experimental results ( \u00a74) show that all the above are essential in helping our model achieve state-of-the-art performance. Since we use only feed-forward networks along with fixed length window representations of the question, answer candidate, and answer context, the vast majority of computation required by our model is trivially parallelizable, and is about 45\u00d7 faster in comparison to recurrent models. We presented a 3-level cascaded model for TriviaQA reading comprehension. Our approach, through the use of feed-forward networks and bag-of-embeddings representations, can handle longer evidence documents and aggregated information from multiple occurrences of answer spans throughout the document. We achieved state-of-the-art performance on both Wikipedia and web domains, outperforming several complex recurrent architectures."
}