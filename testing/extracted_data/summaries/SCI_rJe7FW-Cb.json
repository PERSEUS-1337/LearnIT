{
    "title": "rJe7FW-Cb",
    "content": "We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\n Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\n Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores. Humans and animals process vasts amounts of information with limited computational resources thanks to attention mechanisms which allow them to focus resources on the most informative chunks of information. These biological mechanisms have been extensively studied (see BID0 ; BID2 ), concretely those mechanisms concerning visual attention, e.g. the work done by BID25 .In this work, we inspire on the advantages of visual and biological attention mechanisms for finegrained visual recognition with Convolutional Neural Networks (CNN) (see BID11 ). This is a particularly difficult task since it involves looking for details in large amounts of data (images) while remaining robust to deformation and clutter. In this sense, different attention mechanisms for fine-grained recognition exist in the literature: (i) iterative methods that process images using \"glimpses\" with recurrent neural networks (RNN) or long short-term memory (LSTM) (e.g. the work done by BID22 ; BID35 ), (ii) feed-forward attention mechanisms that augment vanilla CNNs, such as the Spatial Transformer Networks (STN) by BID8 , or a top-down feed-forward attention mechanism (FAM) BID19 ). Although it is not applied to fine-grained recognition, the Residual Attention introduced by BID28 is another example of feed-forward attention mechanism that takes advantage of residual connections BID6 ) to enhance or dampen certain regions of the feature maps in an incremental manner.Inspired by all the previous research about attention mechanisms in computer vision, we propose a novel feed-forward attention architecture (see FIG0 ) that accumulates and enhances most of the desirable properties from previous approaches:1. Detect and process in detail the most informative parts of an image: more robust to deformation and clutter. 2. Feed-forward trainable with SGD: faster inference than iterative models, faster convergence rate than Reinforcement Learning-based (RL) methods like the ones presented by BID22 ; BID15 . The proposed mechanism. Feature maps at different levels are processed to generate spatial attention masks and use them to output a class hypothesis based on local information and a confidence score (C). The final prediction consists of the average of all the hypotheses weighted by the normalized confidence scores. We have presented a novel attention mechanism to improve CNNs for fine-grained recognition. The proposed mechanism finds the most informative parts of the CNN feature maps at different depth levels and combines them with a gating mechanism to update the output distribution.Moreover, we thoroughly tested all the components of the proposed mechanism on Cluttered Translated MNIST, and demonstrate that the augmented models generalize better on the test set than their plain counterparts. We hypothesize that attention helps to discard noisy uninformative regions, avoiding the network to memorize them.Unlike previous work, the proposed mechanism is modular, architecture independent, fast, and simple and yet WRN augmented with it show higher accuracy in each of the following tasks: Age and Gender Recognition (Adience dataset), CUB200-2011 birds, Stanford Dogs, Stanford Cars, and UEC Food-100. Moreover, state of the art performance is obtained on gender, dogs, and cars. Figure 6: Test accuracy logs for the five fine-grained datasets. As it can be seen, the augmented models (WRNA) achieve higher accuracy at similar convergence rates. For the sake of space we only show one of the five folds of the Adience dataset."
}