{
    "title": "Syl7OsRqY7",
    "content": "End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders. A requirement of scalable and practical question answering (QA) systems is the ability to reason over multiple documents and combine their information to answer questions. Although existing datasets enabled the development of effective end-to-end neural question answering systems, they tend to focus on reasoning over localized sections of a single document (Hermann et al., 2015; Rajpurkar et al., 2016; 2018; Trischler et al., 2017) . For example, Min et al. (2018) find that 90% of the questions in the Stanford Question Answering Dataset are answerable given 1 sentence in a document. In this work, we instead focus on multi-evidence QA, in which answering the question requires aggregating evidence from multiple documents (Welbl et al., 2018; Joshi et al., 2017) .Our multi-evidence QA model, the Coarse-grain Fine-grain Coattention Network (CFC), selects among a set of candidate answers given a set of support documents and a query. The CFC is inspired by coarse-grain reasoning and fine-grain reasoning. In coarse-grain reasoning, the model builds a coarse summary of support documents conditioned on the query without knowing what candidates are available, then scores each candidate. In fine-grain reasoning, the model matches specific finegrain contexts in which the candidate is mentioned with the query in order to gauge the relevance of the candidate. These two strategies of reasoning are respectively modeled by the coarse-grain and fine-grain modules of the CFC. Each module employs a novel hierarchical attention -a hierarchy of coattention and self-attention -to combine information from the support documents conditioned on the query and candidates. FIG0 illustrates the architecture of the CFC.The CFC achieves a new state-of-the-art result on the blind Qangaroo WikiHop test set of 70.6% accuracy, beating previous best by 3% accuracy despite not using pretrained contextual encoders. In addition , on the TriviaQA multi-paragraph question answering task (Joshi et al., 2017) , reranking Most of this work was done while Victor Zhong was at Salesforce Research. outputs from a traditional span extraction model (Clark & Gardner, 2018) using the CFC improves exact match accuracy by 3.1% and F1 by 3.0%.Our analysis shows that components in the attention hierarchies of the coarse and fine-grain modules learn to focus on distinct parts of the input. This enables the CFC to more effectively represent a large collection of long documents. Finally, we outline common types of errors produced by CFC, caused by difficulty in aggregating large quantity of references, noise in distant supervision, and difficult relation types. We presented CFC, a new state-of-the-art model for multi-evidence question answering inspired by coarse-grain reasoning and fine-grain reasoning. On the WikiHop question answering task, the CFC achieves 70.6% test accuracy, outperforming previous methods by 3% accuracy. We showed in our analysis that the complementary coarse-grain and fine-grain modules of the CFC focus on different aspects of the input, and are an effective means to represent large collections of long documents."
}