{
    "title": "BJelsDvo84",
    "content": "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use. Text classification is a fundamental task in natural language processing (NLP). Machine learning and deep learning have achieved high accuracy on tasks ranging from sentiment analysis (Tang et al., 2015) to topic classification BID24 , but high performance is often dependent on the size and quality of training data, which is often tedious to collect. Automatic data augmentation is commonly used in vision BID20 BID22 BID10 and speech (Cui et al., 2015; BID7 and can help train more robust models, particularly when using smaller datasets. However, because it is difficult to come up with generalized rules for language transformation, universal data augmentation techniques in NLP have not been explored.Previous work has proposed techniques for data augmentation in NLP. One popular study generated new data by translating sentences into French and back into English BID28 . Other works have used predictive language models for synonym replacement BID8 and data noising as smoothing BID27 . Although these techniques are valid, they are not often used in practice because they have a high cost of implementation relative to performance gain.In this paper, we present a simple set of universal data augmentation techniques for NLP called EDA (easy data augmentation). To the best of our knowledge, we are the first to comprehensively explore text editing techniques for data augmentation. We systematically evaluate EDA on five benchmark classification tasks, and results show that EDA provides substantial improvements on all five tasks and is particularly helpful for smaller datasets. Code will be made publicly available.Operation Sentence None A sad, superior human comedy played out on the back roads of life. SR A lamentable, superior human comedy played out on the backward road of life. RI A sad, superior human comedy played out on funniness the back roads of life. RS A sad, superior human comedy played out on roads back the of life. RD A sad, superior human out on the roads of life. We have shown that simple data augmentation operations can boost performance on text classification tasks. Although improvement is at times marginal, EDA substantially boosts performance and reduces overfitting when training on smaller datasets. Continued work on this topic could include exploring the theoretical underpinning of the EDA operations. We hope that EDA's simplicity makes a compelling case for its widespread use in NLP."
}