{
    "title": "r1gFV9Sj2N",
    "content": "In this work, we approach one-shot and few-shot learning problems as methods for finding good prototypes for each class, where these prototypes are generalizable to new data samples and classes. We propose a metric learner that learns a Bregman divergence by learning its underlying convex function. Bregman divergences are a good candidate for this framework given they are the only class of divergences with the property that the best representative of a set of points is given by its mean. We propose a flexible extension to prototypical networks to enable joint learning of the embedding and the divergence, while preserving computational efficiency. Our preliminary results are comparable with the prior work on the Omniglot and Mini-ImageNet datasets, two standard benchmarks for one-shot and few-shot learning. We argue that our model can be used for other tasks that involve metric learning or tasks that require approximate convexity such as structured prediction and data completion. Deep learning methods have shown tremendous performance on many tasks involving large-scale data. However, collecting large amounts of data is costly or even infeasible for many applications BID8 BID0 . The few-shot learning problem aims to achieve good performance on adapting to novel classes where only a small number of examples per novel class are available. In scenarios with few examples, classical classification, fine-tuning, or retraining methods fail due to severe overfitting, catastrophic forgetting, or inflexibility to adapt to new samples and categories BID3 .This problem has been of increasing interest to researchers, some of whom have been inspired by humans' ability to 1 Boston University, MA, USA. Correspondence to: Kubra Cilingir <kubra@bu.edu>, Brian Kulis <bkulis@bu.edu>. recognize novel classes very successfully with very few examples. The most recent approaches to solve the few-shot learning problem involve meta learning, which attempts to learn transferable knowledge between classes and tasks at training time, in order to help generalization and adaptivity at test time. Information is stored either in the initialization of the weights BID4 , in a recurrent memory unit BID16 , in the optimization strategy BID13 , or in an embedded space BID17 . In this work, we focus on the last approach due to its simplicity and compelling results, whereas the other methods require complex training mechanisms, complex inference, or the gathering of many similar tasks.In particular, we based our approach on prototype networks BID17 , which learn an embedding of the input data, and then construct prototypes for classes via averages or weighted averages over points in each class. A single vector representation per class is assumed to be sufficient to contain class-specific features BID14 . In BID17 , the Euclidean distance is used to measure distance between a query point and a class prototype. In contrast to existing work, we treat the problem as a joint embedding and metric learning problem. Because prototypes are typically represented by means of points, for the metric learning function we choose to learn a Bregman divergence as the underlying divergence. This class of divergences has the key property that the best representative of a set of points (in terms of the sum of divergences between the points and the representative) is given by the mean, which we argue makes it appropriate for constructing prototypes of classes for our problem.Compared to existing methods such as relation networks BID18 , ours is a more flexible approach since we focus on Bregman divergences, of which squared Euclidean distances are a special case. We may favor Bregman divergences over Euclidean distances since symmetry and the triangle inequality may not be necessary for data in a few-shot learning problem. In FIG1 we show a possible scenario to demonstrate this. Suppose image A and image B have the same shape, and image B and image C have the same color. Image A and image C need not be similar , but the triangle inequality forces a resemblance between A and C. Similarly, representations and similarity measures of class members' may not be desired to be symmetric. Proto- types share the abstract representative features with the data points, but each point has idiosyncratic features, that may break symmetry when interpreting the embedding space.Each Bregman divergence is parametrized by a convex function; furthermore, this relationship is a surjection. We design the metric learning function of our deep learning model with a convexity constraint with respect to the embedding space. This convex function is used to calculate the Bregman divergence as a learned metric. Our formulation also provides flexibility for the architectural design of the convex function, with a regularization term to improve generalizability. We empirically measure a convexity score by drawing random points from the convex hull of the data samples to verify our claim.Overall, we propose a model that has a learnable embedding and a learnable Bregman divergence that can be trained simultaneously. Compared with the state-of-art, our initial results are promising. Other than improving the results with the current model and testing on other datasets, our preliminary work has two clear future directions: (i) Taking our convex framework to other sets of problems such as semi-supervised learning, similarity learning, structured prediction, etc.(ii) Following different approaches to satisfy and measure convexity such as modifying the constraint or the optimization algorithm itself. In this paper we proposed an alternative method for few shot learning. Our method is based on two jointly learnable functions: a nonlinear embedding function followed by a metric learning function. Our metric function learns a suitable Bregman divergence via approximating a convex function, with a motivation that using Bregman divergences allows the mean to be the most representative point for the relevant class. We achieve comparable preliminary results sufficient to validate their potential.We plan to further investigate our model and do more extensive parameter and architecture search to improve our results. We can utilize different constraints or optimization methods to satisfy convexity, or apply an alternating training between embedding and metric function. We can also carry our method to other problems that contains convexity such as semi-supervised learning and structured prediction."
}