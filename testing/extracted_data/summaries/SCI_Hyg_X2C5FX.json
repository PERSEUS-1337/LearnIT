{
    "title": "Hyg_X2C5FX",
    "content": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\n In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models. Generative Adversarial Networks (GANs) BID11 have been able to produce photorealistic images, often indistinguishable from real images. This remarkable ability has powered many real-world applications ranging from visual recognition BID35 , to image manipulation , to video prediction . Since their invention in 2014, many GAN variants have been proposed BID29 BID41 , often producing more realistic and diverse samples with better training stability.Despite this tremendous success, many questions remain to be answered. For example, to produce a church image (Figure 1a) , what knowledge does a GAN need to learn? Alternatively, when a GAN sometimes produces terribly unrealistic images (Figure 1f) , what causes the mistakes? Why does one GAN variant work better than another? What fundamental differences are encoded in their weights?In this work, we study the internal representations of GANs. To a human observer, a well-trained GAN appears to have learned facts about the objects in the image: for example, a door can appear on a building but not on a tree. We wish to understand how a GAN represents such structure. Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive? If the GAN does contain variables for doors and trees, do those variables cause the generation of those objects, or do they merely correlate? How are relationships between objects represented? Figure 1: Overview: (a) Realistic outdoor church images generated by Progressive GANs BID18 . (b) Given a pre-trained GAN model, we identify a set of interpretable units whose featuremap is correlated to an object class across different images. For example, one unit in layer4 localizes tree regions with diverse visual appearance. (c) We force the activation of the units to be zero and quantify the average casual effect of the ablation. Here we successfully remove trees from church images. (d) We activate tree causal units in other locations. These same units synthesize new trees, visually compatible with their surrounding context. In addition, our method can diagnose and improve GANs by identifying artifact-causing units (e). We can remove the artifacts that appear (f) and significantly improve the results by ablating the \"artifact\" units (g). Please see our demo video.We present a general method for visualizing and understanding GANs at different levels of abstraction, from each neuron, to each object, to the contextual relationship between different objects. We first identify a group of interpretable units that are related to object concepts ( Figure 1b ). These units' featuremaps closely match the semantic segmentation of a particular object class (e.g., trees). Second, we directly intervene within the network to identify sets of units that cause a type of objects to disappear (Figure 1c) or appear ( Figure 1d ). We quantify the causal effect of these units using a standard causality metric. Finally, we examine the contextual relationship between these causal object units and the background. We study where we can insert object concepts in new images and how this intervention interacts with other objects in the image (Figure 1d ). To our knowledge, our work provides the first systematic analysis for understanding the internal representations of GANs.Finally, we show several practical applications enabled by this analytic framework, from comparing internal representations across different layers, GAN variants and datasets; to debugging and improving GANs by locating and ablating \"artifact\" units ( Figure 1e) ; to understanding contextual relationships between objects in scenes; to manipulating images with interactive object-level control. By carefully examining representation units, we have found that many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output. These interpretable effects can be used to compare, debug, modify, and reason about a GAN model. Our method can be potentially applied to other generative models such as VAEs BID20 and RealNVP BID7 .We have focused on the generator rather than the discriminator (as did in BID29 ) because the generator must represent all the information necessary to approximate the target distribution, while the discriminator only learns to capture the difference between real and fake images. Alternatively , we conference room church living room kitchen bedroom Figure 10 : Comparing the effect of ablating 20 window-causal units in GANs trained on five scene categories. In each case , the 20 ablated units are specific to the class and the generator and independent of the image. In some scenes , windows are reduced in size or number rather than eliminated, or replaced by visually similar objects such as paintings. DISPLAYFORM0 Figure 11: Inserting door units by setting 20 causal units to a fixed high value at one pixel in the representation. Whether the door units can cause the generation of doors is dependent on its local context: we highlight every location that is responsive to insertions of door units on top of the original image, including two separate locations in (b) (we intervene at left). The same units are inserted in every case, but the door that appears has a size, alignment, and color appropriate to the location. Emphasizing a door that is already present results in a larger door (d). The chart summarizes the causal effect of inserting door units at one pixel with different contexts.can train an encoder to invert the generator BID8 . However, this incurs additional complexity and errors. Many GANs also do not have an encoder.Our method is not designed to compare the quality of GANs to one another, and it is not intended as a replacement for well-studied GAN metrics such as FID, which estimate realism by measuring the distance between the generated distribution of images and the true distribution BID2 surveys these methods). Instead, our goal has been to identify the interpretable structure and provide a window into the internal mechanisms of a GAN.Prior visualization methods BID40 BID1 BID17 have brought new insights into CNN and RNN research. Motivated by that, in this work we have taken a small step towards understanding the internal representations of a GAN, and we have uncovered many questions that we cannot yet answer with the current method. For example: why can a door not be inserted in the sky? How does the GAN suppress the signal in the later layers? Further work will be needed to understand the relationships between layers of a GAN. Nevertheless, we hope that our work can help researchers and practitioners better analyze and develop their own GANs. In Section 4.2, we have improved GANs by manually identifying and ablating artifact-causing units. Now we describe an automatic procedure to identify artifact units using unit-specific FID scores.To compute the FID score BID13 for a unit u, we generate 200, 000 images and select the 10, 000 images that maximize the activation of unit u, and this subset of 10, 000 images is compared to the true distribution (50, 000 real images) using FID. Although every such unit-maximizing subset of images represents a skewed distribution, we find that the per-unit FID scores fall in a wide range, with most units scoring well in FID while a few units stand out with bad FID scores: many of them were also manually flagged by humans, as they tend to activate on images with clear visible artifacts. FIG1 shows the performance of FID scores as a predictor of manually flagged artifact units. The per-unit FID scores can achieve 50% precision and 50% recall. That is, of the 20 worst-FID units, 10 are also among the 20 units manually judged to have the most noticeable artifacts. Furthermore, repairing the model by ablating the highest-FID units works: qualitative results are shown in FIG8 and quantitative results are shown in TAB4 . (a) unit118 in layer4 DISPLAYFORM1 Figure 14: Two examples of generator units that our dissection method labels differently from humans. Both units are taken from layer4 of a Progressive GAN of living room model. In (a), human label the unit as 'sofa' based on viewing the top-20 activating images, and our method labels as 'ceiling'. In this case, our method counts many ceiling activations in a sample of 1000 images beyond the top 20. In (b), the dissection method has no confident label prediction even though the unit consistently triggers on white letterbox shapes at the top and bottom of the image. The segmentation model we use has no label for such abstract shapes."
}