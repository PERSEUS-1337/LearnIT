{
    "title": "rkh-agjMG",
    "content": "Prospection is an important part of how humans come up with new task plans, but has not been explored in depth in robotics. Predicting multiple task-level is a challenging problem that involves capturing both task semantics and continuous variability over the state of the world. Ideally, we would combine the ability of machine learning to leverage big data for learning the semantics of a task, while using techniques from task planning to reliably generalize to new environment. In this work, we propose a method for learning a model encoding just such a representation for task planning. We learn a neural net that encodes the k most likely outcomes from high level actions from a given world. Our approach creates comprehensible task plans that allow us to predict changes to the environment many time steps into the future. We demonstrate this approach via application to a stacking task in a cluttered environment, where the robot must select between different colored blocks while avoiding obstacles, in order to perform a task. We also show results on a simple navigation task. Our algorithm generates realistic image and pose predictions at multiple points in a given task.\n How can we allow robots to plan as humans do? Humans are masters at solving problems. When attempting to solve a difficult problem, we can picture what effects our actions will have, and what the consequences will be. Some would say this act -the act of prospection -is the essence of intelligence BID26 .Consider the task of stacking a series of colored blocks in a particular pattern as explored in prior work BID8 BID35 . A traditional planner would view this as a sequence of high-level actions, such as pickup(block), place(block,on block), and so on. The planner will then decide which object gets picked up and in which order. Such tasks are often described using a formal language such as the Planning Domain Description Language (PDDL) BID12 . To execute such a task on a robot, specific goal conditions and cost functions must be defined, and the preconditions and effects of the each action must be specified -which is a large and time consuming undertaking BID3 . Humans, on the other hand , do not require that all of this information be given beforehand. We can learn models of task structure purely from observation or demonstration. We work directly with high dimensional data such as images, and can reason over complex paths without being given an explicit structure.As a result, there has been much interest in learning prospective models for planning and action. Deep generative models such as conditional GANS BID15 or multiple-hypothesis models for image prediction BID25 BID5 allow us to generate realistic future scenes. In addition, a recent line of work in robotics focuses on making structured prediction BID10 a) ; BID4 proposed SE3-nets, which predict object motion masks and six degree of freedom movement for each object; BID11 predict trajectories to move to intermediate goals. However, so far these approaches focus on making relatively short-term predictions, and do not take into account variability in the ways a task can be performed in a stochastic world.In general, deep policy learning has proven successful at learning well-scoped, short horizon robotic tasks BID29 BID9 . Recent work on one-shot imitation learning learned general-purpose models for manipulating blocks, but relies on a task solution fromFigure 1: A simple stacking task including an obstacle that must be avoided. The robot must decide which blocks to pick up and move, and which block to put them on, taking into account its workspace and the obstacle. The right side shows how predictions change as the robot moves. a human expert and does not generate prospective future plans for reliable performance in new environments BID8 BID35 . These are very data intensive as a result : BID8 used 140,000 demonstrations.Instead, we propose a model that learns this high level task structure and uses it to generate interpretable task plans by predicting sequences of movement goals. These movement goals can then be connected via traditional trajectory optimization or motion planning approaches that can operate on depth data without a semantic understanding of the world. Fig. 1 shows the task as well as predictions resulting from our algorithm at different stages.To summarize, our contributions are:\u2022 Approach for learning a predictive model over world state transitions from a large supervised dataset, suitable for task planning.\u2022 Analysis of the parameters that make such learning feasible in a stochastic world.\u2022 Experimental results from a simulated navigation and a block-stacking domain. We described an approach for learning a predictive model that can be used to generate interpretable task plans for robot execution. This model supports complex tasks, and requires only minimal labeling. It can also be applied to many different domains with minimal adaptation. We also provided an analysis of how to create and train models for this problem domain, and describe the validation of the final model architecture in a range of different domains.Still, there are clear avenues for improvement in future work. First, in this work we assume that lowlevel \"actor\" policies are provided. This is sufficient for most manipulation and navigation tasks, but may not capture complex object interactions. Second, we assume the existence of mid-level supervisory labels in this work to extract change-points. In the future, we would prefer to detect change-points automatically using an approach such as that proposed by BID21 . Finally, we plan to expand this method into a full planning algorithm using predicted value and action priors that operates on sensor data."
}