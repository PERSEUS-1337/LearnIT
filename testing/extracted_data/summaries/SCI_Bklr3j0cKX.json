{
    "title": "Bklr3j0cKX",
    "content": "This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals. One core objective of deep learning is to discover useful representations, and the simple idea explored here is to train a representation-learning function, i.e. an encoder, to maximize the mutual information (MI) between its inputs and outputs. MI is notoriously difficult to compute, particularly in continuous and high-dimensional settings. Fortunately, recent advances enable effective computation of MI between high dimensional input/output pairs of deep neural networks (Belghazi et al., 2018) . We leverage MI estimation for representation learning and show that, depending on the downstream task, maximizing MI between the complete input and the encoder output (i.e., global MI) is often insufficient for learning useful representations. Rather, structure matters: maximizing the average MI between the representation and local regions of the input (e.g. patches rather than the complete image) can greatly improve the representation's quality for, e.g., classification tasks, while global MI plays a stronger role in the ability to reconstruct the full input given the representation.Usefulness of a representation is not just a matter of information content: representational characteristics like independence also play an important role (Gretton et al., 2012; Hyv\u00e4rinen & Oja, 2000; Hinton, 2002; Schmidhuber, 1992; Bengio et al., 2013; Thomas et al., 2017) . We combine MI maximization with prior matching in a manner similar to adversarial autoencoders (AAE, Makhzani et al., 2015) to constrain representations according to desired statistical properties. This approach is closely related to the infomax optimization principle (Linsker, 1988; Bell & Sejnowski, 1995) , so we call our method Deep InfoMax (DIM). Our main contributions are the following:\u2022 We formalize Deep InfoMax (DIM), which simultaneously estimates and maximizes the mutual information between input data and learned high-level representations.\u2022 Our mutual information maximization procedure can prioritize global or local information, which we show can be used to tune the suitability of learned representations for classification or reconstruction-style tasks.\u2022 We use adversarial learning (\u00e0 la Makhzani et al., 2015) to constrain the representation to have desired statistical characteristics specific to a prior.\u2022 We introduce two new measures of representation quality, one based on Mutual Information Neural Estimation (MINE, Belghazi et al., 2018 ) and a neural dependency measure (NDM) based on the work by Brakel & Bengio (2017) , and we use these to bolster our comparison of DIM to different unsupervised methods. In this work, we introduced Deep InfoMax (DIM), a new method for learning unsupervised representations by maximizing mutual information, allowing for representations that contain locally-consistent information across structural \"locations\" (e.g., patches in an image). This provides a straightforward and flexible way to learn representations that perform well on a variety of tasks. We believe that this is an important direction in learning higher-level representations. Here we show the relationship between the Jensen-Shannon divergence (JSD) between the joint and the product of marginals and the pointwise mutual information (PMI). Let p(x ) and p(y ) be two marginal densities, and define p(y|x) and p(x, y ) = p(y|x)p(x ) as the conditional and joint distribution, respectively. Construct a probability mixture density, m(x, y) = 1 2 (p(x)p(y) + p(x, y)). It follows that m(x) = p(x), m(y) = p(y), and m(y|x ) = 1 2 (p(y) + p(y|x)). Note that : DISPLAYFORM0 Discarding some constants: DISPLAYFORM1 The quantity inside the expectation of Eqn. 10 is a concave, monotonically increasing function of the ratio p(y|x) p(y) , which is exactly e PMI(x, y) . Note this relationship does not hold for the JSD of arbitrary distributions, as the the joint and product of marginals are intimately coupled.We can verify our theoretical observation by plotting the JSD and KL divergences between the joint and the product of marginals, the latter of which is the formal definition of mutual information (MI). As computing the continuous MI is difficult, we assume a discrete input with uniform probability, p(x) (e.g., these could be one-hot variables indicating one of N i.i.d. random samples), and a randomly initialized N \u00d7 M joint distribution, p(x, y), such that M j=1 p(x i , y j ) = 1 \u2200i. For this joint distribution, we sample from a uniform distribution, then apply dropout to encourage sparsity to simulate the situation when there is no bijective function between x and y, then apply a softmax. As the distributions are discrete, we can compute the KL and JSD between p(x, y) and p(x)p(y).We ran these experiments with matched input / output dimensions of 8, 16, 32, 64, and 128, randomly drawing 1000 joint distributions, and computed the KL and JSD divergences directly. Our results ( Figure A.1) indicate that the KL (traditional definition of mutual information) and the JSD have an approximately monotonic relationship. Overall, the distributions with the highest mutual information also have the highest JSD."
}