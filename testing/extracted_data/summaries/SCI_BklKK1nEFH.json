{
    "title": "BklKK1nEFH",
    "content": "Transforming one probability distribution to another is a powerful tool in Bayesian inference and machine learning. Some prominent examples are constrained-to-unconstrained transformations of distributions for use in Hamiltonian Monte-Carlo and constructing flexible and learnable densities such as normalizing flows. We present Bijectors.jl, a software package for transforming distributions implemented in Julia, available at github.com/TuringLang/Bijectors.jl. The package provides a flexible and composable way of implementing transformations of distributions without being tied to a computational framework. \n\n We demonstrate the use of Bijectors.jl on improving variational inference by encoding known statistical dependencies into the variational posterior using normalizing flows, providing a general approach to relaxing the mean-field assumption usually made in variational inference. When working with probability distributions in Bayesian inference and probabilistic machine learning, transforming one probability distribution to another comes up quite often. For example, when applying Hamiltonian Monte Carlo on constrained distributions, the constrained density is usually transformed to an unconstrained density for which the sampling is performed (Neal, 2012) . Another example is to construct highly flexible and learnable densities often referred to as normalizing flows (Dinh et al., 2014; Huang et al., 2018; Durkan et al., 2019) ; for a review see Kobyzev et al. (2019) . When a distribution P is transformed into some other distribution Q using some measurable function b, we write Q = b * P and say Q is the push-forward of P . When b is a differentiable bijection with a differentiable inverse, i.e. a diffeomorphism or a bijector (Dillon et al., 2017) , the induced or pushed-forward distribution Qit is obtained by a simple application of change of variables. Specifically, given a distribution P on some \u2126 \u2286 R d with density p : \u2126 \u2192 [0, \u221e), and a bijector b : \u2126 \u2192\u03a9 for some\u03a9 \u2286 R d , the induced or pushed forward distribution Q = b * P has density q(y ) = p b \u22121 ( y) |det J b \u22121 (y)| or q b(x ) = p(x ) |det J b (x)| We presented Bijectors.jl, a framework for working with bijectors and thus transformations of distributions. We then demonstrated the flexibility of Bijectors.jl in an application of introducing correlation structure to the mean-field ADVI approach. We believe Bijectors.jl will be a useful tool for future research, especially in exploring normalizing flows and their place in variational inference. An interesting note about the NF variational posterior we constructed is that it only requires a constant number of extra parameters on top of what is required by mean-field normal VI. This approach can be applied in more general settings where one has access to the directed acyclic graph (DAG) of the generative model we want to perform inference. Then this approach will scale linearly with the number of unique edges between random variables. It is also possible in cases where we have an undirected graph representing a model by simply adding a coupling in both directions. This would be very useful for tackling issues faced when using mean-field VI and would be of interest to explore further. For related work we have mainly compared against Tensorflow's tensorflow probability, which is used by other known packages such pymc4, and PyTorch's torch.distributions, which is used by packages such as pyro. Other frameworks which make heavy use of such transformations using their own implementations are stan, pymc3, and so on. But in these frameworks the transformations are mainly used to transform distributions from constrained to unconstrained and vice versa with little or no integration between those transformation and the more complex ones, e.g. normalizing flows. pymc3 for example support normalizing flows, but treat them differently from the constrained-to-unconstrained transformations. This means that composition between standard and parameterized transformations is not supported. Of particular note is the bijectors framework in tensorflow probability introduced in (Dillon et al., 2017) . One could argue that this was indeed the first work to take such a drastic approach to the separation of the determinism and stochasticity, allowing them to implement a lot of standard distributions as a TransformedDistribution. This framework was also one of the main motivations that got the authors of Bijectors.jl interested in making a similar framework in Julia. With that being said, other than the name, we have not set out to replicate tensorflow probability and most of the direct parallels were observed after-the-fact, e.g. a transformed distribution is defined by the TransformedDistribution type in both frameworks. Instead we believe that Julia is a language well-suited for such a framework and therefore one can innovate on the side of implementation. For example in Julia we can make use of code-generation or meta-programming to do program transformations in different parts of the framework, e.g. the composition b \u2022 b \u22121 is transformed into the identity function at compile time."
}