{
    "title": "BJE-4xW0W",
    "content": "We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions. We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph. We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young. We preserve the dependency structure between the labels with a given causal graph. We devise a two-stage procedure for learning a CiGM over the labels and the image. First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels. Later, we combine this with a conditional GAN to generate images conditioned on the binary labels. We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN. We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels. The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image. We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset. An implicit generative model BID7 ) is a mechanism that can sample from a probability distribution without an explicit parameterization of the likelihood. Generative adversarial networks (GANs) arguably provide one of the most successful ways to train implicit generative models. GANs are neural generative models that can be trained using backpropagation to sample from very high dimensional nonparametric distributions (Goodfellow et al. (2014) ). A generator network models the sampling process through feedforward computation given a noise vector. The generator output is constrained and refined through feedback by a competitive adversary network, called the discriminator, that attempts to distinguish between the generated and real samples. The objective of the generator is to maximize the loss of the discriminator (convince the discriminator that it outputs samples from the real data distribution). GANs have shown tremendous success in generating samples from distributions such as image and video BID20 ).An extension of GANs is to enable sampling from the class conditional data distributions by feeding class labels to the generator alongside the noise vectors. Various neural network architectures have been proposed for solving this problem BID6 ; BID10 ; Antipov et al. Figure 1: Observational and interventional samples from CausalBEGAN. Our architecture can be used to sample not only from the joint distribution (conditioned on a label) but also from the interventional distribution, e.g., under the intervention do(M ustache = 1). The two distributions are clearly different since P(M ale = 1|M ustache = 1) = 1 and P(Bald = 1|M ale = 0) = 0 in the data distribution P.(2017)). However, these architectures do not capture the dependence between the labels. Therefore, they do not have a mechanism to sample images given a subset of the labels, since they cannot sample the remaining labels. In this paper, we are interested in extending the previous work on conditional image generation by i) capturing the dependence between labels and ii) capturing the causal effect between labels. We can think of conditional image generation as a causal process: Labels determine the image distribution. The generator is a non-deterministic mapping from labels to images. This is consistent with the causal graph \"Labels cause the Image\", denoted by L \u2192 I, where L is the random vector for labels and I is the image random variable. Using a finer model, we can also include the causal graph between the labels, if available.As an example, consider the causal graph between Gender (G) and Mustache (M ) labels. The causal relation is clearly Gender causes Mustache , denoted by the graph G \u2192 M . Conditioning on Gender = male, we expect to see males with or without mustaches, based on the fraction of males with mustaches in the population. When we condition on Mustache = 1, we expect to sample from males only since the population does not contain females with mustaches. In addition to sampling from conditional distributions , causal models allow us to sample from various different distributions called interventional distributions. An intervention is an experiment that fixes the value of a variable in a causal graph. This affects the distributions of the descendants of the intervened variable in the graph. But unlike conditioning, it does not affect the distribution of its ancestors. For the same causal graph, intervening on Mustache = 1 would not change the distribution of Gender. Accordingly, the label combination (Gender = female, Mustache = 1) would appear as often as Gender = female after the intervention. Please see Figure 1 for some of our conditional and interventional samples, which illustrate this concept on the Bald and Mustache variables.In this work we propose causal implicit generative models (CiGM): mechanisms that can sample not only from the correct joint probability distributions but also from the correct conditional and interventional probability distributions. Our objective is not to learn the causal graph: we assume that the true causal graph is given to us. We show that when the generator structure inherits its neural connections from the causal graph, GANs can be used to train causal implicit generative models. We use Wasserstein GAN (WGAN) (Arjovsky et al. (2017) ) to train a CiGM for binary image labels, as the first step of a two-step procedure for training a CiGM for the images and image labels. For the second step, we propose two novel conditional GANs called CausalGAN and CausalBEGAN. We show that the optimal generator of CausalGAN can sample from the true conditional distributions (see Theorem 1).We show that combining CausalGAN with a CiGM on the labels yields a CiGM on the labels and the image, which is formalized in Corollary 1 in Section 5. Our contributions are as follows:\u2022 We observe that adversarial training can be used after structuring the generator architecture based on the causal graph to train a CiGM. We empirically show that WGAN can be used to learn a CiGM that outputs essentially discrete 1 labels, creating a CiGM for binary labels.\u2022 We consider the problem of conditional and interventional sampling of images given a causal graph over binary labels. We propose a two-stage procedure to train a CiGM over the binary labels and the image. As part of this procedure, we propose a novel conditional GAN architecture and loss function. We show that the global optimal generator provably samples from the class conditional distributions.\u2022 We propose a natural but nontrivial extension of BEGAN to accept labels: using the same motivations for margins as in BEGAN (Berthelot et al. (2017) ), we arrive at a \"margin of margins\" term. We show empirically that this model, which we call CausalBEGAN, produces high quality images that capture the image labels.\u2022 We evaluate our CiGM training framework on the labeled CelebA data BID2 ).We empirically show that CausalGAN and CausalBEGAN can produce label-consistent images even for label combinations realized under interventions that never occur during training, e.g., \"woman with mustache\" 2 . We proposed a novel generative model with label inputs. In addition to being able to create samples conditioned on labels, our generative model can also sample from the interventional distributions. Our theoretical analysis provides provable guarantees about correct sampling under such interventions.Top: Intervene Narrow Eyes=1, Bottom: Condition Narrow Eyes=1Figure 7: Intervening/Conditioning on Narrow Eyes label in CelebA Causal Graph with CausalBEGAN. Since Smiling \u2192 Narrow Eyes in CelebA Causal Graph, we do not expect do(Narrow Eyes = 1) to affect the probability of Smiling = 1, i.e., P(Smiling = 1|do(Narrow Eyes = 1)) = P(Smiling = 1) = 0.48. However on the bottom row, conditioning on Narrow Eyes = 1 increases the proportion of smiling images (From 0.48 to 0.59 in the dataset), although 10 images may not be enough to show this difference statistically. As a rare artifact, in the dark image in the third column the generator appears to rule out the possibility of Narrow Eyes = 0 instead of demonstrating Narrow Eyes = 1.Causality leads to generative models that are more creative since they can produce samples that are different from their training samples in multiple ways. We have illustrated this point for two models (CausalGAN and CausalBEGAN)."
}