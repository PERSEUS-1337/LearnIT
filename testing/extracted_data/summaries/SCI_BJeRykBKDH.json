{
    "title": "BJeRykBKDH",
    "content": "Through many recent advances in graph representation learning, performance achieved on tasks involving graph-structured data has substantially increased in recent years---mostly on tasks involving node-level predictions. The setup of prediction tasks over entire graphs (such as property prediction for a molecule, or side-effect prediction for a drug), however, proves to be more challenging, as the algorithm must combine evidence about several structurally relevant patches of the graph into a single prediction.\n Most prior work attempts to predict these graph-level properties while considering only one graph at a time---not allowing the learner to directly leverage structural similarities and motifs across graphs. Here we propose a setup in which a graph neural network receives pairs of graphs at once, and extend it with a co-attentional layer that allows node representations to easily exchange structural information across them. We first show that such a setup provides natural benefits on a pairwise graph classification task (drug-drug interaction prediction), and then expand to a more generic graph regression setup: enhancing predictions over QM9, a standard molecular prediction benchmark. Our setup is flexible, powerful and makes no assumptions about the underlying dataset properties, beyond anticipating the existence of multiple training graphs. We study the task of graph-level representation learning: i.e., computing representations of entire input graphs, for the purposes of downstream tasks (such as graph classification or regression). This is typically a step-up in complexity compared to node classification or link prediction, given that the learning algorithm must aggregate useful structural information across the graph into a single prediction-relying on only this global supervision signal (as opposed to having feedback from every node/edge of the graph). Perhaps the highest challenge this kind of architecture must face is inductivity and generalisation across structures. Specifically, an inductive model must be readily applicable across several graph structures-including ones unseen during training. Additionally, the model is tasked with discovering interesting structural \"motifs\" across the entire dataset of graphs, whose presence or absence may help determine the overall predictions. However, even enabling inductivity is not a traditionally simple task in graph representation learning, as many prior approaches (Bruna et al., 2013; Perozzi et al., 2014; Defferrard et al., 2016) are not inductive by design. Furthermore, even the models that are currently used for graph-level representation learning; e.g. Gilmer et al. (2017) ; ; Xu et al. (2018) ; Lu et al. (2019) , operate over only a single graph at a time-making it challenging for them to reason about common substructures across graphs from a graph-level supervision signal alone. In this manuscript, we propose the approach of paired training-i.e., learning representations over pairs of input graphs at once. Intuitively, as long as we allow for dataflow between the representations of the two graphs within a pair, this allows the graph neural network to directly observe related (sub)structures from other inputs, to solidify its decision making. We note that in the context of graph-structured inputs this may be particularly useful as, unlike simpler inputs such as images or text, there are no guarantees that different graphs within a dataset will have equal or even similar overall structure. To facilitate this dataflow, we propose the usage of graph co-attention for exchanging representations of nodes across the two graphs. Intuitively, this operator performs attention (Bahdanau et al., 2014; Vaswani et al., 2017) over the fully-connected bipartite graph, with one part corresponding to all nodes in one graph. This allows every node of the first graph to detect and reuse useful patch representations in the second graph (in a form of hierarchical graph matching), and vice-versa. Initially, we validate our model performance on a pairwise graph classification task-classifying drug pairs for side effects caused by drug-drug interactions (DDI) (Jin et al., 2017; Zitnik et al., 2018) . In this setting, a pairwise approach is natural, as we inherently have to classify pairs of graphs. We demonstrate that learning a joint representation using graph co-attention provides substantial benefits to predictive power, setting the state-of-the-art result on this task. From there, we demonstrate the applicability of our approach to arbitrary multi-graph datasets; for this, we leverage the QM9 dataset for predicting quantum chemistry properties of small molecules (Ramakrishnan et al., 2014) . As such, it represents a challenging graph regression problem. We propose using paired training to perform regression on two molecules at once, demonstrating clear benefits to doing so. In a similar vein, we execute variants of our model on standard graph kernel classification benchmarks (Kersting et al., 2016) , showing advantages to generic graph classification. Our approach paves the way to a promising direction for graph-level prediction tasks, that is in principle applicable to any kind of multi-graph dataset, especially under availability of large quantities of labelled examples. Our model builds up on a large existing body of work in graph convolutional networks (Bruna et al., 2013; Defferrard et al., 2016; Kipf & Welling, 2016a; Gilmer et al., 2017; , that have substantially advanced the state-of-the-art in many tasks requiring graph-structured input processing (such as the chemical representation (Gilmer et al., 2017; De Cao & Kipf, 2018; of the drugs leveraged here). Furthermore, we build up on work proposing co-attention (Lu et al., 2016; Deac et al., 2018) as a mechanism to allow for individual set-structured datasets (such as nodes in multimodal graphs) to interact. Specifically, such mechanisms have already been used for explicit matching of graph structure motifs (Li et al., 2019) , and therefore represent a natural methodology for our purposes. Overall, these (and related) techniques lie within the domain of graph representation learning, one of the latest major challenges of machine learning (Bronstein et al., 2017; Hamilton et al., 2017; Battaglia et al., 2018) , with transformative potential across a wide spectrum of potential applications, extending outside the biochemical domain. We have presented a novel way of training graph neural networks on multi-graph datasets, relying on making predictions jointly, in pairs of graphs-the paired training approach. Additionally, we allowed for arbitrary representation exchange between these graphs by way of co-attentive mechanisms. The two combined allow for extraction of stronger and more robust representations as opposed to single-graph learning, which is a claim we verified across several established molecular prediction tasks: polypharmacy side effect prediction (where we set a state-of-the-art result), quantum chemistry properties prediction and graph classification. As a flexible and generic approach which doesn't rely on dataset properties in any way, so long as it consists of multiple graphs, we believe it to be a useful direction to explore for graph representation learning as a whole."
}