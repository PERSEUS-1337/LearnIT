{
    "title": "ryacTMZRZ",
    "content": "Computing distances between examples is at the core of many learning algorithms for time series. Consequently, a great deal of work has gone into designing effective time series distance measures. We present Jiffy, a simple and scalable distance metric for multivariate time series. Our approach is to reframe the task as a representation learning problem---rather than design an elaborate distance function, we use a CNN to learn an embedding such that the Euclidean distance is effective. By aggressively max-pooling and downsampling, we are able to construct this embedding using a highly compact neural network. Experiments on a diverse set of multivariate time series datasets show that our approach consistently outperforms existing methods. Measuring distances between examples is a fundamental component of many classification, clustering, segmentation and anomaly detection algorithms for time series BID38 BID43 BID13 . Because the distance measure used can have a significant effect on the quality of the results, there has been a great deal of work developing effective time series distance measures BID18 BID28 BID1 BID15 . Historically, most of these measures have been hand-crafted. However, recent work has shown that a learning approach can often perform better than traditional techniques BID16 BID33 BID9 .We introduce a metric learning model for multivariate time series. Specifically , by learning to embed time series in Euclidean space, we obtain a metric that is both highly effective and simple to implement using modern machine learning libraries. Unlike many other deep metric learning approaches for time series, we use a convolutional, rather than a recurrent, neural network, to construct the embedding. This choice , in combination with aggressive maxpooling and downsampling, results in a compact, accurate network.Using a convolutional neural network for metric learning per se is not a novel idea BID35 BID45 ; however, time series present a set of challenges not seen together in other domains, and how best to embed them is far from obvious. In particular , time series suffer from:1. A lack of labeled data. Unlike text or images , time series cannot typically be annotated post-hoc by humans. This has given rise to efforts at unsupervised labeling BID4 , and is evidenced by the small size of most labeled time series datasets. Of the 85 datasets in the UCR archive BID10 , for example, the largest dataset has fewer than 17000 examples, and many have only a few hundred. 2. A lack of large corpora. In addition to the difficulty of obtaining labels, most researchers have no means of gathering even unlabeled time series at the same scale as images, videos, or text. Even the largest time series corpora, such as those on Physiobank BID19 , are tiny compared to the virtually limitless text, image, and video data available on the web. 3. Extraneous data. There is no guarantee that the beginning and end of a time series correspond to the beginning and end of any meaningful phenomenon. I.e., examples of the class or pattern of interest may take place in only a small interval within a much longer time series. The rest of the time series may be noise or transient phenomena between meaningful events BID37 BID21 .4. Need for high speed. One consequence of the presence of extraneous data is that many time series algorithms compute distances using every window of data within a time series BID34 BID4 BID37 . A time series of length T has O(T ) windows of a given length, so it is essential that the operations done at each window be efficient.As a result of these challenges, an effective time series distance metric must exhibit the following properties:\u2022 Efficiency: Distance measurement must be fast, in terms of both training time and inference time.\u2022 Simplicity: As evidenced by the continued dominance of the Dynamic Time Warping (DTW) distance BID42 in the presence of more accurate but more complicated rivals, a distance measure must be simple to understand and implement.\u2022 Accuracy: Given a labeled dataset, the metric should yield a smaller distance between similarly labeled time series. This behavior should hold even for small training sets .Our primary contribution is a time series metric learning method, Jiffy, that exhibits all of these properties: it is fast at both training and inference time, simple to understand and implement, and consistently outperforms existing methods across a variety of datasets.We introduce the problem statement and the requisite definitions in Section 2. We summarize existing state-of-the-art approaches (both neural and non-neural) in Section 3 and go on to detail our own approach in Section 4. We then present our results in Section 5. The paper concludes with implications of our work and avenues for further research. We present Jiffy, a simple and efficient metric learning approach to measuring multivariate time series similarity. We show that our method learns a metric that leads to consistent and accurate classification across a diverse range of multivariate time series. Jiffy's resilience to hyperparameter choices and consistent performance across domains provide strong evidence for its utility on a wide range of time series datasets.Future work includes the extension of this approach to multi-label classification and unsupervised learning. There is also potential to further increase Jiffy's speed by replacing the fully connected layer with a structured BID6 or binarized BID39"
}