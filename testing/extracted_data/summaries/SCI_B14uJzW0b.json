{
    "title": "B14uJzW0b",
    "content": "Deep learning models can be efficiently optimized via stochastic gradient descent, but there is little theoretical evidence to support this. A key question in optimization is to understand when the optimization landscape of a neural network is amenable to gradient-based optimization. We focus on a simple neural network two-layer ReLU network with two hidden units, and show that all local minimizers are global. This combined with recent work of Lee et al. (2017); Lee et al. (2016) show that  gradient descent converges to the global minimizer. In this paper, we provided recovery guarantee of stochastic gradient descent with random initialization for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation functions and Gaussian inputs. Experiments are also done to verify our results. For future work, here we list some possible directions. In conclusion, based on the assumption that \u03b8 1 \u2264 \u03b8 2 there are four critical points in the 2D case: Assume the manifold is R = {(w 1 , w 2 ) : w 1 2 = w 2 2 = 1}, then the Hessian on the manifold is DISPLAYFORM0 DISPLAYFORM1 where z = (z 1 , z 2 ) satisfies w DISPLAYFORM2 and DISPLAYFORM3 Then we can get when w 1 = w 2 and w 1 = \u2212w 2 , DISPLAYFORM4 So this point is a saddle point. In conclusion, we have four critical points: one is global maximal, the other three are saddle points. DISPLAYFORM0 ) is a critical point, then there exists a set of standard orthogonal basis (e 1 , e 2 , e 3 ) such that e 1 = w * 1 , e 2 = w * 2 and w 1 , w 2 lies in span{e 1 , e 2 , e 3 }.Proof . If ( w 1 , w 2 ) is a critical point, then DISPLAYFORM1 where matrix (I \u2212 w 1 w T 1 ) projects a vector onto the tangent space of w 1 . Since DISPLAYFORM2 we get DISPLAYFORM3 DISPLAYFORM4 )w * 2 lies in the direction of w 1 . If \u03b8 w1,w2 = \u03c0, i.e., w 1 = \u2212w 2 , then of course the four vectors have rank at most 3, so we can find the proper basis. If \u03b8 w1,w2 < \u03c0, then we know that there exists a real number r such that DISPLAYFORM5 Since \u03b8 w1,w2 < \u03c0, we know that the four vectors w 1 , w 2 , w * 1 and w * 2 are linear dependent. Thus , they have rank at most 3 and we can find the proper basis."
}