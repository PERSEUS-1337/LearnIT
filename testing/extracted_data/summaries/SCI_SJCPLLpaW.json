{
    "title": "SJCPLLpaW",
    "content": "DeePa is a deep learning framework that explores parallelism in all parallelizable dimensions to accelerate the training process of convolutional neural networks. DeePa optimizes parallelism at the granularity of each individual layer in the network. We present an elimination-based algorithm that finds an optimal parallelism configuration for every layer. Our evaluation shows that DeePa achieves up to 6.5\u00d7 speedup compared to state-of-the-art deep learning frameworks and reduces data transfers by up to 23\u00d7. Training convolutional neural networks (CNNs) is increasingly compute-intensive and timeconsuming. It takes days or even weeks to train deep CNNs from scratch BID10 BID12 BID9 BID11 . Existing deep learning frameworks such as TensorFlow, PyTorch, and Caffe2 parallelize the training process onto multiple processors (usually GPUs) using image parallelism 1 dividing the entire image dataset into batches with the same number of images and assigning each batch to a dedicated processor.The standard parallelization of CNN training only exploits image parallelism. However, other dimensions can also parallelize the training process. For example, in CNNs for 2D images, data is commonly organized as 4-dimensional tensors (i.e., image, height, width, channel). The image dimension includes an index for each image in the input dataset. The height and width dimensions specify a position in an image. For a particular position, the channel dimension 2 indexes different neurons for that position. Exploring these other parallelizable dimensions can potentially reduce the compute time and data transfer cost when training CNNs (see Section 2). Moreover, different layers in a CNN may prefer different parallelism configurations for achieving optimal performance.We propose DeePa, a deep learning framework that explores parallelism in all parallelizable dimensions to accelerate the training of CNNs. To the best of our knowledge, DeePa is the first system that models and exploits the parallelism of neural networks at the granularity of each individual layer. To generate a parallelism configuration for each layer, DeePa uses an elimination-based algorithm that automatically finds the configuration with the best estimated performance.The main contributions of this paper are:\u2022 We present DeePa, a deep learning framework that explores parallelism in all parallelizable dimensions to accelerate the training of CNNs.\u2022 The parallelization strategy is selected at the granularity of each individual layer.\u2022 We present an elimination-based algorithm for finding the parallelism configuration with optimal estimated performance for each layer.\u2022 Our evaluation shows that, compared to state-of-the-art deep learning frameworks (e.g., TensorFlow and PyTorch), DeePa achieves 6.5\u00d7, 1.9\u00d7, and 1.5\u00d7 speedup for AlexNet, TAB0 Conv3x3 in an Inception-v3 module respectively . The performance improvement comes from reducing overall data transfers, automatically overlapping computation with data movement, and accelerating computation throughput. We have presented DeePa, a deep learning framework that explores parallelism in all parallelizable dimensions to accelerate the training of CNNs. DeePa optimizes the parallelism configuration chosen at the granularity of individual layers. DeePa achieves up to 6.5\u00d7 for training CNNs and reduces overall data transfers by up to 23\u00d7 compared to state-of-the-art deep learning frameworks.Proof. The Cost function is defined in Equation 1. Let g be any configuration. We first compute the difference between Cost(g, (V, E)) and Cost(g, (V , E )). DISPLAYFORM0 =w.compute(g(w )) + w.update(g(w )) DISPLAYFORM1 Now assume g is an optimal configuration for (V, E). Then we have w.compute(g(w)) + w.update(g(w)) + e 1 .xfer(g(u ), g(w )) + e 2 .xfer(g(w) , g(v) ) = min cw {w.compute(c w ) + w.update(c w ) + e 1 .xfer(g(u), c w ) + e 2 .xfer(c w , g(v))}Therefore , g is an optimal configuration of (V , E ). For the other direction, note that if g is an optimal configuration of (V , E ), then it can be extended to an optimal configuration of (V, E) by adding the node w with the same minimal assignment."
}