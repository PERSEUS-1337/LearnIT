{
    "title": "Byef4anQcE",
    "content": "Providing transparency of AI planning systems is crucial for their success in practical applications. In order to create a transparent system, a user must be able to query it for explanations about its outputs. We argue that a key underlying principle for this is the use of causality within a planning model, and that argumentation frameworks provide an intuitive representation of such causality. In this paper, we discuss how argumentation can aid in extracting causalities in plans and models, and how they can create explanations from them. Explainability of AI decision-making is crucial for increasing trust in AI systems, efficiency in human-AI teaming, and enabling better implementation into real-world settings. Explainable AI Planning (XAIP) is a field that involves explaining AI planning systems to a user. Approaches to this problem include explaining planner decision-making processes as well as forming explanations from the models. Past work on model-based explanations includes an iterative approach BID14 as well as using explanations for more intuitive communication with the user BID5 . With respect to human-AI teaming, the more helpful and illustrative the explanations, the better the performance of the system overall.Research into the types of questions and motivations a user might have includes work with contrastive questions BID9 . These questions are structured as 'Why F rather than G?', where F is some part (i.e. action(s) in a plan) of the original solution and G is something the user imagines to be better. While contrastive questions are useful, they do not consider the case when a user doesn't have something else in mind (i.e. G) or has a more general question about the model. This includes the scenario in which the user's understanding of the model is incomplete or inaccurate. Research in the area of model reconciliation attempts to address this knowledge gap BID1 .More broadly, questions such as 'Why A?', where A is an action in the plan, or 'How G?', where G is a (sub)goal, must be answerable and explainable. Questions like these are inherently based upon definitions held in the domain related to a particular problem and solution. The user's motivation behind such questions can vary: he could think the action is unnecessary, be unsure as to its effects, or think there is a better option. Furthermore , questions regarding particular state information may arise, such as 'Why A here?' and 'Why can't A go here?'. For these, explanations that include relevant state information would vastly improve their efficiency when communicating with a user BID9 . This is especially true for long plans, when a user does not have access to a domain, or the domain is too complex to be easily understood. Thus, extracting relevant information about action-state causality from the model is required.In the space of planning, causality underpins a variety of research areas including determining plan complexity BID6 and heuristics BID7 . Many planners also can create causal graph visualizations of plans for a user to interact with BID12 . The general structure of causality in planning is 'action causes state'. Indirectly, this can be seen as 'action enables action', where the intermediary state is sufficient for the second action to occur. Hilton describes different 'causal chains ' which mirror the types of causality found in planning; action-state causality can be identified as either a 'temporal' or 'unfolding' chain, while action-action causality is similar to an 'opportunity chain' BID8 . For now, we will focus on these two types of general causality.To represent the causality of a model, argumentation is a good candidate; as detailed by BID0 , argumentation frameworks and causal models can be viewed as two versions of one entity. A recent related work uses argumentation for explainable scheduling (Cyras et al. 2019) . We consider an ASPIC + (Modgil and Prakken 2013) style framework with defeasible rules capturing the relationships between actions in a plan and strict rules capturing actionstate causality. This structure allows more than a causal representation of a plan; it allows multiple types of causality to be distinguished and different causal 'chunks' to be created and combined to be used as justification for explanations.In this paper we present an initial approach for using argumentation to represent causality, which can then be used to form more robust explanations. In the following sections, a motivating scenario will be introduced and used to showcase our current approaches of abstracting causalities and state information into argumentation frameworks.Consider a simple logistics scenario in which three trucks are tasked with delivering three packages to different locations. The user analyzing the planner output has the plan as well as a general, non-technical understanding of the model and the goals of the problem; the user knows that trucks can move between certain waypoints that have connecting roads of differing lengths, there are refueling stations at waypoints B and E, and some subgoals of the problem are to have package 1 delivered to waypoint C, package 2 delivered to waypoint G, and package 3 delivered to waypoint D. The user is also aware that the three trucks and three packages are at waypoint A in the initial state. A basic map of the domain and plan are shown in FIG1 , respectively . Even with a simple and intuitive problem such as this, questions may arise which cannot be answered trivially. One such question is 'Why drive truck 1 to waypoint E?'. Addressing this question requires the causal consequence of applying the action; in other words, how does driving truck 1 to waypoint E help in achieving the goal(s)?As discussed previously, tracking state information throughout a plan can be useful for explanations. This is especially true when values of state variables are not obvious at any given point in a plan and their relevance to a question is not known. A question such as 'Why drive truck 3 to waypoint B?' has this property . These two questions will be addressed in the following sections. We acknowledge that this is a preliminary step and more work is required to expand on the ideas presented in this paper. One such future work involves defining exactly what questions, which range from action-specific to model-based, can be answered and explained using our approach. Also, how these questions are captured from a user is an open question. The query, 'Why didn't truck 3 deliver any packages?' can be answered using the causal information captured in the framework, but how one converts this question to a form that the system understands requires further research. Potential methods for communicating a user question include a dialogue system or Natural Language Processing techniques. Along with expanding the set of questions that can be addressed, extensions to the argumentation framework itself should be considered. Better methods for creating causal 'chunks' for specific user questions are needed. It may be advantageous to use argumentation schemes to help identify relevant topics of chunks and which causal chains should be included from the framework. This relates to the idea of 'context' and identifying the motivation of a question. If the system can be more precise in extracting the relevant information, the explanations themselves will be more effective.Related to this is the need to explore other ways of presenting an explanation to a user. Research into the efficacy of explanations and how to properly assess the effectiveness of the explanations in practice are future areas of research, and will require user studies. Our starting point will be the approach outlined in Section 4.3 which has been shown empirically to be effective in contexts such as human-robot teaming BID13 . In this paper we proposed an initial approach to explainable planning using argumentation in which causal chains are extracted from a plan and model and abstracted into an argumentation framework. Our hypothesis is that this allows ease of forming and communicating explanations to a user. Furthermore, causal 'chunks' can be created by combining relevant causal links from the chains which explain the causalities surrounding one 'topic'. We believe these help with making more precise explanations, and that chunks can be used to provide hierarchical explanations. Overall, the approach is a first step towards exploiting the intuitive functionality of argumentation in order to use causality for explanations."
}