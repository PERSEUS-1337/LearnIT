{
    "title": "HJgY6R4YPH",
    "content": "The difficulty of obtaining sufficient labeled data for supervised learning has motivated domain adaptation, in which a classifier is trained in one domain, source domain, but operates in another, target domain. Reducing domain discrepancy has improved the performance, but it is hampered by the embedded features that do not form clearly separable and aligned clusters. We address this issue by propagating labels using a manifold structure, and by enforcing cycle consistency to align the clusters of features in each domain more closely. Specifically, we prove that cycle consistency leads the embedded features distant from all but one clusters if the source domain is ideally clustered. We additionally utilize more information from approximated local manifold and pursue local manifold consistency for more improvement. Results for various domain adaptation scenarios show tighter clustering and an improvement in classification accuracy. Classifiers trained through supervised learning have many applications (Bahdanau et al., 2015; Redmon et al., 2016) , but it requires a great deal of labeled data, which may be impractical or too costly to collect. Domain adaptation circumvents this problem by exploiting the labeled data available in a closely related domain. We call the domain where the classifier will be used at, the target domain, and assume that it only contains unlabeled data {x t }; and we call the closely related domain the source domain and assume that it contains a significant amount of labeled data {x s , y s }. Domain adaptation requires the source domain data to share discriminative features with the target data (Pan et al., 2010) . In spite of the common features, a classifier trained using only the source data is unlikely to give satisfactory results in the target domain because of the difference between two domains' data distributions, called domain shift (Pan et al., 2010) . This may be addressed by fine-tuning on the target domain with a small set of labeled target data, but it tends to overfit to the small labeled dataset (Csurka, 2017) . Another approach is to find discriminative features which are invariant between two domains by reducing the distance between the feature distributions. For example, domain-adversarial neural network (DANN) (Ganin et al., 2016) achieved remarkable result using generative adversarial networks (GANs) (Goodfellow et al., 2014) . However, this approach still has room to be improved. Because the classifier is trained using labels from the source domain, the source features become clustered, and they determine the decision boundary. It would be better if the embedded features from the target domain formed similar clusters to the source features in class-level so that the decision boundary does not cross the target features. Methods which only reduce the distance between two marginal distributions bring the features into general alignment, but clusters do not match satisfactorily, as shown in Fig. 1(a) . As a consequence, the decision boundary is likely to cross the target features, impairing accuracy. In this work, we propose a novel domain adaptation method to align the manifolds of the source and the target features in class-level, as shown in Fig. 1(b) . We first employ label propagation to evaluate the relation between manifolds. Then, to align them, we reinforce the cycle consistency that is the correspondence between the original labels in the source domain and the labels that are propagated from the source to the target and back to the source domain. The cycle consistency draws features from both domains that are near to each other to converge, and those that are far apart to diverge. The proposed method exploits manifold information using label propagation which had not been taken into account in other cycle consistency based methods. As a result, our approach outperforms other baselines on various scenarios as demonstrated in Sec. 4. Moreover, the role of cycle consistency is theoretically explained in Sec. 3.2 that it leads to aligned manifolds in class-level. To acquire more manifold information within the limited number of mini-batch samples, we utilize local manifold approximation and pursue local manifold consistency. In summary, our contributions are as follows: \u2022 We propose a novel domain adaptation method which exploits global and local manifold information to align class-level distributions of the source and the target. \u2022 We analyze and demonstrate the benefit of the proposed method over the most similar baseline, Associative domain adaptation (AssocDA) (Haeusser et al., 2017) . \u2022 We present the theoretical background on why the proposed cycle consistency leads to class-level manifold alignment, bringing better result in domain adaptation. \u2022 We conduct extensive experiments on various scenarios and achieve the state-of-the-art performance. In this paper, we proposed a novel domain adaptation which stems from the objective to correctly align manifolds which might result in better performance. Our method achieved it, which was supported by intuition, theory and experiments. In addition, its superior performance was demonstrated on various benchmark dataset. Based on graph, our method depends on how to construct the graph. Pruning the graph or defining a similarity matrix considering underlying geometry may improve the performance. Our method also can be applied to semi supervised learning only with slight modification. We leave them as future work. A PROOF OF THEOREM 1 Theorem 1. Let {e i |1 \u2264 i \u2264 C} be the standard bases of C-dimensional Euclidean space. For the sake of simplicity, source data x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x Ns are assumed to be arranged so that the first n 1 data belong to class 1, the n 2 data to class 2, and so forth. Assume that 1) the source data is ideally clustered, in the sense that T ss has positive values if the row and the column are the same class and zero otherwise, i.e. , T ss = diag(T 1 , T 2 , \u00b7 \u00b7 \u00b7 , T C ), the block diagonal where T i is a n i \u00d7 n i positive matrix for i = 1, 2, \u00b7 \u00b7 \u00b7 , C and 2)\u0177 s = y s . Then for all 1 \u2264 j \u2264 C, there exists a nonnegative vector v j \u2208 R Ns such that 1) the part where source data belongs to j th class (from [n 1 + n 2 + \u00b7 \u00b7 \u00b7 + n j\u22121 + 1] th element to [n 1 + n 2 + \u00b7 \u00b7 \u00b7 + n j ] th element) are positive and the other elements are all zero and 2) v j T st\u0177 t e i = 0 for all 1 \u2264 i \u2264 C, i = j. From the assumption, T ss is a block diagonal matrix of which block elements are T 1 ,T 2 ,\u00b7 \u00b7 \u00b7 ,T C . v j is all zero except n j elements in the middle of v j . The n j elements are all positive and their indices correspond to those of T j in T ss . In the proof, the left eigenvector u j of T j will be substituted to this part. Proof. From the Perron-Frobenius Theorem (Frobenius et al., 1912; Perron, 1907) that positive matrix has a real and positive eigenvalue with positive left and right eigenvectors, T j , the block diagonal element of T ss , has a positive left eigenvector u j with eigenvalue \u03bb j for all j = 1, 2, \u00b7 \u00b7 \u00b7 C. Then, as shown below, v j = ( 0 0 \u00b7\u00b7\u00b7 0 u j 0 \u00b7\u00b7\u00b7 0 ) where n 1 + n 2 + \u00b7 \u00b7 \u00b7 + n j\u22121 zeros, u j and n j+1 + n j+2 + \u00b7 \u00b7 \u00b7 + n C zeros are concatenated, is a left eigenvector of T ss with eigenvalue \u03bb j by the definition of eigenvector. From the label propagation, we have,\u0177 By multiplying v j (I \u2212 T ss ) on the left and e i on the right to the both sides in Equation 13 and combining with the assumption\u0177 s = y s , we have, The last zero comes from the definition of v j ."
}