{
    "title": "H1gJ2RVFPH",
    "content": "The point estimates of ReLU classification networks, arguably the most widely used neural network architecture, have recently been shown to have arbitrarily high confidence far away from the training data. This architecture is thus not robust, e.g., against out-of-distribution data. Approximate Bayesian posteriors on the weight space have been empirically demonstrated to improve predictive uncertainty in deep learning. The theoretical analysis of such Bayesian approximations is limited, including for ReLU classification networks. We present an analysis of approximate Gaussian posterior distributions on the weights of ReLU networks. We show that even a simplistic (thus cheap), non-Bayesian Gaussian distribution fixes the asymptotic overconfidence issue. Furthermore, when a Bayesian method, even if a simple one, is employed to obtain the Gaussian, the confidence becomes better calibrated. This theoretical result motivates a range of Laplace approximations along a fidelity-cost trade-off. We validate these findings empirically via experiments using common deep ReLU networks. As neural networks have been successfully applied in ever more domains, including safety-critical ones, the robustness of their predictions and the calibration of their predictive uncertainty have moved into focus, subsumed under the notion of AI safety (Amodei et al., 2016) . A principal goal of uncertainty calibration is that learning machines (and neural networks in particular) should assign low confidence to test cases not explained well by the training data or prior information (Gal, 2016) . The most obvious such instance are test points that lie \"far away\" from the training data. Many methods to achieve this goal have been proposed, both Bayesian (Gal & Ghahramani, 2016; Blundell et al., 2015; Louizos & Welling, 2017) and non-Bayesian (Lakshminarayanan et al., 2017; Liang et al., 2018; Hein et al., 2019) . ReLU networks are currently among the most widely used neural architectures. This class comprises any network that can be written as a composition of linear layers (including fully-connected, convolutional, and residual layers) and a ReLU activation function. But while ReLU networks often achieve high accuracy, the uncertainty of their predictions has been shown to be miscalibrated (Guo et al., 2017) . Indeed, Hein et al. (2019) demonstrated that ReLU networks are always overconfident \"far away from the data\": scaling a training point x (a vector in a Euclidean input space) with a scalar \u03b4 yields predictions of arbitrarily high confidence in the limit \u03b4 \u2192 \u221e. This means ReLU networks are susceptible to adversarial or out-of-distribution (OOD) examples. Bayesian methods have long been known empirically to improve predictive uncertainty calibration. MacKay (1992) demonstrated empirically that the predictive uncertainty of Bayesian neural networks will naturally be high in regions not covered by training data. Results like this raise the hope that the overconfidence problem of ReLU networks, too, might be mitigated by the use of Bayesian methods. This paper offers a theoretical analysis of the binary classification case of ReLU networks with logistic output layer. We show that equipping such networks with virtually any Gaussian probability distribution (i.e. regardless of whether it is motivated in a Bayesian fashion or not) mitigates the aforementioned theoretical problem, so that predictive confidence far away from the training data approaches a known constant, bounded away from one, whose value is controlled by the covariance (cf. Figure 1) . At the same time, this treatment does not change the decision boundary of the trained network, so it has no negative effect on the predictive performance. Figure 1: Binary classification on a toy dataset using a MAP estimate (a) and various Gaussian approximations over the weights, sorted by their complexity of inverting the precision matrix. These approximations are carried out only at the last layer of the network and d denotes the number of hidden units at that layer. The shade of color represents the confidence of the prediction (darker shade means higher confidence). The decision boundary is in thick black. Even an arbitrary (i.e. nonBayesian) isotropic (b) or diagonal (c) covariance makes the confidence bounded away from one. Using the data in a more Bayesian fashion (d) calibrates the uncertainty further, in particular in regions close to the data. A central aspect of our result is that asymptotic overconfidence can be mitigated with an essentially arbitrary Gaussian distribution on the weight space, including one of simple diagonal or even scalar covariance, and one whose covariance need not even depend on the training data. Achieving calibration at finite distances from the training data requires increasing levels of fidelity towards full Bayesian inference, for which our results also give some quantification. Our results thus answer a question about \"how Bayesian\" one needs to be to achieve certain levels of calibration. This is valuable because even approximate Bayesian treatments of deep learning, such as through Laplace approximations, can have high computational cost. We empirically validate our results through a simple Laplace approximation to only the last layer of deep ReLU architectures, and find that this cheap procedure is already competitive to recently proposed non-Bayesian methods specifically constructed to overcome the overconfidence problem of ReLU networks. We also show that this cheap Bayesian approach yields good performance in the multi-class classification setting, indicating that our analysis may carry over to this case. Section 2 begins with a rigorous problem statement and assumptions, then develops the main theoretical results. We discuss related work in Section 3, while empirical results are in Section 4. We have shown that even an extremely approximate and virtually non-Bayesian probabilistic Gaussian treatment mitigates the most extreme aspects of overconfidence in ReLU networks. Our analytical results bound the confidence of the Bayesian prediction of linear classifiers and ReLU networks far away from the training data away from one. This motivates a spectrum of approximations, from ad-hoc isotropic to \"full Bayesian\" Laplace approximations. In the Laplace approximation case, the bound asymptotically converges to a constant whose value can be controlled via the prior. We validated our results experimentally by constructing a simple Laplace method that can still capture the properties we have shown, specifically by only approximating the last-layer's posterior distribution. In contrast to other approximations, this method is cheap and simple to implement, yet already yields competitive performance compared to the more expensive, recently proposed non-Bayesian method for combating the overconfidence problem. While more elaborate Laplace approximations can improve fidelity the further, our results provide virtually any ReLU network with a simple and computationally lightweight way to mitigate overconfidence. 1/2 = 0. Notice, the denominator of the l.h.s. is positive. Thus, it follows that \u00b5 f must be 0, implying that \u03c3(\u00b5 f ) = 0.5. Lemma A.2. Let x \u2208 R n be a vector and A \u2208 R n\u00d7n be an SPD matrix. If \u03bb min (A) is the minimum eigenvalue of A, then x T Ax \u2265 \u03bb min x 2 . Proof. Since A is SPD, it admits an eigendecomposition A = Q\u039bQ T and \u039b = \u039b 1 2 \u039b 1 2 makes sense. Therefore, by keeping in mind that Q T x is a vector in R n , we have where the last equality is obtained as Q T x 2 = x T Q T Qx and noting that Q is an orthogonal matrix. Proposition A.3. Let f : R n \u2192 R be a binary linear classifier defined by f (x) := w T x and p(w|D) := N (w|\u00b5, \u03a3) be the distribution over w. Then for any x \u2208 R n , Furthermore, if x \u2208 R n then as \u03b4 > 0 goes to infinity Proof. The first result follows directly from Lemma A.2 and by noting that the denominator of eq. (4) is positive since \u03a3 is symmetric positive-definite (SPD) by definition. For the second result, let x \u2208 R n be arbitrary. By computation and again since the denominator of eq. (4) is positive, we have We would like to inspect the asymptotic behavior of z(\u03b4x) with respect to \u03b4. First, for the sake of completeness, we can compute that lim \u03b4\u21920 |z(\u03b4x)| = 0. This reflects the case when \u03b4x goes to the decision boundary. Now, for the case when \u03b4 \u2192 \u221e, we can see that since 1/\u03b4 2 \u2192 0 as \u03b4 \u2192 \u221e. Therefore, using Lemma A.2 and Cauchy-Schwarz inequality, we have thus the proof is complete. Under review as a conference paper at ICLR 2020 Lemma A.4 (Hein et al. (2019)). Let {Q i } R l=1 be the set of linear regions associated to the ReLU network \u03c6 : R n \u2192 R n . For any x \u2208 R n there exists \u03b1 \u2208 R with \u03b1 > 0 and t \u2208 {1, . . . , R} such that \u03b4x \u2208 Q t for all \u03b2 \u2265 \u03b1. Furthermore, the restriction of \u03c6 to Q t can be written as an affine function. Theorem A.5. Let f : R d \u2192 R be a binary linear classifier defined by f \u2022 \u03c6(x) := w T \u03c6(x) where \u03c6 : R n \u2192 R d is a ReLU network and let p(w|D) := N (w|\u00b5, \u03a3) be the distribution over w. Then for any x \u2208 R n , where V \u2208 R d\u00d7n and a \u2208 R d are some matrix and vector that depend on x. Furthermore, as \u03b4 > 0 goes to infinity such that x \u2208 Q and \u03c6| Q (x) := Vx + a. Applying eq. (4) to \u03c6| Q (x) and following the proof of Proposition 2.3 yield thus the first result is obtained. , such that for any \u03b4 \u2265 \u03b1, we have that \u03b4x \u2208 R and the restriction \u03c6| R can be written as Ux + c. Therefore, for any such \u03b4, Now, notice that as \u03b4 \u2192 \u221e, 1/\u03b4 2 and 1/\u03b4 goes to zero. So, in the limit, we have that Again, following the proof of Proposition 2.3 (i.e. using Cauchy-Schwarz and Lemma A.2), we can upper-bound this limit with which concludes the proof. Corollary A.6 (\u03bb min (\u03a3) from a desired upper confidence bound on ReLU networks). Let f \u2022 \u03c6, with \u03c6 : R n \u2192 R d and f : R d \u2192 R, be a ReLU network defined by f \u2022 \u03c6(x) := w T \u03c6(x) and N (w|\u00b5, \u03a3) be the distribution over w where the mean \u00b5 is fixed and \u03a3 is any SPD matrix. Then: (i) For any > 0 there exists \u03a3 such that for any x \u2208 R n far away from the training data, we have that |z \u2022 \u03c6(x)| \u2264 . (ii) For any 0.5 < p < 1 there exists \u03a3 such that for any x \u2208 R n far away from the training data, we have that \u03c3(|z \u2022 \u03c6(x)|) \u2264 p. Proof. We begin with (i). Let > 0 and \u03b4 = 8 \u03c0 \u00b5 2 . Pick any \u03a3 SPD with \u03bb min (\u03a3) = \u03b4. Then, by eq. (12) of Theorem 2.4 and our choice of \u03bb min (\u03a3), for any z \u2208 R n , asymptotically we have that which is the desired result. For (ii), let 0.5 < p < 1 be arbitrary. Observe that the inverse logistic function is given by \u03c3 \u22121 (x) := log x/(1 \u2212 x) for 0 < x < 1 and it is positive for 0.5 < x < 1. Therefore by setting in (i) with 2 and verify that for any x \u2208 R n this gives |z(x)| \u2264 \u03c3 \u22121 (p). Thus, for any x \u2208 R n far away from the training data, since \u03c3 is monotonic, we have that and the proof is complete. . Let p(w|D) := N (w|\u00b5, \u03a3) be the posterior over w, obtained via a Laplace approximation with prior N (w|0, \u03c3 2 0 I). Suppose H is the Hessian w.r.t. w at \u00b5 of the negative log-likelihood of the model. Then (ii) For each i = 1, . . . , d, the ith eigenvalue \u03bb i (\u03a3) of \u03a3 is a non-decreasing function of \u03c3 Proof. The negative log-likelihood of Bernoulli distribution is given by Now, observing that \u03c3 (x) = \u03c3(x)(1 \u2212 \u03c3(x)) for all x \u2208 R, we can compute T ."
}