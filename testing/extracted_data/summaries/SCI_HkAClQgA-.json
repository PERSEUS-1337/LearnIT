{
    "title": "HkAClQgA-",
    "content": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \n Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\n However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\n We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries. Text summarization is the process of automatically generating natural language summaries from an input document while retaining the important points. By condensing large quantities of information into short, informative summaries, summarization can aid many downstream applications such as creating news digests, search, and report generation.There are two prominent types of summarization algorithms. First, extractive summarization systems form summaries by copying parts of the input BID5 BID22 . Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text BID4 .Neural network models based on the attentional encoder-decoder model for machine translation BID0 were able to generate abstractive summaries with high ROUGE scores. However , these systems have typically been used for summarizing short input sequences (one or two sentences) to generate even shorter summaries. For example , the summaries on the DUC-2004 dataset generated by the state-of-the-art system by BID40 are limited to 75 characters. also applied their abstractive summarization model on the CNN/Daily Mail dataset BID8 , which contains input sequences of up to 800 tokens and multisentence summaries of up to 100 tokens. But their analysis illustrates a key problem with attentional encoder-decoder models: they often generate unnatural summaries consisting of repeated phrases.We present a new abstractive summarization model that achieves state-of-the-art results on the CNN/Daily Mail and similarly good results on the New York Times dataset (NYT) BID30 . To our knowledge, this is the first end-to-end model for abstractive summarization on the NYT dataset. We introduce a key attention mechanism and a new learning objective to address the FIG2 : Illustration of the encoder and decoder attention functions combined. The two context vectors (marked \"C\") are computed from attending over the encoder hidden states and decoder hidden states. Using these two contexts and the current decoder hidden state (\"H\"), a new word is generated and added to the output sequence. repeating phrase problem : (i) we use an intra-temporal attention in the encoder that records previous attention weights for each of the input tokens while a sequential intra-attention model in the decoder takes into account which words have already been generated by the decoder. (ii) we propose a new objective function by combining the maximum-likelihood cross-entropy loss used in prior work with rewards from policy gradient reinforcement learning to reduce exposure bias.Our model achieves 41.16 ROUGE-1 on the CNN/Daily Mail dataset. Moreover, we show, through human evaluation of generated outputs, that our model generates more readable summaries compared to other abstractive approaches. We presented a new model and training procedure that obtains state-of-the-art results in text summarization for the CNN/Daily Mail, improves the readability of the generated summaries and is better suited to long output sequences. We also run our abstractive model on the NYT dataset for the first time. We saw that despite their common use for evaluation, ROUGE scores have their shortcomings and should not be the only metric to optimize on summarization model for long sequences. Our intra-attention decoder and combined training objective could be applied to other sequence-tosequence tasks with long inputs and outputs, which is an interesting direction for further research.A NYT DATASET"
}