{
    "title": "rJxA-h05KQ",
    "content": "We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output. We extend softmax layer with an additional constant input. The corresponding additional output is able to represent the uncertainty of the network. The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets. We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains. The applications of computational learning systems might cause intrusive effects if we assume that predictions are always as accurate as during the experimental phase. Examples include misclassified traffic signs BID5 and an image tagger that classified two African Americans as gorillas BID3 . This is often caused by overconfidence of models that has been observed in the case of deep neural networks BID8 . Such malfunctions can be prevented if we estimate correctly the uncertainty of the machine learning system. Beside AI safety, uncertainty is useful in the active learning setting in which data collection process is expensive or time consuming BID12 BID32 .While uncertainty estimation in neural networks is an active field of research, the current methods are rarely adopted. It is desirable to develop a method that does not create an additional computational overhead. Such a method could be used in environments that focus on quick training and/or inference. If such a method is simple, the ease of implementation should encourage practitioners to develop danger-aware systems in their work.We suggest a method that measures uncertainty of the neural networks with a softmax output layer. We replace this layer with Inhibited Softmax layer BID33 , and we show that it can be used to express the uncertainty of the model. In our experiments the method outperforms baselines and performs comparably with more computationally expensive methods on the out-of-distribution detection task.We contribute with:\u2022 The mathematical explanation why the additional Inhibited Softmax output can be interpreted as an uncertainty measure.\u2022 The additions to the Inhibited Softmax that improve its uncertainty approximation properties.\u2022 The benchmarks comparing Inhibited Softmax, baseline and contemporary methods for measuring uncertainty in neural networks.The modern Bayesian Neural Networks BID0 BID11 BID25 BID27 BID37 BID9 Zhang et al., 2018; BID15 aim to confront this issue by inferring distribution over the models' weights. This approach has been inspired by Bayesian approaches suggested as early as the nineties BID2 BID29 . A very popular regularisation mean -dropout -also can be a source of approximate Bayesian inference BID7 . Such technique, called Monte Carlo dropout BID6 , belongs to the Bayesian Neural Networks class and has been since used in the real-life scenarios (e.g. Leibig et al., 2017) . In the Bayesian Neural Networks the uncertainty is modelled by computing the predictive entropy or mutual information over the probabilities coming from stochastic predictions BID34 .Other methods to measure uncertainty of neural networks include a non-Bayesian ensemble BID19 , a student network that approximates the Monte Carlo posterior predictive distribution BID16 , modelling Markov chain Monte Carlo samples with a GAN BID38 , Monte Carlo Batch Normalization BID35 and the nearest neighbour analysis of penultimate layer embedding BID28 .The concept of uncertainty is not always considered as a homogeneous whole. Some of the authors distinguish two types of uncertainties that influence predictions of machine learning models BID14 : epistemic uncertainty and aleatoric uncertainty. Epistemic uncertainty represents the lack of knowledge about the source probability distribution of the data. This uncertainty can be reduced by increasing the size of the training data. Aleatoric uncertainty arises from homoscedastic , heteroscedastic and label noises and cannot be reduced by the model. We will follow another source BID27 ) that defines the third type: distributional uncertainty. It appears when the test distribution differs from the training distribution, i.e. when new observations have different nature then the ones the model was trained on.A popular benchmark for assessing the ability of the models to capture the distributional uncertainty is distinguishing the original test set from out-of-distribution dataset BID10 . There are works that focus only on this type of uncertainty BID22 . ODIN BID24 does not require changing already existing network and relies on gradient-based input preprocessing. Another work BID4 ) is close to the functionality of our method , as it only adds a single densely connected layer and uses a single forward pass for a sample.Bayesian neural networks are more computationally demanding as they usually require multiple stochastic passes and/or additional parameters to capture the priors specification.To the best of our knowledge, our method is the first that improves upon the baseline, and meets all the following criteria:\u2022 No additional learnable parameters required.\u2022 Only single forward pass needed.\u2022 No additional out-of-distribution or adversarial observations required.\u2022 No input preprocessing.The technique we use, Inhibited Softmax, has been successfully used for the prediction of background class in the task of extraction the objects out of aerial imagery BID33 . The original work does not mention other possible applications of this softmax modification. We presented a new method for uncertainty estimation -Inhibited Softmax. The method can be easily applied to various multilayer neural network architectures and does not require additional parameters, multiple stochastic forward passes or OOD examples.The results show that the method outperforms baseline and performs comparably to the other methods. The method does not deteriorate predictive performance of the classifier.The predictive performance from IMDB/Movie Reviews experiment suggests that even if the observation comes from another probability distribution and the uncertainty measure is able to detect it, the network can still serve as a useful classifier.The improvement of the baseline on the sentiment task after adding suggested regularisation indicates it might be worth to apply such measures to other uncertainty estimation methods."
}