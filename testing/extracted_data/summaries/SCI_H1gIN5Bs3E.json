{
    "title": "H1gIN5Bs3E",
    "content": "When considering simultaneously a finite number of tasks, multi-output learning enables one to account for the similarities of the tasks via appropriate regularizers. We propose a generalization of the classical setting to a continuum of tasks by using vector-valued RKHSs. Several fundamental problems in machine learning and statistics can be phrased as the minimization of a loss function described by a hyperparameter. The hyperparameter might capture numerous aspects of the problem: (i) the tolerance w. r. t. outliers as the -insensitivity in Support Vector Regression (Vapnik et al., 1997) , (ii) importance of smoothness or sparsity such as the weight of the l 2 -norm in Tikhonov regularization (Tikhonov & Arsenin, 1977) , l 1 -norm in LASSO (Tibshirani, 1996) , or more general structured-sparsity inducing norms BID3 , (iii) Density Level-Set Estimation (DLSE), see for example one-class support vector machines One-Class Support Vector Machine (OCSVM, Sch\u00f6lkopf et al., 2000) , (iv) confidence as exemplified by Quantile Regression (QR, Koenker & Bassett Jr, 1978) , or (v) importance of different decisions as implemented by Cost-Sensitive Classification (CSC, Zadrozny & Elkan, 2001) . In various cases including QR, CSC or DLSE, one is interested in solving the parameterized task for several hyperparameter values. Multi-Task Learning (Evgeniou & Pontil, 2004 ) provides a principled way of benefiting from the relationship between similar tasks while preserving local properties of the algorithms: \u03bd-property in DLSE (Glazer et al., 2013) or quantile property in QR (Takeuchi et al., 2006) .A natural extension from the traditional multi-task setting is to provide a prediction tool being able to deal with any value of the hyperparameter. In their seminal work, (Takeuchi et al., 2013) extended multi-task learning by considering an infinite number of parametrized tasks in a framework called Parametric Task Learning (PTL) . Assuming that the loss is piecewise affine in the hyperparameter, the authors are able to get the whole solution path through parametric programming, relying on techniques developed by Hastie et al. (2004) .In this paper 1 , we relax the affine model assumption on the tasks as well as the piecewise-linear assumption on the loss, and take a different angle. We propose Infinite Task Learning (ITL) within the framework of functionvalued function learning to handle a continuum number of parameterized tasks using Vector-Valued Reproducing Kernel Hilbert Space (vv-RKHS, Pedrick, 1957) ."
}