{
    "title": "ByeLmn0qtX",
    "content": "This paper proposes variational domain adaptation, a unified, scalable, simple framework for learning multiple distributions through variational inference. Unlike the existing methods on domain transfer through deep generative models, such as StarGAN (Choi et al., 2017) and UFDN (Liu et al., 2018), the variational domain adaptation has three advantages. Firstly, the  samples from the target are not required. Instead, the framework requires one known source as a prior $p(x)$ and binary discriminators, $p(\\mathcal{D}_i|x)$, discriminating the target domain $\\mathcal{D}_i$ from others. Consequently, the framework regards a target as a posterior that can be explicitly formulated through the Bayesian inference, $p(x|\\mathcal{D}_i) \\propto p(\\mathcal{D}_i|x)p(x)$, as exhibited by a further proposed model of dual variational autoencoder (DualVAE). Secondly, the framework is scablable to large-scale domains. As well as VAE encodes a sample $x$ as a mode on a latent space: $\\mu(x) \\in \\mathcal{Z}$, DualVAE encodes a domain $\\mathcal{D}_i$ as a mode on the dual latent space $\\mu^*(\\mathcal{D}_i) \\in \\mathcal{Z}^*$, named domain embedding. It reformulates the posterior with a natural paring $\\langle, \\rangle: \\mathcal{Z} \\times \\mathcal{Z}^* \\rightarrow \\Real$, which can be expanded to uncountable infinite domains such as continuous domains as well as interpolation. Thirdly, DualVAE fastly converges without sophisticated automatic/manual hyperparameter search in comparison to GANs as it requires only one additional parameter to VAE. Through the numerical experiment, we demonstrate the three benefits with multi-domain image generation task on CelebA with up to 60 domains, and exhibits that DualVAE records the state-of-the-art performance outperforming StarGAN and UFDN. \"...we hold that all the loveliness of this world comes by communion in Ideal-Form. All shapelessness whose kind admits of pattern and form, as long as it remains outside of Reason and Idea, is ugly from that very isolation from the Divine-Thought. Agents that interact in various environments have to handle multiple observation distributions . Domain adaptation BID0 ) is a methodology employed to exploit deep generative models, such as adversarial learning BID2 and variational inference BID8 , that can handle distributions that vary with environments and other agents. Further, multi-task learning and domain transfer are examples of how domain adaptation methodology is used. We focus on domain transfer involving transfers across a distribution between domains. For instance, pix2pix BID5 ) outputs a sample from the target domain that corresponds to the input sample from the source domain. This can be achieved by learning the pair relation of samples from the source and target domains. CycleGAN BID21 transfers the samples between two domains using samples obtained from both domains. Similarly, UNIT BID12 , DiscoGAN , and DTN BID20 have been proposed in previous studies.However, the aforementioned method requires samples that are obtained from the target domains, and because of this requirement, it cannot be applied to domains for which direct sampling is expensive or often impossible. For example, the desired, continuous, high-dimensional action in the environment, intrinsic reward (e.g., preference and curiosity) and the policy of interacting agents other than itself cannot be sampled from inside, and they can only discriminate the proposed input. Even for ourselves, the concept of beauty or interest in our conscious is subjective, complex, and difficult to be sampled from the inside, although it is easy to discriminate on the outside. The key concept of variational domain adaptation. a) Given the proposal drawn from the prior, the discriminator discriminates the target domain from the others. Each domain is posterior for the prior N (z|0, 1); further, the distribution in the latent space is observed to be a normal distribution using the conjugate likelihood. b) Domain transfer is represented by the mean shift in the latent space. c) Domain embedding: After training, all the domains can only be represented by vectors \u00b5 i .In this study, we propose variational domain adaptation, which is a framework for targets that pose challenges with respect to direct sampling. One solution is multi-domain semi-supervision, which converts the problem to semi-supervised learning, thereby making is possible to perform variational inference. In this supervision, a source domain is regarded as a prior p(x) and a target domain is considered to be a posterior p(x|D i ) by referring to the label given by a supervised discriminator p(D i |x) that distinguishes the target domain from others. Our model imitates the behavior of the discriminator and models the target domain using a simple conclusion of the Bayesian theorem, p \u03b8 (x|D i ) \u221d p \u03b8 (D i |x)p \u03b8 (x). The end-to-end learning framework also makes it possible to learn good prior p \u03b8 (x) with respect to all the domains. After the training was completed, the posterior p \u03b8 (x|D i ) succeeded in deceiving the discriminator p(D i |x). This concept is similar to rejection sampling in the Monte Carlo methods. Further , variational domain adaptation is the first important contribution from this study.The second contribution from this study is a model of dual variational autoencoder (DualVAE), which is a simple extension of the conditional VAE BID9 ), employed to demonstrate our concept of multi-domain semi-supervision. DualVAE learns multiple domains in one network by maximizing the variational lower bound of the total negative KL-divergence between the target domain and the model. DualVAE uses VAE to model the prior p(x) and an abstract representation for the discriminator p(D i |x). The major feature of DualVAE is domain embedding that states that all the posteriors are modeled as a normal distribution N (z|\u00b5 i , \u03c3 2 ) in the same latent space Z using the conjecture distribution of the prior. Here, \u00b5 i is the domain embedding that represents the domain D i . This enables us to sample from p \u03b8 (x|D i ). Our major finding was that the discriminator of DualVAE was a simple inner product between the two means of domain embedding and the VAE output: DISPLAYFORM0 that acts as a natural paring between the sample and the domain. The probabilistic end-to-end model learns multiple domains in a single network, making it possible to determine the effect of transfer learning and to learn data that multi-domains cannot observe from sparse feedback. Domain embedding is a powerful tool and allows us to use VAEs instead of GANs.The third contribution of this study is that DualVAE was validated for use in a recommendation task using celebA BID13 . In the experiment , using celebA and face imaging data obtained based on evaluations by 60 users, an image was generated based on the prediction of user evaluation and an ideal image that was determined to be good by multiple users. We demonstrated that the image could be modified to improve the evaluation by interpolating the image, and the image was evaluated using the domain inception score (DIS), which is the score of the model that has learned the preference of each user. We present the beauty inside each evaluator by simply sampling p \u03b8 (x|D i ). The DIS of DualVAE is higher than that of a single domain, and the dataset and code are available online. Variational domain adaptation, which is a unified framework for learning multiple distributions in a single network, is proposed in this study. Our framework uses one known source as a prior p(x) and binary discriminator p(D i |x), thereby discriminating the target domain D i from the others; this is in contrast with the existing frameworks in which samples undergo domain transfer through deep generative models. Consequently, our framework regards the target as a posterior that is characterized through Bayesian inference, p(x|D i ) \u221d p(D i |x)p(x). This was exhibited by the proposed DualVAE. The major feature of the DualVAE is domain embedding, which is a powerful tool that encodes all the domains and the samples obtained from the prior into normal distributions in the same latent space as that learned by a unified network through variational inference. In the experiment, we applied our framework and model to a multi-domain image generation task. celebA and face image data that were obtained based on evaluation by 60 users were used, and the result revealed that the DualVAE method outperformed StarGAN and UFDN.Several directions should be considered for future research. First, we intend to expand DualVAEs for learning in complex domains, such as high-resolution images with several models, for example, glow BID7 . Second, we will perform an experiment to consider wider domains with respect to beauty. We expect that our proposed method will contribute to society in a number of ways and will help to deal with the paradigm of multiple contexts-multimodal, multi-task, and multi-agent contexts."
}