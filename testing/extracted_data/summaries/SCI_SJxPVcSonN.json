{
    "title": "SJxPVcSonN",
    "content": "Machine learned large-scale retrieval systems require a large amount of training data representing query-item relevance. However, collecting users' explicit feedback is costly. In this paper, we propose to leverage user logs and implicit feedback as auxiliary objectives to improve relevance modeling in retrieval systems. Specifically, we adopt a two-tower neural net architecture to model query-item relevance given both collaborative and content information. By introducing auxiliary tasks trained with much richer implicit user feedback data, we improve the quality and resolution for the learned representations of queries and items. Applying these learned representations to an industrial retrieval system has delivered significant improvements. In this paper, we propose a novel transfer learning model architecture for large-scale retrieval systems. The retrieval problem is defined as follows: given a query and a large set of candidate items, retrieve the top-k most relevant candidates. Retrieval systems are useful in many real-world applications such as search BID28 and recommendation BID6 BID31 BID10 . The recent efforts on building large-scale retrieval systems mostly focus on the following two aspects:\u2022 Better representation learning. Many machine learning models have been developed to learn the mapping of queries and candidate items to an embedding space BID14 BID15 . These models leverage various features such as collaborative and content information BID29 the top-k relevant items given the similarity (distance) metric associated with the embedding space BID3 BID8 .However , it is challenging to design and develop real-world large-scale retrieval systems for many reasons:\u2022 Sparse relevance data. It is costly to collect users' true opinions regarding item relevance. Often , researchers and engineers design human-eval templates with Likert scale questions for relevance BID5 , and solicit feedback via crowd-sourcing platforms (e.g., Amazon Mechnical Turk).\u2022 Noisy feedback. In addition , user feedback is often highly subjective and biased, due to human bias in designing the human-eval templates, as well as the subjectivity in providing feedback.\u2022 Multi-modality feature space. We need to learn relevance in a feature space generated from multiple modalities, e.g., query content features, candidate content features, context features, and graph features from connections between query and candidate BID29 BID21 BID7 .In this paper, we propose to learn relevance by leveraging both users' explicit answers on relevance and users' implicit feedback such as clicks and other types of user engagement. Specifically, we develop a transfer-learning framework which first learns the effective query and candidate item representations using a large quantity of users' implicit feedback, and then refines these representations using users' explicit feedback collected from survey responses. The proposed model architecture is depicted in FIG1 .Our proposed model is based on a two-tower deep neural network (DNN) commonly deployed in large-scale retrieval systems BID15 . This model architecture , as depicted in FIG0 , is capable of learning effective representations from multiple modalities of features. These representations can be subsequently served using highly efficient nearest neighbor search systems BID8 .To transfer the knowledge learned from implicit feedback to explicit feedback, we extend the two-tower model by adopting a shared-bottom architecture which has been widely used in the context of multi-task learning BID4 . Specifically, the final loss includes training objectives for both the implicit and explicit feedback tasks. These two tasks share some hidden layers, and each task has its own independent sub-tower. At serving time, only the representations learned for explicit feedback are used and evaluated.Our experiments on an industrial large-scale retrieval system have shown that by transferring knowledge from rich implicit feedback, we can significantly improve the prediction accuracy of sparse relevance feedback.In summary, our contributions are as follows:\u2022 We propose a transfer learning framework which leverages rich implicit feedback in order to learn better representations for sparse explicit feedback.\u2022 We design a novel model architecture which optimizes two training objectives sequentially.\u2022 We evaluate our model on a real-world large-scale retrieval system and demonstrate significant improvements.The rest of this paper is organized as follows: Section 2 discusses related work in building large-scale retrieval systems. Section 3 introduces our problem and training objectives . Section 4 describes our proposed approach. Section 5 reports the experimental results on a large-scale retrieval system. Finally, in Section 6, we conclude with our findings. The success of transfer learning hinges on a proper parameterization of both the auxiliary and main tasks. On one hand, we need sufficient capacity to learn a high-quality representation from a large amount of auxiliary data. On the other hand, we want to limit the capacity for the main task to avoid over-fitting to its sparse labels. As a result, our proposed model architecture is slightly different from the traditional pre-trained and fine-tuning model BID12 . Besides shared layers, each task has its own hidden layers with different capacities. In addition, we apply a two-stage training with stop gradients to avoid potential issues caused by the extreme data skew between the main task and auxiliary task.Our experiences have motivated us to continue our work in the following directions:\u2022 We will consider multiple types of user implicit feedback using different multi-task learning frameworks, such as Multi-gate Mixture-of-Expert BID17 and Sub-Network Routing BID18 . We will continue to explore new model architectures to combine transfer learning with multi-task learning.\u2022 The auxiliary task requires hyper-parameter tuning to learn the optimal representation for the main task. We will explore AutoML BID26 techniques to automate the learning of proper parameterizations across tasks for both the query and the candidate towers. In this paper, we propose a novel model architecture to learn better query and candidate representations via transfer learning. We extend the two-tower neural network approach to enhance sparse task learning by leveraging auxiliary tasks with rich implicit feedback. By introducing auxiliary objectives and jointly learning this model using implicit feedback, we observe a significant improvement for relevance prediction on one of Google's large-scale retrieval systems."
}