{
    "title": "rJ7yZ2P6-",
    "content": "Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.   We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. The ability for a machine to converse with human in a natural and coherent manner is one of challenging goals in AI and natural language understanding. One problem in chat-oriented humanmachine dialog system is to reply a message within conversation contexts. Existing methods can be divided into two categories: retrieval-based methods BID31 BID8 BID36 and generation based methods BID28 . The former is to rank a list of candidates and select a good response. For the latter, encoder-decoder framework BID28 or statistical translation method BID19 are usually used to generate a response. It is not easy to main the fluency of the generated texts.Ubuntu dialogue corpus BID13 is the public largest unstructured multi-turns dialogue corpus which consists of about one-million two-person conversations. The size of the corpus makes it attractive for the exploration of deep neural network modeling in the context of dialogue systems. Most deep neural networks use word embedding as the first layer. They either use fixed pre-trained word embedding vectors generated on a large text corpus or learn word embedding for the specific task. The former is lack of flexibility of domain adaptation. The latter requires a very large training corpus and significantly increases model training time. Word out-of-vocabulary issue occurs for both cases. Ubuntu dialogue corpus also contains many technical words (e.g. \"ctrl+alt+f1\", \"/dev/sdb1\"). The ubuntu corpus (V2) contains 823057 unique tokens whereas only 22% tokens occur in the pre-built GloVe word vectors 1 . Although character-level representation which models sub-word morphologies can alleviate this problem to some extent BID7 BID3 BID11 , character-level representation still have limitations: learn only morphological and orthographic similarity, other than semantic similarity (e.g. 'car' and 'bmw') and it cannot be applied to Asian languages (e.g. Chinese characters).In this paper, we generate word embedding vectors on the training corpus based on word2vec BID16 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and taskdomain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-ofthe-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BID34 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on R 10 @1 is 3.8% and our ensemble model on R 10 @1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on P @1 is 3.6%.Our contributions in this paper are summarized below:1. We propose an algorithm to combine pre-trained word embedding vectors with those generated on the training corpus to address out-of-vocabulary word issues and experimental results have shown that it is very effective.2. ESIM with our method has achieved the state-of-the-art results on both Ubuntu Dialogue corpus and Douban conversation corpus.3. We investigate performance impact of two special tags on Ubuntu Dialogue Corpus: endof-utterance and end-of-turn.The rest paper is organized as follows. In Section 2, we review the related work. In Section 3 we provide an overview of ESIM (baseline) model and describe our methods to address out-ofvocabulary issues. In Section 4, we conduct extensive experiments to show the effectiveness of the proposed method. Finally we conclude with remarks and summarize our findings and outline future research directions. We propose an algorithm to combine pre-trained word embedding vectors with those generated on training set as new word representation to address out-of-vocabulary word issues. The experimental results have shown that the proposed method is effective to solve out-of-vocabulary issue and improves the performance of ESIM, achieving the state-of-the-art results on Ubuntu Dialogue Corpus and Douban conversation corpus. In addition, we investigate the performance impact of two special tags: end-of-utterance and end-of-turn. In the future, we may design a better neural architecture to leverage utterance structure in multi-turn conversations."
}