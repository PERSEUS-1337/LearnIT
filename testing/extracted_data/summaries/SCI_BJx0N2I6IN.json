{
    "title": "BJx0N2I6IN",
    "content": "In this paper, we present a reproduction of the paper of Bertinetto et al. [2019] \"Meta-learning with differentiable closed-form solvers\" as part of the ICLR 2019 Reproducibility Challenge. In successfully reproducing the most crucial part of the paper, we reach a performance that is comparable with or superior to the original paper on two benchmarks for several settings. We evaluate new baseline results, using a new dataset presented in the paper. Yet, we also provide multiple remarks and recommendations about reproducibility and comparability.   After we brought our reproducibility work to the authors\u2019 attention, they have updated the original paper on which this work is based and released code as well. Our contributions mainly consist in reproducing the most important results of their original paper, in giving insight in the reproducibility and in providing a first open-source implementation. The ability to adapt to new situations and learn quickly is a cornerstone of human intelligence. When given a previously unseen task, humans can use their previous experience and learning abilities to perform well on this new task in a matter of seconds and with a relatively small amount of new data. Artificial learning methods have been shown to be very effective for specific tasks, often times surpassing human performance BID11 , BID2 ). However, by relying on standard supervised-learning or reinforcement learning training paradigms, these artificial methods still require much training data and training time to adapt to a new task.An area of machine learning that learns and adapts from a small amount of data is called few-shot learning. A shot corresponds to a single example, e.g. an image and its label. In few-shot learning the learning scope is expanded to a variety of tasks with a few shots each, compared to the classic setting of a single task with many shots. A promising approach for few-shot learning is the field of meta-learning. Meta-learning, also known as learning-to-learn, is a paradigm that exploits cross-task information and training experience to perform well on a new unseen task.In this work we reproduce the paper of BID1 (referenced as \"their paper\"); it falls into the class of gradient-based meta-learning algorithms that learn a model parameter intialization for rapid fine-tuning with a few shots BID4 , BID9 ). The authors present a new meta-learning method that combines a deep neural network feature extractor with differentiable learning algorithms that have closed-form solutions. This reduces the overall complexity of the gradient based meta-learning process, while advancing the state-of-the-art in terms of accuracy across multiple few-shot benchmarks.We interacted with the authors through OpenReview 1 , bringing our reproducibility work and TensorFlow code 2,3 to their attention. Because of this, they have recently updated their original paper with more details to facilitate reproduction and they have released an official PyTorch implementation 4 . In this work we have presented a reproducibility analysis of the ICLR 2019 paper \"Meta-learning with differentiable closed-form solvers\" by BID1 . Some parameters and training methodologies, which would be required for full reproducibility, such as stride and padding of the convolutional filters, and a clear stopping criterion, are not mentioned in the original paper or in its appendix BID1 ). However, by making reasonable assumptions, we have been able to reproduce the most important parts of the paper and to achieve similar results. Most importantly we have succeeded in reproducing the increase in performance of the proposed method over some reproduced baseline results, which supports the conclusions of the original paper. However, the different neural network architectures should be taken into consideration when comparing results. Table 1 . Table 1 : N -way K-shot classification accuracies on CIFAR-FS with 95% confidence intervals."
}