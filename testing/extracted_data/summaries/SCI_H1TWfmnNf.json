{
    "title": "H1TWfmnNf",
    "content": "We present a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of state-of-the-art neural networks for pixel-level tasks. We aim to understand and uncover the synthesis/prediction mechanisms of state-of-the-art convolutional neural networks. To this end, we primarily analyze the synthesis process of generative models and the prediction mechanism of discriminative models. The main hypothesis of this work is that convolutional neural networks for pixel-level tasks learn a fast compositional nearest neighbor synthesis/prediction function. Our experiments on semantic segmentation and image-to-image translation show qualitative and quantitative evidence supporting this hypothesis. Convolutional neural networks (CNNs) have revolutionized computer vision, producing impressive results for discriminative tasks such as image classification and semantic segmentation. More recently, they have also produced startlingly impressive results for image generation through generative models. However, in both cases, such feed-forward networks largely operate as \"black boxes.\" As a community, we are still not able to succinctly state why and how such feed-forward functions generate a particular output from a given input. If a network fails on a particular input, why? How will a network behave on never-before-seen data? To answer such questions, there is a renewed interest in so-called explainable AI In this paper, we have presented a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of convolutional neural networks for spatial prediction tasks. Our analysis suggests that CNNs behave as compositional nearest neighbor operators over a training set of patch-label pairs that act as an associative memory. But beyond simply memorizing, CNNs can generalize to novel data by composing together local patches from different training instances. Also, we argued that networks for pixel-level tasks learn sufficient statistics that enable the gener- Table 1 : We compare compositional nearest neighbors (CompNN) to the baseline CNN and different global nearest neighbor approaches, obtained by matching feature maps from different layers (Global-Bottleneck and Global-Decode2). We report mean pixel accuracy and intersection-overunion, where predicted segmentation labels are compared to ground-truth labels. We specifically use the embedding learned by BID27 for Facades-to-Labels (Facades) and CityScape, and embedding learned by BID2 for CamVid. On average, CompNN performs 5% worse than the baseline CNN, though in some cases (CityScapes) it performs equally. However, compositional matching dramatically outperforms global matching, sometimes by a factor of 2X (Facade and CityScape IoU). In terms of global matching, the last feature layer (Decode2) strictly outperforms the intermediate Bottleneck layer, but is significantly larger (128 3 versus 512 dimensions). Finally, self-supervised labels (SS) overall perform similarly to the original labels (O), but almost consistently help for compositional matching and consistently hurt for global matching. We posit that this is due to the fact that self-supervised labels tend to be overly-smoothed, and so act as a form of spatial regularization for compositional matching. ation of pixel predictions. Our analysis and experiments not only support this argument, but also enables example-based explanations of network behavior and explicit modulation of the implicit biases learned by the network. We hope that our framework enables further analysis of convolutional networks from a non-parametric perspective. FIG0 : Global NN v.s. Comp NN. We show synthesized images using our CompNN methods and four global NN approaches (global nearest neighbor on bottleneck feature embedding and Decode2 feature embedding using self-supervised labels and original labels respectively). We can observe that (1) compositional nearest neighbor outperforms other global nearest neighbor approaches, (2) using Decode2 features (the penultimate layer) sometimes can generate more similar structures (See row 1,4). FIG0 shows the synthesized images using several global NN approaches and a CompNN approach. We can observe that the results of global NN approaches overall resembles global properties of the output of the Convolutional Neural Network (CNN) and of the CompNN approach. For instance, in the top two rows, the output of the global NN resembles the color of the facade and structural properties of the buildings. Also, in the bottom two rows, we can observe that the global NN overall captures the organization of the scene because many labels in the global NN overlap considerably with the output of the CNN and the ground truth."
}