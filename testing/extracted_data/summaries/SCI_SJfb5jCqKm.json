{
    "title": "SJfb5jCqKm",
    "content": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods. The deployment of deep learning models in applications with demanding decision-making components such as autonomous driving or medical diagnosis hinges on our ability to monitor and control their statistical uncertainties. Conceivably, the Bayesian framework offers a principled approach to infer uncertainties from a model; however, there are computational hurdles in implementing it for deep neural networks BID9 . Presently, practically feasible (say, for image classification) uncertainty estimation methods for deep learning are based on signals emerging from standard (non Bayesian) networks that were trained in a standard manner. The most common signals used for uncertainty estimation are the raw softmax response BID4 , some functions of softmax values (e.g., entropy), signals emerging from embedding layers BID20 , and the MC-dropout method BID9 ) that proxies a Bayesian inference using dropout sampling applied at test time. These methods can be quite effective, but no conclusive evidence on their relative performance has been reported. A recent NIPS paper provides documentation that an ensemble of softmax response values of several networks performs better than the other approaches BID17 .In this paper, we present a method of confidence estimation that can consistently improve all the above methods, including the ensemble approach of BID17 . Given a trained classifier and a confidence score function (e.g., generated by softmax response activations), our algorithm will learn an improved confidence score function for the same classifier. Our approach is based on the observation that confidence score functions extracted from ordinary deep classifiers tend to wrongly estimate confidence, especially for highly confident instances. Such erroneous estimates constitute a kind of artifact of the training process with an stochastic gradient descent (SGD) based optimizers. During this process , the confidence in \"easy\" instances (for which we expect prediction with high confidence) is quickly and reliably assessed during the early SGD epochs. Later on, when the optimization is focused on the \"hard\" points (whose loss is still large), the confidence estimates of the easy points become impaired.Uncertainty estimates are ultimately provided in terms of probabilities. Nevertheless, as previously suggested BID10 BID20 BID17 , in a non-Bayesian setting (as we consider here) it is productive to decouple uncertainty estimation into two separate tasks: ordinal ranking according to uncertainty, and probability calibration. Noting that calibration (of ordinal confidence ranking) already has many effective solutions BID21 BID23 BID27 BID11 , our main focus here is on the core task of ranking uncertainties. We thus adopt the setting of BID17 , and others BID10 BID20 , and consider uncertainty estimation for classification as the following problem. Given labeled data, the goal is to learn a pair (f, \u03ba), where f (x) is a classifier and \u03ba(x) is a confidence score function. Intuitively, \u03ba should assign lower confidence values to points that are misclassified by f , relative to correct classifications (see Section 2 for details).We propose two methods that can boost known confidence scoring functions for deep neural networks (DNNs). Our first method devises a selection mechanism that assigns for each instance an appropriate early stopped model, which improves that instance's uncertainty estimation. The mechanism selects the early-stopped model for each individual instance from among snapshots of the network's weights that were saved during the training process. This method requires an auxiliary training set to train the selection mechanism, and is quite computationally intensive to train. The second method approximates the first without any additional examples. Since there is no consensus on the appropriate performance measure for scoring functions, we formulate such a measure based on concepts from selective prediction BID10 BID26 . We report on extensive experiments with four baseline methods (including all those mentioned above) and four image datasets. The proposed approach consistently improves all baselines, often by a wide margin. For completeness, we also validate our results using probably-calibrated uncertainty estimates of our method that are calibrated with the well-known Platt scaling technique BID23 and measured with the negative log-likelihood and Brier score Brier (1950)."
}