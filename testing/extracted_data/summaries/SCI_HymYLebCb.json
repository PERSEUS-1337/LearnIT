{
    "title": "HymYLebCb",
    "content": "We propose a novel subgraph image representation for classification of network fragments with the target being their parent networks. The graph image representation is based on 2D image embeddings of adjacency matrices. We use this image representation in two modes. First, as the input to a machine learning algorithm. Second, as the input to a pure transfer learner. Our conclusions from multiple datasets are that\n 1. deep learning using structured image features performs the best compared to graph kernel and classical features based methods; and,\n 2. pure transfer learning works effectively with minimum interference from the user and is robust against small data.\n Our experiments overwhelmingly show that the structured image representation of graphs achieves successful graph classification with ease. The image representation is lossless, that is the image embeddings contain all the information in the corresponding adjacency matrix. Our results also show that even with very little information about the parent network, Deep network models are able to extract network signatures. Specifically, with just 64-node samples from networks with up to 1 million nodes, we were able to predict the parent network with > 90% accuracy while being significantly better than random with only 8-node samples. Further, we demonstrated that the image embedding approach provides many advantages over graph kernel and feature-based methods.We also presented an approach to graph classification using transfer learning from a completely different domain. Our approach converts graphs into 2D image embeddings and uses a pre-trained image classifier (Caffe) to obtain label-vectors. In a range of experiments with real-world data sets, we have obtained accuracies from 70% to 94% for 2-way classification and 61% for multi-way classification. Further, our approach is highly resilient to training-to-test ratio, that is, can work with sparse training samples. Our results show that such an approach is very promising, especially for applications where training data is not readily available (e.g. terrorist networks).Future work includes improvements to the transfer learning by improving the distance function between label-vectors, as well as using the probabilities from Caffe. Further , we would also look to generalize this approach to other domains, for example classifying radio frequency map samples using transfer learning."
}