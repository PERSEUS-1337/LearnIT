{
    "title": "ByquB-WC-",
    "content": "Memory Network based models have shown a remarkable progress on the task of relational reasoning.\n Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \n Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\n We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \n We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \n As a result, our model is as simple as RN but the computational complexity is reduced to linear time.\n It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. Neural network has made an enormous progress on the two major challenges in artificial intelligence: seeing and reading. In both areas, embedding methods have served as the main vehicle to process and analyze text and image data for solving classification problems. As for the task of logical reasoning, however, more complex and careful handling of features is called for. A reasoning task requires the machine to answer a simple question upon the delivery of a series of sequential information. For example, imagine that the machine is given the following three sentences: \"Mary got the milk there.\", \"John moved to the bedroom.\", and \"Mary traveled to the hallway.\" Once prompted with the question, \"Where is the milk?\", the machine then needs to sequentially focus on the two supporting sentences, \"Mary got the milk there.\" and \"Mary traveled to the hallway.\" in order to successfully determine that the milk is located in the hallway.Inspired by this reasoning mechanism, J. has introduced the memory network (MemNN), which consists of an external memory and four components: input feature map (I), generalization (G), output feature map (O), and response (R). The external memory enables the model to deal with a knowledge base without loss of information. Input feature map embeds the incoming sentences. Generalization updates old memories given the new input and output feature map finds relevant information from the memory. Finally, response produces the final output.Based on the memory network architecture, neural network based models like end-to-end memory network (MemN2N) BID11 , gated end-to-end memory network (GMemN2N) BID7 , dynamic memory network (DMN) BID6 , and dynamic memory network + (DMN+) BID13 are proposed. Since strong reasoning ability depends on whether the model is able to sequentially catching the right supporting sentences that lead to the answer, the most important thing that discriminates those models is the way of constructing the output feature map. As the output feature map becomes more complex, it is able to learn patterns for more complicate relations. For example, MemN2N, which has the lowest performance among the four models, measures the relatedness between question and sentence by the inner product, while the best performing DMN+ uses inner product and absolute difference with two embedding matrices.Recently, a new architecture called Relation Network (RN) BID9 has been proposed as a general solution to relational reasoning. The design philosophy behind it is to directly capture the supporting relation between the sentences through the multi-layer perceptron (MLP). Despite its simplicity, RN achieves better performance than previous models without any catastrophic failure.The interesting thing we found is that RN can also be interpreted in terms of MemNN. It is composed of O and R where each corresponds to MLP which focuses on the related pair and another MLP which infers the answer. RN does not need to have G because it directly finds all the supporting sentences at once. In this point of view, the significant component would be MLP-based output feature map. As MLP is enough to recognize highly non-linear pattern, RN could find the proper relation better than previous models to answer the given question.However, as RN considers a pair at a time unlike MemNN, the number of relations that RN learns is n 2 when the number of input sentence is n. When n is small, the cost of learning relation is reduced by n times compared to MemNN based models, which enables more data-efficient learning BID9 . However, when n increases, the performance becomes worse than the previous models. In this case, the pair-wise operation increases the number of non-related sentence pairs more than the related sentence pair, thereby confuses RN's learning. BID9 has suggested attention mechanisms as a solution to filter out unimportant relations; however, since it interrupts the reasoning operation, it may not be the most optimal solution to the problem.Our proposed model, \"Relation Memory Network\" (RMN), is able to find complex relation even when a lot of information is given. It uses MLP to find out relevant information with a new generalization which simply erase the information already used. In other words, RMN inherits RN's MLP-based output feature map on Memory Network architecture. Experiments show its state-ofthe-art result on the text-based question answering tasks."
}