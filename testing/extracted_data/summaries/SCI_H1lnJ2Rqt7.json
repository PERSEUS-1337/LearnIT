{
    "title": "H1lnJ2Rqt7",
    "content": "Stochastic Gradient Descent (SGD) methods using randomly selected batches are widely-used to train neural network (NN) models. Performing design exploration to find the best NN for a particular task often requires extensive training with different models on a large dataset,  which is very computationally expensive. The most straightforward method to accelerate this computation is to distribute the batch of SGD over multiple processors. However, large batch training often times leads to degradation in accuracy, poor generalization, and even poor robustness to adversarial attacks.   Existing solutions for large batch training either do not work or require massive hyper-parameter tuning. To address this issue, we propose a novel large batch training method which combines recent results in adversarial training (to regularize against ``sharp minima'') and second order optimization (to use curvature information to change batch size adaptively during training). We extensively evaluate our method on Cifar-10/100, SVHN, TinyImageNet, and ImageNet datasets, using multiple NNs, including residual networks as well as compressed networks such as SqueezeNext.   Our new approach exceeds the performance of the existing solutions in terms of both accuracy and the number of SGD iterations (up to 1\\% and $3\\times$, respectively). We emphasize that this is achieved without any additional hyper-parameter tuning to tailor our method to any of these experiments.\n Finding the right NN architecture for a particular application requires extensive hyper-parameter tuning and architecture search, often times on a very large dataset. The delays associated with training NNs is often the main bottleneck in the design process. One of the ways to address this issue to use large distributed processor clusters; however, to efficiently utilize each processor, the portion of the batch associated with each processor (sometimes called the mini-batch) must grow correspondingly. In the ideal case, the hope is to decrease the computational time proportional to the increase in batch size, without any drop in generalization quality. However, large batch training has a number of well known draw backs. These include degradation of accuracy, poor generalization, and poor robustness to adversarial perturbations BID17 BID36 .In order to address these drawbacks, many solutions have been proposed BID14 BID37 BID7 BID29 BID16 . However , these methods either work only for particular models on a particular dataset, or they require massive hyperparameter tuning, which is often times not discussed in the presentation of results. Note that while extensive hyper-parameter turning may result in good result tables, it is antithetical to the original motivation of using large batch sizes to reduce training time.One solution to reduce the brittleness of SGD to hyper-parameter tuning is to use second-order methods. Full Newton method with line search is parameter-free, and it does not require a learning rate. This is achieved by using a second-order Taylor series approximation to the loss function, instead of a first-order one as in SGD, to obtain curvature information. BID25 ; BID34 BID2 show that Newton/quasi-Newton methods outperform SGD for training NNs. However, their re-sults only consider simple fully connected NNs and auto-encoders. A problem with second-order methods is that they can exacerbate the large batch problem, as by construction they have a higher tendency to get attracted to local minima as compared to SGD. For these reasons, early attempts at using second-order methods for training convolutional NNs have so far not been successful.Ideally, if we could find a regularization scheme to avoid local/bad minima during training, this could resolve many of these issues. In the seminal works of El Ghaoui & BID9 ; BID33 , a very interesting connection was made between robust optimization and regularization. It was shown that the solution to a robust optimization problem for least squares is the same as the solution of a Tikhonov regularized problem BID9 . This was also extended to the Lasso problem in BID33 . Adversarial learning/training methods , which are a special case of robust optimization methods, are usually described as a min-max optimization procedure to make the model more robust. Recent studies with NNs have empirically found that robust optimization usually converges to points in the optimization landscape that are flatter and are more robust to adversarial perturbation BID36 .Inspired by these results, we explore whether second order information regularized by robust optimization can be used to do large batch size training of NNs. We show that both classes of methods have properties that can be exploited in the context of large batch training to help reduce the brittleness of SGD with large batch size training, thereby leading to significantly improved results. We introduce an adaptive batch size algorithm based on Hessian information to speed up the training process of NNs, and we combine this approach with adversarial training (which is a form of robust optimization, and which could be viewed as a regularization term for large batch training). We extensively test our method on multiple datasets (SVHN, Cifar-10/100, TinyImageNet and ImageNet) with multiple NN models (AlexNet, ResNet, Wide ResNet and SqueezeNext). As the goal of large batch is to reduce training time, we did not perform any hyper-parameter tuning to tailor our method for any of these tests. Our method allows one to increase batch size and learning rate automatically, based on Hessian information. This helps significantly reduce the number of parameter updates, and it achieves superior generalization performance, without the need to tune any of the additional hyper-parameters. Finally, we show that a block Hessian can be used to approximate the trend of the full Hessian to reduce the overhead of using second-order information. These improvements are useful to reduce NN training time in practice. \u2022 L(\u03b8) is continuously differentiable and the gradient function of L is Lipschitz continuous with Lipschitz constant L g , i.e. DISPLAYFORM0 for all \u03b8 1 and \u03b8 2 .Also , the global minima of L(\u03b8) is achieved at \u03b8 * and L(\u03b8 * ) = L * .\u2022 Each gradient of each individual l i (z i ) is an unbiased estimation of the true gradient, i.e. DISPLAYFORM1 where V(\u00b7) is the variance operator, i.e. DISPLAYFORM2 From the Assumption 2, it is not hard to get, DISPLAYFORM3 DISPLAYFORM4 With Assumption 2, the following two lemmas could be found in any optimization reference, e.g. . We give the proofs here for completeness. Lemma 3 . Under Assumption 2, after one iteration of stochastic gradient update with step size \u03b7 t at \u03b8 t , we have DISPLAYFORM5 where DISPLAYFORM6 Proof. With the L g smooth of L(\u03b8), we have DISPLAYFORM7 From above, the result follows.Lemma 4. Under Assumption 2, for any \u03b8, we have DISPLAYFORM8 Proof. Let DISPLAYFORM9 Then h(\u03b8) has a unique global minima at\u03b8 DISPLAYFORM10 The following lemma is trivial, we omit the proof here. DISPLAYFORM11 PROOF OF THEOREM 1Given these lemmas, we now proceed with the proof of Theorem 1.Proof. Assume the batch used at step t is b t , according to Lemma 3 and 5, DISPLAYFORM12 where the last inequality is from Lemma 4. This yields DISPLAYFORM13 It is not hard to see, DISPLAYFORM14 which concludes DISPLAYFORM15 Therefore, DISPLAYFORM16 We show a toy example of binary logistic regression on mushroom classification dataset 2 . We split the whole dataset to 6905 for training and 1819 for validation. \u03b7 0 = 1.2 for SGD with batch size 100 and full gradient descent. We set 100 \u2264 b t \u2264 3200 for our algorithm, i.e. ABS. Here we mainly focus on the training losses of different optimization algorithms. The results are shown in FIG3 . In order to see if \u03b7 0 is not an optimal step size of full gradient descent, we vary \u03b7 0 for full gradient descent; see results in FIG3 ."
}