{
    "title": "BviYjfnIk",
    "content": "Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility. However, manual remote piloting of a drone is prone to errors. In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections. Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest. The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy. A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system. Researchers in telepresence have long envisioned 'beyond being there' [21] . Replicating all relevant local experiences, while remote, should not be the only goal of telepresence; rather, we should also strive to create telepresence systems which can enable benefits that are not possible when the person is physically present. As such, telepresence goes from replication to augmentation. One particular instance of this vision is enabled by camera drones: our local bodies can only walk on the ground, but our remote bodies can fly. Current commercial remote robotic presence platforms have mostly been designed to replicate face-to-face conversation experiences [38, 55, 56] . Researchers have also explored their usage in a wider range of scenarios, such as shopping in a mall [55] or attending conferences [41] , and noted a number of social and functional issues due to their insufficient mobility [2, 19, 55] . With unmanned micro aerial vehicles (called 'drones' hereafter) becoming more affordable and reliable, they hold the potential for enabling more flexible remote presence and visual inspection experiences [16, 20, 25, 39, 48, 56] for the general population. While drones offer promise for such telepresence applications, they are challenging to manually control remotely, due to numerous factors including high degrees of freedom, narrow camera field-of-views, and network delays [40] . Their control interfaces -virtual or physical joysticks for consumer drones -are also unfamiliar for many users and take extended training time to master [48] . To relieve the burden of manual piloting, autopilot techniques have been applied to drone control. Most existing drone autopilot interfaces are based on specifying a series of planned waypoints in a 2D or 3D global map [10, 13, 48, 58] . However, in a situation where a user wishes to perform a real-time inspection, setting waypoints a priori may not be efficient for producing the viewer's desired viewpoints. For example, the viewer may wish to see something from a closer distance, from a different viewpoint, or view an area they didn't know about when the waypoints were set. Some autonomous systems avoid the use of waypoints and execute higher-level plans [11, 26, 33] , such as following a subject to form canonical shots [26] , but they typically do not offer the flexibility for exploring remote environments. The difficulty of drone piloting poses a significant barrier for the widespread adoption of free-flying robots. The goal of this research is to design a camera drone control interface to support efficient and flexible remote visual inspection for now universally adopted touchscreens. Inspired by recent work in semi-autonomous hybrid systems [33, 40] , we wish to combine the strengths of both manual and automatic piloting into a single hybrid navigation interface. Our work is also inspired by decades of research in interactive graphics, for which many camera navigation techniques have been established [5, 18, 29, 34] . Most relevant, we build upon object-centric techniques, where zooming, panning, and orbiting occurs relative to the location of a 3D object of interest. Existing object-aware drone navigation interfaces, such as DJI ActiveTrack [61] and XPose [33] , have been designed for aerial photography within visual line-of-sight. As such, they lack support for two important requirements of remote navigation and inspection: first, free exploration of a remote environment, which may include objects out of the initial camera field-of-view; and second, flexible inspection from various viewing angles or distances in relation to the objectof-interest. We propose StarHopper, a remote object-centric camera drone navigation interface that is operated through familiar touch interactions and relies on minimal geometric information of the environment (Figure 1 ). The system is designed based on a set of design goals for remote objectcentric drone navigation. It consists of an overhead camera view for context and a 3D-tracked drone's first-person view for focus. New objects of interest can be specified through simple touch gestures on both camera views. We combine automatic and manual control via four navigation mechanisms that can complement each other with unique strengths, to support efficient and flexible visual inspection. The system focuses on indoor environments, representative of tasks such as remote warehouse inspection [48] and museum visits [50, 51] , and where positional tracking technology is more reliable. A remote object inspection study showed that StarHopper was 35.4% faster than a manual baseline interface, for both simple and complex navigation routes. Users expressed general preferences towards object-centric navigation for remote inspection. We conclude by discussing potential design opportunities and future research to further increase the efficiency of remote visual inspection tasks. In this section, we discuss the implications of the results from our study, clarify important considerations and limitations related to our work, and propose future lines of research. To remove the barriers of using drones as free-flying remote inspection platforms, we explored touch-based object-centric navigation for camera drones through our prototype system, StarHopper. An in-lab study showed that users were able to achieve notable efficiency improvement using StarHopper for remote visual inspection, in comparison to a baseline condition using a touchscreen joystick for manual control. A strength of StarHopper comes from its combined use of a suite of automated, semiautomated, and manual control mechanisms to achieve efficiency and flexibility. In future work, we plan to empirically study users' mental models when working with automated camera drones, to understand how to build better human-automation collaboration for remote inspection. We are also interested in extending the usage of StarHopper to larger spaces and outdoor environments with a second drone as the overview camera. Our object-centric navigation techniques took inspiration from classical techniques in interactive graphics. As 3D sensing, reconstruction, object recognition, and other related fields advance, more powerful techniques initially developed for the virtual world may be applicable to telepresence navigation, taking us even closer to the vision of unconstrained 'beyond being there' telepresence."
}