{
    "title": "B1n8LexRZ",
    "content": "We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper. High-dimensional distributions that are only analytically tractable up to a normalizing constant are ubiquitous in many fields. For instance, they arise in protein folding BID41 , physics simulations BID37 , and machine learning BID1 . Sampling from such distributions is a critical task for learning and inference BID31 , however it is an extremely hard problem in general.Markov Chain Monte Carlo (MCMC) methods promise a solution to this problem. They operate by generating a sequence of correlated samples that converge in distribution to the target. This convergence is most often guaranteed through detailed balance, a sufficient condition for the chain to have the target equilibrium distribution. In practice, for any proposal distribution, one can ensure detailed balance through a Metropolis-Hastings BID20 accept/reject step.Despite theoretical guarantees of eventual convergence, in practice convergence and mixing speed depend strongly on choosing a proposal that works well for the task at hand. What's more, it is often more art than science to know when an MCMC chain has converged (\"burned-in\"), and when the chain has produced a new uncorrelated sample (\"mixed\"). Additionally, the reliance on detailed balance, which assigns equal probability to the forward and reverse transitions, often encourages random-walk behavior and thus slows exploration of the space BID24 .For densities over continuous spaces, Hamiltonian Monte Carlo (HMC; BID12 BID36 introduces independent, auxiliary momentum variables, and computes a new state by integrating Hamiltonian dynamics. This method can traverse long distances in state space with a single Metropolis-Hastings test. This is the state-of-the-art method for sampling in many domains. However , HMC can perform poorly in a number of settings. While HMC mixes quickly spatially, it struggles at mixing across energy levels due to its volume-preserving dynamics. HMC also does not work well with multi-modal distributions, as the probability of sampling a large enough momentum to traverse a very low-density region is negligibly small. Furthermore , HMC struggles with ill-conditioned energy landscapes BID14 and deals poorly with rapidly changing gradients BID44 .Recently, probabilistic models parameterized by deep neural networks have achieved great success at approximately sampling from highly complex, multi-modal empirical distributions BID27 BID39 BID16 . Building on these successes , we present a method that, given an analytically described distribution, automatically returns an exact sampler with good convergence and mixing properties, from a class of highly expressive parametric models. The proposed family of samplers is a generalization of HMC; it transforms the HMC trajectory using parametric functions (deep networks in our experiments), while retaining theoretical guarantees with a tractable Metropolis-Hastings accept/reject step. The sampler is trained to minimize a variation on expected squared jumped distance (similar in spirit to BID38 ). Our parameterization reduces easily to standard HMC. It is further capable of emulating several common extensions of HMC such as withintrajectory tempering BID34 and diagonal mass matrices BID4 .We evaluate our method on distributions where HMC usually struggles, as well as on a the real-world task of training latent-variable generative models.Our contributions are as follows:\u2022 We introduce a generic training procedure which takes as input a distribution defined by an energy function, and returns a fast-mixing MCMC kernel.\u2022 We show significant empirical gains on various distributions where HMC performs poorly.\u2022 We finally evaluate our method on the real-world task of training and sampling from a latent variable generative model, where we show improvement in the model's log-likelihood, and greater complexity in the distribution of posterior samples. In this work, we presented a general method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, analytically known up to a constant, our method provides a fast-mixing sampler, able to efficiently explore the state space. Our hope is that our method can be utilized in a \"black-box\" manner, in domains where sampling constitutes a huge bottleneck such as protein foldings BID41 or physics simulations BID37 .... DISPLAYFORM0 Figure 4: Diagram of our L2HMC-DGLM model. Nodes are functions of their parents. Round nodes are deterministic , diamond nodes are stochastic and the doubly-circled node is observed."
}