{
    "title": "S1l5s7298H",
    "content": "We introduce a \u201clearning-based\u201d algorithm for the low-rank decomposition problem: given an $n \\times d$ matrix $A$, and a parameter $k$, compute a rank-$k$ matrix $A'$ that minimizes the approximation loss $||A- A'||_F$. The algorithm uses a training set of input matrices in order to optimize its performance. Specifically, some of the most efficient approximate algorithms for computing low-rank approximations proceed by computing a projection $SA$, where $S$ is a sparse random $m \\times n$ \u201csketching matrix\u201d, and then performing the singular value decomposition of $SA$. We show how to replace the random matrix $S$ with a \u201clearned\u201d matrix of the same sparsity to reduce the error.\n\n Our experiments show that, for multiple types of data sets, a learned sketch matrix can substantially reduce the approximation loss compared to a random matrix $S$, sometimes by one order of magnitude. We also study mixed matrices where only some of the rows are trained and the remaining ones are random, and show that matrices still offer improved performance while retaining worst-case guarantees. The success of modern machine learning made it applicable to problems that lie outside of the scope of \"classic AI\". In particular, there has been a growing interest in using machine learning to improve the performance of \"standard\" algorithms, by fine-tuning their behavior to adapt to the properties of the input distribution, see e.g., [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] . This \"learning-based\" approach to algorithm design has attracted a considerable attention over the last few years, due to its potential to significantly improve the efficiency of some of the most widely used algorithmic tasks. Many applications involve processing streams of data (video, data logs, customer activity etc) by executing the same algorithm on an hourly, daily or weekly basis. These data sets are typically not \"random\" or \"worst-case\"; instead, they come from some distribution which does not change rapidly from execution to execution. This makes it possible to design better algorithms tailored to the specific data distribution, trained on past instances of the problem. The method has been particularly successful in the context of compressed sensing. In the latter framework, the goal is to recover an approximation to an n-dimensional vector x, given its \"linear measurement\" of the form Sx, where S is an m \u00d7 n matrix. Theoretical results [14, 15] show that, if the matrix S is selected at random, it is possible to recover the k largest coefficients of x with high probability using a matrix S with m = O(k log n) rows. This guarantee is general and applies to arbitrary vectors x. However, if vectors x are selected from some natural distribution (e.g., they represent images), recent works [8, 9, 11] show that one can use samples from that distribution to compute matrices S that improve over a completely random matrix in terms of the recovery error. Compressed sensing is an example of a broader class of problems which can be solved using random projections. Another well-studied problem of this type is low-rank decomposition: given an n \u00d7 d matrix A, and a parameter k, compute a rank-k matrix Low-rank approximation is one of the most widely used tools in massive data analysis, machine learning and statistics, and has been a subject of many algorithmic studies. In particular, multiple algorithms developed over the last decade use the \"sketching\" approach, see e.g., [16] [17] [18] [19] [20] [21] [22] [23] [24] . Its idea is to use efficiently computable random projections (a.k.a., \"sketches\") to reduce the problem size before performing low-rank decomposition, which makes the computation more space and time efficient. For example, [16, 19] show that if S is a random matrix of size m \u00d7 n chosen from an appropriate distribution, for m depending on , then one can recover a rank-k matrix A such that by performing an SVD on SA \u2208 R m\u00d7d followed by some post-processing. Typically the sketch length m is small, so the matrix SA can be stored using little space (in the context of streaming algorithms) or efficiently communicated (in the context of distributed algorithms). Furthermore, the SVD of SA can be computed efficiently, especially after another round of sketching, reducing the overall computation time. See the survey [25] for an overview of these developments. In light of the aforementioned work on learning-based compressive sensing, it is natural to ask whether similar improvements in performance could be obtained for other sketch-based algorithms, notably for low-rank decompositions. In particular, reducing the sketch length m while preserving its accuracy would make sketch-based algorithms more efficient. Alternatively, one could make sketches more accurate for the same values of m. This is the problem we address in this paper. Our Results. Our main finding is that learned sketch matrices can indeed yield (much) more accurate low-rank decompositions than purely random matrices. We focus our study on a streaming algorithm for low-rank decomposition due to [16, 19] , described in more detail in Section 2. Specifically, suppose we have a training set of matrices Tr = {A 1 , . . . , A N } sampled from some distribution D. Based on this training set, we compute a matrix S * that (locally) minimizes the empirical loss where SCW(S * , A i ) denotes the output of the aforementioned Sarlos-Clarkson-Woodruff streaming low-rank decomposition algorithm on matrix A i using the sketch matrix S * . Once the the sketch matrix S * is computed, it can be used instead of a random sketch matrix in all future executions of the SCW algorithm. We demonstrate empirically that, for multiple types of data sets, an optimized sketch matrix S * can substantially reduce the approximation loss compared to a random matrix S, sometimes by one order of magnitude (see Figure 1) . Equivalently, the optimized sketch matrix can achieve the same approximation loss for lower values of m. A possible disadvantage of learned sketch matrices is that an algorithm that uses them no longer offers worst-case guarantees. As a result, if such an algorithm is applied to an input matrix that does not conform to the training distribution, the results might be worse than if random matrices were used. To alleviate this issue, we also study mixed sketch matrices, where (say) half of the rows are trained and the other half are random. We observe that if such matrices are used in conjunction with the SCW algorithm, its results are no worse than if only the random part of the matrix was used 2 . Thus, the resulting algorithm inherits the worst-case performance guarantees of the random part of the sketching matrix. At the same time, we show that mixed matrices still substantially reduce the approximation loss compared to random ones, in some cases nearly matching the performance of \"pure\" learned matrices with the same number of rows. Thus, mixed random matrices offer \"the best of both worlds\": improved performance for matrices from the training distribution, and worst-case guarantees otherwise."
}