{
    "title": "BJerQWcp6Q",
    "content": "Biomedical knowledge bases are crucial in modern data-driven biomedical sciences, but auto-mated biomedical knowledge base construction remains challenging. In this paper, we consider the problem of disease entity normalization, an essential task in constructing a biomedical knowledge base.   We present NormCo, a deep coherence model which considers the semantics of an entity mention, as well as the topical coherence of the mentions within a single document. NormCo mod-els entity mentions using a simple semantic model which composes phrase representations from word embeddings, and treats coherence as a disease concept co-mention sequence using an RNN rather than modeling the joint probability of all concepts in a document, which requires NP-hard inference.   To overcome the issue of data sparsity, we used distantly supervised data and synthetic data generated from priors derived from the BioASQ dataset.   Our experimental results show thatNormCo outperforms state-of-the-art baseline methods on two disease normalization corpora in terms of (1) prediction quality and (2) efficiency, and is at least as performant in terms of accuracy and F1 score on tagged documents. Modern biomedical sciences are data-driven and depend on reliable databases of biomedical knowledge. These knowledge bases are particularly crucial in domains such as precision medicine BID3 , where the idea is to treat patients with the same condition differently according to their genetic profiles. Knowledge bases for precision medicine contain known associations between genetic variants, disease conditions, treatments, and reported outcomes (see, e.g., BID20 BID9 ).In the past decade, natural language processing has advanced in the biomedical domain and has been utilized to automatically extract knowledge from research publications in order to construct knowledge bases. For certain biomedical entities, algorithms are available to provide reliable extraction, for example genes and their protein products BID37 BID16 BID43 , diseases Gonzalez, 2008, Dang et al., 2018] , and chemicals . However , current NLP algorithms are far from practically useful to cope with the rapid growth of new publications. Meanwhile , AI, NLP, and machine learning in other domains are advancing quickly. It is therefore important to translate these advancements into the biomedical domain.This paper focuses on the problem of disease normalization, an essential step in the construction of a biomedical knowledge base as diseases are central to biomedicine.The problem has been studied with promising solutions, such as DNorm BID25 and TaggerOne BID24 ; however these approaches are based on surface-form feature engineering and shallow learning methods. The closest problems for which deep learning has been successfully used are entity linking and word sense disambiguation, with deep models that consider context and coherence. However, to the best of our knowledge the problem of how to apply deep learning to solve the problem of disease normalization has not been successfully addressed. Given this, we are concerned with the following questions in this work: How can one design a deep learning model for disease normalization that has high accuracy? How can one train this model given the relative lack of training data and the notorious need for large datasets when training deep models? How efficiently can such a model be trained compared to existing shallow learning approaches?To address these questions we present NormCo, a deep model designed to tackle issues unique to disease normalization. NormCo makes the following contributions:\u2022 A combination of two sub-models which leverage both semantic features and topical coherence to perform disease normalization.\u2022 Addressing the data sparsity problem by augmenting the relatively small existing disease normalization datasets with two corpora of distantly supervised data, extracted through two different methods from readily available biomedical datasets created for non-normalizationrelated purposes.\u2022 Outperforming state-of-the-art disease normalization methods in prediction quality when taking into account the severity of errors, while being at least as performant or better in terms of accuracy and F1 score on tagged documents.\u2022 Significantly faster training than existing state-of-the-art approaches (by 2 orders of magnitude faster than the next-best baseline approach, depending on the size of the training dataset).The paper is structured as follows: We start by describing the problem of disease normalization in Section 2 and surveying related work in disease normalization and entity linking, explaining why disease normalization is unique compared to generic entity linking in Section 3. Sections 4 and 5 present the architecture of our model and its implementation details, respectively. Finally, Section 6 presents the experimental evaluation of NormCo and Section 7 concludes the paper with a discussion of the results and future work toward a fully automated approach to biomedical knowledge base construction. Though deep models perform well in many domains, there is less evidence of them being utilized well in the acceleration and improvement of the construction of biomedical knowledge bases. This is due to the unique challenges in the biomedical domain, including a lack of a large training corpora. In this paper, we propose a deep model which considers semantics and coherence simultaneously to solve the problem of disease normalization, an essential step in the automated construction of biomedical knowledge bases. We demonstrate that the MeSH ontology and BioASQ dataset can be used as a useful source of additional data to support the training of a deep model to achieve good performance. Finally, we show that a model based on semantic features and coherence provides higher quality predictions for the task of disease normalization over state-of-the-art solutions. Still, many uniquely challenging AI problems await to be solved before fully automated construction of biomedical knowledge bases is possible. These problems deserve more attention and investment from the AI research community."
}