{
    "title": "HJM4SjR5KQ",
    "content": "We propose a framework to model the distribution of sequential data coming from\n a set of entities connected in a graph with a known topology. The method is\n based on a mixture of shared hidden Markov models (HMMs), which are trained\n in order to exploit the knowledge of the graph structure and in such a way that the\n obtained mixtures tend to be sparse. Experiments in different application domains\n demonstrate the effectiveness and versatility of the method. Hidden Markov models (HMMs) are a ubiquitous tool for modelling sequential data. They started by being applied to speech recognition systems and from there they have spread to almost any application one can think of, encompassing computational molecular biology, data compression, and computer vision. In the emerging field of cognitive radars BID12 , for the task of opportunistic usage of the spectrum, HMMs have been recently used to model the occupancy of the channels by primary users BID21 .When the expressiveness of an HMM is not enough, mixtures of HMM have been adopted. Roughly speaking, mixtures of HMMs can be interpreted as the result of the combination of a set of independent standard HMMs which are observed through a memoryless transformation BID5 BID8 BID22 BID13 .In many real-life settings one does not have a single data stream but an arbitrary number of network connected entities that share and interact in the same medium and generate data streams in real-time. The streams produced by each of these entities form a set of time series with both intra and inter relations between them. In neuroimaging studies, the brain can be regarded as a network: a connected system where nodes, or units, represent different specialized regions and links, or connections, represent communication pathways. From a functional perspective, communication is coded by temporal dependence between the activities of different brain areas BID6 . Also team sports intrinsically involve fast, complex and interdependent events among a set of entities (the players), which interact as a team BID24 BID23 . Thus, understanding a player's behavior implies understanding the behavior of his teammates and opponents over time.The extraction of knowledge from these streams to support the decision-making process is still challenging and the adaptation of HMM to this scenario is immature at best. BID9 proposed a hybrid approach combining the Self-Organizing Map (SOM) and the HMM with applications in clustering, dimensionality reduction and visualization of large-scale sequence spaces. Note that the model at each node is limited to a simple HMM. Wireless local area networks have also been modeled with Markov-based approaches. For instance, BID1 use HMMs for outlier detection in 802.11 wireless access points. However, the typical approaches include a common HMM model for all nodes (with strong limited flexibility) and a HMM model per node, independent of the others (not exploring the dependencies between nodes). BID4 built a sparse coupled hidden Markov model (SCHMM) framework to parameterize the temporal evolution of data acquired with functional magnetic resonance imaging (fMRI). The coupling is captured in the transition matrix, which is assumed to be a function of the activity levels of all the streams; the model per node is still restricted to a simple HMM.In general, in networked data streams, the stream observed in each sensor is often modeled by HMMs but the intercorrelations between sensors are seldom explored. The proper modeling of the intercorrelations has the potential to improve the learning process, acting as a regularizer in the learning process. In here we propose to tackle this void, by proposing as observation model at each node a sparse mixture of HMMs, where the dependencies between nodes are used to promote the sharing of HMM components between similar nodes. In this work we propose a method to model the generative distribution of sequential data coming from nodes connected in a graph with a known fixed topology. The method is based on a mixture of HMMs where its coefficients are regularized during the learning process in such a way that affine nodes will tend to have similar coefficients, exploiting the known graph structure. We also prove that the proposed regularizer promotes sparsity in the mixtures, which is achieved through a fully differentiable loss function (i.e. with no explicit L 0 penalty term). We evaluate the method's performance in two completely different tasks (anomaly detection in Wi-Fi networks and human motion forecasting), showing its effectiveness and versatility.For future work, we plan to extend/evaluate the usage of SpaMHMM for sequence clustering. This is an obvious extension that we did not explore thoroughly in this work, since its main focus was modeling the generative distribution of data. In this context, extending the idea behind SpaMHMM to mixtures of more powerful generative distributions is also in our plans. As is known, HMMs have limited expressiveness due to the strong independence assumptions they rely on. Thus, we plan to extend these ideas to develop an architecture based on more flexible generative models for sequence modeling, like those attained using deep recurrent architectures. After building the usual variational lower bound for the log-likelihood and performing the E-step, we get the following well-known objective:Jp\u03b8, \u03b8 -q \" \u00ff z,H ppX, z, H|y, \u03b8 -q log ppX, z, H|y, \u03b8q,which we want to maximize with respect to \u03b8 and where \u03b8 -are the model parameters that were kept fixed in the E-step. Some of the parameters in the model are constrained to represent valid probabilities, yielding the following Lagrangian: DISPLAYFORM0 Clearly, J r p\u03b8q\u00b4V r p\u03b8, qq \" 1 N\u02c6l og ppX|y, \u03b8q\u00b4E z,H\"q \" log ppX, z, H|y, \u03b8q qpz, Hq"
}