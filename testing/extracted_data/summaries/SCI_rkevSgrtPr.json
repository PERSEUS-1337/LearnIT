{
    "title": "rkevSgrtPr",
    "content": "The universal approximation theorem, in one of its most general versions, says that if we consider only continuous activation functions \u03c3, then a standard feedforward neural network with one hidden layer is able to approximate any continuous multivariate function f to any given approximation threshold \u03b5, if and only if \u03c3 is non-polynomial. In this paper, we give a direct algebraic proof of the theorem. Furthermore we shall explicitly quantify the number of hidden units required for approximation. Specifically, if X in R^n is compact, then a neural network with n input units, m output units, and a single hidden layer with {n+d choose d} hidden units (independent of m and \u03b5), can uniformly approximate any polynomial function f:X -> R^m whose total degree is at most d for each of its m coordinate functions. In the general case that f is any continuous function, we show there exists some N in O(\u03b5^{-n}) (independent of m), such that N hidden units would suffice to approximate f. We also show that this uniform approximation property (UAP) still holds even under seemingly strong conditions imposed on the weights. We highlight several consequences: (i) For any \u03b4 > 0, the UAP still holds if we restrict all non-bias weights w in the last layer to satisfy |w| < \u03b4. (ii) There exists some \u03bb>0 (depending only on f and \u03c3), such that the UAP still holds if we restrict all non-bias weights w in the first layer to satisfy |w|>\u03bb. (iii) If the non-bias weights in the first layer are *fixed* and randomly chosen from a suitable range, then the UAP holds with probability 1. A standard (feedforward) neural network with n input units, m output units, and with one or more hidden layers, refers to a computational model N that can compute a certain class of functions \u03c1 : R n \u2192 R m , where \u03c1 = \u03c1 W is parametrized by W (called the weights of N ). Implicitly, the definition of \u03c1 depends on a choice of some fixed function \u03c3 : R \u2192 R, called the activation function of N . Typically, \u03c3 is assumed to be continuous, and historically, the earliest commonly used activation functions were sigmoidal. A key fundamental result justifying the use of sigmoidal activation functions was due to Cybenko (1989) , Hornik et al. (1989) , and Funahashi (1989) , who independently proved the first version of what is now famously called the universal approximation theorem. This first version says that if \u03c3 is sigmoidal, then a standard neural network with one hidden layer would be able to uniformly approximate any continuous function f : X \u2192 R m whose domain X \u2286 R n is compact. Hornik (1991) extended the theorem to the case when \u03c3 is any continuous bounded non-constant activation function. Subsequently, Leshno et al. (1993) proved that for the class of continuous activation functions, a standard neural network with one hidden layer is able to uniformly approximate any continuous function f : X \u2192 R m on any compact X \u2286 R n , if and only if \u03c3 is non-polynomial. Although a single hidden layer is sufficient for the uniform approximation property (UAP) to hold, the number of hidden units required could be arbitrarily large. Given a subclass F of real-valued continuous functions on a compact set X \u2286 R n , a fixed activation function \u03c3, and some \u03b5 > 0, let N = N (F, \u03c3, \u03b5) be the minimum number of hidden units required for a single-hidden-layer neural network to be able to uniformly approximate every f \u2208 F within an approximation error threshold of \u03b5. If \u03c3 is the rectified linear unit (ReLU) x \u2192 max(0, x), then N is at least \u2126( 1 \u221a \u03b5 ) when F is the class of C 2 non-linear functions (Yarotsky, 2017) , or the class of strongly convex differentiable functions (Liang & Srikant, 2016) ; see also (Arora et al., 2018) . If \u03c3 is any smooth non-polynomial function, then N is at most O(\u03b5 \u2212n ) for the class of C 1 functions with bounded Sobolev norm (Mhaskar, 1996) ; cf. (Pinkus, 1999, Thm. 6.8) , (Maiorov & Pinkus, 1999) . As a key highlight of this paper, we show that if \u03c3 is an arbitrary continuous non-polynomial function, then N is at most O(\u03b5 \u2212n ) for the entire class of continuous functions. In fact, we give an explicit upper bound for N in terms of \u03b5 and the modulus of continuity of f , so better bounds could be obtained for certain subclasses F, which we discuss further in Section 4. Furthermore, even for the wider class F of all continuous functions f : X \u2192 R m , the bound is still O(\u03b5 \u2212n ), independent of m. To prove this bound, we shall give a direct algebraic proof of the universal approximation theorem, in its general version as stated by Leshno et al. (1993) (i.e. \u03c3 is continuous and non-polynomial). An important advantage of our algebraic approach is that we are able to glean additional information on sufficient conditions that would imply the UAP. Another key highlight we have is that if F is the subclass of polynomial functions f : X \u2192 R m with total degree at most d for each coordinate function, then n+d d hidden units would suffice. In particular, notice that our bound N \u2264 n+d d does not depend on the approximation error threshold \u03b5 or the output dimension m. We shall also show that the UAP holds even under strong conditions on the weights. Given any \u03b4 > 0, we can always choose the non-bias weights in the last layer to have small magnitudes no larger than \u03b4. Furthermore, we show that there exists some \u03bb > 0 (depending only on \u03c3 and the function f to be approximated), such that the non-bias weights in the first layer can always be chosen to have magnitudes greater than \u03bb. Even with these seemingly strong restrictions on the weights, we show that the UAP still holds. Thus, our main results can be collectively interpreted as a quantitative refinement of the universal approximation theorem, with extensions to restricted weight values. Outline: Section 2 covers the preliminaries, including relevant details on arguments involving dense sets. Section 3 gives precise statements of our results, while Section 4 discusses the consequences of our results. Section 5 introduces our algebraic approach and includes most details of the proofs of our results; details omitted from Section 5 can be found in the appendix. Finally, Section 6 concludes our paper with further remarks. The universal approximation theorem (version of Leshno et al. (1993) ) is an immediate consequence of Theorem 3.2 and the observation that \u03c3 must be non-polynomial for the UAP to hold, which follows from the fact that the uniform closure of P \u2264d (X) is P \u2264d (X) itself, for every integer d \u2265 1. Alternatively, we could infer the universal approximation theorem by applying the Stone-Weirstrass theorem (Theorem 2.1) to Theorem 3.1. Given fixed n, m, d, a compact set X \u2286 R n , and \u03c3 \u2208 C(R)\\P \u2264d\u22121 (R), Theorem 3.1 says that we could use a fixed number N of hidden units (independent of \u03b5) and still be able to approximate any function f \u2208 P \u2264d (X, R m ) to any desired approximation error threshold \u03b5. Our \u03b5-free bound, although possibly surprising to some readers, is not the first instance of an \u03b5-free bound: Neural networks with two hidden layers of sizes 2n + 1 and 4n + 3 respectively are able to uniformly approximate any f \u2208 C(X), provided that we use a (somewhat pathological) activation function (Maiorov & Pinkus, 1999) ; cf. (Pinkus, 1999) . Lin et al. (2017) showed that for fixed n, d, and a fixed smooth non-linear \u03c3, there is a fixed N (i.e. \u03b5-free), such that a neural network with N hidden units is able to approximate any f \u2208 P \u2264d (X). An explicit expression for N is not given, but we were able to infer from their constructive proof that N = 4 n+d+1 d \u2212 4 hidden units are required, over d \u2212 1 hidden layers (for d \u2265 2). In comparison, we require less hidden units and a single hidden layer. Our proof of Theorem 3.2 is an application of Jackson's theorem (Theorem 2.2) to Theorem 3.1, which gives an explicit bound in terms of the values of the modulus of continuity \u03c9 f of the function f to be approximated. The moduli of continuity of several classes of continuous functions have explicit characterizations. For example, given constants k > 0 and 0 < \u03b1 \u2264 1, recall that a continuous function f : for all x, y \u2208 X, and it is called \u03b1-H\u00f6lder if there is some constant c such that |f (x)\u2212f (y)| \u2264 c x\u2212y \u03b1 for all x, y \u2208 X. The modulus of continuity of a k-Lipschitz (resp. \u03b1-H\u00f6lder) continuous function f is \u03c9 f (t) = kt (resp. \u03c9 f (t) = ct \u03b1 ), hence Theorem 3.2 implies the following corollary. n \u2192 R is \u03b1-H\u00f6lder continuous, then there is a constant k such that for every \u03b5 > 0, there exists some An interesting consequence of Theorem 3.3 is the following: The freezing of lower layers of a neural network, even in the extreme case that all frozen layers are randomly initialized and the last layer is the only \"non-frozen\" layer, does not necessarily reduce the representability of the resulting model. Specifically, in the single-hidden-layer case, we have shown that if the non-bias weights in the first layer are fixed and randomly chosen from some suitable fixed range, then the UAP holds with probability 1, provided that there are sufficiently many hidden units. Of course, this representability does not reveal anything about the learnability of such a model. In practice, layers are already pre-trained before being frozen. It would be interesting to understand quantitatively the difference between having pre-trained frozen layers and having randomly initialized frozen layers. Theorem 3.3 can be viewed as a result on random features, which were formally studied in relation to kernel methods (Rahimi & Recht, 2007) . In the case of ReLU activation functions, Sun et al. (2019) proved an analog of Theorem 3.3 for the approximation of functions in a reproducing kernel Hilbert space; cf. (Rahimi & Recht, 2008) . For a good discussion on the role of random features in the representability of neural networks, see (Yehudai & Shamir, 2019) . The UAP is also studied in other contexts, most notably in relation to the depth and width of neural networks. Lu et al. (2017) proved the UAP for neural networks with hidden layers of bounded width, under the assumption that ReLU is used as the activation function. Soon after, Hanin (2017) strengthened the bounded-width UAP result by considering the approximation of continuous convex functions. Recently, the role of depth in the expressive power of neural networks has gathered much interest (Delalleau & Bengio, 2011; Eldan & Shamir, 2016; Mhaskar et al., 2017; Mont\u00fafar et al., 2014; Telgarsky, 2016) . We do not address depth in this paper, but we believe it is possible that our results can be applied iteratively to deeper neural networks, perhaps in particular for the approximation of compositional functions; cf. (Mhaskar et al., 2017) . Theorem 5.6 is rather general, and could potentially be used to prove analogs of the universal approximation theorem for other classes of neural networks, such as convolutional neural networks and recurrent neural networks. In particular, finding a single suitable set of weights (as a representative of the infinitely many possible sets of weights in the given class of neural networks), with the property that its corresponding \"non-bias Vandermonde matrix\" (see Definition 5.5) is non-singular, would serve as a straightforward criterion for showing that the UAP holds for the given class of neural networks (with certain weight constraints). We formulated this criterion to be as general as we could, with the hope that it would applicable to future classes of \"neural-like\" networks. We believe our algebraic approach could be emulated to eventually yield a unified understanding of how depth, width, constraints on weights, and other architectural choices, would influence the approximation capabilities of arbitrary neural networks. Finally, we end our paper with an open-ended question. The proofs of our results in Section 5 seem to suggest that non-bias weights and bias weights play very different roles. We could impose very strong restrictions on the non-bias weights and still have the UAP. What about the bias weights? First, we recall the notion of generalized Wronskians as given in (LeVeque, 1956, Chap. 4.3) . Let \u2206 0 , . . . , \u2206 N \u22121 be any N differential operators of the form and let x = (x 1 , . . . , x n ). Recall that \u03bb 1 < \u00b7 \u00b7 \u00b7 < \u03bb N are all the n-tuples in \u039b n \u2264d in the colexicographic order. For each be the coefficient of the monomial q k (x) in \u2206 \u03bbi p(x). Consider an arbitrary W \u2208 U, and for each 1 \u2264 j \u2264 N , define f j \u2208 P \u2264d (R n ) by the map x \u2192 p(w 1,j x 1 , . . . , w n,j x n ). Note that F p,0n (W ) = (f 1 , . . . , f N ) by definition. Next, define the matrix M W (x) := [\u2206 i f j (x)] 1\u2264i,j\u2264N , and note that det M W (x) is the generalized Wronskian of (f 1 , . . . , f N ) associated to \u2206 1 , . . . , \u2206 N . In particular, this generalized Wronskian is well-defined, since the definition of the colexicographic order implies that \u03bb k,1 + \u00b7 \u00b7 \u00b7 + \u03bb k,n \u2264 k for all possible k. Similar to the univariate case, (f 1 , . . . , f N ) is linearly independent if (and only if) its generalized Wronskian is not the zero function (Wolsson, 1989) . Thus, to show that W \u2208 p U ind , it suffices to show that the evaluation det M W (1 n ) of this generalized Wronskian at x = 1 n gives a non-zero value, where 1 n denotes the all-ones vector in R n . Observe that the (i, j)-th entry of M W (1 n ) equals ( w It follows from the definition of the colexicographic order that \u03bb j \u2212 \u03bb i necessarily contains at least one strictly negative entry whenever j < i, hence we infer that M is upper triangular. The diagonal entries of M are \u03b1 0n , . . . , \u03b1 0n , and note that \u03b1 \u03bbi for each 1 \u2264 i \u2264 N , where \u03bb i,1 ! \u00b7 \u00b7 \u00b7 \u03bb i,n ! denotes the product of the factorials of the entries of the n-tuple \u03bb i . In particular, \u03bb i,1 ! \u00b7 \u00b7 \u00b7 \u03bb i,n ! = 0, and \u03b1 (1) \u03bbi , which is the coefficient of the monomial q i (x) in p(x), is non-zero. Thus, det(M ) = 0. We have come to the crucial step of our proof. If we can show that det(M ) = det(Q[W ]) = 0, then det(M W (1 n )) = det(M ) det(M ) = 0, and hence we can infer that W \u2208 p U ind . This means that p U ind contains the subset U \u2286 U consisting of all W such that Q[W ] is non-singular. Note that det(Q [W ] ) is a polynomial in terms of the non-bias weights in W (1) as its variables, so we could write this polynomial as r = r(W ). Consequently, if we can find a single W \u2208 U such that Q[W ] is non-singular, then r(W ) is not identically zero on U, which then implies that U = {W \u2208 U : r(W ) = 0} is dense in U (w.r.t. the Euclidean metric)."
}