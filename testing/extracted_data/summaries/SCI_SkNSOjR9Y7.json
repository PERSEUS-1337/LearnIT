{
    "title": "SkNSOjR9Y7",
    "content": "The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\n Standard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO). Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD). However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible. Till now, there\nexist no simple solutions to circumvent this problem.\n In this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations. Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t In this paper, we derived an easy estimator for the ELBO, which does not rely on reparametrized sampling and therefore can be used to obtain differentiable estimates, even if reparametrization is not possible, e.g. if the latent variables z are Bernoulli or Categorically distributed. We have shown theoretically and in experiments, close to the optimal parameter configuration, the variance of the estimator approaches zero. This is a very desirable property for training."
}