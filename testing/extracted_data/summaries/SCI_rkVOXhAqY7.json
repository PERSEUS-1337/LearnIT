{
    "title": "rkVOXhAqY7",
    "content": "We present a new family of objective functions, which we term the Conditional Entropy Bottleneck (CEB). These objectives are motivated by the Minimum Necessary Information (MNI) criterion. We demonstrate the application of CEB to classification tasks. We show that CEB gives: well-calibrated predictions; strong detection of challenging out-of-distribution examples and powerful whitebox adversarial examples; and substantial robustness to those adversaries. Finally, we report that CEB fails to learn from information-free datasets, providing a possible resolution to the problem of generalization observed in Zhang et al. (2016). The field of Machine Learning has suffered from the following well-known problems in recent years 1 :\u2022 Vulnerability to adversarial examples. Essentially all machine-learned systems are currently believed by default to be highly vulnerable to adversarial examples. Many defenses have been proposed, but very few have demonstrated robustness against a powerful, general-purpose adversary. Lacking a clear theoretical framework for adversarial attacks, most proposed defenses are ad-hoc and fail in the presence of a concerted attacker BID8 BID5 ).\u2022 Poor out-of-distribution detection. Classifiers do a poor job of signaling that they have received data that is substantially different from the data they were trained on. Ideally, a trained classifier would give less confident predictions for data that was far from the training distribution (as well as for adversarial examples). Barring that , there would be a clear, principled statistic that could be extracted from the model to tell whether the model should have made a low-confidence prediction. Many different approaches to providing such a statistic have been proposed BID18 BID28 BID19 BID32 BID30 BID13 , but most seem to do poorly on what humans intuitively view as obviously different data.\u2022 Miscalibrated predictions. Related to the issues above, classifiers tend to be very overconfident in their predictions BID18 . This may be a symptom, rather than a cause, but miscalibration does not give practitioners confidence in their models.\u2022 Overfitting to the training data. BID48 demonstrated that classifiers can memorize fixed random labelings of training data, which means that it is possible to learn a classifier with perfect inability to generalize. This critical observation makes it clear that a fundamental test of generalization is that the model should fail to learn when given what we call information-free datasets. We have presented the basic form of the Conditional Entropy Bottleneck (CEB), motivated by the Minimum Necessary Information (MNI) criterion for optimal representations. We have shown through careful experimentation that simply by switching to CEB, you can expect substantial improvements in OoD detection, adversarial example detection and robustness, calibration, and generalization. Additionally, we have shown that it is possible to get all of these advantages without using any additional form of regularization, and without any new hyperparameters. We have argued empirically that objective hyperparameters can lead to hard-to-predict suboptimal behavior, such as memorizing random labels, or reducing robustness to adversarial examples. In Appendix E and in future work, we will show how to generalize CEB beyond the simple case of two observed variables.It is our perspective that all of the issues explored here -miscalibration, failure at OoD tasks, vulnerability to adversarial examples, and dataset memorization -stem from the same underlying issue, which is retaining too much information about the training data in the learned representation. We believe that the MNI criterion and CEB show a path forward for many tasks in machine learning, permitting fast, amortized inference while ameliorating major problems. a b Figure 4 : Geometry of the optimal surfaces for both CEB (purple) and IB (green) for models that can only come within of the optimal surface (a: = 0.1I(X; Y); b: = 0.01I(X; Y)). The tangent lines have the slope of the corresponding \u03b2 -the tangent point on the ball corresponds to the point on the pareto-optimal frontier for the corresponding model. Note that \u03b2 determines the \"exchange rate\" between bits of I(X; Z) and I(Y; Z), which is how we determine the coordinate of the center of the ball. For IB to achieve the MNI point, 2 bits of I(Y; Z) are needed for every bit of I(X; Z). Consequently, even for an infitely powerful model (corresponding to = 0), the only value of \u03b2 that hits the MNI point is \u03b2 = 2. Thus, knowing the function (\u03b2) for a given model and dataset completely determines the model's pareto-optimal frontier.Here we collect a number of results that are not critical to the core of the paper, but may be of interest to particular audiences.A Analysis of CEB and IB From Equation FORMULA5 and the definition of CEB in Equation (6), the following equivalence between CEB and IB is obvious: DISPLAYFORM0 where we are parameterizing IB with \u03b2 on the I(Y; Z) term for convenience. This equivalence generalizes as follows: DISPLAYFORM1 DISPLAYFORM2 In Figure 4 , we show the combined information planes for CEB and IB given the above parameterization. The figures show the simple geometry that determines a point on the pareto-optimal frontier for both objectives. Every such point is fully determined by the function (\u03b2) for a given model and dataset, where is the closest the model can approach the true optimal surface. (\u03b2) = 0 corresponds to the \"infinite\" model family that exactly traces out the boundaries of the feasible region. The full feasible regions can be seen in Figure 2 .From this geometry we can immediately conclude that if an IB model and a CEB model have the same value of > 0 at equivalent \u03b2, the CEB model will always yield a value of I(Y; Z) closer to I(X; Y). This is because the slope of the tangent lines for CEB are always lower, putting the tangent points higher on the ball. This gives part of a theoretical justification for the empirical observations above that V IB 0.5 (equivalent to IB 2 in the parameterization we are describing here) fails to capture as much of the necessary information as the CEB model. Even at the pareto-optimal frontier, V IB 0.5 cannot get I(Y; Z) as close to I(X; Y) as CEB can. Of course , we do not want to claim that this effect accounts for the fairly substantial difference in performance -that is likely to be due to a combination of other factors, including the fact that it is often easier to train continuous conditional distributions (like b(z|y)) than it is to train continuous marginal distributions (like m(z)).We also think that this analysis of the geometry of IB and CEB supports our preference for targeting the MNI point and treating CEB as an objective without hyperparameters. First, there are only a maximum of 4 points of interest in both the IB and CEB information planes (all 4 are visibile in Figure 2 ): the origin, where there is no information in the representation; the MNI point; the point at (I(Y; Z) = I(X; Y), I(X; Z) = H(X)) (which is an MDL-compatible representation BID17 ); and the point at (I(Y; Z) = 0, I(X; Z) = H(X|Y)) (which would be the optimal decoder for an MNI representation). These are the only points naturally identified by the dataset -selecting a point on one of the edges between those four points seems to need additional justification. Second, if you do agree with the MNI criterion, for a given model it is impossible to get any closer to the MNI point than by setting CEB's \u03b2 = 1, due to the convexity of the pareto-optimal frontier. Much more useful is making changes to the model, architecture, dataset, etc in order to make smaller. One possibility in that direction that IB and CEB models offer is inspecting training examples with high rate or residual information to check for label noise, leading to a natural human-in-the-loop model improvement algorithm. Another is using CEB's residual information as a measure of the quality of the trained model, as mentioned in Appendix C."
}