{
    "title": "HyxlHsActm",
    "content": "Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. Many central problems in machine learning and signal processing are most naturally formulated as optimization problems. These problems are often both nonconvex and highdimensional. High dimensionality makes the evaluation of second-order information prohibitively expensive, and thus randomly initialized first-order methods are usually employed instead. This has prompted great interest in recent years in understanding the behavior of gradient descent on nonconvex objectives (18; 14; 17; 11) . General analysis of first-and second-order methods on such problems can provide guarantees for convergence to critical points but these may be highly suboptimal, since nonconvex optimization is in general an NP-hard probem BID3 . Outside of a convex setting (28) one must assume additional structure in order to make statements about convergence to optimal or high quality solutions. It is a curious fact that for certain classes of problems such as ones that involve sparsification (25; 6) or matrix/tensor recovery (21; 19; 1) first-order methods can be used effectively. Even for some highly nonconvex problems where there is no ground truth available such as the training of neural networks first-order methods converge to high-quality solutions (40).Dictionary learning is a problem of inferring a sparse representation of data that was originally developed in the neuroscience literature (30), and has since seen a number of important applications including image denoising, compressive signal acquisition and signal classification (13; 26) . In this work we study a formulation of the dictionary learning problem that can be solved efficiently using randomly initialized gradient descent despite possessing a number of saddle points exponential in the dimension. A feature that appears to enable efficient optimization is the existence of sufficient negative curvature in the directions normal to the stable manifolds of all critical points that are not global minima BID0 . This property ensures that the regions of the space that feed into small gradient regions under gradient flow do not dominate the parameter space. FIG0 illustrates the value of this property: negative curvature prevents measure from concentrating about the stable manifold. As a consequence randomly initialized gradient methods avoid the \"slow region\" of around the saddle point. Negative curvature helps gradient descent. Red: \"slow region\" of small gradient around a saddle point. Green: stable manifold associated with the saddle point. Black: points that flow to the slow region. Left: global negative curvature normal to the stable manifold. Right: positive curvature normal to the stable manifold -randomly initialized gradient descent is more likely to encounter the slow region.The main results of this work is a convergence rate for randomly initialized gradient descent for complete orthogonal dictionary learning to the neighborhood of a global minimum of the objective. Our results are probabilistic since they rely on initialization in certain regions of the parameter space, yet they allow one to flexibly trade off between the maximal number of iterations in the bound and the probability of the bound holding.While our focus is on dictionary learning, it has been recently shown that for other important nonconvex problems such as phase retrieval BID7 performance guarantees for randomly initialized gradient descent can be obtained as well. In fact, in Appendix C we show that negative curvature normal to the stable manifolds of saddle points (illustrated in FIG0 ) is also a feature of the population objective of generalized phase retrieval, and can be used to obtain an efficient convergence rate. The above analysis suggests that second-order properties -namely negative curvature normal to the stable manifolds of saddle points -play an important role in the success of randomly initialized gradient descent in the solution of complete orthogonal dictionary learning. This was done by furnishing a convergence rate guarantee that holds when the random initialization is not in regions that feed into small gradient regions around saddle points, and bounding the probability of such an initialization. In Appendix C we provide an additional example of a nonconvex problem that for which an efficient rate can be obtained based on an analysis that relies on negative curvature normal to stable manifolds of saddles -generalized phase retrieval. An interesting direction of further work is to more precisely characterize the class of functions that share this feature.The effect of curvature can be seen in the dependence of the maximal number of iterations T on the parameter \u03b6 0 . This parameter controlled the volume of regions where initialization would lead to slow progress and the failure probability of the bound 1 \u2212 P was linear in \u03b6 0 , while T depended logarithmically on \u03b6 0 . This logarithmic dependence is due to a geometric increase in the distance from the stable manifolds of the saddles during gradient descent, which is a consequence of negative curvature. Note that the choice of \u03b6 0 allows one to flexibly trade off between T and 1 \u2212 P. By decreasing \u03b6 0 , the bound holds with higher probability, at the price of an increase in T . This is because the volume of acceptable initializations now contains regions of smaller minimal gradient norm. In a sense, the result is an extrapolation of works such as (23) that analyze the \u03b6 0 = 0 case to finite \u03b6 0 .Our analysis uses precise knowledge of the location of the stable manifolds of saddle points.For less symmetric problems, including variants of sparse blind deconvolution (41) and overcomplete tensor decomposition, there is no closed form expression for the stable manifolds. However , it is still possible to coarsely localize them in regions containing negative curvature. Understanding the implications of this geometric structure for randomly initialized first-order methods is an important direction for future work.One may hope that studying simple model problems and identifying structures (here, negative curvature orthogonal to the stable manifold) that enable efficient optimization will inspire approaches to broader classes of problems. One problem of obvious interest is the training of deep neural networks for classification, which shares certain high-level features with the problems discussed in this paper. The objective is also highly nonconvex and is conjectured to contain a proliferation of saddle points BID10 , yet these appear to be avoided by first-order methods BID15 for reasons that are still quite poorly understood beyond the two-layer case (39).[19] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. DISPLAYFORM0 . Thus critical points are ones where either tanh( q \u00b5 ) = 0 (which cannot happen on S n\u22121 ) or tanh( q \u00b5 ) is in the nullspace of (I \u2212 qq * ), which implies tanh( q \u00b5 ) = cq for some constant b. The equation tanh( x \u00b5 ) = bx has either a single solution at the origin or 3 solutions at {0, \u00b1r(b)} for some r(b). Since this equation must be solves simultaneously for every element of q, we obtain \u2200i \u2208 [n] : q i \u2208 {0, \u00b1r(b)}. To obtain solutions on the sphere, one then uses the freedom we have in choosing b (and thus r(b)) such that q = 1. The resulting set of critical points is thus DISPLAYFORM1 To prove the form of the stable manifolds, we first show that for q i such that |q i | = q \u221e and any q j such that |q j | + \u2206 = |q i | and sufficiently small \u2206 > 0, we have DISPLAYFORM2 For ease of notation we now assume q i , q j > 0 and hence \u2206 = q i \u2212 q j , otherwise the argument can be repeated exactly with absolute values instead. The above inequality can then be written as DISPLAYFORM3 If we now define DISPLAYFORM4 where the O(\u2206 2 ) term is bounded. Defining a vector r \u2208 R n by DISPLAYFORM5 we have r 2 = 1. Since tanh(x) is concave for x > 0, and |r i | \u2264 1, we find DISPLAYFORM6 From DISPLAYFORM7 and thus q j \u2265 1 \u221a n \u2212 \u2206. Using this inequality and properties of the hyperbolic secant we obtain DISPLAYFORM8 and plugging in \u00b5 = c \u221a n log n for some c < 1 DISPLAYFORM9 log n + log log n + log 4).We can bound this quantity by a constant, say h 2 \u2264 1 2 , by requiring DISPLAYFORM10 ) log n + log log n \u2264 \u2212 log 8and for and c < 1, using \u2212 log n + log log n < 0 we have DISPLAYFORM11 Since \u2206 can be taken arbitrarily small, it is clear that c can be chosen in an n-independent manner such that A \u2264 \u2212 log 8. We then find DISPLAYFORM12 since this inequality is strict, \u2206 can be chosen small enough such that O(\u2206 2 ) < \u2206(h 1 \u2212 h 2 ) and hence h > 0, proving 9.It follows that under negative gradient flow, a point with |q j | < ||q|| \u221e cannot flow to a point q such that |q j | = ||q || \u221e . From the form of the critical points, for every such j, q must thus flow to a point such that q j = 0 (the value of the j coordinate cannot pass through 0 to a point where |q j | = ||q || \u221e since from smoothness of the objective this would require passing some q with q j = 0, at which point grad [f Sep ] (q ) j = 0).As for the maximal magnitude coordinates , if there is more than one coordinate satisfying |q i1 | = |q i2 | = q \u221e , it is clear from symmetry that at any subsequent point q along the gradient flow line q i1 = q i2 . These coordinates cannot change sign since from the smoothness of the objective this would require that they pass through a point where they have magnitude smaller than 1/ \u221a n, at which point some other coordinate must have a larger magnitude (in order not to violate the spherical constraint), contradicting the above result for non-maximal elements. It follows that the sign pattern of these elements is preserved during the flow. Thus there is a single critical point to which any q can flow, and this is given by setting all the coordinates with |q j | < q \u221e to 0 and multiplying the remaining coordinates by a positive constant to ensure the resulting vector is on S n . Denoting this critical point by \u03b1, there is a vector b such that q = P S n\u22121 [a(\u03b1) + b] and supp(a(\u03b1)) \u2229 supp(b) = \u2205, b \u221e < 1 with the form of a(\u03b1) given by 5 . The collection of all such points defines the stable manifold of \u03b1.Proof of Lemma 2: (Separable objective gradient projection). i) We consider the sign(w i ) = 1 case; the sign(w i ) = \u22121 case follows directly.Recalling that DISPLAYFORM13 qn , we first prove DISPLAYFORM14 for some c > 0 whose form will be determined later. The inequality clearly holds for w i = q n .To DISPLAYFORM15 verify that it holds for smaller values of w i as well, we now show that \u2202 \u2202w i tanh w i \u00b5 \u2212 tanh q n \u00b5 w i q n \u2212 c(q n \u2212 w i ) < 0 which will ensure that it holds for all w i . We define s 2 = 1 \u2212 ||w|| 2 + w 2 i and denote q n = s 2 \u2212 w 2 i to extract the w i dependence, givingWhere in the last inequality we used properties of the sech function and q n \u2265 w i . We thus want to show DISPLAYFORM16 and it follows that 10 holds. For \u00b5 < 1 BID15 we are guaranteed that c > 0.From examining the RHS of 10 (and plugging in q n = s 2 \u2212 w 2 i ) we see that any lower bound on the gradient of an element w j applies also to any element |w i | \u2264 |w j |. Since for |w j | = ||w|| \u221e we have q n \u2212 w j = w j \u03b6, for every log( 1 \u00b5 )\u00b5 \u2264 w i we obtain the bound DISPLAYFORM17 Proof of Theorem 1: (Gradient descent convergence rate for separable function).We obtain a convergence rate by first bounding the number of iterations of Riemannian gradient descent in C \u03b60 \\C 1 , and then considering DISPLAYFORM18 . Choosing c 2 so that \u00b5 < 1 2 , we can apply Lemma 2, and for u defined in 7, we thus have DISPLAYFORM19 Since from Lemma 7 the Riemannian gradient norm is bounded by \u221a n, we can choose c 1 , c 2 such that \u00b5 log( DISPLAYFORM20 . This choice of \u03b7 then satisfies the conditions of Lemma 17 with r = \u00b5 log( DISPLAYFORM21 , M = \u221a n, which gives that after a gradient step DISPLAYFORM22 for some suitably chosenc > 0. If we now define by w (t) the t-th iterate of Riemannian gradient descent and DISPLAYFORM23 and the number of iterations required to exit C \u03b60 \\C 1 is DISPLAYFORM24 To bound the remaining iterations, we use Lemma 2 to obtain that for every w \u2208 C \u03b60 \\B \u221e r , DISPLAYFORM25 where we have used ||u DISPLAYFORM26 We thus have DISPLAYFORM27 Choosing DISPLAYFORM28 where L is the gradient Lipschitz constant of f s , from Lemma 5 we obtain DISPLAYFORM29 According to Lemma B, L = 1/\u00b5 and thus the above holds if we demand \u03b7 < \u00b5 2 . Combining 12 and 13 gives DISPLAYFORM30 .To obtain the final rate, we use in g(w 0 ) \u2212 g * \u2264 \u221a n andc\u03b7 < 1 \u21d2 1 log(1+c\u03b7) <C c\u03b7 for som\u1ebd C > 0. Thus one can choose C > 0 such that DISPLAYFORM31 From Lemma 1 the ball B \u221e r contains a global minimizer of the objective, located at the origin.The probability of initializing in \u0202 C \u03b60 is simply given from Lemma 3 and by summing over the 2n possible choices of C \u03b60 , one for each global minimizer (corresponding to a single signed basis vector)."
}