{
    "title": "rkxUfANKwB",
    "content": "Variational autoencoders (VAEs) defined over SMILES string and graph-based representations of molecules promise to improve the optimization of molecular properties, thereby revolutionizing the pharmaceuticals and materials industries. However, these VAEs are hindered by the non-unique nature of SMILES strings and the computational cost of graph convolutions. To efficiently pass messages along all paths through the molecular graph, we encode multiple SMILES strings of a single molecule using a set of stacked recurrent neural networks, harmonizing hidden representations of each atom between SMILES representations, and use attentional pooling to build a final fixed-length latent representation. By then decoding to a disjoint set of SMILES strings of the molecule, our All SMILES VAE learns an almost bijective mapping between molecules and latent representations near the high-probability-mass subspace of the prior. Our SMILES-derived but molecule-based latent representations significantly surpass the state-of-the-art in a variety of fully- and semi-supervised property regression and molecular property optimization tasks. The design of new pharmaceuticals, OLED materials, and photovoltaics all require optimization within the space of molecules (Pyzer-Knapp et al., 2015) . While well-known algorithms ranging from gradient descent to the simplex method facilitate efficient optimization, they generally assume a continuous search space and a smooth objective function. In contrast, the space of molecules is discrete and sparse. Molecules correspond to graphs, with each node labeled by one of ninety-eight naturally occurring atoms, and each edge labeled as a single, double, or triple bond. Even within this discrete space, almost all possible combinations of atoms and bonds do not form chemically stable molecules, and so must be excluded from the optimization domain, yet there remain as many as 10 60 small molecules to consider (Bohacek et al., 1996) . Moreover, properties of interest are often sensitive to even small changes to the molecule (Stumpfe & Bajorath, 2012) , so their optimization is intrinsically difficult. Efficient, gradient-based optimization can be performed over the space of molecules given a map between a continuous space, such as R n or the n-sphere, and the space of molecules and their properties (Sanchez-Lengeling & Aspuru-Guzik, 2018) . Initial approaches of this form trained a variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014) on SMILES string representations of molecules (Weininger, 1988) to learn a decoder mapping from a Gaussian prior to the space of SMILES strings (G\u00f3mez-Bombarelli et al., 2018) . A sparse Gaussian process on molecular properties then facilitates Bayesian optimization of molecular properties within the latent space (Dai et al., 2018; G\u00f3mez-Bombarelli et al., 2018; Kusner et al., 2017; Samanta et al., 2018) , or a neural network regressor from the latent space to molecular properties can be used to perform gradient descent on molecular properties with respect to the latent space (Aumentado-Armstrong, 2018; Jin et al., 2018; Liu et al., 2018; Mueller et al., 2017) . Alternatively, semi-supervised VAEs condition the decoder on the molecular properties (Kang & Cho, 2018; Lim et al., 2018) , so the desired properties can be specified directly. Recurrent neural networks have also been trained to model SMILES strings directly, and tuned with transfer learning, without an explicit latent space or encoder (Gupta et al., 2018; Segler et al., 2017) . SMILES, the simplified molecular-input line-entry system, defines a character string representation of a molecule by performing a depth-first pre-order traversal of a spanning tree of the molecular graph, emitting characters for each atom, bond, tree-traversal decision, and broken cycle (Weininger, 1988) . The resulting character string corresponds to a flattening of a spanning tree of the molecular graph, as shown in Figure 1 . The SMILES grammar is restrictive, and most strings over the appropriate character set do not correspond to well-defined molecules. Rather than require the VAE decoder to explicitly learn this grammar, context-free grammars (Kusner et al., 2017) , and attribute grammars (Dai et al., 2018) have been used to constrain the decoder, increasing the percentage of valid SMILES strings produced by the generative model. Invalid SMILES strings and violations of simple chemical rules can be avoided entirely by operating on the space of molecular graphs, either directly (De Cao & Kipf, 2018; Ma et al., 2018; Li et al., 2018; Liu et al., 2018; Simonovsky & Komodakis, 2018) or via junction trees (Jin et al., 2018) . Every molecule is represented by many well-formed SMILES strings, corresponding to all depth-first traversals of every spanning tree of the molecular graph. The distance between different SMILES strings of the same molecule can be much greater than that between SMILES strings from radically dissimilar molecules (Jin et al., 2018) , as shown in Figure 8 of Appendix A. A generative model of individual SMILES strings will tend to reflect this geometry, complicating the mapping from latent space to molecular properties and creating unnecessary local optima for property optimization (Vinyals et al., 2015) . To address this difficulty, sequence-to-sequence transcoders (Sutskever et al., 2014) have been trained to map between different SMILES strings of a single molecule (Bjerrum, 2017; Bjerrum & Sattarov, 2018; Winter et al., 2019b; a) . Reinforcement learning, often combined with adversarial methods, has been used to train progressive molecule growth strategies (Guimaraes et al., 2017; Jaques et al., 2017; Olivecrona et al., 2017; Putin et al., 2018; You et al., 2018; Zhou et al., 2018) . While these approaches have achieved state-of-the-art optimization of simple molecular properties that can be evaluated quickly in silico, critic-free techniques generally depend upon property values of algorithm-generated molecules (but see (De Cao & Kipf, 2018; Popova et al., 2018) ), and so scale poorly to real-world properties requiring time-consuming wet-lab experiments. Molecular property optimization would benefit from a generative model that directly captures the geometry of the space of molecular graphs, rather than SMILES strings, but efficiently infers a latent representation sensitive to spatially distributed molecular features. To this end, we introduce the All SMILES VAE, which uses recurrent neural networks (RNNs) on multiple SMILES strings to implicitly perform efficient message passing along and amongst many flattened spanning trees of the molecular graph in parallel. A fixed-length latent representation is distilled from the variablelength RNN output using attentional mechanisms. From this latent representation, the decoder RNN reconstructs a set of SMILES strings disjoint from those input to the encoder, ensuring that the latent representation only captures features of the molecule, rather than its SMILES realization. Simple property regressors jointly trained on this latent representation surpass the state-of-the-art for molecular property prediction, and facilitate exceptional gradient-based molecular property optimization when constrained to the region of the prior containing almost all the probability around it. We further demonstrate that the latent representation forms a near-bijection with the space of molecules, and is smooth with respect to molecular properties, facilitating effective optimization. For a complete delineation of our novel contributions relative to past work, see Appendix B.4. For each molecule, the All SMILES encoder uses stacked, pooled RNNs on multiple SMILES strings to efficiently pass information throughout the molecular graph. The decoder targets a disjoint set of SMILES strings of the same molecule, forcing the latent space to develop a consistent representation for each molecule. Attentional mechanisms in the approximating posterior summarize spatially diffuse features into a fixed-length, non-factorial approximating posterior, and construct a latent representation on which linear regressors achieve state-of-the-art semi-and fully-supervised property prediction. Gradient-based optimization of these regressor outputs with respect to the latent representation, constrained to a subspace near almost all probability in the prior, produces state-of-the-art optimized molecules when coupled with a simple RNN decoder. A.1 ZINC For molecular property optimization and fully supervised property prediction, we train the All SMILES VAE on the ZINC250k dataset of 250,000 organic molecules with between 6 and 38 heavy atoms, and penalized logPs 5 from -13 to 5 (G\u00f3mez-Bombarelli et al., 2018). This dataset is curated from a subset of the ZINC12 dataset (Irwin et al., 2012) , and available from https: //github.com/aspuru-guzik-group/chemical_vae. The distribution of molecular diameters in ZINC250k is shown in Figure 9 . For semi-supervised property prediction on logP, MW, and QED, we train on the ZINC310k dataset of 310,000 organic molecules with between 6 and 38 heavy atoms (Kang & Cho, 2018 ). This dataset is curated from the full ZINC15 dataset (Sterling & Irwin, 2015) , and available from https://github.com/nyu-dl/conditional-molecular-design-ssvae. Figure 9 : Histogram of molecular diameters in the ZINC250k dataset. The diameter is defined as the maximum eccentricity over all atoms in the molecular graph. The mean is 11.1; the maximum is 24. Typical implementations of graph convolution use only three to seven rounds of message passing (Duvenaud et al., 2015; Gilmer et al., 2017; Jin et al., 2018; Kearnes et al., 2016; Liu et al., 2018; Samanta et al., 2018; You et al., 2018) , and so cannot propagate information across most molecules in this dataset."
}