{
    "title": "rJxEso0osm",
    "content": "A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data. In addition, it has been noted that the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back-propagation algorithm used for neural networks (Eisner (2016)). Do these observations suggest that, despite their many apparent differences, HMMs are a special case of RNNs? In this paper, we show that that is indeed the case, and investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization. In particular, we investigate three key design factors\u2014independence assumptions between the hidden states and the observation, the placement of softmaxes, and the use of non-linearities\u2014in order to pin down their empirical effects. We present a comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction. The sequence is a common structure among many forms of naturally occurring data, including speech, text, video, and DNA. As such, sequence modeling has long been a core research problem across several fields of machine learning and AI. By far the most widely used approach for decades is Hidden Markov Models BID1 BID10 , which assumes a sequence of discrete latent variables to generate a sequence of observed variables. When the latent variables are unobserved, unsupervised training of HMMs can be performed via the Baum-Welch algorithm (which, in turn, is based on the forward-backward algorithm), as a special case of Expectation-Maximization (EM) BID4 . Importantly, the discrete nature of the latent variables has the benefit of interpretability, as they recover contextual clustering of the output variables. In contrast, Recurrent Neural Networks (RNNs) BID11 BID6 introduced later assume continuous latent representations. Their hidden states have no probabilistic interpretation, regardless of many different architectural variants, such as LSTMs BID9 , GRUs BID3 and RANs BID13 .Despite their many apparent differences, both HMMs and RNNs model hidden representations for sequential data. At the heart of both models are: a state at time t, a transition function f : h t\u22121 \u2192 h t in latent space, and an emission function g : h t \u2192 x t . In addition , it has been noted that the backward computation in the Baum-Welch algorithm is a special case of back-propagation for neural networks BID5 . Therefore, a natural question arises as to the fundamental relationship between HMMs and RNNs. Might HMMs be a special case of RNNs?In this paper , we investigate a series of architectural transformations between HMMs and RNNsboth through theoretical derivations and empirical hybridization. In particular , we demonstrate that forward marginal inference for an HMM-accumulating forward probabilities to compute the marginal emission and hidden state distributions at each time step-can be reformulated as equations for computing an RNN cell. In addition, we investigate three key design factors-independence Figure 1 : Above each of the models we indicate the type of transition and emission cells used. H for HMM, R for RNN/Elman and F is a novel Fusion defined in \u00a73.3. It is particularly important to track when a vector is a distribution (resides in a simplex) versus in the unit cube (e.g. after a sigmoid non-linearity). These are indicated by c i and c i , respectively. SM stands for softmax rows.assumptions between the hidden states and observations, the placement of softmaxes, and the use of non-linearities-in order to pin down their empirical effects. While we focus on HMMs with discrete outputs, our analysis framework could be extended to HMMs over continuous observations.Our work builds on earlier work that have also noted the connection between RNNs and HMMs BID23 BID25 (see \u00a77). Our contribution is to provide the first thorough theoretical investigation into the model variants, carefully controlling for every design choices, along with comprehensive empirical analysis over the spectrum of possible hybridization between HMMs and RNNs.We find that the key elements to better performance of the HMMs are the use of a sigmoid instead of softmax linearity in the recurrent cell, and the use of an unnormalized output distribution matrix in the emission computation. On the other hand, multiplicative integration of the previous hidden state and input embedding, and intermediate normalizations in the cell computation are less consequential. We also find that HMMs outperform other RNNs variants for unsupervised prediction of the next POS tag, demonstrating the advantages of discrete bottlenecks for increased interpretability.The paper is structured as follows. First, we present the derivation of HMM marginal inference as a special case of RNN computation ( \u00a72). Next we explore a gradual transformation of HMMs into RNNs ( \u00a73), followed by the reverse transformation of Elman RNNs back to HMMs ( \u00a74). Finally we provide empirical analysis in \u00a75 and \u00a76 to pin point the effects of varying design choices over possible hybridizations between HMMs and RNNs."
}