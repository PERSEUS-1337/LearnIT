{
    "title": "S1l6Qa3mcN",
    "content": "An important type of question that arises in Explainable Planning is a contrastive question, of the form \"Why action A instead of action B?\". These kinds of questions can be answered with a contrastive explanation that compares properties of the original plan containing A against the contrastive plan containing B. An effective explanation of this type serves to highlight the differences between the decisions that have been made by the planner and what the user would expect, as well as to provide further insight into the model and the planning process. Producing this kind of explanation requires the generation of the contrastive plan. This paper introduces domain-independent compilations of user questions into constraints. These constraints are added to the planning model, so that a solution to the new model represents the contrastive plan. We introduce a formal description of the compilation from user question to constraints in a temporal and numeric PDDL2.1 planning setting. Explainable AI (XAI) is an emerging and important research area within AI. Recent work has shown that AI Planning is an important tool in XAI, as its decision-making mechanisms are model-based and so in principle more transparent. This recent work includes many approaches towards providing explanations in AI planning. BID3 gives an in-depth overview of this work and different terms used within the XAI landscape. In particular, BID16 shows that if an AI system behaves \"explicably\" there is less of a need for explanations. However, this is not always possible and explanation is sometimes required. BID2 tackles explanation as a model reconciliation problem, arguing that the explanation must be a difference between the human model and AI model. BID14 show that by representing plans as first order logic formulae generating explanations is feasible in real time. In contrast, in this paper we focus on contrastive \"why\" questions. BID4 highlight some important questions in XAIP and discuss possible answers, and also describe how these \"why\" questions are especially important. BID15 outlines the approach to planning as an iterative process for bet- ter modelling preferences and providing explanations. We propose to follow this same approach.The aim of explanations is to improve the user's levels of understanding and trust in the system they are using. These explanations can be local (regarding a specific plan) or global (concerning how the planning system works in general). In this paper we focus on local explanations of temporal and numeric planning problems, introducing an approach for explaining why a planner has made a certain decision. Through active exploration of these specific cases, the user may also gain global insight into the way in which the planner makes decisions. (See BID9 BID10 Ribeiro, Singh, and Guestrin 2016) ).To achieve an understanding of a decision, it is important that explanations adapt to the specific context and mental model of the user. One step towards this is to support the user iteratively asking different questions suitable for their context. BID6 identify ten question types that a user might have about an intelligent system, also described by BID13 . BID8 show in a grounded study that of these, the questions why and why not provided the most benefit in terms of objective understanding and feelings of trust. In the context of planning why not questions are contrastive questions, because the user is asking why some action was selected rather than some other action that was not.Instead, Miller argues that all such questions can be asked as contrastive questions of the form \"Why action A rather than action B?\" BID11 . Contrastive questions capture the context of the question; they more precisely identify the gaps in the user's understanding of a plan that needs to be explained BID7 . A contrastive question about a plan can be answered by a contrastive explanation. Contrastive explanations will compare the original plan against a contrastive plan that accounts for the user expectation. Providing contrastive explanations is not only effective in improving understanding, but is simpler than providing a full causal analysis BID12 .Following the approach of Smith (2012 ) we propose an approach to contrastive explanations through a dialogue with the user. The proposed approach consists of an iterative four-stage process illustrated in FIG0 . First the user asks a contrastive question in natural language. Second, a constraint is derived from the user question, in the following we refer to this constraint as the formal question. Third a hypothetical model (HModel) is generated which encapsulates this constraint. A solution to this model is the hypothetical plan (HPlan) that can be compared to the original plan to show the consequence of the user suggestion. The user can compare plans and iterate the process by asking further questions, and refining the HModel. This allows the user to combine different compilations to create a more constrained HModel, producing more meaningful explanations, until the explanation is satisfactory. Each stage of this process represents a vital research challenge. This paper describes and formalises the third stage of this process: compiling the formal question into a hypothetical model for temporal and numeric planning.We are interested in temporal and numeric planning problems, for which optimal solutions are difficult to find. Therefore, while the process described above serves for explanation, the insight of the user can also result in guiding the planning process to a more efficient solution. As noted by BID15 , the explanations could also give the user the opportunity to improve the plan with respect to their own preferences. The user could have hidden preferences which have not been captured in the model. The user could ask questions which enforce constraints that favour these preferences. The new plan could be sub-optimal, but more preferable to the user.The contribution of this paper is a formalisation of domain-independent and planner-agnostic compilations from formal contrastive questions to PDDL2.1 (Fox and Long 2003) , necessary for providing contrastive explanations. The compilations shown are not exhaustive. However, they do cover an interesting set of questions which users would commonly have about both classical and temporal plans. The paper is organised as follows. The next section describes the planning definitions we will use throughout the paper. In Section 3 we describe the running example that we use to demonstrate our compilations throughout the paper. In Section 4 we list the set of formal questions that we are interested in, and formalise the compilations of each of these into constraints. Finally, we conclude the paper in Section 5 whilst touching on some interesting future work."
}