{
    "title": "Byldr3RqKX",
    "content": "Deep generative models such as Generative Adversarial Networks (GANs) and\n Variational Auto-Encoders (VAEs) are important tools to capture and investigate\n the properties of complex empirical data. However, the complexity of their inner\n elements makes their functionment challenging to assess and modify. In this\n respect, these architectures behave as black box models. In order to better\n understand the function of such networks, we analyze their modularity based on\n the counterfactual manipulation of their internal variables. Our experiments on the\n generation of human faces with VAEs and GANs support that modularity between\n activation maps distributed over channels of generator architectures is achieved\n to some degree, can be used to better understand how these systems operate and allow meaningful transformations of the generated images without further training.\n erate and edit the content of generated images. Deep generative models have proven powerful in learning to design realistic images in a variety of complex domains (handwritten digits, human faces, interior scenes). Complex neural architectures are now used to learn complex empirical data distributions by designing a non-linear function mapping a latent space to the space observations. In particular, two distinct approaches have recently emerged as state of the art: Generative Adversarial Networks (GANs) BID4 , and Variational Autoencoders (VAEs) BID8 BID17 . Such architectures relate to a question that work in Neuroscience and Computer vision have long since tried to address: the relation between an observed scene and its high level internal representation. This has been framed using two objects: (1) the mapping of a 3D scene to its perceived (2D) image, called forward optics, (2) the converse mapping, called inverse optics, (see e.g. BID7 ). Many computer vision algorithms have relied on inverse graphics approaches that model both forward and inverse optics simultaneously BID9 . In recent years, emphasis has been put on producing compact descriptions of the scene in terms of high level features reflecting a disentangled latent representation that can be mapped back to the image. However, the fundamental asymmetry between the structure of original forward and inverse optics mappings has received less attention. A key difference is that forward optics can be concisely described with a restricted set of equations taking into account physical parameters of the scene, while inverse optics does not have an explicit form and relies heavily on prior assumptions to be solved numerically. The simplicity of the forward optics may allow an agent to efficiently manipulate and update internal representations, for instance to plan interactions with the outside world, following a predictive coding principle BID16 . This supports that modularity of generative models should be assessed and enforced in order to understand and manipulate representations.Achieving this aim for deep architectures is challenging, because they mostly behave as black boxes, making it difficult for users to interact with the generative process. Indeed, we can act on how the network is trained (e.g. the optimized objective), what it learns to generate, but not on how the learned generative process operates. For example, to use a face generator to create a face combining the eyes of one generated face with remaining features of another one may be achieved by either additional training or complex manipulation of the network's input or output. Directly influencing the generative process learned by the network on the other hand is made difficult due to the complexity of the function class entailed by the networks' non-linearities and high dimensional parameter space. To grasp the properties of such a system, a possible approach is to intervene on parts of the architecture that implements the generative function. Ideally, the effect of such interventions on the output would be interpretable. This suggests we should uncover a modular structure in those architectures, such that each part of a network can be assigned a specific function.In this paper, we propose that modularity can be quantified and exploited in a causal framework to infer whether modules within the architecture can be further disentangled. This hypothesis relies on the general principle of Independence of Mechanisms stating that the various mechanisms involved in generating the observed data can be modified individually without affecting each other BID14 . It has been recently demonstrated that this principle can be applied to generative models encountered in machine learning BID0 . One key aspect of causality frameworks is to allow evaluating with counterfactuals how the outcome of a observed system would have changed, provided some variables would have taken different values. We use such counterfactuals to assess the role of specific internal variables in the overall functioning of trained deep generative models and uncover the modular structure of these systems. We start by introducing this perspective formally with the notion of intrinsic disentanglement, and show that it extends the classical notion of disentangled representation investigated in the deep learning literature. Then, we introduce tools to analyze this disentanglement in existing systems. We show empirically how VAEs and GANs trained on a human face dataset express a form of modularity with intermediate activation maps responsible for encoding different parts of the generated images.Related work. The issue of interpretability in convolutional neural networks has been the topic of intensive investigation. Most of that research however has focused on discriminative neural networks, not generative ones. In the discriminative case, efforts have been made to find optimal activation patterns for filters BID20 , BID2 ), to find correlation between intermediate feature space and data features BID3 , BID23 or to disentangle patterns detected by various filters to compute an explanatory graph BID22 . Furthermore, explicitly enforcing modularity in networks has been tried recently with Capsule networks architectures BID18 ), although Capsule network explicitly separate the architecture in different modules before training. A more detailed overview can found in review BID21 . It is important to emphasize discriminative and generative processes differ significantly, and working on generative processes allows to directly observe the effect of changes in intermediate representations on the generated picture rather than having to correlate it back input images. The recent InfoGAN network BID1 ) and other works BID11 ; BID9 ; BID5 ) in disentanglement of latent variables in generative models can be seen as what we define as extrinsic disentanglement. As such, we believe our intrinsic disentanglement perspective should be complementary with such approaches and are not in direct competition. Finally our approach relates to modularity and invariance principles formulated in the field of causality, in particular as formalized by BID0 . The purpose of this paper was to introduce a methodology to assess modularity in deep networks. Modularity may involve different aspects, and strongly depends on the nature of the modeled data. In this paper, we focused on features of the image that preferentially occur in specific parts of the generated images. This is a reasonable assumption for the CelebA dataset, given that the faces are spatially aligned. To some extent this is also true for the CIFAR10 dataset, where objects preferentially appear at the center of the image and the soil and sky in the background will be found at the bottom and top respectively. This approach may however have some limitations when looking at different datasets deprived from such spatial organization. In this case, capturing the structure of output variations induced by hybridization may require a more general approach. In principle, multidimensional technique such as Principal Component Analysis and non-linear generalizations may be able to characterize counterfactuals of each channels in order to further generate relevant modules following the steps described in the present work.Another aspect that is left to further work is how to optimize modularity in deep generative networks. We believe that classical (extrinsic) disentanglement approaches will not help, as they focus on the input of the network without control on its internal structure. While current generative models seem to exhibit some amount of modularity, improving it may require specific learning objectives as well as an appropriate choice of architectures."
}