{
    "title": "Hyez1CVYvr",
    "content": "Deep neural networks have achieved great success in classi\ufb01cation tasks during the last years. However, one major problem to the path towards arti\ufb01cial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classi\ufb01cation algorithms assume that all classes are known prior to the training stage. In this work, we propose a methodology for training a neural network that allows it to ef\ufb01ciently detect out-of-distribution (OOD) examples without compromising much of its classi\ufb01cation accuracy on the test examples from known classes. Based on the Outlier Exposure (OE) technique, we propose a novel loss function that achieves state-of-the-art results in out-of-distribution detection with OE both on image and text classi\ufb01cation tasks. Additionally, the way this method was constructed makes it suitable for training any classi\ufb01cation algorithm that is based on Maximum Likelihood methods. Modern neural networks have recently achieved superior results in classification problems (Krizhevsky et al., 2012; He et al., 2016) . However, most of the classification algorithms proposed so far make the assumption that data generated from all the class conditional distributions are available during training time i.e., they make the closed-world assumption. In an open world environment (Bendale & Boult, 2015) , where examples from novel class distributions might appear during test time, it is necessary to build classifiers that are able to detect OOD examples while having high classification accuracy on known class distributions. It is generally known that deep neural networks can make predictions for out-of-distribution (OOD) examples with high confidence (Nguyen et al., 2015) . High confidence predictions are undesirable since they consist a symptom of overfitting (Szegedy et al., 2015) . They also make the calibration of neural networks difficult. observed that modern neural networks are miscalibrated by experimentally showing that the average confidence of deep neural networks is usually much higher than their accuracy. A simple yet effective method to address the problem of the inability of neural networks to detect OOD examples is to train them so that they make highly uncertain predictions for examples generated by novel class distributions. In order to achieve that, Lee et al. (2018a) defined a loss function based on the Kullback-Leibler (KL) divergence metric to minimize the distance between the output distribution given by softmax and the uniform distribution for samples generated by a GAN (Goodfellow et al., 2014) . Using a similar loss function, Hendrycks et al. (2019) showed that the technique of Outlier Exposure (OE) that draws anomalies from a real and diverse dataset can outperform the GAN framework for OOD detection. Using the OE technique, our main contribution is threefold: \u2022 We propose a novel loss function consisting of two regularization terms. The first regularization term minimizes the l 1 norm between the output distribution given by softmax and the uniform distribution which constitutes a distance metric between the two distributions (Deza & Deza, 2009 ). The second regularization term minimizes the Euclidean distance between the training accuracy of a DNN and its average confidence in its predictions on the training set. \u2022 We experimentally show that the proposed loss function outperforms the previous work of Hendrycks et al. (2019) and achieves state-of-the-art results in OOD detection with OE both on image and text classification tasks. \u2022 We experimentally show that our proposed method can be combined with the Mahalanobis distance-based classifier (Lee et al., 2018b) . The combination of the two methods outperforms the original Mahalanobis method in all of the experiments and to the best of our knowledge, achieves state-of-the-art results in the OOD detection task. 2 RELATED WORK used the GAN framework (Goodfellow et al., 2014) to generate negative instances of seen classes by finding data points that are close to the training instances but are classified as fake by the discriminator. Then, they used those samples in order to train SVM classifiers to detect examples from unseen classes. Similarly, Kliger & Fleishman (2018) used a multi-class GAN framework in order to produce a generator that generates a mixture of nominal data and novel data and a discriminator that performs simultaneous classification and novelty detection. Hendrycks & Gimpel (2017) proposed a baseline for detecting misclassified and out-of-distibution examples based on their observation that the prediction probability of out-of-distribution examples tends to be lower than the prediction probability for correct examples. Recently, Corbi\u00e8re et al. (2019) also studied the problem of detecting overconfident incorrect predictions. A single-parameter variant of Platt scaling (Platt, 1999) , temperature scaling, was proposed by for calibration of modern neural networks. For image data, based on the idea of Hendrycks & Gimpel (2017) , Liang et al. (2018) observed that simultaneous use of temperature scaling and small perturbations at the input can push the softmax scores of in-and out-of-distribution images further apart from each other, making the out-of-distribution images distinguishable. Lee et al. (2018a) generated GAN examples and forced the neural network to have lower confidence in predicting their classes. Hendrycks et al. (2019) substituted the GAN samples with a real and diverse dataset using the technique of OE. Similar works (Malinin & Gales, 2018; Bevandi\u0107 et al., 2018 ) also force the model to make uncertain predictions for OOD examples. Using an ensemble of classifiers, Lakshminarayanan et al. (2017) showed that their method was able to express higher uncertainty in OOD examples. Liu et al. (2018) provided theoretical guarantees for detecting OOD examples under the assumption that an upper bound of the fraction of OOD examples is available. Under the assumption that the pre-trained features of a softmax neural classifier can be fitted well by a class-conditional Gaussian distribution, Lee et al. (2018b) defined a confidence score using the Mahalanobis distance that can efficiently detect abnormal test samples. As also mentioned by Lee et al. (2018b) , Euclidean distance can also be used but with less efficiency. We prefer to call these methods Distance-Based Post-Training (DBPT) methods for OOD detection. In this paper, we proposed a method for simultaneous classification and out-of-distribution detection. The proposed loss function includes two regularization terms where the first minimizes the l 1 norm between the output distribution of the softmax layer of a DNN and the uniform distribution, while the second minimizes the Euclidean distance between the training accuracy of a DNN and its average confidence in its predictions on the training set. Experimental results showed that the proposed loss function achieves state-of-the-art results in OOD detection with OE (Hendrycks et al., 2019) in both image and text classification tasks. Additionally, we experimentally showed that our method can be combined with DBPT methods for OOD detection like the Mahalanobis distance-based classifier (Lee et al., 2018b) Table 5 : Image OOD example detection for the maximum softmax probability (MSP) baseline detector after fine-tuning with OE (Hendrycks et al., 2019) versus fine-tuning with our proposed loss function given by (3). All results are percentages and averaged over 10 runs. Values are rounded to the first decimal digit. The Street View House Number (SVHN) dataset (Netzer et al., 2011) consists of 32 \u00d7 32 color images out of which 604,388 are used for training and 26,032 are used for testing. The dataset has 10 classes and was collected from real Google Street View images. Similar to Hendrycks et al. (2019) , we rescale the pixels of the images to be in [0, 1]. CIFAR 10: This dataset (Krizhevsky & Hinton, 2009) contains 10 classes and consists of 60,000 32 \u00d7 32 color images out of which 50,000 belong to the training and 10,000 belong to the test set. Before training, we standardize the images per channel similar to Hendrycks et al. (2019) . CIFAR 100: This dataset (Krizhevsky & Hinton, 2009 ) consists of 20 distinct superclasses each of which contains 5 different classes giving us a total of 100 classes. The total number of images in the dataset are 60,000 and we use the standard 50,000/10,000 train/test split. Before training, we standardize the images per channel similar to Hendrycks et al. (2019) . 80 Million Tiny Images: The 80 Million Tiny Images dataset (Torralba et al., 2008) consists of 10,000 images belonging to 50 classes of icons. As part of preprocessing, we removed the class \"Number\" in order to make it disjoint from the SVHN dataset. Textures: This dataset contains 5,640 textural images (Cimpoi et al., 2014) . LSUN: It consists of around 1 million large-scale images of scenes (Yu et al., 2015) . Rademacher: A synthetic image dataset created by sampling from a symmetric Rademacher distribution. (Socher et al., 2013 ) is a binary classification dataset for sentiment prediction of movie reviews containing around 10,000 examples. WikiText-2: This dataset contains over 2 million articles from Wikipedia and is exclusively used as D OE out in our experiments. We used the same preprocessing as in Hendrycks et al. (2019) in order to have a valid comparison. SNLI: The Stanford Natural Language Inference (SNLI) corpus is a collection of 570,000 humanwritten English sentence pairs (Bowman et al., 2015) . IMDB: A sentiment classification dataset containing movies reviews. Multi30K: A dataset of English and German descriptions of images (Elliott et al., 2016) . For our experiments, only the English descriptions were used. WMT16: A dataset used for machine translation tasks. For our experiments, only the English part of the test set was used. Yelp: A dataset containing reviews of users for businesses on Yelp."
}