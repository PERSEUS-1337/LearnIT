{
    "title": "HygOjhEYDH",
    "content": "Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. The standard way of learning in such models is by estimating the conditional intensity function.   However, parameterizing the intensity function usually incurs several trade-offs. We show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times.   We draw on the literature on normalizing flows to design models that are flexible and efficient. We additionally propose a simple mixture model that matches the flexibility of flow-based models, but also permits sampling and computing moments in closed form.   The proposed models achieve state-of-the-art performance in standard prediction tasks and are suitable for novel applications, such as learning sequence embeddings and imputing missing data. Visits to hospitals, purchases in e-commerce systems, financial transactions, posts in social media -various forms of human activity can be represented as discrete events happening at irregular intervals. The framework of temporal point processes is a natural choice for modeling such data. By combining temporal point process models with deep learning, we can design algorithms able to learn complex behavior from real-world data. Designing such models, however, usually involves trade-offs along the following dimensions: flexibility (can the model approximate any distribution?), efficiency (can the likelihood function be evaluated in closed form?), and ease of use (is sampling and computing summary statistics easy?). Existing methods (Du et al., 2016; Mei & Eisner, 2017; Omi et al., 2019) that are defined in terms of the conditional intensity function typically fall short in at least one of these categories. Instead of modeling the intensity function, we suggest treating the problem of learning in temporal point processes as an instance of conditional density estimation. By using tools from neural density estimation (Bishop, 1994; Rezende & Mohamed, 2015) , we can develop methods that have all of the above properties. To summarize, our contributions are the following: \u2022 We connect the fields of temporal point processes and neural density estimation. We show how normalizing flows can be used to define flexible and theoretically sound models for learning in temporal point processes. \u2022 We propose a simple mixture model that performs on par with the state-of-the-art methods. Thanks to its simplicity, the model permits closed-form sampling and moment computation. \u2022 We show through a wide range of experiments how the proposed models can be used for prediction, conditional generation, sequence embedding and training with missing data. as a sequence of strictly positive inter-event times \u03c4 i = t i \u2212 t i\u22121 \u2208 R + . Representations in terms of t i and \u03c4 i are isomorphic -we will use them interchangeably throughout the paper. The traditional way of specifying the dependency of the next arrival time t on the history H t = {t j \u2208 T : t j < t} is using the conditional intensity function \u03bb * (t) := \u03bb(t|H t ). Here, the * symbol reminds us of dependence on H t . Given the conditional intensity function, we can obtain the conditional probability density function (PDF) of the time \u03c4 i until the next event by integration (Rasmussen, 2011) as p * (\u03c4 i ) := p(\u03c4 i |H ti ) = \u03bb * (t i\u22121 + \u03c4 i ) exp \u2212 \u03c4i 0 \u03bb * (t i\u22121 + s)ds . Learning temporal point processes. Conditional intensity functions provide a convenient way to specify point processes with a simple predefined behavior, such as self-exciting (Hawkes, 1971 ) and self-correcting (Isham & Westcott, 1979) processes. Intensity parametrization is also commonly used when learning a model from the data: Given a parametric intensity function \u03bb * \u03b8 (t) and a sequence of observations T , the parameters \u03b8 can be estimated by maximizing the log-likelihood: \u03b8 * = arg max \u03b8 i log p * \u03b8 (\u03c4 i ) = arg max \u03b8 i log \u03bb * The main challenge of such intensity-based approaches lies in choosing a good parametric form for \u03bb Universal approximation. The SOSFlow and DSFlow models can approximate any probability density on R arbitrarily well (Jaini et al., 2019, Theorem 3), (Krueger et al., 2018, Theorem 4) . It turns out, a mixture model has the same universal approximation (UA) property. Theorem 1 (DasGupta, 2008, Theorem 33.2) . Let p(x) be a continuous density on R. If q(x) is any density on R and is also continuous, then, given \u03b5 > 0 and a compact set S \u2282 R, there exist number of components K \u2208 N, mixture coefficients w \u2208 \u2206 K\u22121 , locations \u00b5 \u2208 R K , and scales This results shows that, in principle, the mixture distribution is as expressive as the flow-based models. Since we are modeling the conditional density, we additionally need to assume for all of the above models that the RNN can encode all the relevant information into the history embedding h i . This can be accomplished by invoking the universal approximation theorems for RNNs (Siegelmann & Sontag, 1992; Sch\u00e4fer & Zimmermann, 2006) . Note that this result, like other UA theorems of this kind (Cybenko, 1989; Daniels & Velikova, 2010) , does not provide any practical guarantees on the obtained approximation quality, and doesn't say how to learn the model parameters. Still, UA intuitively seems like a desirable property of a distribution. This intuition is supported by experimental results. In Section 5.1, we show that models with the UA property consistently outperform the less flexible ones. Interestingly, Theorem 1 does not make any assumptions about the form of the base density q(x). This means we could as well use a mixture of distribution other than log-normal. However, other popular distributions on R + have drawbacks: log-logistic does not always have defined moments and gamma distribution doesn't permit straightforward sampling with reparametrization. Intensity function. For both flow-based and mixture models, the conditional cumulative distribution function (CDF) F * (\u03c4 ) and the PDF p * (\u03c4 ) are readily available. This means we can easily compute the respective intensity functions (see Appendix A). However, we should still ask whether we lose anything by modeling p * (\u03c4 ) instead of \u03bb * (t). The main arguments in favor of modeling the intensity function in traditional models (e.g. self-exciting process) are that it's intuitive, easy to specify and reusable (Upadhyay & Rodriguez, 2019) . \"Intensity function is intuitive, while the conditional density is not.\" -While it's true that in simple models (e.g. in self-exciting or self-correcting processes) the dependence of \u03bb * (t) on the history is intuitive and interpretable, modern RNN-based intensity functions (as in Du et al. (2016) ; Mei & Eisner (2017); Omi et al. (2019) ) cannot be easily understood by humans. In this sense, our proposed models are as intuitive and interpretable as other existing intensity-based neural network models. \"\u03bb * (t) is easy to specify, since it only has to be positive. On the other hand, p * (\u03c4 ) must integrate to one.\" -As we saw, by using either normalizing flows or a mixture distribution, we automatically enforce that the PDF integrates to one, without sacrificing the flexibility of our model. \"Reusability: If we merge two independent point processes with intensitites \u03bb * 1 (t) and \u03bb * 2 (t), the merged process has intensity \u03bb * (t) = \u03bb * 1 (t) + \u03bb * 2 (t).\" -An equivalent result exists for the CDFs F * 1 (\u03c4 ) and F * 2 (\u03c4 ) of the two independent processes. The CDF of the merged process is obtained as 2 (\u03c4 ) (derivation in Appendix A). As we just showed, modeling p * (\u03c4 ) instead of \u03bb * (t) does not impose any limitation on our approach. Moreover, a mixture distribution is flexible, easy to sample from and has well-defined moments, which favorably compares it to other intensity-based deep learning models. We use tools from neural density estimation to design new models for learning in TPPs. We show that a simple mixture model is competitive with state-of-the-art normalizing flows methods, as well as convincingly outperforms other existing approaches. By looking at learning in TPPs from a different perspective, we were able to address the shortcomings of existing intensity-based approaches, such as insufficient flexibility, lack of closed-form likelihoods and inability to generate samples analytically. We hope this alternative viewpoint will inspire new developments in the field of TPPs. Constant intensity model as exponential distribution. The conditional intensity function of the constant intensity model (Upadhyay et al., 2018 ) is defined as \u03bb H is the history embedding produced by an RNN, and b \u2208 R is a learnable parameter. By setting c = exp(v T h i + b), it's easy to see that the PDF of the constant intensity model p * (\u03c4 ) = c exp(\u2212c) corresponds to an exponential distribution. Summary The main idea of the approach by Omi et al. (2019) is to model the integrated conditional intensity function using a feedforward neural network with non-negative weights are non-negative weight matrices, and (3) \u2208 R are the remaining model parameters. FullyNN as a normalizing flow Let z \u223c Exponential(1), that is We can view f : R + \u2192 R + as a transformation that maps \u03c4 to z We can now use the change of variables formula to obtain the conditional CDF and PDF of \u03c4 . Alternatively, we can obtain the conditional intensity as and use the fact that p Both approaches lead to the same conclusion However, the first approach also provides intuition on how to draw samples\u03c4 from the resulting distribution p * (\u03c4 ) -an approach known as the inverse method (Rasmussen, 2011) 1. Samplez \u223c Exponential(1) 2. Obtain\u03c4 by solving f (\u03c4 ) \u2212z = 0 for \u03c4 (using e.g. bisection method) Similarly to other flow-based models, sampling from the FullyNN model cannot be done exactly and requires a numerical approximation."
}