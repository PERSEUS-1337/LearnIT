{
    "title": "rkxmPgrKwB",
    "content": "Neural network training depends on the structure of the underlying loss landscape, i.e. local minima, saddle points, flat plateaus, and loss barriers. In relation to the structure of the landscape, we study the permutation symmetry of neurons in each layer of a deep neural network, which gives rise not only to multiple equivalent global minima of the loss function but also to critical points in between partner minima. In a network of $d-1$ hidden layers with $n_k$ neurons in layers $k = 1, \\ldots, d$, we construct continuous paths between equivalent global minima that lead through a `permutation point' where the input and output weight vectors of two neurons in the same hidden layer $k$ collide and interchange. We show that such permutation points are critical points which lie inside high-dimensional subspaces of equal loss, contributing to the global flatness of the landscape. We also find that a permutation point for the exchange of neurons $i$ and $j$ transits into a flat high-dimensional plateau that enables all $n_k!$ permutations of neurons in a given layer $k$ at the same loss value. Moreover , we introduce higher-order permutation points by exploiting the hierarchical structure in the loss landscapes of neural networks, and find that the number of $K$-th order permutation points is much larger than the (already huge) number of equivalent global minima -- at least by a polynomial factor of order $K$. In two tasks, we demonstrate numerically with our path finding method that continuous paths between partner minima exist: first, in a toy network with a single hidden layer on a function approximation task and, second, in a multilayer network on the MNIST task. Our geometric approach yields a lower bound on the number of critical points generated by weight-space symmetries and provides a simple intuitive link between previous theoretical results and numerical observations. The structure of the loss landscape plays an important role in the optimization of neural network parameters. A large number of numerical (Dauphin et al., 2014; Goodfellow et al., 2014; Li et al., 2018; Sagun et al., 2014; 2016; Ballard et al., 2017; Garipov et al., 2018; Draxler et al., 2018; Baity-Jesi et al., 2018) and theoretical (Choromanska et al., 2015; Rasmussen, 2003; Freeman and Bruna, 2016; Soudry and Carmon, 2016; Nguyen and Hein, 2017) studies have explored the properties of the loss landscape. In particular, in a multilayer network of d \u2212 1 hidden layers with n neurons each, there are (n!) d\u22121 equivalent configurations corresponding to the permutation of neuron indices in each layer of the network (Goodfellow et al., 2016; Bishop, 1995) . The permutation symmetries give rise to a loss landscape where any given global minimum in the weight space must have (n!) d\u22121 \u2212 1 completely equivalent partner minima. This property of neural network landscapes is called weight-space symmetry. Several (Saad and Solla, 1995; Amari et al., 2006; Wei et al., 2008) works explored the implications of weight-space symmetry for training dynamics in two-layer networks and found that training dynamics slow down near the singular regions caused by weight-space symmetry. Dauphin et al. (2014) ; Orhan and Pitkow (2017) argue that optimization paths may get close to the singular regions induced by weight-space symmetry and this, in turn, slows down training for deep neural networks. Exploiting weight-space symmetries, we give insights into and partial explanations of three observations on neural network landscapes. Observation 1. Training dynamics are slow near singular regions caused by weight-space symmetry and stochastic gradient descent might travel near these regions throughout training (Saad and Solla, 1995; Wei et al., 2008; Amari et al., 2006; Dauphin et al., 2014; Orhan and Pitkow, 2017) . Observation 2. The Hessian of the loss function has numerous almost-zero eigenvalues throughout training, thus the landscape is flat in many directions Papyan, 2018; Ghorbani et al., 2019) . Related to observation 1 and 2, we prove the existence of numerous connected high-dimensional plateaus extending across the landscape due to weight-space symmetries. Observation 3. The number of saddles can grow exponentially in neural network landscapes (Auer et al., 1996; Dauphin et al., 2014; Choromanska et al., 2015) . Related to observation 3, we prove that there are at least polynomially many more saddles than the global minima due to weight-space symmetries in neural networks, without any further assumptions. In addition, we propose a novel low-loss path finding algorithm to find barriers between partner minima. We start from the known permutation symmetries and consider continuous low-loss paths that connect two equivalent global minima by merging the weight vectors of two neurons in a specific way. At a so-called permutation point, where the distance between the input and output weight vectors of the two neurons vanishes, the indices of the two neurons can be interchanged at no extra cost. After the change, the system returns on the 'mirrored' path back to the original configuration -except for the permutation of one pair of indices. Surprisingly, we find that we can permute all neuron indices in the same layer at the same cost as the loss at a permutation point reached by moving along the path that merges a single pair of neurons. These constant-loss permutations are possible because each permutation point lies in a high-dimensional plateau of critical points. Our theory can be extended to higher-order saddles and provides explicit lower bounds for the number of first-and higher-order permutation points. Numerically, we confirm the existence of first-order permutation saddles. In particular, the specific contributions of our work are: \u2022 A simple low-loss path-finding algorithm linking partner global minima via a permutation point, implemented by minimization under a single scalar constraint (distance of weight vectors). \u2022 The theoretical characterization of permutation points, for example that these are critical points and several permutation points are connected via paths at equal loss. \u2022 A lower bound for the number of first-and higher-order permutation points and their corresponding plateaus. \u2022 Numerical demonstrations of the path finding method in multilayer neural networks trained on MNIST. The surprising training performance of neural networks despite their highly non-convex nature has been drawing attention to the structure of the loss landscape. In this paper, we explored how weight-space symmetry induces saddles and plateaus in the neural network loss landscape. We found that special critical points, so-called permutation points, are embedded in high-dimensional flat plateaus. We proved that all permutation points in a given layer are connected with equal-loss paths, suggesting new perspectives on loss landscape topology. We provided a novel lower bound for the number of first-and higher-order permutation points and proposed a low-loss path finding method to connect equivalent minima. The empirical validation of our path finding algorithm in a multilayer network trained on MNIST showed that permutation points could indeed be reached in practice. Additionally, we observed that the loss at the permutation point (barrier) decreased with network size and thus confirmed Freeman and Bruna (2016) (1, 0.5, 0.5) Figure 7: A. Zooming in permutations points of one of the permutation sets (red in Fig. 6) . B. Visualizing how permutation points (at layer k = 1, see A) lie inside equal-loss lines in the weight space of the layer k + 1 = 2. Only two out of three lines are shown for simplicity. We observe that the number of such equal-loss lines (hyperplanes) is equal to the number of permutation points, i.e. each permutation point lies inside one distinct line."
}