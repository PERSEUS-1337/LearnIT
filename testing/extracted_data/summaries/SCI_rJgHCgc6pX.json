{
    "title": "rJgHCgc6pX",
    "content": "Creating a knowledge base that is accurate, up-to-date and complete remains a significant challenge despite substantial efforts in automated knowledge base construction.   In this paper, we present Alexandria -- a system for unsupervised, high-precision knowledge base construction. Alexandria uses a probabilistic program to define a process of converting knowledge base facts into unstructured text.   Using probabilistic inference, we can invert this program and so retrieve facts, schemas and entities from web text. The use of a probabilistic program allows uncertainty in the text to be propagated through to the retrieved facts, which increases accuracy and helps merge facts from multiple sources. Because Alexandria does not require labelled training data, knowledge bases can be constructed with the minimum of manual input. We demonstrate this by constructing a high precision (typically 97\\%+) knowledge base for people from a single seed fact. Search engines and conversational assistants require huge stores of knowledge in order to answer questions and understand basic facts about the world. As a result, there has been significant interest in creating such knowledge bases (KBs) and corresponding efforts to automate their construction and maintenance (see BID15 for a review). For example, KnowledgeVault BID4 , NELL BID0 BID11 , YAGO2 BID7 , DIG [P. BID12 , and many other systems aim either to construct a KB automatically or make an existing KB more complete. Despite these efforts, there remain significant ongoing challenges with keeping KBs up-to-date, accurate and complete. Existing automated approaches still require manual effort in the form of at least one of the following: a provided set of entities used for supervised training of components such as entity linkers/recognizers; a provided schema used to define properties/relations of entities; or a provided set of annotated texts used to train fact extractors/part of speech taggers. The holy grail of KB construction and maintenance would be a system which could learn and update its own schema, which could automatically discover new entities as they come into existence, and which could extract facts from natural text with such high precision that no human checking is needed.With this goal in mind, we present Alexandria -a system for unsupervised, high-precision knowledge base construction. At the core of Alexandria is a probabilistic program that defines a process of generating text from a knowledge base consisting of a large set of typed entities. By applying probabilistic inference to this program, we can reason in the inverse direction: going from text back to facts. This inverse reasoning allows us to retrieve facts, schemas and entities from web text. The use of a probabilistic program also provides an elegant way to handle the uncertainty inherent in natural text. An important advantage of using a generative model is that Alexandria does not require labelled data, which means it can be applied to new domains with little or no manual effort. The model is also inherently task-neutral -by varying which variables in the model are observed and which are inferred, the same model can be used for: learning a schema (relation discovery), entity discovery, entity linking, fact retrieval and other tasks, such as finding sources that support a particular fact. In this paper we demonstrate schema learning, fact retrieval, entity discovery and entity linking. We will evaluate the former two tasks, while the latter two are performed as part of these main tasks.An attractive aspect of our approach is that the entire system is defined by one coherent probabilistic model. This removes the need to create and train many separate components such as tokenizers, named entity recognizers, part-of-speech taggers, fact extractors, linkers and so on; a disadvantage of having such multiple components is that they are likely to encode different underlying assumptions, reducing the accuracy of the combined system. Furthermore, the use of a single probabilistic program allows uncertainty to be propagated consistently throughout the system -from the raw web text right through to the extracted facts (and back).Related work -There has been a significant amount of work on automated knowledge base construction BID15 . Because the Alexandria system can be used to perform many different tasks, it is related to a range of previous, task-specific systems. Here we describe the most relevant.Unsupervised learning -Open IE (Information Extraction) systems, such as Reverb BID5 and OLLIE BID8 aim to discover information across multiple domains without having labelled data for new domains. Such systems do not have an underlying schema, but instead retain information in a lexical form. This can lead to duplication of the same fact stored using different words, or can even allow conflicting facts to be stored. Representing facts in lexical form makes them hard for applications to consume, since there is no schema to query against. In contrast, Alexandria aims to infer the underlying schema of new domains and to extract facts in a consistent form separate from their lexical representation.Schema learning -the closest existing work is Biperpedia BID6 which aims to discover properties for many classes at once. Biperpedia uses search engine query logs as well as text to discover attributes, in a process that involves a number of trained classifiers and corresponding labelled training data. Alexandria's key differences are its unsupervised approach and the fact that schema learning is integrated into a single probabilistic model also used to perform other tasks.Web scale fact extraction -several existing systems can extract facts against a known schema across the entire web. These include KnowledgeVault [Dong et al., 2014] , NELL BID11 and DeepDive BID17 . Of these, KnowledgeVault is the largest scale and has performed KB completion (filling in missing values for entities where most values are known) of over 250M facts. DeepDive is perhaps the most similar system to Alexandria in that it is based around a probabilistic model -a Markov Logic Network (MLN) BID13 . DeepDive uses hand-constructed feature extractors to extract candidate facts from text for incorporation into the MLN. Because Alexandria uses a generative model of text, it can be applied directly to web text, without the need for feature extractors; in the mode described in this paper only a single seed fact is needed. In this paper, we have shown how Alexandria can perform schema learning and high-precision fact extraction unsupervised, except for a single seed example. Whilst the results in this paper are for people entities, the system has been designed to be generally applicable to many types of entity. It's worth noting that, in the process of learning about people, we have learned seed examples for other classes such as places, which we could use to do schema learning and fact extraction for these classes. By repeating this process recursively, we create the exciting possibility of using Alexandria like an Open IE system, to learn schemas, discovery entities and extract facts automatically across a large number of domains -this is our focus for future work. Our hope is that our high accuracy and strong typing will prevent 'drift' from occurring, which has reduced accuracy in previous Open IE systems.The Alexandria model does not use any joint prior across property values -such as the graph prior and tensor factorization priors used in BID4 . Incorporating such priors into the model has the potential to increase precision yet further.Alexandria's template-based language model is relatively simple compared to some NLP systems used in related work. In contrast, Alexandria's model of types and values is in general more sophisticated, particularly in its handling and propagation of uncertainty. We believe that this has allowed the system to achieve very high precision. We expected to need a more sophisticated language model to achieve high recall -in fact, the ability to process the entire web means that we can still achieve good recall -a fact expressed in a complex way on one page is often expressed simply elsewhere.The simplicity of the language model has one advantage -that it can be readily applied to text in many different languages. Indeed, we found that the system learned by itself to extract data from some non-English pages. To make full use of this would require making the built-in types multi-lingual, for example, allowing different month names in dates and different ways of writing numbers. The benefit would be to improve recall and also to learn how facts are expressed differently in different locales.We believe that Alexandria makes a step towards the holy grail of completely automatic KB construction and maintenance -we look forward to trying out the system in many new domains to see if the successful unsupervised learning of people can be replicated for other entity types."
}