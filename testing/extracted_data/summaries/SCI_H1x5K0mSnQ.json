{
    "title": "H1x5K0mSnQ",
    "content": "Deep learning has made remarkable achievement in many fields. However, learning\n the parameters of neural networks usually demands a large amount of labeled\n data. The algorithms of deep learning, therefore, encounter difficulties when applied\n to supervised learning where only little data are available. This specific task\n is called few-shot learning. To address it, we propose a novel algorithm for fewshot\n learning using discrete geometry, in the sense that the samples in a class are\n modeled as a reduced simplex. The volume of the simplex is used for the measurement\n of class scatter. During testing, combined with the test sample and the\n points in the class, a new simplex is formed. Then the similarity between the test\n sample and the class can be quantized with the ratio of volumes of the new simplex\n to the original class simplex. Moreover, we present an approach to constructing\n simplices using local regions of feature maps yielded by convolutional neural networks.\n Experiments on Omniglot and miniImageNet verify the effectiveness of\n our simplex algorithm on few-shot learning. Deep learning has exhibited outstanding ability in various disciplines including computer vision, natural language processing and speech recognition BID10 . For instance, AlexNet has made a breakthrough on recognizing millions of imagery objects by means of deep Convolutional Neural Network (CNN) BID8 . In the past five years, the algorithmic capability of comprehending visual concepts has been significantly improved by elaborately well-designed deep learning architectures BID4 BID22 . However, training deep neural networks such as the widely employed CNNs of AlexNet BID8 , Inception BID23 , VGG BID20 , and ResNet BID5 , needs the supervision of many class labels which are handcrafted. For example, the number of samples of each class in the ImageNet of object recognition benchmark BID17 is more than one thousand. In fact, the number of labelled samples used for learning parameters of CNNs is far more than that because data augmentation is usually applied. This kind of learning obviously deviates from the manner of human cognition. A child can recognize a new object that she/he has never seen only by several examples, from simple shapes like rectangles to highly semantic animals like tigers. However, deep learning algorithms encounter difficulty in such scenarios where only very sparse data are available for learning to recognize a new category, thus raising the research topic of one-shot learning or few-shot learning BID1 BID25 .The seminal work BID2 models few-shot learning with the Bayesian framework. Empirical knowledge of available categories is learned and parameterized as a probability density function. The unseen class with a handful of examples is modeled as the posterior by updating the prior. Bayesian theory provides a simple and elegant idea for solving learning problems with little data. If decomposed into parts or programs, an object can be described by the joint distribution of Bayesian criterion. In this manner, human-level performance on one-shot learning has been derived for discovering simple visual concepts such as ancient handwritten characters BID9 .With the prevalence of deep learning, the recent work for few-shot learning focuses on the application of deep neural networks that have more capacity to accommodate the complexity of object representations. Siamese neural network facilitates the performance of few-shot recognition by means of twin networks of sharing parameters, optimizing the distances of representative features in intraclasses BID7 . The counterpart of learning data structures by distance is also formulated by triplet loss in BID11 . Researchers in BID11 assert that the distance metrics can learn the intrinsic manifold structures of training data such that the network is more general and robust when employed for untrained objects. A very recent work pertaining to distance-based optimization, named Prototypical Networks BID21 , significantly improves the capability of few-shot recognition. Prototypical Networks attempt to minimize the distance of the test sample to the center of each class and are learned in the end-to-end manner.Memory-augmented architectures are also proposed to help assimilate new classes with more accurate inference BID18 . Matching network embeds metric learning in neural network in the light of attention mechanism which is embodied by softmax BID26 . In a very recent work, the large-scale memory without the need of resetting during training is formulated as an embedded module for arbitrary neural networks to remember the information of rare events BID6 . In order to obtain rapid learning with limited samples, meta learning is exploited both in memory network and matching network. This \"learning to learn\" technique is extended to deal with few-shot learning from the point of view of optimization BID15 . To be specific, a LSTM-based meta learner learns to mimic the exact optimization algorithm and then harnesses the acquired capability to train the learner applied for the few-shot cases. The latest meta learning algorithms also deal with few-shot learning from different angles, e.g. the fast adaptation of neural networks BID3 , and temporal convolution BID13 .In addition to the application of memory module or attention model in LSTM, there is another type of algorithms digging the effective way of transferring the discriminative power of pre-trained models to few-shot circumstances. Resorting to the correlation between the activations in the last feature layers and the associated parameters for softmax, a transformation is learned to derive the parameters for predicting new classes from corresponding activations BID14 .The algorithms based on deep learning can learn more expressive representations for objects, essentially boosting the quality of feature extraction. However, the softmax classifier discriminates all categories by class boundaries , bypassing the steps that carefully characterize the structure of each class. Thus the algorithmic performance will deteriorate grossly if the distribution of new class cannot be accurately modeled by trained networks. Besides softmax, another commonly applied method, k nearest neighbors (KNN), is a point-to-point measurement and is incapable of conveying global structural information.To address this issue, we propose a geometric method for few-shot learning. Our perspective is that accurate geometric characterization for each class is essential when only a handful of samples are available, because such sparse data are usually insufficient to fit well-converged parameterized classifier. To this end, we harness convex polytope to fit a class, in the sense that we construct a convex polytope by selecting the samples in the class as the vertices of the polytope. The volume of the polytope is taken as the measurement of class scatter. Thus the polytopal volume may be improved after including the query sample in the test set during the testing trial. The normalized volume with respect to the original counterpart is applied to compute the distance from the test sample to the test set. To highlight the structural details of object parts, we present the construction of polytope based on convolutional feature maps as well.To the best of our understanding, however, there is no exact formula to calculating the volume of general convex polytope. To make our algorithm feasible, therefore, we use the simplest convex polytope -simplex instead. The volume of a simplex can be expressed by the Cayley-Menger determinant BID0 , thus casting the problem of few-shot recognition as a simple calculation of linear algebra. Experiments on Omniglot and miniImageNet datasets verify the effectiveness of our simple algorithm. In this paper, we designed a novel method to deal with few-shot learning problems. Our idea was from the point of view of high dimensional convex geometry and transformed the learning problem to the study of volumes of simplices. The relation between a test sample and a class was investigated via the volumes of different polytopes. By harnessing the power of simplex, we gave a rigorous mathematical formulation for our approach. We also conduced extensive simulations to validate our method. The results on various datasets showed the accuracy and robustness of the geometry-based method, compared to the state-of-the-art results in the literature."
}