{
    "title": "B1xJAsA5F7",
    "content": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n The goal of drug discovery is to design molecules with desirable chemical properties. The task is challenging since the chemical space is vast and often difficult to navigate. One of the prevailing approaches, known as matched molecular pair analysis (MMPA) BID16 BID11 , learns rules for generating \"molecular paraphrases\" that are likely to improve target chemical properties. The setup is analogous to machine translation: MMPA takes as input molecular pairs {(X, Y )}, where Y is a paraphrase of X with better chemical properties. However, current MMPA methods distill the matched pairs into graph transformation rules rather than treating it as a general translation problem over graphs based on parallel data.In this paper, we formulate molecular optimization as graph-to-graph translation. Given a corpus of molecular pairs, our goal is to learn to translate input molecular graphs into better graphs. The proposed translation task involves many challenges. While several methods are available to encode graphs BID12 BID32 , generating graphs as output is more challenging without resorting to a domain-specific graph linearization. In addition, the target molecular paraphrases are diverse since multiple strategies can be applied to improve a molecule. Therefore, our goal is to learn multimodal output distributions over graphs.To this end, we propose junction tree encoder-decoder, a refined graph-to-graph neural architecture that decodes molecular graphs with neural attention. To capture diverse outputs, we introduce stochastic latent codes into the decoding process and guide these codes to capture meaningful molecular variations. The basic learning problem can be cast as a variational autoencoder, where the posterior over the latent codes is inferred from input molecular pair (X, Y ). Further, to avoid invalid translations, we propose a novel adversarial training method to align the distribution of graphs generated from the model using randomly selected latent codes with the observed distribution of valid targets. Specifically, we perform adversarial regularization on the level of the hidden states created as part of the graph generation. We evaluate our model on three molecular optimization tasks, with target properties ranging from drug likeness to biological activity. 1 As baselines, we utilize state-of-the-art graph generation methods BID23 BID47 and MMPA BID9 . We demonstrate that our model excels in discovering molecules with desired properties, outperforming the baselines across 2 RELATED WORK Molecular Generation/Optimization Prior work on molecular optimization approached the graph translation task through generative modeling BID14 BID42 BID28 BID8 BID23 BID39 BID31 and reinforcement learning BID17 BID36 BID37 BID47 . Earlier approaches represented molecules as SMILES strings BID46 , while more recent methods represented them as graphs. Most of these methods coupled a molecule generator with a property predictor and solved the optimization problem through Bayesian optimization or reinforcement learning. In contrast, our model is trained to translate a molecular graph into a better graph through supervised learning, which is more sample efficient.Our approach is closely related to matched molecular pair analysis (MMPA) BID16 BID11 in drug de novo design, where the matched pairs are hard-coded into graph transformation rules. MMPA's main drawback is that large numbers of rules have to be realized (e.g. millions) to cover all the complex transformation patterns. In contrast, our approach uses neural networks to learn such transformations, which does not require the rules to be explicitly realized.Graph Neural Networks Our work is related to graph encoders and decoders. Previous work on graph encoders includes convolutional BID40 BID4 BID20 BID12 BID35 BID10 BID27 and recurrent architectures BID32 BID7 . Graph encoders have been applied to social network analysis BID26 BID19 and chemistry BID24 BID13 BID41 . Recently proposed graph decoders BID44 BID33 BID23 BID48 BID34 focus on learning generative models of graphs. While our model builds on BID23 to generate graphs, we contribute new techniques to learn multimodal graph-to-graph mappings.Image/Text Style Translation Our work is closely related to image-to-image translation BID21 , which was later extended by to learn multimodal mappings. Our adversarial training technique is inspired by recent text style transfer methods BID43 BID49 ) that adversarially regularize the continuous representation of discrete structures to enable end-to-end training. Our technical contribution is a novel adversarial regularization over graphs that constrains their scaffold structures in a continuous manner. In conclusion, we have evaluated various graph-to-graph translation models for molecular optimization. By combining the variational junction tree encoder-decoder with adversarial training, we can generate better and more diverse molecules than the baselines."
}