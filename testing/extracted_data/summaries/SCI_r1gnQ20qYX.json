{
    "title": "r1gnQ20qYX",
    "content": "Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications. However, the interpretability of those models are often lacking. On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists). In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes. The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists. Thanks to the prediction power of neural networks, the rule lists from\t\t\t\t prototypes are more concise and hence provide better interpretability. On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%. Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning. The rapid growth of sizes and complexities of electronic health records (EHR) data has motivated the use of deep learning models, which demonstrated state-of-the-art performance in many tasks, including diagnostics and disease detection BID7 BID38 , medication prediction BID16 , risk prediction BID9 Xiao et al., 2018b) , and patient subtyping BID1 BID4 . Although deep learning models can produce accurate predictions and classifications, they are often treated as black-box models that lack interpretability and transparency of their inner working BID20 . This is a critical problem as it can limit the adoption of deep learning in medical decision making.Recently, there have been great efforts of trying to explain black-box deep models, including via attention mechanism BID7 BID40 , visualization BID31 , and explanation by examples or prototypes BID17 . To bring deep models into real clinical practice, clinicians often need to understand why a certain output is produced and how the model generates this output for a given input BID24 . Rule learning and prototype learning are two promising directions to achieve clinical model interpretability.Rule learning generates a set of rules from training data, in which its prediction is done at leaf levels via simple models such as majority vote or regression. For example, the results of rule learning are rule lists composed of multiple if-then statements BID0 . Those rules can be interpretable to domain experts as they are expressed in simple logical forms BID30 BID3 . However, because of such a simple prediction model, the accuracy of rule-based models is often lower than deep neural networks. Moreover, the interpretability can be undermined as the depth of rules becomes very large and thus incomprehensible for human with tens or hundreds of levels of the rules.Prototype learning is another interpretable model inspired by case-based reasoning BID14 , where observations are classified based on their proximity to a prototype point in the dataset. Many machine learning models have incorporated prototype concepts BID28 BID2 BID12 , and learn to compute prototypes (as actual data points or synthetic points) that can represent a set of similar points. However prototypes alone may not lead to interpretable models as we still need an intuitive way to represent and explain what a prototype is, especially given recent deep prototype works BID17 .Both approaches were explored in healthcare applications. For example, rule learning was employed to identify how likely patients were to be readmitted to a hospital after they had been released, each probability associated with a set of rules as criteria BID35 . While prototype could be selected from actual patients and genes for clinicians to make sense of large patient cohort or gene data BID2 . However , there are still open challenges: How to construct simple rules with more accurate prediction and classification performance? How to produce accurate and intuitive definitions of prototypes?In this work, we propose Prototype lEArning via Rule List (PEARL), which combines rule learning and prototype learning on deep neural networks to harness the benefits of both approaches and alleviate their shortcomings for an accurate and interpretable prediction model. In particular , we iteratively learn rule lists, via a data reweighing procedure using prototypes, and then update prototypes via neural networks with learned rules. PEARL not only generates simple and interpretable rule lists and prototypes, but also provides neural network models which can infer the similarity of a query datum to all the prototypes. To summarize, we make the following contributions in this paper.1. We propose an integrative method to combine rule list and prototype learning, enabling PEARL to harness the power of these methods.2. PEARL automatically learns prototypes corresponding to rules in a rule list, which are more concise than conventional rule list learning methods and more explainable than prototype learning methods by providing logic reasoning.3. On real-world electronic health record datasets, PEARL demonstrates both accurate prediction performance and simple interpretation. In this paper, we proposed PEARL, an integrative prototype learning neural network that combines rule learning and prototype learning on deep neural networks to harness the benefits of these methods. We empirically demonstrated that PEARL is more accurate , thanks to an iterative data reweighing algorithm, and more interpretable than rule learning, since it explains diagnostic decisions using much fewer clinical variables. PEARL is an initial attempt to combine traditional rule learning with deep neural networks. In future research, we will try to extend PEARL to other interpretable models."
}