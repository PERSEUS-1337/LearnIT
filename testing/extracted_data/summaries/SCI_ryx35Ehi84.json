{
    "title": "ryx35Ehi84",
    "content": "  Analysis methods which enable us to better understand the\n  representations and functioning of neural models of language are\n  increasingly needed as deep learning becomes the dominant approach\n  in NLP. Here we present two methods based on Representational\n  Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to\n  directly quantify how strongly the information encoded in neural\n  activation patterns corresponds to information represented by\n  symbolic structures such as syntax trees. We first validate our\n  methods on the case of a simple synthetic language for arithmetic\n  expressions with clearly defined syntax and semantics, and show that\n  they exhibit the expected pattern of results. We then apply our methods to\n  correlate neural representations of English sentences with their\n  constituency parse trees. Analysis methods which allow us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach to natural language processing. A popular technique for analyzing neural representations involves predicting information of interest from the activation patterns, typically using a simple predictive model such as a linear classifier or regressor. If the model is able to predict this information with high accuracy, the inference is that the neural representation encodes it. We refer to these as diagnostic models.One important limitation of this method of analysis is that it is only easily applicable to relatively simple types of target information, which are amenable to be predicted via linear regression or classification. Should we wish to decode activation patterns into a structured target such as a syntax tree, we would need to resort to complex structure prediction algorithms, running the risk that the analytic method becomes no simpler than the actual neural model.Here we introduce an alternative approach based on correlating neural representations of sentences and structured symbolic representations commonly used in linguistics. Crucially, the correlation is in similarity space rather than in the original representation space, removing most constraints on the types of representations we can use. Our approach is an extension of the Representational Similarity Analysis (RSA) method, initially introduced by BID19 in the context of understanding neural activation patterns in human brains.In this work we propose to apply RSA to neural representations of strings from a language on one side, and to structured symbolic representations of these strings on the other side. To capture the similarities between these symbolic representations, we use a tree kernel, a metric to compute the proportion of common substructures between trees. This approach enables straightforward comparison of neural and symbolic-linguistic representations. Furthermore, we introduce RSA REGRESS , a similarity-based analytic method which combines features of RSA and of diagnostic models.We validate both techniques on neural models which process a synthetic language for arithmetic expressions with a simple syntax and semantics and show that they behave as expected in this controlled setting. We further apply our techniques to two neural models trained on English text, Infersent BID9 and BERT BID13 , and show that both models encode a substantial amount of syntactic information compared to random models and simple bag-of-words representations; we also show that according to our metrics syntax is most salient in the intermediate layers of BERT. We present two RSA-based methods for correlating neural and syntactic representations of language, using tree kernels as a measure of similarity between syntactic trees. Our results on arithmetic expressions confirm that both versions of structured RSA capture correlations between different representation spaces, while providing complementary insights. We apply the same techniques to English sentence embeddings, and show where and to what extent each representation encodes syntactic information. The proposed methods are general and applicable not just to constituency trees, but given a similarity metric, to any symbolic representation of linguistic structures including dependency trees or Abstract Meaning Representations. We plan to explore these options in future work. A toolkit with the implementation of our methods is available at https://github.com/gchrupala/ursa."
}