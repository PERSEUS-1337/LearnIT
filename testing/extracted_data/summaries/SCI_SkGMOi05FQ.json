{
    "title": "SkGMOi05FQ",
    "content": "In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs). GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer. In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data. Attempts have been made for utilizing GANs with word embeddings for text generation. This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures. The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used. Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis BID35 ) and machine translation BID7 BID24 ). Early techniques for generating text conditioned on some input information were template or rule-based engines, or probabilistic models such as n-gram. In recent times, state-of-the-art results on these tasks have been achieved by recurrent BID23 BID20 ) and convolutional neural network models trained for likelihood maximization. This work proposes an approach for text generation using Generative Adversarial Networks with Skip-Thought vectors.GANs BID9 ) are a class of neural networks that explicitly train a generator to produce high-quality samples by pitting against an adversarial discriminative model. GANs output differentiable values and hence the task of discrete text generation has to use vectors as differentiable inputs. This is achieved by training the GAN with sentence embedding vectors produced by SkipThought ), a neural network model for learning fixed length representations of sentences. 4.1 CONDITIONAL GENERATION OF SENTENCES.GANs can be conditioned on data attributes to generate samples BID21 ; Radford et al.) . In this experiment, both the generator and discriminator are conditioned on Skip-Thought encoded vectors ). The encoder converts 70000 sentences from the BookCorpus dataset collected in with a training/test/validation split of 5/1/1 into vectors used as real samples for discriminator. The decoded sentences are used to evaluate model performance under corpus level BLEU-2, BLEU-3 and BLEU-4 metrics (Papineni et al.) , once using only test set as reference and then entire corpus as reference. TAB0 compares these results for different architectures that have been experimented with in this paper."
}