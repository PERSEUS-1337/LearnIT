{
    "title": "HyGcghRct7",
    "content": "We propose a new learning-based approach to solve ill-posed inverse problems in imaging. We address the case where ground truth training samples are rare and the problem is severely ill-posed---both because of the underlying physics and because we can only get few measurements. This setting is common in geophysical imaging and remote sensing. We show that in this case the common approach to directly learn the mapping from the measured data to the reconstruction becomes unstable. Instead, we propose to first learn an ensemble of simpler mappings from the data to projections of the unknown image into random piecewise-constant subspaces. We then combine the projections to form a final reconstruction by solving a deconvolution-like problem. We show experimentally that the proposed method is more robust to measurement noise and corruptions not seen during training than a directly learned inverse. A variety of imaging inverse problems can be discretized to a linear system y = Ax + \u03b7 where y \u2208 R M is the measured data, A \u2208 R M \u00d7N is the imaging or forward operator, x \u2208 X \u2282 R N is the object being probed by applying A (often called the model), and \u03b7 is the noise. Depending on the application, the set of plausible reconstructions X could model natural, seismic, or biomedical images. In many cases the resulting inverse problem is ill-posed, either because of the poor conditioning of A (a consequence of the underlying physics) or because M N .A classical approach to solve ill-posed inverse problems is to minimize an objective functional regularized via a certain norm (e.g. 1 , 2 , total variation (TV) seminorm) of the model. These methods promote general properties such as sparsity or smoothness of reconstructions, sometimes in combination with learned synthesis or analysis operators, or dictionaries BID44 ).In this paper, we address situations with very sparse measurement data (M N ) so that even a coarse reconstruction of the unknown model is hard to get with traditional regularization schemes. Unlike artifact-removal scenarios where applying a regularized pseudoinverse of the imaging operator already brings out considerable structure, we look at applications where standard techniques cannot produce a reasonable image (Figure 1 ). This highly unresolved regime is common in geophysics and requires alternative, more involved strategies BID12 ).An appealing alternative to classical regularizers is to use deep neural networks. For example, generative models (GANs) based on neural networks have recently achieved impressive results in regularization of inverse problems BID7 , BID29 ). However, a difficulty in geophysical applications is that there are very few examples of ground truth models available for training (sometimes none at all). Since GANs require many , they cannot be applied to such problems. This suggests to look for methods that are not very sensitive to the training dataset. Conversely, it means that the sought reconstructions are less detailed than what is expected in data-rich settings; for Figure 1 : We reconstruct an image x from its tomographic measurements. In moderately ill-posed problems, conventional methods based on the pseudoinverse and regularized non-negative least squares (x \u2208 [0, 1] N , N is image dimension) give correct structural information. In fact, total variation (TV) approaches give very good results. A neural network BID23 ) can be trained to directly invert and remove the artifacts (NN). In a severely ill-posed problem on the other hand (explained in FIG2 ) with insufficient ground truth training data, neither the classical techniques nor a neural network recover salient geometric features.an example, see the reconstructions of the Tibetan plateau BID51 ).In this paper, we propose a two-stage method to solve ill-posed inverse problems using random low-dimensional projections and convolutional neural networks. We first decompose the inverse problem into a collection of simpler learning problems of estimating projections into random (but structured) low-dimensional subspaces of piecewise-constant images. Each projection is easier to learn in terms of generalization error BID10 ) thanks to its lower Lipschitz constant.In the second stage, we solve a new linear inverse problem that combines the estimates from the different subspaces. We show that this converts the original problem with possibly non-local (often tomographic) measurements into an inverse problem with localized measurements, and that in fact, in expectation over random subspaces the problem becomes a deconvolution. Intuitively, projecting into piecewise-constant subspaces is equivalent to estimating local averages-a simpler problem than estimating individual pixel values. Combining the local estimates lets us recover the underlying structure. We believe that this technique is of independent interest in addressing inverse problems.We test our method on linearized seismic traveltime tomography BID8 BID20 ) with sparse measurements and show that it outperforms learned direct inversion in quality of achieved reconstructions, robustness to measurement errors, and (in)sensitivity to the training data. The latter is essential in domains with insufficient ground truth images. We proposed a new approach to regularize ill-posed inverse problems in imaging, the key idea being to decompose an unstable inverse mapping into a collection of stable mappings which only estimate Figure 7 : Reconstructions on checkerboards and x-rays with 10dB measurement SNR tested on 10dB trained networks. Red annotations highlight where the direct net fails to reconstruct correct geometry. low-dimensional projections of the model. By using piecewise-constant Delaunay subspaces, we showed that the projections can indeed be accurately estimated. Combining the projections leads to a deconvolution-like problem. Compared to directly learning the inverse map, our method is more robust against noise and corruptions. We also showed that regularizing via projections allows our method to generalize across training datasets. Our reconstructions are better both quantitatively in terms of SNR and qualitatively in the sense that they estimate correct geometric features even when measurements are corrupted in ways not seen at training time. Future work involves getting precise estimates of Lipschitz constants for various inverse problems, regularizing the reformulated problem using modern regularizers BID46 ), studying extensions to non-linear problems and developing concentration bounds for the equivalent convolution kernel."
}