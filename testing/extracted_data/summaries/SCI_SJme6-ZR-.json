{
    "title": "SJme6-ZR-",
    "content": "The goal of survival clustering is to map subjects (e.g., users in a social network, patients in a medical study) to $K$ clusters ranging from low-risk to high-risk. Existing survival methods assume the presence of clear \\textit{end-of-life} signals or introduce them artificially using a pre-defined timeout. In this paper, we forego this assumption and introduce a loss function that differentiates between the empirical lifetime distributions of the clusters using a modified Kuiper statistic. We learn a deep neural network by optimizing this loss, that performs a soft clustering of users into survival groups. We apply our method to a social network dataset with over 1M subjects, and show significant improvement in C-index compared to alternatives. Free online subscription services (e.g., Facebook, Pandora) use survival models to predict the relationship between observed subscriber covariates (e.g. usage patterns, session duration, gender, location, etc.) and how long a subscriber remains with an active account BID26 BID11 . Using the same tools, healthcare providers make extensive use of survival models to predict the relationship between patient covariates (e.g. smoking, administering drug A or B) and the duration of a disease (e.g., herpes, cancer, etc.). In these scenarios, rarely there is an end-of-life signal: non-paying subscribers do not cancel their accounts, tests rarely declare a patient cancer-free. We want to assign subjects into K clusters, ranging from short-lived to long-lived subscribers (diseases).Despite the recent community interest in survival models BID1 BID33 , existing survival analysis approaches require an unmistakable end-of-life signal (e.g., the subscriber deletes his or her account, the patient is declared disease-free), or a pre-defined endof-life \"timeout\" (e.g., the patient is declared disease-free after 5 years, the subscriber is declared permanently inactive after 100 days of inactivity). Methods that require end-of-life signals also include BID23 BID8 BID3 BID14 BID24 BID29 BID31 BID47 BID9 BID19 BID41 BID40 BID17 BID48 BID26 BID0 BID4 BID5 BID35 BID46 BID30 .In this work, we propose to address the lifetime clustering problem without end-of-life signals for the first time, to the best of our knowledge. We begin by describing two possible datasets where such a clustering approach could be applied.\u2022 Social Network Dataset : Users join the social network at different times and participate in activities defined by the social network (login, send/receive comments). The covariates are the various attributes of a user like age, gender, number of friends, etc., and the inter-event time is the time between user's two consecutive activities. In this case, censoring is due to a fixed point of data collection that we denote t m , the time of measurement. Thus, time till censoring for a particular user is the time from her last activity to t m . Lifetime of a user is defined as the time from her joining till she permanently deletes her account.\u2022 Medical Dataset : Subjects join the medical study at the same time and are checked for the presence of a particular disease. The covariates are the attributes of the disease-causing cell in subject, inter-event time is the time between two consecutive observations of the presence of disease. The time to censoring is the difference between the time of last observation when the disease was present and the time of final observation. If the final observation for a subject indicates presence of the disease, then time to censoring is zero. Lifetime of the disease is defined as the time between the first observation of the disease and the time until it is permanently cured.We use a deep neural network and a new loss function, with a corresponding backpropagation modification, for clustering subjects without end-of-life signals. We are able to overcome the technical challenges of this problem, in part, thanks to the ability of deep neural networks to generalize while overfitting the training data BID49 . The task is challenging for the following reasons:\u2022 The problem is fully unsupervised, as there is no pre-defined end-of-life timeout. While semisupervised clustering approaches exist BID0 BID4 BID5 BID35 BID46 , they assume that end-of-life signals appearing before the observation time are observed; to the best of our knowledge, there are no fully unsupervised approach that can take complex input variables.\u2022 There is no hazard function that can be used to define the \"cure\" rate, as we cannot determine whether the disease is cured, or whether the subscriber will never return to the website, without observing for an infinitely long time.\u2022 Cluster assignments may depend on highly complex interactions between the observed covariates and the observed events. The unobserved lifetime distributions may not be smooth functions.Contributions. Using the ability of deep neural networks to model complex nonlinear relationships in the input data, our contribution is a loss function (using the p-value from a modified Kuiper nonparametric two-sample test BID28 ) and a backpropagation algorithm that can perform model-free (nonparametric) unsupervised clustering of subjects based on their latent lifetime distributions, even in the absence of end-of-life signals. The output of our algorithm is a trained deep neural network classifier that can (soft) assign test and training data subjects into K categories, from high-risk and to low-risk individuals. We apply our method to a large social network dataset and show that our approach is more robust than competing methods and obtains better clusters (higher C-index scores).Why deep neural networks. As with any optimization method that returns a point estimate (a set of neural network weights W in our case), our approach is subject to overfitting the training data. And because our loss function uses p-values, the optimization and overfitting have a rather negative name: p-hacking BID36 . That is, the optimization is looking for a W (hypothesis) that decreases the p-value. Deep neural networks, however, are known to both overfit the training data and generalize well BID49 . That is, the hypothesis (W ) tends to also have small p-values in the (unseen) test data, despite overfitting in the training data (p-hacking).Outline: In section 3, we describe the traditional survival analysis concepts that assume the presence of end-of-life signals. In section 4, we define a loss function that quantifies the divergence between empirical lifetime distributions of two clusters without assuming end-of-life signals. We also provide a neural network approach to optimize said loss function. We describe the dataset used in our experiments followed by results in section 5. In section 6, we describe a few methods in literature that are related to our work. Finally, we present our conclusions in section 7. In this work we introduced a Kuiper-based nonparametric loss function, and a corresponding backpropagation procedure (which backpropagates the loss over clusters rather than the loss per training example). These procedures are then used to train a feedforward neural network to inductively assign observed subject covariates into K survival-based clusters, from high-risk to low-risk subjects, without requiring an end-of-life signal. We showed that the resultant neural network produces clusters with better C-index values than other competing methods. We also presented the survival distributions of the clusters obtained from our procedure and concluded that there were only two groups of users in the Friendster dataset.Both parts (a) and (b) of our proof need definition 3 that translates the observed data D u for subject u into a stochastic process.Proof of (a): If the two clusters have distinct lifetime distributions, it means that the distributions of T 0 and T 1 in eq. (2) are different. Then, either the right-censoring \u03b4 in eq. (3) does not allow us to see the difference between T 0 and T 1 , and then there is no mappingsp and\u03ba that can get the distribution of S 0 (t;\u03ba,p) and S 1 (t;\u03ba,p) to be distinct, implying an L(\u03ba, p) \u2192 0, as n \u2192 \u221e as the observations come from the same distribution, making the Kuiper score asymptotically equal to one; or \u03b4 does allow us to see the difference and then, clearlyp \u2261 0 with a mapping\u03ba that assigns more than half of the subjects to their correct clusters, which would allow us to see the difference in H 0 and H 1 , would give Kuiper score asymptotically equal to zero. Thus, L(\u03ba, p) \u2192 \u2212\u221e, as n \u2192 \u221e.Proof of (b): Because \u03ba only take the subject covariates as input, and there are no dependencies between the subject covariates and the subject lifetime in eq. (2), any clustering based on the covariates will be a random assignment of users into clusters. Moreover , from eq. (3), the censoring time of subject u, S u , has the same distribution for both clusters because the RMPPs are the same. Thus, H 0 d = H 1 , i.e., H 0 and H 1 have the same distributions, and the Kuiper p-value test returns zero, L(\u03ba, p) \u2192 0, as n \u2192 \u221e. Table 4 : C-index (%) over different learning rates and batch sizes for the proposed NN approach with Kuiper loss (with learnt exponential) and K = 2."
}