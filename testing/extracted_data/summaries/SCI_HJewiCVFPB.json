{
    "title": "HJewiCVFPB",
    "content": "While deep learning and deep reinforcement learning systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge, particularly as these algorithms learn individual tasks from scratch. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single task learning are not fully understood. Motivated by the insight that gradient interference causes optimization challenges, we develop a simple and general approach for avoiding interference between gradients from different tasks, by altering the gradients through a technique we refer to as \u201cgradient surgery\u201d. We propose a form of gradient surgery that projects the gradient of a task onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task reinforcement learning problems, we find that this approach leads to substantial gains in efficiency and performance.   Further, it can be effectively combined with previously-proposed multi-task architectures for enhanced performance in a model-agnostic way. While deep learning and deep reinforcement learning (RL) have shown considerable promise in enabling systems to perform complex tasks, the data requirements of current methods make it difficult to learn a breadth of capabilities particularly when all tasks are learned individually from scratch. A natural approach to such multi-task learning problems is to train a single network on all tasks jointly, with the aim of discovering shared structure across the tasks in a way that achieves greater efficiency and performance than solving the tasks individually. However, learning multiple tasks all at once results in a difficult optimization problem, sometimes leading to worse overall performance and data efficiency compared to learning tasks individually (Parisotto et al., 2015; Rusu et al., 2016a) . These optimization challenges are so prevalent that multiple multi-task RL algorithms have considered using independent training as a subroutine of the algorithm before distilling the independent models into a multi-tasking model Parisotto et al., 2015; Rusu et al., 2016a; Ghosh et al., 2017; Teh et al., 2017) , producing a multi-task model but losing out on the efficiency gains over independent training. If we could tackle the optimization challenges of multi-task learning effectively, we may be able to actually realize the hypothesized benefits of multi-task learning without the cost in final performance. While there has been a significant amount of research in multi-task learning (Caruana, 1997; Ruder, 2017) , the optimization challenges are not well understood. Prior work has described varying learning speeds of different tasks (Chen et al., 2017) and plateaus in the optimization landscape (Schaul et al., 2019) as potential causes, while a range of other works have focused on the model architecture (Misra et al., 2016b; Liu et al., 2018) . In this work, we instead hypothesize that the central optimization issue in multi-task learning arises from gradients from different tasks conflicting with one another. In particular, we define two gradients to be conflicting if they point away from one another (i.e., have a negative cosine similarity). As a concrete example, consider the 2D optimization landscapes of two task objectives shown in Figure 1 . The optimization landscape of each task consists of a deep valley, as has been characterized of neural network optimization landscapes in the past (Goodfellow et al., 2014) . When considering the combined optimization landscape for multiple tasks, SGD produces gradients that struggle to efficiently find the optimum. This occurs due to a gradient thrashing phenomenon, where the gradient of one task destabilizes optimization in the valley. We can observe this in Figure 1 (d) when the optimization reaches the deep valley of task 1, but is prevented from traversing the valley to an optimum. In Section 6.2, we find experimentally that this thrashing phenomenon also occurs in a neural network multi-task learning problem. The core contribution of this work is a method for mitigating gradient interference by altering the gradients directly, i.e. by performing \"gradient surgery\". If two gradients are conflicting, we alter the gradients by projecting each onto the normal plane of the other, preventing the interfering components of the gradient from being applied to the network. We refer to this particular form of gradient surgery as projecting conflicting gradients (PCGrad). PCGrad is model-agnostic, requiring only a single modification to the application of gradients. Hence, it is easy to apply to a range of problem settings, including multi-task supervised learning and multi-task reinforcement learning, and can also be readily combined with other multi-task learning approaches, such as those that modify the architecture. We evaluate PCGrad on multi-task CIFAR classification, multi-objective scene understanding, a challenging multi-task RL domain, and goal-conditioned RL. Across the board, we find PCGrad leads to significant improvements in terms of data efficiency, optimization speed, and final performance compared to prior approaches. Further, on multi-task supervised learning tasks, PCGrad can be successfully combined with prior state-of-the-art methods for multi-task learning for even greater performance. In this work, we identified one of the major challenges in multi-task optimization: conflicting gradients across tasks. We proposed a simple algorithm (PCGrad) to mitigate the challenge of conflicting gradients via \"gradient surgery\". PCGrad provides a simple way to project gradients to be orthogonal in a multi-task setting, which substantially improves optimization performance, since the task gradients are prevented from negating each other. We provide some simple didactic examples and analysis of how this procedure works in simple settings, and subsequently show significant improvement in optimization for a variety of multi-task supervised learning and reinforcement learning problems. We show that, once some of the optimization challenges of multi-task learning are alleviated by PCGrad, we can obtain the hypothesized benefits in efficiency and asymptotic performance that are believed to be possible in multi-task settings. While we studied multi-task supervised learning and multi-task reinforcement learning in this work, we suspect the problem of conflicting gradients to be prevalent in a range of other settings and applications, such as meta-learning, continual learning, multi-goal imitation learning (Codevilla et al., 2018) , and multi-task problems in natural language processing applications (McCann et al., 2018) . Due to its simplicity and model-agnostic nature, we expect that applying PCGrad in these domains to be a promising avenue for future investigation. Further, the general idea of gradient surgery may be an important ingredient for alleviating a broader class of optimization challenges in deep learning, such as the challenges in the stability challenges in two-player games (Roth et al., 2017) and multi-agent optimizations (Nedic & Ozdaglar, 2009 ). We believe this work to be a step towards simple yet general techniques for addressing some of these challenges. Proof. We will use the shorthand || \u00b7 || to denote the L 2 -norm and \u2207L = \u2207 \u03b8 L, where \u03b8 is the parameter vector. Let g 1 = \u2207L 1 , g 2 = \u2207L 2 , and \u03c6 be the angle between g 1 and g 2 . At each PCGrad update, we have two cases: cos(\u03c6) \u2265 0 or cos(\u03c6 < 0). If cos(\u03c6) \u2265 0, then we apply the standard gradient descent update using t \u2264 1 L , which leads to a strict decrease in the objective function value L(\u03c6) unless \u2207L(\u03c6) = 0, which occurs only when \u03b8 = \u03b8 * (Boyd & Vandenberghe, 2004 ). In the case that cos(\u03c6) < 0, we proceed as follows: Our assumption that \u2207L is Lipschitz continuous with constant L implies that \u2207 2 L(\u03b8) \u2212 LI is a negative semidefinite matrix. Using this fact, we can perform a quadratic expansion of L around L(\u03b8) and obtain the following inequality: Now, we can plug in the PCGrad update by letting \u03b8 We then get: (Expanding, using the identity (Expanding further and re-arranging terms) (Note that cos(\u03c6) < 0 so the final term is non-negative) Plugging this into the last expression above, we can conclude the following: 2 will always be positive unless \u2207L(\u03b8) = 0. This inequality implies that the objective function value strictly decreases with each iteration where cos(\u03c6) > \u22121. Hence repeatedly applying PCGrad process can either reach the optimal value L(\u03b8) = L(\u03b8 * ) or cos(\u03c6) = \u22121, in which case Note that this result only holds when we choose t to be small enough, i.e. t \u2264 1 L ."
}