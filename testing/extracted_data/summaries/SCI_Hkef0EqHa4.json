{
    "title": "Hkef0EqHa4",
    "content": "Neural sequence-to-sequence models are a recently proposed family of approaches used in abstractive summarization of text documents, useful for producing condensed versions of source text narratives without being restricted to using only words from the original text. Despite the advances in abstractive summarization, custom generation of summaries (e.g. towards a user's preference) remains unexplored. In this paper, we present CATS, an abstractive neural summarization model, that summarizes content in a sequence-to-sequence fashion but also introduces a new mechanism to control the underlying latent topic distribution of the produced summaries. Our experimental results on the well-known CNN/DailyMail dataset show that our model achieves state-of-the-art performance. Automatic document summarization is defined as producing a shorter, yet semantically highly related, version of a source document. Solutions to this task are typically classified into two categories: Extractive summarization and abstractive summarization.Extractive summarization refers to methods that select sentences of a source text based on a scoring scheme, and eventually combine those exact sentences in order to produce a summary. Conversely, abstractive summarization aims at producing shortened versions of a source document by generating sentences that do not necessarily appear in the original text. Recent advances in neural sequence-to-sequence modeling have sparked interest in abstractive summarization due to its flexibility and broad range of applications.The majority of research on text summarization thus far has been focused on extractive summarization BID16 , due its simplicity compared to abstractive methods.Beyond providing a generic summary of a longer passage of text, a system which would allow selective summarization based on a user's preference of topic would be of great value in an array of domains. For example, in the field of information retrieval, it could be used to summarize the results of a user search based on the content of the query.Summarization is also extensively used in other domains such as concisely describing the gist of news articles and stories BID21 BID19 , supporting the minutetaking process BID20 in corporate meetings and in the electronic health record domain BID5 , to name a few.In this paper, we introduce CATS, a customizable abstractive topic-based sequence-to-sequence summarization model, which is not only capable of summarizing text documents with an improved performance as compared to the state of the art, but also allows to selectively focus on a range of desired topics of interest when generating summaries. Our experiments corroborate that our model can selectively add or remove certain topics from the summary. Furthermore, our experimental results on a publicly available dataset indicate that the proposed neural sequence-to-sequence model can effectively outperform state-of-the-art baselines in terms of ROUGE.The main contributions of this paper are: (1) We introduce a novel neural sequence-tosequence model based on an encoder-decoder architecture that outperforms the state-of-the-art baselines in the task of abstractive summarization on a benchmark dataset.(2 ) We show how the attention mechanism BID0 may be used for simultaneously identifying important topics as well as recognizing those parts of the encoder output that are vital to be focused on.The remainder of this paper is organized is as follows: Section 2 discusses related work on abstractive neural summarization. In Section 3, we introduce the CATS summarization model. In Section 4, we discuss our experimental setup and results comparing CATS to a broad range of competitive state-of-the-art baselines. Finally , in Section 5, we conclude this paper and present future directions of inquiry. In this paper we present CATS, an abstractive summarization model that makes use of latent topic information in a source document, and is thereby capable of controlling the topics appearing in an output summary of a source document. This can enable customization of generated texts based on user profiles or explicitly given topics, in order to present content tailored to a user's information needs.Our experimental results show that our CATS+coverage model achieves state-of-the-art performance in terms of standard evaluation metrics for summarization (i.e ROUGE) on an important benchmark dataset, while enabling customization in producing summaries.CATS can serve as a foundation for future work in the domain of automatic summarization. Based on the results of this paper, we believe the future work on summarization systems to be exciting, in that a generated summary could be customized to users' needs. We envision three ways of controlling the focus of output summaries using our models: First, as demonstrated in the experiment in Section 4.4.3, certain topics could be disabled in the output of the topic model and be consequently discarded from output summaries. Second, a reference document could be provided to the topic model, its topics could be extracted and subsequently direct the focus of generated summaries. This is useful when a user wants to see summaries/updates primarily or only regarding issues discussed in an existing reference document. Third, content extracted from user profiles (e.g. history of web pages of interest) could be provided to the topic model, their salient themes extracted by the model and then taken into account whenever presenting users with summaries. All three directions are interesting future works of this paper."
}