{
    "title": "HklfNQFL8H",
    "content": "Developing effective biologically plausible learning rules for deep neural networks is important for advancing connections between deep learning and neuroscience. To date, local synaptic learning rules like those employed by the brain have failed to match the performance of backpropagation in deep networks. In this work, we employ meta-learning to discover networks that learn using feedback connections and local, biologically motivated learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding any biologically implausible weight transport. It can be shown mathematically that this approach has sufficient expressivity to approximate any online learning algorithm. Our experiments show that the meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Moreover, we demonstrate empirically that this model outperforms a state-of-the-art gradient-based meta-learning algorithm for continual learning on regression and classification benchmarks. This approach represents a step toward biologically plausible learning mechanisms that can not only match gradient descent-based learning, but also overcome its limitations. Deep learning has achieved impressive success in solving complex tasks, and in some cases its learned representations have been shown to match those in the brain [19, 10] . However, there is much debate over how well the learning algorithm commonly used in deep learning, backpropagation, resembles biological learning algorithms. Causes for skepticism include the facts that (1) backpropagation ignores the nonlinearities imposed by neurons in the backward pass and assumes instead that derivatives of the forward-pass nonlinearities can be applied, (2) in backpropagation, feedback path weights are exactly tied to feedforward weights, even as weights are updated with learning, and (3) backpropagation assumes alternating forward and backward passes [12] . The question of how so-called credit assignment -appropriate propagation of learning signals to non-output neurons -can be performed in biologically plausible fashion in deep neural networks remains open. We propose a new learning paradigm that aims to solve the credit assignment problem in more biologically plausible fashion. Our approach is as follows: (1) endow a deep neural network with feedback connections that propagate information about target outputs to neurons at all layers, (2) apply local plasticity rules (e.g. Hebbian or neuromodulated plasticity) to update feedforward synaptic weights following feedback projections, and (3) employ meta-learning to optimize for the initialization of feedforward weights, the setting of feedback weights, and synaptic plasticity levels. On a set of online regression and classification learning tasks, we find that meta-learned deep networks can successfully perform useful weight updates in early layers, and that feedback with local learning rules can in fact outperform gradient descent as an inner-loop learning algorithm on challenging few-shot and continual learning tasks. This work demonstrates that meta-learning procedures can optimize for neural networks that learn online using local plasticity rules and feedback connections. Several follow-up directions could be pursued. First, meta-learning of this kind is computationally expensive, as the meta-learner must backpropagate through the network's entire training procedure. In order to scale this approach, it will be important to find ways to meta-train networks that generalize to longer lifetimes than were used during meta-training, or to explore alternatives to backprop-based meta-training (e.g. evolutionary algorithms). The present work focused on the case of online learning, but the case of learning from repeated exposure to large datasets is also of interest, and scaling the method in this fashion will be crucial to exploring this regime. Future work could also increase the biological plausibility of the method. For instance, in the present implementation the feedforward and feedback + update passes occur sequentially. However, a natural extension would enable them to run in parallel. This requires ensuring (through appropriate meta-learning and/or a segregated dendrites model [6] ) that feedforward and feedback information do not interfere destructively. Third, the meta-learning procedure in this work optimizes for a precise feedforward and feedback weight initialization. Optimizing instead for a distribution of weight initializations or connectivity patterns would better reflect the stochasticity in synapse development. Another direction is to apply meta-learning to understand biological learning systems (see [9] for an example of such an effort). Well-constrained biological learning models meta-optimized in this manner might show emergence of learning circuits used in biology and even suggest new ones. ["
}