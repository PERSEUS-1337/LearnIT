{
    "title": "BJlLQlrFwS",
    "content": "Continual learning is a longstanding goal of artificial intelligence, but is often counfounded by catastrophic forgetting that prevents neural networks from learning tasks sequentially. Previous methods in continual learning have demonstrated how to mitigate catastrophic forgetting, and learn new tasks while retaining performance on the previous tasks. We analyze catastrophic forgetting from the perspective of change in classifier likelihood and propose a simple L1 minimization criterion which can be adapted to different use cases. We further investigate two ways to minimize forgetting as quantified by this criterion and propose strategies to achieve finer control over forgetting. Finally, we evaluate our strategies on 3 datasets of varying difficulty and demonstrate improvements over previously known L2 strategies for mitigating catastrophic forgetting. Machine learning has achieved successes in many applications, including image recognition, gameplaying, content recommendation and health-care (LeCun et al., 2015) . Most of these systems require large amounts of training data and careful selection of architecture and parameters. Moreover, such systems often have to adapt to changing real-world requirements, and therefore changes in the data. Under these circumstances it is usually desired to retain performance on previous data while learning to perform well on training data with a different distribution. This is what constitutes continual learning (McCloskey, 1989) . A well known problem in the context of continual learning is \"catastrophic forgetting\" (Goodfellow et al., 2013) , which occurs when the training process ends up modifying weights crucial to the performance on the previous data. There has been a lot of work in trying to overcome catastrophic forgetting. Broadly, the approaches in the literature try to mitigate forgetting in three ways: (a) architectural approaches (Yoon et al., 2018; Li et al., 2019) try to incrementally grow the network to learn the new task through added capacity, (b) regularization approaches (Kirkpatrick et al., 2016; Zenke et al., 2017; Wiewel & Yang, 2019) regularize changes to crucial weights, so that the network can learn to perform well on the new task while preserving the performance on the previous tasks (assuming the network has enough capacity for all tasks), and (c) memory approaches (Lopez-Paz, 2017; Nguyen et al., 2018 ) store examples from each task being learned and then learn a new task while simultaneously maximizing performance on each of the stored memories. Performance in these works is often judged with respect to overall accuracy. In the present work, we specifically consider exactly what has been forgotten and what has been learned. Such considerations may be important in safety-critical systems or in systems that have been calibrated. For example, in safety-critical systems, it may not be acceptable to maintain overall performance by trading validated decisions for correct decisions that have not been validated. Likewise, the calibration of a system may require that all decisions, good and bad, remain the same. For the purposes of this paper, we focus on regularization strategies. Regularization strategies typically formulate continual learning in two ways: (a) from a Bayesian perspective (Kirkpatrick et al., 2016; Lee et al., 2017; Liu et al., 2018; Chaudhry et al., 2018) where the goal is to learn the newest task while simultaneously minimizing the KL-divergence between the posterior log likelihood distribution and the prior (see Section 2), or (b) by trying to minimize large changes to influential weights for previous tasks (Zenke et al., 2017; Wiewel & Yang, 2019) . Both these formulations produce an L 2 regularization objective and mitigate forgetting by penalizing changes to weights important to task performances. However, their exact effect on change in classifier likelihood is not known. In this paper, we attempt to quantify this change in classifier likelihood more directly and then use it to provide a generic criterion that can be adapted to different use cases of likelihood preservation. Our contributions are as follows: we propose a more general framework to mitigate catastrophic forgetting, which involves directly penalizing the change in the classifier likelihood functions. Specifically: (a) we analyze catastrophic forgetting and provide a generic L 1 minimization criterion to mitigate it, (b) we propose two strategies to utilize this criteria and discuss how the cross-entropy loss can be reformulated to achieve finer control over forgetting, and (c) we evaluate these strategies on three datasets and demonstrate improvements over traditional L 2 regularization strategies like elastic weight consolidation (EWC) (Kirkpatrick et al., 2016) and synaptic intelligence (SI) (Zenke et al., 2017) . In this section we give further insights about our results. Hyperparameter choice. As can be seen in Table 1 , EWC often requires a high \u03bb to remember previous tasks better. In contrast, the L 1 methods perform well even with a small \u03bb . This can be explained by the fact that minimization with an L 2 method contains a (|\u2206\u03b8|) 2 term instead of (|\u2206\u03b8|). This means that the weights (which are typically quite small) are squared in the L 2 methods, which then requires a stronger \u03bb to compensate for the squaring. So, L 1 methods require a hyperparameter search over a smaller range of values. Degree of preservation. A higher p in DM-p has the same effect as as a low c 1 , c 2 in constrained DM-I, II, III, IV. If c 1 , c 2 are too low, then the training switches off very early, and likewise, if p is too high, the requisite weights never change enough to adapt to the newest task. For the datasets considered, we find that fixing 20 \u2212 40% of the weights typically works the best in DM-p. Improvements over L 2 methods. \u2022 P-MNIST and Sim-EMNIST: EWC and SI are already known to perform well on P-MNIST. In our experiments with the 5-task variant of P-MNIST, they reach an average final accuracy of \u223c 94%. All of DM-I, DM-II, DM-III, DM-IV and DM-p outperform EWC and SI on the 5 task P-MNIST for the same number of epochs, as evidenced by Table 2 . A large improvement was not expected, since EWC already performs well on these datasets. \u2022 S-MNIST: S-MNIST is a difficult dataset because it only involves 2-class classification for each task, which means that the decision boundary found by the network at each task is very susceptible to change in the decision boundary at the next task. This is why EWC is unable to reach beyond \u223c 69% on S-MNIST. DM-p improves on this by a few points, but DM-I, II, III, IV all improve on EWC by \u223c 7 \u2212 10%. Effect of constrained learning. As can be seen in Table 3 , tuned constrained DM-I, II, III, IV all perform better or similar than the tuned unconstrained counterparts. Effect of different types of preservation on performance. While DM-I, II might be suited to specific applications, DM-III, IV typically perform the best in terms of accuracy improvement. This is expected, since DM-III, IV directly regularize change in predicted likelihood for the ground truth. Effect of different types of preservation on retention. We observe mixed results with respect to retention. While it is expected that a higher retention should correspond to a lower amount of forgetting, Table 4 does not show that the L 1 criterion universally has the best retention across the tested datasets. Specifically, the retention advantage of the L 1 criterion is clear for P-MNIST, but it is not as clear for S-MNIST or Sim-EMNIST. We speculate that this is because of the \u03bb chosen for S-MNIST and Sim-EMNIST during the hyperparameter search. During the search, \u03bb is optimized for the best accuracy. In order for EWC to have the best accuracy for these datasets (S-MNIST, Sim-EMNIST), the required hyperparameter \u03bb is huge (10 4 ), which leads to an over-preservation of past classifier likelihood at the expense of the learning the likelihood for the newest task, while the proposed DM strategies use a normal \u03bb for their corresponding best performance. In fact, the huge \u03bb leads to sub-optimal performance for the newest task in EWC, but maximizes the average final accuracy. The retention metric does not capture this sub-optimal performance. Out of DM-I, II, III, IV, the method DM-III retains the most amount of predictions, empirically. For DM-p, the retention advantage is clearly better than plain EWC for P-MNIST and S-MNIST, and close to plain EWC for Sim-EMNIST. Most real-world classification systems rely on connectionist networks, which are known to suffer from catastrophic forgetting when subjected to sequential learning tasks. Existing (regularization) strategies to mitigate catastrophic forgetting typically minimize an L 2 criterion, which can produce non-sparse solutions and require a costly hyperparameter search for the appropriate penalty weight. In this paper, we proposed a more general criterion that involves direct minimization of the change in classifier likelihood and explained how to adapt the criterion to four broad use cases. Using this criterion, we identified two ways to improve the classifier performance: (a) by directly softregularizing the change in classifier likelihood and (b) by freezing influential weights. Both of these perform better than, or at least similar to, existing L 2 strategies. We further discussed the effect of various proposed classifier likelihood preservation methods and showed that preserving the classifier likelihood with respect to the ground truth is a good strategy to preserve classifier performance. Future Work. Having compared our method to existing L 2 strategies, it would be interesting to compare and contrast the benefits and problems of the proposed L 1 strategies with other non-L 2 strategies for continual learning, e.g., IMM (Lee et al., 2017) and VCL (Nguyen et al., 2018) . It would be also be interesting to see the effect of direct minimization strategies for more complicated and realistic image classification datasets, like CIFAR100 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009 )."
}