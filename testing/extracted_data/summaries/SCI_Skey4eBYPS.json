{
    "title": "Skey4eBYPS",
    "content": "We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks. Neural Processes (NPs; Garnelo et al., 2018b; a) are a rich class of models that define a conditional distribution p(y|x, Z, \u03b8) over output variables y given input variables x, parameters \u03b8, and a set of observed data points in a context set Z = {x m , y m } M m=1 . A key component of NPs is the embedding of context sets Z into a representation space through an encoder Z \u2192 E(Z), which is achieved using a DEEPSETS function approximator (Zaheer et al., 2017 ). This simple model specification allows NPs to be used for (i) meta-learning (Thrun & Pratt, 2012; Schmidhuber, 1987) , since predictions can be generated on the fly from new context sets at test time; and (ii) multi-task or transfer learning (Requeima et al., 2019) , since they provide a natural way of sharing information between data sets. Moreover, conditional NPs (CNPs; Garnelo et al., 2018a) , a deterministic variant of NPs, can be trained in a particularly simple way with maximum likelihood learning of the parameters \u03b8, which mimics how the system is used at test time, leading to strong performance (Gordon et al., 2019) . Natural application areas of NPs include time series, spatial data, and images with missing values. Consequently, such domains have been used extensively to benchmark current NPs (Garnelo et al., 2018a; b; Kim et al., 2019) . Often, ideal solutions to prediction problems in such domains should be translation equivariant: if the data are translated in time or space, then the predictions should be translated correspondingly (Kondor & Trivedi, 2018; Cohen & Welling, 2016) . This relates to the notion of stationarity. As such, NPs would ideally have translation equivariance built directly into the modelling assumptions as an inductive bias. Unfortunately, current NP models must learn this structure from the data set instead, which is sample and parameter inefficient as well as impacting the ability of the models to generalize. The goal of this paper is to build translation equivariance into NPs. Famously, convolutional neural networks (CNNs) added translation equivariance to standard multilayer perceptrons (LeCun et al., 1998; Cohen & Welling, 2016) . However, it is not straightforward to generalize NPs in an analogous way: (i) CNNs require data to live \"on the grid\" (e.g. image pixels form a regularly spaced grid), while many of the above domains have data that live \"off the grid\" (e.g. time series data may be observed irregularly at any time t \u2208 R). (ii) NPs operate on partially observed context sets whereas CNNs typically do not. (iii) NPs rely on embedding sets into a finite-dimensional vector space for which the notion of equivariance with respect to input translations is not natural, as we detail in Section 3. In this work, we introduce the CONVCNP, a new member of the NP family that accounts for translation equivariance. 1 This is achieved by extending the theory of learning on sets to include functional representations, which in turn can be used to express any translation-equivariant NP model. Our key contributions can be summarized as follows. (i) We provide a representation theorem for translation-equivariant functions on sets, extending a key result of Zaheer et al. (2017) to functional embeddings, including sets of varying size. (ii) We extend the NP family of models to include translation equivariance. (iii) We evaluate the CONVCNP and demonstrate that it exhibits excellent performance on several synthetic and real-world benchmarks. We have introduced CONVCNP, a new member of the CNP family that leverages embedding sets into function space to achieve translation equivariance. The relationship to (i) the NP family, and (ii) representing functions on sets, each imply extensions and avenues for future work. Deep sets. Two key issues in the existing theory on learning with sets (Zaheer et al., 2017; Qi et al., 2017a; Wagstaff et al., 2019) are (i) the restriction to fixed-size sets, and (ii) that the dimensionality of the embedding space must be no less than the cardinality of the embedded sets. Our work implies that by considering appropriate embeddings into a function space, both issues are alleviated. In future work, we aim to further this analysis and formalize it in a more general context. Point-cloud models. Another line of related research focuses on 3D point-cloud modelling (Qi et al., 2017a; b) . While original work focused on permutation invariance (Qi et al., 2017a; Zaheer et al., 2017) , more recent work has considered translation equivariance as well (Wu et al., 2019) , leading to a model closely resembling CONVDEEPSETS. The key differences with our work are the following: (i) Wu et al. (2019) Correlated samples and consistency under marginalization. In the predictive distribution of CON-VCNP (Equation (2)), predicted ys are conditionally independent given the context set. Consequently, samples from the predictive distribution lack correlations and appear noisy. One solution is to instead define the predictive distribution in an autoregressive way, like e.g. PixelCNN++ (Salimans et al., 2017) . Although samples are now correlated, the quality of the samples depends on the order in which the points are sampled. Moreover, the predicted ys are then not consistent under marginalization (Garnelo et al., 2018b; Kim et al., 2019) . Consistency under marginalization is more generally an issue for neural autoregressive models (Salimans et al., 2017; Parmar et al., 2018) , although consistent variants have been devised (Louizos et al., 2019) . To overcome the consistency issue for CONVCNP, exchangeable neural process models (e.g. Korshunova et al., 2018; Louizos et al., 2019) may provide an interesting avenue. Another way to introduce dependencies between ys is to employ latent variables as is done in neural processes (Garnelo et al., 2018b) . However, such an approach only achieves conditional consistency: given a context set, the predicted ys will be dependent and consistent under marginalization, but this does not lead to a consistent joint model that also includes the context set itself. For each dataset, an image is randomly sampled, the first row shows the given context points while the second is the mean of the estimated conditional distribution. From left to right the first seven columns correspond to a context set with 3, 1%, 5%, 10%, 20%, 30%, 50%, 100% randomly sampled context points. In the last two columns, the context sets respectively contain all the pixels in the left and top half of the image. CONVCNPXL is shown for all datasets besides ZSMM, for which we show the fully translation equivariant CONVCNP."
}