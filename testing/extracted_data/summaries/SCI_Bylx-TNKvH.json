{
    "title": "Bylx-TNKvH",
    "content": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n Ever since its early successes, deep learning has been a puzzle for machine learning theorists. Multiple aspects of deep learning seem at first sight to contradict common sense: single-hidden-layer networks suffice to approximate any continuous function (Cybenko, 1989; Hornik et al., 1989 ), yet in practice deeper is better; the loss surface is highly non-convex, yet it can be minimised by first-order methods; the capacity of the model class is immense, yet deep networks tend not to overfit (Zhang et al., 2017) . Recent investigations into these and other questions have emphasised the role of overparameterisation, or highly redundant function representation. It is now known that overparameterised networks enjoy both easier training (Allen-Zhu et al., 2019; Du et al., 2019; Frankle & Carbin, 2019) , and better generalisation (Belkin et al., 2019; Neyshabur et al., 2019; Novak et al., 2018) . However, the specific mechanism by which over-parameterisation operates is still largely a mystery. In this work, we study one particular aspect of over-parameterisation, namely the ability of neural networks to represent a target function in many different ways. In other words, we ask whether many different parameter configurations can give rise to the same function. Such a notion of parameterisation redundancy has so far remained unexplored, despite its potential connections to the structure of the loss landscape, as well as to the literature on neural network capacity in general. Specifically, we consider feed-forward ReLU networks, with weight matrices W 1 , . . . , W L , and biases b 1 , . . . , b L ,. We study parameter transformations which preserve the output behaviour of the network h(z) = W L \u03c3(W L\u22121 \u03c3(. . . W 1 z + b 1 . . . ) + b L\u22121 ) + b L for all inputs z in some domain Z. Two such transformations are known for feed-forward ReLU architectures: 1. Permutation of units (neurons) within a layer, i.e. for some permutation matrix P, 2. Positive scaling of all incoming weights of a unit coupled with inverse scaling of its outgoing weights. Applied to a whole layer, with potentially different scaling factors arranged into a diagonal matrix M, this can be written as Our main theorem applies to architectures with non-increasing widths, and shows that there are no other function-preserving parameter transformations besides permutation and scaling. Stated formally: Theorem 1. Consider a bounded open nonempty domain Z \u2286 R d0 and any architecture For this architecture, there exists a ReLU network h \u03b8 : Z \u2192 R, or equivalently a setting of the weights \u03b8 (W 1 , b 1 , . . . , W L , b L ), such that for any 'general' ReLU network h \u03b7 : Z \u2192 R (with the same architecture) satisfying h \u03b8 (z) = h \u03b7 (z) for all z \u2208 Z, there exist permutation matrices P 1 , . . . P L\u22121 , and positive diagonal matrices M 1 , . . . , M L\u22121 , such that where \u03b7 (W 1 , b 1 , . . . , W L , b L ) are the parameters of h \u03b7 . In the above, 'general' networks is a class of networks meant to exclude degenerate cases. We give a more precise definition in Section 3; for now it suffices to note that almost all networks are general. The proof of the result relies on a geometric understanding of prediction surfaces of ReLU networks. These surfaces are piece-wise linear functions, with non-differentiabilities or 'folds' between linear regions. It turns out that folds carry a lot of information about the parameters of a network, so much in fact, that some networks are uniquely identified (up to permutation and scaling) by the function they implement. This is the main insight of the theorem. In the following sections, we introduce in more detail the concept of a fold-set, and describe its geometric structure for a subclass of ReLU networks. The paper culminates in a proof sketch of the main result. The full proof, including proofs of intermediate results, is included in the Appendix. In this work, we have shown that for architectures with non-increasing widths, certain ReLU networks are almost uniquely identified by the function they implement. The result suggests that the function-equivalence classes of ReLU networks are surprisingly small, i.e. there may be only little redundancy in the way ReLU networks are parameterised, contrary to what is commonly believed. This apparent contradiction could be explained in a number of ways: \u2022 It could be the case that even though exact equivalence classes are small, approximate equivalence is much easier to achieve. That is, it could be that h \u03b8 \u2212 h \u03b7 \u2264 is satisfied by a disproportionately larger class of parameters \u03b7 than h \u03b8 \u2212 h \u03b7 = 0. This issue is related to the so-called inverse stability of the realisation map of neural nets, which is not yet well understood. \u2022 Another possibility is that the kind of networks we consider in this paper is not representative of networks typically encountered in practice, i.e. it could be that 'typical networks' do not have well connected dependency graphs, and are therefore not easily identifiable. \u2022 Finally, we have considered only architectures with non-increasing widths, whereas some previous theoretical work has assumed much wider intermediate layers compared to the input dimension. It is possible that parameterisation redundancy is much larger in such a regime compared to ours. However, gains from over-parameterisation have also been observed in practical settings with architectures not unlike those considered here. We consider these questions important directions for further research. We also hypothesise that our analysis could be extended to convolutional and recurrent networks, and to other piece-wise linear activation functions such as leaky ReLU. Definition A.1 (Partition). Let S \u2286 Z. We define the partition of Z induced by S, denoted P Z (S), as the set of connected components of Z \\ S. Definition A.2 (Piece-wise hyperplane). Let P be a partition of Z. We say H \u2286 Z is a piece-wise hyperplane with respect to partition P, if H = \u2205 and there exist (w, b) = (0, 0) and P \u2208 P such that H = {z \u2208 P | w z + b = 0}. Definition A.3 (Piece-wise linear surface / pwl. surface). A set S \u2286 Z is called a piece-wise linear surface on Z of order \u03ba if it can be written as , and no number smaller than \u03ba admits such a representation. Lemma A.1. If S 1 , S 2 are piece-wise linear surfaces on Z of order k 1 and k 2 , then S 1 \u222a S 2 is a piece-wise linear surface on Z of order at most max {k 1 , k 2 }. We can write H Given sets Z and S \u2286 Z, we introduce the notation (The dependence on Z is suppressed.) By Lemma A.1, i S is itself a pwl. surface on Z of order at most i. Lemma A.2. For i \u2264 j and any set S, we have i j S = j i S = i S. Proof. We will need these definitions: j S = {S \u2286 S | S is a pwl. surface of order at most j}, i j S = {S \u2286 j S | S is a pwl. surface of order at most i}, surface of order at most j}. Consider first the equality j i S = i S. We know that j i S \u2286 i S because the square operator always yields a subset. At the same time, i S \u2286 j i S, because i S satisfies the condition for membership in (6). To prove the equality i j S = i S, we use the inclusion j S \u2286 S to deduce i j S \u2286 i S. Now let S \u2286 S be one of the sets under the union in (3), i.e. it is a pwl. surface of order at most i. Then it is also a pwl. surface of order at most j, implying S \u2286 j S. This means S is also one of the sets under the union in (5), proving that i S \u2286 i j S. Lemma A.3. Let Z and S \u2286 Z be sets. Then one can write k+1 S = k S \u222a i H i where H i are piece-wise hyperplanes wrt. P Z ( k S). Proof. At the same time, is a pwl. surface of order at most k + 1 because k S is a pwl. surface of order at most k and H k+1 i can be decomposed into piece-wise hyperplanes wrt. Definition A.4 (Canonical representation of a pwl. surface). Let S be a pwl. surface on Z. The pwl. is a pwl. surface in canonical form, then \u03ba is the order of S. Proof. Denote the order of S by \u03bb. By the definition of order, \u03bb \u2264 \u03ba, and S = \u03bb S. Then, since It follows that \u03ba = \u03bb. Lemma A.5. Every pwl. surface has a canonical representation. Proof. The inclusion l\u2208[k],i\u2208[n l ] H l i \u2286 k S holds for any representation. We will show the other inclusion by induction in the order of S. If S is order one, 1 S \u2286 S = i\u2208[n1] H 1 i holds for any representation and we are done. Now assume the lemma holds up to order \u03ba \u2212 1, and let S be order \u03ba. Then by Lemma A.3, S = \u03ba S = \u03ba\u22121 S \u222a i H \u03ba i , where H \u03ba i are piece-wise hyperplanes wrt. P Z ( \u03ba\u22121 S). By the inductive assumption, \u03ba\u22121 S has a canonical representation, Proof. Let k \u2208 [\u03ba]. Because both representations are canonical, we have where H k i and G k j are piece-wise hyperplanes wrt. where on both sides above we have a union of hyperplanes on an open set. The claim follows. Definition A.5 (Dependency graph of a pwl. surface). Let S be a piece-wise linear surface on Z, and let S = l\u2208[\u03ba],i\u2208[n l ] H l i be its canonical representation. We define the dependency graph of S as the directed graph that has the piece-wise hyperplanes H l i l,i as vertices, and has an edge"
}