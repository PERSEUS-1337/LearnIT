{
    "title": "S1xjJpNYvB",
    "content": "Although few-shot learning research has advanced rapidly with the help of meta-learning, its practical usefulness is still limited because most of the researches assumed that all meta-training and meta-testing examples came from a single domain. We propose a simple but effective way for few-shot classification in which a task distribution spans multiple domains including previously unseen ones during meta-training.\n The key idea is to build a pool of embedding models which have their own metric spaces and to learn to select the best one for a particular task through multi-domain meta-learning. This simplifies task-specific adaptation over a complex task distribution as a simple selection problem rather than modifying the model with a number of parameters at meta-testing time. Inspired by common multi-task learning techniques, we let all models in the pool share a base network and add a separate modulator to each model to refine the base network in its own way. This architecture allows the pool to maintain representational diversity and each model to have domain-invariant representation as well. \n Experiments show that our selection scheme outperforms other few-shot classification algorithms when target tasks could come from many different domains. They also reveal that aggregating outputs from all constituent models is effective for tasks from unseen domains showing the effectiveness of our framework. Few-shot learning in the perspective of meta-learning aims to train models which can quickly solve novel tasks or adapt to new environments with limited number of examples. In case of few-shot classification, models are usually evaluated on a held-out dataset which does not have any common class with the training dataset. In the real world, however, we often face harder problems in which novel tasks arise arbitrarily from many different domains even including previously unseen ones. In this study, we propose a more practical few-shot classification algorithm to generalize across domains beyond the common assumption, i.e., meta-training and meta-testing within a single domain. Our approach to cover a complex multi-domain task distribution is to construct a pool of multiple models and learn to select the best one given a novel task through meta-training over various domains. This recasts task-specific adaption across domains as a simple selection problem, which could be much easier than manipulating high-dimensional parameters or representations of a single model to adapt to a novel task. Furthermore, we enforce all models to share some of the parameters and train per-model modulators with model-specific parameters on top of that. By doing so, each model could keep important domain-invariant features while the model pool has representational diversity as a whole without a significant increase of model parameters. We train and test our algorithms on various image classification datasets with different characteristics. Experimental results show that the proposed selection scheme outperforms other state-of-theart algorithms in few-shot classification tasks from many different domains without being given any knowledge of the domain which the task belongs to. We also show that even few-shot classification tasks from previously unseen domains, i.e., domains which have never appeared during meta-training, can be done successfully by averaging outputs of all models. We proposed a new few-shot classification method which is capable of dealing with many different domains including unseen domains. The core idea was to build a pool of embedding models, each of which was diversified by its own modulator while sharing most of parameters with others, and to learn to select the best model for a target task through cross-domain meta-learning. The simplification of the task-specific adaptation as a small classification problem made our selection-based algorithm easy to learn, which in turn helped the learned model to work more effectively for multidomain few-shot classification. The architecture with one shared model and disparate modulators encouraged our pool to maintain domain-invariant knowledge as well as cross-domain diversity. It helped our algorithms to generalize to heterogeneous domains including unseen ones even when we used one best model solely or all models collectively. We believe that there is still a large room for improvement in this challenging task. It would be one promising extension to find the optimal way to build the pool without the constraint on the number of models (i.e., one model per dataset) so that it can work even with a single source dataset with large diversity. Soft selection or weighted averaging can be also thought as one of future research directions because a single model or uniform averaging is less likely to be optimal. We can also consider a more scalable extension to allow continual expansion of the pool only by training a modulator for an incoming source domain without re-training all existing models in the pool. Although the number of parameters does not increase much by virtue of the parameter sharing between models, the computational cost in the averaging-based methods needs to be improved over the current linear increase with the number of models."
}