{
    "title": "SylCrnCcFX",
    "content": "Deep networks realize complex mappings that are often understood by their locally linear behavior at or around points of interest. For example, we use the derivative of the mapping with respect to its inputs for sensitivity analysis, or to explain (obtain coordinate relevance for) a prediction. One key challenge is that such derivatives are themselves inherently unstable. In this paper, we propose a new learning problem to encourage deep networks to have stable derivatives over larger regions. While the problem is challenging in general, we focus on networks with piecewise linear activation functions. Our algorithm consists of an inference step that identifies a region around a point where linear approximation is provably stable, and an optimization step to expand such regions. We propose a novel relaxation to scale the algorithm to realistic models. We illustrate our method with residual and recurrent networks on image and sequence datasets. Complex mappings are often characterized by their derivatives at points of interest. Such derivatives with respect to the inputs play key roles across many learning problems, including sensitivity analysis. The associated local linearization is frequently used to obtain explanations for model predictions BID3 BID24 BID28 BID26 ; explicit first-order local approximations BID22 BID17 BID31 Koh & Liang, 2017; BID1 ; or used to guide learning through regularization of functional classes controlled by derivatives BID19 BID5 Mroueh et al., 2018) . We emphasize that the derivatives discussed in this paper are with respect to the input coordinates rather than parameters.The key challenge lies in the fact that derivatives of functions parameterized by deep learning models are not stable in general BID14 . State-of-the-art deep learning models (He et al., 2016; Huang et al., 2017) are typically over-parametrized BID37 , leading to unstable functions as a by-product. The instability is reflected in both the function values BID17 as well as the derivatives BID14 BID0 . Due to unstable derivatives, first-order approximations used for explanations therefore also lack robustness BID14 BID0 .We note that gradient stability is a notion different from adversarial examples. A stable gradient can be large or small, so long as it remains approximately invariant within a local region. Adversarial examples, on the other hand, are small perturbations of the input that change the predicted output BID17 . A large local gradient, whether stable or not in our sense, is likely to contribute to finding an adversarial example. Robust estimation techniques used to protect against adversarial examples (e.g., (Madry et al., 2018) ) focus on stable function values rather than stable gradients but can nevertheless indirectly impact (potentially help) gradient stability. A direct extension of robust estimation to ensure gradient stability would involve finding maximally distorted derivatives and require access to approximate Hessians of deep networks.In this paper, we focus on deep networks with piecewise linear activations to make the problem tractable. The special structure of this class of networks (functional characteristics) allows us to infer lower bounds on the p margin -the maximum radius of p -norm balls around a point where derivatives are provably stable. In particular, we investigate the special case of p = 2 since the lower bound has an analytical solution, and permits us to formulate a regularization problem to maximize it. The resulting objective is, however , rigid and non-smooth, and we further relax the learning problem in a manner resembling (locally) support vector machines (SVM) BID29 BID8 .Both the inference and learning problems in our setting require evaluating the gradient of each neuron with respect to the inputs, which poses a significant computational challenge. For piecewise linear networks, given D-dimensional data, we propose a novel perturbation algorithm that collects all the exact gradients by means of forward propagating O(D) carefully crafted samples in parallel without any back-propagation. When the GPU memory cannot fit O(D) samples in one batch, we develop an unbiased approximation to the objective with a random subset of such samples.Empirically, we examine our inference and learning algorithms with fully-connected (FC), residual (ResNet) (He et al., 2016) , and recurrent (RNN) networks on image and time-series datasets with quantitative and qualitative experiments. The main contributions of this work are as follows:\u2022 Inference algorithms that identify input regions of neural networks, with piecewise linear activation functions, that are provably stable.\u2022 A novel learning criterion that effectively expand regions of provably stable derivatives.\u2022 Novel perturbation algorithms that scale computation to high dimensional data.\u2022 Empirical evaluation with several types of networks. This paper introduces a new learning problem to endow deep learning models with robust local linearity. The central attempt is to construct locally transparent neural networks, where the derivatives faithfully approximate the underlying function and lends itself to be stable tools for further applications. We focus on piecewise linear networks and solve the problem based on a margin principle similar to SVM. Empirically, the proposed ROLL loss expands regions with provably stable derivatives, and further generalize the stable gradient property across linear regions. DISPLAYFORM0 , and the feasible set of the activation pattern is equivalent to DISPLAYFORM1 Ifx is feasible to the fixed activation pattern o 1 j , it is equivalent to thatx satisfies the linear constraint DISPLAYFORM2 in the first layer.Assumex has satisfied all the constraints before layer i > 1. We know if all the previous layers follows the fixed activation indicators, it is equivalent to rewrite each DISPLAYFORM3 Then for j \u2208 [N i ], it is clear that z DISPLAYFORM4 The proof follows by induction."
}