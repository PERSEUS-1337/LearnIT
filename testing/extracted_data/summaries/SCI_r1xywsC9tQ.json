{
    "title": "r1xywsC9tQ",
    "content": " In this paper, we investigate mapping the hyponymy relation of\n wordnet to feature vectors.\n   We aim to model lexical knowledge in such a way that it can be used as\n  input in generic machine-learning models, such as phrase entailment\n  predictors.\n   We propose two models. The first one leverages an existing mapping of\n  words to feature vectors (fasttext), and attempts to classify\n  such vectors as within or outside of each class. The second model is fully supervised,\n  using solely wordnet as a ground truth. It maps each concept to an\n  interval or a disjunction thereof.\n   On the first model, we approach, but not quite attain state of the\n  art performance. The second model can achieve near-perfect accuracy.\n Distributional encoding of word meanings from the large corpora BID8 BID12 have been found to be useful for a number of NLP tasks. These approaches are based on a probabilistic language model by BID2 of word sequences, where each word w is represented as a feature vector f (w) (a compact representation of a word, as a vector of floating point values).This means that one learns word representations (vectors) and probabilities of word sequences at the same time.While the major goal of distributional approaches is to identify distributional patterns of words and word sequences, they have even found use in tasks that require modeling more fine-grained relations between words than co-occurrence in word sequences. Folklore has it that simple manipulations of distributional word embedding vectors is inadequate for problems involving detection of other kinds of relations between words rather than their co-occurrences. In particular , distributional word embeddings are not easy to map onto ontological relations and vice-versa. We consider in this paper the hyponymy relation, also called the is-a relation, which is one of the most fundamental ontological relations.Possible sources of ground truth for hyponymy are WORDNET Fellbaum (1998), FRAMENET Baker et al. (1998) , and JEUXDEMOTS 1 BID6 . These resources have been designed to include various kinds of lexical relations between words, phrases, etc. However these resources have a fundamentally symbolic representation, which can not be readily used as input to neural NLP models. Several authors have proposed to encode hyponymy relations in feature vectors BID14 BID13 BID0 BID10 . However, there does not seem to be a common consensus on the underlying properties of such encodings. In this paper, we aim to fill this gap and clearly characterize the properties that such an embedding should have. We additionally propose two baseline models approaching these properties: a simple mapping of FASTTEXT embeddings to the WORDNET hyponymy relation, and a (fully supervised) encoding of this relation in feature vectors. We found that defining the problem of representing HYPONYMY in a feature vector is not easy. Difficulties include 1. the sparseness of data, 2. whether one wants to base inclusion on an underlying (possibly relaxed) inclusion in the space of vectors, and 3. determining what one should generalize.Our investigation of WORDNET over fastText demonstrates that WORDNET classes are not cleanly linearly separated in fastText, but they are sufficiently well separated to give a useful recall for an approximate inclusion property. Despite this, and because the negative cases vastly outnumber the positive cases, the rate of false negatives is still too high to give any reasonable precision. One could try to use more complex models, but the sparsity of the data would make such models extremely sensitive to overfitting.Our second model takes a wholly different approach: we construct intervals directly from the HY-PONYMY relation. The main advantage of this method is its simplicity and high-accuracy. Even with a single dimension it rivals other models. A possible disadvantage is that the multi-dimensional version of this model requires disjunctions to be performed. Such operations are not necessarily available in models which need to make use of the HYPONYMY relation. At this stage, we make no attempt to match the size of intervals to the probability of a word. We aim to address this issue in future work.Finally, one could see our study as a criticism of WORDNET as a natural representative of HY-PONYMY. Because it is almost structured like a tree, one can suspect that it in fact misses many hyponymy relations. This would also explain why our simple fastText-based model predicts more relations than present in WORDNET. One could think of using other resources, such as JEUXDE-MOTS. Our preliminary investigations suggest that these seem to suffer from similar flaws -we leave complete analysis to further work."
}