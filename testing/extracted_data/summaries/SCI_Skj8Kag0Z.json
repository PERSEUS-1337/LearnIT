{
    "title": "Skj8Kag0Z",
    "content": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates. Adversarial networks play an important role in a variety of applications, including image generation (Zhang et al., 2017; Wang & Gupta, 2016) , style transfer BID2 Taigman et al., 2017; Wang & Gupta, 2016; BID17 , domain adaptation (Taigman et al., 2017; Tzeng et al., 2017; BID11 , imitation learning BID15 , privacy BID9 BID0 , fair representation (Mathieu et al., 2016; BID9 , etc. One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models BID13 Radford et al., 2016; BID7 Mirza & Osindero, 2014) .While adversarial networks have the power to attack a wide range of previously unsolved problems, they suffer from a major flaw: they are difficult to train. This is because adversarial nets try to accomplish two objectives simultaneously; weights are adjusted to maximize performance on one task while minimizing performance on another. Mathematically , this corresponds to finding a saddle point of a loss function -a point that is minimal with respect to one set of weights, and maximal with respect to another.Conventional neural networks are trained by marching down a loss function until a minimizer is reached ( FIG0 ). In contrast, adversarial training methods search for saddle points rather than a minimizer, which introduces the possibility that the training path \"slides off\" the objective functions and the loss goes to \u2212\u221e FIG0 ), resulting in \"collapse\" of the adversarial network. As a result, many authors suggest using early stopping, gradients/weight clipping , or specialized objective functions BID13 Zhao et al., 2017; to maintain stability.In this paper, we present a simple \"prediction\" step that is easily added to many training algorithms for adversarial nets. We present theoretical analysis showing that the proposed prediction method is asymptotically stable for a class of saddle point problems. Finally, we use a wide range of experiments to show that prediction enables faster training of adversarial networks using large learning rates without the instability problems that plague conventional training schemes. If minimization (or, conversely , maximization) is more powerful, the solution path \"slides off\" the loss surface and the algorithm becomes unstable, resulting in a sudden \"collapse\" of the network. We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks. We present theoretical results showing that the prediction step is asymptotically stable for solving saddle point problems. We show, using a variety of test problems, that prediction steps prevent network collapse and enable training with a wider range of learning rates than plain SGD methods."
}