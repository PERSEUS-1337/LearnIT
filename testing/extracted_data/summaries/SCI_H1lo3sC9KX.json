{
    "title": "H1lo3sC9KX",
    "content": "Asynchronous distributed gradient descent algorithms for training of deep neural\n networks are usually considered as inefficient, mainly because of the Gradient delay\n problem. In this paper, we propose a novel asynchronous distributed algorithm\n that tackles this limitation by well-thought-out averaging of model updates, computed\n by workers. The algorithm allows computing gradients along the process\n of gradient merge, thus, reducing or even completely eliminating worker idle time\n due to communication overhead, which is a pitfall of existing asynchronous methods.\n We provide theoretical analysis of the proposed asynchronous algorithm,\n and show its regret bounds. According to our analysis, the crucial parameter for\n keeping high convergence rate is the maximal discrepancy between local parameter\n vectors of any pair of workers. As long as it is kept relatively small, the\n convergence rate of the algorithm is shown to be the same as the one of a sequential\n online learning. Furthermore, in our algorithm, this discrepancy is bounded\n by an expression that involves the staleness parameter of the algorithm, and is\n independent on the number of workers. This is the main differentiator between\n our approach and other solutions, such as Elastic Asynchronous SGD or Downpour\n SGD, in which that maximal discrepancy is bounded by an expression that\n depends on the number of workers, due to gradient delay problem. To demonstrate\n effectiveness of our approach, we conduct a series of experiments on image\n classification task on a cluster with 4 machines, equipped with a commodity communication\n switch and with a single GPU card per machine. Our experiments\n show a linear scaling on 4-machine cluster without sacrificing the test accuracy,\n while eliminating almost completely worker idle time. Since our method allows\n using commodity communication switch, it paves a way for large scale distributed\n training performed on commodity clusters. Distributed training of deep learning models is devised to reduce training time of the models. Synchronous distributed SGD methods, such as BID0 and BID7 , perform training using mini-batch size of several dozens of thousands of images. However, they either require expensive communication switch for fast gradient sharing between workers or, otherwise, introduce a high communication overhead during gradient merge, where workers are idle waiting for communicating gradients over communication switch.Distributed asynchronous SGD methods reduce the communication overhead on one hand, but usually introduce gradient delay problem on the other hand, as described in BID0 . Indeed, usually in an asynchronous distributed approach, a worker w obtains a copy of the central model, computes a gradient on this model and merges this gradient back into the central model. Note, however, that since the worker obtained the copy of the central model till it merges its gradient back into the central model, other workers could have merged their gradients into the central model. Thus, when the worker w merges its gradient into a central model, that model may have been updated and, thus, the gradient of the worker w is delayed, leading to gradient delay problem. We will refer to algorithms that suffer from gradient delay problem as gradient delay algorithms, e.g. Downpour SGD BID1 .As our analysis reveals in Section 3, the quantity that controls the convergence rate of an asynchronous distributed algorithm, is maximal pairwise distance -the maximal distance between local models of any pair of workers at any iteration. Usually gradient delay algorithms do not limit this distance and it may depend on the number of asynchronous workers, which may be large in large clusters. This may explain their poor scalability, convergence rate and struggle to reach as high test accuracy as in synchronous SGD algorithms, as experimentally shown in BID0 .While Elastic Averaging SGD Zhang et al. (2015) is also a gradient delay algorithm, it introduces a penalty for workers, whose models diverge too far from the central model. This, in turn , helps to reduce the maximal pairwise distance between local models of workers and, thus, leads to better scalability and convergence rate. In contrast, our analysis introduces staleness parameter that directly controls the maximal pair distance of the asynchronous workers.Our analysis builds on the work of BID10 , who studied convergence rate of gradient delay algorithms, when the maximum delay is bounded. They provided analysis for Lipschitz continuous losses, strongly convex and smooth losses. While they show that bounding staleness can improve convergence rate of a gradient delay algorithm, in their algorithm each worker computes exactly one gradient and is idle, waiting to merge the gradient with PS model and download the updated PS model back to the worker.The main contributions of this paper are the following. We present and analyze a new asynchronous distributed SGD method that both reduces idle time and eliminates gradient delay problem. Our main theoretic result shows that an asynchronous distributed SGD algorithm can achieve convergence rate as good as in sequential online learning. We support this theoretic result by conducting experiments that show that on a cluster with up to 4 machines, with a single GPU per machine and a commodity communication switch, our asynchronous method achieves linear scalability without degradation of the test accuracy. We presented a new asynchronous distributed SGD method. We show empirically that it reduces both idle time and gradient delay. We analyze the synchronous part of the algorithm, and show theoretical regret bounds.The proposed method shows promising results on distributed training of deep neural networks. We show that our method eliminates waiting times, which allows significant improvements in run time, compared to fully synchronous setup. The very fact of efficient hiding of communication overhead opens opportunity for distributed training over commodity clusters. Furthermore, the experiments show linear scaling of training time from 1 to 4 GPU's without compromising the final test accuracy.Proof. We start with RHS of (8) DISPLAYFORM0 By the definition of average parameter vector (1), the first summand in RHS of (29) equals x i\u03b2+t , while the last two summands cancel each other.Proof of Lemma 3.3.Proof. At iteration (i \u2212 1)\u03b2 after reset operation FORMULA7 , workers w and w start updating their local parameter vectors for \u03b2 iterations, so that at the end of the cycle before the reset operation (7), DISPLAYFORM1 The reset operation FORMULA7 at iteration i\u03b2 adds the updates of workers w and w , computed at iterations (i \u2212 1)\u03b2 + 1, . . . , i\u03b2, to the common copy of PS model, so that x (i\u22121)\u03b2,w \u2212 x (i\u22121)\u03b2,w = 0. Thus, after the reset operation DISPLAYFORM2 Finally, from (31), our assumption on the size of the gradient \u2207f t (x) and from decreasing learning rate, during this cycle that started at iteration (i \u2212 1)\u03b2, the distance between workers w and w grows at most by DISPLAYFORM3 Proof of Lemma 3.4.Proof. We decompose our progress as follows DISPLAYFORM4 To prove (33) from (32), we used (12) Dividing both sides by \u03b7 i\u03c4 +t and moving < x i\u03c4 +t \u2212 x * ,g i\u03c4 +t > to the LHS completes the proof.Proof of Theorem 3.5.Proof. First we state a useful inequality: For n vectors a i , i = 1, . . . , n (by induction on n): DISPLAYFORM5 Also we will use the following sum bounds: DISPLAYFORM6 and DISPLAYFORM7 We start with summing (13) along iterations: DISPLAYFORM8 Next, we borrow the derivation of expressions (38) and (39) from the proof of Theorem 2, BID9 . Note, however, that we added a new term to (37) -the last term that is specific to Algorithm 2. This term does not appear in the proof of Theorem 2, BID9 . By the Lipschitz property of gradients and the definition of \u03b7 t , we can bound the first summand of the above regret expression via DISPLAYFORM9 Also DISPLAYFORM10 We omit the negative factor \u2212 DISPLAYFORM11 . Now we start the analysis of the last term in (37), which is new and specific to Algorithm 2. Using (34), we bound the last summand of (37) DISPLAYFORM12 Substituting FORMULA3 , FORMULA3 and FORMULA4 into FORMULA3 , we get DISPLAYFORM13 We use (14) in the above expression DISPLAYFORM14 Now, note that the last summand in (50) corresponds to the summand in (18) for k = j \u2212 1. This completes the proof.Proof of Lemma 3.7.Proof. First note that from (19) it follows that \u03b7 t \u03b2H \u2264 0.1 .Since \u03b7 i shrinks down, as i grows up, and \u03b7 t H + 1 > 1, (18) yields ||x i\u03b2+j,w \u2212 x i\u03b2+j,w || \u2264 (\u03b7 i\u03b2 H + 1) \u03b2 \u00b7 ||x i\u03b2,w \u2212 x i\u03b2,w || + \u03b7 i\u03b2 \u03b2\u22121 k=0 ||e i\u03b2+k,w \u2212 e i\u03b2+k,w || .After the expansion of (\u03b7 i\u03b2 H + 1) \u03b2 in (52) into Taylor sequence and applying a simple algebra DISPLAYFORM15 From FORMULA1 we can bound DISPLAYFORM16 Assigning FORMULA4 into FORMULA2 , ||x i\u03b2+j,w \u2212 x i\u03b2+j,w || \u2264 1.2 \u00b7 ||x i\u03b2,w \u2212 x i\u03b2,w || + 1.2 \u00b7 \u03b7 i\u03b2 \u03b2\u22121 k=0 ||e i\u03b2+k,w \u2212 e i\u03b2+k,w || .Since e i\u03c4 +k,w and e i\u03c4 +k,w are i.i.d. with mean 0, E||e i\u03b2+k,w \u2212 e i\u03b2+k,w || = var(e i\u03b2+k,w \u2212 e i\u03b2+k,w ) = 2var(e i\u03b2+k,w ) .Taking expectation of the both sides of (55), substituting (56) into the resulting inequality and using the assumption on variance of random variables e t,w , we complete the proof.Proof of Lemma 3.8.Proof. From Lemma 3.3, ||x i0\u03b2,w \u2212 x i0\u03b2,w || \u2264 2L\u03c4 \u03b7 (i0\u22121)\u03b2 .From FORMULA2 , each cycle j after iteration i 0 \u03b2, reduces the distance between w and w by factor of 0.2, while adding to the distance the value of 1.2 \u00b7 \u03b7 i\u03b2+j \u03c4 s. This means that after the number of cycles j 0 j 0 = log 1.2 \u00b7 \u03b7 i0\u03b2 \u03c4 s 2L\u03c4 \u03b7 (i0\u22121)\u03b2 = log s L , for any i \u2265 i 0 + j 0 , the first summand in (22) gets bounded by 0.8 \u00b7 \u03b7 i\u03b2 \u03c4 s.Proof of Theorem 3.9.Proof. To prove this theorem , we follow the proof of Theorem 3.5 and develop an alternative bound on ||x t,w \u2212 x t || in (42).We will split the sum in (40) into two ranges of t: t < t 0 + j 0 \u03b2 and t \u2265 t 0 + j 0 \u03b2 for t 0 , j 0 , defined in FORMULA1 and FORMULA2 respectively. To simplify notation, from (23), we can assume that j 0 is a constant and we can assume that t 0 + j 0 \u2264 2t 0 . For t < 2t 0 , we use (48) to show 2t0\u22121 t=1 ||x t \u2212 x * || \u00b7 ||g t \u2212g t || \u2264 2\u03c4 F LH(2\u03c4 + 2\u03c3 DISPLAYFORM17 For t \u2265 2t 0 , we first observe DISPLAYFORM18 Next we use (24) to bound the above expression.E||x t,w \u2212 x t || \u2264 2\u03b7 t\u2212\u03c4 \u03c4 s .Substituting this bound into (42), we get E||g t \u2212g t || \u2264 2H\u03b7 t\u2212\u03c4 \u03c4 s .Using FORMULA1 Combining FORMULA5 and FORMULA2 in FORMULA4 and using the definition (19) of t 0 DISPLAYFORM19 Using the assumption (25), and substituting into (41) we prove (26). Finally we use value \u03c3 = F L to prove (27)."
}