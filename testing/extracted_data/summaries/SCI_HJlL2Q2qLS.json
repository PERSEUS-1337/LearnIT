{
    "title": "HJlL2Q2qLS",
    "content": "Bayesian inference is used extensively to infer and to quantify the uncertainty in a field of interest from a measurement of a related field when the two are linked by a mathematical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and/or have prior distributions that are difficult to characterize mathematically. In this work we demonstrate how the approximate distribution learned by a generative adversarial network (GAN) may be used as a prior in a Bayesian update to address both these challenges. We demonstrate the efficacy of this approach by inferring and quantifying uncertainty in a physics-based inverse problem and an inverse problem arising in computer vision. In this latter example, we also demonstrate how the knowledge of the spatial variation of uncertainty may be used to select an optimal strategy of placing the sensors (i.e. taking measurements), where information about the image is revealed one sub-region at a time. Bayesian inference is a principled approach to quntify uncertainty in inverse problems that are constrained by mathematical model (Kaipio and Somersalo [2006] , Dashti and Stuart [2016] , Polpo et al. [2018] ). It has found applications in diverse fields such as geophysics (Gouveia and Scales [1997] , Martin et al. [2012] , Isaac et al. [2015] ), climate modeling (Jackson et al. [2004] ), chemical kinetics ), heat conduction (Wang and Zabaras [2004] ), astrophysics (Loredo [1990] , Asensio Ramos et al. [2007] ), materials modeling (Sabin et al. [2000] ) and the detection and diagnosis of disease (Siltanen et al. [2003] , Kolehmainen et al. [2006] ). The two critical ingredients of a Bayesian inference problem are -an informative prior representing the prior belief about the parameters and an efficient method for sampling from the posterior distribution. In this manuscript we describe how a deep generative model (generative adversarial networks (GANs)) can be used in these roles. In a typical inverse problem, we wish to infer a vector of parameters x \u2208 R N from the measurement of a related vector y \u2208 R P , where the two are related through a forward model y = f (x). A noisy measurement of y is denoted by\u0177 = f (x) + \u03b7, where \u03b7 \u2208 R P represents noise. While the forward map is typically well-posed, its inverse is not, and hence to infer x from the measurement\u0177 requires techniques that account for this ill-posedness. Classical techniques based on regularization tackle this ill-posedness by using additional information about the sought parameter field explicitly or implicitly (Tarantola [2005] ). Bayesian inference offers a different solution to this problem by modeling the unknown parameter and the measurements as random variables and allows for the characterization of the uncertainty in the inferred parameter field. For additive noise, the posterior distribution of x, determined using Bayes' theorem after accounting for the observation\u0177 is given by where Z is the prior-predictive distribution of y, p prior X (x) is the prior distribution of x, and p l (y|x) is the likelihood, often determined by the distribution of the error in the model, denoted by p \u03b7 . Despite its numerous applications, Bayesian inference faces significant challenges. These include constructing a reliable and informative prior distribution from a collection of prior measurements denoted by the S = {x (1) , \u00b7 \u00b7 \u00b7 , x (S) }, and efficiently sampling from the posterior distribution when the dimension of x is large. In this work we consider the use of GANs (Goodfellow et al. [2014] ) in addressing these challenges. These networks are useful in this role because of (a) they are able to generate samples of x from p gen X (x) while ensuring closeness (in an appropriate measure) between p gen X (x) and the true distribution, and (b) because they accomplish this by sampling from the much simpler distribution of the latent vector z, whose dimension is much smaller than that of x. Related work and our contribution: The main idea in this work involves training a GAN using the sample set S, and then using the distribution learned by the GAN as the prior distribution in Bayesian inference. This leads to a useful method for representing complex prior distributions and an efficient approach for sampling from the posterior distribution in terms of the latent vector z. The solution of inverse problems using sample-based priors has a rich history (see Vauhkonen et al. [1997] , Calvetti and Somersalo [2005] for example). As does the idea of dimension reduction in parameter space , Lieberman et al. [2010] ). However, the use of GANs in these tasks is novel. Recently, a number of authors have considered the use machine learning-based methods for solving inverse problems. These include the use of convolutional neural networks (CNNs) to solve physics-driven inverse problems (Adler and \u00d6ktem [2017] , Jin et al. [2017] , Patel et al. [2019] ), and GANs to solve problems in computer vision (Chang et al., Kupyn et al. [2018] , Yang et al. [2018] , Ledig et al., Anirudh et al. [2018] , Isola et al. [2016] , Zhu et al. [2017] , Kim et al. [2017] ). There is also a growing body of work on using GANs to learn regularizers in inverse problems (Lunz et al. [2018] ) and in compressed sensing (Bora et al. [2017 (Bora et al. [ , 2018 , Kabkab et al. [2018] , Wu et al. [2019] , Shah and Hegde [2018] ). However, these approaches differ from ours in that they solve the inverse problem as an optimization problem and do not quantify uncertainty in a Bayesian framework . More recently, the approach described in (Adler and \u00d6ktem [2018] ) utilizes GANs in a Bayesian setting; however the GAN is trained to approximate the posterior distribution, and training is done in a supervised fashion with paired samples of the measurement\u0177 and the corresponding true solution x."
}