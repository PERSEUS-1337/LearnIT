{
    "title": "B1lMMx1CW",
    "content": "We present a personalized recommender system using neural network for recommending\n products, such as eBooks, audio-books, Mobile Apps, Video and Music.\n It produces recommendations based on customer\u2019s implicit feedback history such\n as purchases, listens or watches. Our key contribution is to formulate recommendation\n problem as a model that encodes historical behavior to predict the future\n behavior using soft data split, combining predictor and auto-encoder models. We\n introduce convolutional layer for learning the importance (time decay) of the purchases\n depending on their purchase date and demonstrate that the shape of the time\n decay function can be well approximated by a parametrical function. We present\n offline experimental results showing that neural networks with two hidden layers\n can capture seasonality changes, and at the same time outperform other modeling\n techniques, including our recommender in production. Most importantly, we\n demonstrate that our model can be scaled to all digital categories, and we observe\n significant improvements in an online A/B test. We also discuss key enhancements\n to the neural network model and describe our production pipeline. Finally\n we open-sourced our deep learning library which supports multi-gpu model parallel\n training. This is an important feature in building neural network based recommenders\n with large dimensionality of input and output data. Recently, deep learning based recommender systems gained significant attention by outperforming conventional approaches BID36 . It shows promising results on products like videos BID6 , mobile apps BID5 , music BID31 etc.In the papers mentioned above, we noticed that NN based recommenders are different for each product category (videos, music, mobile apps), requiring unique feature extraction methods and NN topologies. All of these challenges makes it harder to scale over different product categories. In this paper we are exploring effectiveness of a multilayer neural network (NN) for personalized recommendations of products which were never purchased before by a customer. The simplicity of this approach allows us to scale it on various categories of Amazon catalog in production. We focus on improving accuracy of the neural network based personalized recommender.It is noticed in BID6 ) that accuracy of NN depends on how the problem is formulated. They found that NN performs better when it is trained to predict the user's next purchase, rather than a set of randomly held-out purchases. We use the same idea, but on top of this, we propose to train NN model to predict not only future purchase, but all future purchases in the certain time (for example in the next week).Capturing temporal popularity (trendiness) also called seasonal changes of consumption pattern is a challenging and important problem in recommender systems BID33 , which can impact the accuracy of the model over time. In BID33 BID19 authors propose methods to capture seasonality changes using sequence modeling. Another approach BID29 ) models both long-term static and short-term temporal user preferences. In both cases they use different versions of recurrent neural networks. In this paper we propose to combine predictor model (which can captures short term preferences and recommend products which are currently popular) with auto-encoder model (which can capture static customer preferences and recommend products which were popular at any time in the past) using feed forward NN. These models are combined by training them jointly. We re-train NN model every day to learn new popular products and changes in customer preferences. Even though our model is simpler then RNN, we show that it captures seasonality changes well.Improving NN based recommender is important problem, for example in BID6 authors observed that adding features and depth significantly improves precision on holdout data from YouTube catalog. In BID5 authors show that wide and deep NN with multiple features can improve performance of the neural network on mobile apps. So both methods BID5 and BID6 ) require different production pipelines for different data sets: video and mobile apps. In this paper we use only purchase history and focus on improvements of NN accuracy by applying different splits of the training data. It simplifies the production pipeline and allows us to reuse it on all digital categories: video, eBooks, audio-books, mobile apps, and music.Another way of improving recommender system is time decay, which was introduced by BID35 for collaborative filtering. We also apply it on input data for the neural network based recommender and observe positive impact on accuracy metrics. Our contribution is to use convolutional layer BID20 for estimating the shape of time decay function. Convolutional layers are used in existing recommender systems, but it is applied for different purposes, for example in BID15 convolutional layer is used for learning local relation between adjacent songs, in BID37 BID18 it is used for text feature extraction and in BID31 it is used for extracting features from audio signal.There can be millions of products in the catalog and it is a hard problem to run NN based recommender in production with such amount of items BID6 . Both BID6 and BID5 are splitting the problem into candidate generation and ranking. Candidate generation retrieves a small subset of products from a large corpus. These candidates should be relevant to customer interest. Ranking does a fine-level scoring of the candidates and in addition to consumption history it can use more features (context, impression, etc) . Another way of scaling this problem is to learn similarity between products using DSSM approach BID8 which is relying on content features. In this paper we focused on training end-to-end one neural network which is using only purchases events. On one hand it simplifies the production pipeline, because there is no splitting into candidate generation and ranking models and there is no special feature extraction step. But on the other hand we have to deal with large dimensionality of input features and labels. To solve this problem we use multi-GPU model parallelization, implemented by our team in DSSTNE library (10). It allows us to re-train large NN models every day and produce fresh recommendations for our customers.In this paper, we are focused on modeling consumption patterns in digital products (For example, recommending movies to customers based on the movies already purchased). Depending on the domain, we also exclude movies that were already purchased by the customer while computing offline metrics as well as recommending online. We present different methods of splitting the training data and observed that it can improve NN based recommender accuracy metrics. Techniques like the one presented here feed into recommendation technology deployed at Amazon.The rest of the paper is organized as follows. Section 2 introduces offline metrics used for algorithm evaluation . Section 3 details our NN model development procedure, including how different methods are compared. Section 4 provides extensive offline evaluation results, in conjunction with model property exploration. Section 5 presents how to run NN model in production and describes on-line A/B test. Finally, section 6 presents our conclusions. We described a personalized neural network based recommender system which was launched in production on categories like eBooks, Audible, Apps and Video. We are currently working on expand-ing it to non-digital categories. We showed that splitting customer purchases into a history period (input) and a future period (output) in our models led to good results, and some of our production models use this approach (with soft split which combines the auto-encoder model with the future predictor model). We have applied time decay learnt by convolutional layer, or defined by parametrical function to consumption event. It captures the importance of recent activity, and combined with soft split, it leads to significant improvements in offline metrics. We demonstrated that two layer neural networks are outperforming other NN based approaches which are more complicated than our method, both on public dataset (MovieLens) and company's internal datasets. Because of simplicity of the NN model we can scale it on all digital categories. We observed significant KPI improvements during online A/B tests in production. Finally we open sourced our deep learning library which supports multi gpu model parallel training and allows us to train large models in timely manner.There are around 6200 products (movies) purchased (rated) by these customers in the above period of time. Distribution of customers sorted by number of purchases is shown on FIG0 , where H(c) number of purchases made by customer c, c customer index. It shows that 90 percent of the customers have less than 400 purchases. Distribution of products in data X and Y are shown on FIG0 (a), where P X(p), P Y (p) number of purchases of product p in the input (X) and output (Y ) training data accordingly, p product index. Both of these distributions have long tail: 90 percent of purchases are covered by 1000 products. During evaluation we feed XY data to the models and produce output scores\u0176 . These scores are sorted and the top K products are returned as recommendations. Before sorting, all previous purchases (products belonging to data XY ) of the selected customer are removed from the recommendations, so that only new products are recommended. These recommendations are compared with future purchases (data Z) for accuracy calculation. We get testing input data XY and testing output data Z by selecting customers who has at least two purchases in period of time dx . . . dyz and at least one purchase in period of time dyz . . . dz. There are 921 customers who satisfy these conditions. Purchases belonging to dates dx . . . dyz assigned to testing input data XY , and belonging to dyz . . . dz assigned to testing output data Z. Accuracy metrics of predictor, soft split and fastXML models are presented on FIG0 . Predictor model has the same PCC@K with soft split and lower precision@K. Both of these models have higher accuracy metrics than fastXML. We observe similar difference in precision between predictor model and fastXML on Figure. 3, but fastXML has higher PCC@K. These results can be used only as an approximation of a performance on real implicit feedbacks (purchase history), because in this section we were using ratings converted to implicit feedbacks."
}