{
    "title": "r1l7E1HFPH",
    "content": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems. The field of Reinforcement learning (RL) span a wide variety of algorithms for solving decisionmaking problems through repeated interaction with the environment. By incorporating deep neural networks into RL algorithms, the field of RL has recently witnessed remarkable empirical success (e.g., Mnih et al. 2015; Lillicrap et al. 2015; Silver et al. 2017 ). Much of this success had been achieved by model-free RL algorithms, such as Q-learning and policy gradient. These algorithms are known to suffer from high variance in their estimations (Greensmith et al., 2004) and to have difficulties handling function approximation (e.g., Thrun & Schwartz 1993; Baird 1995; Van Hasselt et al. 2016; Lu et al. 2018 ). These problems are intensified in decision problems with long horizon, i.e., when the discount factor, \u03b3, is large. Although using smaller values of \u03b3 addresses the \u03b3-dependent issues and leads to more stable algorithms (Petrik & Scherrer, 2009; Jiang et al., 2015) , it comes with a cost, as the algorithm may return a biased solution, i.e., it may not converge to an optimal solution of the original decision problem (the one with large value of \u03b3). Efroni et al. (2018a) recently proposed another approach to mitigate the \u03b3-dependant instabilities in RL in which they study a multi-step greedy versions of the well-known dynamic programming (DP) algorithms policy iteration (PI) and value iteration (VI) (Bertsekas & Tsitsiklis, 1996) . Efroni et al. (2018a) also proposed an alternative formulation of the multi-step greedy policy, called \u03ba-greedy policy, and studied the convergence of the resulted PI and VI algorithms: \u03ba-PI and \u03ba-VI. These two algorithms iteratively solve \u03b3\u03ba-discounted decision problems, whose reward has been shaped by the solution of the decision problem at the previous iteration. Unlike the biased solution obtained by solving the decision problem with a smaller value of \u03b3, by iteratively solving decision problems with a smaller \u03b3\u03ba horizon, the \u03ba-PI and \u03ba-VI algorithms could converge to an optimal policy of the original decision problem. In this work, we derive and empirically validate model-free deep RL (DRL) implementations of \u03ba-PI and \u03ba-VI. In these implementations, we use DQN (Mnih et al., 2015) and TRPO (Schulman et al., 2015) for (approximately) solving \u03b3\u03ba-discounted decision problems (with shaped reward), which is the main component of the \u03ba-PI and \u03ba-VI algorithms. The experiments illustrate the performance of model-free algorithms can be improved by using them as solvers of multi-step greedy PI and VI schemes, as well as emphasize important implementation details while doing so. In this work we formulated and empirically tested simple generalizations of DQN and TRPO derived by the theory of multi-step DP and, specifically, of \u03ba-PI and \u03ba-VI algorithms. The empirical investigation reveals several points worth emphasizing. 1. \u03ba-PI is better than \u03ba-VI for the Atari domains.. In most of the experiments on the Atari domains \u03ba-PI-DQN has better performance than \u03ba-VI-DQN. This might be expected as the former uses extra information not used by the latter: \u03ba-PI estimates the value of current policy whereas \u03ba-VI ignores this information. 2 . For the Gym domains \u03ba-VI performs slightly better than \u03ba-PI. For the Gym domains \u03ba-VI-TRPO performs slightly better than \u03ba-PI-TRPO. We conjecture that the reason for the discrepancy relatively to the Atari domains lies in the inherent structure of the tasks of the Gym domains: they are inherently short horizon decision problems. For this reason, the problems can be solved with smaller discount factor (as empirically demonstrated in Section 5.3) and information on the policy's value is not needed. 3. Non trivial \u03ba value improves the performance. In the vast majority of our experiments both \u03ba-PI and \u03ba-VI improves over the performance of their vanilla counterparts (i.e., \u03ba = 1), except for the Swimmer and BeamRider domains from Mujoco and Atari suites. Importantly, the performance of the algorithms was shown to be 'smooth' in the parameter \u03ba. This suggests careful hyperparameter tuning of \u03ba is not of great necessity. 4. Using the 'naive' choice of N (\u03ba) = T deteriorates the performance. Choosing the number of iteration by Eq. 8 improves the performance on the tested domains. An interesting future work would be to test model-free algorithms which use other variants of greedy policies (Bertsekas & Tsitsiklis, 1996; Bertsekas, 2018; Efroni et al., 2018a; Sun et al., 2018; Shani et al., 2019) . Furthermore, and although in this work we focused on model-free DRL, it is arguably more natural to use multi-step DP in model-based DRL (e.g., Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019) . Taking this approach, the multi-step greedy policy would be solved with an approximate model. We conjecture that in this case one may set \u03ba -or more generally, the planning horizon -as a function of the approximate model's 'quality': as the approximate model gets closer to the real model larger \u03ba can be used. We leave investigating such relation in theory and practice to future work. Lastly, an important next step in continuation to our work is to study algorithms with an adaptive \u03ba parameter. This, we believe, would greatly improve the resulting methods, and possibly be done by studying the relation between the different approximation errors (i.e., errors in gradient and value estimation, Ilyas et al., 2018) , the performance and the \u03ba value that should be used by the algorithm. A DQN IMPLEMENTATION OF \u03ba-PI AND \u03ba-VI"
}