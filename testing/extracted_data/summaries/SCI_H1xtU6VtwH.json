{
    "title": "H1xtU6VtwH",
    "content": "Chinese text classification has received more and more attention today. However, the problem of Chinese text representation still hinders the improvement of Chinese text classification, especially the polyphone and the homophone in social media. To cope with it effectively, we propose a new structure, the Extractor, based on attention mechanisms and design novel attention networks named Extractor-attention network (EAN). Unlike most of previous works, EAN uses a combination of a word encoder and a Pinyin character encoder instead of a single encoder. It improves the capability of Chinese text representation. Moreover, compared with the hybrid encoder methods, EAN has more complex combination architecture and more reducing parameters structures. Thus, EAN can take advantage of a large amount of information that comes from multi-inputs and alleviates efficiency issues. The proposed model achieves the state of the art results on 5 large datasets for Chinese text classification. Recently, Chinese text classification, as an important task of Chinese natural language processing (NLP), is extensively applied in many fields. Deep learning has gotten great results on Chinese text classification. However, the relevant studies are still insufficient compared with English, especially the method of Chinese text representation or encoding. It is considered to be closely related to the result of Chinese text classification models. Specifically, there are some issues in previous representation methods: (i) The word embedding (Le & Mikolov (2014) ; Mikolov et al. (2013) ; Pennington et al. (2014) ) is the most common method to represent the text, but it may become less effective when processing texts with the ambiguous word boundary such as Chinese texts. (ii) The character embedding (Zhang et al. (2015) ) can avoid the word segment. However, using Pinyin characters loses the ideographic ability of Chinese characters, and using Chinese characters requires more training data because there are thousands of Chinese characters that are often used in daily life. (iii) Both the word embedding and the Chinese character embedding are hard to encode some intricate Chinese language phenomena about pronunciations, such as the polyphone and the homophone. We notice that humans have associated the word or character with the corresponding pronunciation and remembered them in the process of learning the language. Thus, when humans read texts in daily life, they spontaneously associate with the corresponding voices. It is very difficult for computers and usually ignored by traditional text classification method. Moreover, using the voice can cope with some representation issues of Chinese characters or words better, The polyphone and the homophone are 2 typical examples. The former means different pronunciations and meanings are from the same character, and the latter means the same pronunciations are from different characters, which are usually used to represent similar meanings in social media. And inspired by recent multimedia domain methods (Gu et al. (2018) ), the extra audio information can obtain better results. However, large amounts of corresponding audio data are required difficultly. Pinyin can precisely express the pronunciation by no more than 6 letters and is easily generated from texts, and it also solves representation issues of Chinese characters or words. There are some typical examples that illustrate these points in detail. Table 1 shows an example (sentence1) of the homophone of social medias.There is a homophone \"\u9e2d\u68a8\u5c71\u5927\" , the pronunciation w\u01d2 zh\u01d0 n\u00e9ng shu\u014d d\u014dng x\u012b sh\u00ec h\u01ceo d\u014dng x\u012b\uff0c1 h\u00e0o de d\u00ecng d\u0101n\uff0c6 h\u00e0o c\u00e1i d\u00e0o\uff0cd\u00ecng d\u0101n sh\u00ec li\u01ceng ji\u00e0n\uff0cy\u012b ji\u00e0n y\u00f9n d\u00e1\uff0cy\u012b ji\u00e0n zh\u014dng t\u014dng\uff0cy\u00f9n d\u00e1 2 ti\u0101n d\u00e0o\uff0czh\u014dng t\u014dng 6 ti\u0101n d\u00e0o\uff0csh\u0101ng ji\u0101 w\u00e1n f\u0113n k\u0101i s\u00f2ng\uff0czh\u0113n sh\u00ec r\u00e0ng m\u01cei ji\u0101 y\u0101 l\u00ed sh\u0101n d\u00e0\uff01 sentence2: \u5927\u5b66\u82f1\u8bed\u516d\u7ea7\u8003\u8bd5\uff1a\u4f18\u9009\u771f\u9898 \u6807\u51c6\u6a21\u62df \u6ca1\u6709\u738b\u957f\u559c\u597d \u597d \u597d \u7528\uff0c\u540e\u6094\u4e86 d\u00e0 xu\u00e9 y\u012bng y\u01d4 li\u00f9 j\u00ed k\u01ceo sh\u00ec\uff1ay\u014du xu\u01cen zh\u0113n t\u00ed bi\u0101o zh\u01d4n m\u00f3 n\u01d0 m\u00e9i y\u01d2u w\u00e1ng zh\u01ceng x\u01d0 h\u01ceo y\u00f2ng\uff0ch\u00f2u hu\u01d0 le sentence3: \u6709\u4e00\u70b9\u70b9\u5c0f(\u6211\u4e2a\u4eba\u7684\u559c\u597d \u597d \u597d)\uff0c\u52c9\u5f3a\u5427 y\u01d2u y\u012b di\u01cen di\u01cen xi\u01ceo (w\u01d2 g\u00e8 r\u00e9n de x\u01d0 h\u00e0o )\uff0cmi\u01cen qi\u01ceng ba (Pinyin) and the meaning of which are the same as \"\u538b\u529b\u5c71\u5927\". Table 1 also shows some examples (sentence2 and sentence3) of the polyphone of social medias. The pronunciation (Pinyin) and the meaning of \"\u597d\" are different in two sentences. Besides,\"h\u00e0o\" can represent \"\u597d\" in sentence3 or \"\u53f7\" in sentence1. In fact, it can represent the pronunciation of dozens of Chinese characters. By those examples, we foucs on some points: In Chinese texts, some intricate language phenomena about pronunciations relatively easier to be recognized by a simple Pinyin encoder than by a complex Chinese character or word encoder. And most of language phenomena about glyph are the opposite. Based on the above points, we propose a new hybrid encoder (including word encoder and Pinyin character encoder) network to obtain better results for Chinese text classification, we call it Extractor-attention network (EAN). Inspired by Transformer (Vaswani et al. (2017) ), we also propose a new structure named the Extractor. The Extractor includes a multi-head self-attention mechanism with separable convolution layers (Chollet (2017) ). In EAN, the Extractor is used to encode the information of Pinyin. Besides, it is repeatedly used to combine word encoder with Pinyin encoder. Compared with previous hybrid encoder methods, our method has relatively simple encoders and a complicated combination part, which uses a deep self-attention mechanism. It makes EAN assign weights between features extracting by each encoder more accurately and avoid huge feature maps. Moreover, we use pooling layers for downsampling and separable convolution layers to compress parameters. Therefore, the Extractor network represent the Chinese text well, improve the classification accuracy, and the computational cost is relatively cheap. The experimental results show that our model outperforms all baseline models on all datasets, and has fewer parameters in comparison to similar works. Our primary contributions (i) Inspired by human language learning and reading, we design a novel method to solve the text representation issue of Chinese text classification, especially the language phenomena about pronunciations such as the polyphone and the homophone. To the best of our knowledge, this is the first time that a hybrid encoding method including Pinyin has been used to solve those language phenomena expression problem. (ii) We propose a new attention architecture named the Extractor to experss Chinese texts information. Besides, to better represent Chinese texts, we design a new hybird encoder method EAN based on the Extractor. We also propose a complex attention method to combine word encoder with Pinyin encoder effectively, which can commendably balance the amount of information transmitted by 2 encoders. (iii) Our method is able to surpass previous methods. It can get the state of the art results on public datasets. This paper proposes a novel attention network, the Extractor-attention network (EAN), for Chinese text classification. Compared to the traditional Chinese text classification methods using only word encoder, our approach uses hybrid encoder including words and Pinyin characters, which takes full advantage of the extra Pinyin information to improve the performance. Moreover, there is a new structure named the Extractor in our work, reduces the number of parameters in EAN and makes it excellent to extract feature. Thus, EAN obtains the state of the art results on 5 public Chinese text classification datasets. Finally, we also analyze the effects of different encoders structures on the method."
}