{
    "title": "rygFWAEFwS",
    "content": "We propose Stochastic Weight Averaging in Parallel (SWAP) to accelerate DNN training by using large mini-batches for quick approximate solutions. The algorithm refines the solution by averaging weights from multiple models computed independently and in parallel. This approach results in models that generalize well and are produced in a significantly shorter time. The effectiveness of SWAP is demonstrated on computer vision datasets CIFAR10, CIFAR100, and ImageNet, showing reduced training time and good generalization performance. Training with larger mini-batches accelerates DNN training by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss. However, there is a maximum batch size that can lead to worse generalization performance. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging the weights of models sampled from the final stages of training. Generating multiple independent SGD sequences and averaging models from each can achieve similar generalization performance. Starting with a model trained with large-batches and using small-batches in independent sequences can result in comparable generalization performance. Stochastic Weight Averaging in Parallel (SWAP) is a simple strategy to accelerate DNN training by better utilizing compute resources. It achieves generalization performance comparable to models trained with small-batches but in a similar time frame to large-batch training. SWAP has been successfully applied to image classification tasks on popular datasets like CIFAR10, CIFAR100, and ImageNet, reducing training times significantly and even outperforming the state of the art on CIFAR10. The training batch size affects generalization performance, with larger mini-batches potentially leading to sharper global minima. Different studies have shown conflicting results on the impact of batch size on model behavior. In (McCandlish et al., 2018), authors predict that increasing batch size up to a critical point maintains accuracy. They suggest that larger batch sizes result in mini-batch gradients close to full gradients, with no significant improvement beyond a certain size. In (Hoffer et al., 2017), authors argue that larger batch sizes lead to fewer model updates, impacting weight initialization distance and generalization performance. Training with large batches for longer periods can improve model generalization. The batch size affects model generalization and optimization process. Larger batch sizes may not improve convergence rate. Adaptive batch size methods exist but may require extensive tuning and inefficiently use computational resources. Post-local SGD is a distributed optimization algorithm that improves model generalization by allowing workers to update models independently before synchronizing. It refines large-batch training outputs and achieves significant speedups. In contrast, SWAP averages models after multiple epochs, resulting in better generalization in Imagenet experiments. In contrast to Post-local SGD, Stochastic Weight Averaging (SWA) involves averaging models from later stages of training to improve generalization. SWA has been effective in various domains and is adapted in this work to accelerate DNN training. SWAP is described as an algorithm in three phases for training models. In three phases (see Algorithm 1), workers train a single model collectively in the first phase with large mini-batch updates and synchronization. In the second phase, each worker refines their model independently with different weights, using smaller batch sizes and lower learning rates. The final phase involves averaging the weights of models and computing new batch-normalization statistics for the final output. Phase 1 ends before reaching zero loss or 100% accuracy to prevent optimization from getting stuck and improve generalization performance. During phase 2 of the training process, small-batch training is performed independently by each worker, resulting in different models being produced. The learning rates are the same for all workers, but their testing accuracies differ due to stochasticity causing model divergence. The averaged model's test accuracy is also plotted to show the difference from individual models. During phase 2, small-batch training is conducted independently by each worker, leading to diverging models with varying test accuracies. The averaged model consistently outperforms individual models. The mechanism behind SWAP is visualized by plotting the error on a plane containing outputs from different phases of the algorithm. During phase 2, small-batch training is conducted independently by each worker, leading to diverging models with varying test accuracies. The averaged model consistently outperforms individual models. The mechanism behind SWAP is visualized by plotting the error on a plane containing outputs from different phases of the algorithm. In Figure 2, the training and testing error for the CIFAR10 dataset are shown, with 'LB' marking the output of phase one, 'SGD' the output of a single worker after phase two, and 'SWAP' the final model. The level-sets of the training error form an almost convex basin, with 'LB' and 'SGD' lying on the outer edges, while 'SWAP' is closer to the center. In the test loss landscape, variations in basin topology affect 'LB' and 'SGD' points, leading to higher errors. However, 'SWAP' is less affected due to its central position. Worker points in Figure 3 are shown to have higher testing errors compared to 'SWAP', which remains close to the center. The weight iterates in SGD behave similarly to an Ornstein Uhlenbeck process in later stages. The weight iterates in SGD behave like an Ornstein Uhlenbeck process, reaching a high-dimensional Gaussian distribution centered at the local minimum. The distribution's covariance grows with the learning rate and is inversely proportional to the batch size. Sampling weights from different SGD runs results in weights spread out on the surface of an ellipsoid, closer to the center. Sampling from different SGD runs during SWAP can be justified if all runs start in the same basin of attraction. According to the model from (Mandt et al., 2017), all runs will converge to the same stationary distribution, allowing each run to generate independent samples. The advantage of SWA and SWAP over SGD is demonstrated by measuring the cosine similarity between the gradient descent direction and the direction towards the output of SWAP. As training progresses, the cosine similarity decreases, indicating that progress slows as the angle between the gradient direction and the basin center increases. Averaging samples from different sides of the basin can help overcome this issue. In this section, the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated. Best hyper-parameters were found using grid searches, training was done using mini-batch SGD with Nesterov momentum and weight decay. Data augmentation was done using cutout, and a custom ResNet 9 model was used. Experiments were run on one machine with 8 NVIDIA Tesla V100 GPUs using Horovod for distribution. Statistics were collected over 10 runs with specific settings for SWAP phase one. The study evaluates the performance of SWAP for image classification on CIFAR10, CIFAR100, and ImageNet datasets. Different batch sizes and GPU configurations were tested, with SWAP phase one using 8 GPUs and phase two using 8 workers with one GPU each. Results show comparisons of test accuracies and training times for models trained with small-batch only, large-batch only, and SWAP. Using SWAP for image classification on CIFAR10 and CIFAR100 datasets resulted in significant improvements in test accuracies. Training with small-batches achieved higher accuracy but took longer, while SWAP terminated in comparable time to large-batch training with similar or better accuracies. SWAP also achieved state-of-the-art training speeds for CIFAR10, reaching 94% test accuracy in 27 seconds with 8 Tesla V100 GPUs. Using SWAP to accelerate a fast-to-train ImageNet model, we conducted small-batch experiments on 8 Tesla V100 GPUs and large-batch experiments on 16 Tesla V100 GPUs. Doubling the batch size reduced test accuracies, but SWAP recovered generalization performance. SWAP phase 1 used large-batch settings for 22 epochs, while phase 2 ran two independent workers with 8 GPUs each using small-batch settings for 6 epochs. The study compared the performance of SWAP and SWA algorithms on the CIFAR100 dataset. SWAP recovered generalization performance with reduced training times by adjusting learning rates and batch sizes. Top1 and Top5 accuracies were reported for small-batch SGD and SWAP experiments. For experiments on CIFAR100 dataset, SWA and SWAP were compared. SWA sampled models with 10 epochs in-between and averaged them, while SWAP used 8 workers for 10 epochs each. SWA did not improve large-batch training accuracy. After comparing SWA and SWAP on the CIFAR100 dataset, it was found that SWA did not improve large-batch training accuracy. Small-batch SWA was evaluated by interrupting the large-batch phase and using a single worker for sequential model sampling. SWA reached test accuracy similar to small-batch runs but took more than three times longer to compute the model. The learning rate schedule was illustrated in Figure 6b. The SWA cyclic learning rate schedule started from the best model found through small-batch training, with the peak learning rate selected through grid-search. The peak learning rate settings were then reused in SWAP, starting phase two from the generated model. SWAP achieves better accuracy than SWA with longer training time. By relaxing constraints, SWAP achieves a speed-up over SWA with a test accuracy of 79.11% in 241 seconds. SWAP is an algorithm that uses a variant of Stochastic Weight Averaging to improve model generalization performance with large mini-batches. The algorithm uses large mini-batches to quickly compute an approximate solution and then refines it by averaging weights of models trained with small-batches. The final model has good generalization performance and is trained in a shorter time. This approach is novel and effective, as confirmed in image classification datasets. Visualizations show that averaged weights are closer to the center of a training loss basin. The large mini-batch algorithm converges to a basin where refined models are found, indicating a connection between regions of good and bad generalization performance. The method requires choosing a transition point between large and small batches, determined through grid search. Future work will focus on a principled method for selecting this transition point and exploring SWAP with other optimization schemes. SWAP allows for the substitution of different optimization schemes like mixed-precision training, post-local SGD, or NovoGrad. Parameters for experiments were obtained through independent grid searches. Momentum and weight decay constants were kept constant for CIFAR experiments. Tables 5 and 6 list the remaining hyperparameters used. A stopping accuracy of 100% indicates the maximum number of epochs were used."
}