{
    "title": "rylwJxrYDS",
    "content": "The proposed vq-wav2vec algorithm learns discrete representations of audio segments through a self-supervised context prediction task. It uses gumbel softmax or online k-means clustering for quantization. Experiments show BERT pre-training achieves state-of-the-art results on TIMIT phoneme classification and WSJ speech recognition. Learning discrete speech representations has gained recent interest, with approaches like autoencoding and self-supervised learning of continuous speech representations. In this paper, the vq-wav2vec algorithm learns discrete representations of speech through a context prediction task. It maps raw audio to dense quantized representations and aggregates them into context representations for training acoustic models. The algorithm utilizes the wav2vec loss and architecture to achieve fixed length segments of audio signal. The vq-wav2vec algorithm learns discrete representations of speech using the wav2vec loss and architecture. It utilizes Gumbel-Softmax and online k-means clustering for choosing discrete variables. Training a Deep Bidirectional Transformer (BERT) on the discretized speech data improves performance on TIMIT and WSJ benchmarks. Discretization enables the application of NLP algorithms to speech data, such as using a sequence to sequence model for speech recognition. The WAV2VEC model learns audio representations through a self-supervised context-prediction task with a contrastive loss function. It uses convolutional neural networks to create representations for each time step and aggregates them to distinguish future samples. The model is optimized by minimizing the contrastive loss for different time steps. Our approach, vq-wav2vec, utilizes a step-specific affine transformation and optimizes loss by summing over different step sizes. The representations produced by the context network are input to the acoustic model instead of log-mel filterbank features. BERT is a pre-training approach for NLP tasks that uses a transformer encoder model with self-attention. The original BERT model combines masked language modeling and next sentence prediction tasks for training. Our approach, vq-wav2vec, utilizes a quantization module to build discrete representations of audio data for a future time-step prediction task. The encoder network maps raw speech segments to dense feature representations, which are then quantized into discrete indices and reconstructed. The aggregator network optimizes a context prediction task using these discrete representations from a fixed size codebook. The Gumbel-Softmax is used for computing one-hot representations and online k-means clustering in the vq-wav2vec approach. Multiple vector quantizations are performed on different parts of z to prevent mode collapse. The Gumbel-Softmax enables selecting discrete codebook variables in a differentiable way, with a linear layer and ReLU applied to the dense representation z. The output probabilities for variable selection are determined using uniform samples during training. The Gumbel-Softmax method is utilized for selecting discrete codebook variables in a differentiable manner. The approach involves optimizing a future time step prediction loss instead of an autoencoder reconstruction loss. Gradients for the encoder network are obtained by back-propagating the Gumbel-Softmax outputs. The final loss includes additional terms with a stop gradient operator and a hyper-parameter. The Gumbel-Softmax method is used for selecting codebook variables in a differentiable way by optimizing future time step prediction loss. Gradients for the encoder network are obtained through back-propagation of Gumbel-Softmax outputs. The final loss includes additional terms with a stop gradient operator and a hyper-parameter, along with strategies to mitigate mode collapse in codebook usage. The dense feature vector is organized into groups and represented by integer indices. Two possible ways to initialize the codebook are shared variables across groups or not sharing them. Sharing the codebook variables yields a smaller codebook size, while not sharing them results in a larger codebook size. In practice, sharing the codebook is observed to be effective. After training a vq-wav2vec model, audio data can be discretized for algorithms requiring discrete inputs. Using the discretized data, BERT pre-training can be applied to predict masked input tokens based on context. This trained BERT model can then improve speech recognition by feeding representations into an acoustic model. To enhance BERT training, consecutive spans of discretized speech tokens are masked instead of single tokens. Training involves masking consecutive spans of discretized speech tokens to improve accuracy. Models are pre-trained on Librispeech data and evaluated on TIMIT and Wall Street Journal datasets for speech recognition. The evaluation protocol involves training acoustic models on 31 graphemes, using vqwav2vec/wav2vec models with 34 \u00d7 10 6 parameters. The encoder has 8 layers with 512 channels each, and the aggregator has 12 layers with skip connections. Training is done with the wav2vec context prediction loss for 400k updates. The training process involves using the wav2vec context prediction loss for 400k updates, with a batch size of 10 and a random section of 150,000 frames for each example. Models are trained on 8 GPUs, with a smaller model used for ablations and experiments on the 100h Librispeech subset. Gumbel-Softmax Models are utilized with 2 groups and 320 latents per group. The linear layer in the model projects encoder features into 640 logits. The Gumbel-Softmax produces one-hot vectors for each group, with the temperature annealed from 2 to 0.5 over 70% of updates. After training on 960h of Librispeech, 13.5k unique codeword combinations are left. Using k-means models with 2 groups and 320 variables per group, vq-wav2vec yields 23k unique codewords. BERT base models have specific dimensions and attention heads. The learning rate is warmed up over the first 10,000 updates to a peak value of 1 \u00d7 10 \u22125, and then linearly decayed over a total of 250k updates. Training is done on 128 GPUs with a batch size of 3072 tokens per GPU. BERT small models are used for ablations with a smaller setup. Wav2letter is used as the acoustic model and trained for 1k epochs on 8 GPUs for both TIMIT and WSJ. Decoding on WSJ involves using a lexicon and a separate language model. The study compares different models for speech recognition, including vq-wav2vec and BERT, trained on Librispeech data. They also evaluate wav2letter models with different language models on the WSJ benchmark. The study compares different models for speech recognition, including vq-wav2vec and BERT, trained on Librispeech data. Table 1 shows that vq-wav2vec with BERT training achieves a new state of the art of 2.34 WER on nov92. Gumbel-Softmax with a limited set of codewords enables training BERT models with a small vocabulary. Comparisons are made between Gumbel-Softmax and k-means for vector quantization, with varying codeword sizes to test model expressiveness. Table 3 presents TIMIT phoneme recognition results in terms of phoneme error rate (PER) for various models, including Li-GRU + fMLLR, wav2vec, Baseline (log-mel), vq-wav2vec with Gumbel and BERT small, and vq-wav2vec with k-means and BERT small. The CNN-8L-PReLU-do0.7 architecture was used for all models. Table 4 shows Librispeech results for a standard sequence to sequence model trained on discretized audio without BERT pre-training. Gumbel-Softmax and k-means clustering perform comparably, with Gumbel-Softmax being more accurate in the no language model setup, but differences disappear with BERT. K-means performs better with a 4-gram LM setup, but differences disappear after BERT training. The large codeword model can significantly reduce the gap between models. After training BERT on discretized speech using vq-wav2vec, a new state-of-the-art phoneme error rate of 11.67 PER was achieved, representing a 21% error reduction compared to wav2vec. Additionally, preliminary experiments with a Big Transformer model on the vq-wav2vec Gumbel-Softmax discretized Librispeech corpus showed promising results, although not as good as the current state of the art. The study explores the compression capabilities of vq-wav2vec by training models with varying numbers of groups and variables to measure accuracy on phoneme recognition. Different bitrate ranges are tested, from 0.53 kbit/s to 33.03 kbit/s, using k-means with different group sizes. The quantization module is placed after the aggregator module, and all models are trained on the 100h clean Librispeech subset. The study evaluates the compression capabilities of vq-wav2vec models on the 100h clean Librispeech subset using various lossy compression algorithms. Results show that acoustic models on vq-wav2vec outperform other codecs across different bitrate settings. Masking entire spans of tokens yields better results than individual tokens, and BERT training on discretized audio data is robust to input masking. vq-wav2vec is a self-supervised algorithm that quantizes audio data for algorithms requiring discrete inputs, improving performance on benchmarks by leveraging BERT pre-training. Future work includes exploring other algorithms for audio data, finetuning models for transcriptions, and studying the relationship between variables and groups. Multiple groups are beneficial compared to a single group with many variables, as shown in Tables 6 and 7. PER on TIMIT dev set for vq-wav2vec models trained on Libri100, based on three random seeds."
}