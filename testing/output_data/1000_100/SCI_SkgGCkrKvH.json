{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, such as Choco-SGD, enables data privacy, on-device learning, and efficient scaling to large compute clusters. This approach achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. The algorithm's practical performance is demonstrated in training deep learning models over decentralized user devices and in datacenters. Distributed machine learning has been successful in research and industry, offering computational scalability. Decentralized training of deep learning models utilizes communication compression for data privacy and efficient scaling. Recent theoretical results show decentralized schemes can be as efficient as centralized approaches. Gradient compression techniques reduce data sent over communication links. Tang et al. (2018) introduce DCD and ECD algorithms for decentralized training of deep neural networks. CHOCO-SGD algorithm, introduced for convex problems by Koloskova et al. (2019), allows for high compression ratios and unbiased compressors. Unlike previous algorithms, it focuses on generalization performance on test sets rather than just training performance. The study evaluates the algorithm on challenging peer-to-peer settings with distributed training data. In a peer-to-peer setting, CHOCO-SGD shows speed-ups over decentralized baseline with less communication overhead. In a datacenter setting, it improves time-to-accuracy on large tasks like ImageNet training. However, decentralized algorithms struggle to match centralized performance with a larger number of nodes. CHOCO-SGD is the first to show convergence on non-convex smooth functions in decentralized training schemes. It achieves a linear speedup in the number of workers and presents a version with momentum for practical performance analysis. In decentralized training over a peer-to-peer social network, reducing bandwidth requirements is crucial. Various methods like decentralized schemes, gradient compression, and asynchronous methods have been proposed for training in communication-restricted settings. In this paper, the focus is on combining decentralized SGD schemes with gradient compression, particularly using gossip averaging methods. The convergence rate of these methods depends on the spectral gap of the mixing matrix. Previous work has shown convergence rates consistent with centralized mini-batch SGD, with the spectral gap affecting the convergence. Decentralized optimization with quantization and gradient compression has been popularized in the deep learning community. Theoretical guarantees have been established for schemes with unbiased compression, and recently extended to biased compression as well. Schemes with error correction show the best practical performance and theoretical guarantees. Proximal updates and variance reduction have also been studied in combination with quantized updates. The CHOCO-SGD algorithm can handle high compression rates and is introduced for decentralized optimization with quantization. Previous work has shown that gossip averaging may not converge with quantization noise, but adaptive schemes with increased compression accuracy can converge at a higher communication cost. Tang et al. proposed DCD and ECD algorithms for deep learning applications that converge at the same rate as the centralized baseline with a constant compression ratio. The decentralized optimization algorithm CHOCO-SGD introduced in (Koloskova et al., 2019) can handle high compression rates for non-convex functions. It outperforms DeepSqueeze in test accuracy under the same tuning conditions. The algorithm utilizes compression operators and gossip-based stochastic optimization for distributed optimization across n nodes. In a distributed setup across n nodes, optimization problems involve local data distributions and communication limited to local neighbors. The weights for communication links are based on node degrees. The goal is to transmit compressed messages, such as quantized or sparsified data. In a distributed setup across n nodes, optimization problems involve local data distributions and communication limited to local neighbors. The goal is to transmit compressed messages using compression operators. These operators, unlike quantization operators, do not need to be unbiased, allowing for a larger class of compression schemes. CHOCO-SGD algorithm is outlined in Algorithm 1, where each worker updates its private variable using stochastic gradient and gossip averaging steps to preserve averages of iterates even in the presence of quantization noise. The CHOCO-SGD algorithm involves nodes communicating with neighbors to update variables using compressed updates. Each node only needs to store 3 vectors at most, regardless of the number of neighbors. Communication and gradient computation can be executed in parallel. The CHOCO-SGD algorithm extends analysis to non-convex problems with bounded variance of stochastic gradients. It converges asymptotically with a linear speed-up compared to SGD on a single node. Compression and graph topology impact higher order terms. Refer to (Koloskova et al., 2019) for further details and proofs. In this section, CHOCO-SGD is experimentally compared to relevant baselines using commonly used compression operators. Momentum is leveraged in all algorithms, including the newly developed momentum version of CHOCO-SGD. The algorithm setup involves a ring topology with 8 nodes training the ResNet20 architecture on the Cifar10 dataset. We compare CHOCO-SGD with other compression algorithms like DCD, ECD, and DeepSqueeze on ResNet20 using momentum. The learning rate is fine-tuned and warmed up gradually. Compression is applied to each layer separately, and training stops at 300 epochs. Hyper-parameter tuning details are in Appendix F. Compression schemes are applied to every layer of ResNet20 separately, with evaluation done on each node over the dataset. Two unbiased compression schemes include quantization and sparsification, while biased schemes involve selecting weights based on magnitude or compressing to sign scaled by vector norm. DCD and ECD are only analyzed for unbiased quantization. Results show that unbiased compression schemes ECD and DCD perform well at low compression ratios but struggle at high ratios, consistent with previous theoretical and experimental findings. DCD performs better with biased sparsification compared to unbiased random sparsification. CHOCO-SGD achieves state-of-the-art accuracy with sign compression, requiring significantly fewer bits per weight than full precision. It can generalize well in all scenarios with a fixed training budget, even in challenging decentralized environments where each device has access only to local data and communication bandwidth is limited. In a fully decentralized setting, training data is split between nodes without shuffling, ensuring privacy. Prior works have not explored this scenario for decentralized deep learning. Centralized approaches like all-reduce are not efficient in this setting. In a fully decentralized setting, training data is split between nodes without shuffling to ensure privacy. Centralized approaches like all-reduce are not efficiently implementable in this scenario. The comparison includes CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Scaling properties are studied on 4, 16, 36, and 64 nodes using different network topologies. Learning rates are kept constant and separately tuned for all methods, with consensus learning rate tuned for CHOCO-SGD. Results are summarized in Figure 1. The comparison between centralized and decentralized algorithms for training data distribution shows that CHOCO-SGD slows down due to graph topology and communication compression. Despite increasing the number of epochs, the performance gap between centralized and decentralized algorithms remains. In a real decentralized scenario, the focus is on reducing communication to save mobile data costs. CHOCO-SGD performs best with slight degradation as nodes increase. Torus topology is beneficial for large node numbers due to good mixing properties. Both Decentralized and Centralized SGD require more bits for reasonable accuracy. Experiments on a Real Social Network Graph involve training models on user devices. Training models on user devices connected by a real social network involves using ResNet20 for image classification and a three-layer LSTM architecture for language modeling on WikiText-2. Results show that the decentralized algorithm performs best for image classification tasks. The decentralized algorithm performs best for image classification tasks, while the centralized scheme achieves the highest test accuracy. CHOCO-SGD outperforms the exact decentralized scheme in terms of test accuracy with less transmitted data. For language modeling, CHOCO-SGD outperforms the centralized SGD in test perplexity. Large-scale training with Resnet-50 on ImageNet-1k in a datacenter setting using \"Sign+Norm\" quantization scheme of CHOCO-SGD shows benefits when scaling to more nodes. Decentralized optimization methods can address scaling issues even in well-connected devices like datacenters with fast connections. Recent studies have shown decentralized schemes outperform centralized ones, with impressive speedups for training on 256 GPUs. Their algorithm includes asynchronous gossip updates, time-varying communication topology, and exact communication. In a datacenter setting, large-scale training with Resnet-50 on ImageNet-1k using decentralized communication with compressed communication in a ring topology. Each machine has 4 Tesla P100 GPUs, and all-reduce is performed within one machine. Mini-batch size is 128 per GPU, following the general SGD training scheme. CHOCO-SGD benefits from its decentralized and parallel structure, taking less time than all-reduce for the same number of epochs with only a slight 1.5% accuracy loss. It shows a 20% time-wise gain over the common all-reduce baseline on commodity hardware. The momentum version of CHOCO-SGD is proposed for decentralized deep learning training. The CHOCO-SGD algorithm enables decentralized deep learning training in bandwidth-constrained environments with theoretical convergence guarantees and a linear speedup in the number of nodes. It performs well in image classification and language modeling tasks, even in strongly communication-restricted environments, showcasing high communication compression capabilities. The proof of Theorem 4.1 presents the analysis of CHOCO-SGD for arbitrary stepsizes \u03b7, showing convergence of algorithms with stochastic gradient updates followed by averaging steps as long as the averaging scheme exhibits linear convergence. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, assuming linear convergence of the averaging scheme. The algorithm uses matrix notation and a Laypunov function for convergence analysis. Exact Averaging is discussed as a consensus averaging algorithm with a mixing matrix W, converging at a rate of c = \u03c1. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, using matrix notation and a Laypunov function for convergence analysis. The algorithm converges at a rate of c = \u03c1, where \u03c1 is an eigengap of the mixing matrix W. To recover CHOCO-SGD, CHOCO-GOSSIP is chosen as the consensus averaging scheme. The order of communication and gradient computation parts can be exchanged without affecting convergence rate. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, converging at a rate of c = \u03c1. The recursion verifying the convergence completes the proof. Under certain assumptions, the averaged iterates of Algorithm 3 show a linear speedup compared to SGD on one node. The convergence rate for exact averaging with W gives a recovery rate similar to D-PSGD. CHOCO-SGD with CHOCO-GOSSIP averaging scheme converges at a rate dependent on the eigengap of the mixing matrix W. Theorem A.2 provides guarantees for the averaged vector of parameters in decentralized settings, where averaging all parameters across machines is costly. Similar guarantees can be obtained for individual iterates x_i as shown in previous work (Assran et al., 2019). Theorem A.2 provides guarantees for decentralized settings, while individual iterates x_i are analyzed in (Assran et al., 2019). Corollary A.3 discusses convergence of local weights, with a relaxation on the required T value. Theorem A.4 presents convergence results for Algorithm 3 under specific assumptions, with a comparison to Theorem A.2. Theorem A.5 states that Algorithm 1 converges at speed c2(T+1) under certain assumptions. Lemma B.1 and B.3 provide inequalities for vectors and matrices. Algorithm 4, CHOCO-SGD, incorporates error feedback and a mixing matrix. Algorithm 2 combines CHOCO-SGD with weight decay and momentum, with the possibility of adapting Nesterov momentum for decentralized settings. In a decentralized setting, Nesterov momentum can be adapted for CHOCO-SGD, an error feedback algorithm. The algorithm saves quantization errors in internal memory and corrects them before compression. The procedure for model training and hyper-parameter tuning is detailed, along with the setup for social networks. Comparisons are made with CHOCO-SGD using sign compression. In this study, CHOCO-SGD with sign compression is compared with decentralized SGD without compression and centralized SGD without compression. Two models are trained: ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. The experimental setup follows Merity et al. (2017) for the language modeling task. The experimental setup includes training ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. Parameters such as BPTT length, gradient clipping, dropout, mini-batch size, learning rate, and momentum are fine-tuned for each model. The learning rate is gradually warmed up and decayed during training epochs, with the optimal learning rate per sample determined by linear scaling. The optimal learning rate per sample is determined by a linear scaling rule, searched in a pre-defined grid to ensure best performance. Fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20 on Cifar10 and a social network topology are reported in tables. Training data is split between nodes with a fixed partition during training. The training data is partitioned between nodes with a fixed partition during training, with a per node mini-batch size of 32 and a maximum node degree of 14. The learning curve for the social network topology is plotted, showing top-1, top-5 accuracy, and test top-5 accuracy. Additionally, the test accuracy of the averaged model and the consensus of local models towards the end of optimization are depicted. The local models initially diverge from the averaged model before reaching consensus, with their test performances aligning only when the stepsize decreases. This behavior was also observed in a previous study by Assran et al. (2019)."
}