{
    "title": "H1cT3NTBM",
    "content": "In this paper, the use of neural networks for music information retrieval tasks is explored. The study focuses on improving convolutional neural networks (CNNs) on spectral audio features for singer classification and singing performance embedding. Three aspects of CNN design are investigated: network depth, residual blocks with grouped convolution, and global time aggregation. Results indicate that global time aggregation significantly enhances CNN performance. Additionally, a singing recording dataset is released for training and evaluation purposes. Advancements in computer vision and natural language processing have improved retrieval problems. Recent experiments explore using ResNet and ResNeXt variants to enhance deep neural networks for time-frequency representations in audio analysis. These variants allow for deeper convolutional layers in neural networks. In this paper, a deeper architecture with more than 5 convolution layers is proposed for music information retrieval using convolutional neural networks. The convolution layers learn local patterns in input matrices, and recurrent neural networks are used to capture temporal relations in time-frequency representations. Recent developments in natural language processing have introduced attention mechanisms as a new approach to model temporal dependencies and relations. The attention mechanism is seen as a global aggregation operation with learnable parameters along the time axis, different from typical aggregation operations like average or max. This paper investigates the effects of global aggregation using average, max, or the attention mechanism through experiments in singer classification of monophonic recordings and singing performance embedding. Singer classification aims to predict the singer's identity from an audio recording, while singing performance embedding creates an embedding space. The goal in singer performance embedding is to create an embedding space where singers with similar styles are closer together. The challenge lies in isolating the \"singer effect\" from the \"song effect\" when analyzing audio recordings. This approach can be applied to various music and audio tasks where similarity is crucial. The singer performance embedding aims to emphasize singer similarity while reducing the song effect in audio recordings. This is analogous to face identification in computer vision, where the identity of a face should not be affected by the environment or pose. The goal is to create an embedded space where singers with similar styles are closer together. The goal is to create an embedding space for singing voice audio recordings that emphasizes singer similarity and identifies singing style. A siamese neural network is used to achieve this by placing recordings of the same identity closer together and those of different identities farther apart. The architecture employs CNNs to extract features and fully connected dense layers for classification and performance embedding. The architecture uses fully connected dense layers for singer identity classification and singing performance embedding. The output layer differs between the tasks, with a softmax layer for classification and a linear layer for embedding. The embedding allows for efficient similarity comparison of spectrogram sequences by calculating Euclidean distance between fixed-length vectors. This enables quick querying of a large database of singing recordings. The paper introduces a new set of \"balanced\" singing recordings to evaluate a singing performance embedding model. The neural network architecture used in the experiments involves feeding input time-frequency features as 2-D images into convolutional layers and then to a global time-wise aggregation layer. The dataset and experiment details are discussed in section 3, with conclusions in section 4. The neural network architecture involves feeding input features to convolutional layers and a global time-wise aggregation layer. Different construction blocks like vanilla convolution, ResNet, and ResNeXt are used in the experiments. The ResNet bottleneck block is y = x + f(g(h(x))), while the ResNeXt bottleneck block is y = x + \u0393(x) with \u0393(\u00b7) being the grouped convolution. A max pooling layer is placed between convolutional blocks, with the first layer followed immediately by a max pooling layer and the rest having max pooling layers between every two consecutive convolutional layers/blocks. The neural network architecture in this paper includes convolutional layers with batch normalization, a global time-wise aggregation layer, dense layers, and an output layer. The 3-D feature map is reshaped before feeding it to the aggregation layer. The attention mechanism was originally introduced for sequence-to-sequence learning in an RNN architecture. The paper introduces a feed-forward attention mechanism in an RNN architecture for learning BID0, allowing predictions to access information from all input hidden sequences in a weighted manner. The attention operation calculates weight vector \u03c3 over time-steps using learnable parameters w and b, resulting in an outputX as a weighted average of input matrix X. This mechanism is used instead of the original sequence-to-sequence prediction in experiments. The attention operation in the RNN architecture is determined by learnable parameters w and b, acting as an aggregation over the time-axis. This family of operations includes feed-forward attention, max, and average, with the latter two having no learnable parameters. The aggregation reduces the dimension of the aggregation axis to 1, different from standard pooling in convolution layers. The network architecture includes convolutional and global aggregation parts, with tasks of singer identity classification and singing performance embedding explored. The singer classification problem offers clear evaluation criteria for model performance with various hyperparameters and network architectures. The embedding task provides a more exploratory understanding of spatial relationships between samples. Evaluation metrics and plots of embedded samples from the singing performance dataset are provided for quantitative and qualitative analysis. The DAMP dataset consists of 34620 solo singing recordings by 3462 singers, each with 10 recordings. The DAMP dataset is unbalanced due to singers singing different songs multiple times, leading to bias in learning algorithms. To address this, the DAMP-balanced dataset was created with 24874 recordings by 5429 singers, featuring 14 songs. The dataset includes a test set of 4 songs and a train/validation split of the remaining 10 songs for model training and evaluation. The DAMP-balanced dataset contains 6/4 songs collections split for singing performance embedding tasks. Time-frequency representations extracted from raw audio signals are used as input for neural networks, represented as 2-D matrices with time and frequency axes. The Mel-scaled magnitude spectrogram (Mel-spectrogram) is used as input for neural networks in this paper, while the constant-Q transformed spectrogram (CQT) is also mentioned but performs worse than Mel-spectrogram versions. The Mel-scaled magnitude spectrograms are obtained using a Fast Fourier Transform with specific parameters. The values are transformed into decibels and adjusted to have values between 0 and 60. The Mel-spectrogram of each singing performance audio recording is chopped into overlapping matrices with specific parameters. Gradient descent is optimized using ADAM with a learning rate of 0.0001 and a batch size of 32. L2 weight regularizations are applied on all learnable weights. Hyperparameters are chosen using Bayesian optimization. Early stopping tests are applied every 50 epochs for singer identity classification and singing performance embedding tasks. Non-linear activation functions are used in all convolution layers and fully connected dense layers. For singer classification, a subset of 46 singers from the DAMP dataset is used with a 10-fold cross validation. Different neural network configurations are explored with specific layer sizes and activation functions. Different combinations of neural network configurations are explored for singer classification, including vanilla CNN or ResNeXt building blocks, different numbers of layers, and types of aggregation. A baseline SVM classifier is included, achieving 27% accuracy, while neural network models exceed this by at least 35%. The number of convolution filters is adjusted to ensure parameter parity across configurations. The neural network models significantly outperformed the baseline by 35% or more. Global aggregation methods improved performance by 5% to 10%. The experiment focused on creating an embedding space that groups recordings by the same singer together and separates recordings by different singers. A siamese neural network architecture was used with an embedding dimension of 16. The embedding dimension for the linear fully connected output layer is chosen to be 16 by SPEARMINT. A siamese network arranges pairs of samples from the dataset and learns the embedding by adjusting the distance between them based on their label. The optimization goal is the contrastive loss, defined as the squared euclidean distance between embedded vectors with a target margin of 1. Training involves pairs of samples from the same or different singers. To train siamese networks, pairs of samples from the same or different singers are randomly fed into the networks. Results show that feed-forward attention and average aggregation tend to overfit more than max and no aggregation. Shallow architectures perform slightly better than deeper ones with similar parameters. Qualitative characteristics of the embedding are shown in Figure 4. The embeddings of performances by 10 singers singing 4 songs each were compared using shallow ResNeXt architecture with/without feed-forward attention and handcrafted features for singer classification. The handcrafted features captured the \"song\" effect, while the learned embeddings grouped performances by singers regardless of the song. t-SNE projections of 6-second clips before summarizing into songs are shown in Figure 4. The t-SNE projections of 6-second clips before summarizing into songs are used for k-nearest neighbor classifications to assess the embeddings. Singer and song classifications are conducted using shallow ResNeXt configurations with/without attention and handcrafted features, showing classification results in Figure 5. The study explores the \"song effect\" in singer identification through deep learning techniques. Global aggregation over time significantly enhances performance, reducing the \"song effect\" while characterizing singers. The balanced dataset enables k-nearest neighbor classification on performed songs, showing improved accuracy with feed-forward global aggregation. The study shows that global time aggregation improves performance significantly. Feed-forward attention accelerates learning compared to other aggregation strategies. The feed-forward attention layer learns \"frequency templates\" for each convolutional channel, allowing focus on different parts along the frequency axis. Training deep neural networks with more than 15 convolutional layers on time-frequency input is feasible with global time aggregation. The authors introduce a dataset of over 20000 single singing voice recordings and propose neural network configurations for music information retrieval tasks. They plan to experiment with replacing max-pooling with striding in convolutional layers and improving global-aggregation by considering temporal order. The DAMP-balanced dataset is separate from the original DAMP but shares the same metadata format. The DAMP-balanced dataset, separate from the original DAMP, has the same metadata format. It is collected by querying the Sing! Karaoke app database and differs in how audio recordings and metadata are gathered. The original DAMP randomly selects 10 performances from 3462 users, while the DAMP-balanced dataset specifically requests users who sang a specific collection of songs at least once. The dataset was collected by querying the Sing! Karaoke app database for users who sang a specific collection of 14 popular songs. Queries were created to retrieve audio recordings and metadata for different combinations of splitting the songs into 6/4 collections. The training set consisted of the first 6 songs and the validation set consisted of the following 4 songs, resulting in 276 performances for training and 88 performances for validation. Different splits led to varying numbers of singers. The dataset was collected by querying the Sing! Karaoke app database for users who sang a specific collection of 14 popular songs. Different splits of the songs into 6/4 collections resulted in varying numbers of singers. For example, a different 6/4 split led to 46 singers for the first 4 songs and 22 singers for the following 6 songs. This \"balanced\" structure allows for different train/validation rotations within the first 10 songs and provides balanced test sets for models training on other datasets."
}