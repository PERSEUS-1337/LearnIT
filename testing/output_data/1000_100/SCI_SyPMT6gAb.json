{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning involves evaluating and improving policies using historical data from a logging policy. This is important because on-policy evaluation is costly and has negative effects. A major challenge in off-policy learning is developing counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization with neural network policies show significant improvement over conventional baseline algorithms. Off-policy learning is crucial for evaluating deterministic policies using historical data, as on-policy evaluation is costly and has negative impacts. Off-policy learning is essential for evaluating new treatment options and online advertising strategies. Various methods like Q learning and doubly robust estimator have been studied in reinforcement learning and contextual bandits. A new direction involves using logged interaction data with limited feedback. In off-policy learning with limited feedback, the challenge lies in handling distribution mismatch between logging policy and new policy for counterfactual inference. BID34 addressed this by deriving the new counterfactual risk. BID34 introduced a new counterfactual risk minimization framework to address the induced generalization error in off-policy learning. They added sample variance as a regularization term but faced limitations with linear stochastic models for policy parametrization and the computation of sample variance regularization. Despite proposing a first-order approximation technique, developing accurate and efficient training algorithms remains a challenge. The paper's contribution includes proposing a new learning principle for off-policy learning with bandit feedback by minimizing distribution divergence to regulate the generalization error of the new policy. The proposed learning objective minimizes distribution divergence between the new policy and the logging policy, balancing empirical risk and sample variance. The policy is parametrized as a neural network for end-to-end training, utilizing variational divergence minimization and Gumbel soft-max sampling. Experimental results demonstrate improved performance over conventional baselines, validating theoretical proofs. In reinforcement learning, stochastic policies define a distribution over actions based on input x. Actions are sampled from this distribution, with each action having a probability of being selected. Feedback is observed by comparing the sampled action to an underlying 'best' action. In reinforcement learning, stochastic policies define a distribution over actions based on input x. Actions are sampled from this distribution, with each action having a probability of being selected. Feedback is observed by comparing the sampled action to an underlying 'best' action. In recommendation systems, a scalar loss function is used to indicate satisfaction with recommended items. Off-policy learning aims to minimize expected risk by finding a policy with lower risk on test data. In the off-line logged learning setting, data from a logging policy is used to improve policy performance. Challenges arise when the logging policy's distribution is skewed, leading to incomplete feedback for certain actions. In reinforcement learning, feedback on actions is crucial for improvement. Empirical estimation using finite samples leads to generalization error and requires regularization. Propensity scoring with importance sampling addresses distribution mismatch. Counterfactual risk minimization addresses flaws in the vanilla approach, such as variance regularization. The authors proposed a regularization term for sample variance to address unbounded variance in reinforcement learning. They approximated the regularization term via first-order Taylor expansion to enable stochastic training. This approximation neglects non-linear terms and introduces errors in reducing sample variance. The parametrized policy h(Y|x) allows for deriving a variance bound directly. The authors proposed a regularization term for sample variance in reinforcement learning by approximating it with a first-order Taylor expansion. The parametrized policy h(Y|x) enables deriving a variance bound directly from the distribution. By using importance sampling weights, an upper bound for the second moment of the weighted loss can be derived. Theorem 1 provides a bound for the second moment of the weighted loss based on joint distribution over x, y. The proof involves a generalization bound between expected risk and empirical risk using distribution divergence function. The bound is derived with probability at least 1 \u2212 \u03b7. The proof of Theorem 1 involves Bernstein inequality and the second moment bound, highlighting bias-variance trade-offs in ERM problems. It suggests minimizing variance regularized objectives in bandit learning instead of directly optimizing reweighed loss. The model hyper-parameter \u03bb controls the trade-off between empirical risk and model variance, but setting it empirically remains a challenge. Distributionally robust learning is explored as a potential solution. In light of recent success in distributionally robust learning, an alternative formulation of regularized ERM is explored. A constrained optimization formulation is studied, with a regularization hyper-parameter \u03c1. The new objective function is a good surrogate for the true risk, with their difference bounded by \u03c1 and approaching 0 as N \u2192 \u221e. This formulation eliminates the need to compute sample variance in existing bounds, especially with a parametrized distribution of h(y|x) and finite samples. Recent advancements in f-gan networks and Gumbel soft-max sampling have shown promise in variational divergence minimization tasks with parametrized distributions and finite samples. The importance of stochasticity in logging policies is highlighted, as deterministic policies with peaked masses and zero exploration regions can hinder learning. The divergence term calculation reflects this challenge, emphasizing the need for exploration in policy design. The unbounded generalization bound in counterfactual learning is due to an unbounded integral result, making it not possible in this case. The variance regularized objective requires minimizing the square root of the conditional dy. By connecting the divergence to the f-divergence measure, a minimization objective of D f (h||h 0 ; P(X)) can be achieved. This draws a connection to the f-GAN for variational divergence minimization method proposed in previous research. The minimization method proposed in BID26 aims to reach a lower bound of the objective by utilizing Fenchel convex duality. The bound is tight when T 0 (x) = f (h/h 0 ), and the universal approximation theorem of neural networks (BID15) allows for the use of neural networks to satisfy the equality condition. The final objective is a saddle point of a function that maps input pairs to a scalar value, with the policy acting as a sampling distribution. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence. The true divergence is denoted by Df, and the empirical estimator is used to approximate it. The estimation error is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation. The error in the approximation of an empirical mean to the true distribution is discussed, with terms related to neural networks and the difference between empirical and population distributions. The strong law of large numbers is applied, leading to the conclusion that the error terms tend towards zero. Generative-adversarial approaches and discriminator networks are also mentioned in the context of the discussion. The T function is represented as a discriminator network T w (x, y) and the policy h(y|x) as a generator neural network h \u03b8 (y|x). Gumbel soft-max sampling is used for differential sampling from h(y|x). The training procedure is outlined for optimizing the generator distribution h \u03b8 * (y|x) to minimize divergence from the initialization h 0. The algorithm presented in the curr_chunk outlines the training process for counterfactual risk minimization using a generator network h \u03b8 (y|x) and discriminator network T w (x, y). Gumbel soft-max sampling is utilized for generating 'fake' samples. The algorithm aims to optimize the generator h * \u03b8 (y|x) to minimize divergence and includes regularization parameters and maximum iteration steps. The algorithm presented focuses on minimizing variance regularized risk through separate training steps. It involves updating the policy parameters to minimize reweighed loss and then updating the generator and discriminator parameters to improve generalization performance. Exploiting historic data is crucial in various bandit problems and has broad applications. Bandit and its variants have wide applications. Doubly robust estimators have been proposed, and recent theoretical studies explored the finite-time minimax risk lower bound of the problem. Bandit problems can be interpreted as single-state reinforcement learning problems, with techniques extended to RL domains. Conventional techniques like Q function learning and temporal difference learning are alternatives for off-policy learning in RL. Recent works in deep RL have addressed off-policy updates through methods like multi-step bootstrapping and off-policy training of Q functions. Off-policy training of Q functions and learning from logs traces using propensity scores are key techniques in bandit learning. The problem is also known as treatment effect estimation, focusing on estimating the effect of an intervention from observational studies. Unbiased counterfactual estimators and techniques to reduce bandit learning to supervised learning have been explored. Variance regularization in off-policy learning with bandit feedback is studied, with insights from generalization bounds in importance sampling problems. The text discusses variance regularization in supervised learning to address distribution mismatch issues. It explores connections to distributionally robust optimization techniques and proposes divergence minimization as an alternative approach. The text discusses empirical evaluation of algorithms by converting supervised learning to bandit feedback method. It involves constructing a logging policy, sampling predictions, and collecting feedback. Bandit feedback datasets are created with logging policies and loss functions. In evaluation, two types of metrics are used for the probabilistic policy h(Y|x): expected loss (EXP) and average hamming loss of maximum a posteriori probability (MAP). The expected loss measures the generalization performance of the learned policy, while MAP predictions focus on regions with the highest probability. A model with high MAP performance but low EXP performance may be overfitting. The text discusses the comparison of different algorithms for probabilistic policy evaluation, including IPS and POEM, using L-BFGS and stochastic optimization. NN-NoReg is used as a baseline. Four multi-label classification datasets are converted from supervised to bandit tasks, with statistics reported in the Appendix. For the converted datasets, a three-layer feed-forward neural network is used for policy distribution, and a two or three layer feed-forward neural network for divergence minimization. Separate training version 2 is chosen for benchmark comparison due to faster convergence and better performance. The networks are trained with Adam optimizer with different learning rates. PyTorch is used for implementation and Nvidia K80 GPU cards for training. Results from 10 experiment runs are averaged and reported with evaluation metrics in TAB0. Regularized neural network policies are presented. Regularized neural network policies with two Gumbel-softmax sampling schemes, NN-Soft and NN-Hard, show improved test performance compared to baseline CRF policies. The introduction of additional variance regularization term further enhances testing and MAP prediction loss. No significant difference is observed between the two sampling schemes. Varying the maximum number of iterations in divergence minimization sub loop quantitatively studies the effectiveness of variance regularization. In divergence minimization sub loops, regularization improves test performance by reducing loss and increasing convergence rate. Increasing the number of iterations for divergence minimization leads to faster decrease in test loss and better generalization to test sets. Stronger regularization results in better test performance. Regularization improves test performance by reducing loss and increasing convergence rate. The number of training samples in the bandit dataset affects test performance, with regularized policies showing better generalization compared to models without regularization. Stronger regularization leads to better generalization ability. In this section, experiments compare two training schemes: cotraining in Alg. 3 and the easier version Alg. 2. The figures suggest that blending weighted loss and distribution divergence performs slightly better than models without regularization. However, training is more challenging due to balancing the gradient of the objective function parts. Two Gumbel-softmax sampling schemes are compared, showing no significant performance difference. In this section, the effect of logging policies on learning performance is discussed. The algorithm's ability to improve policy depends on the stochasticity of the logging policy. By introducing a temperature multiplier \u03b1 to modify the parameter h0, the stochasticity of the policy can be tested. Varying \u03b1 in the range of 2 [-1,1,...,8] results in a more peaked distribution for h0, ultimately leading to a deterministic policy as \u03b1 approaches infinity. The effect of logging policies on learning performance is discussed by introducing a temperature multiplier \u03b1 to modify the parameter h0. Varying \u03b1 in the range of 2 [-1,1,...,8] shows that NN policies perform better than logging policies when h0's stochasticity is sufficient. However, as the temperature parameter exceeds 2/3, it becomes harder to learn improved NN policies. Stronger regularization in NN policies leads to slightly better performance, indicating the robustness of the learning principle. Our regularization helps improve the model's robustness and generalization performance as the stochasticity of h0 decreases. The difficulty increases as h0 quality improves, with models consistently outperforming baselines. The impact of logging policies on learned improved policies is discussed, highlighting the trade-off between policy accuracy and sampling biases. Varying the proportion of training data points used to train the logging policy shows the performance of improved policies. In this paper, a new training principle is proposed to improve the generalization performance of off-policy learning for logged bandit datasets by explicitly regularizing variance. Theoretical discussions guided the development of a training objective combining importance reweighted loss and a regularization term measuring distribution match between the logging policy and improved policies. As the logging policy improves, both NN and NN-Reg policies outperform it, addressing sampling biases and showing relative policy improvement. By minimizing variational divergence and using Gumbel soft-max sampling, neural network policies are trained to reduce variance. Benchmark evaluations and case studies confirm the effectiveness of the learning principle and training algorithm. However, a limitation is the need for propensity scores, which may not always be available. Estimating propensity scores and incorporating them into the training framework would enhance the algorithm's applicability. Learning importance weights directly as suggested by BID6 could provide comparable theoretical guarantees. The work focuses on off-policy learning from logged data, with potential extensions to supervised and reinforcement learning. Applying Lemma 1 to importance sampling weight functions and loss functions, theorems are derived using Reni divergence and Bernstein's concentration bounds. These bounds provide guarantees for importance sampling in bandit learning. The curr_chunk discusses the optimization process of a generator and discriminator in a co-training algorithm. It involves updating the discriminator and generator iteratively while sampling fake and real data batches. The algorithm aims to minimize variance regularized risk. The text also mentions reporting dataset statistics and the effect of stochasticity on test loss. The curr_chunk discusses the impact of stochasticity on test loss and the performance of neural network policies compared to a baseline policy. It highlights that neural network policies can still improve over the baseline in expected loss, but struggle to outperform the baseline in MAP predictions. Further investigation is suggested to understand this phenomenon."
}