{
    "title": "BJInEZsTb",
    "content": "In this paper, a deep autoencoder network is introduced for studying representation learning and generative modeling of geometric data represented as point clouds. The learned representations outperform the state of the art in 3D recognition tasks and enable shape editing applications. Different generative models, including GANs and Gaussian mixture models, are studied, with GMMs trained in the latent space of the autoencoders producing samples of the best fidelity and diversity. The paper introduces a deep autoencoder network for studying representation learning and generative modeling of geometric data represented as point clouds. It proposes simple measures of fidelity and diversity based on matching point clouds for quantitative evaluation of generative models. Different encodings like view-based projections, volumetric grids, and graphs are discussed, highlighting the limitations in semantics and suitability for generative models. Recent advances in deep learning have revolutionized the design of generative models, eliminating the need for complex parametric models and manual feature crafting. Deep learning architectures like autoencoders and Generative Adversarial Networks (GANs) excel at learning complex data representations and generating realistic samples. Deep learning architectures for view-based projections, volumetric grids, and graphs have shown promise in this domain. In this paper, the focus is on 3D point clouds, which are a compact representation of surface geometry commonly used in devices like the Kinect and iPhone. Only a few deep learning architectures exist for 3D point clouds, such as PointNet for classification and segmentation tasks. The paper focuses on using deep architectures to learn representations and generative models for 3D point clouds. It addresses the challenges of training GAN-based generative pipelines and evaluating generative models for fidelity and coverage. The authors propose simple methods to deal with these issues in the target domain. The paper introduces a new AE architecture for learning compact representations of point clouds with high reconstruction quality. It also presents generative models capable of generating point clouds similar to training and test data, without the need for joint learning and training of GANs. The paper introduces a new AE architecture for learning compact representations of point clouds with high reconstruction quality. It discusses the benefits of training a GAN in a compact latent space, showing that latent GANs are easier to train and achieve superior reconstruction. Additionally, GMMs trained in the latent space of fixed AEs perform the best overall. Multi-class GANs are found to work almost as well as dedicated GANs when trained in the latent space. Various metrics are evaluated for learning good representations and evaluating generated samples, with the Chamfer distance metric showing limitations in discriminating certain cases. The paper introduces new fidelity and coverage metrics for generative models of point clouds, addressing limitations of the Chamfer distance metric. It outlines the necessary background, evaluation metrics, models for latent representations, and generation of point clouds. The models are evaluated quantitatively and qualitatively in Section 4, with further results available in the appendix. The code for all models is publicly accessible. Generative Adversarial Networks (GANs) are advanced models that involve a game between a generator (G) and a discriminator (D) to create realistic data samples. The Encoder (E) compresses data into a low-dimensional representation, while the Decoder (D) reconstructs the data from this representation. The discriminator and generator networks use specific loss functions during training. The improved Wasserstein GAN is used in addition to the classical GAN formulation. Point clouds pose unique challenges due to their lack of grid-like structure, making them harder to encode than images or voxel grids. Recent work on point cloud classification bypasses these challenges by avoiding 2D convolutions and addressing the issue of unordered data. The issue with point clouds is their unordered nature, making comparisons difficult. Two metrics for comparing unordered point sets are Earth Mover's distance (EMD) and Chamfer distance (CD). EMD transforms one set to match the other, while CD measures the squared distance between points and their nearest neighbors. The Chamfer distance (CD) measures the squared distance between points in one set to their nearest neighbor in the other set, making it more computationally efficient. It is used to evaluate the quality of representation and generative models by comparing point clouds to their ground truth counterparts. This comparison helps assess faithfulness, diversity, and potential mode-collapse in the models. The metrics used for comparison include Coverage, where the closest neighbor in the ground truth distribution is found for each point-cloud in the given set. The Chamfer distance (CD) measures the distance between points in sets G and A, yielding COV-CD and COV-EMD metrics for coverage. High coverage indicates representation of most of G in A, but fidelity is measured by matching each point cloud in G to the closest one in A using MMD-CD and MMD-EMD. Jensen-Shannon Divergence (JSD) compares marginal distributions in 3D space for realistic elements in A. The text discusses measuring the similarity between point clouds A and B using axis-aligned data and a voxel grid. It describes the architectures of representation and generative models for point clouds, including an autoencoder design, a GAN tailored to point-cloud data, and a generative model based on Gaussian Mixtures. The input to the AE network is a point cloud with 2048 points representing a 3D shape. The encoder architecture follows the principle of 1-D convolutional layers with kernel size 1. The text discusses the architecture of an autoencoder for point clouds, utilizing 1-D convolutional layers with increasing features and a symmetric function. The encoder consists of 5 1-D conv layers followed by ReLU and batch-norm layers, producing a k-dimensional latent vector. The decoder comprises 3 fully connected layers to generate a 2048 \u00d7 3 output. Two distinct AE models, AE-EMD and AE-CD, are explored for permutation invariant objectives using EMD-distance and Chamfer-Distance as structural losses. The text discusses the architecture of an autoencoder for point clouds, utilizing 1-D convolutional layers with increasing features and a symmetric function. The encoder consists of 5 1-D conv layers followed by ReLU and batch-norm layers, producing a k-dimensional latent vector. The decoder comprises 3 fully connected layers to generate a 2048 \u00d7 3 output. Two distinct AE models, AE-EMD and AE-CD, are explored for permutation invariant objectives using EMD-distance and Chamfer-Distance as structural losses. Architecture parameters were varied to find the best latent-space size, with k = 128 showing the best generalization error on test data. The raw point cloud GAN operates directly on the raw 2048 \u00d7 3 point set input, with the discriminator architecture being similar to the AE but with leaky ReLUs instead of ReLUs. The generator in the Latent-space GAN (l-GAN) maps a 128-dimensional noise vector to a 2048 \u00d7 3 output using 5 FC-ReLU layers. The l-GAN operates on the bottleneck variable of a pre-trained autoencoder, simplifying the architecture with shallow designs for both the generator and discriminator. The output of the generator is decoded to a point cloud via the autoencoder decoder after GAN training. The study explores the use of Gaussian Mixture Models (GMMs) in addition to Latent-space GANs for generating realistic shapes. GMMs are trained on latent spaces learned by autoencoders, and can be used as point-cloud generators by sampling from the GMM distribution and decoding with the autoencoder's decoder. The shapes are reconstructed from the ShapeNet repository using class-specific autoencoders. The study evaluates unsupervised representation learning algorithms by using them as feature extractors on supervised datasets. The autoencoder was trained on 57,000 models from 55 categories of man-made objects with a bigger bottleneck of 512. Features for a 3D shape are obtained by feeding its point-cloud to the network and extracting a 512-dimensional bottleneck layer vector. The study evaluates unsupervised representation learning algorithms by using them as feature extractors on supervised datasets. Features for a 3D shape are obtained by feeding its point-cloud to the network and extracting a 512-dimensional bottleneck layer vector. The linear classification SVM trained on ModelNet BID32 shows comparative results with previous state of the art BID31. The 512-dimensional feature is more intuitive and parsimonious compared to the 7168-long feature derived from several layers of a GAN. CD loss performs better when the variation within the collection increases, possibly due to its more local and less smooth nature. The experiment demonstrates the domain-robustness of the learned features through qualitative evaluation. Reconstruction results show the ability to generalize to unseen shapes and enable shape editing applications. The AEs are able to reconstruct unseen shapes, as shown in the results. The experiment highlights the ability of AEs to reconstruct unseen shapes, demonstrated through qualitative evaluation. Five generative models are trained on point-cloud data of the chair category, including AEs with 128-dimensional bottleneck trained with CD or EMD loss, referred to as AE-CD and AE-EMD. l-GANs are trained in each AE's latent space, with additional models utilizing Wasserstein objective with gradient-penalty in the AE-EMD space. The study involves training various generative models on point-cloud data of the chair category, including AEs with different loss functions and l-GANs with the Wasserstein objective. Model selection is based on synthetic results matching the ground-truth distribution using JSD or MMD-CD metrics, with selection epochs shown in a table. The study involves training generative models on chair point-cloud data, selecting models based on JSD and MMD-CD criteria. Optimal Gaussian components were determined, with GMMs performing better with full covariance matrices. Evaluation of 5 generators on chair dataset showed an average classification score of 84.7%. The study compares generative models based on their ability to generate synthetic samples. Experiments measure how well the generated samples match the train and test splits of the ground truth distribution. Results are reported in Table 2, including the average classification probability for samples recognized as chairs. Another experiment evaluates 5 generators on the test-split of the chair dataset. The study compares generative models based on their ability to generate synthetic samples. Results show that training a Gaussian mixture model in the latent space of the EMD-based AE yields the best results in terms of fidelity and coverage. The achieved fidelity and coverage are very close to the reconstruction baseline. Comparing the results in TAB10, the GMMs perform comparably to the AE-EMD in terms of MMD-EMD values. The study compares generative models based on their ability to generate synthetic samples. Results show that training a Gaussian mixture model in the latent space of the EMD-based AE yields the best results in terms of fidelity and coverage. Comparing the results in TAB10, the GMMs perform comparably to the AE-EMD in terms of MMD-EMD values. The number of synthetic point clouds generated for the train split experiment is equal to the size of the train dataset, while for the test split experiment and validation split comparisons, synthetic datasets three times bigger than the ground truth dataset are generated to reduce sampling bias. This is highlighted in more detail in FIG0 in the appendix. The r-GANs appear to have a small advantage, but qualitative inspection reveals issues with the chamfer distance in distinguishing pathological cases. Examples are shown in Fig. 3, comparing r-GAN and l-GAN results. Distances between nearest neighbors in synthetic sets and ground truth are reported using CD and EMD values. The r-GAN results show a discrepancy between CD and EMD metrics, with CD values not capturing the lower quality of the generated shapes. The concentration of points in specific areas by r-GANs leads to a small CD value, while EMD promotes one-to-one mapping and correlates better with visual quality. This difference results in higher CD-based coverage metrics compared to EMD. During training, r-GAN struggles to provide good coverage of the test set, as shown by JSD distance, EMD-based MMD, and Coverage metrics. In contrast, l-GAN (AE-CD) performs better in terms of fidelity with fewer epochs. The CD distance favors r-GAN results due to high-density areas in the synthesized point sets. Switching to an EMD-based AE for the representation in the latent GAN architecture (l-GAN, AE-EMD) improves coverage and fidelity. However, mode collapse issues are still present in both l-GANs, which can be mitigated by using a latent WGAN. This study is the first to propose GANs on point-cloud data, with comparisons made to voxel-based methods. The study is the first to propose GANs on point-cloud data, comparing their models to a voxel-grid based approach. Results show that r-GAN slightly outperforms BID31 in diversity and realism, while l-GANs perform even better. The l-GANs outperform r-GAN in classification and diversity with fewer training epochs. Training time for l-GAN is significantly shorter due to its smaller architecture. Qualitative evaluation shows high-quality results from l-GANs and a 32-component GMM model trained on AE-EMD latent space. The learned representation allows the simple GMM model to perform well. The l-GAN produces clearer results than the r-GAN, showing the advantage of using structural loss on the pre-trained AE. Synthetic point clouds were generated using l-GAN and 32-component GMM models trained on AE-EMD latent space. Experiments were conducted with an AE-EMD trained on a mixed set of point clouds from 5 categories, achieving good results after 1000 epochs of training. Comparisons were made with class-specific AEs using a 85-5-10 train-val-test split. The study compares l-WGANs trained on multi-class AE-EMD with class-specific AEs, showing similar performance in terms of fidelity and visual quality. MMD-CD measurements for l-WGANs trained on dedicated and multi-class EMD-AEs are presented in Table 4. The study compares l-WGANs trained on multi-class AE-EMD with class-specific AEs, showing similar performance in terms of fidelity and visual quality. Limitations of the models include failure cases where chairs with rare geometries are not faithfully decoded, missing high-frequency geometric details, and struggling to create realistic shapes for certain classes like cars. Future work includes designing more robust raw-GANs for point clouds. In the study, VAEs are discussed with a focus on over-regularization issues. Methods to address this problem include starting with only a reconstruction penalty and gradually increasing the regularizer weight. The authors propose a novel set of architectures for 3D point-cloud representation learning and generation, showing good generalization to unseen data. The best-performing generative model in their experiments is a GMM trained in the fixed latent space of an AE. The model used in the experiments is a GMM trained in the fixed latent space of an AE. Simple latent GMMs should not be overlooked, as they can be as powerful as adversarially trained models. The AE used had specific encoder and decoder configurations, with batch normalization and online data augmentation. Training was done with CD loss and EMD for different epochs. Other AEs had different encoder and decoder configurations. The decoder in the model consisted of 3 FC-ReLU layers with 256, 256, 2048 \u00d7 3 neurons each. Different AE setups did not provide any noticeable advantage over the \"vanilla\" architecture. Adding drop-out layers resulted in worse reconstructions, while using batch-norm on the encoder only sped up training and slightly improved generalization error. The discriminator had 1D-convolutions with {64, 128, 256, 256, 512} filters in the first 5 layers, followed by leaky-ReLU and featurewise max-pool. The generator had {128, 64} neurons in the last 2 FC-leaky-ReLU layers leading to a single sigmoid neuron. The generator and discriminator architectures for the r-GAN and l-Wasserstein-GAN models were described. The r-GAN used 5 FC-ReLU layers for the generator and 2 FC-ReLU layers for the discriminator, while the l-Wasserstein-GAN used 2 FC-ReLUs for both. Training parameters and noise distribution were consistent between the two models. In Section 4.1, experiments utilized a one-versus-rest linear SVM classifier with an l2 norm penalty and balanced class weights. Table 5 shows the training parameters for SVMs with different structural loss models. The reconstruction quality of CD and EMD-based AEs is comparable between training and test datasets, measured by JSD of the reconstructed datasets compared to their ground truth counterparts. The quality of reconstruction is similar between training and test datasets, showing the ability of AEs to generalize across different shapes. Using a learned embedding across all object classes allows for shape editing applications, demonstrated by examples like tuning car appearance towards convertibles and adding armrests to chairs. Shape modifications are guided by shape annotations, enabling the editing of specific structural properties within object categories. Using latent representation, structural differences between objects A and B can be modeled. By transforming the latent representation of an object A, its properties can be changed, as shown in Figure 8. Interpolating between latent representations of shapes allows for the generation of intermediate variants, creating a \"morph-like\" sequence. The latent representation allows for morphing between shapes, enabling the removal and merging of shape parts. It also supports morphing between shapes of different classes and finding analogous shapes through linear manipulations and nearest-neighbor searching in the latent space. This is demonstrated with images in FIG0. In this section, preliminary results of point-cloud generators working with voxel-based AEs are presented. The latent space was learned by an AE working with occupancy grids of 3D shapes, using a full-GMM model with 32 centers. Grid resolutions of 32^3 and 64^3 were tested on ShapeNet's chair class. Generated voxel-grids were converted into 2048 points by extracting a mesh and sampling points. Comparisons were made with established point-cloud generators. The latent AE-based GMM models outperform Wu et al.'s voxel-based GAN architecture. Using the latent representation provides a vast improvement over the \"raw\" voxel GAN architecture. The performance of the 64^3 voxel-based GMM is comparable to the one operating at 32^3 resolution. The fidelity of results is not affected by lack of high-frequency details in ground-truth data. Point-cloud-based models outperform voxel-based models in fidelity to ground truth. Voxel-based models may artificially increase coverage due to matching poor quality shapes to ground truth. The voxel-based method shows a heavier \"tail\" indicating poor quality matchings. Qualitative inspection confirmed that the covering mostly came from very poor quality partial shapes. The study evaluated MMD and Coverage metrics on voxel-based methods at different resolutions matched against the chair test set. Volumetric models used GMMs with full covariances and different latent codes. Mesh conversion was done using the marching cubes algorithm. The results with point-cloud based GMM were also shown. The voxel-based AE architecture is described with fully-convolutional layers and specific parameters for the encoder and decoder. Training details include using Adam optimizer, binary cross-entropy loss, and comparison to a state-of-the-art method for reconstruction quality. The dense voxel-based AE architecture is compared to the state-of-the-art method of BID28 for reconstruction quality on the ShapeNetCars dataset. The intersection-over-union is used to measure the quality, with a comparison between the GMM-generator and a model that memorizes the training data of the chair class. Different sizes of training sets are evaluated against the point-clouds of the test split. The coverage/fidelity of the generative models is slightly lower when memorizing the training data. The generative models show good coverage and fidelity when trained on the same population as the test set. Using a learned representation allows for compact data representation and generating novel shapes. Despite some mode collapse, the generative models achieve almost identical fidelity to the memorization case. Additional comparisons with BID32 for ShapeNet classes are provided in Tables 10, 11, and 12. In Table 10, JSD-based comparisons are provided for two models, while TAB12 shows MMD/Coverage comparisons on the test split. The GMM/32 model is a GM model trained on the latent space of an AE with EMD structural loss. Generalization error of various GAN models is shown in FIG0, with measurements using JSD and MMD-CD metrics. Figure 17 displays GMM model selection with varying Gaussians and covariance types trained on the latent space learned by an AE with EMD and a bottleneck of 128. Models trained with full covariance achieve smaller JSD than those with diagonal covariance. 30 or more clusters are sufficient for minimal JSD. The 32 centers of the GMM fitted to the latent codes are shown in Figure 18. Evaluation of five generators on chair data is presented in TAB10, with models selected based on minimal MMD-CD on the validation-split. The quality of selected models remains consistent regardless of the selection metric used. GMM-40-F represents a GMM with 40 Gaussian components with full covariances. In this section, a GMM with 40 Gaussian components with full covariances (GMM-40-F) is evaluated by sampling synthetic point-clouds and comparing them to the ground truth using MMD-CD. This evaluation complements a previous measure shown in FIG2, demonstrating similar behavior."
}