{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) is a variational-inference method that achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. This method relies on reparametrizations and deals with breakdowns in inference-network gradients as the number of Monte Carlo samples increases. Different approaches, such as 'sticking-the-landing' IWAE (IWAE-STL) and 'doubly-reparametrised' IWAE (IWAE-DREG), have been proposed to address these issues. In this work, the authors argue that optimizing the proposal distribution in importance sampling is preferable to optimizing IWAE-type multi-sample objectives. They introduce an adaptive-importance sampling framework called AISLE, which generalizes the RWS algorithm. The framework includes IWAE-STL and IWAE-DREG as special cases. The generative model p \u03b8 (z, x) = p \u03b8 (z)p \u03b8 (x|z) is used to model observations x and latent variables z. The work focuses on algorithms for variational inference to learn the generative model and find the model parameters \u03b8. The text discusses learning the generative model and constructing a variational approximation in a setting with a single latent representation-observation pair. It also mentions the absence of shared parameters between the generative model and the variational approximation, and the potential for amortised inference. The text discusses stochastic gradient-ascent algorithms for optimizing parameters of an inference network using Monte Carlo samples. Different approaches like IWAE-DREG and RWS are proposed to address bias and improve gradient approximation. In the context of optimizing parameters of an inference network using Monte Carlo samples, the text compares the IWAE and RWS methods. The IWAE method suffers from \u03c6-gradient breakdown, while RWS optimizes the proposal distribution directly. The study shows that directly optimizing the proposal distribution, as done by RWS, is preferable to the multi-sample objective approach of IWAE. The study compares IWAE and RWS methods for optimizing parameters of an inference network using Monte Carlo samples. RWS is preferred over IWAE due to \u03c6-gradient breakdown in IWAE and the direct optimization of the proposal distribution by RWS. The study formalizes this argument by generalizing the RWS algorithm to create an adaptive importance-sampling framework called AISLE. In Section 3, the AISLE framework is introduced, which allows for the derivation of various gradient estimators in a principled manner. The IWAE-STL gradient can be recovered as a special case of AISLE, indicating a connection between the two methods. This work provides a theoretical foundation for IWAE-STL, which was previously only heuristically understood. Our work provides a theoretical foundation for IWAE-STL and proves that AISLE also admits the IWAE-DREG gradient as a special case. The learning rate for IWAE \u03c6-gradient should be scaled as O(K), while AISLE does not require scaling with K. AISLE leads to a new family of gradient estimators for \u03b1-divergences. The supplementary materials discuss the impact of self-normalization bias on importance-sampling based gradient approximations and compare algorithms empirically. The focus of the work is not to derive new algorithms or determine the best AISLE special case. Algorithms are compared empirically on Gaussian models, with extensive comparisons available in other works. Notation shorthand is used for concise notation. Expectations of test functions can be estimated using IID samples. The self-normalised importance sampling estimate approximates expectations of a function. The importance weighted autoencoder aims to maximize a lower bound on the log-marginal likelihood. The IWAE tightens the evidence bound by optimizing the inference network parameters. For K > 1, it extends the VAE on an auxiliary-variable construction. The IWAE objective gradient can be approximated using Monte Carlo, but high variance makes it noisy. Reparameterization is used to reduce this variance. The IWAE uses reparameterization to reduce the high variance in the objective gradient approximation, which is noisy. The gradient has drawbacks related to the 'score-function' terms in the \u03c6-gradient portion. The 'score-function' terms in the \u03c6-gradient portion of the IWAE have drawbacks, including reliance on reparametrisations to reduce high variance and issues with signal-to-noise ratio. Modifications have been proposed to address these challenges. Two modifications of \u2207 iwae \u03c6 \u03b8, z have been proposed to avoid score-function terms in the IWAE gradient. IWAE-STL and IWAE-DREG gradients aim to achieve stable signal-to-noise ratio and zero variance under certain conditions. The RWS algorithm and the use of self-normalised importance sampling approximation are also discussed. The RWS algorithm utilizes self-normalized importance sampling approximation to optimize both \u03b8 and \u03c6 simultaneously. The bias relative to importance sampling is of order O(1/K), impacting the \u03c6-gradient. The lack of a joint objective for \u03b8 and \u03c6 is a drawback, but RWS aims to minimize this bias. In high-dimensional settings, self-normalized weights are mainly supported on the two particles with the largest weights. The RWS algorithm optimizes \u03b8 and \u03c6 simultaneously using self-normalized weights. Optimizing \u03c6 can reduce error in the \u03b8-gradient approximation. Adapting the proposal distribution in importance-sampling schemes may not always require minimizing the KL-divergence. Alternative techniques like minimizing the \u03c7 2 -divergence exist in the literature. The RWS-objective is slightly generalized as maximizing log Z \u03b8 for \u03b8 and minimizing D\u0192(\u03c0\u03b8 q \u03c6) for \u03c6, where D\u0192 is some \u0192-divergence from p to q. The unified framework presented in this work allows for the derivation of robust \u03c6-gradient estimators that do not degenerate as K \u2192 \u221e. Optimization is done through stochastic gradient ascent, with the \u03b8-gradient being the same for all algorithms discussed. The AISLE algorithm interprets the \u03b8-gradient as a self-normalized importance-sampling approximation of the gradient \u2207 \u03b8 log Z \u03b8 for the 'exact' objective. The text discusses approximating the expectation and normalizing constant Z \u03b8 using the vanilla Monte Carlo method, with a bias of order O(K \u22121 ) and standard deviation of order O(K \u22121/2 ). It also mentions optimizing \u0192-divergences in intractable models without relying on Z \u03b8, and using self-importance sampling for approximating integrals. The text discusses self-importance sampling for approximating integrals, reparametrised estimators, and deriving IWAE-STL in a principled manner from AISLE without the need for a multi-sample objective. IWAE-STL can be derived from AISLE without relying on a multi-sample objective, providing a theoretical basis for IWAE-STL. This approach exhibited good empirical performance even when RWS broke down, suggesting that the breakdown may not be due to RWS' lack of optimizing a joint objective. AISLE-KL is derived by applying Lemma 1 to the exact \u03c6-gradient, reducing bias and variance. The \u03b1-divergence between distributions p and q is expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz. Minimizing this divergence is natural in importance sampling. AISLE-\u03b1-NOREP, without reparametrisation, yields Equation (13) which is proportional to the 'score gradient'. AISLE-KL is derived by applying Lemma 1 to the exact \u03c6-gradient, reducing bias and variance. The special case of AISLE-\u03b1-NOREP yields Equation (13) proportional to the 'score gradient' from Dieng et al. (2017, Appendix G). IWAE-DREG can be derived from AISLE in a principled manner without the need for a multi-sample objective. The learning rate needs to be scaled as O(K) for the IWAE or IWAE-DREG \u03c6-gradients. The 'exclusive' KL-divergence is expressed as KL(q \u03c6 \u03c0 \u03b8 ) = f (w \u03c8 (z))q \u03c6 (z) dz + C(\u03b8) with f (y) = log(y). This leads to the approximation. The curr_chunk discusses the importance of optimising the 'exclusive' KL-divergence for faster convergence of \u03c6 in VAEs. It highlights the potential negative impact on learning of \u03b8 when minimising the exclusive divergence. The adaptive-importance sampling paradigm of RWS is preferred over IWAEs due to better performance. The curr_chunk discusses the role of self-normalisation bias in importance-sampling approximation, showing how the number of particles affects the accuracy of estimators in VAEs. The curr_chunk discusses the self-normalisation bias in importance-sampling approximations, particularly in the context of IWAEs and VAEs. It highlights the impact of the number of particles on the accuracy of estimators in VAEs. The curr_chunk discusses the importance of ensuring that the proposal distribution q \u03c6 is close to the target distribution \u03c0 \u03b8 in order to have well-behaved importance weights. It distinguishes between 'inclusive' and 'exclusive' KL-divergence and the flexibility of the family of proposal distributions Q. The curr_chunk emphasizes the need for the proposal distribution q \u03c6 to closely match the target distribution \u03c0 \u03b8 for well-behaved importance weights. It discusses the implications of insufficiently expressive Q and the potential consequences of minimizing the exclusive KL-divergence. It also mentions that using a gradient-descent algorithm to minimize the exclusive divergence may sometimes be preferable for faster convergence. The discussion in the curr_chunk highlights the importance of choosing the right number of particles, K, for faster convergence in some scenarios. Increasing K can help reduce the variance of gradient approximations, even if setting K = 1 may not be optimal. Additionally, optimizing the exclusive KL-divergence could lead to poorly behaved importance-sampling approximations when \u03c6 is far from optimal. The importance of selecting the appropriate number of particles, K, for faster convergence is emphasized. Increasing K can lower gradient approximation variance, but setting K = 1 may not be ideal. Optimizing the exclusive KL-divergence could result in suboptimal importance-sampling approximations when \u03c6 is far from optimal. Various \u03c6-gradient estimators are compared, including AISLE-KL-NOREP and AISLE-KL. The gradient for AISLE based on different divergences is discussed, including KL-divergence and \u03c72-divergence, with and without reparametrization. The gradient for IWAE employing the reparametrization trick is also mentioned. The importance of selecting the appropriate number of particles for faster convergence is emphasized. The IWAE gradient with the reparametrization trick is discussed, along with IWAE-DREG and RWS-DREG gradients. The joint law of observations and latent variables is factorized, with a known matrix D and specific calculations for \u03b8. The proposal distribution is a fully factored Gaussian with parameters to optimize. The mean of the proposal coincides with the mean of the posterior under certain conditions. This model is similar to benchmarks in previous studies, with slight variations for a more realistic scenario. In a more realistic scenario, latent vectors z can be correlated in the generative model. The variational approximation, however, remains fully factored and may not fully capture uncertainty about the latent variables. The variance of certain gradients approaches zero as C \u2192 1 2 I, indicating near-zero variance for proposal mean parameters in this model. The text discusses the benefits of reparametrization-trick gradients in Gaussian settings and compares algorithms for varying particle numbers and model dimensions. The generative model is specified, and results are shown for different model settings. The text discusses the benefits of reparametrization-trick gradients in Gaussian settings and compares algorithms for varying particle numbers and model dimensions. Results are shown for different model settings, including scenarios where the generative model cannot fully mimic the dependence structure of the latent variables. Gradient-ascent algorithms are initialized with standard normal distributions and different optimization methods are used. The total number of iterations is 10,000 with learning-rate parameters at each step. The covariance matrix is not diagonal, with logarithmic scaling on the second axis."
}