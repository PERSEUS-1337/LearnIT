{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models for learning from uncertain supervision and inferring soft-inclusions among concepts. The Box Lattice model by Vilnis et al. (2018) showed promising results in modeling soft-inclusions through high-dimensional hyperrectangles. However, the hard edges of the boxes present optimization challenges. This work presents a novel hierarchical embedding model inspired by the Box Lattice model. In this work, a novel hierarchical embedding model is introduced, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions. The approach improves optimization robustness in the disjoint case and demonstrates enhanced performance on various tasks, especially with sparse data. Embedding methods play a crucial role in converting semantic problems into geometric ones in machine learning. Recent years have seen a resurgence in interest in structured or geometric representations for information retrieval. Instead of simple points in a vector space, methods now associate objects like images, words, and sentences with complex geometric structures such as density functions, convex cones, or hyperrectangles. These geometric objects better capture ideas of asymmetry, entailment, and ordering, providing a strong bias for various tasks. This work focuses on the probabilistic Box Lattice model. The focus is on the probabilistic Box Lattice model BID22, known for its strong empirical performance in modeling transitive relations and complex joint probability distributions. Box embeddings replace vector lattice ordering with overlapping boxes, but face challenges in gradient-based optimization when boxes become disjoint despite overlap in the ground truth. The Box Lattice model BID22 uses box embeddings to model transitive relations and joint probability distributions. Challenges arise when boxes become disjoint despite overlap in the ground truth. To address this, a new model is proposed that smooths the hard edges of the boxes into density functions using Gaussian convolution, showing superior results in various tasks. In the market basket dataset, we achieve state-of-the-art results and improve in the pseudosparse regime. Relevant work includes order embeddings of BID20, a probabilistic extension by BID9, and the box lattice model of BID22. Another model, box embeddings by BID18, offers a different interpretation with deterministic edge presence criteria. In the context of learning hierarchical embeddings, a nonprobabilistic model based on embedding points in hyperbolic space has been proposed. The model optimizes an energy function and aims to learn tree structures. To improve the energy landscape, Gaussian convolution is used, a technique common in mollified optimization and continuation methods in machine learning models. Our focus is on embedding orderings and transitive relations in knowledge graph embedding. We aim to learn an embedding model mapping concepts to subsets of event space, with an inductive bias for transitive relations and fuzzy concepts of inclusion and entailment. We introduce methods for representing ontologies as geometric objects using order theory and vector and box lattices. FIG0 illustrates these representations in a simple two-dimensional example. A poset is a binary relation that generalizes the concept of a totally ordered set, allowing for incomparable elements. Posets are useful for representing acyclic directed graph data with transitive relations. A lattice, a type of poset, has a unique least upper bound and greatest lower bound for any subset of elements. In a bounded lattice, there are additional elements denoting the least upper bound and greatest lower bound of the entire set. Lattices have binary operations for join and meet, satisfying specific properties. The extended real numbers form a bounded lattice and a totally ordered set. The real numbers, R \u222a {\u2212\u221e, \u221e}, form a bounded lattice under min and max operations. Sets partially ordered by inclusion also form a lattice with \u2229 and \u222a as operations. The fourth property is absorption. Swapping \u2227 and \u2228 operations and reversing the poset relation gives a valid lattice, called the dual lattice. A semilattice has only a meet or join. A vector lattice, or Riesz space, is a vector space with a lattice structure. The Order Embeddings of BID20 embed partial orders as vectors using the reverse product order, corresponding to the dual lattice, and restrict the vectors to be positive. The vector of all zeroes represents, and embedded objects become \"more specific\" as they get farther away from the origin. FIG0 demonstrates a toy, two-dimensional example of the Order Embedding vector lattice representation of a simple ontology. Shading represents the probability measure assigned to this lattice in the probabilistic extension of BID9. Vilnis et al. FORMULA2 introduced a box lattice, wherein each concept in a knowledge graph is associated with two vectors, the minimum. The box lattice in a knowledge graph is represented by two vectors for each concept, forming a lattice structure with partial order. The lattice meet is the largest box contained in both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are determined by the volume of boxes and their intersections under a suitable probability measure. The probability measure for events in a box lattice is determined by the volume of boxes, their complements, and intersections under a uniform measure. The use of the uniform measure constrains boxes to the unit hypercube, ensuring probabilities are \u2264 1. When using gradient-based optimization for learning box embeddings, an issue arises when two concepts are incorrectly labeled as disjoint, leading to zero gradient flow due to zero intersection volume. The authors propose a surrogate function to optimize in cases of (pseudo-)sparse lattices where most boxes have little or no intersection. They suggest using a more principled framework to develop alternate measures that avoid optimization issues and improve model quality. The authors introduce a new approach to optimize sparse lattices by relaxing the hard edges of standard box embeddings. They rewrite the joint probability measure of intervals as an integral of the product of indicator functions. The authors propose using kernel smoothing with Gaussian kernels to replace indicator functions with infinite support in optimizing sparse lattices. This approach involves applying the diffusion equation to the original embeddings and leads to a closed form solution for evaluating lattice elements. The solution to equation 2 involves the softplus function and the logistic sigmoid. In the zero-temperature limit, the formula converges to the original equation 1. The function lattice does not support a valid meet operation due to idempotency issues. A modification of equation 3 allows for a function p(x \u2227 x) = p(x) while maintaining optimization properties. The hinge function m h satisfies a specific identity, unlike the softplus function. Equation 6 holds true for both the hinge function and the softplus. Equation 7 is idempotent for overlapping intervals, leading to the definition of probabilities p(x) and p(x, y). Softplus, bounding the hinge function, requires normalization for values greater than 1. In experiments with a small number of entities, boxes are normalized by dividing each dimension by the global minimum and maximum size. For data where repeated computation is not feasible, projection onto the unit hypercube and normalization by m soft (1) is used. The final probability p(x) is the product over dimensions. This approach retains the inductive bias of the original model. Our approach in the box model retains the inductive bias, equivalent in the limit, and ensures p(x, x) = p(x). Comparing 3 functions in FIG2, the softplus overlap performs better for disjoint boxes than the Gaussian model. Experiments on WordNet hypernym prediction show improvements in practice. The WordNet hierarchy has 837,888 edges. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, requiring less hyper-parameter tuning. Further experiments are conducted using different numbers of positive and negative examples from the WordNet mammal subset to confirm the performance in the sparse regime. The training data for the WordNet mammal subset contains 1,176 positive examples, with 209 positive examples in the dev and test sets. Negative examples are generated randomly. Models like OE baseline, Box, and Smoothed Box perform well with balanced data, but Smoothed Box outperforms the others on imbalanced data. This is crucial for real-world entailment graph learning. Experiments were conducted on the Flickr entailment dataset to compare F1 scores of different models under label imbalance. The Flickr entailment dataset contains 45 million image caption pairs. Experimental results show a slight performance gain compared to the original model, with improvements focused on unseen captions. The method is also applied to a market-basket task using the MovieLens dataset. The task involves predicting users' preference for movie A based on their liking of movie B using the MovieLens dataset. Pairs of user-movie ratings higher than 4 points are collected from the dataset, resulting in 8545 movies. Various models are compared, including low-rank matrix factorization and hierarchical embedding methods. Separate embeddings are used for target and conditioned movies, with an additional vector for the \"imply\" relation in the complex bilinear model. Evaluation is done using KL divergence and Pearson correlation on the test set. Our smoothed box embedding method outperforms the original box lattice and all other baselines, especially in Spearman correlation, a key metric for recommendation tasks. The model is easier to train with fewer hyperparameters and achieves state-of-the-art results on various datasets. The model outperforms baselines, especially in Spearman correlation for recommendation tasks. It is easier to train with fewer hyperparameters and achieves state-of-the-art results on various datasets. The research explores learning problems in geometrically-inspired embedding models and aims to continue exploring function lattices and constraint-based approaches. A proof of Gaussian overlap formula is provided for evaluating lattice elements. The antiderivative of \u03c6 is the normal CDF, allowing us to evaluate the integral of interest. Fubini's theorem is applied to simplify the equation, leading to the MovieLens dataset being suitable for optimization by the smoothed model. Additional experiments test the model's robustness to initialization. The study explores the model's robustness to disjoint boxes by adjusting the width parameter of the initial distribution. Results show that the smoothed model performs well even with disjoint initialization, while the original box model degrades significantly. This suggests that the smoothed model's strength lies in its ability to optimize smoothly. The smoothed box model excels in optimizing smoothly in the disjoint regime. Methodology and hyperparameter selection details for experiments can be found at https://github.com/Lorraine333/smoothed_box_embedding. The model is evaluated on the development set for a fixed number of epochs in WordNet experiments. Baseline models use BID22 parameters, while the smoothed model's hyperparameters are chosen based on the development set. Negative examples are randomly generated for each batch of positive examples in the 12 experiments conducted. A parameter sweep is done for all models to select the best results. The experimental setup involves using a single-layer LSTM model with box embeddings parameterized by min and delta. The model is trained for a fixed number of epochs and evaluated on the development set. Hyperparameters are determined on the development set, and the best model is used to score the test set. Optimization is stopped if the development set score does not improve after 200 steps."
}