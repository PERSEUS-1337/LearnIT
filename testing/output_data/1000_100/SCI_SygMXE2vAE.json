{
    "title": "SygMXE2vAE",
    "content": "BERT and other Transformer-based models have achieved state-of-the-art results in Natural Language Processing tasks. A layer-wise analysis of BERT's hidden states reveals valuable information beyond attention weights. The focus is on models fine-tuned for Question Answering, examining how token vectors are transformed to find answers. Probing tasks show the information stored in each representation layer, with visualizations providing additional insights. Our qualitative analysis of BERT's hidden state visualizations reveals insights into its reasoning process. The system can implicitly incorporate task-specific information into token representations, with little impact from fine-tuning on semantic abilities. Prediction errors can be detected in early layer vector representations. Transformer models have gained popularity in Natural Language Processing, surpassing RNNs in Machine Translation. Large-scale pre-training has further boosted their prevalence. The paper discusses BERT BID8, a popular Transformer model in Natural Language Processing. It highlights the system's ability to incorporate task-specific information and detect prediction errors in early layer representations. Transformer models have become more popular than RNNs in Machine Translation due to their large-scale pre-training. BERT BID8 is a popular Transformer model that has shown significant improvements over previous models in various benchmarks and tasks. The lack of transparency and reliability in black box models is a major issue. This paper takes a different approach by examining hidden states between encoder layers instead of focusing on attention values. This paper examines hidden states between encoder layers in Transformers to address questions about decompositional answering, task-solving differences in layers, fine-tuning effects, and network failure analysis in Question Answering tasks. The study focuses on BERT architecture and its implications for various Natural Language Processing tasks. The study focuses on the BERT architecture and its implications for various Natural Language Processing tasks. It proposes a layer-wise visualization of token representations to reveal information about the internal state of Transformer networks. Additionally, it applies NLP Probing Tasks and QA-specific tasks to analyze BERT's abilities within layers and how they are impacted by fine-tuning. The study shows that BERT's transformations go through similar phases, even when fine-tuned on different tasks, with general language properties encoded in earlier layers and used to solve downstream tasks in later layers. The study focuses on BERT and Transformer models like GPT-2, Universal Transformer, and TransformerXL. It discusses interpretability and probing tasks in NLP. Interpretability and probing tasks in NLP have become a significant area of research, with a focus on applying probing tasks to trained models like BERT. Recent advances include a novel \"edge-probing\" framework that examines both semantic and syntactic information in contextualized word embeddings. Recent research in NLP has focused on interpretability and probing tasks, particularly on models like BERT. Various studies have analyzed attention values in different layers of BERT and performance based on different layers. Some research has also explored models through qualitative visual analysis, such as examining phoneme recognition in DNNs and training diagnostic classifiers to support hypotheses. Li et al. have looked at word vectors and the importance of specific dimensions. Li et al. BID17 analyze the importance of specific dimensions in word vectors for sequence tagging and classification tasks. Their work is distinct from Liu et al. BID20, who focus on probing pre-trained models without considering fine-tuned models or specific phases of BERT. Their motivation stems from Jain and Wallace BID15, who argue that attention may not be effective for explainability and interpretability, proposing a reevaluation of hidden states and token evaluation methods. In analyzing fine-tuned BERT models, the focus is on evaluating hidden states and token representations. Two approaches are taken: qualitative analysis of transforming token vectors in vector space and quantitative analysis of language abilities on QA-related tasks. The architecture of BERT allows tracking transformations of each token throughout the network, enabling an analysis of changes in token representations in every layer. Hidden states are collected from each layer for both correctly and falsely predicted samples for qualitative analysis of these transformations. The text discusses the process of collecting hidden states from each layer of BERT models to analyze token representations. Dimensionality reduction techniques like PCA are applied to visualize semantic relations between tokens in a two-dimensional space. The text discusses using PCA to visualize distinct clusters in data and applying k-means clustering to verify the distribution in high-dimensional space. Semantic probing tasks are used to analyze information stored in transformed tokens after each layer in BERT models. Edge Probing principle is applied to understand how language information is maintained or forgotten by the model. The text discusses using Edge Probing to analyze language tasks like Named Entity Labeling, Coreference Resolution, and Relation Classification. Additional tasks of Question Type Classification and Supporting Fact Identification are included for Question Answering. Named Entity Labeling involves predicting entity categories based on token spans. Coreference Resolution requires predicting coreferences in text. The Coreference task involves predicting if two mentions refer to the same entity, while Relation Classification predicts the relation type between two entities. Question Type Classification identifies the type of question being asked. Source code for experiments will be made publicly available. The extraction of Supporting Facts is crucial for Question Answering tasks, especially in multi-hop scenarios. By analyzing BERT's token transformations, we aim to understand how important context parts are distinguished from distracting ones. A probing task is constructed to identify Supporting Facts, testing the hypothesis that token representations contain information about their relevance to the question. HotpotQA and bAbI datasets provide sentencewise Supporting Facts, while SQuAD focuses on single-hop reasoning. The probing task involves labeling samples as supporting facts or not based on BERT token embeddings. Only tokens of labeled edges within a sample are considered for classification, with tokens pooled for a fixed-length representation. The probing task involves classifying tokens for fixed-length representation and feeding them into a MLP classifier for predicting label-wise probability scores. Pretrained BERT models are analyzed without fine-tuning to understand their abilities. Question Answering tasks like SQUAD, bAbI, and HotpotQA are considered to diversify the analysis results. Detention is a common punishment in schools in various countries, including the UK and Ireland. HotpotQA is a Multihop QA task with 112,000 question-answer pairs designed to combine information from different parts of a context. The context in HotpotQA includes supporting and distracting facts with an average size of 900 words. The QA bAbI tasks are artificial toy tasks designed to test neural models' abilities in reasoning over multiple sentences. These tasks require Positional Reasoning, Argument Relation Extraction, and Coreference Resolution. The tasks are simple with a vocabulary size of 230 and short contexts. The analysis is based on BERT BID8 and GPT-2 BID28 models, both of which are Transformers. Both BERT and GPT-2 are Transformer models that build upon previous ideas. They have similar architectures, with GPT-2 focusing on the decoder half and BERT utilizing both encoder and decoder. The models are fine-tuned on datasets and use specific pre-trained versions for experiments. The models are fine-tuned on datasets by adjusting hyperparameters and training modalities. Input length varies for different tasks, with evaluations conducted to select the best model. Different settings like span prediction and sequence classification are considered for tasks like bAbI. In order to make span prediction work for HotpotQA, all possible answers are appended to the base context. The HotpotQA Support Only task simplifies the task by using only Supporting Facts as the question context, while the HotpotQA Distractor task includes distracting sentences within the 512 token limit. Training results show high accuracy on the SQuAD task, nearing human performance, but tasks derived from HotpotQA are more challenging. The tasks from HotpotQA are more challenging, especially the distractor setting. BERT and GPT-2 easily solve bAbI tasks, but GPT-2 performs better on bAbI compared to SQuAD and HotpotQA. BERT struggles with tasks 17 and 19 in bAbI, which require positional reasoning. Qualitative analysis reveals recurring patterns in vector transformations. Examples from SQuAD and bAbI are presented, while HotpotQA examples are in supplementary material due to space constraints. Results from probing tasks on BERT models are compared in macro-averaged F1 over all network layers. The PCA representations show multiple phases in answering questions across different QA tasks. The first phase involves semantic clustering in early layers of BERT-based models. In the middle layers of BERT-based models, entities are connected by their relation within a certain input context rather than topical similarity. Task-specific clusters filter question-relevant entities, as shown in FIG4 with words like countries, schools, detention, and country names. In BERT-based models, task-specific clusters filter question-relevant entities like schools, detention, and country names. The model's ability to recognize entities, identify mentions, and find relations improves in higher network layers. Named Entity Labeling, Coreference Resolution, and Relation Recognition are visualized in FIG7. In BERT-based models, higher network layers improve the recognition of Named Entities, coreferences, and relations. BERT models match question tokens with relevant context tokens by transforming token representations, as shown in FIG4. This step is crucial for Question Answering and Information Retrieval. The BERT-based models show improved ability to distinguish relevant information in higher layers, as seen in probing tasks. Performance for this task increases over successive layers for SQuAD and bAbI datasets. However, the fine-tuned HotpotQA model does not reach high accuracy, indicating its inability to identify correct Supporting Facts. Vector representations help in retracing decisions and making the model more transparent. The model dissolves previous clusters in the last network layers, separating correct answer tokens from the rest. The vector representation at this stage is task-specific and learned during fine-tuning, leading to a performance drop in NLP probing tasks. The fine-tuned BERT model on HotpotQA loses the ability to perform well on certain tasks, resembling human reasoning phases. The first phase of semantic clustering in BERT involves basic language knowledge, while the second phase focuses on building relations between context parts. Separating important information and grouping answer candidates are also part of the process, with BERT able to run multiple processes concurrently. Comparing BERT to GPT-2, one major difference lies in how they handle hidden states. GPT-2's hidden states focus on the first token of a sequence, leading to a separation of clusters in all layers except for the Embedding Layer, the first Transformer block, and the last one. The first token is masked during dimensionality reduction in further analysis. GPT-2, like BERT, separates relevant information in the vector space but also extracts additional sentences with similar meaning. Unlike BERT, GPT-2 does not distinctly separate the correct answer. The findings in GPT-2 show that unlike BERT, the correct answer \"cats\" is not clearly separated but remains part of its sentence. This observation extends beyond BERT to other Transformer networks. Future work will involve more probing tasks to confirm this initial finding. Visualizations can reveal failure states in neural networks, showing when, why, and how they fail. Hidden state representations can also indicate the difficulty of a specific task. When network confidence is low, transformations in the layers do not follow the usual phases. Tokens are mostly kept in a single cluster, with little relevance to the prediction. In some cases, the network maintains a 'Semantic Clustering' phase similar to Word2Vec, especially when overall confidence is low. The network maintains a 'Semantic Clustering' phase similar to Word2Vec, even in later layers. Fine-tuning has little impact on the core NLP abilities of the model. Pretrained model holds sufficient information about words and their relations, making it successful in multiple downstream tasks. Fine-tuning only applies small weight changes and forces the model to forget some information. The model retains positional embedding, crucial for Transformer network performance. The positional embedding in Transformers is crucial for performance, even in later layers. Fine-tuning on SQuAD dataset improves ability to resolve question types, while bAbI tasks show a loss in this ability. Static structure of bAbI samples may be the cause. The static structure of bAbI samples may cause the model to recognize answer candidates based on sentence structure and word patterns rather than question type. Fine-tuning on HotpotQA does not significantly improve performance compared to the model without fine-tuning. Transformer networks store interpretable information in hidden states, which can help identify misclassified examples and model weaknesses. The curr_chunk discusses the importance of understanding which parts of the context are crucial for model decision-making, the transferability of lower layers in Transformer networks for different tasks, and the modularity of specific layers in solving different problems. It suggests further research on methods to process this information, choosing layer depth for Transfer Learning tasks, and exploring skip connections in Transformer layers for information transfer between non-adjacent layers. Our work aims to reveal internal processes in Transformer-based models, suggesting research to understand state-of-the-art models and improve their performance by focusing on specific tasks during pre-training instead of using end-to-end language model tasks."
}