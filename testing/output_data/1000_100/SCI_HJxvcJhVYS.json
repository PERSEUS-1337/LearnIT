{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A proposed method uses Bayesian optimization to approximate inference by applying Stein variational gradient descent on Gaussian process model estimates. Initial results show promise for likelihood-free inference in reinforcement learning environments. Estimating parameters of a physical system in reinforcement learning environments often requires likelihood-free methods like approximate Bayesian computation or conditional density estimation. Recent methods aim to improve efficiency by using conditional density estimators from joint data or mixture density networks. This paper explores combining variational inference methods with Bayesian optimization to efficiently estimate parameters of physical systems in reinforcement learning environments. The approach involves using a Thompson sampling strategy to refine variational approximations to a black-box posterior and proposing parameters for new simulations using Stein variational gradient descent. The approach combines variational inference with Bayesian optimization to estimate simulator parameters efficiently. It uses Stein variational gradient descent over Gaussian process samples and optimally subsamples variational approximations for batch evaluations. The goal is to approximate a posterior distribution without access to a likelihood function, using a discrepancy measure between simulator outputs and observations. The method minimizes the discrepancy between the approximated distribution and the target distribution using kernelised Stein discrepancy. The approach uses a black-box method to solve Equation 1 without requiring gradients of the target distribution. It involves a GP model for likelihood approximation, Thompson sampling for candidate selection, and kernel herding for parameter sampling. By bypassing the parameter space mapping, the algorithm learns q directly via Stein variational gradient descent. The approach utilizes a GP to model g : \u03b8 \u2192 \u2212\u2206 \u03b8 for synthetic likelihood estimation. The simulations-observations discrepancy \u2206 \u03b8 is costly to evaluate and non-differentiable, so the GP provides a cheap and differentiable approximation. Candidate distributions q n \u2208 Q are selected using Thompson sampling. Thompson sampling is used for Bayesian optimization problems, selecting point candidates \u03b8 \u2208 \u0398. It accounts for uncertainty by sampling functions from the GP posterior, such as in sparse spectrum Gaussian processes (SSGPs). The acquisition function is defined based on the target posterior approximation using SVGD with particles initialized from the prior distribution. Thompson sampling is utilized for Bayesian optimization, selecting candidates \u03b8 \u2208 \u0398. It involves sampling functions from the GP posterior, like in SSGPs. The acquisition function is defined based on the posterior approximation using SVGD with particles initialized from the prior distribution. The particles are optimized through smooth perturbations using the SSGP kernel, guiding them towards local maxima of logp n and encouraging diversification. Gradients of logp n are available for SSGP models with differentiable mean functions. Running evaluations of \u2206 \u03b8 from samples \u03b8 \u223c q n updates the GP model. Representing q by a large number of particles M improves the algorithm. The algorithm improves with a large number of particles M by exploring the approximate posterior surface using SVGD. However, to run simulations efficiently, a subset of query parameters {\u03b8 n,j } S j=1 is selected through optimal subsampling of candidate q n using kernel herding. This minimizes error in empirical estimates under distribution q, bounded by maximum mean discrepancy (MMD). The algorithm for SSGPs involves subsampling query parameters using kernel herding to select informative samples based on GP posterior kernel. The distributional Bayesian optimization (DBO) algorithm is summarized in Algorithm 1, with experimental results comparing it against mixture density networks (MDNs) in synthetic data scenarios. The proposed method is compared to mixture density networks (MDNs) in synthetic data scenarios using OpenAI Gym's 3 cart-pole environment. A dataset is generated with fixed physics parameters, and discrepancies are calculated. Results show the method recovers the target system's posterior curve-shaped distribution better than MDNs. The paper presents a Bayesian optimization approach for inverse problems on simulator parameters, showing that distributional Bayesian optimization outperforms MDNs in terms of MMD. Results demonstrate the method's potential for reinforcement learning applications, offering a more sample-efficient approach for inferring parameters in a classical reinforcement learning environment. Future work includes scalability and theoretical analysis. The paper introduces a Bayesian optimization method for inverse problems on simulator parameters, demonstrating superior performance compared to MDNs in terms of MMD. It highlights the potential for reinforcement learning applications by offering a more sample-efficient approach for parameter inference. The method allows for fast incremental updates of the GP posterior with constant time complexity O(M^2) regardless of the number of data points N."
}