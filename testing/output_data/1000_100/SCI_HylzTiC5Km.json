{
    "title": "HylzTiC5Km",
    "content": "The Subscale Pixel Network (SPN) is proposed as a conditional decoder architecture to generate high fidelity images by encoding vast previous context and preserving global semantic coherence and detail exactness. It uses multidimensional upscaling to grow images in size and depth efficiently. The Subscale Pixel Network (SPN) proposes using multidimensional upscaling to grow images in size and depth efficiently. It achieves state-of-the-art likelihood results in various settings and sets new benchmarks in unexplored scenarios for image generation. Autoregressive models have high fidelity and generalize well on held-out data in various domains like text, audio, images, and videos. However, in large-scale image generation, AR models struggle with long-range structure and semantic coherence. The relationship between model scores and sample fidelity poses challenges in achieving high-quality image generation. The relationship between model scores and sample fidelity in large-scale image generation poses challenges due to the high dimensionality of images and the need for architectural connections to learn dependencies among positions. The challenges in large-scale image generation include high memory and computation requirements. The goal is to learn the distribution of 8-bit RGB images up to 256 \u00d7 256 size with high fidelity, focusing on visually salient subsets of the distribution. The distribution of 8-bit RGB images up to 256 \u00d7 256 size is learned by focusing on visually salient subsets. This includes sub-images of smaller size and the few most significant bits of each RGB channel. Multidimensional Upscaling is used to map between these subsets by upscaling images in size or depth. Three networks are trained: a decoder for small size, low depth image slices, a size-upscaling decoder, and a depth upscaling network. The Subscale Pixel Network (SPN) architecture is developed to address difficulties in training decoders for small size, low depth image slices, size-upscaling decoders, and depth upscaling networks. SPN divides images into sub-images and generates them one slice at a time, capturing a form of size upscaling. It consists of a conditioning network and a decoder that predicts a target slice given context embedding, sharing weights for image slices with the same spatial structure. The Subscale Pixel Network (SPN) acts on image slices with the same spatial structure, sharing weights. It can be used for size upscaling and is evaluated on CelebAHQ-256 and ImageNet benchmarks, achieving state-of-the-art results. The SPN shows benefits in multidimensional upscaling and improves sample fidelity. The Subscale Pixel Network (SPN) demonstrates strong benefits in multidimensional upscaling and sample fidelity. It produces high-quality CelebAHQ-256 and ImageNet-128 samples at full 8-bit resolution, setting a fidelity baseline for future methods. The SPN's impact on sample quality is evident, showcasing its effectiveness in generating images with intrinsic measures of generalization. The ordering of row, column, and color channel indices in AR models is conventionally done in raster scan. A deep neural network parametrizes each conditional distribution. An alternative ordering divides large images into equally sized slices, allowing for compact encoding of long-range dependencies and inducing spatial structure. This ordering enables the same decoder to be applied consistently to all slices within the Subscale Pixel Network (SPN), facilitating the use of self-attention without local contexts. The Subscale Pixel Network (SPN) allows for the ordering of slices in large images, enabling the use of self-attention without local contexts. The subscale ordering captures size upscaling implicitly and facilitates the application of a consistent decoder to all slices within the network. Size upscaling can be performed explicitly by training a single slice decoder on subimages to generate the first slice of a subscale ordering. The Subscale Pixel Network (SPN) captures this subscale ordering implicitly and can act as a full-blown image model as well as a size upscaling model. Another formulation is the Parallel Multi-Scale BID12 ordering, where pixels in an image are doubled at every stage by distinct neural networks in parallel. Depth upscaling is performed in stages by distinct neural networks, generating image bits in parallel without sequentiality. It applies upscaling not only in height and width but also in channel depth. Each stage generates the most significant bits of an image, with subsequent stages conditioned on previous bits. Weights are not shared among networks at different stages of depth upscaling. Depth upscaling is performed in stages by distinct neural networks, generating image bits in parallel without sequentiality. The goal of multidimensional upscaling is to focus on visually salient bits of an image. Existing AR approaches require superlinear computation and memory, with self-attention becoming limiting for larger images. The Subscale Pixel Network (SPN) addresses challenges in modeling global dependencies in large images by using a scaling factor to obtain slices of the original image, allowing for efficient processing of high-resolution images. The Subscale Pixel Network (SPN) uses a scaling factor to obtain slices of the original image, ensuring efficient processing of high-resolution images. The SPN architecture consists of an embedding part for slices at preceding metapositions that conditions the decoder for the current slice being generated. The embedding part is a convolutional neural network with residual blocks that takes input from preceding slices concatenated along the depth dimension, maintaining relative meta-positions of each slice with respect to the target slice. The Subscale Pixel Network (SPN) utilizes a scaling factor to process high-resolution images efficiently. The embedding part of the architecture conditions the decoder for the current slice by aligning slices in the same position along the depth axis. It also incorporates meta-position and pixel intensity values as inputs for the embedding network, which then passes through self-attention layers. The decoder takes the encoded slice tensor in a position-preserving manner, using representations of pixels from preceding slices. The decoder in the hybrid architecture combines masked convolution and self-attention to process the target slice in raster-scan order. It utilizes 1D self-attention layers to gather context, reshapes the slice into a 1D tensor, and uses masked 1D self-attention layers. The output is reshaped back into a 2D tensor and concatenated with the output of the slice embedding network for conditioning input to a Gated PixelCNN. The PixelCNN network models the target slice with full masking over pixels and channel dimensions. The decoder in the hybrid architecture combines masked convolution and self-attention to process the target slice efficiently. Memory requirements are significantly lower due to the compact concatenation along the channel dimension. Maximum likelihood learning is achieved through stochastic gradient descent on a Monte Carlo estimate. The SPN serves as a size-upscaling network when initialized with an externally generated subimage. The SPN decoder upscales image depth by dividing the image into slices and concatenating them along the channel dimension. The model is trained on low bit depth data and can generate high fidelity samples at high resolution. Our model demonstrates high fidelity samples at high resolution, outperforming the Glow model BID7 and improving MLE scores on CelebA-HQ samples. It also achieves state-of-the-art log-likelihoods on high-resolution ImageNet images, with the first benchmark on 256x256 ImageNet. The networks operate on small images (32 \u00d7 32 slices) allowing for large networks in terms of hidden units and network depth. The context-embedding network and masked decoder have specific layer configurations depending on the dataset. The 1D Transformer in the decoder has 8 to 10 layers depending on the dataset. The hybrid decoder performs well on 32x32 and 64x64 Downsampled ImageNet, achieving state-of-the-art log-likelihoods. SPN hurts performance in low-resolution settings, but scores similarly to PixelSNAIL at higher resolutions. The improvement over Glow in the 5-bit setting is significant. The improvement over Glow in the 5-bit setting is significant, with SPN improving log-likelihood from 3.55 bits/dim to 3.08 bits/dim on 128 \u00d7 128 ImageNet. Samples with depth upscaling show semantic coherence and multidimensional upscaling increases overall sample success rate. High-fidelity 256 \u00d7 256 ImageNet samples can be produced. The SPN and Multidimensional Upscaling model achieves high-fidelity samples of celebrity faces from the CelebAHQ dataset at 256 \u00d7 256 resolution, outperforming other models like Glow and GANs BID6. The MLE scores are significantly improved compared to previous reports. The model addresses the challenge of learning complex natural image distributions, showcasing samples at different bit depths and temperature settings. This advancement marks a significant step towards solving the problem of generating high-quality images from complex domains like CelebAHQ-256. The model achieves state-of-the-art MLE scores on large-scale images from complex domains like CelebAHQ-256 and ImageNet-128. It can generate high fidelity 8-bit samples without altering the sampling process. The samples show semantic coherence and exactness of details even at large scales. Experiments are conducted at a large scale in terms of compute and network size. The model achieves state-of-the-art MLE scores on large-scale images from complex domains like CelebAHQ-256 and ImageNet-128. Experiments are conducted at a fairly large scale in terms of compute and network size, with batch sizes up to 2048 achieved through data parallelism on Google Cloud TPU pods. SPN architectures have between \u223c50M and \u223c250M parameters depending on the dataset. The SPN architecture for each dataset involves depth-upscaling and size-upscaling, doubling the number of parameters. The maximal parameter count reaches \u223c650M in the multidimensional upscaling setting for ImageNet 128, with a decoder-only network having \u223c150M parameters."
}