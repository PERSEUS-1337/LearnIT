{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization is a bilevel optimization problem where optimal parameters depend on hyperparameters. Regularization hyperparameters for neural networks are adapted by fitting approximations to the best-response function. Scalable best-response approximations are constructed by modeling the best-response as a single network with gated hidden units. This model is fitted using a gradient-based hyperparameter optimization algorithm. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training without the need for differentiation of the training loss. This allows us to tune discrete hyperparameters, data augmentation, and dropout probabilities. Empirically, STNs outperform other hyperparameter optimization methods on large-scale deep learning problems. Regularization hyperparameters such as weight decay, data augmentation, and dropout are crucial for neural network generalization but are difficult to tune. Popular hyperparameter optimization approaches include grid search, random search, and Bayesian optimization. Hyperparameter optimization methods like grid search, random search, and Bayesian optimization are commonly used but can be inefficient for high-dimensional spaces. Formulating hyperparameter optimization as a bilevel optimization problem can lead to faster convergence. The best-response function can be used to minimize validation loss directly through gradient descent. Hyperparameter optimization methods like grid search, random search, and Bayesian optimization are commonly used but can be inefficient for high-dimensional spaces. The validation loss can be minimized directly by gradient descent using Equation 2, offering speed-ups over black-box methods. Approximating the best-response w * with a parametric function \u0175 \u03c6 is proposed, optimizing \u03c6 and \u03bb jointly. Constructing a compact approximation by modeling the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of the hyperparameters is shown. Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering advantages over traditional optimization methods. They are easy to implement and ensure computational effort is not wasted. This online adaption yields hyperparameter schedules that outperform fixed schedules. The STN training algorithm does not require differentiating the training loss with respect to hyperparameters, allowing tuning of discrete hyperparameters. Empirical evaluation shows STNs outperform baseline methods on deep-learning problems with Penn Treebank and CIFAR-10 datasets. Bilevel optimization involves upper and lower-level problems, where the upper-level problem must be solved subject to optimality of the lower-level problem. Bilevel programs involve upper and lower-level problems, with the upper-level objective being the negative of the lower-level objective. These problems are used in various fields, including machine learning for tasks like hyperparameter optimization and neural architecture search. Despite being NP-hard, most work focuses on linear, quadratic, and convex functions, while we aim to find local solutions in nonconvex, differentiable, and unconstrained settings. In the nonconvex, differentiable, and unconstrained setting, the aim is to solve a problem subject to certain constraints using a gradient-based algorithm for speed. Simultaneous gradient descent may give incorrect solutions due to the dependence of parameters. A more principled approach involves using the best-response function to address this issue. The best-response function is used to convert a multi-level problem into a single-level problem, allowing for gradient descent optimization. Conditions for unique optima and differentiability are crucial, with Lemma 1 providing sufficient conditions for their existence in a neighborhood of a given point. The gradient of the upper-level objective decomposes into direct and response gradients, with the latter stabilizing optimization by converting the bilevel problem into a single-level one. This conversion ensures a conservative gradient vector field, avoiding potential issues. The paragraph discusses approximating the best-response w* or its Jacobian \u2202w*/\u2202\u03bb in gradient-based hyperparameter optimization methods. Approaches to approximate w* directly were proposed by Lorraine & Duvenaud (2018), including a global approximation algorithm using a differentiable function \u0175\u03c6 with parameters \u03c6. The paragraph discusses locally approximating w* in hyperparameter optimization using a factorized Gaussian noise distribution. An alternating gradient descent scheme updates \u03c6 and \u03bb to minimize specific equations, effective for problems with L2 regularization. In hyperparameter optimization, a factorized Gaussian noise distribution is used to approximate w*. An alternating gradient descent scheme updates \u03c6 and \u03bb to minimize equations, effective for problems with L2 regularization on MNIST. The approach is memory efficient and scales to large neural networks, with an automatic adjustment of the neighborhood scale for \u03c6 training. The algorithm easily handles discrete and stochastic hyperparameters, updating their own hyperparameters online during training. Self-Tuning Networks (STNs) update their own hyperparameters online during training. The architecture computes weight/bias with an additional scaled correction based on hyperparameters. It is memory-efficient and requires parameters to represent weight and bias. Self-Tuning Networks (STNs) update hyperparameters online during training, enabling parallelism and improving sample efficiency. The best-response function can be represented exactly using a linear network with Jacobian norm regularization. The best-response function is represented by a linear network with Jacobian norm regularization, using a 2-layer linear network to predict targets from inputs. The squared-error loss is regularized with an L2 penalty on the Jacobian, with the penalty weight mapped using exp. The architecture includes a sigmoidal gating of hidden units. The architecture includes a linear gating of hidden units to approximate the best-response for deep, nonlinear networks. By using an affine approximation for a narrow hyperparameter distribution, the weights become affine in the hyperparameters. This approach ensures gradient descent on the approximate objective function. The best-response Jacobian ensures gradient descent on the approximate objective function. The sampled neighborhood size affects the gradient matching of the approximation to the best-response. The scale of the hyperparameter distribution controls the flexibility of the model. Adjusting \u03c3 during training based on the sensitivity of the upper-level objective to sampled hyperparameters is proposed to capture the shape locally around current hyperparameter values. An entropy term weighted by \u03c4 \u2208 R + enlarges the entries of \u03c3, resulting in an objective similar to variational inference. This approach interpolates between variational optimization and variational inference. The algorithm proposed by Khan et al. (2018) aims to balance the probability mass towards an optimum \u03bb * by adjusting \u03c3 to avoid heavy entropy penalties. The STN training algorithm can tune hyperparameters that other gradient-based algorithms cannot handle, such as discrete or stochastic hyperparameters. The hyperparameters are represented as \u03bb \u2208 R n and the algorithm evaluates F (\u03bb,\u0175 \u03c6 (\u03bb)) at the deterministic current hyperparameter \u03bb 0. The algorithm uses an unconstrained parametrization \u03bb \u2208 R n for hyperparameters, with r mapping \u03bb to a constrained space. Training and validation losses, L T and L V, are functions of hyperparameters and parameters. STNs are trained using a gradient descent scheme alternating between updating \u03c6 and updating \u03bb and \u03c3. The non-differentiability of r due to discrete hyperparameters is not an issue. The derivative of the training objective is estimated to minimize the loss function. The algorithm utilizes an unconstrained parametrization for hyperparameters, with training and validation losses being functions of hyperparameters and parameters. The derivative of the training objective is estimated to minimize the loss function. To estimate the derivative of E with respect to \u03c6, the reparametrization trick is used, considering two cases for discrete hyperparameter \u03bb i. Case 1 involves regularization schemes where the gradient is through \u0175 \u03c6, while Case 2 uses the REINFORCE gradient estimator for \u03bb i explicitly affecting the validation loss. The algorithm utilizes an unconstrained parametrization for hyperparameters, with training and validation losses being functions of hyperparameters and parameters. We do not show this in Algorithm 1, since we do not tune any hyperparameters which fall into this case. We applied our method to convolutional networks and LSTMs, yielding self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). STNs discover schedules for adapting the hyperparameters online, which can outperform any fixed hyperparameter. The ST-LSTM discovered a schedule for output dropout that outperformed fixed hyperparameters, achieving 82.58 vs 85.83 validation perplexity. This improvement was attributed to the schedule, ruling out stochasticity from sampling hyperparameters during training. The ST-LSTM discovered a schedule for output dropout that outperformed fixed hyperparameters, achieving 82.58 vs 85.83 validation perplexity. STNs outperformed perturbation methods on PTB word-level language modeling and CIFAR-10 image-classification tasks. Training a standard LSTM from scratch using the schedule for output dropout discovered by the ST-LSTM showed similar performance to STN, indicating the schedule's importance. Using the final dropout value found by the STN did not perform as well as following the schedule. The STN schedule implements a curriculum by starting with low dropout rate early in training and gradually increasing it for better generalization. The ST-LSTM was evaluated on the PTB corpus with 2-layer LSTM and 650 hidden units per layer. The study utilized a 2-layer LSTM with 650 hidden units and 650-dimensional word embeddings. 7 hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. STNs were compared to grid search, random search, and Bayesian optimization, showing superior performance in achieving lower validation perplexity quickly. Additional details on the experimental setup and hyperparameters can be found in Appendix D. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture. Various forms of dropout were utilized throughout training to prevent overfitting with high-capacity networks. Hyperparameters controlling activation dropout, input dropout, scaling noise, data augmentation, and cut-out holes were tuned for improved performance. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture with various forms of dropout to prevent overfitting. Hyperparameters controlling noise levels and data augmentation were tuned for better performance. STNs outperformed grid search, random search, and Bayesian optimization in finding optimal hyperparameter configurations efficiently. Our work involves using KKT conditions as constraints for the upper-level problem, resembling trust-region methods. Hypernetworks are functions mapping to neural net weights, with various applications in predicting weights for CNNs and RNNs. Gradient-Based Hyperparameter Optimization is also discussed in the context of hypernetworks. Gradient-Based Hyperparameter Optimization involves two main approaches. The first approach approximates the best-response using gradient descent steps, while the second approach uses the Implicit Function Theorem under certain conditions. These methods have been applied in various fields such as neural networks, log-linear models, kernel selection, and image reconstruction. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. Different methods can be used to construct the model iteratively and choose the next hyperparameters to train on by maximizing an acquisition function that balances exploration and exploitation. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. Different methods can be used to construct the model iteratively and choose the next hyperparameters to train on by maximizing an acquisition function that balances exploration and exploitation. Model-free approaches like grid search and random search, as well as Successive Halving and Hyperband, extend random search by adaptively allocating resources to promising configurations using multi-armed bandit techniques. These methods ignore structure in the problem, unlike the approach discussed in the previous paragraph. Population Based Training (PBT) considers schedules for hyperparameters, where a population of networks is trained in parallel. The weights of under-performing networks are replaced by better-performing ones, and hyperparameters are tuned using gradients during a single training run. Self-Tuning Networks (STNs) efficiently approximate best-response of parameters to hyperparameters using gradients for hyperparameter tuning. STNs outperform fixed hyperparameters, achieve better generalization performance in less time, and offer a path towards automated hyperparameter tuning for neural networks. Supported by NSERC awards and CIFAR Canadian AI Chairs program. The text discusses the best-response of parameters to hyperparameters using gradients for hyperparameter tuning, with a focus on the validation loss response gradient and hyperparameter gradient. The analysis includes mathematical proofs and conditions for optimality, highlighting the relationship between parameters and hyperparameters in neural networks. The text discusses the unique solution to Problem 4b for all \u03bb \u2208 U, following from Hastie et al. (2001). It involves the SVD decomposition of the data matrix X and simplifying the function y(x; w) by setting u = s Q. The optimal solution to Problem 13 simplifies to standard L2-regularized least-squares linear regression. The solution involves the change-of-basis matrix Q0 and solving the unregularized regression problem. There are not unique solutions, so best-response functions Q*(\u03bb) and s*(\u03bb) are chosen. These functions meet the criteria with specific parameters. The text discusses the quadratic function f and its properties, including the existence of matrices A, B, C, vectors d, e, and the computation of certain expressions. The text also mentions the optimization process involving the function w\u0302\u03c6(\u03bb) and the simplification of equations using linearity of expectation. The text discusses the optimization process of a quadratic function f using matrix derivatives and the best-response Jacobian. The model parameters were updated, but hyperparameters were not. Training was terminated when the learning rate dropped below 0.0003. Variational dropout was tuned during the process. We terminated training when the learning rate dropped below 0.0003. Variational dropout was tuned on the input, hidden state, and output of the LSTM. Embedding dropout and DropConnect regularization were also applied. Activation regularization penalizes large activations, while temporal activation regularization promotes slowness. For the CNN experiments, 20% of the training data was held out for validation. The baseline CNN was trained using SGD with initial learning rate 0.01 and momentum 0.9, on mini-batches of size 128. The learning rate was decayed by 10 when the validation loss did not decrease for 60 epochs, and training ended if the learning rate fell below 10^-5 or validation loss did not decrease for 75 epochs. The search spaces for baselines-grid search, random search, and Bayesian optimization were defined. The hyperparameters for the ST-CNN were optimized using Adam with a learning rate of 0.003. Training alternated between the best-response approximation and hyperparameters following the schedule of T train = 2 steps and T valid = 1 step. An entropy weight of \u03c4 = 0.001 was used in the objective function. Cutout length was limited to {0, 24} and the number of cutout holes was restricted. The ST-CNN hyperparameters were initialized with dropout rates and data augmentation noise parameters set to 0.05. Cutout length was set to 4 and the number of cutout holes to 1. The model showed robustness to hyperparameter initialization, with low regularization aiding optimization in early epochs. Curriculum learning, a continuation method, was used to optimize non-convex functions by solving a sequence of ordered functions of increasing difficulty. In this section, hyperparameter schedules are explored as a form of curriculum learning. Grid searches were performed to understand the effects of different hyperparameter settings during training. Greedy hyperparameter schedules were found to outperform fixed values, as shown in the validation perplexity results. During training, different combinations of input and output dropout were tested to optimize validation perplexity. Initially, small dropout values were best, but larger rates performed better as training progressed. A grid search for output dropout values showed that a schedule with varying dropout rates outperformed fixed values, leading to faster generalization. The schedule for dropout values during training led to a fast decrease in validation perplexity initially with small dropout values, and better overall performance with larger dropout values later on. The perturbed values for output dropout were analyzed to determine if the improved performance of STNs is due to regularization. PyTorch code listings for HyperLinear and HyperConv2D classes used in ST-LSTMs and ST-CNNs are provided, along with optimization steps for the training and validation sets."
}