{
    "title": "H1srNebAZ",
    "content": "Neural networks trained through stochastic gradient descent (SGD) have been around for over 30 years, but their inner workings remain elusive. This paper takes an experimental approach, focusing on the behavior of single neurons in deep neural networks. The experiments reveal that hidden neurons function as binary classifiers during training and testing, separating inputs into distinct categories. This observation sheds light on the internal mechanics of deep neural networks and their information processing capabilities. Deep neural networks with binary weights and activations can perform image classification and object detection efficiently. The learned representations are universal and can be reused for new tasks in different domains. The characteristics of SGD trained neural networks allow for these behaviors to emerge. The limitations of deep neural networks trained with SGD include difficulties in continuous learning, robustness, and unsupervised learning. Understanding the intricate nature of these networks is crucial for future theoretical and practical developments, with experiments offering valuable insights into key mechanisms supporting their success. Analyzing how neural networks work can lead to improvements in their performance. The workings of hidden neurons in deep neural networks are still a mystery, with many studies focusing on their interpretability. It is believed that hidden neurons represent concepts with increasing abstraction at deeper layers. Some research suggests that intermediate feature maps can detect higher-level objects, but it is unclear if this interpretation captures all relevant information. The dynamics of training and encoding of information by neurons are not fully understood. This paper reveals that a neuron's behavior can be approximated by a binary classifier during training. The neuron pushes activation of samples either up or down, dividing inputs into two categories during training. During testing, quantization and binarization experiments reveal that hidden neurons in neural networks exhibit a simple binary partition behavior that encapsulates core information for predictions. This behavior, observed across different layers and networks on various problem scales, raises intriguing questions for future investigations. Previous works focused on interpreting neuron function in terms of semantically relevant concepts, but this newly discovered behavior suggests a different perspective. Several recent works have explored methods to visualize image structures that activate a neuron, such as training a deconvolution network and analyzing how neuron activation changes when parts of the input image are occluded. Inverse problem formulations have also been used to reconstruct images from network representations. Recent advancements include quantifying the interpretability of signals extracted from visualization methods. Recent works have explored visualizing image structures that activate neurons, with advancements in quantifying interpretability of signals extracted from visualization methods. Object detection emerges in neurons with highest activation in CNNs trained to recognize scenes, suggesting a binary form of encoding. However, it is unclear if these observations capture all relevant information in the feature maps. Investigating further into the emergence of concepts in neurons is motivated by the fact that object detection only works on a subset of feature maps, leaving others unexplained. The paper focuses on validating a complete description of information encoding in neurons. The paper challenges the binary nature of individual neurons by showing that a bimodal activation pattern naturally emerges during training, without forcing binary activations. Previous methods have approximated weights and inputs with binary values, leading to negligible loss in accuracy. This suggests that the conventional continuous definition of activations may be redundant. Our work challenges the binary nature of individual neurons by showing that a bimodal activation pattern naturally emerges during training, even in deep linear networks. We analyze gradients with respect to activations on single samples, highlighting the representation learned by neurons in a neural network. This perspective is crucial for understanding neuron behavior. The behavior of neurons in a neural network is described by associating neurons with activation functions. Each application of a non-linear function to a single value defines one neuron. Different pixels of a feature map are considered as different activations from the same neuron when studying statistical distributions. Three different architectures are experimented with: a 2-layer MLP with 0.5 dropout trained on MNIST, a 12-layer CNN with batchnorm trained on CIFAR-10, and a 50-layer ResNet trained on ImageNet. In addition to ReLU activation, other activation functions are also considered. The ResNet50 network is analyzed along with other models using different activation functions. Specific layers of the networks are referred to by their stage and position within the stage. The cifar CNN is divided into 4 stages of three layers each. Our study focuses on understanding neurons in the ResNet models by analyzing gradients flowing through them. We use Keras and Tensorflow libraries for our experiments and observe how activations contribute to the representation of a single sample. This approach provides valuable insights beyond just analyzing gradients of the loss with respect to parameters during training. The study analyzes gradients flowing through neurons in ResNet models using Keras and Tensorflow. The average sign of partial derivatives with respect to activations is computed for each input sample-neuron pair to determine if increased activation benefits or penalizes sample classification. Zero partial derivatives may occur during training when the sample is correctly classified. The study analyzes gradients flowing through neurons in ResNet models using Keras and Tensorflow. Derivatives in training are small when samples are correctly classified, with signs ignored in calculating average sign. Histograms in FIG1 show average signs of neurons, mostly 1 or -1, indicating consistent derivative signs during training. Neurons aim to partition input distribution into two categories, with positive and negative derivatives. Training neural networks can be noisy, but clear patterns emerge. The study analyzes gradients flowing through neurons in ResNet models using Keras and Tensorflow. The regularity of training has been observed in weights, now seen in activations. Neurons aim to improve prediction by pushing activation in the same direction throughout training. The behavior is less apparent in layers far from the output, with early layers potentially hiding regular dynamics under noise. The study examines noise in gradients in ReLU-networks, with the linear version of cifar CNN showing clearer signals. The presence of noise in gradients raises questions about its impact on learning dynamics and whether neurons effectively separate input categories during training. The analysis leaves open the possibility that noise may be a crucial aspect of the learning process. The study explores noise in gradients in ReLU-networks, with the linear cifar CNN showing clearer signals. It raises questions about the impact of noise on learning dynamics and the effectiveness of neurons in separating input categories during training. The visualization reveals a struggle to separate positive and negative derivatives, with training stopping before complete separation. This prompts a discussion on the mechanism regulating well-partitioned samples in neurons. The final highest pre-activations are highlighted in yellow in the visualizations, showing histograms of average derivative signs in different layers of a neural network trained on MNIST and CIFAR-10 datasets. Neurons act as binary classifiers, receiving consistent information on how to affect sample activations. The high and low categories in a neural network are determined by the average sign of the loss function derivative with respect to activation, which remains constant during training. The network's parameters initialization mainly fixes the categories, with derivative signs heavily influenced by the input class. Neurons in the output layer have derivative signs dependent on the class label rather than the input, leading to categories being a random subset selection. The category definition in a neural network is determined by random initial parameters, leading to separation of high and low categories during training. The evolution of pre-activation distributions across training shows distinct separation of categories, with highlighted highest pre-activations to demonstrate non-linear translation. Further exploration of these mechanisms is left for future work. Neurons operate like binary classifiers during training, but does this reflect how they encode information during testing? This section tests if all information transmitted by a neuron is encoded in the binary partition observed earlier. Strategies are used to modify activations of a trained layer to highlight the binary aspect and reveal structural components. ResNet50 is also studied in this section. The Section studies ResNet50 and tests the robustness of a neural network to quantization of pre-activations. Quantization is based on percentiles of pre-activations, with thresholds separating pre-activations into two sets. Eleven thresholds are tried for the experiment. The experiment tested the robustness of neural networks to quantization of pre-activations using eleven thresholds between 0 and 100. Results showed that neural networks are remarkably robust to quantization, with a preference for higher percentile ranks. Only the conv1 layer from ResNet50 showed a significant decrease in accuracy when pre-activations were quantized. The experiment in the curr_chunk focuses on sliding window binarization to gain insights into how pre-activations are encoded in neural networks. Instead of using a single percentile rank as a threshold, two thresholds are used to form a window, where activations between them are mapped to 1 and outside to 0. This experiment aims to address questions about the categorization of pre-activations in neural networks. The experiment involves sliding window binarization in neural networks, where pre-activations are mapped to 1 if they fall within a window defined by two thresholds, and to 0 if outside. This method helps understand how information is encoded in neural networks by observing the binary partition used for encoding. The experiment involves sliding window binarization in neural networks to observe the binary partition used for encoding information. The transformed pre-activations are analyzed for test accuracy after reinitialization and retraining of subsequent layers. Quantization is performed on a single layer at a time using percentile ranks as thresholds to improve network performance. The experiment involves sliding window binarization in neural networks to observe the binary partition used for encoding information. The networks show robustness to quantization, with neurons providing a binary signal to the next layers. The average percentile rank of zero preactivation is provided, indicating ReLU's and sigmoid's thresholds. Performance decreases in fuzzy regions where both categories are present, as shown in Figure 4 across all layers and networks. The results in Figure 4 demonstrate a clear signal in all layers and networks, showing that the network's performance improves as the window center moves away from rank 50. There is a symmetry around percentile rank 50, indicating a fuzzy partition of two categories with a threshold at rank 50. Neurons encode information by partitioning inputs into two distinct but overlapping categories of varying sizes. The binary behavior of neurons in neural networks is not directly related to the thresholding nature of activation functions. This behavior is observed symmetrically around the 50th percentile rank, even in linear networks without thresholding effects. Previous studies focused on activation binarization at the threshold, but this new observation challenges that notion. The hypothesis in this paper aims to explain the binary behavior of neurons during training and testing. Neurons in neural networks behave like binary classifiers, separating inputs into two categories provided by backpropagated gradients. Experiments on networks of different depths and widths validate this behavior, which has implications for neuron interpretability. While previous studies focused on highest activations, our experiments show neurons consistently learn concepts that distinguish half of observed samples, challenging previous notions of activation binarization at the threshold. The analysis challenges previous notions of activation binarization at the threshold by observing binary behavior of neurons in deep networks. Further investigations are needed to understand the regularity of gradients and the role of activation functions in training dynamics. Our results suggest a precise role for activation functions in promoting binary encoding in neurons, with well-positioned binarization thresholds in the forward pass and considering local sample partitioning in the backward pass. This new perspective offers insights into the generalization gap in deep learning models. Neurons prioritize samples with common patterns during training, observed indirectly in previous studies. A sliding window binarization experiment reveals a clear pattern of binary partitioning of inputs in two categories. This new perspective offers insights into the generalization gap in deep learning models. Neurons prioritize samples with common patterns during training, observed indirectly in previous studies. A sliding window binarization experiment reveals a clear pattern of binary partitioning of inputs in two categories. The layers are part of a network trained on MNIST, CIFAR-10, and ImageNet with different activation functions. The dynamics behind this prioritization between samples of the same category provide insights about the generalization puzzle. The regularity of gradients and the prioritization effect suggest that the slope leading to local minima also matters for generalization abilities. The observation discusses the constant sign of the loss function derivative for neurons close to the output, indicating their role in partitioning samples. Two experiments challenge neuron behavior in network layers, suggesting that binarizing pre-activations preserves task information. The findings raise important questions about network learning capabilities. The curr_chunk discusses questions regarding neuron convergence in the presence of noisy partial derivatives, activation function design, and generalization. Training details include learning rates, batch sizes, and architecture information. The network is based on ResNet50 from Keras applications. Training information is not provided. In Section 4 of resnet50.py, gradients and pre-activations are recorded for different numbers of samples across various layers. Percentiles are computed on randomly selected samples for efficiency. Probes on ResNet50 use 100,000 training samples from ImageNet, while test error is calculated on the complete ImageNet validation set. Table 1 shows the average and standard deviation of the percentile rank of the ReLU threshold across neurons, indicating convergence to a precise position in the last layers. The ReLU threshold remains consistent across neurons, indicating precise convergence in pre-activation distribution. Neurons exhibit binary behavior, with some consistently increasing activation and others decreasing. This suggests neurons receive consistent information on how to affect sample activation. The neuron-wise histograms in FIG1 show that input samples have negative or positive derivatives, allowing neurons to act as binary classifiers. Layers from the first two rows are trained on MNIST with ReLU and sigmoid activation functions, while the third and fourth rows are trained on CIFAR-10 with ReLU and no activation function. Pre-activation distributions evolve during training, separating into high and low categories based on average partial derivative signs. The pre-activation distributions evolve during training, separating into high and low categories based on average partial derivative signs. Neurons act as binary classifiers, with input samples having negative or positive derivatives. The final highest pre-activations of the high category are highlighted to show it is not a simple translation. These dynamics can be viewed in video format on a specific YouTube channel. The final highest pre-activations of the high category are highlighted to show it is not a simple translation. Histogram shows consistency between sample class and belonging to low neuron category in dense2-relu. Most elements of a class are in the same category, as seen by peaks at 0 and 100%. Video format available on YouTube channel."
}