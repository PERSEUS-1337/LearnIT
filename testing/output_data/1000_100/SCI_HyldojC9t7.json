{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology constructs positive definite kernels from dissimilarity measures on structured inputs like time series or discrete structures. It utilizes random feature maps to build a kernel specified by the distance measure, improving generalizability over universal Nearest-Neighbor estimates. D2KE generalizes Random Features methods for complex structured inputs, subsuming representative-set method and relating to distance substitution kernel. Our proposed framework excels in classification experiments across various domains like time series, strings, and histograms for texts and images. It outperforms existing distance-based learning methods in testing accuracy and computational time. It is often easier to specify a dissimilarity function between instances than to construct a feature representation, especially for structured inputs like real-valued time series or discrete structures such as strings, histograms, and graphs. There are well-developed dissimilarity measures available for complex structured inputs, such as Dynamic Time Warping for time series. The curr_chunk discusses distance-based methods in machine learning, specifically focusing on Nearest-Neighbor Estimation (NNE) and the challenges it faces with high variance when neighbors are far apart. Research has been done to develop global distance-based machine learning methods to address this issue. The curr_chunk discusses the development of global distance-based machine learning methods by using similarity functions and kernel methods. One approach involves treating the data similarity matrix as a kernel Gram matrix for standard kernel-based methods like SVM. However, the challenge lies in most similarity measures not providing a positive-definite kernel, leading to non-convex problems. Researchers have focused on estimating a positive-definite Gram matrix to approximate the similarity matrix. The curr_chunk discusses methods for approximating a positive-definite Gram matrix from a similarity matrix. Various techniques like clipping, flipping, or shifting eigenvalues are used, but these modifications can lead to information loss and inconsistency between training and testing data. Another approach involves selecting a subset of training samples as a representative set. This method can be interpreted as a special instance of a more general framework that provides a richer family of kernels. The paper proposes a novel framework called D2KE to construct a family of positive-definite kernels from a dissimilarity measure on structured inputs. This approach builds novel kernels specifically designed for a given distance measure, ensuring Lipschitz-continuity in the corresponding Reproducing Kernel Hilbert Space (RKHS). Additionally, a tractable estimator for functions in this RKHS is provided, offering better generalization properties than nearest-neighbor estimation. Our framework, D2KE, constructs positive-definite kernels from a distance measure for structured inputs, ensuring Lipschitz-continuity in the RKHS. It provides a feature embedding for each instance, improving generalization compared to nearest-neighbor estimation. In classification experiments across various domains, our framework outperforms existing distance-based methods in accuracy and computational time, especially with large datasets or structured inputs. Key contributions include proposing a methodology for constructing PD kernels via Random Features and providing theoretical and empirical justifications for this approach. Our framework, D2KE, utilizes Random Features to accelerate kernel machines on structured inputs like time-series and strings. Existing approaches for Distance-Based Kernel Learning have limitations, but our method constructs positive-definite kernels from a distance measure, improving generalization and outperforming existing methods in accuracy and computational time. The curr_chunk discusses various dissimilarity measures and approaches for kernel learning, including Euclidean embedding, SVM solver in Krein spaces, and building positive-definite kernels from structured inputs like text and time-series. Randomized feature maps have gained interest for approximating non-linear kernel machines, reducing training and testing times significantly. Various explicit nonlinear random feature maps have been developed for different types of kernels, such as Gaussian, Laplacian, intersection, additive, dot product, and semigroup kernels. Among these, the Random Fourier Features (RFF) method approximates a Gaussian Kernel function. The Random Fourier Features (RFF) method approximates a Gaussian Kernel function using a Gaussian random matrix. Various methods have been proposed to accelerate RFF on high-dimensional input data matrices. D2KE differs from existing RF methods by considering structured inputs of different sizes and computing the RF with a structured distance metric. D2KE constructs a new PD kernel through a random feature map, making it computationally feasible via RF. It offers a unified framework for various structured inputs beyond the limits of existing methods like BID49, providing a general theoretical analysis regarding KNN and other distance-based kernel methods. The target function f is learned from samples of structured input objects x i and output observations y i. The dissimilarity measure d between input objects is used instead of a feature representation of x. The dissimilarity measure needs to be a metric for some analyses. The ideal feature representation is compact and ensures that the target function f is a simple function of x. The target function f is learned from input objects x i and output observations y i using a dissimilarity measure d. The ideal feature representation is compact and ensures that f is a simple function of x. Lipschitz Continuity is preferred for the target function with a small constant L with respect to the dissimilarity measure d. The effective dimension in the context of measuring the space of Multiset is defined as the minimum value that satisfies a certain condition. This dimension is crucial in estimating the error of a Nearest-Neighbor Estimator in structured input spaces. The (modified) Hausdorff Distance is defined for a set that allows duplicate elements. It involves measuring the distance between elements in a set using a ground distance. The covering number of a set under a ground distance is denoted by N(\u03b4; V, \u2206). Effective dimension is used to bound the estimation error of the k-Nearest-Neighbor estimate of a target function. The proof for the parameter k in DISPLAYFORM4 is similar to the analysis of k-NN's estimation error in BID21, with space partition number replaced by covering number and dimension by effective dimension. Estimation error of k-NN decreases slowly with n when p X,d is large, requiring samples to scale exponentially. An estimatorf based on a RKHS derived from the distance measure is developed with better sample complexity for higher effective dimension problems. A simple approach D2KE constructs positive-definite kernels from a given distance measure for structured input domain X. The kernel in Equation (4) is constructed from a family of kernels using a random structured object \u03c9, with a distribution p(\u03c9) over \u03c9. The kernel is parameterized by p(\u03c9) and \u03b3, and can be interpreted as a soft version of the distance substitution kernel. The kernel in Equation FORMULA15 is determined by min \u03c9 \u2208\u2126 d(x, \u03c9) + d(\u03c9, y), which equals d(x, y) if X \u2286 \u2126. Unlike the distance-substituion kernel, our kernel in Equation FORMULA13 is always PD by construction. Random Feature Approximation can be used to approximate the kernel in Equation FORMULA12. The RF approximation allows for efficient learning of a target function as a linear function of the RF feature map, minimizing domain-specific empirical risk. This approach is different from selecting random features in a supervised setting. The overall RF-based empirical risk minimization for D2KE kernels is outlined in Algorithm 1. The random feature embeddings are computed using a structured distance measure between original and generated inputs, followed by an exponent function with parameter \u03b3. This differs from traditional RF methods that use matrix multiplication with a Gaussian matrix. A detailed analysis of the estimator is provided in Algorithm 1 in Section 5, contrasting its performance with K-nearest-neighbor. The relationship to the Representative-Set Method is discussed, showing a connection to the data distribution through a kernel equation. A Random-Feature approximation can be obtained by holding out part of the training data as samples. The random feature embeddings are created by holding out a portion of the training data as samples from p(\u03c9) and generating an R-dimensional feature embedding. This approach provides a better generalization error bound even as R approaches infinity, unlike the Representative-Set Method where a small representative set size is needed for reasonable performance. The choice of p(\u03c9) significantly impacts the kernel, with \"close to uniform\" distributions yielding better results across various domains. In various domains, \"close to uniform\" choices of p(\u03c9) outperform the data distribution p(\u03c9) = p(x) used in the Representative-Set Method. For example, in time-series with DTW, random time series with Gaussian elements perform better. In string classification with edit distance, random strings from an alphabet yield better results. Similarly, for classifying sets of vectors with Hausdorff distance, random sets from a unit sphere outperform RSM. The chosen distributions p(\u03c9) from a unit sphere outperform RSM due to the ability to generate unlimited random features, leading to better approximation of the exact kernel. Additionally, the selected p(\u03c9) captures more relevant semantic information for estimating f(x) under the dissimilarity measure d(x, \u03c9). The proposed framework analyzes error decomposition in the feature map under the dissimilarity measure. The RKHS corresponding to the kernel is denoted as H, with population and empirical risk minimizers DISPLAYFORM0 and DISPLAYFORM1. The estimated function f R from random feature approximation is used to calculate population and empirical risks as L( f ) and L( f ) respectively. The risk decomposition is discussed, focusing on function approximation error in a smaller function space compared to Lipschitz-continuous functions. The text discusses the Lipschitz continuity of functions in the RKHS with additional smoothness constraints imposed by the RKHS norm and kernel parameter. It aims to approximate the true function well with estimation error bounds based on eigenvalues of the kernel. The goal is to minimize the tuning parameter \u03bb as a function of n for better estimation error. The text discusses minimizing the tuning parameter \u03bb for better estimation error in RKHS estimators. The estimation error has a better dependency on n compared to k-nearest-neighbor methods, especially for higher effective dimensions. Random Feature Approximation is also discussed, with error bounds based on RKHS norm. The analysis of D \u03bb for the kernel in Equation FORMULA12 is challenging due to the lack of an analytic form of the kernel. In the context of minimizing the tuning parameter \u03bb for better estimation error in RKHS estimators, the text focuses on the approximation error of the kernel DISPLAYFORM3. It discusses uniform convergence and provides bounds on the empirical risk L(f R ) \u2212 L(f n ) using Proposition 2 and the Representer theorem. Corollary 1 states conditions for guaranteeing L(f R ) \u2212 L(f n ) \u2264 with probability 1 \u2212 \u03b4. The text discusses the approximation error of the kernel DISPLAYFORM7, stating conditions for achieving an approximation error with a small number of Random Features. It also presents Claim 1, showing that the proposed framework can achieve suboptimal performance in estimating the target function. The proposed method evaluates different dissimilarity measures in various domains such as time-series, strings, texts, and images. It compares distance-based methods using well-known measures like Dynamic Time Warping, Edit Distance, Earth Mover's distance, and (Modified) Hausdorff distance. The study evaluates dissimilarity measures in different domains including time-series, strings, texts, and images. Various distance-based methods are compared using measures like Dynamic Time Warping, Edit Distance, Earth Mover's distance, and (Modified) Hausdorff distance. Datasets for experiments include multivariate time-series, strings with varying alphabet sizes and lengths, and partially overlapped text documents. The datasets used in the study vary in length and are derived from Kaggle. SIFT descriptors are computed for image data, with feature vector sizes ranging from 1 to 914. The datasets are divided into 70/30 train and test subsets. The study compares D2KE against 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, and KSVM. The study compares D2KE against 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, and KSVM, which have quadratic complexity in both the number of data samples and the length of the sequences. In contrast, D2KE has linear complexity in both parameters. The best parameters for each method are determined through 10-fold cross validation, with D2KE using random samples to achieve performance close to an exact kernel. D2KE outperforms baseline methods like KNN, DSK_RBF, DSK_ND, and KSVM in classification accuracy while requiring less computation time. It is a strong alternative to KNN and achieves better performance than other methods operating on indefinite similarity matrices. Our method outperforms RSM on indefinite similarity matrices, indicating a superior use of data. D2KE samples random objects like time-series, strings, or sets, leading to significantly better performance. The proposed framework derives a positive-definite kernel and feature embedding function from dissimilarity measures for structured input domains. Our framework extends existing approaches and introduces a new direction for creating embeddings of structured objects based on distance to random objects. The goal is to develop distance-based embeddings within a deep architecture to support structured inputs in an end-to-end learning system. The proof involves bounding the magnitude of Hoefding's inequality for a given input pair. The method utilizes a covering of the input space to achieve the desired bound. Our framework extends existing approaches by introducing distance-based embeddings within a deep architecture for structured inputs. The proof involves bounding Hoefding's inequality magnitude for a given input pair, utilizing a covering of the input space. The method searches for optimal parameters through 10-fold cross-validation and uses specific kernels for different methods. Our new method, D2KE, utilizes random sampling to generate data samples for performance close to an exact kernel. The range of samples used is typically between 4 to 4096, with larger samples leading to better accuracy. Linear SVM is employed for embedding-based methods, while precomputed dissimilarity kernels use LIBSVM. Datasets are sourced from various public websites, with detailed properties listed in a table. The datasets from four different domains were analyzed using a DELL dual-socket system with Intel Xeon processors. Multithreading with 12 threads was used for distance computations. Gaussian distribution with parameter \u03c3 was applied to all datasets. D2KE showed superior classification accuracy and faster computation for multivariate time-series compared to other methods. Our method, D2KE, outperforms KNN by 26.62% on IQ_radio due to its insensitivity to data noise. Compared to DSK_RBF, DSK_ND, and KSVM, D2KE achieves much better performance by utilizing a truly p.d. kernel. RSM, while similar in feature matrix construction, is outperformed by D2KE's random time series sampling. Our method, D2KE, outperforms RSM by utilizing random time series sampling to denoise and find patterns in the data. Unlike RSM, which suffers from noise and redundant information, D2KE samples short random sequences, making the feature space more abundant. Additionally, D2KE's computational cost is lower for long time-series data. Results show that D2KE outperforms other distance-based baselines, including DSK_RBF, on relatively large datasets. By utilizing Levenshtein distance as a metric, D2KE offers a clear advantage over baseline methods, with improved performance and lower computational cost for long time-series data. Our method, D2KE, outperforms other baselines with quadratic complexity in both number and length of data samples. For text data, we use the earth mover's distance as our distance measure between documents, showing strong performance when combined with KNN for document classifications. Bag of Words is computed for each document, represented as a histogram of word vectors using google pretrained word vectors. Random documents are generated with word vectors uniformly sampled from the unit sphere. Results show that D2KE outperforms other baselines on all datasets, with distance-based kernel methods performing better than KNN. D2KE also excels due to its random documents of short length being well-suited for document classification tasks. It achieves a significant speedup compared to other methods, thanks to the use of random features. In image data analysis, the modified Hausdorff distance (MHD) is used as the distance measure between images. SIFT-descriptors with dimension 128 are generated using the OpenCV library, and the distance between sets of SIFT-descriptors is computed using MHD. Random images of each SIFT-descriptor are generated from the unit sphere of the embedding vector space. D2KE outperforms other baselines in all cases, with the best performance in three cases, while DSK_RBF performs best on dataset decor. The parameters for \u03b3 and the length of random SIFT-descriptor sequence are optimized. The quadratic complexity of DSK_RBF, DSK_ND, and KSVM makes it difficult to scale to large data due to the length of SIFT descriptor sequences. D2KE still outperforms KNN and RSM, showing it can be a strong alternative across applications."
}