{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a major challenge for neural networks in dynamic data scenarios. A Differentiable Hebbian Consolidation model addresses this by combining rapid learning plasticity with fixed parameters, allowing retention of learned representations over time. Task-specific synaptic consolidation methods can be integrated to maintain important weights for each target task. Our proposed model penalizes changes in important slow weights for each target task, outperforming baselines on various benchmarks. It addresses the challenges of class imbalance and concept drift without needing additional hyperparameters. Human intelligence's ability to adapt in dynamic environments is difficult to embed in artificial intelligence. Machine learning models face non-stationarity during real-world deployment. During real-world deployment, machine learning models face non-stationarity where data distributions change over time. This leads to performance degradation when models are further trained with new data, known as catastrophic forgetting. Continual learning aims to adapt to consecutive tasks without forgetting previous ones, crucial for deep neural networks tasked with lifelong learning. In real-world applications, machine learning systems face challenges with non-stationarity, such as concept drift and imbalanced class distributions. This leads to the stability-plasticity dilemma, where models need to balance integrating new knowledge with preserving existing knowledge. This continual learning challenge is crucial for deep neural networks in lifelong learning. In the context of continual learning challenges for deep neural networks, synaptic plasticity is crucial for learning and memory. Two major theories, synaptic consolidation and complementary learning system theory, explain how humans retain and consolidate important synaptic parameters for previously learned tasks. Recent work on differentiable plasticity has shown that neural networks can be trained with \"fast weights\" that change quickly based on input representations, allowing for reactivation of long-term memory traces stored in \"slow weights\". This approach leverages Hebbian learning rules to optimize synaptic connections and enhance learning and memory processes. Differentiable Hebbian Consolidation 1 model extends work on differentiable plasticity to task-incremental continual learning. It adapts quickly to changing environments by selectively adjusting synapse plasticity, enhancing learning and memory processes. The Differentiable Hebbian Consolidation model proposes augmenting the final FC layer with plastic weights using Differentiable Hebbian Plasticity. It combines task-specific synaptic consolidation approaches to overcome catastrophic forgetting and leverages compressed episodic memories in the softmax layer to remember previous knowledge. Tested on benchmark problems like Permuted MNIST and Split MNIST, the model enables rapid adaptation to new data while consolidating synapses. The Imbalanced Permuted MNIST problem is introduced, showing that plastic networks with task-specific synaptic consolidation methods outperform networks with uniform plasticity. Hebbian learning theory suggests that learning and memory are attributed to weight plasticity, where correlated activation of pre-and post-synaptic neurons strengthens the connection between them. Recent approaches in meta-learning incorporate fast weights into neural networks for one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed a model with fast weights implemented using Hebbian learning-based associative memory. Rae et al. (2018) introduced a Hebbian Softmax layer to improve learning of rare classes. Miconi et al. (2018) suggested differentiable plasticity using SGD. Miconi et al. (2018) proposed differentiable plasticity, utilizing fast weights that automatically adjust based on activity, demonstrated on RNNs for pattern memorization and maze exploration. Our work also incorporates fast weights in the FC layer using DHP, updating only the softmax output layer for fast learning and knowledge retention. The curr_chunk discusses strategies to overcome catastrophic forgetting, including task-specific synaptic consolidation and the CLS theory. These methods aim to protect previously learned knowledge and enable rapid learning of new instances. Notable works inspired by these strategies are categorized as regularization strategies in continual learning literature. Regularization strategies in continual learning literature aim to estimate the importance of each parameter or synapse, adjusting plasticity to prevent changes to important parameters of previously learned tasks. Different methods are used to compute the importance of each parameter. Different methods are used to compute the importance of each parameter in regularization strategies for continual learning. Elastic Weight Consolidation (EWC) computes the importance using the Fisher information matrix, while an online variant was proposed to improve scalability. Synaptic Intelligence (SI) measures parameter importance by the cumulative change in synapses, and Memory Aware Synapses (MAS) measures it by the sensitivity of the learned function to parameter perturbation. Our work is inspired by CLS theory, which uses a dual memory system to represent memories. Various approaches based on CLS principles include pseudo-rehearsal, exact replay, and generative replay. While methods like iCaRL use external memory for rehearsal and regularization, our focus is on neuroplasticity. In our work, we focus on neuroplasticity techniques inspired by CLS theory to alleviate catastrophic forgetting. Previous research has shown how slow and fast weights in synaptic connections can help store long-term knowledge and prevent memory loss. Recent studies have explored different approaches such as replacing soft attention mechanisms with fast weights in RNNs, using Hebbian Softmax layers, and incorporating differentiable plasticity. Neuromodulated differentiable plasticity was not compared to neuroplasticity-inspired CLS methods as they were designed for meta-learning, which would be unfair. These methods focus on rapid learning on simple tasks or meta-learning over a distribution of tasks, with a few examples seen per class during training for one-shot and few-shot learning. The Hebbian Softmax layer adjusts its parameters through annealing between Hebbian and SGD updates, achieving fast binding for rarer classes but switching to SGD updates when many examples from the same class are observed frequently. In continual learning setups, the model switches to SGD updates, rendering the fast weights memory storage ineffective. The goal is to metalearn a local learning rule for the fast weights using the slow weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights and a Hebbian plastic component with a scaling parameter \u03b1 and Hebbian traces accumulating mean hidden activations for each target label in the mini-batch. The final hidden layer activations for target labels in a mini-batch are computed using pre-synaptic activations. Post-synaptic activations are then calculated to obtain unnormalized log probabilities. The softmax function is applied to get predicted probabilities. The \u03b7 parameter controls learning rate for plastic connections and prevents instability in Hebbian traces. Network parameters are optimized using gradient descent in continual learning setups. The model is trained sequentially on different tasks in continual learning. The weight connection has fixed weights, equivalent to setting plasticity coefficients \u03b1 = 0. Hebbian weight update is used, and hidden activations are accumulated during training. Hebbian traces are updated only during training, and the most recent traces are used for predictions at test time. The model explores an optimization scheme where hidden activations are directly accumulated into the softmax output layer weights for better initial representations and longer retention of learned deep representations. Fast learning with a plastic weight component improves test accuracy, with selective consolidation into a stable component between tasks to prevent interference. DHP Softmax simplifies implementation without requiring additional space or computation compared to external memory. The DHP Softmax simplifies implementation by easily scaling with increasing tasks. It utilizes Hebbian updates to accumulate hidden activations for each class, forming compressed episodic memory traces. This allows for rapid learning and sparse parameter updates to store recent experiences without interference. Hebbian Synaptic Consolidation improves learning of rare classes and speeds up binding of class labels to deep representations without additional hyperparameters. It regularizes the loss and updates synaptic importance parameters online, enhancing network performance. The network parameters are the weights of connections between neurons, with task-specific consolidation methods only regularizing the slow weights. The synaptic importance parameter is set to 0 for all methods except for SI, which estimates it during training. The plastic component of the softmax layer helps prevent catastrophic forgetting by optimizing the plasticity of connections. Our approach increases DNN capacity with plastic weights, compared to vanilla neural networks with Online EWC, SI, and MAS. We add slow weights to the softmax layer for fair evaluation. Tested on various benchmarks, including Permuted MNIST and Split MNIST, our model shows improved memory retention and classification accuracy. The study focuses on evaluating memory retention and flexibility of a model by measuring test performance on the first and most recent tasks. Forgetting is assessed using the backward transfer metric, BWT. Neural networks are trained with Online EWC, SI, and MAS consolidation methods sequentially on all tasks to establish a baseline for comparison. The hyperparameters of the consolidation methods remain consistent with and without DHP Softmax. In the Permuted MNIST and Imbalanced Permuted MNIST benchmarks, a multi-layered perceptron (MLP) network with two hidden layers is used. The input distribution changes between tasks, indicating concept drift. The plastic component's \u03b7 value is set at 0.001 with little tuning effort. Performance is compared between a network with DHP Softmax and a fine-tuned vanilla MLP. Details of hyperparameters can be found in Appendix A. The network with DHP Softmax showed improved performance in alleviating catastrophic forgetting compared to a fine-tuned vanilla MLP network. When task-specific consolidation methods were applied, the DHP Softmax with consolidation maintained higher test accuracy during sequential task learning. An ablation study was conducted to analyze the network's structural parameters and Hebb traces for further interpretability. The proposed model's behavior during training on 10 tasks in the Permuted MNIST benchmark is analyzed. The synaptic connections become more plastic initially to acquire new information quickly, then decay to prevent interference between learned representations. The Hebb trace grows without runaway positive feedback, maintaining a memory of recent activity, and plasticity coefficients increase within each task, leveraging the structure in the plastic component. The network uses gradient descent and backpropagation for meta-learning to tune structural parameters in the plastic component. The Imbalanced Permuted MNIST problem introduces imbalanced class distributions, with DHP Softmax achieving 80.85% accuracy after learning 10 tasks sequentially, showing a 4.41% improvement over the standard. The compressed episodic memory mechanism in the Hebbian traces shows a 4.41% improvement over the standard neural network baseline. DHP Softmax with MAS achieves an average test accuracy of 88.80% and outperforms all other methods in a sequence of 5 binary classification tasks on the MNIST dataset. An MLP network with two hidden layers of 256 ReLU nonlinearities each, and a cross-entropy loss, achieved a 7.80% improvement in test performance compared to a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and led to higher average test accuracy across all tasks, especially the most recent one, T 5. Continual learning was performed on a sequence of 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. In the study, a CNN architecture similar to previous works was used with an initial \u03b7 parameter value of 0.0001. The network was trained with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. Results showed that combining DHP Softmax with MAS improved average test accuracy by 2.14%. Additionally, SI with DHP Softmax outperformed other methods with an average test performance of 81.75% after learning all five tasks. Adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. The compressed episodic memory allows new information to be learned without interference, enabling the model to generalize across experiences. The \u03b1 parameter in the plastic component scales the magnitude of the plastic connections in the Hebbian traces effectively. The neural network with DHP Softmax improves performance compared to traditional softmax with slow changing weights. DHP Softmax does not add hyperparameters and allows for flexibility in model improvement through Hebbian Synaptic Consolidation. Regularizing slow weights using EWC, SI, or MAS improves a model's ability to alleviate catastrophic forgetting after learning multiple tasks. DHP Softmax combined with SI outperforms other consolidation methods on Split MNIST and 5-Vision Datasets Mixture. Combining DHP Softmax and MAS leads to superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. MAS computes synaptic importance parameters based on Hebb's rule, prioritizing highly correlated connections. The model shows lower negative BWT and higher test accuracy across benchmarks compared to other methods. Hebbian plasticity enables neural networks to learn continually and remember distant memories, reducing catastrophic forgetting in dynamic environments. Continual synaptic plasticity aids in learning from limited labeled data and scaling at long timescales. This work aims to explore gradient descent optimized Hebbian consolidation for learning and memory in DNNs to enable continual learning on sequential tasks. In a continual learning setup, a model is trained on sequential tasks with associated training data. Each task has its own loss function to prevent forgetting. The model learns an approximated mapping to the true function, mapping new inputs to target outputs for all tasks learned. Different classes can be present in each task. Experiments were conducted on Nvidia GPUs. The experiments were conducted on Nvidia GPUs using a sequence of tasks trained with mini-batches and optimized with plain SGD. Training involved early stopping and resetting network weights if validation error increased. Hyperparameters were set for different task-specific consolidation methods. In the experiments, hyperparameters were optimized for task-specific consolidation methods using a grid search. For Online EWC, values of \u03bb were tested in a range, for SI -\u03bb in another range, and for MAS -\u03bb in a different range. Training samples were artificially removed from each class in the Imbalanced Permuted MNIST problem based on random probabilities. The distribution of classes in each imbalanced dataset for tasks T n=1:10 is shown in Table 2. The total number of samples for each class across all tasks is also provided. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. A grid search was conducted to find the best hyperparameter combination for each method. In the Split MNIST experiments, specific hyperparameters were used as shown in Figure 2b. In the Split MNIST experiments, hyperparameters for task-specific consolidation methods were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. A grid search was performed to determine the best hyperparameter combination for each method using a 5 task binary classification sequence. The network was trained on a sequence of 5 tasks with mini-batches of size 64 and optimized using plain SGD with a fixed learning rate of 0.01 for 10 epochs. The Vision Datasets Mixture benchmark includes tasks with different image classification datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. Each dataset has specific characteristics and image sizes for training and testing. The CNN architecture is used for the tasks. The CNN architecture for the Vision Datasets Mixture benchmark includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling operations. A multi-headed approach is used due to different class definitions between datasets. Each connection in the final output layer has a trainable \u03b7 value for optimization stability. The CNN architecture for the Vision Datasets Mixture benchmark includes 2 convolutional layers with 20 and 50 channels, LeakyReLU nonlinearities, and max-pooling. Each connection in the final output layer has a trainable \u03b7 value for stability. Different hyperparameters were used for Online EWC, SI, and MAS methods to optimize test performance. The sensitivity analysis was conducted on the Hebb decay term \u03b7, showing its impact on average test performance in continual learning. Low values of \u03b7 were found to be most effective in reducing catastrophic forgetting across different benchmarks like Permuted MNIST, Imbalanced Permuted MNIST, and Split MNIST. The average test accuracy was presented for the MNIST-variant in Table 4. The average test accuracy for MNIST-variant benchmarks is shown in Table 4, corresponding to sensitivity analysis plots. The text includes details about learning rate and Hebbian traces for the next iteration. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to a neural network's final output layer through plastic connections. It outperforms Finetune in class-incremental learning setups on CIFAR-10 and CIFAR-100 tasks. Finetune (yellow) on tasks in class-incremental learning setup. DHP Softmax performs as well or better than training from scratch (light green) on some tasks. Test accuracies of Finetune, training from scratch, and SI (turquoise) from von Oswald et al. (2019)."
}