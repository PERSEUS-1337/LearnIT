{
    "title": "BkecJjCEuN",
    "content": "The aim of the present work is to enhance the label efficiency of large neural networks for audio data by combining multitask learning and self-supervised learning on unlabeled data. Through various self-supervised tasks, it was shown that performance in supervised classification tasks can be improved by up to 6% in scenarios with limited labeled training data. Incorporating data augmentation into multitask learning improves performance of deep neural networks for audio data classification. Unsupervised learning using self-supervised auditory tasks enhances model generalization by utilizing unlabeled data sources effectively. Incorporating self-supervised auditory tasks during model training improves performance by utilizing unlabeled data effectively. The study demonstrates the successful identification of appropriate tasks and shows how WaveNet can be used as a feature extractor for rich audio representations. The framework is applied to supervised classification tasks, such as audio tagging, speaker identification, and speech command recognition, showing improved performance with the use of unlabeled data and data augmentation techniques. The study demonstrates that self-supervised tasks can enhance model performance by utilizing unlabeled data effectively. It suggests that models trained for multiple tasks may uncover underlying structures, leading to better single-task performance with less data. This approach aligns with prevailing wisdom in sensory environments literature. Self-supervised learning has shown promising results in the visual domain, but little previous work has taken advantage of self-supervision in the audio domain. An end-to-end audio processing network was implemented to find a common embedding of the acoustic waveform within a \"trunk\" network modeled after the WaveNet architecture. The study implemented a trunk network based on the WaveNet architecture to embed the acoustic waveform, followed by task-specific head networks. The trunk consists of 3 blocks with 6 dilation stacks each, resulting in an effective receptive field length of 190 samples. The setup was tested on supervised tasks like audio tagging, speaker identification, and speech command recognition. The study implemented a WaveNet-based trunk network for embedding acoustic waveforms, followed by task-specific head networks. The audio tagging task is trained on the FSDKaggle2018 dataset with 11,073 audio files, cropped to 2 seconds and padded with zeros if needed. The output is averaged across time to produce a single vector for the entire audio sequence. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Individual clips are sourced from interviews with celebrities in various settings, with one interview held out for a test set. Each audio segment is cropped to 2 seconds and normalized before being fed into the network. The task's head architecture includes a global average pooling layer and a 2-layer fully-connected layer with 512 units and ReLU nonlinearity. The speech command recognition task is trained on the Speech Commands dataset with 65,000 utterances of 30 short words. The head architecture includes a global average pooling layer, 2-layer perceptron with 1024 units per layer, batch normalization, ReLU nonlinearity, and a softmax layer. The dataset consists of 12 categories, with 10 words classified and the rest as unknown or silence. The recognition head features three 1D convolutions with batch normalization, dropout, ReLU nonlinearity, and final softmax layer. The self-supervised tasks for next-step prediction, noise reduction, and upsampling were trained on both main task data and unlabeled data from the Librispeech dataset. The tasks share a common head architecture with convolutional layers and a regression-type loss function. The goal was to create a generic multitask framework for audio using waveform inputs. When working with audio data, it is recommended to use waveform inputs instead of high-level feature representations like spectrograms. Different tasks may require varying network architectures, emphasizing the benefits of multitask learning with raw audio inputs to improve performance. This approach allows for a broader range of audio processing tasks without restricting model capabilities. Joint training with three self-supervised tasks improved performance on supervised tasks like audio tagging, showing an increase in MAP@3 score and classification rate. Incorporating larger unlabeled datasets led to further improvements in performance metrics, with up to a .056 increase in MAP@3 with an additional 500 hours of data. Joint training with three self-supervised tasks improved performance on supervised tasks like audio tagging, speech command classification, and speaker identification. Additional unlabeled data showed improvements in classification rates for both tasks, with speech command classification reaching 93.78% and speaker identification peaking at 75.22%. Multitask learning proved to enhance performance without the need for extra labeled data. Comparing these results with data augmentation techniques was also considered. Training a single task model on audio tagging with pitch shifting and additive noise augmentation resulted in performance gains. Pitch-shift augmentation increased MAP@3 by .066, noise augmentation by .024, and combining both with self-supervised tasks yielded the highest increase of .089. These methods for improving label efficiency are complementary to each other. Transfer learning from self-supervised tasks trained on unlabeled data to supervised tasks has shown promise in computer vision. Transfer learning from self-supervised tasks trained on unlabeled data to supervised tasks has shown promise in computer vision. In this approach, self-supervised tasks are pre-trained on unlabeled data before fine-tuning with a smaller quantity of labeled data for a supervised task. Transfer learning experiments favored this method over simultaneously training all tasks together, showing improved performance on audio tasks with limited labeled data. Our approach of transfer learning from self-supervised tasks on unlabeled data to supervised tasks in audio classification has shown improved performance with more unlabeled data. The multitasking model's ability to forecast audio frames, remove noise, and perform upsampling suggests a learned audio representation that could be further explored and extracted. Our chosen auxiliary tasks require higher temporal resolutions, so we built our model following the WaveNet architecture. WaveNet models use causal dilated convolutions to process sequential inputs in parallel, making them faster to train compared to RNNs. Our model follows the WaveNet architecture, utilizing small task-specific neural networks on top of a task-agnostic trunk. The trunk consists of stacked dilated causal convolutions, with outputs fed into task-specific heads. The WaveNet trunk is composed of N blocks, each with S dilated causal convolution layers, residual connections, and nonlinearities. Each layer in the trunk involves \"Filter\" and \"Gate\" computations, producing hidden state vectors and layer outputs. The WaveNet trunk consists of N blocks with S dilated causal convolution layers, resulting in a total receptive field of \u03c4 = 190, equivalent to 12 milliseconds of audio sampled at 16kHz. The WaveNet trunk consists of N blocks with S dilated causal convolution layers, resulting in a total receptive field of 12 milliseconds of audio sampled at 16kHz. Each task-specific head is a neural network that processes input data through a shared trunk. Task-specific heads are free to process input independently, with each task having its own objective function and optimizer. Supervised tasks are designated as primary, while self-supervised tasks are auxiliary. In experiments, \"audio tagging\" was the primary task, with \"next-step prediction\", \"noise reduction\", and \"upsampling\" as auxiliary tasks trained on unlabeled data. Task-specific head parameters can be found in the accompanying supplement. The head architectures in the study were designed to be simple, using few layers to solve tasks and forcing the shared trunk to learn a representation suitable for multiple audio tasks. The next-step prediction task involves predicting the next value in a sequence of audio frames. This allows for cheaply obtaining large training datasets from unlabeled audio data. The next-step prediction head consists of a 2-layer stack of 1 \u00d7 1 convolutional layers with ReLU nonlinearities, with the first layer containing 128 units and the second layer containing a single output unit. The next-step prediction head in the study uses a regression approach to predict the next frame of audio, calculating the mean squared error between predicted and actual values as the loss function. This differs from the original WaveNet implementation, which treated next-step prediction as a classification problem. The noise reduction head in the study aims to predict the clean audio waveform from noisy samples by minimizing a smoothed L1 loss between the clean and noisy versions of the waveform inputs. The model is trained to predict the clean sample given a window of noisy samples, similar to the next-step prediction task. The study focused on using smooth L1 loss for noise reduction in audio tasks. It also discussed creating an unsupervised upsampling task by downsampling the audio source and inferring high frequency information lost during the transform. This task is analogous to the \"super-resolution\" task in computer vision. The study utilized a smooth L1 loss function for noise reduction in audio tasks. They also implemented an unsupervised upsampling task using raw audio waveform inputs from specific datasets. The inputs were cropped to two seconds, downsampled to 16 kHz, and normalized to lie in the interval [-1, 1]. Noise for the noise-reduction task was added at a randomly chosen SNR from 10dB. The study involved adding noise sampled from ChiME3 datasets to audio inputs at varying SNRs. Hyperparameter search was conducted for the network architecture, with focus on the number of blocks, layers, and units. Performance was found to be largely unaffected by specific architecture specifications. Additionally, searches were done for the depth and width of auxiliary task heads. The final choice of hyper-parameters for the model was made by selecting values that optimized performance on both the main task and auxiliary tasks. The model was trained on all tasks simultaneously using a uniform weighting strategy. The \"Adam\" optimizer was used with specific parameters, and the learning rate was decayed every 5 epochs. A batch size of 48 was used for all experiments. The batch size of 48 was used for all experiments due to computational constraints. Noise reduction and upsampling tasks required separate forward propagations. Important model parameters can be found in TAB3."
}