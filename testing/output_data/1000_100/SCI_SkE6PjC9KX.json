{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. NPs efficiently fit observed data with linear complexity and can learn a wide family of conditional distributions. However, they suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data. To address this issue, attention is incorporated into NPs, allowing each input location to attend to relevant context points for prediction, resulting in improved accuracy and faster training. Regression tasks involve modelling the distribution of output given input using a deterministic function like a neural network. An alternative approach uses a distribution over functions to make predictions, allowing for reasoning about multiple functions consistent with the data. Non-parametric models like Gaussian Processes are popular choices for this approach in Bayesian machine learning literature. Neural Processes (NPs) offer an efficient method for modeling a distribution over regression functions, with prediction complexity linear in the context set size. They can predict the distribution of an arbitrary target output conditioned on a set of context input-output pairs of any size, allowing them to model data generated from a stochastic process. NPs are trained on samples from multiple realizations of a stochastic process, unlike Gaussian Processes (GPs) which are usually trained on observations from one realization. Despite their appealing properties, NPs have a weakness in that they tend to. Neural Processes (NPs) have appealing properties but tend to underfit the context set, leading to inaccurate predictions. The encoder aggregates context to a fixed-length summary, causing a bottleneck in the mean-aggregation step. This results in difficulties for the decoder to learn the importance of context points in prediction. To address underfitting in Neural Processes (NPs), a mechanism inspired by Gaussian Processes (GPs) is implemented using differentiable attention. This allows the model to learn which context points are relevant for a given target prediction, while maintaining permutation invariance. The Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by enhancing expressiveness and speed of training for regression functions. ANPs maintain permutation invariance in contexts and are able to model a wider range of functions. The NP maps input x to output y and defines conditional distributions based on observed contexts and targets, invariant to ordering. The model defines conditional distributions for arbitrary C and T, with C typically a subset of T. It uses a deterministic function to aggregate context pairs (x, y) into a finite representation with permutation invariance. The likelihood is modelled by a Gaussian factorised across targets, with mean and variance obtained through an MLP. The NP model includes a global latent variable z to address uncertainty in predictions. The model incorporates a global latent variable z, modelled by a factorised Gaussian parametrised by s, to complement the deterministic path. The encoder and decoder parameters are learned to create an expressive model that can incorporate attention. The encoder and decoder parameters are learned through maximising ELBO for a subset of contexts and targets using the reparametrisation trick. The NP reconstructs targets with a KL term to keep the summaries close. NPs offer scalability and flexibility in learning conditional distributions efficiently. The NP offers scalability and flexibility in learning conditional distributions efficiently, linearly at O(n+m) for n contexts and m targets at train and prediction time. It defines a wide family of distributions and exhibits permutation invariance, but lacks consistency in contexts. Maximum-likelihood learning minimises the KL between the conditional distributions of the data-generating process and the NP's distributions. The attention mechanism computes weights for key-value pairs based on a query, allowing for value aggregation. This permutation invariance property is crucial for its application in neural networks. Differentiable addressing mechanisms have been successfully used in deep learning for tasks like handwriting generation and machine translation. Self-attention has been employed for expressive sequence-to-sequence mappings in natural language processing and image processing. In natural language processing and image modeling, attention mechanisms are used to compute weights for key-value pairs based on a query. Different types of attention mechanisms include locality-based weighting and dot-product attention. Multihead attention is a parametrised extension that allows for parallel computation of query values using matrix multiplications and softmax. The multihead attention mechanism allows for parallel computation of query values by linearly transforming keys, values, and queries for each head. Self-attention is applied to context points to compute representations of each (x, y) pair, enabling the model to predict the target output by attending to these context representations. The self-attention mechanism models interactions between context points by allowing queries to focus on relevant points. This helps in obtaining richer representations of context points and encoding their relations. The model uses stacked self-attention for higher order interactions. In the deterministic path, a cross-attention mechanism replaces mean-aggregation to allow each query to attend closely to relevant context points for prediction. This mechanism is not present in the latent path. The latent path preserves global latent dependencies in target predictions, while the deterministic path models local structure. The decoder remains the same, with a query-specific representation. ANP is trained using Gaussian likelihood and diagonal Gaussian for added expressivity and accuracy. The NP with attention increases computational complexity but maintains training time comparable to NPs. ANPs learn faster in terms of training iterations and wall-clock time, despite being slower at prediction time. The (A)NP learns a stochastic process and should be trained on multiple functions. At each training iteration, a batch of realisations is drawn from the data generating stochastic process. Random points on these realisations are selected as targets and contexts to optimize the loss. The (A)NPs are trained on data generated from a Gaussian Process with a squared-exponential kernel and small likelihood noise. The hyperparameters of the kernel can be fixed or vary randomly at each training iteration. The number of contexts and targets are chosen randomly at each iteration. The (A)NPs are trained on data generated from a Gaussian Process with a squared-exponential kernel and small likelihood noise. The number of contexts and targets are chosen randomly at each iteration. For this 1D data, cross-attention is explored in the deterministic path without self-attention. ANP shows faster learning and lower reconstruction error compared to NP, especially with dot product and multihead attention. The computation times of Laplace and dot-product ANP are similar to NP for the same value of d, while multihead ANP takes around twice the time. Increasing the bottleneck size (d) in NPs improves reconstructions up to a limit where learning becomes slow. ANPs offer significant benefits over raising the bottleneck size in NPs, as shown in a qualitative comparison of attention mechanisms in FIG2. The predictive mean of the NP underfits the context, while Laplace and dotproduct attention show similar behavior. Dotproduct attention gives accurate predictive means for almost all context points by using parameterized representations of x-values. Dotproduct attention outperforms Laplace attention due to computed similarities in a learned representation space. However, dotproduct attention displays non-smooth predictions. The multiple heads in multihead attention help smooth out interpolations, giving good reconstruction of contexts and prediction of targets with increased uncertainty away from contexts. The ANP is more expressive than the NP, learning a wider range of functions. Trained (A)NPs are used in a toy Bayesian Optimization problem to find the minimum of test functions from a GP prior, showing proof-of-concept. The (A)NP is used for function regression on image data, treating images as generated from a stochastic process. Training is done on MNIST and CelebA datasets with up to 200 context/target points. Self-attentional layers are explored in the encoder for this application. See Appendix D for experimental details. The (A)NP models are used for function regression on image data, with self-attentional layers explored in the encoder. Results of three different models are shown on MNIST and CelebA datasets. The NP gives reasonable predictions with diversity for fewer contexts, while the Stacked Multihead ANP provides accurate reconstructions of the whole image. The use of attention in the Stacked Multihead ANP improves inpaintings and model smooth 2D functions better than the NP. Different values of z show diversity in faces and digits, supporting z's ability to model global image structure. The model generalizes well with limited context points, achieving improved context reconstruction error and NLL compared to the NP. The Stacked Multihead ANP improves context reconstruction error and NLL for target points compared to the NP. Qualitatively, there are gains in crispness and global coherence with stacked self-attention. Each head of Multihead ANP for CelebA has different roles in focusing on pixels. One illustrative application of (A)NPs trained on images is mapping images from one resolution to another by exploiting symmetry of faces. The model can predict pixel intensities in a continuous space, allowing for resolution mapping. However, inaccurate reconstructions may result in target resolution looking different from the original image. The (A)NPs trained on images can accurately map images from one resolution to another, as shown in FIG6. The model can map low resolutions to realistic 32 \u00d7 32 outputs and even higher resolutions up to 256 \u00d7 256, despite never seeing images beyond the original resolution. The model can generate high resolution images with sharper edges compared to baseline methods, showing evidence of learning internal representations of facial features like eyes. The images were produced using the same model for both MNIST and CelebA datasets, optimizing loss function over random context and target pixels. The ANP is not meant to replace state-of-the-art image inpainting algorithms. The ANP is not intended to replace state-of-the-art image inpainting algorithms. Attention in NPs is related to Deep Kernel Learning, where a GP is applied to learned data representations. In Gaussian Processes (GPs) and Neural Processes (NPs), learning is done differently, making direct comparison challenging. GPs rely on kernel choice for predictive uncertainties, while NPs learn uncertainties from data. GPs offer exact covariance expressions but have high computational costs. The marginal variance of predictions can be expressed in closed form, a feature lacking in current Neural Processes (NPs). Variational Implicit Processes (VIP) are related to NPs, approximating the process and its posterior with a Gaussian Process (GP). Meta-Learning (A)NPs focus on few-shot learning, reasoning about new functions using input-output pairs. Various works in few-shot classification use attention mechanisms to locate relevant images/prototypes. Attention has been used for tasks in Meta-RL, few-shot density estimation, and regression settings. Various works explore multitask learning in the GP literature. Generative Query Networks are models for image/prototype retrieval. Generative Query Networks (GQN) are models for spatial prediction that render a scene given a viewpoint. ANPs augment NPs with attention to improve predictions, training speed, and expand function modeling capabilities. Future work includes incorporating cross-attention into the model architecture. One way to incorporate cross-attention into the model architecture is by introducing a global latent, similar to the Neural Statistician setup but adapted for regression. ANPs could be trained on text data to fill in missing information stochastically. The Image Transformer (ImT) BID21 has connections with ANPs, with local self-attention used for predicting pixel blocks. By replacing the MLP in the ANP decoder with self-attention across target pixels, a model similar to ImT on arbitrary pixel orderings can be created. This contrasts with the original ImT, which assumes a fixed pixel order and is trained autoregressively. Incorporating self-attention in the decoder of ANPs can extend their expressiveness, but the ordering and grouping of targets become crucial. Architectural details of NP and Multihead ANP models for 1D and 2D regression experiments are shown. Latent path outputs parameterize distributions for z and y, with specific non-linearities used in the model. The 1D and 2D regression experiments incorporate self-attention in the decoder of ANPs. The self-attention module stacks 2 layers for Stacked Multihead ANP in the 2D Image regression experiments. Stacking more layers did not show significant improvements. The squared exponential kernel parameters are set to l = 0.6 and \u03c32f = 1 for the data generating GP. For the experiments, a length scale of 0.6 and kernel scale of 1 are used for the fixed kernel hyperparameters. Random kernel hyperparameters are sampled from a uniform distribution. A batch size of 16 is used, and Adam Optimiser with a fixed learning rate is employed. The predictions of Multihead ANP are closer to the oracle GP compared to NP in the regression experiments. The Multihead ANP is closer to the oracle GP than the NP but still underestimates predictive variance due to variational inference. The dot-product attention in the conditional distributions shows non-smooth behavior, collapsing to a local minimum and giving good reconstructions but poor interpolations. Investigating how to address this issue would be interesting. The KL term in the NP loss differs between training on fixed and random kernel hyperparameter GP data. In the fixed hyperparameter case, the model quickly deems the deterministic path sufficient for accurate predictions. However, in the random hyperparameter case, the attention gives a non-zero KL to model uncertainty in the stochastic process, using latents to explain variations in the data. Using (A)NPs trained on 1D GP data, the BO problem of finding the minimum of test functions drawn from a GP prior is tackled. ANPs consider all previous function evaluations as context points for informed surrogate modeling. Results show that NP with multihead attention has the smallest simple regret, approaching the oracle GP. Thompson sampling is used to act according to the minimal predicted value. The cumulative regret decreases rapidly for multihead NP compared to the oracle GP due to under-exploration. Random pixels of images are used as targets and contexts, with rescaled x and y values. A batch size of 16 is used for MNIST and CelebA datasets, with specific learning rates for each. The stacked self-attention architecture used for MNIST and CelebA datasets does not include Dropout or positional embeddings. Little tuning has been done on the architectural hyperparameters. The NP overestimates predictive variance, but with attention, uncertainty is reduced as context sets increase. The uncertainty in the NP is reduced with attention as the number of contexts increases, almost disappearing for the full context. Stacked Multihead ANP improves results significantly over Multihead ANP, providing sharper images with better global coherence even when the face isn't axis-aligned. Different heads play various roles, with all heads becoming useful for target prediction when the context is disjoint from the target. Visualizations show pixels attended by each head of multihead attention in the NP given a target pixel and separate context."
}