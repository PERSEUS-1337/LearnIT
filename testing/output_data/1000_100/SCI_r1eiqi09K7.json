{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, and algorithms and convergence proofs are provided for geodesically convex objectives in the case of a product of Riemannian manifolds. The generalization allows adaptivity across manifolds in the cartesian product, with experimentally faster convergence shown. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when optimizing a large number of parameters. Recent advancements have led to successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD. These algorithms are designed for parameters in a Euclidean space and have shown faster convergence and lower train loss values in tasks like embedding the WordNet taxonomy in the Poincare ball. Recent advancements in optimization algorithms have focused on parameters in a Riemannian manifold, allowing for non-Euclidean geometries. These algorithms have been applied to various tasks such as solving Lyapunov equations, matrix factorization, and dictionary learning. Seminal methods like Riemannian stochastic gradient descent (RSGD) have been adapted for this setting, with new methods for convergence analysis in the geodesically convex case. In this work, the authors discuss the challenges of generalizing adaptive optimization algorithms to Riemannian manifolds and propose new algorithms for a product of manifolds. They provide convergence analysis and empirical support for their claims using hyperbolic taxonomy embedding as a realistic task. The authors developed Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces, benefiting algorithms like GloVe for optimizing word embeddings. The absence of Riemannian adaptive algorithms could hinder competitive optimization-based Riemannian embedding methods, especially in hyperbolic spaces. A manifold M of dimension n is a space that can locally be approximated by a Euclidean space R n. It can be understood as a generalization to higher dimensions of the notion of surface. At each point x \u2208 M, one can define the tangent space T x M, which is an n-dimensional vector space and can be seen as a first order local approximation of M around x. A Riemannian metric \u03c1 is a collection of inner-products on T x M, varying smoothly with x, defining the geometry locally on M. A Riemannian manifold is defined as a pair (M, \u03c1) where \u03c1 is a Riemannian metric inducing a global distance function on M. Riemannian SGD involves updating a smooth function f on M using the Riemannian gradient and the exponential map. The exponential map allows for updates along the shortest path in the relevant direction on a Riemannian manifold. In practice, the exponential map can be approximated by a retraction map. Two main algorithms of interest are ADAGRAD and ADAM, which involve rescaled updates based on past gradients and momentum terms, respectively. The ADAM algorithm introduces a momentum term and an adaptivity term, which helps in forgetting past gradients over time. A recent improvement called AMSGRAD fixes a convergence issue in ADAM by either modifying the algorithm or using a time-dependent schedule for a parameter. The ADAM algorithm introduces momentum and adaptivity terms to forget past gradients. Intrinsic updates on a Riemannian manifold require a coordinate system, with quantities defined intrinsically to the manifold. The gradient of a smooth function on a Riemannian manifold can be intrinsically defined, but its Hessian is only defined at critical points. The RSGD update is intrinsic as it involves exp and grad, intrinsic objects to the manifold. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free manner. One possible approach is to fix a canonical coordinate system in the tangent space at initialization and parallel-transport it along the optimization trajectory. In Euclidean space, parallel transport between two points x and y is path-independent due to no curvature. However, in a Riemannian manifold, curvature introduces a rotational component to parallel transport, affecting gradient sparsity and adaptivity. The interpretation of adaptivity as optimizing different features at different speeds is lost as the coordinate system for gradients depends on the optimization path. Theorems' techniques do not apply to updates defined differently. Additional structure is assumed on (M, \u03c1) as the cartesian product of n Riemannian manifolds, with induced product metric. In a Riemannian manifold, the induced distance function and parallel transport are defined by the product metric. Designing adaptive schemes in such manifolds is challenging due to the absence of intrinsic coordinates. A proposed solution is to treat each component as a coordinate, allowing for a simple adaptation of the gradient rescaling. This approach differs from traditional Euclidean settings and involves techniques like ADAGRAD, ADAM, and AMSGRAD. ADAM is a combination of ADAGRAD with momentum and an exponential moving average. AMSGRAD is a modification of ADAM for improved convergence. ADAMNC is ADAM with a non-constant schedule for parameters. The schedule proposed for ADAMNC allows recovery of ADAGRAD's squared-gradients sum. Riemannian AMSGRAD is presented in FIG1, a modification of ADAM for improved convergence. It involves a family of differentiable, geodesically convex functions on a product manifold. The convergence guarantees bound the regret after T rounds. Riemannian AMSGRAD is a modification of ADAM for improved convergence, presented in FIG1. The convergence guarantee for RAMSGRAD is discussed in Theorem 1, with \u03b6 defined as a key quantity. Comparisons with the original AMSGRAD algorithm and the impact of curvature on regret bounds are also explored. The convergence guarantees for RAMSGRAD and RADAMNC algorithms are discussed in Theorems 1 and 2 respectively. The role of convexity in the convergence proofs is highlighted, with a comparison between differentiable functions and geodesic convexity. The differentiable functions f : R n \u2192 R and g : M \u2192 R are convex and geodesically convex respectively. Regret bounds for convex objectives involve bounding g t , x t \u2212 x * using a cosine law. For an SGD update, this simplifies to a telescopic summation and requires a well-chosen decreasing schedule for \u03b1. In Riemannian manifolds, this is generalized using lemma 6. In Riemannian manifolds, lemma 6 is generalized for geodesically convex subsets with lower bounded sectional curvature. The curvature dependent quantity \u03b6 from Eq. (10) allows us to bound \u03c1. Sparse gradients can lead to improved bounds, especially when updating just a few words at a time. The choice of \u03d5 i does not need to be specified for convergence theorems, suggesting potential improvements with momentum/acceleration. Empirical assessment of the proposed algorithms is conducted. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed compared to the non-adaptive RSGD method. The WordNet noun hierarchy is embedded in the Poincar\u00e9 model of hyperbolic geometry, known for its suitability in embedding tree-like graphs. The optimization tools in the Poincar\u00e9 model are justified by closed form expressions for all quantities used. The curr_chunk discusses the use of gradients, exponential and logarithmic maps, and parallel transport in a WordNet taxonomy graph embedding model. The dataset consists of nouns and hypernymy relations, with a loss function similar to log-likelihood. The metrics reported include loss value and other evaluation measures. The curr_chunk discusses using the direction of edges in a graph for metrics. It reports loss value and mean average precision for each directed edge. Different settings are used for reconstruction and link prediction. Training details include a \"burn-in phase\" and sampling negative words based on graph degree. This strategy improves all metrics by sampling negatives uniformly and using optimization methods like RADAM over RAMS-GRAD. Convergence to lower loss values was observed when replacing the true exponential map with its first-order approximation. Results for \"exponential\" and \"retraction\" based methods are shown in FIG2. In FIG2, results for \"exponential\" and \"retraction\" based methods with different learning rates are shown. RADAM consistently achieves the lowest training loss and outperforms other methods on the MAP metric for both reconstruction and link prediction settings. RAMSGRAD converges faster in terms of MAP for link prediction, indicating better generalization capability compared to RADAM and RSGD. Various first-order Riemannian methods have emerged after the introduction of Riemannian SGD by BID1, including Riemannian SVRG, Riemannian Stein variational gradient descent, and Riemannian accelerated gradient descent. Stochastic gradient Langevin dynamics has also been generalized to improve optimization on the probability simplex. Riemannian counterparts of SGD with momentum and RMSprop have been proposed by BID20, suggesting the transportation of the momentum term using parallel translation. The curr_chunk discusses the limitations of previous algorithms for Riemannian optimization on the Grassmann manifold G(1, n) and proposes a generalized approach for adaptive optimization tools. The algorithm by BID3 removes the adaptive component and lacks adaptivity across manifolds, while also lacking convergence analysis. The proposed approach aims to address these shortcomings by extending popular adaptive optimization tools to Cartesian products. The curr_chunk introduces a method that extends popular adaptive optimization tools to Cartesian products of Riemannian manifolds, showing improved performance over non-adaptive methods like RSGD. The text also includes mathematical formulas and inequalities to derive convergence rates. The curr_chunk discusses mathematical inequalities and lemmas related to the convergence of gradient-based optimization algorithms in Alexandrov spaces. These tools are used to prove convergence rates for geodesically convex functions. Lemma 8 (BID0) states a mathematical inequality for non-negative real numbers y1, ..., yt. This lemma is crucial for proving convergence in optimization algorithms in Alexandrov spaces."
}