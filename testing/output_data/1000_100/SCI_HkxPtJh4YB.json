{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for exponential family defined over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, addressing the problem of marginal inference for large permutations. The effectiveness of this method is demonstrated in probabilistic neuron identification in C.elegans. The Sinkhorn variational marginal inference method efficiently computes the matrix of expectations \u03c1 by approximating it as S(L), the Sinkhorn operator applied to L. This approach is shown to produce the best results in probabilistic neuron identification in C.elegans. The Sinkhorn approximation is effective in producing optimal results for marginal inference. It is based on the relation between marginal inference and the normalizing constant in exponential families. The optimization problem links marginal inference and computation of the permanent, with the dual function playing a key role in the process. The Sinkhorn approximation is used for marginal inference by replacing the intractable dual function with component-wise entropy. This approximation, called the Sinkhorn permanent, provides bounds for the normalizing constant. It has been proposed independently but without a theoretical framework. The Bethe variational inference method is a general approach for obtaining variational approximations in graphical models. It approximates the dual function A*(\u00b5) by assuming a tree structure in the underlying Markov random field. This method has been successfully applied to permutations, with the approximate marginal B(L) computed through belief propagation. The Bethe approximation of the permanent has known bounds, offering better theoretical guarantees than the Sinkhorn approximation. However, there are significant computational differences. The Sinkhorn and Bethe algorithms have computational differences. Sinkhorn produces better marginals in some cases, while Bethe offers better permanents. Fig 1 (b) shows Bethe's better permanent approximations. The Sinkhorn approximation scaled better for moderate n compared to Bethe. A comparison of Bethe and Sinkhorn approximations was done using submatrices from the C.elegans dataset. Sampling-based methods can also be used for marginal estimation. Recent advances in neurotechnology have enabled whole brain imaging of the C.elegans worm, a species with a stereotypical nervous system. The number of neurons (roughly 300) and their connections remain unchanged from animal to animal. However, a technical challenge remains in identifying the neurons from volumetric images before studying how brain activity relates to behavior. Sampling-based methods, including an elementary MCMC sampler, have been explored for marginal inference, but practical limitations exist. In the context of whole brain imaging of C.elegans, a methodology for probabilistic neural identification using NeuroPAL was applied. The goal is to assign canonical labels to observed worm neurons based on position and color vectors. Gaussian models are used to estimate probabilities of neuron identification, providing uncertainty estimates for model predictions. NeuroPAL methodology assigns canonical labels to worm neurons based on position and color vectors. Gaussian models estimate probabilities for neuron identification, allowing uncertainty estimates for model predictions. Posterior over P is induced by likelihood of observing data Y, with a downstream task involving manual labeling of neurons with uncertain model estimates. The uncertainty in neuron identification decreases as humans annotate neurons, leading to increased accuracy. Different approximation methods were compared, including Sinkhorn, Bethe, MCMC, random baseline, naive baseline, and ground truth labeling. The Sinkhorn and Bethe approximations show similar results, with Sinkhorn slightly outperforming Bethe due to more accurate estimates of low probability marginals. Both methods outperform all baselines except for the oracle. MCMC does not perform better than the naive baseline, indicating convergence issues and comparable computational times to approximated methods. Sinkhorn approximation is proposed as a faster and more accurate alternative to sampling for marginal inference. The curr_chunk discusses the relation between quality of permanent approximation and corresponding marginals, as well as the (log) Sinkhorn approximation of the permanent of L. It also mentions using a dataset of NeuroPAL worm heads with log-likelihood matrices. The curr_chunk discusses the efficient implementation of message passing algorithms for Sinkhorn and Bethe approximations, using 200 iterations for each. Results were obtained using MCMC sampler with 100 chains of length 1000. All computations were done on a desktop computer with an Intel Xeon W-2125 processor. The parameter eps was introduced for numerical stability. The parameter eps is introduced for numerical stability. 1000 submatrices of size n were randomly drawn from ten log likelihood C.elegans matrices for analysis. Error bars were too small to be displayed."
}