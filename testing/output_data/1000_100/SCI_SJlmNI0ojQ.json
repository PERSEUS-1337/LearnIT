{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are popular for their ease of training, scalability, and lack of a lexicon requirement. Contextual acoustic word embeddings are constructed from a supervised sequence-to-sequence model, showing competitive performance on standard sentence evaluation tasks and spoken language understanding. In the natural language processing and speech recognition communities, research focuses on learning fixed-size representations for variable length data like words or sentences. Popular methods include word2vec, GLoVE, CoVe, and ELMo for text-based embeddings, while speech recognition deals with short-term audio features. The challenge lies in capturing variability in speakers, acoustics, and microphones for word representations. Our work focuses on learning acoustic word embeddings from utterance-level acoustics for speech recognition. Unlike traditional methods that ignore audio context, we present various techniques for obtaining these embeddings using an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word (A2W) speech recognition. In this paper, the authors demonstrate the usability of attention for aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE) in direct Acoustic-to-Word (A2W) speech recognition. They show that their methods for constructing word representations directly from a speech recognition model are highly effective. The authors demonstrate the effectiveness of constructing word representations directly from a speech recognition model using Contextual Acoustic Word Embeddings (CAWE). They show that CAWE is competitive with text-based word2vec embeddings on various evaluation benchmarks and can be used for transfer learning in speech-based downstream tasks. The text discusses generating out-of-vocabulary words in speech recognition models by restricting vocabulary and using smaller units like characters or sub-words. It mentions S2S models for large vocabulary A2W recognition and improving training for this task. The authors also explore ways to learn acoustic embeddings from the same model. The text discusses methods to learn acoustic word embeddings, with BID4 proposing an unsupervised approach using fixed context, while BID6 uses a supervised CNN model for speech recognition. Learning text-based word embeddings is also mentioned as a rich area of research. Our work ties A2W speech recognition model with learning contextual word embeddings from speech. The S2S model is similar in structure to the Listen, Attend and Spell model, consisting of an encoder network, a decoder network, and an attention model. The encoder utilizes a pyramidal multi-layer bi-directional Long Short Term Memory (BLSTM) network to map input acoustic features vectors into higher-level features. The decoder network is an LSTM network. The decoder network in the A2W speech recognition model is an LSTM network that models the output distribution over the next target based on previous predictions. It uses an attention mechanism to generate targets from the hidden state. The model follows the experimental setup of word-based models with the difference of learning 300 dimensional acoustic feature vectors instead of 320 dimensional. Our method involves obtaining acoustic word embeddings from an end-to-end trained speech recognition system. The model uses hidden representations from the encoder and attention weights from the decoder to construct \"contextual\" embeddings. A location-aware attention mechanism helps in aligning input speech with output words, allowing for automatic segmentation of continuous speech into words. The method involves segmenting continuous speech into words and obtaining word embeddings using attention weights on acoustic frames' hidden representations. This process constructs word representations based on the importance of each frame to the word. The encoder generates higher-level features for each frame, which are then used to create contextual acoustic word embeddings. The model obtains mappings of words to acoustic frames using attention weights. Three methods are described for obtaining acoustic word embeddings: unweighted Average, Attention weighted Average, and maximum attention. These methods utilize the hidden representations of acoustic frames to create contextual acoustic word embeddings. The Contextual Acoustic Word Embeddings (CAWE) are obtained by using attention scores to find the hidden representation of acoustic frames with the highest attention score for a given word. Two datasets are used for evaluation, the Switchboard corpus and a subset of the How2 dataset. The A2W method achieves a word error rate of 22.2% on Switchboard. The A2W method achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome from the Switchboard Eval2000 test set, and 24.3% on dev5 test set of How2. The embeddings are evaluated in 16 benchmark tasks covering Semantic Textual Similarity, classification, sentiment analysis, question type, subjectivity/objectivity, opinion polarity, entailment, semantic relatedness, and paraphrase detection using various datasets. The SentEval toolkit BID26 is used to evaluate classification tasks with logistic regression. CAWE-M outperforms U-AVG and CAWE-W on Switchboard and How2 datasets for STS tasks. The comparison between CAWE-M and U-AVG/CAWE-W shows that CAWE-M performs better due to using the most confident attention scores, while U-AVG performs worse by weighting all encoder hidden representations equally. The datasets for downstream tasks remain the same as described earlier. Training details involve comparing embeddings from the speech recognition model's training set and text-based word embeddings from a CBOW word2vec model trained on all transcripts. The comparison between CAWE-M and U-AVG/CAWE-W shows that CAWE-M outperforms due to using confident attention scores. Despite A2W model's limited vocabulary, CAWE performs competitively with word2vec CBOW. Evaluations with CAWE-M from Switchboard training demonstrate superior performance on 10 out of 16 tasks when acoustic embeddings are combined with text embeddings. The concatenated embedding of CAWE-M improves the CBOW embedding, especially in Switchboard compared to How2 dataset. CAWE is evaluated on the ATIS dataset for Spoken Language Understanding, which is similar in domain to Switchboard. The model architecture is a simple RNN based on BID29 investigation. Our model architecture includes an embedding layer, a single layer RNN variant (Simple RNN, GRU), a dense layer, and softmax. We train the model for 10 epochs with RMSProp (learning rate 0.001) and test it 3 times with different seed values. Results show that speech-based word embeddings can perform as well as text-based embeddings in downstream tasks, highlighting the utility of our approach. We compare test scores using CAWE-M, CAWE-W, and CBOW embeddings, fine-tuning them based on the task. The method presented learns contextual acoustic word embeddings from a speech recognition model, showing competitive performance with word2vec text embeddings. Two variants of these embeddings outperform the simple average method by up to 34% in semantic textual similarity tasks. These embeddings match text-based embeddings in spoken language understanding, indicating their potential for pre-trained models in speech-based tasks. Contextual audio embeddings are expected to enhance downstream tasks similar to text embeddings, despite challenges from noisy audio input. In the future, the model will be scaled to larger corpora and vocabularies, and compared with non-contextual acoustic word embedding methods. This research was supported by the Center for Machine Learning and Health at Carnegie Mellon University and Facebook."
}