{
    "title": "SJg9z6VFDr",
    "content": "Recently, various neural networks have been proposed for irregularly structured data such as graphs and manifolds. All existing graph networks have discrete depth, but a new model called graph ordinary differential equation (GODE) extends the idea of continuous-depth models to graph data. The derivative of hidden node states is parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. Two efficient training methods for GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. Direct backprop has been shown to outperform the adjoint method in experiments. Additionally, a family of bijective blocks is introduced to enable efficient memory consumption. GODE is a model for graph neural networks with $\\mathcal{O}(1)$ memory consumption. It improves accuracy in various tasks and is adaptable to different graph networks. Unlike CNNs, which are limited to grid data like images and text, GODE works well with irregularly structured datasets represented as graphs. The graph data structure represents objects as nodes and relations as edges, widely used for modeling irregularly structured data like social networks, protein interactions, and citation graphs. Traditional methods like random walk and graph embedding have been surpassed by graph neural networks (GNN), inspired by CNNs, which generalize convolution operations to capture local information on graphs through spectral and non-spectral methods. Graph neural networks (GNN) utilize spectral and non-spectral methods for graph data processing. Spectral methods involve computing the graph Laplacian for filtering, while non-spectral methods directly perform convolution in the graph domain. Existing GNN models have discrete layers, limiting their ability to model continuous diffusion processes in graphs. The neural ordinary differential equation (NODE) approach views a neural network as an ordinary differential equation to address this limitation. In this work, the authors propose graph ordinary differential equations (GODE) to model message propagation on graphs. They address the limitations of NODEs in image classification tasks by identifying errors in gradient estimation during training and introducing a memory-efficient framework for accurate gradient estimation. The authors propose a memory-efficient framework for accurate gradient estimation in free-form ODEs, demonstrating high accuracy for both NODE and GODE in benchmark tasks. Their framework significantly improves performance on benchmark classification, achieves constant memory usage for restricted-form invertible blocks, generalizes ODE to graph data with GODE models, and shows improved performance on various graph models and datasets. Several new architectures based on numerical methods in ODE solver have been proposed, including NODE which treats the neural network as a continuous ODE. The adjoint method has been applied to ODE, and augmented neural ODEs have been proposed to improve the expressive capacity of NODEs. However, none of these methods address the inaccurate gradient estimation issue, leading to inferior empirical performances of NODE in benchmark classification tasks. Spectral and non-spectral Graph Neural Networks (GNNs) have different approaches in filtering information on graphs. Spectral GNNs operate in the Fourier domain of a graph, requiring information of the entire graph for filtering, while non-spectral GNNs focus on message aggregation around neighbor nodes, leading to localized computations. Various spectral methods have been introduced to improve the efficiency of graph convolution, such as incorporating graph estimation procedures and parameterizing filters with smooth coefficients. Chebyshev expansion has also been used to approximate filters in GNNs. Defferrard et al. (2016) and Kipf & Welling (2016) introduced efficient methods for graph convolution using Chebyshev expansion and localized first-order approximation, respectively. Other approaches like MoNet, GraphSAGE, graph attention networks, and GIN have also been proposed to improve graph convolution performance. The GIN (graph isomorphism network) has a structure as expressive as the Weisfeiler-Lehman graph isomorphism test. Invertible blocks are neural network blocks with a bijective mapping, used in normalizing flow for invertibility. Bijective blocks have been utilized by Jacobsen et al. (2018) to build invertible networks and by Gomez et al. (2017) for memory-efficient backpropagation without storing activation. Invertible blocks are used in normalizing flow for invertibility. Discrete-layer models with residual connections can be represented as neural ordinary differential equations. The forward pass of models with discrete layers involves differentiable functions and shared weights. The forward pass of a NODE involves integrating states with an output layer applied at the end. Various ODE solvers can be used for integration, and the adjoint method is commonly used for optimal process control. Model parameters are denoted as \u03b8, which remains constant over time. The direct back-propagation through ODE solver involves saving evaluation time points during forward pass and re-building the computation graph during backward pass. This method ensures accurate reconstruction of the hidden state, leading to precise gradient evaluation for the loss function L. The reverse-time integration in the optimization process involves solving equations backward to minimize the loss function L. This method requires determining z(t) by solving Eq. 2 in reverse-time, which can lead to inaccurate gradients in adjoint methods. The backward pass involves determining f(z(t), t, \u03b8) and solving Eq. 2 in reverse-time, with initial conditions from Eq. 5 at time T. The hidden state solved forward-time (z(t i )) and reverse-time (h(t i )) may not be equal due to instability of reverse-time ODE, causing a mismatch between z(t) and h(t). Error h(t) \u2212 z(t) affects gradient dL d\u03b8. Proposition 1 states that if the Jacobian of the ODE has eigenvalues with non-zero real parts, either forward-time or reverse-time ODE is unstable. Large |Re(\u03bb)| makes ODE sensitive to numerical errors. This instability impacts accuracy. The instability of reverse-time ODE affects the accuracy of computed gradients. To address this, direct back-propagation through the ODE solver is proposed, ensuring accurate hidden states h(t i). This can be achieved by saving activation z(t i) in cache or reconstructing z(t i) at evaluated time points {t i}. This method guarantees accurate direct back-propagation regardless of the model evaluation time points. The direct back-propagation through the ODE solver guarantees accurate hidden states regardless of stability. The adjoint for each step in the discrete forward-time ODE solution is defined, with Eq. 7 as a numerical discretization of Eq. 6. The derivation of Eq. 6-7 from an optimization perspective is detailed in appendix E and F. Algorithm 1 outlines accurate gradient estimation in ODE solver for free-form functions. During forward pass, the solver integrates numerically with adaptive stepsize based on error estimation and outputs the integrated value and evaluation time points. All middle activations are deleted to save memory. During backward pass, the solver rebuilds the computation graph by directly evaluating at saved time points without adaptive searching, performing a numerical reverse-time integration. During the backward pass, the solver performs reverse-time integration numerically. Our algorithm supports free-form continuous dynamics without constraints on the function form. Memory consumption analysis shows our method is more efficient than a naive solver, with reduced memory usage when using step-wise checkpoint method. The solver can handle free-form functions and restricted form invertible blocks. The algorithm supports free-form continuous dynamics during the backward pass, with reduced memory consumption using invertible blocks. The input is split into two parts, processed by differentiable neural networks, and bijective functions are utilized for block operations. The algorithm supports free-form continuous dynamics during the backward pass, with reduced memory consumption using invertible blocks. Bijective functions are utilized for block operations, enabling accurate reconstruction of x from y without storing activations. Graph neural networks are introduced with discrete layers, extending to graph ordinary differential equations (GODE). GNNs are generally represented in a message passing scheme, where nodes and edges are used to visualize graphs. Graph neural networks (GNN) utilize message passing, aggregation, and updating stages for node information processing. The message passing stage involves neighbor nodes sending information to a specific node, followed by aggregation of messages and node state updates. GNNs operate with permutation invariant functions like mean and sum for message aggregation. Graph neural networks (GNN) utilize message passing and aggregation stages for node information processing. The graph ordinary differential equation (GODE) converts discrete-time GNN to continuous-time GNN, capturing highly non-linear functions. GODE's asymptotic stability is related to over-smoothing phenomena. Graph convolution is a special case of Laplacian smoothing, demonstrating the potential of GODE to outperform discrete-layer counterparts. The continuous smoothing process of the symmetrically normalized Laplacian in GODE leads to asymptotic stability, causing over-smoothing phenomena. Integration time T affects the similarity of node features, impacting classification accuracy in experiments on image and graph datasets. The study evaluated their method on various benchmark graph datasets, including bioinformatic, social network, and citation networks. They input raw datasets into their models without pre-processing for graph classification tasks and followed specific train-validation-test splits for node classification tasks. For image classification tasks, they modified a ResNet18 into a NODE model with a specific function for each block. GODE can be applied to any graph neural network by replacing f with corresponding structures. It is easily generalized to existing GNN architectures like GCN, GAT, ChebNet, and GIN. The study trained GNNs with different depths of layers and reported the best results for each model structure. The study compared different GNN structures with varying depths of hidden layers and hyper-parameters. Direct back-propagation was found to yield higher accuracy than the adjoint method. CNN-NODE modified a ResNet18 into NODE18 for classification tasks. The study compared different GNN structures with varying depths of hidden layers and hyper-parameters. Direct back-propagation outperformed the adjoint method. NODE18, a modified ResNet18, showed superior performance on image classification tasks and benchmark graph datasets. Robustness to ODE solvers was demonstrated with adaptive solvers of different orders. The study demonstrated the robustness of the method to different orders of ODE solvers and support for free-form functions in NODE and GODE models. Generalized bijective blocks were shown with adaptable neural networks and differentiable mappings. Results for different \u03c8 are reported in Table 3, showing that most GODE models outperformed their discrete-layer counterparts significantly. Different \u03c8 functions behaved similarly on node classification tasks, highlighting the importance of the continuous-time model. Lower memory cost was also validated, with details in appendix B. Results for various models on graph classification tasks are summarized in Table 4, with comparisons between GODE and its discrete-layer counterparts. The study compared GODE models with discrete-layer counterparts, showing better performance for GODE. Integration time in NODE and GODE models was tested during inference, with results indicating the importance of continuous diffusion processes on graphs. A memory-efficient back-propagation method was proposed for NODEs, demonstrating superior performance in image classification and graph data tasks. The over-smoothing of GNN was linked to the asymptotic stability of ODE. Our paper addresses the gradient estimation problem for NODE, improving accuracy on benchmark tasks to match state-of-the-art discrete layer models. Experiments were conducted on various datasets including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained with modifications to a family of bijective blocks. Our paper introduces a parameter state checkpoint method for bijective blocks, allowing for accurate inversion. The algorithm is outlined in Algo. 2, with pseudo code for forward and backward functions in PyTorch. Memory consumption is reduced by keeping only necessary outputs in the forward function. The bijective block is shown to be memory efficient through experiments with a GODE model. We trained a GODE model with bijective blocks and compared memory consumption using our memory-efficient function to a memory-inefficient method. Results on the MUTAG dataset showed that our method significantly reduced memory usage compared to conventional backpropagation. The memory consumption of bijective blocks was measured at different depths, with our memory-efficient approach showing minimal increase in memory usage as depth increased. Our bijective block is designed to only store necessary outputs, resulting in O(1) memory usage. The memory-efficient network reduces memory consumption by caching states of F and G, with minimal impact compared to input data. The stability of ODEs in forward and reverse time requires eigenvalues of J to have non-positive real parts. The stability of ODEs in forward and reverse time requires eigenvalues of J to have non-positive real parts. Theorem 1 states that a bijective block with defined mappings is a bijective mapping. The proof shows that the forward mapping is both injective and surjective. In the proposition statement, the mappings Forward and Reverse are defined. By constructing specific x values, the mapping is shown to be surjective and bijective. A computation graph is used to derive gradients for a neural-ODE model, extending from continuous to discrete cases. Notations for hidden states, parameters, inputs, targets, and losses are defined, with a focus on optimization perspectives. The continuous model is defined by an ODE with a forward pass and loss function. The training process is formulated as an optimization problem, considering one ODE block. The Lagrangian Multiplier Method is used to solve the optimization problem, with Karush-Kuhn-Tucker (KKT) conditions being necessary for optimality. The Karush-Kuhn-Tucker (KKT) conditions are necessary for optimality in a continuous model defined by an ODE. Derivatives are used to derive results, with a focus on perturbations and Leibniz integral rule. Transitioning to discrete cases involves replacing integrations with finite sums, resulting in a discrete version of the ODE condition. The discrete version of the ODE condition corresponds to the analysis in Eq. 10 and 11."
}