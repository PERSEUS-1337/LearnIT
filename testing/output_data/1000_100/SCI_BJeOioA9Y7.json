{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The teachers and student can have different structures and be trained on different tasks. The student becomes independent of the teachers after training, outperforming fine-tuning and other knowledge exchange methods in various learning tasks. Research communities have developed numerous deep net architectures for various tasks, with some trained from scratch and others fine-tuned using structurally similar networks. In reinforcement learning, methods like progressive neural nets, PathNet, 'Growing a Brain,' and Actor-mimic involve utilizing multiple teachers to extract useful features for new tasks. Knowledge flow addresses limitations of existing techniques by transferring knowledge from multiple models to a smaller student model, overcoming issues such as computational intensity and reliance on a single pretrained model for performance. Knowledge flow addresses limitations of existing techniques by transferring knowledge from multiple models to a smaller student model, ensuring independence of the student at the final training stage. The framework allows flexibility in choosing teacher models and is applicable to various tasks, from reinforcement learning to fully-supervised training. The approach is evaluated on different tasks and compared based on the discount factor for expected future rewards. The goal of reinforcement learning is to maximize future rewards by finding a policy through the A3C formulation. The policy and value function are modeled by deep nets with parameters \u03b8 \u03c0 and \u03b8 v. The policy parameters are optimized using a loss function based on negative log-likelihood and entropy regularization. The empirical k-step return is calculated to update the policy. The empirical k-step return is used to optimize the policy and value function in reinforcement learning. The entropy function encourages exploration by favoring a uniform probability distribution. To address shortcomings, a framework called knowledge flow moves 'knowledge' from deep nets to improve learning efficiency. Knowledge flow involves transferring 'knowledge' from multiple pre-trained deep nets, referred to as 'teachers', to a deep net under training, known as the 'student'. The student initially relies heavily on one teacher, but as training progresses, the student's weight increases while the teacher's weight decreases until the student becomes independent. This process is illustrated using example deep nets in FIG0. Teachers' parameters are fixed and obtained from pre-trained models on different tasks using various algorithms such as A3C, A2C, or DQN for reinforcement learning. The student net is modified by adding transformed and scaled intermediate representations from teacher deep nets. Teacher representations are transformed by a trainable matrix and scaled via a normalized weight. The normalized weights determine which representations to trust at each student layer. The student model should perform well on the target task without relying on teachers after training. The student model is encouraged to gradually become independent from the teacher's knowledge during training. This is achieved by introducing two additional loss functions, one measuring the student's reliance on teachers and the other ensuring a smooth transition as the teacher's influence decreases. During training, the student model gradually becomes independent from the teacher's influence by introducing additional loss functions to measure reliance on teachers and ensure a smooth transition. Loss terms are modified to decrease teacher influence, controlled by parameters \u03bb 1 and \u03bb 2. During training, the student model gradually becomes independent from the teacher's influence by introducing additional loss functions to measure reliance on teachers and ensure a smooth transition. The weight for teacher layers is decreased to reduce negative transfer effects. Low level knowledge transfer from teachers to students is observed in experiments. Modifications to deep nets and loss functions dep and KL are described to decrease teacher influence in the student model. The student model combines layers from multiple teachers to create its own representation. Normalized weights are used to decide which representation to trust at each layer. The combined intermediate representation is obtained using specific formulas. The framework limits the number of introduced matrices. Not every student layer is linked to every teacher layer. The student gradually becomes independent from teacher influence during training. In practice, it is recommended to link one teacher layer to one or two student layers, introducing matrices Q. Additional trainable parameters Q and w are used as auxiliary knobs to help the student learn faster. In the final training stage, the student becomes independent and no longer relies on Q, w, or any transformed representations. During training, the student gradually becomes independent from the teachers by increasing its weights for each layer. This is achieved by minimizing the dependence cost, encouraging the student to rely less on the transformed representations from teachers. Fast decrease of teacher influence can degrade performance as it takes time to find good transformations. During training, the student model gradually becomes independent from the teachers by decreasing their influence slowly to prevent performance loss. A Kullback-Leibler regularizer is used to control the speed of change in the student's output distribution. Knowledge flow is evaluated in reinforcement and supervised learning tasks, with results reported using only the student model to avoid any influence from teachers. In reinforcement learning, Atari games are used as input to the agent. The agent learns to predict actions based on rewards and input images from the environment. It chooses an action every four frames, with the last action repeated on skipped frames. The model has three hidden layers with specific filter sizes and strides. Outputs include a probability distribution over actions and an estimated value function. Hyper-parameter settings are similar to previous work, except for the learning rate. We use Adam with shared statistics instead of RMSProp as in BID17, setting the learning rate to 10^-4. To select \u03bb 1 and \u03bb 2, we follow BID23 by randomly sampling values. Each experiment is repeated 25 times with different seeds. Evaluation is done by playing each game for 30 episodes using 16 agents on 16 CPU cores in parallel, following the procedure of BID16. The student models are evaluated by playing each game for 30 episodes, following the 'no-op' procedure. Results show that compared to PathNet and progressive neural net (PNN), our transfer framework achieves higher scores in most experiments. The student model trained with our framework has fewer parameters than PNN but still performs better in several experiments, demonstrating effective knowledge transfer from teachers to students. In our framework, increasing the number of teachers from one to two significantly improves student performance across all experiments. Training curves in FIG1 show our approach performing well. Different environment/teacher settings were tested, with results summarized in TAB2. \"Ours w/ expert\" has one expert teacher, \"ours w/ non-expert\" has both non-expert teachers, \"Fine-tune\" involves fine-tuning from a non-expert, \"A3C baseline\" is our implementation of the A3C baseline. Our A3C implementation outperforms the scores reported by BID17. Knowledge flow with expert teachers transfers knowledge successfully to students, outperforming baseline. Multiple teachers in knowledge flow benefit student learning, avoiding negative impacts from insufficiently pretrained teachers. The training process benefits from knowledge flow with expert teachers, even if input, output, and objectives differ. The student model achieves significantly higher scores when learning from teachers compared to learning without them. Various image classification benchmarks are used for supervised learning, with parameters determined using validation sets. The study evaluates student models using top-1 error rates on CIFAR-10 and CIFAR-100 datasets. Training and test sets contain 50,000 and 10,000 images respectively. Densenet (depth 100, growth rate 24) is used as a baseline for experiments with standard data augmentation. Teachers are trained on CIFAR-10, CIFAR-100, and SVHN before training the student model with different teacher combinations. Results are compared to fine-tuning and the baseline. The study compares results of fine-tuning from different teachers on CIFAR-10 and CIFAR-100 datasets. Knowledge flow improves by 13% over the baseline when leveraging a good teacher's knowledge. Results are similar on the CIFAR-100 dataset, demonstrating the effectiveness of knowledge flow. Additional results are available in the appendix. In contrast to previous approaches like PathNet and Progressive Net, our method utilizes multiple pre-trained teacher nets and ensures independence of the student during training. Distral combines distillation and transfer learning for joint training of multiple tasks, sharing a distilled policy for common behavior. Knowledge flow leverages multiple teachers' information to help a student learn a new task, different from multi-task learning where information from different tasks is shared to boost performance. Other related work includes actor-mimic, learning without forgetting, policy distillation, domain adaptation, knowledge distillation, and lifelong learning. The text discusses a general knowledge flow approach for training deep nets from multiple teachers, showing improvements in reinforcement learning and supervised learning. They plan to explore when to use different teachers and how to swap them during student training. Experiments were conducted on MNIST, MNIST with missing digit '3', CIFAR-100, and ImageNet using teacher and student models with varying parameters. The student model is an MLP with two hidden layers of 800 units. It follows the structure of the teacher model but with halved output channels. For CIFAR-100, the teacher model is from Chen FORMULA2, and for ImageNet, it is a 50-layer ResNet BID8 compared to an 18-layer ResNet for the student model. The framework outperforms KD as the student benefits from both the output layer and intermediate layer representations of the teacher. The 'EMNIST Letters' dataset has 26 balanced classes of handwritten letters, while the 'EMNIST Digits' dataset consists of images of digits. The 'EMNIST Digits' dataset contains 10 balanced classes of 28x28 pixel images of handwritten digits, with 240,000 training and 40,000 test images. The study compares student learning with expert, semi-expert, and non-expert teachers, showing better performance than baseline and fine-tuning methods. Results are presented in Table 5 and accuracy over epochs is illustrated in FIG6. In our experiment, we use the STL-10 dataset with 10 balanced classes and 5,000 labeled images for training. We compare our results to fine-tuning and the baseline model, showing improved accuracy. Teachers pretrained on CIFAR-10 and CIFAR-100 are effective in reducing model weights. In our framework, using weights pretrained on CIFAR-10 and CIFAR-100, our model reduces test errors by over 10% compared to fine-tuning. Additionally, student model training further decreases test error by 3%. We only train on labeled data, unlike other approaches that use it for testing semi-supervised methods. Our results, obtained with less data, may not be directly comparable. We also compare to Distral BID26, a multi-task reinforcement learning framework, on Atari games with three tasks. Our framework, using weights pretrained on CIFAR-10 and CIFAR-100, reduces test errors by over 10% compared to fine-tuning. Student model training further decreases test error by 3%. Distral, a multi-task reinforcement learning framework, is suboptimal as it aims to learn a multi-task agent and struggles with decreasing teacher influence when tasks are very different. In contrast, our framework can reduce a teacher's influence and negative transfer. Averaged normalized weight (p w) for teachers and the student in the C10 experiment is plotted, showing that the C100 teacher has a higher p w value than the SVHN teacher, as C100 is more relevant to C10. The plot in FIG8 shows that the C100 teacher has higher p w values than the SVHN teacher throughout training. An ablation study confirms that learning with knowledgeable teachers leads to better performance than learning with untrained teachers. For example, in the hero task, learning with knowledgeable teachers achieves an average reward of 30928, while learning with untrained teachers only achieves 15934. The results show that knowledge flow achieves higher rewards than training with untrained teachers in different environments and teacher-student settings. The KL term prevents drastic changes in the student's output distribution when teachers' influence decreases. Ablation study with KL coefficient set to zero shows significant performance drops without the KL term. Learning with the KL term achieves an average reward of 2907, while learning without it achieves 1215. In additional experiments, architectures for the teacher differ from the student model. The teacher model has 3 convolutional layers with 32, 64, and 64 filters, followed by a fully connected layer with 512 ReLUs. The student model has 2 convolutional layers with 16 and 32 filters, followed by a fully connected layer with 256 ReLUs. Both models have output layers for actions and values. Each teacher's convolutional layers are linked to the student's. In the experiment, teachers with different architectures were used to train a student model for the KungFu Master task. Despite the architectural differences, the results showed that learning with these teachers still achieved similar performance. Learning with teachers of different architectures resulted in an average reward of 37520, while learning with teachers of the same architecture had an average reward of 35012. Knowledge flow between teachers and the student model enabled higher rewards. Using an average network to obtain parameters \u03b8 old through an exponential running average of model weights resulted in similar performance compared to using a single model. In a specific scenario with a Boxing target task and a Riverraid expert teacher, the average reward achieved with an average network was 96.2, slightly higher than the 96.0 achieved with a single network. Additional results on using an average network are shown in FIG0. The curr_chunk discusses various techniques for knowledge transfer in neural networks, including fine-tuning, PathNet, 'Growing a Brain', actor-mimic, and learning without forgetting. It contrasts these methods with the approach of using multiple teacher nets for transfer learning. PathNet enables multiple agents to train the same deep net by reusing parameters and avoiding catastrophic forgetting. The curr_chunk introduces Progressive Net BID23, which uses lateral connections to avoid catastrophic forgetting. It also discusses a method with scaling and normalized weights to ensure student independence. Distral, a combination of 'distill & transfer learning', involves joint training of multiple tasks with a shared policy for consistency. Knowledge flow focuses on single-task learning, while multi-task learning addresses multiple tasks simultaneously. In multi-task learning, information from different tasks is shared to improve performance, while knowledge flow leverages information from multiple teachers to help a student learn a new task. Knowledge distillation involves transferring information from a larger deep net to a smaller one trained on the same dataset. Actor-mimic enables an agent to address multiple tasks simultaneously and generalize knowledge to new domains. Our proposed technique in multi-task learning involves using a teacher's representation at the beginning of training to add a new task to a deep net without forgetting the original capabilities. This method only uses data from the new task and retains old capabilities by recording the old network's output on the new data. This approach differs from other techniques by explicitly transferring knowledge from teacher networks."
}