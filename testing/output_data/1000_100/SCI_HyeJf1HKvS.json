{
    "title": "HyeJf1HKvS",
    "content": "This work introduces a two-stage neural architecture for learning structural correspondences between graphs. It utilizes localized node embeddings from a graph neural network to rank soft correspondences initially and then employs synchronous message passing networks to iteratively refine the rankings for matching consensus in local neighborhoods. The method is shown to be effective in real-world tasks such as computer vision and entity alignment between knowledge graphs. Graph matching is essential for establishing structural correspondences between nodes in graphs, considering node and edge similarities. It is crucial for various applications like cheminformatics, bioinformatics, social network analysis, and computer vision tasks. The problem has been extensively studied in theory. Neural architectures have been proposed to tackle graph matching and similarity tasks in a data-dependent fashion, addressing the limitations of NP-hard combinatorial approaches. Graph matching is often formulated as an edge-preserving, quadratic assignment problem, aiming to find correspondences based on neighborhood consensus to prevent adjacent nodes in the source graph from being mapped to different regions in the target graph. In this work, the problem of supervised and semi-supervised matching of graphs is addressed by incorporating the concept of neighborhood consensus into the model. The supervised setting involves learning from pair-wise ground-truth correspondences to generalize to unseen graph pairs, while the semi-supervised setting utilizes complete graph structures with limited ground-truth correspondences. The proposed deep graph matching architecture is described in detail, emphasizing the use of end-to-end learning. The architecture involves two stages: local feature matching and iterative refinement using synchronous message passing networks. The feature matching step computes initial correspondence scores based on node embeddings similarity. The iterative refinement strategy aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. Scaling the method to large inputs is also discussed. The architecture involves two stages: local feature matching computes initial correspondence scores based on node embeddings similarity. Updates on S are performed by a neural network based on pair-wise color differences. The method aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The architecture involves two stages: local feature matching computes initial correspondence scores based on node embeddings similarity. Updates on S are performed by a neural network based on pair-wise color differences. The method aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The Graph Neural Network (GNN) \u03a8 \u03b81 is trained in a supervised fashion against ground truth correspondences by minimizing the negative log-likelihood of correct correspondence scores. The proposed algorithm utilizes graph neural networks to detect violations of neighborhood consensus criteria in local neighborhoods and iteratively refine correspondences. The method aims to resolve false correspondences by detecting violations and refining correspondences in an iterative fashion. The proposed algorithm utilizes graph neural networks to detect violations of neighborhood consensus criteria in local neighborhoods and iteratively refine correspondences. Key to the algorithm is the soft correspondence matrix S, which maps node functions between domains. Consensus is achieved by mapping node indicator functions and performing synchronous message passing on both graphs using a shared graph neural network. This allows for the comparison of results to measure neighborhood consensus between node pairs for trainable updates of correspondence scores. The algorithm uses graph neural networks to refine correspondences by detecting violations of neighborhood consensus criteria. It employs a soft correspondence matrix to map node functions between domains and achieve consensus through synchronous message passing. The consensus stage improves the correspondence scores based on an MLP, combining feature matching error and neighborhood consensus error. The process iteratively enhances the consensus in local neighborhoods by distributing global node colorings to resolve ambiguities and false matchings. The measure d i,j evaluates how well local neighborhoods are matched by the soft correspondence between graphs G s and G t. The algorithm utilizes graph neural networks to refine correspondences by detecting violations of neighborhood consensus criteria. It employs a soft correspondence matrix to map node functions between domains and achieve consensus through synchronous message passing. The consensus stage improves correspondence scores based on an MLP, combining feature matching error and neighborhood consensus error. The process iteratively enhances consensus in local neighborhoods by distributing global node colorings to resolve ambiguities and false matchings. The measure d i,j evaluates how well local neighborhoods are matched by the soft correspondence between graphs G s and G t. The proofs for the theorems can be found in Appendices B and C. Injectivity of graph neural networks is a key topic in recent literature, with a focus on using powerful GNNs like the Weisfeiler & Lehman heuristic. This approach relates to classical graph matching techniques, such as the graduated assignment algorithm, which iteratively computes solutions based on linear assignment problems. The softassign operator is implemented using sinkhorn normalization on rescaled inputs, with a growing scaling factor in each iteration. The gradient Q is related to a neighborhood consensus scheme for a non-trainable GNN instantiation. Correspondence scores are updated via trainable neural networks based on the difference between certain values. This model can be seen as a deep parameterized generalization of the graduated assignment algorithm. Specifying node and edge attribute similarities in graph matching is often challenging. Our approach supports continuous node and edge features using GNN models. We optimize the algorithm for scalability and propose sparsifying initial correspondences to reduce memory footprint. The time complexity of the refinement phase is reduced by sparsifying initial correspondences, optimizing initial feature matching loss is crucial, and replacing node indicator functions with randomly drawn node functions is proposed for efficiency. The refinement phase reduces time complexity by sparsifying initial correspondences and optimizing feature matching loss. Softmax normalization is proposed to resolve ambiguities and avoid inefficient computations in the neighborhood consensus method. The supervised refinement procedure resolves violations by re-ranking false correspondences via neighborhood consensus. Row-wise normalization leads to convergence, and varying the number of refinement iterations speeds up training runtime. Decreasing L during training does not affect convergence abilities during testing. The method is verified on three different tasks, showing benefits in an ablation study on synthetic graphs. Our method is implemented in PYTORCH using the PYTORCH GEOMETRIC and KEOPS libraries for efficient processing of sparse mini-batches with GPU acceleration. Optimization is done via ADAM with a fixed learning rate. Hits@k is used to evaluate our model in various experiments. In our experiments, we evaluate our model using Hits@k to measure correct entity matches in top k rankings on synthetic graphs. The method is trained and tested on pairs of graphs with different node and edge probabilities. Additional experiments in Appendix E test the approach's robustness to node changes. The graph neural network operators are implemented using the GIN operator with three layers. The study evaluates a model using Hits@k to measure correct entity matches in top k rankings on synthetic graphs. The GIN operator with three layers is used for graph neural network operations. The model's performance is tested on pairs of graphs with varying node and edge probabilities, showing a decrease in accuracy with increasing structural noise. The proposed two-stage architecture can recover all correspondences regardless of noise levels. The proposed two-stage architecture can recover all correspondences, independent of structural noise levels. It includes an initial formulation using sinkhorn and an optimized architecture with random node indicator sampling and row-wise normalization softmax. The refinement strategy shows improved performance on sparsified top k correspondences, even when training does not converge. The proposed two-stage architecture can recover all correspondences, independent of structural noise levels. It consistently converges to the perfect solution of Hits@1 \u2248 Hits@k when the correct match is included in the initial top k ranking of correspondences. This makes it an excellent option to scale algorithms to large graphs. Experiments were conducted on PASCALVOC and WILLOW-OBJECTCLASS datasets, resulting in 6,953 and 1,671 annotated images. The PASCALVOC dataset contains instances with varying scale, pose, and illumination, while the WILLOW-OBJECTCLASS dataset has consistent orientations for each category. The model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 images per class for training. Key points are used to construct graphs via Delaunay triangulation, and input features are based on a pre-trained VGG16 model. The model used in the study is a pre-trained VGG16 on IMAGENET. They adopt SPLINECNN as their graph neural network operator with trainable B-spline based kernel function. Results are evaluated for both isotropic and anisotropic cases. The study utilizes SPLINECNN with edge features for graph neural network operations, employing a kernel size of 5, hidden dimensionality of 256, ReLU activation, and a network architecture with two convolutional layers. Training involves pairing examples of the same category and evaluating with test graph pairs. The model is trained using negative log-likelihood and compared with displacement loss. The study evaluates the architecture using isotropic and anisotropic GNNs for different values of L and includes ablation results using MLP for local node matching. Results for Hits@1 on PASCALVOC and WILLOW-OBJECTCLASS datasets are shown in tables. The refinement strategy outperforms competing methods and non-refined baselines, reducing errors by half on WILLOW-OBJECTCLASS. Starting from a weaker baseline, improvements of up to 14 percentage points are observed on PASCALVOC. The study evaluates the architecture using isotropic and anisotropic GNNs for different values of L and includes ablation results using MLP for local node matching. Results for Hits@1 on PASCALVOC and WILLOW-OBJECTCLASS datasets are shown in tables. The refinement strategy outperforms competing methods and non-refined baselines, reducing errors by half on WILLOW-OBJECTCLASS. Starting from a weaker baseline, improvements of up to 14 percentage points are observed on PASCALVOC. In the next stage, the approach aims to improve performance further by using task-specific isotropic or anisotropic GNNs for \u03a8 \u03b81. The model's generalization capabilities are tested on the PASCALPF dataset following the experimental setup of Zhang & Lee (2019). Training involves generating a synthetic set of graph pairs by sampling source points, adding Gaussian noise, introducing outliers, and constructing graphs by connecting nodes with their k-nearest neighbors. The unmodified anisotropic keypoint architecture is trained with specific inputs until it has seen 32,000 instances. The trained model is evaluated on the PASCALPF dataset, showing improvements over the state-of-the-art results in various categories. The consensus architecture outperforms the baseline, demonstrating the benefits of the consensus stage. Additionally, the model performs well even without visual information. Evaluation on the DBP15K datasets linking entities from different knowledge graphs also shows promising results. The dataset contains 15,000 links between entities, split for training and testing. Entity input features are obtained using monolingual FASTTEXT embeddings aligned into the same vector space. A graph neural network operator similar to Xu et al. (2019d) is used, with ReLU and dropout for non-linearity. A three-layer GNN is used for obtaining initial similarities and refining alignments. Our model uses a three-layer GNN for obtaining initial similarities and refining alignments with dimensionality 256 and 32. Training is done in a semi-supervised manner using negative log likelihood. Results show improvement over previous models in Hits@1 and Hits@10 metrics. Our approach improves upon the state-of-the-art on all categories with gains of up to 9.38 percentage points. The refinement strategy consistently enhances Hits@1 of initial correspondences significantly, while scalability allows for multiple refinement iterations with large hidden feature dimensionalities. Experimental results demonstrate effective solutions to real-world problems, although limitations related to the expressive power of GNNs and the WL heuristic for graph isomorphism testing are inherited. One limitation of the approach is the potential failure to converge when two nodes are assigned the same color by WL, leading to non-convergence due to equal initial correspondence distributions. Resolving these ambiguities by adding noise is unlikely due to the inherent noise in real-world datasets. Various related problems include maximum common subgraph, network alignment, graph edit distance, and graph matching. Graph neural networks have become a focus of research for deep graph matching techniques. A two-stage neural architecture was presented for learning node correspondences between graphs in a supervised or semi-supervised fashion, aiming to reach a neighborhood consensus between matchings and scale to large input domains. The architecture presented aims to scale to large input domains and consistently improve upon the state-of-the-art when evaluated on real-world datasets. The final optimized algorithm is detailed in Algorithm 1, showcasing its ability to map neighborhoods around nodes to vectorial representations effectively. The algorithm presented extends the graduated assignment algorithm by introducing trainable parameters. It ensures isomorphism between graph structures and maps neighborhoods around nodes to vectorial representations effectively. The algorithm introduced extends the graduated assignment algorithm with trainable parameters, improving results compared to fixed-function message passing schemes. It can learn to utilize node and edge features for refinement and offers flexibility in choosing task-dependent GNN operators. The potential for higher-order GNNs is left for future exploration. The algorithm introduced extends the graduated assignment algorithm with trainable parameters, improving results compared to fixed-function message passing schemes. It can learn to utilize node and edge features for refinement and offers flexibility in choosing task-dependent GNN operators. The potential for higher-order GNNs is left for future exploration. Our approach is experimentally validated for robustness towards node addition or removal, showing that the consensus stage is extremely robust while the first stage alone struggles with finding the right matching. Identifying correspondences between nodes in graphs is a common problem studied in various domains. The maximum common subgraph isomorphism problem is NP-hard and difficult to approximate. Our neural architecture can detect and reduce false positive influence of unmatched nodes in the refinement stage. For more details, refer to the survey by Kriege et al. (2019b). In graph matching, techniques are developed for aligning large networks without specific structural properties. The problem involves minimizing a function for two graphs with adjacency matrices, typically using permutation matrices. The goal is to find optimal solutions by minimizing a specific equation. Equation (12) is equivalent to Equation (1) in terms of optimal solutions. Research has focused on minimizing Equation (12) using a Frank-Wolfe algorithm and projecting the fractional solution. The applicability of relaxation and projection is still not well understood, with limited theoretical results available. The WL heuristic by Tinhofer (1991) distinguishes between two graphs based on the objective function in Equation (12). Kersting et al. (2014) modified the Frank-Wolfe algorithm for improved results. The Frank-Wolfe algorithm was modified by Kersting et al. (2014) to obtain the WL partition. Aflalo et al. (2015) showed that the standard relaxation gives correct solutions for certain asymmetric graphs characterized by their adjacency matrix's spectral properties. Bento & Ioannidis (2018) studied different relaxations for graph matching. Other methods include spectral relaxations (Umeyama, 1988; Leordeanu & Hebert, 2005) and random walks (Gori et al., 2005). Graph matching is closely linked to the quadratic assignment problem (QAP) (Zhou & De la Torre, 2016), with recent literature focusing on weighted versions considering node and edge similarities. Lawler's QAP involves a weighted affinity matrix, which Zhou & De la Torre (2016) proposed to factorize for computational efficiency. Zhang et al. (2019c) explored kernelized graph matching using Koopmans-Beckmann's QAP in Hilbert space. Swoboda et al. (2017) studied Lagrangean decompositions solved by dual ascent algorithms for graph matching, achieving state-of-the-art performance. Recently, advancements in graph matching tasks have led to the proposal of functional representation to avoid constructing the affinity matrix. The graph edit distance, a concept in computer vision, measures the minimum cost to transform one graph into another by adding, deleting, and substituting vertices and edges. Despite being NP-hard, several exact algorithms have been developed to compute the graph edit distance. Network alignment algorithms have been proposed based on the assignment problem, with heuristics that can reduce running time to quadratic or even linear for restricted cost functions. The problem involves computing a node-to-node similarity matrix and solving the assignment problem for alignment. ISORANK, proposed by Singh et al. (2008), is based on the adjacency matrix. ISORANK, proposed by M. Singh et al. (2008), is based on the adjacency matrix of the product graph K = A s \u2297 A t of G s and G t. Various approaches have been suggested to improve ISORANK, such as an efficient approximation by Kollias et al. (2012) and an extension by Zhang (2016) supporting vertex and edge similarities. Klau (2009) proposed a linear programming approach for network alignment, while Bayati et al. (2013) developed a message passing algorithm for sparse network alignment. In network alignment, the goal is to find optimal correspondences between vertices of two graphs using various techniques. Learning node and edge similarity functions for specific tasks has been proposed, along with deep graph matching procedures that utilize local node feature matchings and cross-graph embeddings. Refining local feature matchings by enforcing neighborhood consistency has been relevant for matching in images. Consistency in matching images has been a focus for years. The functional maps framework addresses similar issues for manifolds. Deep graph matching has been extensively studied recently, with various approaches such as supervised deep graph matching networks and differentiable spectral graph matching solvers. In recent years, deep graph matching has been a focus of study, with approaches like compositional message passing algorithms and Gromov-Wasserstein discrepancy being used. Xu et al. (2019a, 2019b) have also worked on graph matching and multi-graph partitioning using optimal transport and node embeddings. Our supervised approach for graph sets also utilizes optimal transport but in a different manner. Recent research has focused on deep graph matching, utilizing various techniques such as CYCLEGANs, NODE2VEC embeddings, and graph neural networks to align and compare graphs. Different approaches like global and local network topology preservation, local node embedding similarity, and graph edit distance approximation have been explored. These methods aim to improve graph matching accuracy and efficiency. In a follow-up work, Bai et al. (2018) proposed a new approach for processing correspondence matrices using traditional CNNs. Wang et al. (2019b) and Xu et al. (2019d) investigated enhancing node embeddings by aggregating information from local neighbors and similar embeddings in other graphs. Wang & Solomon (2019) addressed the problem of finding unknown rigid motions between point clouds through a point cloud matching procedure. The curr_chunk discusses the use of rigid motion between point clouds for point cloud matching, utilizing a Transformer module and feature matching based on inner product similarity scores. It highlights the limitations of localized node embeddings in achieving consistent matching and suggests using these methods to enhance initial feature matching procedures. The importance of neighborhood consensus for image matching is also emphasized, with references to past works in computer vision. The curr_chunk introduces a deep neural network for neighborhood consensus using 4D convolution, which cannot be directly applied to the graph domain due to computational limitations. The algorithm infers errors for the product graph but conducts computations on the original graphs, focusing on functional maps for defining continuous maps between function spaces on manifolds."
}