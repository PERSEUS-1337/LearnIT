{
    "title": "rkhlb8lCZ",
    "content": "Wavelet Pooling is introduced as an alternative to traditional neighborhood pooling in Convolutional Neural Networks for image and object classification. This method decomposes features into a second level decomposition, reducing feature dimensions and addressing overfitting issues. Experimental results show that Wavelet Pooling outperforms or performs comparably to other pooling methods like max, mean, mixed, and stochastic pooling. Convolutional Neural Networks (CNNs) have become the standard in image and object classification due to their high accuracy rates compared to vector-based deep learning techniques. Researchers constantly upgrade CNN components like the convolutional and pooling layers to improve accuracy and efficiency. Pooling, rooted in predecessors like Neocognitron and Cresceptron, subsamples convolutional layer results to enhance CNN performance. Pooling in deep learning involves subsampling the results of convolutional layers to reduce spatial dimensions, leading to benefits such as parameter reduction, increased computational efficiency, and overfitting regulation. Popular pooling methods include max pooling and average pooling, but they have limitations that hinder optimal network learning. Other approaches like mixed pooling and stochastic pooling use probabilistic methods to address these issues. Despite variations, all pooling operations utilize a neighborhood approach similar to nearest neighbor interpolation in image processing, offering speed, simplicity, and efficiency. Our proposed wavelet pooling algorithm aims to reduce artifacts in interpolation techniques by using second-level wavelet decomposition for subsampling features. Compared to traditional methods like max, mean, mixed, and stochastic pooling, our approach shows promising results on image classification datasets like MNIST, CIFAR-10, SHVN, and Karolinska Directed Emotional. The paper discusses the use of wavelet pooling algorithm for subsampling features in image classification datasets like CIFAR-10, SHVN, and Karolinska Directed Emotional Faces. The pooling methods include max pooling and average pooling to condense the output of the convolutional layer. The paper explores wavelet pooling for subsampling features in image classification datasets like CIFAR-10, SHVN, and Karolinska Directed Emotional Faces. It discusses max and average pooling methods to condense the output of the convolutional layer. Max pooling can erase details from an image if main details have less intensity, while average pooling can dilute pertinent details by averaging data with values much lower than significant details. Researchers have developed probabilistic pooling methods to address shortcomings in max and average pooling. Mixed pooling randomly selects between max and average pooling during training, applied in three different ways within a layer. Stochastic pooling improves upon max pooling by sampling from neighborhood regions based on activation probabilities. Stochastic pooling method selects activations based on probabilities calculated within each region, avoiding the limitations of max and average pooling. The method samples from a multinomial distribution to choose activations, with higher probabilities indicating a higher chance of selection. This approach offers advantages over deterministic pooling methods and utilizes wavelets in image processing. Our proposed pooling method utilizes wavelets to reduce feature map dimensions and minimize artifacts in image interpolation. By discarding first-order subbands, our approach captures data compression more organically, reducing jagged edges and other artifacts that could affect image classification. The wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT), which is a more efficient implementation of the two-dimensional discrete wavelet transform (DWT). The proposed pooling method utilizes wavelets for feature map dimension reduction and minimizing artifacts in image interpolation. It involves a 2nd order decomposition using the fast wavelet transform (FWT) on images, resulting in detail and approximation subbands at each decomposition level. The image features are reconstructed using only the 2nd order wavelet subbands, pooling them by a factor of 2 with the inverse FWT (IFWT) based on the inverse DWT (IDWT). The proposed wavelet pooling algorithm performs backpropagation by reversing the process of its forward propagation. It involves 1st and 2nd order wavelet decompositions, using the Haar wavelet basis for even, square subbands. The backpropagation algorithm is detailed in FIG4, utilizing MatConvNet for CNN experiments and stochastic gradient descent for training. The study utilizes a 64-bit operating system with an Intel Core i7-6800k CPU @ 3.40 GHz processor and 64.0 GB of RAM. Two GeForce Titan X Pascal GPUs with 12 GB of video memory are used for training. Different CNN structures are tested with various regularization techniques like Dropout and Batch Normalization on datasets like CIFAR-10 and SHVN. The network architecture is based on the MNIST structure with batch normalization. All pooling methods use a 2x2 window for comparison. The study compares different pooling methods on the MNIST database of handwritten digits. The proposed method outperforms others, with max pooling showing signs of overfitting. Mixed and stochastic pooling have a rocky trajectory but do not overfit. Average and wavelet pooling show smoother learning curves. Two sets of experiments are run, one without dropout layers and the other with dropout and batch normalization. The network structure for the CIFAR-10 experiments is shown in FIG7. The network structure for the CIFAR-10 experiments is shown in FIG7. The proposed method has the second highest accuracy, with wavelet pooling resisting overfitting. Different pooling methods are tested with and without dropout layers to observe their effects on learning progression. The network structure for the SHVN experiments uses dropout to observe the effects of changes. The method proposed has the second lowest accuracy, with max and wavelet pooling slightly overfitting the data. Mixed, stochastic, and average pooling show a slow progression of learning. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. The dataset had errors which were fixed by mirroring missing or corrupted images in MATLAB and manually cropping them to match specific dimensions. The training and test data are not designated, so they were shuffled for the experiments. Our proposed method for the KDEF dataset involves shuffling 3,900 training images and 1,000 test images, resizing them to 128x128. Dropout layers help regulate the network and prevent overfitting, with wavelet pooling showing resistance to overfitting. Computational complexity is a challenge for our wavelet pooling implementation, presented as a proof-of-concept. The proposed method for the KDEF dataset involves shuffling training and test images, resizing them to 128x128, and using dropout layers to prevent overfitting. Wavelet pooling is resistant to overfitting but lacks computational efficiency. The code implementation is not optimized, leaving room for improvement in terms of accuracy and efficiency calculations for different pooling methods. The study compares different pooling methods for image processing. Average pooling is the most computationally efficient, followed by mixed pooling and max pooling. Stochastic pooling is less efficient, while wavelet pooling is the least efficient method. Wavelet pooling is the least computationally efficient method, using significantly more mathematical operations than average pooling. However, with improvements in coding practices, GPUs, and the FTW algorithm, it can be a viable option. Various enhancements to the FTW algorithm, such as multidimensional wavelets and parallelization, aim to improve efficiency in speed and memory. The proposed wavelet pooling method shows potential to outperform traditional methods in CNNs, performing exceptionally well in the MNIST dataset and competitively in CIFAR-10 and KDEF datasets. The proposed wavelet pooling method, with the addition of dropout and batch normalization, shows promising results in various datasets. It competes well with traditional pooling methods in the SHVN dataset and outperforms most in the CIFAR-10 and KDEF datasets. Future work could explore different wavelet bases for improved performance. Improving downsampling factors in image decomposition and reconstruction can enhance feature reduction. Retaining discarded subbands for backpropagation may improve accuracy. Enhancing the Fast Wavelet Transform (FTW) method could boost computational efficiency. Analyzing the Structural Similarity (SSIM) of wavelet pooling compared to other methods could validate its effectiveness."
}