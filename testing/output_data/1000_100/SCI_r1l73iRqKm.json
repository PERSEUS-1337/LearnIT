{
    "title": "r1l73iRqKm",
    "content": "In open-domain dialogue, intelligent agents struggle to incorporate knowledge effectively. Existing models often rely on generic utterances rather than recalled knowledge as context. To address this, a new dataset grounded in knowledge from Wikipedia has been created. Architectures capable of retrieving, reading, and using this knowledge to generate natural responses have been developed. These models have shown success in conducting knowledgeable discussions on various topics, as evaluated by both automatic metrics and human assessments. The current state-of-the-art in open-domain dialogue aims for machines to master language comprehension, memory retention, reasoning, and generating captivating responses. Existing models struggle to incorporate memory and knowledge effectively, hindering progress towards the goal of humans conversing with machines. In open-domain dialogue, machines need to encode input sequences, reason with limited knowledge, and decode outputs. To converse intelligently, direct knowledge memory mechanisms are crucial. This task involves two speakers engaging in open-ended conversations, broadening or focusing on related themes. It requires components not found in standard models, leading to the design of specific architectures for this purpose. We introduce Transformer Memory Networks, a novel architecture combining Memory Network and Transformer elements for generating text outputs. A supervised dataset of human-human conversations was created using crowd-sourced workers, with topics linked to Wikipedia for knowledge grounding. This approach trains a conversation agent with a memory component to recall and ground on existing text. Our Transformer Memory Network architectures combine Memory Network and Transformer elements to recall and ground on existing text for generating engaging conversations with humans. A new benchmark in ParlAI aims to encourage further improvements in this research direction by evaluating models' ability to locate and use knowledge effectively. Many existing dialogue tasks do not explicitly study the use of knowledge, highlighting the importance of this research area. Our work investigates unstructured knowledge across a large, diverse set of topics potentially spanning all of Wikipedia, focusing on the importance of retrieving and conditioning knowledge for question answering tasks. Our work focuses on natural human dialogues containing a diverse set of utterances, not just questions and answers. Previous research has explored non-goal directed dialogue incorporating knowledge, such as using Memory Networks to discuss movies and open-ended topics from Reddit. Other studies have linked Reddit to structured knowledge and used unstructured text, like discussing news articles using Wikipedia summaries as knowledge. Our work compares Memory Networks BID19 and Transformers in handling full multi-turn dialogues with external knowledge, such as Wikipedia summaries and Foursquare tips. The model architecture combines these approaches to improve dialogue understanding in an open-domain setting. Our paper demonstrates models working on full multi-turn dialogue in an open-domain setting, where two participants engage in chitchat with one being a knowledgeable expert (wizard) and the other a curious learner (apprentice). The apprentice aims to delve deeply into a chosen topic of interest, making the conversation engaging and fun. This task differs from shallow chit-chat tasks by focusing on knowledge exploration. The task involves a knowledgeable expert (wizard) and a curious learner (apprentice) engaging in deep conversations about a chosen topic. The wizard uses an information retrieval system to provide relevant knowledge in a fun and engaging way during the conversation. The conversation between a wizard and an apprentice involves choosing a topic, exchanging messages, and responding based on selected sentences. The goal is to replace the human wizard with a learned agent in future interactions. Topics for the dialogue are sourced from a set of 1365 natural, open-domain topics. The wizard in the dialogue has access to a set of passages of knowledge relevant to the conversation context. A fixed retriever is used to present results to the annotator, similar to the Open-SQuAD dataset. The retriever compares articles and queries using TF-IDF weighted bag-of-word and n-gram vectors, retrieving the top 7 articles for the last two turns of dialogue. During data collection, the wizard can select relevant articles and sentences to respond to the apprentice in the dialogue UI. Only one article and one sentence can be chosen per turn. An image of the Wizard's UI is provided in Appendix A.1. This work focuses on training dialogue models to replace human wizards in conversations. In this work, dialogue models are trained to replace human wizards in conversations. The models have access to a knowledge source, such as Wikipedia, to ground the conversation. Two classes of models are developed: retrieval models that select from a set of candidate responses, and generative models that generate responses word-by-word. The models use Memory Network and Transformer architectures to retrieve relevant information from a large memory and generate dialogue utterances. The dialogue models are trained to replace human wizards in conversations, with access to a knowledge source like Wikipedia. Two classes of models are developed: retrieval models that select candidate responses and generative models that generate responses word-by-word. The models use Memory Network and Transformer architectures to retrieve information and generate dialogue utterances. Knowledge retrieval involves a large memory base organized hierarchically into documents, with standard information retrieval techniques used to return a smaller set of candidates for selection. The retriever in the dialogue model operates on the topic and the last two turns, calling the IR system three times with different queries. Top 7 articles are retrieved for each lookup, and sentences are flattened with article titles. An attention mechanism is used for fine-grained selection of knowledge sentences for the next dialogue turn. The dialogue model's retriever operates on topic and last two turns, using an attention mechanism to select knowledge sentences for the next dialogue turn. Each sentence in memory is encoded with a Transformer, and dot-product attention is performed between memory candidates and dialogue context. The final stage involves predicting the output utterance for the next turn. Different variants of knowledge attention and utterance prediction are considered for retrieval and generative models. The model uses dot-product attention to combine knowledge sentences with dialogue context to generate responses. Two versions of the model are considered: Two-stage and End-to-end. The model is trained to minimize cross-entropy loss and employs a beam search of 5 for response selection. Generative models use BPE encoding for effective rare word copying. The model employs BPE encoding for effective rare word copying and uses a shared Transformer encoder to encode candidates and dialogue history. The highest selected knowledge is concatenated with the dialogue encoding and passed into a Transformer decoder. Training minimizes negative log-likelihood of response utterance and can include additional supervision for correct knowledge selection. In the Two-stage version, two separately trained models are used for knowledge selection and utterance prediction. Knowledge dropout (K.D.) is employed to improve decoder performance by preventing the model from attending to knowledge during training. This technique helps the generator be more resilient to errors and speeds up training. In the study, various techniques were used, including feature dropout in BID25. Experimental setups and results were described, focusing on model ability to select knowledge and perform dialogue tasks. Transformers outperformed baselines, especially when pretrained on a large dataset like Reddit. Multi-tasking on SQuAD had minimal impact on performance. The study focused on model performance in dialogue tasks, with Transformers outperforming baselines when pretrained on Reddit. Multi-tasking on SQuAD had minimal impact. The addition of knowledge improved all models, as shown in the results in TAB2. The addition of knowledge significantly improves model performance in dialogue tasks, with the Transformer MemNet reaching 87 R@1. Generative models like the End-to-end and Two-stage Transformer Memory Network outperform baselines and show substantial improvements when provided with gold knowledge. The Two-stage model performs well with predicted knowledge, while the End-to-end model excels with gold knowledge. The End-to-end model benefits from additional knowledge selection supervision, improving performance on all metrics. Knowledge dropout also proves to be beneficial. The conversation revolves around a preference for physical books over e-books due to the tactile experience and ownership feeling. Additionally, human evaluation of models is conducted using crowd-sourced workers to rate dialogue quality. The study collected conversations to evaluate models based on engagingness and knowledge exhibited. Retrieval models outperformed generative models in engagingness evaluation by human workers. The study compared retriever and generative models based on engagingness and knowledge. Retrieval models showed higher engagingness but generative models performed better in conveying knowledge, especially on unseen data. The use of knowledge significantly improved human engagingness ratings for both types of models. In this work, dialogue agents are developed with large memory systems containing encyclopedic knowledge to engage in open-domain conversations. Transformer Memory Network models are used to retrieve and output responses based on this knowledge. The effectiveness of these models is demonstrated using the Wizard of Wikipedia dataset, with a new benchmark released for further exploration. Our new benchmark encourages model exploration for significant advances in research. Future work includes bridging the gap between retrieval and generative models, learning to retrieve and reason simultaneously, and investigating knowledge-grounded dialogue. The goal is to create an engaging and knowledgeable conversational agent. An information retrieval system is used over Wikipedia for dialogue in which apprentices ask and answer questions, with wizards selecting sentences 6.2% of the time. Questions are asked in 13.9% of training set utterances, answers given 39.5% of the time, and new statements made 49.3% of the time. Conversations involve various dialogue acts, with topics chosen from the Persona-Chat dataset. The Persona-Chat dataset consists of \u223c1000 personas with 4-5 sentences describing interests mapped to relevant Wikipedia pages. 1,431 topics are obtained for conversation starters during data collection. Additional experiments include testing the performance of models trained for the full dialogue task. The study tested models trained for full dialogue tasks on knowledge selection tasks, finding that the retrieval system could be improved with auxiliary loss. Human evaluation experiments showed stark differences between human-human conversations and bot conversations, with humans engaging in more small talk and using topics as icebreakers. The study found that human-human conversations involve more small talk and use topics as icebreakers, while bot conversations focus on producing factual sentences. The retriever without knowledge tends to go off-topic, while the retriever with knowledge sticks to the chosen topic but struggles if the subject changes. The study compared different retrieval methods for dialogue tasks, including a two-stage system using a knowledge selection model. Results showed improvements in F1 score but not Recall@1. Human experiments were also conducted to evaluate performance. The study compared retrieval methods for dialogue tasks, including a two-stage system with a knowledge selection model. The model produced factually inaccurate answers but had inviting responses for natural conversation flow. The generator without knowledge exhibited typical seq2seq system behaviors. The generator with knowledge in seq2seq systems has fewer issues with repetition and can act as a selfish conversationalist. It sometimes produces inaccurate statements but can successfully generalize to unseen topics using Wikipedia knowledge. The generator with knowledge in seq2seq systems can successfully generalize to unseen topics using Wikipedia. Selected conversations with the generator may be found in Figure 5."
}