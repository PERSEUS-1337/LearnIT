{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"conditionally invariant\" features that do not change across domains and \"orthogonal\" features that can vary across domains. It is important to focus on the \"conditionally invariant\" features to guard against future adversarial domain shifts. The domain itself is considered a latent variable in this approach. In previous work, the domain is treated as a latent variable, making it impossible to directly observe the distributional change of features across different domains. Data augmentation involves generating multiple images from an original image, using an ID variable to refer to the relevant original image. Only a small fraction of images need to have an ID variable. In a causal framework, the ID variable is added to the model to treat the domain as a latent variable. Samples with the same class and identifier are considered counterfactuals under different style interventions. Regularizing the network with a graph Laplacian improves performance in settings with changing domains. This approach links to interpretability, fairness, and transfer learning in deep neural networks. Deep neural networks (DNNs) excel in prediction tasks like visual object and speech recognition, but issues can arise from domain shifts during deployment in production. The \"Russian tank legend\" exemplifies how sampling biases in training data can lead to degraded predictive performance in real-world scenarios. The system learned to discriminate between images of different qualities but may fail in practice due to hidden confounding factors. Deep learning requires large sample sizes to account for indirect associations and achieve invariance to known factors. Adversarial examples show how human and artificial cognition can deviate in misclassifying inputs. Adversarial examples challenge machine learning models by presenting perturbed inputs that are misclassified. These examples do not deceive humans, who can achieve invariance to rotations with just one rotated example. There is interest in mimicking human learning abilities to control biases in input data and align features used by deep neural networks with human cognition. Biases in training datasets can be replicated in ML models, leading to issues like Google's photo app tagging non-white people as \"gorillas\" in 2015. In June 2015, Google's photo app mistakenly tagged two non-white people as \"gorillas\" due to biased training data. To address such issues, counterfactual regularization (CORE) is proposed to control latent features extracted by an estimator. CORE focuses on 'conditionally invariant' (core) features for stable classification, making the estimator robust against domain shifts and adversarial attacks. Counterfactual regularization (CORE) is robust against adversarial domain shifts by utilizing \"counterfactuals\" and grouping instances related to the same object. The manuscript discusses how CORE reduces the need for data augmentation, improves predictive performance in small sample sizes, introduces counterfactual regularization, and evaluates its performance in various experiments using the CelebA dataset. The CelebA dataset BID26 contains face images of celebrities for classifying whether a person wears glasses. Grouping information is used to ensure the same prediction for all images of the same person. Counterfactual observations are included, and the training set includes 10 identities with a total sample size of 321 images. In the subsampled CelebA dataset and augmented MNIST dataset, exploiting grouping information reduces test error significantly. In the CelebA dataset, the test error is reduced by 32%, while in the MNIST dataset, the error on rotated digits is reduced by 50%. Overall, utilizing group structure decreases average test error from 24.76% to 16.89%, making data augmentation more efficient. Data augmentation involves creating additional samples by modifying original inputs, such as rotating, translating, or flipping images. This process generates invariance of the estimator with respect to style features. Using grouping information for CORE enforces invariance more strongly compared to normal data augmentation. For example, in the MNIST dataset, rotations are sampled uniformly at random from [35, 70]. Using CORE reduces average test error on rotated examples from 32.86% to 16.33%. Similar works include BID14 and Domain-Adversarial Neural Networks (DANN). BID13 focuses on learning a representation without discriminative information about input origin through adversarial training. Unlike these works, our approach does not require data from different domains, only different realizations of the same. The data generating process assumed in BID14 is similar to our model, where they identify conditionally independent features by adjusting a transformation of variables to minimize the squared MMD distance between distributions in different domains. The fundamental difference is that we use a different data basis and our approach has a latent domain identifier. Causal modeling in transfer learning aims to guard against adversarial domain shifts. Causal models aim to ensure valid predictions under large interventions on predictor variables. However, transferring these results to adversarial domain changes in image classification poses challenges due to the anti-causal nature of the task and the need to guard against style feature shifts. Standard causal inference may not directly address these issues. Various approaches have been developed to tackle these challenges. Various approaches leverage causal motivations for deep learning or use deep learning for causal inference to guard against large domain shifts. The Neural Causation Coefficient (NCC) estimates the probability of X causing Y and distinguishes between object features and context features. Structural equation modeling and CGANs are compared, with one CGAN fitted for X \u2192 Y and another for Y \u2192 X. BID33 discusses the use of CGANs in determining causal directions, while BID16 utilizes generative neural networks for cause-effect inference. Bahadori et al. (2017) introduce a regularizer combining penalties with estimated probabilities of features being causal. Besserve et al. (2017) connect GANs and causal generative models using a group theoretic framework, and Kocaoglu et al. (2017) propose causal implicit generative models for sampling from conditional and interventional distributions. The generator structure in CausalGAN is based on the causal graph. The use of deep latent variable models and proxy variables to estimate individual treatment effects is proposed. Causal reasoning is exploited to characterize fairness considerations in machine learning, deriving causal nondiscrimination criteria. Algorithms avoiding proxy discrimination require classifiers to be constant as a function of the proxy variables in the causal graph, showing structural similarity to style features. The distinction between core and style features is akin to disentangling factors of variation, which has garnered interest in generative modeling. In a study by Matsuo et al. (2017), they introduce a \"Transform Invariant Autoencoder\" to reduce dependence of latent representation on specific object transforms in images. The goal is to learn a latent representation that excludes certain style features, such as location, image quality, posture, brightness, background, and contextual information. The approach also addresses confounding situations in domain distribution and features. In a variational autoencoder framework, Bouchacourt et al. (2017) aim to separate style and content in grouped observations. They assume samples within a group share a common unknown value for one factor of variation while the style can differ. The goal is to solve a classification task directly without estimating latent factors explicitly. The standard notation for classification is described before developing a causal graph to compare adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. In regression or classification, Y = R or Y = {1, . . . , K}. X \u2208 R p is a predictor, like image pixels. The prediction y is f \u03b8 (x) with parameters \u03b8 \u2208 R d in a DNN. The goal is to minimize expected loss by penalized empirical risk minimization using training data samples (x i , y i ). The weights are chosen as \u03b8 = argmin \u03b8 L n (\u03b8) with a penalty term. The penalty term in the structural model for variables is determined by a ridge penalty or penalties exploiting geometries like Laplacian regularized least squares. The ID variable is added to group observations and can be considered latent. The prediction is anti-causal, with predictors X being non-ancestral to Y. Causal effects from Y on X are mediated by core features X ci and orthogonal features X \u22a5. The core features X ci are conditionally invariant, while the style features X \u22a5 are orthogonal and can be intervened upon. The distribution of X ci remains constant across domains, while the distribution of X \u22a5 can vary. The style features are confounded by the latent domain D, while the core features are conditionally independent of D given Y. The style intervention variable influences both the latent style X \u22a5 and the image X. In potential outcome notation, X \u22a5 (\u2206 = \u03b4) represents the style under intervention \u2206 = \u03b4, and X(Y, ID, \u2206 = \u03b4) represents the image for class Y, identity ID, and style intervention \u2206. The prediction under the style intervention \u2206 = \u03b4 is denoted as f \u03b8 (X(\u2206 = \u03b4)). The causal graph is used to explain domain adaptation, transfer learning, and guarding against adversarial examples. The intervention magnitude \u2206 is typically assumed to be within an -ball in q-norm around the origin. The goal is to devise a classification in a graph that minimizes adversarial loss by intervening on style features X \u22a5. Adversarial domain shifts involve strong interventions on style features X \u22a5, aiming to minimize the adversarial loss under these interventions. The term \"adversarial\" is used to refer to interventions on style features X \u22a5, while domain adversarial neural networks describe the training procedure. The motivation is to protect against shifts in test data distribution by distinguishing between core and style features. Causal inference faces the challenge of never observing a counterfactual, where changing treatment while holding confounders constant is impossible. In causal inference, counterfactuals involve changing interventions while keeping confounders constant. This allows for observing outcomes under different conditions, such as in image analysis. The style intervention plays a similar role to treatment in medical examples, enabling the study of the same object under various treatments. In image analysis, the style intervention (\u2206) allows for studying the same object under different conditions, similar to treatments in medical examples. The focus is not on the treatment effect but on ruling out parts of the feature space for classification. The goal is to penalize any changes in classification under different style interventions while keeping class and identity constant. The pooled estimator treats all examples identically by summing over the loss with a ridge penalty. The adversarial loss of the pooled estimator may be infinite. Using a specific figure, it can be shown that the pooled estimator performs well in terms of the adversarial loss. The pooled estimator performs well in terms of the adversarial loss by ensuring that X ci extracts information from the image X, and the relations between Y, X ci, and X \u22a5 are not deterministic. To minimize the adversarial loss, f \u03b8 (x(\u2206)) should be constant for all x \u2208 R p. The adversarial loss is minimized when f \u03b8 is a function of the core features x ci only. The adversarial loss is minimized when the predictor is a function of the core features X ci only, which are not directly observable. To approximate the optimal invariant parameter vector, empirical risk minimization is used with an empirically invariant space I n defined by variance and regularization. The regularization constant \u03c4 \u2265 0 controls the degree of variations in estimated predictions for class labels across counterfactuals of an image. The true invariant space I is a subset of the empirically invariant subspace I n. The graph Laplacian regularization penalizes the sum of variances \u03c3 2. The graph Laplacian regularization penalizes the sum of variances \u03c32i(\u03b8) in the sample space induced by the identifier variable ID. It is crucial to define the graph in terms of ID to guard against adversarial domain shifts. The outcome is not strongly dependent on the penalty \u03bb. In \u00a7A, the adversarial loss for binary classification is analyzed for the pooled and CORE estimator in a one-layer network. The structural equation for the image X is briefly discussed. In \u00a7A, the adversarial loss for binary classification is analyzed for the pooled and CORE estimator in a one-layer network. The interventions are additive, and logistic regression is used to predict class labels. Experimental results include studying how CORE handles confounded training data sets and changing style features in test distributions, as well as classifying elephants and horses based on color. Additional experiments involve gender and wearing glasses, and wearing glasses and brightness. In addition to experimental results in \u00a72, a TensorFlow implementation of CORE will be provided along with code for reproducing experiments. The tuning parameter \u03c4 or penalty \u03bb in Lagrangian form is an open question, with performance not highly sensitive to \u03bb. Stickmen images are used as an example, with Y representing {adult, child} and X ci denoting height, a core feature for differentiation. Age and movement in the training dataset show a dependence. The training dataset shows a dependence between age and movement, influenced by a hidden common cause. Large movements are associated with children and small movements with adults. Removing the edge between the common cause and movement in test sets 2 and 3 eliminates the dependence between Y and movement. The training dataset reveals a link between age and movement, affected by a hidden common cause. In test sets 2 and 3, removing this link eliminates the connection between age and movement. The misclassification rates for CORE and the pooled estimator differ, with CORE performing well with as few as 50 counterfactual observations. Including more counterfactual examples does not improve the pooled estimator's performance due to bias. The CelebA dataset is used to classify images based on the person's identity. The CelebA dataset is used to classify images based on whether the person is wearing eyeglasses. An intervention is made on image quality, with images of people wearing glasses having lower quality. Counterfactual observations are created by sampling new image quality values. Different methods for constructing counterfactual observations are discussed. In the CelebA dataset, counterfactual observations are created by altering image quality. Misclassification rates for CORE and the pooled estimator are compared on different test sets. The pooled estimator outperforms CORE on test set 1 by utilizing image quality information. However, it performs poorly on test sets 2-4, where its learned representation relies too heavily on image quality as a predictor. In the context of image quality interventions, CORE's predictive performance is minimally affected compared to other methods. The study aims to assess if CORE can exclude color from its learned representation by including counterfactuals of different colors using the AwA2 dataset. The study evaluates CORE's performance in classifying images of horses and elephants using counterfactual examples. Test sets with different color modifications show that the pooled estimator struggles on certain sets. The study evaluates CORE's performance in classifying images of horses and elephants using counterfactual examples. The pooled estimator struggles on test sets 2 and 3 due to exploiting color predictiveness, while CORE shows color invariance. Adding grayscale images helps CORE achieve color invariance, highlighting the importance of fairness considerations regarding protected attributes like \"color.\" The study evaluates CORE's performance in achieving color invariance in image classification by using counterfactual regularization. CORE ensures fairness by not including \"color\" as a predictive attribute, unlike the pooled estimator. By demanding invariance amongst instances of the same object, CORE achieves robustness against interventions on style features such as image quality, fashion type, color, or body posture. The study evaluates CORE's performance in achieving color invariance in image classification through counterfactual regularization. CORE ensures fairness by not including \"color\" as a predictive attribute, achieving robustness against interventions on style features such as image quality, fashion type, or body posture. Larger models like Inception or ResNet architectures could be explored for further research. Using Inception V3 features does not protect against interventions on implicit style features. Assessing the benefits of CORE for training Inception-style models in terms of sample efficiency and generalization performance is desired. Exploring video data for grouping information and counterfactual regularization could be a future direction. Linear structural equations for image X and style features X \u22a5 are considered, with logistic regression predicting class labels. Additive interventions on style features X \u22a5 are assumed for notational convenience. The text discusses the linear relationship between image X and style features X \u22a5, with core features X ci being conditionally invariant. Logistic regression is used to predict class labels from image data X. The training involves estimating \u03b8 with logistic loss for sample efficiency and generalization performance. The focus is on assessing benefits of CORE for training Inception-style models. Losses on test data include DISPLAYFORM2 where the X in the first loss is a shorthand notation for X(\u2206 = 0), representing images without style interventions. The second loss involves adversarial style interventions. The benchmarks are DISPLAYFORM3. Theorem 1 relies on assumptions including sampling conditions for \u2206 and full rank matrix W. The number of counterfactual examples must be at least as large as the dimension of style variables. The sampling process involves collecting independent samples from a distribution that satisfies certain constraints. The pooled estimator has infinite adversarial loss under Assumption 1. The CORE estimator converges to a certain result as the number of samples approaches infinity. Misclassification loss can also be considered instead of logistic loss. The misclassification loss is used instead of logistic loss, with infinity replaced by 1. To prove that W t\u03b8pool = 0 with probability 1, the oracle estimator \u03b8* is constrained to be orthogonal to the column space of W. If W t\u03b8pool = 0, the constraint becomes non-active and we have \u03b8pool = \u03b8*. The derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8*. The text discusses the relationship between interventions and the oracle estimator \u03b8*. It explains that interventions only affect the column space of W in X, leading to the derivative g(\u03b4) being written as a difference between two formulas. The text also mentions that the estimator \u03b8* remains the same regardless of interventions, and introduces the concept of eigenvalues of W t W being positive. The interventions \u2206 i,j do not affect the estimator \u03b8* whether trained on original or counterfactual data. The left hand side of the equation has a continuous distribution, and the probability of it not being 0 is 1. With probability 1, \u03b8 core = \u03b8* in the linear subspace I = {\u03b8 : W t \u03b8 = 0}. The number of counterfactual examples exceeds the rank q of W and X \u22a5 has a linear influence on X. Therefore, with probability 1, I = I n and \u03b8 core = \u03b8*. The estimator remains unchanged using data without interventions as training data. Comparing formulas, we have uniform convergence to the population loss. This completes the proof. The proof is completed by showing that \u03b8 core = \u03b8* with probability 1 under (A3). Using the CelebA dataset, a confounding problem is created by including mostly images of men wearing glasses and women without glasses. Counterfactuals are used by showing images of the same person without glasses if male and with glasses if female, known as \"CF setting 2\". Test set 2 has a flipped association between gender and glasses. In this example, the study compares training a four-layer CNN end-to-end with using Inception V3 features and retraining the softmax layer. Results show similar trends as c increases, with the performance difference between CORE and the pooled estimator decreasing. The pooled estimator performs worse on test set 2 as m increases, indicating a larger exploitation of X \u22a5. The study focuses on classifying whether a person in an image is wearing eyeglasses in a confounded setting using the CelebA dataset. The study analyzes a confounded setting where the brightness of images is related to whether a person wears glasses. Test sets vary in brightness interventions. The pooled estimator outperforms CORE in the analysis. The pooled estimator outperforms CORE on test set 1 by utilizing brightness information, but struggles on test sets 2 and 4 due to differences in brightness distributions. CORE's predictive performance is more stable across varying brightness levels.\u03b2 \u2208 {5, 10, 20} and c \u2208 {200, 5000} results can be seen in FIG0.5. Different brightness interventions and counterfactual observations are explored in the study. The study explores different counterfactual settings to improve predictive performance. Counterfactual setting 1 works best by controlling only brightness variation. Setting 2, using different images of the same person, presents challenges in isolating brightness. Grouping images of different persons also aids predictive performance to some extent. The optimal tuning parameter \u03c4 or penalty \u03bb remains an open question. The study explores counterfactual settings to improve predictive performance by varying the number of identities in the training dataset. Results show that CORE helps predictive performance, especially when the sample size is small, by mitigating potential confounders. The study explores how CORE improves predictive performance by reducing confounders in small sample sizes. Results show that as sample sizes increase, CORE's performance becomes comparable to the pooled estimator. Varying the number of augmented training examples also affects misclassification rates, with CORE showing lower rates on rotated digits. In the experiment introduced in \u00a75.1, results show that the CORE estimator's performance is not sensitive to the number of counterfactual examples once there are enough in the training set. The pooled estimator struggles with predictive performance on test sets 2 and 3. Counterfactual setting 1 works best, with small differences between settings 2 and 3 in predictive performance. In experiments comparing counterfactual settings 2 and 3, a significant performance difference was observed between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator. The latter command rotates image colors cyclically, and in test set 3, all images are converted to grayscale. The models were implemented in TensorFlow, with the same network architecture and training procedure for CORE and the pooled estimator, differing only in the loss function. Experimental results were based on training each model five times to assess variance. During training, the data is shuffled randomly to ensure mini batches contain counterfactual observations, with a batch size of 120. This makes optimization more challenging when not all batches contain counterfactual data."
}