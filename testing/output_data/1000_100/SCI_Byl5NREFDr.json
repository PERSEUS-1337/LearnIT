{
    "title": "Byl5NREFDr",
    "content": "We study model extraction in natural language processing, where an adversary can reconstruct a victim model using only query access. The attacker can successfully mount the attack without real training data or meaningful queries, using random word sequences and task-specific heuristics. This exploit is enabled by the shift towards transfer learning methods in NLP. With a query budget of a few hundred dollars, the attacker can extract a model that performs slightly worse than the victim model. Machine learning models are valuable intellectual property, often only accessible through web APIs. Malicious users may attempt \"model stealing\" or \"model extraction\" by issuing queries to train a local copy of the model. Defense strategies like membership classification and API watermarking can be circumvented by clever adversaries. NLP APIs based on ELMo and BERT are popular for transfer learning due to their contextualized pretrained representations. These models can be easily extracted by training a local copy using collected input-output pairs, posing a risk of intellectual property theft and leakage of sensitive training data. In this paper, it is demonstrated that NLP models obtained by fine-tuning a pretrained BERT model can be extracted without access to training data. Extraction attacks are possible with randomly sampled word sequences and simple heuristics. Using randomly-sampled sentences and paragraphs from Wikipedia as queries further improves extraction performance. These attacks are cost-effective. Extraction attacks on NLP models can be conducted using randomly sampled word sequences and simple heuristics, without access to training data. These attacks are cost-effective, with the most expensive one costing around $500. The attacker samples words to form queries, fine-tunes their own BERT model using the victim's outputs as labels, and extracts models successfully. Despite the effectiveness of these random queries, they are mostly nonsensical and uninterpretable. In the study, it was found that queries closer to the original data distribution work better for model extraction. Pretraining on the attacker's side also facilitates model extraction. Two defenses against extraction - membership classification and API watermarking - were tested, showing effectiveness against naive adversaries but failing against more sophisticated ones. The research aims to inspire stronger defenses against model extraction and a better understanding of vulnerabilities in models and datasets. The work is related to prior efforts on model extraction in computer vision and connects to zero-shot distillation and NLP rubbish input studies. Model extraction attacks have been studied empirically and theoretically, mostly against image classification APIs. Prior work on NLP systems attempted extraction using pool-based active learning, while our study focuses on nonsensical inputs for question answering tasks on modern BERT-large models. Our work focuses on distilling knowledge from larger models to smaller models for question answering tasks, using nonsensical inputs. Prior work has explored model extraction using active learning and white-box access to the teacher model, but our approach involves generating rubbish inputs to extract knowledge from deeper neural networks. BERT, a 24-layer transformer model, is trained on unnatural text inputs to perform well on NLP tasks without real examples during training. BERT's parameters are learned through masked language modeling on a large corpus of natural text, leading to state-of-the-art performance in NLP tasks with minimal supervision. Fine-tuning involves using a task-specific network in conjunction with BERT. Extraction attacks involve reconstructing a local copy of a black-box API model for a specific task. The attacker attempts to reconstruct a local copy of a model by using a task-specific query generator to create nonsensical word sequences as queries. The resulting dataset is used to train the model, fine-tuning a public release of a model on different NLP tasks. The study explores different query generators for tasks involving entailment, contradiction, neutral distribution, extractive question answering, and boolean question answering. The RANDOM generator uses nonsensical word sequences, while the WIKI generator uses actual sentences from WikiText-103. These generators were found insufficient for tasks with complex interactions in the input space. The study examines query generators for tasks with complex interactions in the input space, such as entailment and question answering. Specific heuristics are applied for tasks like MNLI and SQuAD/BoolQ to generate relevant queries. Evaluation is done with an equal number of queries as the original training data. In a controlled setting, attackers use the same number of queries as the original training dataset. Different query budgets are explored for each task, with commercial cost estimates provided using Google Cloud Platform's Natural Language API calculator. Two metrics are used for evaluation: accuracy of extracted models on the original development set and agreement with the victim model's outputs. Extracted models show high accuracy on original development sets across tasks, even with low query budgets. The extracted models show high accuracy on original development sets of all tasks, even when trained with nonsensical inputs. However, their agreement with the victim model's outputs is only slightly better than accuracy in most cases. The agreement is even lower on held-out sets constructed using different sampling schemes. The API's argmax labels for classification datasets were tested in WIKI experiments for SST2, MNLI, and BoolQ, showing minimal accuracy drop. Access to full probability distribution was deemed non-critical for model extraction. Query efficiency was measured with varying budgets, showing extraction success even with small queries, but accuracy gains diminish with more queries. In this section, an analysis is performed to understand why nonsensical input queries are effective for extracting NLP models based on BERT. The questions raised include the properties of these queries, the consistency of answers from different victim models, and the interpretability of nonsensical queries to humans. In this section, the study examines the interpretability of nonsensical queries to humans and the consistency of answers from different victim models. Five victim SQuAD models were trained with varying random seeds, showing high agreement on SQuAD training and development set queries but significantly lower agreement on WIKI and RANDOM queries. This suggests that victim models tend to be brittle on nonsensical queries. High-agreement queries are more useful for model extraction, showing large F1 improvements compared to random and low-agreement subsets. This indicates that agreement between victim models is a good indicator of input-output pair quality for extraction. Future work could explore integrating this observation into active learning objectives for better extraction. The study investigates if high-agreement nonsensical queries can be interpreted by humans. Annotators matched victim models' answers 23% on WIKI subset and 22% on RANDOM subset, but scored significantly higher on original SQuAD questions (77% exact match). This suggests that nonsensical queries may not have a clear human interpretation. In interviews, annotators used word overlap heuristic to select answer spans, but many nonsensical question-answer pairs remain mysterious. The study explores how extraction accuracy is affected by different pretraining setups, such as fine-tuning a different base model or training a QA model from scratch. BERT comes in two sizes: BERT-large and BERT-base. Accuracy is higher when the attacker starts from BERT-large. Fine-tuning BERT gives attackers a headstart due to the good starting point. The importance of fine-tuning from a good starting point is highlighted by training a QANet model on SQuAD without pretraining. The model initially has 1.3 million randomly initialized parameters. Results show high accuracy with original SQuAD inputs but a significant F1 drop with nonsensical queries, indicating the need for better pretraining to simplify extraction. This leads to investigating defense strategies against model extraction vulnerabilities in BERT-based models. The defense strategy against model extraction vulnerabilities in BERT-based models involves using membership inference to detect nonsensical inputs or adversarial examples, issuing random outputs to eliminate extraction signals. The defense strategy against model extraction vulnerabilities in BERT-based models involves using membership inference to detect nonsensical inputs or adversarial examples, issuing random outputs to eliminate extraction signals. Additionally, classifiers trained on MNLI and SQuAD examples transfer well to a balanced development set with the same distribution as their training data, remaining robust to query generation processes like RANDOM or SHUFFLE. Ablation studies on input features and watermarking are also discussed as defenses against extraction. The study by Szyller et al. (2019) explores the use of \"watermarked queries\" to defend against model extraction vulnerabilities in deep neural networks. By storing manipulated queries and outputs on the API side, the defense aims to detect extracted models that memorize these queries. Evaluation on MNLI and SQuAD datasets shows high accuracy in predicting watermarked outputs and low accuracy in predicting original labels, indicating successful watermarking. The study explores the use of watermarked queries to defend against model extraction vulnerabilities in deep neural networks. Watermarked models behave oppositely to non-watermarked models, with high WM Label Acc and low Victim Label Acc. Watermarking works effectively but can only be used after an attack has been carried out. Model extraction attacks against NLP APIs serving BERT-based models are effective even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, prompting the need for further research. Further research is needed to develop robust defenses against adaptive adversaries in model extraction attacks on NLP APIs serving BERT-based models. Future directions include leveraging nonsensical inputs for model distillation, diagnosing dataset complexity using query efficiency, and exploring victim model agreement for input distribution proximity identification in active learning setups. The Natural Language APIs have a character limit of 1000 per query for pricing. Costs were extrapolated for tasks not covered by Google Cloud APIs. Estimates were based on models being similar to BERT-large. It is challenging to estimate query costs, but some providers offer free queries. Attackers could use multiple accounts for data extraction. API costs can vary depending on infrastructure and revenue models. It is important to focus on the low costs needed for data extraction rather than actual estimates. For example, extracting a large speech recognition dataset can cost around $430.56. In (Godfrey et al., 1992), a conversational speech recognition dataset with 300 hours of speech is discussed, along with $2000.00 allocated for 1 million translation queries. Input generation algorithms for datasets like SST2 involve building a vocabulary from wikitext103 and randomly sampling tokens from the top 10000 vocabulary. The text discusses the process of sampling words from the top-10000 wikitext103 vocabulary for different tasks like MNLI, SST2, and SQuAD. It involves replacing words in the premise and hypothesis with randomly chosen words from the vocabulary. Additionally, a vocabulary is built using wikitext103 for SQuAD, and paragraphs are constructed by sampling tokens from the vocabulary. The process involves randomly sampling paragraph tokens from wikitext103 to create questions of varying lengths. Question starter words are chosen randomly from a list, and questions are appended with a ? symbol. This method is used for tasks like SQuAD, BoolQ, and WIKI. In this section, additional query generation heuristics are studied by comparing extraction datasets for SQuAD 1.1 and MNLI. Findings show that starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. The study on MNLI reveals that the model's predictions are influenced by the lexical overlap between the premise and hypothesis. The study on query generation heuristics for SQuAD 1.1 and MNLI found that starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. The model's predictions in MNLI are influenced by the lexical overlap between the premise and hypothesis. An experiment with human annotators showed that lexical overlap affects dataset balance and extraction signal. In an ablation study on input features for the membership classifier, two candidates were considered: 1) the logits of the BERT classifier indicating confidence scores, and 2) the last layer representation containing lexical, syntactic, and semantic information. The study also showed the inter-annotator agreement among victim models for different types of questions. The ablation study compared the effectiveness of using the last layer representations versus the logits in distinguishing between real and fake inputs. Results showed that both feature sets together yielded the best results. Last layer representations were found to be more effective in classifying points as real or fake."
}