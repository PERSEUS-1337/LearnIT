{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as a source of domain knowledge representation. However, ontology information is rarely considered in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, showing improved performance by incorporating ontological information. Humans can recognize various sounds in their environment, which can be categorized into more abstract classes. Ontologies provide a structure for sound event classification datasets but are underutilized in current models. Neural networks are commonly used for sound event classification but often do not leverage available ontological information. Neural networks for sound event classification often do not consider ontologies, which provide a formal representation of domain knowledge through categories and relationships. Ontologies can enhance training data and network architecture. Different types of ontologies exist, such as those based on abstraction hierarchies or interactions between objects and materials. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Ontologies can enhance neural networks for sound event classification by providing hierarchical relations, allowing for back-off to general categories, disambiguating acoustically similar classes, and penalizing misclassifications differently. Ontology-based network architectures have shown performance improvements in computer vision and music, but are rarely used in sound event classification. Authors in BID16 proposed an ontology-based deep restricted Boltzmann machine for textual topic classification, showing improved performance and reduced overfitting. Another example used a perceptron for each node in the hierarchy, leading to an improvement in performance through class disambiguation. Building on these approaches, the proposed ontology-based networks aim to leverage deep learning architectures for handling ontological information effectively. In this paper, the authors propose a framework for learning architectures based on ontologies. They describe the assumptions and implications of the ontologies they work with, introduce a Feed-forward model with constraints, and extend the learning model to compute ontology-based embeddings using Siamese Neural Networks. The framework is designed to utilize ontology structure and model neural network architectures, with a focus on ontologies with two levels. The training data consists of audio representations associated with labels from the ontology. The authors propose a framework for learning architectures based on ontologies, utilizing audio representations associated with labels from the ontology. The ontology consists of hierarchical levels where each class is mapped to the next level. This mapping allows for inferring labels at higher levels based on known labels at lower levels. The framework formalizes this inference using a probabilistic formulation. The authors propose a framework for learning architectures based on ontologies, utilizing audio representations associated with labels from the ontology. The framework formalizes inference using a probabilistic formulation, improving model performance by relating different classes in the ontology. The proposed framework designs ontology-based neural network architectures, introducing an ontological layer that utilizes the ontology structure. The Feed-forward Network (FFN) with Ontological Layer consists of a base network, an intermediate vector z, and two outputs for each ontology level. The base network learns weights at every parameter update, using input audio features x to generate a vector z. This vector is used to produce probability vectors for different classes in C1 and C2. The FFN can predict any class in C1 and C2 for any input x after training. The ontological layer in the FFN architecture reflects the relation between super classes and sub classes in the ontology. Equation 3 describes how the layer is used, with M as the incidence matrix. The weights in the ontological layer are not trainable but are part of the training data. To train the model, a gradient-based method is applied to minimize the loss function, which combines two categorical cross-entropy functions. In this section, we describe how we learned the ontology-based embeddings using a Siamese neural network (SNN) to preserve the ontological structure. The SNN architecture enforces samples of the same class to be closer and separates samples of different classes. The goal is to create embeddings that reflect the ontological relationships. The architecture of the SNN with the Feed-forward Network with Ontological Layer is shown in FIG1. The twin networks use the same base architecture with shared weights to compute similarity metrics based on ontological embeddings. The distance between embeddings indicates the difference between samples in terms of ontology. Output probabilities are generated for different levels based on the ontological structure. The Feed-forward Model with Ontological layer using Ontology-based embeddings is trained with three types of audio example pairs. Evaluation of sound event classification performance of ontological-based neural network architectures is presented, along with datasets, ontologies, baseline and proposed architectures, and classification performance at different hierarchy levels. The dataset for Making Sense of Sounds Challenge 2 - MSoS aims to classify the highest level classes in its taxonomy, with an ontology having 97 classes at level 1 and 5 classes at level 2. Audio files are sourced from Freesound database, ESC-50 dataset, and Cambridge-MT Multitrack Download Library, with a development dataset of 1500 audio files. The Multitrack Download Library dataset consists of 1500 audio files divided into five categories, each with 300 files. The evaluation dataset includes 500 audio files, with 100 files per category. All files are in single-channel 44.1 kHz, 16-bit .wav format, 5 seconds long, and randomly partitioned for training and testing. The Urban Sounds -US8K dataset evaluates urban sound classification with a taxonomy adjusted to avoid redundant levels. The ontology in FIG2 shows two levels with 10 classes at level 1 and 4 classes at level 2. The dataset contains 8,732 audio files in 10 subsets, with 9 folds used for training and tuning parameters. Audio recordings were represented using state-of-the-art Walnet features BID1, transformed via a CNN trained on AudioSet. The base network architecture is a feed-forward multi-layer perceptron with 4 layers. The network consists of 4 layers: input layer (1024 dimensions), 2 dense layers (512 and 256 dimensions), and output layer (128 dimensions). Dense layers use Batch Normalization, dropout rate of 0.5, and ReLU activation function. Parameters were tuned in the Net box and for transforming z into p(y 1 |x). Baseline models were considered for different data sets, without ontological information. Base Network Architecture was used with an additional output layer for level 1 or level 2. Training the Feed-forward model with Ontological Layer using \u03bb = 1 is equivalent to level 1 training. Loss function for level 2 is not considered with \u03bb = 1. The baseline models for MSoS and US8K data sets at level 1 and level 2 showed different performances. The best performance for MSoS data set was achieved with \u03bb = 0.8, improving classification results. The ontological layer had an impact on classification in both levels, with values of \u03bb other than 0 or 1 showing increased performance. In the case of the MSoS data set, the best performance was achieved with \u03bb = 0.8, resulting in 0.74 and 0.913 accuracy for levels 1 and 2. This led to a 5.4% and 6% improvement over baseline models. On the US8K data set, a smaller improvement was observed with \u03bb = 0.7, yielding accuracies of 0.82 and 0.86 for levels 1 and 2, respectively, resulting in a 2.5% and 0.2% improvement over baseline models. The t-SNE plots show the impact of ontological embeddings on class grouping at different levels. The study evaluated the performance of ontology-based embeddings for sound event classification using a Siamese neural network. The embeddings resulted in tighter clusters, and t-SNE plots illustrated the clustering at different levels. The SNN was trained for 50 epochs with tuned hyper-parameters, achieving good performance with input features. Different super and sub class pairs were used for training data, ranging from 100 pairs. The study found that using 100,000 pairs for training data yielded the best performance. Different lambda values were tested for the loss function, affecting overall performance. Results showed accuracy performance for MSoS and US8K in level 1 and 2. The architecture outperformed the baseline but slightly underperformed without embeddings. Ontology-based embeddings showed better grouping. t-SNE plots were created for MSoS data. The study compared the performance of FF + Ontology vectors and ontology-based embeddings using t-SNE plots for level 2 classes. The ontology-based embeddings showed tighter clusters. Performance on the US8K dataset was limited due to a similar number of sub classes and super classes. The Feed-forward Network with Ontological Layer achieved 0.88 accuracy in the Making Sense of Sounds Challenge. In this paper, a framework for designing neural networks for sound event classification using hierarchical ontologies was proposed. Two methods were presented to incorporate structure into deep learning models without adding more parameters. A Feed-forward Network with an ontological layer and a Siamese neural Network were used to relate predictions across different levels in the hierarchy and compute ontology-based embeddings, respectively. Results from datasets and the Making Sense of Sounds Challenge showed improvements over baselines, paving the way for further exploration of ontologies and relations in sound event classification. The paper proposed a framework for sound event classification using hierarchical ontologies, showing improvements over baselines. Further exploration of ontologies and relations is essential due to acoustic diversity and limited lexical terms for sound description."
}