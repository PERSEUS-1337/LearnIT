{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. Combining quantization-aware training with weight matrix factorization significantly reduces model size and computation for small-footprint keyword spotting while maintaining performance. Quantization-aware training is used to optimize small-footprint low-power keyword spotting models by considering quantized weights in full precision representation. This approach enables successful training of 8 bit and 4 bit quantized models. Dynamic quantization is employed to calculate shifts and scales for quantizing DNN weight matrices. The quantization approach involves calculating shifts and scales for quantizing DNN weight matrices independently column-wise. Inputs are quantized row-wise on the fly during training. The keyword 'Alexa' is used in experiments with a 500 hrs far-field corpus. Models are evaluated using DET curves and AUC. Training is done in 3 stages with a small ASR DNN pre-trained in the 1st stage. In the experiment, a small ASR DNN with 3 hidden layers of 128 units is pre-trained using full ASR phone-targets. The performance of the quantized models is evaluated, showing an improvement in AUC. DET curves for different quantization levels are compared, with 16 bit and 8 bit models not significantly different from full-precision."
}