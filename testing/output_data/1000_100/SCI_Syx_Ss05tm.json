{
    "title": "Syx_Ss05tm",
    "content": "Deep neural networks are vulnerable to adversarial attacks in computer vision, where perturbations to images can cause the model to make mistakes. A new attack method reprograms the target model to perform a task chosen by the attacker without specifying the desired output for each input. This single perturbation can be added to all inputs to make the model perform the desired task, even if it was not trained for it. This method was demonstrated on six ImageNet classification models. The study focuses on adversarial reprogramming of ImageNet models for counting and classification tasks. Adversarial attacks aim to cause model errors with minimal input changes, such as altering images to deceive self-driving cars or insurance models. Various methods have been proposed to construct and defend against these attacks, primarily untargeted ones. In this work, the focus is on adversarial attacks that aim to reprogram the model to perform a task chosen by the attacker, without needing to compute the specific desired output. This novel and challenging goal goes beyond traditional targeted or untargeted attacks, requiring proactive measures to anticipate different adversarial goals for enhancing machine learning system security. The attacker can reprogram a model to perform a different task by learning adversarial reprogramming functions that map between the original and adversarial tasks. This involves converting inputs and mapping outputs to achieve the desired adversarial goal. Adversarial reprogramming involves repurposing a model to perform a new task by learning transformation functions between input and output formats. The attack does not necessarily need to be imperceptible to humans to be successful, and potential consequences include theft of computational resources. Adversarial reprogramming involves repurposing AI-driven assistants into spies or spam bots, abusing machine learning services, and theft of computational resources. The flexibility of neural networks allows for repurposing through changes to network inputs, as shown by studies on network expressive power and output patterns. In this paper, the concept of adversarial reprogramming is introduced, which involves repurposing AI-driven assistants for malicious activities. The authors present a training procedure for crafting adversarial programs that can cause a neural network to perform a new task. Experimental demonstrations show how these adversarial programs can alter the function of convolutional neural networks designed for ImageNet classification. Adversarial programs can alter network function for different tasks like counting squares, classifying digits, and images. Trained and untrained networks are susceptible to this reprogramming. Adversarial data can be used to reprogram tasks without transfer learning. Adversarial examples are intentionally designed inputs to cause model mistakes. The possibility of concealing adversarial programs and data is also demonstrated. Adversarial attacks involve using a gradient-based optimizer to find images that cause model mistakes, either untargeted or targeted. These attacks have been applied in various domains like malware detection and generative models. Reprogramming methods aim to produce specific functionality rather than a hardcoded output, extending the existing work on adversarial attacks. Adversarial attacks aim to produce specific functionality rather than a hardcoded output. A single adversarial program can cause a model to process multiple input images according to the adversarial program. Parasitic computing forces a system to solve complex tasks by exploiting network communication protocols, while weird machines use crafted inputs to run arbitrary code on a targeted computer. Adversarial reprogramming repurposes neural networks to perform new tasks without gaining access to the host computer. Transfer learning and adversarial reprogramming share the goal of repurposing networks, with neural networks possessing properties useful for various tasks such as developing features resembling Gabor filters when trained on images. Transfer learning involves training a convolutional neural network for one task and then using a linear SVM classifier to adapt it for other tasks. Adversarial reprogramming, on the other hand, allows for changing model parameters for new tasks, making it more challenging than transfer learning. This work explores adversarial reprogramming across tasks with different datasets, where an adversary can access the parameters of a neural network. Adversarial reprogramming involves an adversary altering a neural network's parameters to perform a new task by adding an adversarial program to the network input. The program is not specific to a single image but applies to all images, with parameters to be learned. This method can be extended to various settings beyond ImageNet classification. The adversarial program alters neural network parameters to perform a new task by adding a program to the network input. The mask M is not required for visualization purposes. The adversarial image is generated based on the input image x. The probability P(y|X) is the likelihood of an ImageNet label y given input image X. A mapping function maps adversarial task labels to ImageNet labels. The adversarial program maps adversarial task labels to ImageNet labels to maximize the probability of correct classification. Optimization is done with a weight norm penalty to reduce overfitting, using Adam with exponentially decaying learning rate. The program has minimal computation cost for the adversary post-optimization, requiring only the input image and mapping to the correct ImageNet label during inference. Adversarial reprogramming exploits nonlinear behavior of target models, unlike traditional adversarial examples. Experiments on six architectures trained on ImageNet showed reprogramming for tasks like counting squares and image classification. Top-1 ImageNet precisions were obtained from TensorFlow-Slim weights. The study examined adversarial reprogramming on ImageNet-trained networks, comparing resistance between trained and random networks. They explored reprogramming with dissimilar data and concealing the adversarial program. Using a simple task of counting squares in images, they embedded adversarial programs to create new images. The study embedded adversarial programs in ImageNet-trained networks to create new images for a counting task. Despite the dissimilarity between ImageNet labels and adversarial labels, the adversarial program successfully mastered the counting task for all networks. The adversarial program successfully reprograms ImageNet networks to classify MNIST digits by adding an adversarial program, demonstrating the vulnerability of neural networks to reprogramming. The adversarial program successfully reprograms ImageNet networks to classify CIFAR-10 images, increasing accuracy from chance to a moderate level with minimal computation cost. This demonstrates the versatility of adversarial programs in repurposing neural networks for different tasks. The study examined the susceptibility of an Inception V3 model trained on ImageNet data to adversarial reprogramming. Results showed that even with adversarial training, the model was still vulnerable to reprogramming, with only a slight reduction in attack success. The study found that standard defense approaches are ineffective against adversarial reprogramming due to differences in goals and magnitude of attacks. Adversarial reprogramming attacks were also successful on models with random weights, showing a dependence on model details. The study showed that adversarial reprogramming attacks were successful on models with random weights, indicating the importance of the original task the neural networks perform. Randomly initialized networks posed a challenge for training and resulted in lower accuracy compared to trained ImageNet models. The study demonstrated successful adversarial reprogramming attacks on models with random weights, highlighting the significance of the original task performed by neural networks. Despite the lack of spatial structure in shuffled MNIST digits, pretrained ImageNet networks were reprogrammed to classify them with comparable accuracy to standard MNIST. The study showed successful reprogramming of neural networks to classify shuffled CIFAR-10 images, with decreased accuracy compared to standard CIFAR-10. This suggests the potential for reprogramming across tasks and domains, indicating susceptibility to adversarial reprogramming beyond knowledge transfer. In experiments with an Inception V3 model, adversarial reprogramming was successful even with small program sizes or imperceptible perturbations. The study also tested concealing the entire adversarial task. The study successfully reprogrammed an Inception V3 model with imperceptible perturbations and small program sizes. They also tested concealing the adversarial task by hiding both the data and program within a normal ImageNet image. The study optimized an adversarial program to reprogram a neural network to classify MNIST digits, using a simple shuffling technique to hide the task within an ImageNet image. Trained neural networks were found to be more susceptible to reprogramming, even when the data structure differed from the original task. This demonstrates the flexibility and effectiveness of adversarial reprogramming. The study showed that trained neural networks can be easily reprogrammed to perform new tasks, even when the data structure differs. This highlights the potential for more flexible and efficient machine learning systems. Future research could explore adversarial reprogramming in various domains beyond image classification, such as audio, video, and text tasks. Reprogramming recurrent neural networks (RNNs) is of particular interest due to their Turing completeness, where finding inputs to induce simple operations could lead to reprogramming the RNN for any task. Reprogramming RNNs could lead to theft of computational resources and ethical violations by adversaries. ML service providers are concerned about protecting ethical principles. Neural networks can be reprogrammed for novel adversarial tasks, showing surprising flexibility and vulnerability. Future research should focus on understanding and defending against these attacks. The shuffled image combined with an adversarial program successfully reprograms the Inception V3 model to classify shuffled digits, despite the adversarial data being unrelated to the original data."
}