{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the importance of providing online explanations during execution to reduce mental workload is emphasized. The challenge lies in the interdependence of different parts of an explanation. A general formulation for online explanation generation is proposed, with three implementations satisfying various online properties. The method is based on a model reconciliation setting from prior work and is evaluated with human subjects in a planning competition domain and in simulation with different problems. As intelligent robots interact more with humans, providing explanations for their decisions is crucial for maintaining trust and shared awareness. Prior work on explanation generation often overlooks the recipient's perspective, but a good explanation should consider the human's understanding. To address this, the agent should account for discrepancies between the human and its own model when generating explanations. In prior work, discrepancies between human and robot models are encapsulated as model differences. The robot's model (M R) and human's model (M H) are used to generate explanations to reconcile the two plans. This process, known as model reconciliation, addresses the need for the robot to explain its behavior in a way that aligns with the human's expectation. However, one issue that remains is the mental workload required for the human to understand these explanations. In this work, the focus is on providing online explanations that intertwine with plan execution to reduce mental workload. Generating online explanations spreads out information smoothly without causing cognitive dissonance, despite the interdependence of different parts. Online explanations help reduce cognitive dissonance by providing information smoothly during plan execution. Mark and Emma's study session illustrates this, as Mark's plan differs from Emma's preference, but he withholds his plan to avoid conflict until after studying for 60 minutes. Mark demonstrates the importance of providing online explanations during plan execution to reduce cognitive dissonance. He gradually reveals his reasoning to Emma, ensuring his plan is acceptable and understandable, even though they have different values. The key is to explain minimally and only when necessary, spreading out the information throughout the execution to reduce mental workload. In this paper, a new method for online explanation generation is developed to reduce mental workload during plan execution. Three different approaches are implemented, focusing on matching the plan prefix, making the next action understandable, and matching the robot's plan with an optimal human plan. The goal is to provide explanations at different times during plan execution to ensure better understanding. The paper introduces a method for online explanation generation to reduce mental workload during plan execution. It focuses on matching the robot's plan with an optimal human plan, ensuring better understanding for the recipient. The importance of explainable AI in human-AI collaboration is highlighted. Explainable AI in human-AI collaboration is crucial for improving human trust and maintaining shared situation awareness. The effectiveness of explainable agency is evaluated based on accurately modeling the human's perception of the AI agent. This model allows the agent to generate legible motions, explicable plans, or assistive actions, often prioritizing explicability over cost optimality. Additionally, the AI agent can signal its intention before execution using this model. One way to use the model is for an AI agent to signal its intention before execution. Another way is for the agent to explain its behavior by generating explanations. Research has focused on generating the \"right\" explanations based on the recipient's perception model. The mental workload required for understanding an explanation is often ignored. In this work, the focus is on how the ordering of information in an explanation can affect perception. The concept of online explanation generation is introduced for complex explanations, intertwining explanation with plan execution. The problem is based on model reconciliation and planning problems defined in prior work. The explanation focuses on defining a tuple (F, A, I, G) using PDDL similar to STRIPS. It involves predicates, actions, preconditions, add and delete effects, initial and goal states. The robot's optimal plan is based on cost and model reconciliation with human expectations. Explanation generation in a model reconciliation setting aims to bring two models, M H and M R, close enough by updating M H so that the robot's plan, \u03c0 * I,G, becomes fully explainable in the human's model. A mapping function in BID6 converts a planning problem into a set of features, transferring any planning problem (F, A, I, G) to a state s in the feature space. The explanation generation problem involves a tuple (\u03c0 * I,G, M R, M H), where an explanation consists of unit feature changes to M H after the changes. Explanation generation in a model reconciliation setting aims to bring two models, M H and M R, close enough by updating M H so that the robot's plan becomes fully explainable in the human's model. A minimal complete explanation (MCE) is defined as containing the minimum number of unit feature changes. Online explanation generation is introduced to address the mental workload requirement of the human for understanding the explanation. Online explanation generation is introduced to address the mental workload requirement of the human for understanding the explanation. It involves providing a minimal amount of information during plan execution to explain the part of the plan that is of interest and not explainable. An online explanation consists of sub-explanations (e k , t k ), where e k represents the kth set of unit features to be made at step t k in the plan. The robot can split an explanation into multiple parts, made in an online fashion as the plan is executed. Online explanation generation aims to reduce the mental workload by providing minimal information during plan execution. It involves generating sub-explanations for an online explanation, considering how model changes affect human expectations. The challenge lies in ensuring that future model changes do not disrupt previously reconciled plan prefixes. This problem is approached by converting explanation generation into model search in the space of possible models. The process of online explanation generation involves creating sub-explanations to reduce mental workload during plan execution. To ensure that future model changes do not disrupt previously reconciled plan prefixes, a search is conducted from MR to MH to find the largest set of model changes that maintain plan consistency. This recursive process involves comparing plan prefixes with human model expectations and identifying the first step where they differ to start the search for sub-explanations. Our approach in step 1 differs from the previous approach in that the search for sub-explanations starts from the robot model, matching plan prefixes with the updated human model. This iterative process, although more computationally expensive, allows us to outperform other methods in terms of computation by considering only a small set of changes at a time. The dotted line marks the maximum state space model modification in the robot model to reconcile both models up to the current plan execution point. The approach in step 1 involves reconciling the robot model with the human model by finding the largest set of model changes. This ensures that the prefix of a plan using the corresponding model matches up to a certain step. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding optimal plans. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. It involves reconciling the robot model with the human model by finding the largest set of model changes to ensure plan alignment. The goal is to have the robot and human plans match at each step of plan execution. The approach involves reconciling between the robot and human models to match the next action in the plan, focusing on the differences between the human plan and the robot's plan. The search is performed from M H \\M H for computational efficiency, ensuring plan alignment at each step of execution. The OEG search process aims to reconcile differences between human and robot plans by focusing on immediate next actions. It combines search from M H and M R for better performance, assuming the robot has only one correct plan. The robot's goal is to reconcile the human's plan with its own using model space search. In a setting with optimal plans, the robot aims to match its plan prefix with the human's plan. To achieve this, a compilation approach is implemented to ensure the robot's plan prefix is always a prefix of the human's plan. The robot aims to match its plan prefix with the human's plan by ensuring that a plan prefix is always satisfied in the compiled model. This is achieved by adding predicates to actions in the model space search process. The approach involves finding differences between two models and checking for an optimal human plan that matches the robot's plan. This process continues until a matching plan is found. The approach was evaluated with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach. The approach was evaluated with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach on different problems in the rover and barman domains. The goal was to see how online explanation differs from MCE in terms of information needed and computation time. The human subject study aimed to confirm the benefits of online explanation generation, hypothesizing that it would reduce mental workload and improve task performance in a modified rover domain scenario on Mars. The rover must calibrate its camera before taking images and have empty storage space to sample rock or soil. It can only store one sample at a time and must drop the current sample to take another. In the barman domain, the robot's goal is to serve drinks using specific objects with constraints on grabbing and filling glasses. The simulation results compare minimally complete explanations (MCE) with OEG-PP, OEG-NA, and OEG-AP approaches for problems in the rover and barman domains. OEG focuses on generating minimal information at each time step, leading to more shared information in total for OEG-PP and OEG-NA compared to MCE in some cases. The OEG-PP and OEG-NA approaches share more information compared to MCE due to the dependence between features and planner behavior. OEG-AP shows the advantage of considering all optimal plans, but there is a distance between robot and human plans in terms of action distance. OEG-NA only considers the immediate next action, while OEG-AP may not return the same plan as the robot's. This is illustrated in FIG3. In our implementation, model updates are sorted based on feature size, with backtracking performed if consistency check fails. This search process capitalizes on the fact that later information often doesn't affect previous explanations. A human study was designed to compare three approaches for online execution, aiming for a smoother adjustment for human mental workload. To test our hypothesis, a human study was conducted using Amazon Mechanical Turk with 3D simulation to compare three approaches for online explanation generation. Explanations were provided in plain English language with GIF images depicting rover actions in a 3D simulated scenario. The subjects acted as the rover's commander in a Mars scenario with a 30-minute time limit for the task. The human subject acts as the rover's commander on Mars, determining if the rover's actions are questionable with explanations provided. Additional spatial puzzles were added to increase cognitive demand. Hidden information was deliberately removed to test mental workload. In the model reconciliation setting, differences between M H and M R introduce hidden information, leading to scenarios where explanations must be provided. The robot shares all information at the beginning in MCE, while in MCE-R, information is broken down and communicated at different steps. The robot uses various approaches for online explanation generation in OEG settings, intertwining explanation communication with plan execution. Subjects are asked to determine the sense of the robot's actions in all settings based on minimally complete explanations. The study evaluated different explanation approaches using the NASA Task Load Index (TLX) questionnaire to assess mental workload. NASA TLX measures mental demand, physical demand, temporal demand, performance, effort, and frustration. The experiment did not include questions related to physical demand. The study assessed mental workload using the NASA Task Load Index questionnaire, focusing on mental demand. Subjects were recruited through Qualtrics and MTurk, with specific criteria for valid responses. The analysis compared human understanding of robot plans with different explanations across various settings. The study compared human understanding of robot plans with different explanations across various settings. Results show that OEG approaches reduce human's mental workload better than MCE approaches, as evidenced by subjective NASA TLX measures. OEG approaches create more temporal demand due to intertwining explanation process with plan execution. FIG6 presents objective performance measures and subjective results of the human study. The study compared human understanding of robot plans with different explanations. OEG approaches showed lower questionable actions and higher accuracy, indicating more trust towards robots. Subjective measures revealed a significant difference in mental workload between OEG approaches and MCEs. Time analysis was also conducted. In this paper, a novel approach for explanation generation was introduced to reduce mental workload during human-robot interaction. The approach involves breaking down complex explanations into smaller parts and conveying them in an online fashion. Three different approaches were provided to ensure correct and easily understandable explanations. Time analysis showed varying completion times for different categories, but no significant difference in accuracy of the secondary task. The study introduced a novel approach for explanation generation to reduce mental workload during human-robot interaction. Three different approaches were evaluated, showing improved task performance and reduced mental workload."
}