{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are used for control policies in reinforcement and imitation learning. A new technique called Quantized Bottleneck Insertion helps create finite representations of RNN memory vectors and observation features. This allows for better analysis and understanding of RNN behavior. Results on synthetic environments and Atari games show small finite representations can lead to improved interpretability in policies. In this paper, the focus is on improving the interpretability of RNN policies used in reinforcement and imitation learning. The challenge lies in understanding and explaining RNN memory, which is typically represented by high-dimensional continuous vectors. The goal is to create more compact memory representations to enhance comprehension and trust in the learned policies. The paper aims to enhance the interpretability of RNN policies by transforming continuous memory vectors into a finite-state representation using Quantized Bottleneck Networks (QBNs). This approach allows for a more direct capture of discrete concepts within the memory, improving comprehension and trust in learned policies. The idea of Quantized Bottleneck Network (QBN) insertion involves training QBNs to encode memory states and observation vectors encountered during RNN operation. These QBNs are then inserted into the RNN policy, creating a Moore Machine Network (MMN) with quantized memory and observations. Training quantized networks like QBNs is simplified with effective gradient estimators. Our approach involves training QBNs to encode memory states and observation vectors in RNN operation, creating MMNs with quantized memory and observations. Experiments on synthetic domains and benchmark grammar learning problems show accurate extraction of MMNs, providing insights into RNN memory use. Additionally, experiments on 6 Atari games using RNNs achieve state-of-the-art performance, with near-equivalent MMNs extracted, revealing insights into memory usage not obvious from observing RNN policy. Games are identified where RNNs do not use memory meaningfully, indicating purely reactive control, while in others, observations are not used effectively, suggesting an open-loop controller implementation. The RNN is implementing an open-loop controller, with no prior work on learning finite-memory representations of continuous RNN policies. Previous work includes extracting Finite State Machines from recurrent networks trained to recognize languages, using methods like gridding or clustering. Our approach involves training QBNs to encode memory states and observation vectors in RNN operation, creating MMNs with quantized memory and observations. Experiments show accurate extraction of MMNs, providing insights into RNN memory use. Our approach involves directly inserting discrete elements into the RNN to preserve its behavior and allow for a finite state characterization. This approach enables fine-tuning and visualization using standard learning frameworks. Compared to previous work on learning FSMs, our method focuses on learning MMNs, which are qualitatively similar to the memory representation of recurrent networks. Our work extends the approach of learning from scratch to describe the behavior of a continuous RNN. Our work extends the approach of learning from scratch to describe the behavior of a continuous RNN by introducing the method of QBN insertion to learn MMNs. This allows for the transformation of any pre-trained recurrent policy into a finite representation, focusing on discrete memory and observations for interpretability rather than efficiency. Our work introduces the method of QBN insertion to transform a pre-trained recurrent policy into a finite representation, focusing on discrete memory and observations for interpretability. An RNN maintains a continuous-valued hidden state h t during execution, influencing action choice based on observation features f t and transitioning to a new state h t+1. The high-dimensional nature of h t and f t can make interpreting memory challenging, motivating the goal of extracting compact quantized representations. Our goal is to extract compact quantized representations of hidden states and observations to capture key features of memory. We introduce Moore Machines and their deep network counterparts, which are described by hidden states, observations, actions, transition functions, and policies. Moore Machines provide a finite system to investigate memory and observations, with transitions between hidden states based on observations. In this work, Moore Machine Networks (MMNs) use deep networks to represent transition functions and policies. MMNs map continuous observations to a finite discrete space and utilize quantized state and observation representations. The memory in an MMN is restricted to k-levels, resembling a traditional RNN. The memory in Moore Machine Networks (MMNs) is restricted to k-levels, resembling a traditional RNN. Learning MMNs from scratch can be challenging for non-trivial problems, such as training high-performing MMNs for Atari games. A new approach leverages the ability to learn RNNs by first learning quantized bottleneck networks (QBNs) for embedding continuous observation features and hidden states into a k-level quantized representation. The approach involves learning quantized bottleneck networks (QBNs) to embed continuous observation features and hidden states into a k-level quantized representation. QBNs are autoencoders with a constrained latent representation composed of k-level activation units, aiming to discretize a continuous space. This results in a network that consumes quantized features and maintains quantized state, resembling an MMN. The QBN utilizes a multilayer encoder to map inputs to a latent encoding, which is then quantized using a 3-level quantization scheme. To support 3-valued quantization, a specific activation function is used to ensure better learning outcomes. However, the introduction of the quantize function makes the network non-differentiable, posing challenges for backpropagation. The QBN utilizes a multilayer encoder for mapping inputs to a latent encoding, quantized using a 3-level scheme. The quantize function poses challenges for backpropagation, but the straight-through estimator effectively handles this issue. Training the QBN as an autoencoder with L2 reconstruction error, it produces a k-level encoding in the last layer of E. Running a recurrent policy in the target environment generates training sequences for observation, observation feature, and hidden state. The approach involves training two QBNs on observed features and states, aiming for low reconstruction error to obtain high-quality encodings. These encodings are then inserted into the original RNN as \"wires\" to propagate input with some noise. If perfect reconstructions are achieved, inserting the QBNs would not alter the RNN's behavior. The RNN can be viewed as an MMN with bottlenecks providing a quantized representation of features and states. Fine-tuning the MMN may be necessary if there is performance degradation, aiming to match the softmax distribution over actions produced by the RNN. Training in this way is more stable than simply outputting the same action as the RNN. The MMN is significantly more stable when trained to match the softmax distribution over actions produced by the RNN. Visualization and analysis tools can be used to investigate the memory and its feature bits for a semantic understanding. Creating a Moore Machine from the MMN can help understand the role of different machine states and their relationships. The Moore Machine is constructed from data to capture transitions, then minimized to reduce states and observations. The goal is to extract MMNs from RNNs without loss in performance and determine the magnitude of states in minimal machines. In this section, the focus is on exploring the general magnitude of states and observations in minimal machines for complex domains like Atari. The study involves two known domains with ground truth Moore Machines: a synthetic environment called Mode Counter and benchmark grammar learning problems. The Mode Counter Environments (MCEs) allow for varying memory requirements, including no memory, and different types of memory usage. MCEs are a type of Partially Observable Markov Decision Process transitioning between modes over time based on a distribution. The Mode Counter Environments (MCEs) involve transitioning between modes over time based on a distribution. Different parameterizations determine the memory requirements for inferring the mode and achieving optimal performance. Three MCE instances test different ways of using memory and observations, such as the Amnesia instance where optimal policy does not require memory for tracking past information. The Mode Counter Environments (MCEs) involve transitioning between modes over time based on a distribution. Three MCE instances test different ways of using memory and observations. The Blind instance requires memory to implement counters for determining optimal actions. The Tracker instance combines observations and memory to select optimal actions. All instances use M = 4 modes and a recurrent architecture with specific layers and nodes. The trained RNNs achieve 100% accuracy on the imitation dataset for the MCEs. The observation and hidden-state QBNs have varying bottleneck units. Training in MCE environments with QBNs is faster than RNN training. QBN training is faster than RNN training as temporal dependencies are not needed. Different bottleneck sizes were tested, with most cases not requiring fine-tuning due to low reconstruction error. Fine-tuning improved performance in all cases except one, which still achieved 98% accuracy. Inserting one bottleneck at a time resulted in perfect performance, showing the combined error accumulation of both bottlenecks. After inserting one bottleneck at a time, perfect performance was achieved, indicating that the combined error accumulation of the two bottlenecks is responsible for reduced performance. The number of states and observations of the Moore Machines extracted from the MMNs before and after minimization showed that typically there are significantly more states and observations before minimization than after. However, after minimization, exact minimal machines were obtained for each MCE domain, except for one case. The ground truth minimal machines found are shown in the Appendix. The MMNs learned via QBN insertions were equivalent to the true minimal machines, except in cases where perfect accuracy was not achieved. Examining these machines helps understand memory use, such as how transitions depend on input observations. Evaluation was done on 7 Tomita Grammars, treating them as environments for policy learning problems with two actions. The focus is on policy learning problems with grammars as environments having 'accept' and 'reject' actions. RNNs are trained with imitation learning using Adam optimizer. Test results show high accuracy except for grammar #6. MMNs are trained to understand memory use in transitions based on input observations. The RNNs used for training were 100% accurate, with MMNs trained without a bottleneck encoder. Bottlenecks were added to the RNNs to create MMNs, which maintained performance without fine-tuning. Results for MM extraction and minimization showed a reduction in state-space while maintaining accuracy. The MMN learning process does not directly lead to minimal machines, but it produces equivalent results. Applying the technique to RNNs trained for Atari games showed promising results despite the complexity of the input observations. The curr_chunk discusses the architecture of Atari agents, including the preprocessing steps for input observations and the neural network structure. The agents have a recurrent architecture with convolutional layers, a GRU layer, and a fully connected layer for predicting the value function. The A3C RL algorithm is used for training. The A3C RL algorithm was used to train the Atari agents with a recurrent architecture. The RNN performance on six games was reported, and training data was generated using noisy rollouts. The study utilized an -greedy version of the RNN policy to increase training data diversity and robustly learn QBNs for Atari games. Bottlenecks were trained with larger values for B h and B f due to game complexity. MMN performance was evaluated before and after fine-tuning, showing comparable scores to the RNN for certain games. The study evaluated MMN performance on Atari games before and after fine-tuning. MMNs achieved similar scores to RNNs in some games, but required fine-tuning for others. In Breakout and Space Invaders, MMNs scored lower due to poor reconstruction in certain game scenarios. After fine-tuning, MMNs show significant reduction in the number of states and observations, making them easier to analyze manually. However, understanding the meaning of observations and states in complex policies remains a challenge. In Atari games, different types of memory use were observed. Pong's memory model has three states and transitions to the same state regardless of the current state. Bowling and Freeway have minimal memory models that ignore input images, acting solely based on time-step. The MM extraction approach provides insight into the policies of Atari games like Bowling and Freeway. Bowling has an open-loop policy structure with an initial sequence of actions followed by a repeated loop. Freeway's policy always takes the Up action. Breakout, Space Invaders, and Boxing use memory and observations in their MM transition structures. Our approach extracts finite state Moore Machines from Atari policies by training Quantized Bottleneck Networks to produce binary encodings of RNN memory and input features. This allows us to accurately extract ground truth machines in known environments and analyze policies in Atari games. The study demonstrates the extraction of ground truth machines from Atari policies using Quantized Bottleneck Networks. It shows that learned MMNs maintain similar performance to original RNN policies in most cases. The analysis reveals insights into memory usage, highlighting the small number of memory states and observations required. Additionally, it identifies cases where policies either did not use memory significantly or relied solely on memory and ignored observations. Future work aims to develop tools for attaching meaning to discrete observations and states for deeper policy insight. The study explores using tools to analyze finite-state machine structure for gaining insight into policies. An MCE is defined by mode number, transition function, lifespan mapping, and count set. The hidden state is represented by a tuple of current mode and time-step count. Mode changes occur based on lifespan reaching a threshold, with transitions determined by a distribution. The agent receives continuous-valued information rather than directly observing the state. The agent receives continuous-valued observations o t \u2208 [0, 1] at each step, determining the mode when in C. To optimize performance, the agent must remember the current mode and track how long it has been active. Experiments are conducted with different MCE instances, including Amnesia with \u2206(m) = 1 for all modes and C = {0}. The experiments test different scenarios to determine if the policy is reactive, using memory, or ignoring observations. The modes and transitions are varied to assess the need for memory in tracking mode sequences. The experiments test different scenarios to determine if the policy is reactive, using memory, or ignoring observations. When c t = 0, memory is used to track the current mode and mode count. The environment's complexity increases with the number of modes and their life-spans. Machines are 100% accurate except for Grammar 6. Accept and reject states are denoted by 'A' and 'R'."
}