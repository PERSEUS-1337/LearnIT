{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging without supervision. A non-statistical framework is proposed, based on identifying a modular organization of the network through counterfactual manipulations. Modularity between groups of channels is achieved to some extent on various generative models, enabling targeted interventions on image datasets for applications like style transfer and assessing robustness to contextual changes. Deep generative models, such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE), have been successful in creating realistic images in various domains. Efforts have been made to produce disentangled latent representations for controlling interpretable properties of images, but the models may not be mechanistic in assigning properties to specific parts of the network architecture. Gaining access to a modular organization of generative models would benefit interpretability and allow extrapolations, consistent with the modular organization of the visual system. Extrapolations likely support adaptability to environmental changes and robust decision making. Leveraging trained deep generative architectures for such extrapolations remains an open problem due to non-linearities and high dimensionality. In this paper, a causal framework is proposed to explore modularity in generative models, allowing for individual modification of causal mechanisms without influencing each other. The principle of Independent Mechanisms is applied to assess how well generative models capture causal mechanisms. Causality is used to assess counterfactuals and the role of specific variables in the outcome. In this study, a causal framework is utilized to examine modularity in generative models, enabling isolated modification of causal mechanisms. The concept of Independent Mechanisms is employed to evaluate the ability of generative models to capture causal mechanisms. Causality is leveraged to analyze counterfactuals and the impact of specific variables on the outcome. The study also explores the interpretability of convolutional neural networks in generative architectures, highlighting the modularity of hidden units in VAEs and GANs trained on image databases. In this study, the concept of intrinsic disentanglement is introduced to uncover the internal organization of networks, arguing that many transformations are statistically dependent and unlikely to be disentangled in the latent space. This approach is contrasted with frameworks that require semantic information or group representation theory for disentanglement. The proposal introduces a flexible approach for arbitrary continuous transformations, independent of representation theory requirements. It also discusses an interventional approach to disentanglement and presents a general framework linking disentanglement to causal concepts within a generative model. Mathematical details are provided in the appendix for interested readers. The text discusses a generative model where data points are embedded in Euclidean space Y. A sample is generated by drawing from a prior latent variable distribution. The term representation refers to a mapping from Y to a representation space R. The generative model is implemented using a non-recurrent neural network with a causal graphical model representation. The text discusses a generative model using a non-recurrent neural network with a causal graphical model representation. It involves latent variables and endogenous variables represented by nodes in a causal graph, with a mapping computed by composing variable assignments and mappings. The endogenous variables can be output activation maps of hidden layers in a convolutional neural network. The internal representation of the network is defined by mild conditions ensuring invertibility. The text discusses a generative model using a non-recurrent neural network with a causal graphical model representation. It involves latent variables and endogenous variables represented by nodes in a causal graph, with a mapping computed by composing variable assignments and mappings. The endogenous variables can be output activation maps of hidden layers in a convolutional neural network. The internal representation of the network is defined by mild conditions ensuring invertibility. In the CGM framework, counterfactuals can be defined following Pearl (2014) and Imbens & Rubin (2015), inducing a transformation of the generative model output. In the context of a generative model, counterfactuals can lead to a transformation of the model's output. Faithfulness of a counterfactual mapping is introduced to ensure that interventions on internal variables do not result in outputs outside the original model's distribution. Non-faithful counterfactuals may produce examples that deviate from the learned data distribution, potentially leading to artifactual outputs or extrapolation to unseen data. The concept of disentangled representation suggests that latent variables encode real-world transformations sparsely, driving supervised approaches to disentangling. Supervised approaches focus on explicit manipulation of identified transformations, while unsupervised methods aim to learn real-world transformations from unlabeled data. State-of-the-art methods encode transformations through changes in latent factors, enforcing conditional independence. However, statistical constraints can lead to issues like imposing independence between factors that are confounded in reality. Finding a disentangled representation in unsupervised learning remains a challenge due to confounding factors in the data generating mechanisms. Current approaches struggle to achieve high visual sample quality on complex real-world datasets compared to synthetic datasets like CelebA. The proposed non-statistical definition of disentanglement aims to mathematically capture transformation-based insights for better representation learning. The text discusses the concept of disentanglement in representation learning, focusing on transformations in the latent space to encode different properties. It introduces the idea of extrinsic disentanglement, where transformations act on the latent representation independently, following the causal principle of independent mechanisms. The text discusses extrinsic disentanglement in representation learning, emphasizing the need for a different representation to uncover statistically related properties that are disentangled according to their definition. This is illustrated in the context of a graphical model where endogenous variables may not be statistically independent but still reflect interesting properties that can be intervened on independently. The text extends the definition of disentanglement to include transformations of internal variables, emphasizing the principle of independence of mechanisms. It introduces the concept of modularity as a structural property of internal representation, enabling arbitrary disentangled transformations. The text introduces the concept of modularity in internal variables, allowing for disentangled transformations within its input domain. This framework extends the definition of disentanglement to include transformations of internal variables, emphasizing independence of mechanisms. The concept of modularity in internal variables is introduced for disentangled transformations within the input domain. This framework extends the definition of disentanglement to include grouping neurons into modules for independent interventions. The concept of modularity in internal variables allows for disentangled transformations within the input domain. By assigning a constant value to a subset of endogenous variables, counterfactual interventions can be defined. Sampling from the marginal distribution of these variables helps avoid characterizing them, as illustrated in a feed-forward neural network example. The hybridization procedure involves taking two independent examples of latent variables to generate original outputs. By combining different aspects of these generated images, a hybrid example can be created to assess the impact of a given module on the output. The hybridization framework assesses how a module affects the generator's output by generating pairs of latent vectors independently. The influence map is estimated by calculating the mean absolute effect, considering unit-level causal effects and averaging them over many samples. The hybridization framework evaluates the impact of modules on the output by averaging causal effects over different interventions. Influence maps are grouped based on similarity to define modules at a coarser scale. Influence maps are grouped by similarity to define modules at a coarser scale. Representative EIMs for channels of convolutional layers of a VAE trained on the CelebA face dataset suggest channels are functionally segregated. Clustering of channels using their EIMs as feature vectors is performed in an unsupervised way. The text discusses the process of thresholding image maps and using Non-negative Matrix Factorization (NMF) to identify cluster template patterns. The NMF algorithm is chosen for its ability to isolate meaningful parts of images. The approach will be compared to the k-means clustering algorithm for validation. The text introduces a toy generative model for further justification of the NMF based approach. It involves a neural network with hidden layers and random choices for model parameters. The text introduces a toy generative model involving a neural network with hidden layers and random choices for model parameters. The model assumes coefficients are sampled i.i.d. from a distribution, with specific conditions enforcing that certain areas in the image are influenced by only one module. The identifiability result shows that the hidden layer partition corresponds to a disentangled representation, justifying the use of NMF for analyzing significant influences on each variable. The text discusses the application of endogenous variables to generate a binary matrix summarizing their influences on each output pixel. The sliding window is used to enforce similarity between influence maps within the same module. The study investigates modularity of generative models on the CelebFaces Attributes Dataset using DCGAN, \u03b2-VAE, and BEGAN. The procedure includes EIM calculations, clustering channels into modules, and hybridizing generator samples using these modules. Hybridization is done by intervening on the output of the intermediate convolutional layer. The study explores clustering channels into modules by intervening on the output of the intermediate convolutional layer. Setting the number of clusters to 3 results in interpretable cluster templates for background, face, and hair. Cluster stability analysis confirms robustness of the clustering. The study confirms the robustness of clustering channels into modules by setting the number of clusters to 3. Results show that NMF-based clustering outperforms k-means, with high consistency (>90%) for 3 clusters. The average cosine similarity of .9 between templates associated with matching clusters further supports the choice of 3 clusters. Influence maps reflect the spread of clusters over image locations. The study confirms the robustness of clustering channels into modules by setting the number of clusters to 3. Results show that NMF-based clustering outperforms k-means, with high consistency (>90%) for 3 clusters. Influence maps reflect the spread of clusters over image locations. Some maps may spread over image locations reflecting different clusters. Applying the hybridization procedure to the resulting 3 modules obtained by clustering leads to a replacement of features while respecting the overall structure of the image. Further work is needed to investigate whether better extrinsic disentanglement could favor intrinsic disentanglement in models. After confirming the robustness of clustering channels into modules, the study explores intrinsic disentanglement in models without explicit enforcement. GAN-like architectures, such as DCGAN, outperform VAE-like approaches in sample quality. The approach was also successful with a pretrained BEGAN model, known for high-resolution face image generation. This suggests the potential application of the approach to models not optimized for disentanglement. The study tested the hypothesis using BEGAN's simple generator architecture and observed that interventions on channels from the same cluster in two successive layers were needed to obtain noticeable effects in counterfactuals. Selective transfer of features from Original 2 to Original 1 was achieved by intervening on layers 5 and 6. The model trained on face images with a tighter frame showed clear hair transfer and different encoding of face features in different modules. The study evaluated the quality of counterfactual images compared to original generated images using the Frechet Inception Distance (FID). The hybridization procedure minimally affected image quality. The approach was tested on the BigGAN-deep architecture, pretrained on the ImageNet dataset, showing scalability to high-resolution generative models. The study evaluated the quality of counterfactual images compared to original generated images using the Frechet Inception Distance (FID). Hybridization procedure minimally affected image quality on the BigGAN-deep architecture, pretrained on the ImageNet dataset, showing scalability to high-resolution generative models. The model receives input from latent variables and class labels, with examples showing effective generation of counterfactuals by intervening on two successive layers within a Gblock. High-quality counterfactuals with modified backgrounds and similar foreground objects were generated, even with objects of different nature like a teddy bear in a tree or a \"teddy-koala\" merging textures on different backgrounds. The study compared the ability of pretrained classifiers to recognize original classes using counterfactual images. Recognition rates tended to increase with layer depth, with Inception resnet performing better at intermediate blocks 5-6. Non-consensual classification results were also observed. The study introduced a mathematical definition of disentanglement and used it to characterize representation in deep generative architectures. Evidence for interpretable modules in generative models was found, leading to a better understanding of complex architectures and applications like style transfer and assessing object recognition system robustness. The research focuses on enhancing the interpretability of deep neural networks and their adaptability to tasks they were not originally trained for. It proposes using trained generator architectures as mechanistic models that can be manipulated independently, with a mathematical representation using structural causal models. This approach aims to foster more sustainable research in Artificial Intelligence in the future. The research proposes using trained generator architectures as mechanistic models represented by structural causal models to enhance interpretability and adaptability of deep neural networks. This approach aims to foster sustainable research in Artificial Intelligence. The research proposes using trained generator architectures as mechanistic models represented by structural causal models to enhance interpretability and adaptability of deep neural networks. The generator's output can be decomposed into two successive steps, aligning with the definition of a deterministic structural causal model by Pearl. CGMs consist of input latent variables, the generator's output, and endogenous variables forming an intermediate representation. The research proposes using trained generator architectures as mechanistic models represented by structural causal models to enhance interpretability and adaptability of deep neural networks. CGMs have specificities reflecting the structure of models encountered in practice, allowing modeling feed-forward networks with latent inputs and deterministic operations. Basic properties in the computational graph of generative networks are guaranteed, enabling the introduction of useful mappings. In an embedded CGM, internal variables and outputs live on manifolds of smaller dimension than the latent space Z. Functions assign values to latent and endogenous variables, constrained to subsets of their ambient space. The CGM is defined by proper embeddings, ensuring invertibility. The image sets in a trained model are constrained by parameters and aim to approximate the data distribution. Matching the output to the data distribution is a key goal for generative models. Transformations must respect the topology of the output set. Generative models aim to match the output to the data distribution by using embeddings as the basic structure, ensuring topology preservation. Injectivity of the transformation function is a key requirement for embedded CGMs, where compactness of the latent space is crucial for injectivity. Generative models with uniformly distributed latent variables are considered embedded CGMs if injective. The embedded CGM approximates the original one for most samples by restricting it to compact intervals. The CGM framework defines counterfactuals in the network, inducing a transformation of the generative model output. Our approach relates counterfactuals to disentanglement, allowing transformations of internal variables in the network. Intrinsic disentanglement in a CGM involves a transformation of endogenous variables that only affects a subset E, illustrated in Fig. 2d. Intrinsic disentanglement in a CGM involves a transformation of endogenous variables that only affects a subset E, allowing for robustness to perturbations. The proof of equivalence between faithful and disentangled transformations is discussed, showing that compactness and injectivity lead to embeddings. The proof shows that a disentangled transformation is an endomorphism of Y M, with a faithful mapping from V to the output. The absence of a common latent ancestor between subsets E and E ensures unambiguous assignment of values. This implies that T is an endomorphism of V M for any choice of endomorphism T E. The i.i.d. assumption for components of Z and structure following Prop. 1 guarantee subsets of endogenous variables. The subsets of endogenous variables associated with each V k are modular and form a disentangled representation. Increasing dimensions and i.i.d. sampling ensure an injective mapping, fulfilling embedded CGM assumptions. Counterfactual hybridization of V k components results in an influence map covering I k. Conditions on I k and thresholding guarantee a rank K binary factorization of matrix B. The uniqueness of this factorization is ensured by classical NMF identifiability results. The \u03b2-VAE architecture is detailed in Supplementary Fig. 7. The \u03b2-VAE architecture, similar to DCGAN, consists of three blocks of convolutional layers with skip connections for image sharpness. The pre-trained model used has the same architecture as in the paper, with filter size (3,3) and upsampling layers. The BigGan-deep architecture was also utilized as a pre-trained model on 256x256 ImageNet without retraining. The architecture of the model used is based on a pre-trained BigGan-deep model on 256x256 ImageNet, consisting of ResBlocks with BatchNorm-ReLU-Conv layers and skip connections. Influence maps were generated by a VAE on the CelebA dataset, showing variance in perturbations. FID analysis of BEGAN hybrids measured distances between real and generated data for different classes. The architecture of the model used is based on a pre-trained BigGan-deep model on 256x256 ImageNet, consisting of ResBlocks with BatchNorm-ReLU-Conv layers and skip connections. Influence maps were generated by a VAE on the CelebA dataset, showing variance in perturbations. FID analysis of BEGAN hybrids measured distances between real and generated data for different classes. The results suggest that Hybridization produces visually plausible images, with entropy values indicating the quality of the generated images based on different interventions. The study suggests that object texture plays a crucial role in the classifier's decision, particularly in hybrids generated from Gblock 4. The entropy analysis of hybrids between classes \"cock\" and \"ostrich\" reveals that interventions on the first module of Gblock 5 produce hybrids mixing shape properties of both birds. The study investigates the use of intervention procedures to assess classifier robustness using koala+teddy hybrids. The hybrids resemble a teddy bear in a koala context, highlighting the importance of object recognition over contextual information. The nasnet large classifier shows more robustness to context changes compared to others."
}