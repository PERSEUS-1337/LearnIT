{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where the source and target domains may not share the same categories. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC), a novel approach that incorporates category-agnostic clusters in the target domain to improve domain adaptation. SE-CC utilizes clustering information in the target domain to enhance Self-Ensembling for domain adaptation. Clustering is used to reveal the data space structure specific to the target domain, ensuring the learned representation preserves this structure. Mutual information maximization further improves the representation. Experimental results on Office and VisDA datasets show superior performance compared to state-of-the-art approaches. Convolutional Neural Networks (CNNs) have advanced vision technologies, but require large amounts of annotated data for training. To address the challenge of domain shift, unsupervised domain adaptation leverages labeled source samples and unlabeled target samples to generalize a target model. Existing models often only align data distributions between domains, limiting their applicability to closed-set scenarios. The difficulty of open-set domain adaptation lies in distinguishing unknown target samples from known ones and learning a hybrid network for both closed-set and open-set scenarios. One approach is to use an additional binary classifier to label each target sample as known or unknown, treating unknown samples as outliers during adaptation. In open-set domain adaptation, distinguishing unknown target samples from known ones is challenging. One novel approach involves clustering all unlabeled target samples to explicitly model the diverse semantics of both known and unknown classes in the target domain. This clustering helps in creating domain-invariant representations for known classes during domain adaptation. In open-set domain adaptation, clustering target samples helps model diverse semantics of known and unknown classes. A new approach, SE-CC, integrates Self-Ensembling with Category-agnostic Clusters to refine representations for target samples by estimating assignment distribution over clusters. This preserves the target domain's inherent structure. The SE-CC framework integrates Self-Ensembling with Category-agnostic Clusters to refine representations for target samples by estimating assignment distribution over clusters. This helps preserve the target domain's inherent structure and enhances feature representation. The task is to learn transferrable features in CNNs by minimizing domain discrepancy through Maximum Mean Discrepancy (MMD). Different approaches like integrating MMD into CNNs, incorporating a residual transfer module, and using a domain discriminator for unsupervised domain adaptation have been explored. Ganin & Lempitsky (2015) formulated domain confusion as a binary classification task and utilized a gradient reversal algorithm to optimize the domain discriminator. The task of open-set domain adaptation involves tackling a realistic scenario where the target domain includes samples from new and unknown classes. Various approaches have been proposed, such as utilizing adversarial training to separate unknown target samples, factorizing source and target data into shared and private subspaces, and exploiting target sample assignments as known/unknown classes during mapping. In open-set domain adaptation, target samples from unknown classes are modeled with a private subspace tailored to the target domain. Clustering is used to decompose unlabeled target samples into category-agnostic clusters, integrated into Self-Ensembling for closed-set and open-set scenarios. A clustering branch in the student model infers cluster assignment distribution for each target sample. SE-CC utilizes unlabeled target samples for learning task-specific classifiers in the open-set scenario by leveraging category-agnostic clusters for representation learning. The feature representation is enforced to preserve the underlying data structure in the target domain, enabling effective alignment of samples. In open-set domain adaptation, Self-Ensembling with Category-agnostic Clusters (SE-CC) model integrates category-agnostic clusters into the domain adaptation procedure to align sample distributions and enhance representation learning. In open-set domain adaptation, the goal is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown target samples. Self-Ensembling, based on Mean Teacher, encourages consistent learning between a student and teacher model with the same architecture. Self-Ensembling aims to ensure consistent classification predictions between teacher and student models with the same architecture by penalizing differences in classification predictions. The student is trained using gradient descent, while the teacher's weights are updated as an exponential moving average of the student's weights. This approach is inspired by previous work on unsupervised conditional learning. The Self-Ensembling approach involves training the student model with unsupervised conditional entropy loss to drive decision boundaries away from high-density regions in the target domain. The training loss includes supervised cross entropy loss on the source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data. Open-set domain adaptation is challenging as it requires classifying both inliers and outliers into known and unknown classes. Clustering is used to model diverse semantics in the target domain as category-agnostic clusters, integrated into Self-Ensembling for domain adaptation. An additional clustering branch aligns cluster assignment distribution with inherent cluster distribution, enforcing domain-invariant features for known classes and discriminative features for unknown classes. In the target domain, clustering is utilized to group unlabeled data using k-means, revealing underlying structure tailored to the domain. Target samples are represented by output features of pre-trained CNNs for clustering. Periodic cluster refreshing did not significantly impact results. In the target domain, clustering is used to group unlabeled data with k-means. The inherent cluster distribution of each target sample is encoded by measuring cosine similarities with cluster centroids. A clustering branch in the student model predicts the distribution over all clusters for cluster assignment of each target sample. The clustering branch in the student model predicts the cluster assignment distribution for each target sample using a modified softmax layer. It is trained with a KL-divergence loss to minimize the mismatch between the estimated and inherent cluster distributions. The KL-divergence loss enforces the learnt representation to preserve the data structure of the target domain and improve discriminative ability for unknown and known classes. Inter-cluster relationships are incorporated as a constraint to maintain relations among cluster assignment parameter matrices. The loss is further relaxed to enhance the target feature in an unsupervised manner. To strengthen the target feature in an unsupervised manner, Mutual Information Maximization (MIM) is used in a multi-task paradigm. A MIM module in the student model estimates and maximizes local and global mutual information among input features, output classification distribution, and cluster assignment distribution. Global Mutual Information is encoded from the output feature map of the last convolutional layer in the student model. The global feature vector G(x is encoded via a convolutional layer and average pooling. It is then concatenated with classification and cluster assignment distributions before being fed into a Mutual information discriminator for alignment assessment. The discriminator consists of three fully-connected networks with nonlinear activation, outputting a probability score for discriminating real input features. The global Mutual Information is estimated via Jensen-Shannon MI estimator, incorporating softplus function and global feature of a different target image. Local Mutual Information is utilized among local input features and output distributions, spatially replicating and concatenating them for discrimination. The local Mutual Information discriminator is constructed with three stacked convolutional layers and nonlinear activation. The final objective for MIM module combines local and global Mutual Information estimations, balanced with tradeoff parameter \u03b1. The SE-CC training objective integrates cross entropy loss on source data, unsupervised self-ensembling loss, conditional entropy loss, and KL-divergence loss of clustering branch. The SE-CC model is evaluated on the Office Saenko et al. VisDA dataset, which consists of 280k images from three domains: synthetic images from 3D CAD models (training domain), real images from COCO Lin et al. (2014) (validation domain), and video frames from YTBB Real et al. (2017) (testing domain). The model's performance is empirically verified through experiments, with source images from the training domain and target images from the validation domain used for evaluation. For open-set adaptation, known classes are defined for source and target domains, with background and other categories as unknown classes. Evaluation metrics include Knwn, Mean, and Overall accuracy. Closed-set adaptation focuses on accuracy of the 12 known classes. ResNet152 is used as the backbone for CNNs in both scenarios. Performance of different models on Office for open-set adaptation is presented. The performances of different models on Office for open-set adaptation are compared, with SE-CC showing better results than other state-of-the-art models like RTN and RevGrad. SE-CC recognizes N-1 known classes and treats target samples as unknown if the predicted probability is below a certain threshold. Overall, SE-CC outperforms AODA, ATI-\u03bb, and FRODA in most transfer directions. The SE-CC model improves classification accuracy on challenging domain transfers, such as D \u2192 A and W \u2192 A. It leverages category-agnostic clusters for open-set domain adaptation, making the feature representation domain-invariant for known classes while effectively segregating target samples. RTN and RevGrad perform better than Source-only by aligning data distributions, but open-set adaptation techniques (AODA, ATI-\u03bb, and FRODA) outperform them by excluding unknown target samples as outliers. SE-CC outperforms AODA, ATI-\u03bb, FRODA, RTN, and RevGrad in open-set domain adaptation by injecting category-agnostic clusters for feature learning. In closed-set domain adaptation, SE-CC also achieves better performance compared to other state-of-the-art techniques on Office and VisDA datasets. The results show the advantage of using category-agnostic clusters in closed-set domain adaptation without unknown samples. Ablation study reveals how each design in SE-CC influences performance: Conditional Entropy drives classifier boundaries away from high-density data regions, KL-divergence Loss aligns cluster assignment distribution, and Mutual Information Maximization enhances feature suitability for downstream tasks. The SE-CC model improves performance in open-set domain adaptation by maximizing mutual information among input features, output classification, and cluster assignment distributions. Different designs like CE, KL, and MIM contribute to a total performance boost of 4.2% in Mean metric. This approach exploits category-agnostic clusters in the target domain for improved adaptation. The SE-CC model utilizes category-agnostic clusters in target domain for domain adaptation in open-set and closed-set scenarios. It decomposes target samples into clusters and integrates a clustering branch into the student model to align cluster assignment distribution. Mutual information among input features, classification outputs, and clustering branches is used to enhance learned features. Experiments on Office and VisDA datasets show improved performance. The SE-CC model utilizes category-agnostic clusters in target domain for domain adaptation in open-set and closed-set scenarios. Experiments on Office and VisDA datasets verify performance improvements compared to state-of-the-art techniques. The implementation of SE-CC is developed with PyTorch and optimized with SGD. Key details include learning rate, mini-batch size, training iterations, and feature dimensions. Settings for cluster number, tradeoff parameters, and alpha are detailed for both datasets. The study evaluates the performance of the SE-CC model in domain adaptation tasks using category-agnostic clusters. The number of clusters is determined using the Gap statistics method, and hyperparameters are tuned within specific ranges. The use of KL-divergence in the loss function is compared to L1 and L2 distance, showing superior performance. Different variants of Mutual Information Maximization are also evaluated. The study evaluates the performance of the SE-CC model in domain adaptation tasks using category-agnostic clusters. Different variants of Mutual Information Maximization are evaluated, showing improvements in performance by exploiting mutual information between input features and outputs of classification and clustering branches. The SE-CC model improves domain adaptation by preserving the target data structure for known and unknown classes, separating unknown target samples from known ones. This allows for domain-invariant representation while maintaining distinguishability between known samples in different domains."
}