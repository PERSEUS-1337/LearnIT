{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified in this paper by explaining why searching in its negative direction minimizes the training loss. The STE-modified chain rule provides a coarse gradient for learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. The proper choice of STE is crucial for this process. The proper choice of straight-through estimator (STE) is crucial for minimizing the training loss in activation quantized neural networks. A well-chosen STE ensures that the expected coarse gradient aligns with the population gradient and leads to convergence to a critical point of the population loss minimization problem. However, a poor choice of STE can result in instability near local minima, as demonstrated in CIFAR-10 experiments. Deep neural networks (DNN) have been successful in various machine learning applications, but their deployment typically requires significant memory storage. The deployment of DNN requires significant memory storage for full-precision parameters and floating-point operations. Recent efforts focus on training coarsely quantized DNN for memory savings and energy efficiency at inference time. Weight quantization of DNN is a challenging optimization problem with a discrete set-constraint. Training activation quantized DNNs involves dealing with the issue of gradient almost everywhere zero, making standard back-propagation inapplicable. To address this, a non-trivial search direction is constructed by modifying the chain rule, using the straight-through estimator (STE) as a proxy derivative. Alternative approaches include stochastic neurons and the feasible target propagation algorithm for learning hard-threshold networks. The straight-through estimator (STE) is a propagation algorithm for learning hard-threshold networks via convex combinatorial optimization. It originates from the perceptron algorithm in the 1950s and has been extended to train multi-layer networks with binary activations. Bengio et al. (2013) proposed a variant of STE using the derivative of the sigmoid function. The straight-through estimator (STE) was proposed by Bengio et al. (2013) as a variant using the derivative of the sigmoid function. Hubara et al. (2016) introduced the saturated STE for training DNN with weights and activations constrained to \u00b11. Later, STE was applied to DNN training with quantized ReLU activations by various researchers. Despite empirical success, there is limited theoretical understanding of STE in training DNN with stair-case activations. Goel et al. (2018) demonstrated the convergence of the Convertron algorithm using the identity STE with leaky ReLU activation. The backward pass in DNN training involves using the straight-through estimator (STE) with leaky ReLU activation, as proposed by Goel et al. (2018). Other approaches, such as implicit weighted nonlocal Laplacian layers and backward pass differentiable approximation, have also been introduced to improve generalization accuracy and break adversarial defenses. These methods modify the chain rule to calculate a coarse gradient for weight variables. The backward pass in DNN training involves using the straight-through estimator (STE) with leaky ReLU activation. The coarse gradient is not the gradient of the loss function and searching in its negative direction minimizes training loss. The choice of STE is non-unique, and understanding STE in training quantized ReLU nets is important. Different STEs are considered for learning a two-linear-layer network with binary activation and Gaussian data. The model of population loss minimization is adopted, and it is proven for the first time that proper STE selection is crucial. The proper choice of straight-through estimator (STE) in training algorithms is crucial for descent directions and stable convergence. Empirical results show that clipped ReLU STE performs best on deeper networks like VGG-11 and ResNet-20, while vanilla and clipped ReLUs work well on shallower networks like LeNet-5. In CIFAR experiments, using identity or ReLU STE in training can lead to instability and inferior minima with higher loss and lower accuracy. Poor STEs produce coarse gradients not compatible with the energy landscape, contradicting convergence guarantees of perceptron and Convertron algorithms. The identity STE is not suitable for networks with two trainable layers, and it's unclear if other STE analyses can be extended. In CIFAR experiments, using identity or ReLU STE in training can lead to instability and inferior minima with higher loss and lower accuracy. Poor STEs produce coarse gradients not compatible with the energy landscape, contradicting convergence guarantees of perceptron and Convertron algorithms. The analyses of other STEs may not be extendable to different scenarios. The quantized activation function's monotonicity is crucial in coarse gradient descent, with clipped ReLU matching quantized ReLU at the extrema to avoid instability issues. The energy landscape of a two-linear-layer network with binary activation and Gaussian data is studied in section 2, with main results and mathematical analysis for STE presented in section 3. Empirical performances of different STEs in 2-bit and 4-bit activation quantization are compared in section 4, highlighting instability phenomena associated with poor STEs in CIFAR experiments. Technical proofs and figures are deferred to the appendix due to space limitations. In the appendix, technical proofs and figures are deferred. Notations include definitions for Euclidean norm, spectral norm, zero vector, one vector, identity matrix, inner product, and Hadamard product. A model similar to (Du et al., 2018) is considered, with trainable weights w and v, and an activation function \u03c3 acting on the input Z. The first layer functions as a convolutional layer, while the second layer is linear. The weight filter w is shared among all patches, and the label is generated using a binary function. Entries of Z are sampled from a Gaussian distribution. The learning task is framed as a population loss minimization problem. Analytic expressions for the objective function and its gradient can be found with the Gaussian assumption on Z. The idea of Substituted Target Estimation (STE) is to replace the zero component \u03c3 in the gradient with a related non-trivial function \u00b5. This approach allows for training a two-linear-layer convolutional neural network (CNN) with binary activation using a coarse gradient descent method. Binary activation enables coarse gradient descent for learning two-linear-layer CNN with STE \u00b5. Preliminaries on the landscape of the population loss function f (v, w) are presented, defining the angle between w and w * as \u03b8(w, w * ). Analytic expressions of f (v, w) and \u2207f (v, w) are elaborated. Population loss f (v, w) is given by Lemma 1 when w = 0 n. Lemma 2 provides partial gradients of f (v, w) w.r.t. v and w when w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0). Local minimizers of the model are discussed, emphasizing stationary points where gradients vanish simultaneously. The text discusses the landscape of the population loss function in the context of learning two-linear-layer CNN with binary activation. It highlights the presence of stationary points, saddle points, and spurious local minimizers in the model, emphasizing the Lipschitz continuity of the population gradient within bounded domains. The text discusses the landscape of the population loss function in the context of learning two-linear-layer CNN with binary activation, focusing on the presence of stationary points, saddle points, and spurious local minimizers. The main results concern the behaviors of the coarse gradient descent algorithm using derivatives of vanilla and clipped ReLUs, proving convergence to critical points with ReLU derivatives but not with the identity function. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples. With a small amount of data, the empirical loss descends roughly along the negative coarse gradient direction. As sample size increases, the loss gains monotonicity and smoothness, explaining the effectiveness of STE with large datasets in deep learning. The same results apply even if the Gaussian assumption on input data is relaxed to follow a rotation-invariant distribution. In the mathematical analysis for the main results, the expected coarse gradient with non-negative correlation is crucial for minimizing the population loss. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples. The text discusses the significance of the expected coarse gradient with non-negative correlation in minimizing population loss. It highlights the behavior of coarse gradient descent and gradient descent on a specific function, emphasizing the importance of a certain estimate for guaranteeing descent property. The convergence of Algorithm 1 is linked to the vanishing of certain terms, indicating saddle points. The text discusses the convergence of Algorithm 1 linked to the vanishing of terms at saddle points. Lemma 6 states that ReLU STE converges to critical points of the population loss function. Using clipped ReLU, the coarse gradient generally correlates positively with the true gradient and vanishes at critical points (Lemma 7 and 8). Lemma 9 to 10 discuss the behavior of coarse gradient descent at local minimizers and the relationship between expected coarse and true gradients. The coarse gradient may not vanish at local minima, impacting Algorithm 1 convergence. Lemma 9 suggests that coarse gradient descent may not converge near spurious minimizers with certain conditions. The performance of vanilla and clipped ReLUs on deeper nets differs, as shown in comparisons on MNIST and CIFAR-10 benchmarks with quantized activations. The 2-bit quantized ReLU and its clipped version are compared in Figure 3. The training algorithm instability due to improper STE is discussed. The resolution \u03b1 for quantized ReLU must be carefully chosen for accuracy. A modified batch normalization layer is used to determine the best \u03b1, which is then fixed during training. The resolution \u03b1 for quantized ReLU is carefully chosen and fixed during training. Batch normalization is added prior to each activation layer. The optimizer used is stochastic gradient descent with momentum = 0.9. Training epochs vary for different models on MNIST and CIFAR-10 datasets. Experimental results are summarized in Table 1, showing training losses and validation accuracies. The experimental results show that the derivative of clipped ReLU performs best, followed by vanilla ReLU and then the identity function. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU shows comparable performance on shallow networks like LeNet-5. The identity function leads to instability issues, with significantly lower validation accuracies compared to ReLU variants. The experimental results show that the derivative of clipped ReLU performs best, followed by vanilla ReLU and then the identity function. Training with the identity STE leads to instability and worse minima, while ReLU STE also shows poor performance due to instability at good minima. ResNet-20 training algorithm instability at good minima shown in Figure 4. Coarse gradient descent using identity STE repelled from good minima. Theoretical justification for STE concept provided. Derivatives of identity function, vanilla ReLU, and clipped ReLU considered for learning CNN with binary activation. Negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing population loss. The identity STE is not a descent direction for minimizing population loss, as it generates a coarse gradient incompatible with the energy landscape. CIFAR experiments confirmed instability issues with improper STE choices. Future work aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. Lemma 11 proves three identities, with the first one showing E{zw>0} = P(z1>0) = 1/2. The last identity involves the polar representation of Gaussian random variables. Lemma 12 states properties of Gaussian random vectors with nonzero vectors w,w. Lemma 13 involves an inequality. Lemma 14 presents a proof involving the Cauchy-Schwarz inequality and the fact sin(x) \u2265 2. It also discusses the projection of vectors onto complement spaces and the angles between them. Lemma 14 proves the Cauchy-Schwarz inequality and properties of vector projections and angles. Lemma 2 discusses partial gradients and saddle points in optimization. The local optimality of stationary points is also examined. The Hessian matrix of the objective function is indefinite, leading to saddle points as stationary points. Perturbed objective values show that small non-zero changes result in increased objective values. Additionally, Lipschitz constants are discussed for differentiable points. The expected partial gradient of (v, w; Z) w.r.t. v is determined by certain inequalities and Lemmas. The expected coarse gradient w.r.t. w is also discussed, along with the inner product between expected coarse and true gradients w.r.t. w under specific conditions. Lemma 5 states that for certain inequalities v \u2264 Cv and w \u2265 cw, there exists a constant Arelu > 0. Lemma 6 discusses conditions where specific expressions are satisfied, and Lemma 7 provides further details on certain scenarios. If w = 0 and \u03b8(w, w*) \u2208 (0, \u03c0), then DISPLAYFORM3 where DISPLAYFORM4 * same as in Lemma 5, and DISPLAYFORM5 2)dr. The inner product between the expected coarse and true gradients w.r.t. w DISPLAYFORM6 Moreover, if v \u2264 Cv and w \u2265 cw, there exists a constant Arelu > 0 depending on Cv and cw, such that DISPLAYFORM7 Proof of Lemma 7. Denote \u03b8 := \u03b8(w, w*). We first compute EZ grelu(v, w; Z). By (5), DISPLAYFORM8 Since \u00b5 = 1 {0<x<1} and \u03c3 = 1 {x>0}, we have In the last equality above, we called Lemma 12. DISPLAYFORM9 Notice that I n \u2212 ww w 2 w = 0 and w* = 1. If \u03b8(w, w*) = 0, \u03c0, then the inner product between EZ grelu(v, w; Z) and Combining the above estimate together with FORMULA2, FORMULA2 and FORMULA2, and using Cauchy-Schwarz inequality, we have DISPLAYFORM10 where p(0, w) and q(\u03b8, w) are uniformly bounded. This completes the proof. Proof of Lemma 8. The proof of Lemma 8 is similar to that of Lemma 6, and we omit it here. The core part is that The proof of Lemma 8 is similar to Lemma 6, with q(\u03b8, w) non-negative and p(0, w) \u2265 p(\u03b8, w) \u2265 p(\u03c0, w) = 0. Lemma 9 states that the expected coarse partial gradient w.r.t. w is \u00b5(x) = x. Lemma 10 shows the inner product between expected coarse and true gradients w.r.t. w if w = 0 and \u03b8(w, w*) \u2208 (0, \u03c0)."
}