{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs involves using the Wasserstein-2 metric proximal on the generators. The approach utilizes the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experimental results show improved speed and stability in training GANs in terms of wall-clock time and FID learning curves. The generator in GANs aims to recreate the density distribution of a noise source by minimizing a discrepancy measure. The Wasserstein distance, or Earth Mover's distance, is used as an alternative to the KL divergence for defining this measure. Optimal transport not only defines the loss function but also introduces structures for optimization, such as the gradient operator. This approach has shown improved speed and stability in training GANs. In this paper, the Wasserstein steepest descent flow for deep generative models in GANs is derived using the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. The Fisher-Rao natural gradient, induced by the KL divergence, is often advantageous in learning problems but problematic in GANs due to low dimensional support sets. Instead, the gradient operator induced by the Wasserstein-2 metric is proposed for GANs. The proximal operator for the generators of GANs is computed as well. In this paper, the Wasserstein steepest descent flow for deep generative models in GANs is derived using the Wasserstein-2 metric. The proximal operator for the generators of GANs is computed, where the regularization is the squared constrained Wasserstein-2 distance. The constrained distance can be approximated by a simple neural network. The constrained Wasserstein-2 metric exhibits a simple structure in implicit generative models. The relaxed proximal operator for generators is introduced to simplify computation, involving only the difference of outputs for parameter updates. This method can be easily implemented as a drop-in regularizer for generator updates. In Section 3, the effectiveness of proposed methods in experiments with various GANs is demonstrated. Optimal transport and its proximal operator on a parameter space are briefly presented, applied to the optimization problems of GANs. Optimal transportation defines distance functions between probability densities, with a focus on the Wasserstein-2 distance. This distance has a dynamical formulation as a trajectory transporting initial density to final density with minimal kinetic energy. The classic theory of optimal transport is extended to cover parameterized density models, where the density path is constrained within a parametrized model. The constrained Wasserstein-2 metric function is defined on the parameter space \u0398, with a formulation involving feasible Borel potential functions and continuous parameter paths. This constrained metric on the parameter space may differ from the Wasserstein-2 distance on the full density. The constrained Wasserstein-2 metric on the parameter space \u0398 allows for a steepest descent optimization scheme, defined through a Riemannian structure. The Wasserstein natural gradient is obtained from this metric, providing a gradient operator for a given loss function F. The steepest descent flow in the constrained Wasserstein metric is defined by the natural gradient operator. The gradient descent iteration uses the forward Euler method with a step size h. The proximal operator, known as the Jordan-Kinderlehrer-Otto scheme, provides an alternative numerical scheme. The parameter update distance is regularized by the distance of the Wasserstein metric. The d W distance can be approximated locally using a second order Taylor expansion. The Semi-Backward Euler method for the gradient flow of loss function F : \u0398 \u2192 R is a simpler alternative to the forward Euler method, as it avoids the need to compute and invert G(\u03b8). It is implemented in implicit generative models by defining the generator for each parameter \u03b8 \u2208 R d. The generator in implicit generative models maps noise prior Z to output samples with density X, defined by parameters \u03b8 \u2208 R d. The constrained Wasserstein-2 metric simplifies the formulation, introducing a relaxed Wasserstein metric and a proximal operator algorithm on generators. The constrained Wasserstein metric requires the generator's derivative w.r.t. parameter paths to be a gradient vector field of another function. When the sample space is 1 dimensional, the gradient constraint is satisfied, but in general, this is not the case. Fitting the gradient constraint is a computational challenge, leading to the consideration of a relaxed Wasserstein metric on the parameter space. The relaxed Wasserstein proximal operator is approximated based on a new metric, with the infimum taken among all feasible continuous parameter paths. When the sample space is high dimensional, the update is a regularization of the generator by the expectation of squared differences. The Relaxed Wasserstein Proximal operator is used in GANs with a toy example illustrating its effectiveness. The operator minimizes a parameterized function and requires specific step sizes and batch sizes for optimization. The proximal regularization for a loss function F : \u0398 \u2192 R is defined for parameters \u03b8 = (a, b). Statistical distance functions like Wasserstein-2 and Euclidean distances are commonly used. The Euclidean distance is independent of the model structure, while the Wasserstein-2 metric depends on it. The Wasserstein proximal solution decreases the objective function more effectively than the Euclidean solution in each update step. The Relaxed Wasserstein Proximal (RWP) algorithm outperforms Euclidean proximal in decreasing the objective function, especially for Wasserstein-1 loss. It offers faster speed and stability in training GANs, making it a valuable tool for improving convergence and efficiency in GAN training. The Relaxed Wasserstein Proximal (RWP) algorithm introduces modifications to the generator update rule in GAN training, focusing on regularization techniques. It utilizes hyperparameters such as the proximal step-size and the number of iterations to improve training efficiency. The algorithm is tested on various GAN types using datasets like CIFAR-10 and CelebA, with the DCGAN architecture for the discriminator. The study utilizes the DCGAN architecture on the CelebA dataset and measures performance using the Fr\u00e9chet Inception Distance (FID). The Relaxed Wasserstein Proximal regularization improves training speed and stability, with hyperparameters detailed in Appendix C. Comparisons are aligned based on wallclock time due to multiple generator iterations with RWP. Our regularization improves convergence speed and FID for all GAN types, with a 20% improvement in sample quality for DRAGAN. Results are consistent on the CelebA dataset as well. Multiple generator iterations may cause initial learning issues for Standard GANs on CelebA, but can be easily detected and resolved. The defect may be addressed with a more stable loss function like WGAN-GP or different hyperparameters. The impact of multiple generator updates versus discriminator updates is also examined. The regularization technique improves convergence speed and FID for all GAN types, with a 20% increase in sample quality for DRAGAN. Multiple generator updates in RWP show improved stability and lower FID compared to omitting regularization. Latent space walks demonstrate that RWP does not cause the GAN to memorize. The use of RWP regularization enhances speed and achieves a lower FID, as shown in the provided graphs. Regularization improves speed and achieves lower FID in GAN training. Multiple generator iterations may initially fail but eventually succeed. An experiment with 10 generator iterations per outer-iteration shows convergence and lower FID with RWP regularization. Without RWP, training is highly variable and FID increases. The Semi-Backward Euler method on CIFAR-10 dataset shows comparable results to standard training with WGAN-GP loss. The experiment involved training using the WGAN-GP loss, averaged over 5 runs. The Semi-Backward Euler method was used to approximate three functions: discriminator, generator, and potential function \u03a6 p. Results show that Semi-Backward Euler is comparable to norm WGAN-GP. Further investigation is needed for the Semi-Backward Euler method in the future. Many studies apply the Wasserstein distance as the loss function in optimal transport for machine learning and GANs. In optimal transport for machine learning and GANs, the Wasserstein distance is commonly used as the loss function. It introduces a statistical estimator based on the geometry of the data and is useful for comparing probability distributions on lower dimensional sets. The Wasserstein-1 distance function is chosen as the loss function in Wasserstein GAN, where the discriminator must satisfy the 1-Lipschitz condition. Regularization techniques are employed to meet this condition. Regularization techniques are used to satisfy the 1-Lipschitz condition of the discriminator in optimal transport for machine learning and GANs. The Wasserstein-2 metric provides a metric tensor structure in the probability space, forming an infinite dimensional Riemannian manifold called the density manifold. The gradient flow in the density manifold is linked to transport-related partial differential equations, such as the Fokker-Planck equation. Learning communities explore leveraging the gradient flow structure in probability space and studying stochastic gradient descent using transition equations in probability over parameters. Nonparametric models are also studied in this context. In this work, the constrained Wasserstein gradient is applied to general implicit generative models, focusing on Gaussian families and elliptical distributions. Previous studies have looked at the Wasserstein gradient flow in Gaussian families, while this approach introduces an approximation towards Kantorovich dual variables and constraints on parameter space. In this work, the constrained Wasserstein gradient is applied to implicit generative models, focusing on regularizing the generator for better convergence speeds. The method computes the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space, introducing a Riemannian structure in density space. The text discusses the tangent space of smooth and strictly positive probability densities, the elliptic operator identifying functions modulo constants, and the Wasserstein gradient operator in Riemannian metric tensor space. It also mentions the gradient flow and analytical results on the Wasserstein-2 gradient flow. The Wasserstein-2 metric and gradient operator are constrained on statistical models defined by a triplet (\u0398, R n , \u03c1). The parameterization function \u03c1 is assumed to be locally injective, and a Riemannian metric g is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The constrained Wasserstein gradient operator in parameter space is studied in Theorem 2, where the distance can be expressed as an action. The distance in parameter space can be expressed as an action function in the Wasserstein statistical manifold. The gradient operator on a Riemannian manifold is defined, and the derivation of the proposed semi-backward method is presented. The claim is then proven by reparameterizing the geodesic path. Equation 6 and 7 are subsequently proved using the maximizer \u03a6*. The Semi-backward method is derived as a consistent numerical method in time. Proposition 4 is proven, and the implicit model with the push-forward relation is presented. The probability density transition equation satisfies the constrained continuity equation, involving gradient and divergence operators. The Semi-backward method is derived as a consistent numerical method in time. Proposition 4 is proven, and the implicit model with the push-forward relation is presented. The probability density transition equation satisfies the constrained continuity equation, involving gradient and divergence operators. DISPLAYFORM16 where \u2207, \u2207\u00b7 are gradient and divergence operators w.r.t. x \u2208 R n. The second to last equality holds from the push forward relation equation 9, and the last equality holds using the integration by parts w.r.t. x. We prove equation 10 by showing equation 12 equals equation 13 for any f \u2208 C \u221e c (R n). Additionally, we calculate the Wasserstein and Euclidean proximal operators explicitly. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments include a batch size of 64 for all experiments. Specific settings for CIFAR-10 with different optimizers and latent space dimensions are provided. The method is designed to be easy to implement. The Relaxed Wasserstein Proximal is an easy-to-implement regularization method showcased on Standard GANs. The algorithm involves updating the discriminator, performing Adam gradient descent, and repeating until a stopping condition is met. The key differences from standard GAN training are the terms involving generator iterations. In this paper, the Relaxed Wasserstein Proximal regularization method is demonstrated on Standard GANs. The FID scores for images generated from Standard GAN with RWP on CelebA dataset and WGAN-GP with RWP on CIFAR-10 dataset are 17.105 and 38.3 respectively. Latent space walk analysis suggests that GANs with RWP regularization do not memorize. Specific hyperparameters for SBE on WGAN-GP include a batch size of 64 and DCGAN architecture for discriminator and generator. The study utilized a one-hidden-layer fully connected network (MLP) with layer normalization for the potential \u03a6p. The Adam optimizer with specific parameters was used for the generator, discriminator, and potential. The latent space dimension was set to 100, and updates were made to the discriminator, generator, and potential in each outer-iteration loop."
}