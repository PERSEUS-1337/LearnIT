{
    "title": "rygVV205KQ",
    "content": "In this work, imitation learning is used to tackle challenges in high-dimensional sparse reward tasks for reinforcement learning agents. Adversarial imitation is shown to be effective in learning a representation of the world from pixels and exploring efficiently with rare reward signals. The adversary, acting as the reward function, can be small with just 128 parameters and trained using a basic GAN formulation. This approach overcomes limitations of other imitation methods, requiring only video demonstrations and sparse rewards to solve complex tasks like robot manipulation. Our agent learns quickly from video demonstrations and sparse rewards, outperforming competitors using dense reward functions. A new adversarial goal recognizer enables learning without task rewards, relying on imitation. GAIL can handle high-dimensional pixel observations with a single layer discriminator network, improving efficiency in environment interactions. The Deep Distributed Deterministic Policy Gradients (D4PG) agent efficiently interacts with the environment using a replay buffer. Different types of features, including self-supervised embeddings and value network features, are successfully utilized. The modified GAIL for off-policy D4PG agents with experience replay is demonstrated. The proposed approach can solve a challenging robotic block stacking task from pixels with only demonstrations and sparse rewards. The curr_chunk discusses a 6-DoF Jaco robot arm agent learning block stacking from demonstration videos and sparse rewards, achieving a 94% success rate. It introduces an adversary-based early termination method for actor processes to improve task performance and learning speed. The agent learns without task rewards using an auxiliary goal recognizer adversary. The agent learns without task rewards using an auxiliary goal recognizer adversary, achieving 55% stacking success from video imitation only. Ablation experiments on Jaco stacking and a 2D planar walker benchmark BID33 are conducted to understand the reasons for improvement in the agent. Random projections with a linear discriminator and using value network features are found to work well. The goal of training an agent is to find a policy that maximizes the expected sum of discounted rewards. DDPG is an actor-critic method where neural networks represent the actor (policy) and critic (action-value function). New transitions are added to a replay buffer for better exploration. The action-value function is trained to match 1-step returns by minimizing the transition sampled from the replay buffer. Target networks are updated every K learning steps for stability. The policy network is trained to produce actions that maximize the action-value function. The network is trained via gradient descent to maximize the action-value function using the deterministic policy gradient. Building on DDPG, D4PG leverages improvements and uses off-policy training with experience replay. GAIL learns a reward function by training a discriminator network to distinguish between agent and expert transitions. The reward function is jointly trained with the actor and critic updates in D4PG. The reward function in D4PG interpolates imitation reward and sparse task reward, using a modified equation to distinguish expert transitions. The discriminator does not use actions, only access to expert videos. The reward is bounded between 0 and 1 for early termination of episodes based on discriminator score. The actor process includes early termination based on discriminator score to prevent drifting from expert trajectories. Multiple CPU actor processes run in parallel with a single GPU learner process. The discriminator network's capacity is a critical design choice for the reward function. The discriminator network's capacity is crucial for the reward function used in feature learning from expert demonstrations. High-dimensional observations can make it easy for the network to distinguish between agent and expert, while low capacity may hinder the agent's learning process. Expert demonstrations provide valuable data for feature learning due to their coverage of state space regions necessary for task-solving. Access to expert actions is not assumed, ruling out behavior cloning as an option. Contrastive predictive coding (CPC) is a representation learning technique that maps observations into a latent space for long-term predictions without the need for a decoder model. It uses a probabilistic contrastive loss with negative sampling, allowing joint training of the encoder and autoregressive model. Contrastive predictive coding (CPC) is a representation learning technique that uses a neural network goal recognizer to replace sparse rewards in agent training. This helps overcome issues with blind spots in the goal recognizer and allows for the detection of goal states based on expert trajectories. The modified reward function includes a discriminator to determine if the agent has reached a goal state, defined as a state in the latter 1/M proportion of the expert demonstration. In our experiments with M = 3, the modified reward function includes a secondary goal discriminator network, D goal, which operates on the same feature space as D. Training D goal involves sampling expert states from the latter 1/M portion of each demonstration trajectory. By training a second discriminator to recognize goal states, agents can surpass the demonstrator by learning to reach the goal faster. The environments include a Kinova Jaco arm with nine degrees of freedom and two blocks on a tabletop. The Kinova Jaco arm has 9 degrees of freedom, controlled by policies setting joint velocity commands. Observations are 128x128 RGB images. Demonstrations were collected using a SpaceNavigator 3D motion controller, with 500 episodes per task. Additional trajectories were gathered for validation and diagnostics purposes. The second environment involves a 2D walker from the DeepMind control suite BID33. Demonstrations were collected using a D4PG agent trained from proprioceptive states to match a target velocity. The imitation method is compared to D4PG and GAIL agents on dense and sparse rewards, showing favorable results. The proposed method using a tiny adversary performs well in modeling future observations for both expert and non-expert trajectories. The proposed method using a tiny adversary performs well in modeling future observations for both expert and non-expert trajectories. Conditioning on k-step predictions improves performance on stacking tasks. D4PG with sparse rewards struggles due to exploration complexity, while imitation methods excel despite sparse rewards. Agent using value network features learns quickly, reaching comparable performance with CPC features. GAIL from pixels performs poorly, while GAIL with tiny adversaries on random projections has limited success. Discriminator network with CPC features has only 128 parameters. The discriminator network with CPC features has only 128 parameters, while the value network features are 2048-dimensional. Norm clipping applied in the critic optimizer may explain why GAIL value features worked while pixel features did not. Using CPC features for temporal predictions did not result in success in Jaco or Walker2D tasks. In ablation experiments on Jaco stacking, adding layers to the discriminator network did not improve performance, and not doing early termination hurt performance. Even with 60 demonstrations, the agent can learn stacking as well as with 500. The experiment aimed to determine if a small discriminator is optimal for imitation learning or if a deeper network can improve results. The performance actually degrades as the discriminator becomes more powerful, indicating the advantage of a small discriminator on meaningful features. In ablation experiments on Jaco stacking, adding layers to the discriminator network did not improve performance. An early termination criterion was introduced to stop episodes when the discriminator score becomes too low, resulting in faster learning. The average episode length during training showed that the agent improved at imitating the expert over time. Data efficiency of the proposed method was evaluated in terms of expert imitation. In the third ablation experiment, the data efficiency of the proposed method is evaluated in terms of expert demonstrations. Results show that even with 60 demonstrations, good performance is achieved. An outlier was observed with 120 demos. Results on the planar walker indicate that the proposed method using value network features and random projections outperformed conventional GAIL methods. Training agents without rewards using expert states as positive examples was also successful. The curr_chunk discusses the success of training an agent without providing any task reward, achieving a 55% success rate. It also highlights the efficiency of the agent in stacking compared to a human teloperator. Leveraging expert demonstrations to improve agent performance is mentioned, with a comparison to previous work in robotics. Imitation learning is a popular approach in deep learning for training agents using expert demonstrations. This method, also known as behavioral cloning, involves replicating desired behaviors from single demonstrations via an encoder network and a state-to-action decoder with an attention mechanism. This approach has been successful in extending the capabilities of deep learning beyond discriminative tasks in computer vision. Our approach differs from behavioral cloning by focusing on the agent learning through interaction with the environment rather than supervised learning. Behavioral cloning requires a large number of demonstrations and limits generalization if the agent encounters states not seen in expert trajectories. It also relies on access to demonstrator actions, which may not always be available. Instead of behavior cloning, BID38 BID25 ; BID0 propose inverse reinforcement learning (IRL) to learn a reward function from demonstrations and optimize it using reinforcement learning. DQfD and DPGfD methods involve adding expert trajectories to train agents along with their own experiences, showing success in tasks like peg insertion without dense rewards. Our contribution involves using minimal adversaries on learned features to solve sparse reward tasks with high-dimensional input spaces, complementing existing methods like Generative Adversarial Networks. Other approaches focus on learning compact representations for imitation learning from expert observations without actions. Our approach involves utilizing features from third person observations to bridge the gap between first and third person views. We use these features to learn tasks from multiple expert trajectories through GAIL, aiming to generalize various initializations of a challenging exploration task. By incorporating static self-supervised features like contrastive predictive coding and dynamic value network features, we successfully train block stacking agents from sparse rewards on pixels. Videos of our trained agents can be found at the provided website. The behavior cloning model includes a residual network pixel encoder architecture, LSTM, and linear layers. The stacking accuracy is around 15%. The CPC model consists of an encoder and autoregressive model optimizing the same loss function. Negative samples are drawn from other examples or timesteps in the minibatch for training. The weights W k for the bilinear mapping z t+k W k c t are learned and depend on the number of latent steps predicting in the future. By optimizing L CPC, the mutual information between z t+k and c t is maximized, resulting in common variables being embedded into compact representations. This is useful for extracting slow features, especially when z t+k and c t are far apart in time. The approach involves model learning via contrastive predictive coding (CPC) and training the agent using CPC future predictions. Reward functions are modified from BID37, and each episode lasts 500 time steps without early stopping. The curr_chunk discusses the use of early stopping in a reinforcement learning task with defined stages and rewards. Actor and critic networks share a residual network with convolutional layers and fully connected layers. Distributional Q is adopted instead of a scalar state-action value function. The curr_chunk introduces Distributional Q functions in reinforcement learning, using a categorical representation of random variable Z. Bootstrap targets are computed with N-step returns, and a categorical projection \u03a6 is adopted for training distributional value functions. Distributional Q functions in reinforcement learning use a categorical representation of random variable Z. Bootstrap targets are computed with N-step returns, and a categorical projection \u03a6 is adopted for training distributional value functions. The cross entropy is represented by H in the equation (\u03b8) = E (st,at,{rt,\u00b7\u00b7\u00b7 ,r t+N \u22121 },s t+N )\u223cB [H(\u03a6(Z ), Z(s t , a t |\u03b8))]. Distributed prioritized experience replay BID29 is used to increase stability and learning efficiency."
}