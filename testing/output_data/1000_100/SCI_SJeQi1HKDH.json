{
    "title": "SJeQi1HKDH",
    "content": "In this work, the social influence is integrated into reinforcement learning to enable agents to learn from both the environment and peers. The Interior Policy Differentiation (IPD) algorithm encourages agents to develop unique policies while solving tasks, resulting in improved performance and diverse behaviors. This approach is inspired by cognition and animal studies. Reinforcement Learning (RL) involves learning through interaction with the environment to maximize rewards. Biodiversity and skill development are essential for species evolution. Behavioral diversity in RL is a growing area of interest. Two approaches to promote diversity are designing rich environments and motivating agents. In this work, the concept of policy differentiation in RL is addressed to improve agent diversity while maintaining task-solving abilities. Inspired by social influence in animal societies, the learning scheme involves agents maximizing rewards while differentiating their actions from others. The curr_chunk discusses implementing social influence in reinforcement learning through social uniqueness motivation and policy differentiation. It introduces a policy distance metric to compare agent similarity and an optimization constraint for immediate feedback. Interior Policy Differentiation (IPD) is proposed to encourage agents to excel in tasks while being different from others. The curr_chunk introduces a method for constrained policy optimization in reinforcement learning. It benchmarks the method on locomotion tasks and shows its effectiveness in learning diverse policies. Various intrinsic motivation methods like VIME and curiosity-driven methods are discussed for encouraging exploration in RL algorithms. The curr_chunk discusses different approaches to quantify intrinsic rewards in reinforcement learning, such as Random Network Distillation (RND) and Competitive Experience Replay (CER). These methods aim to balance external rewards from environments with intrinsic rewards provided by heuristics. The Task-Novelty Bisector (TNB) learning method aims to optimize extrinsic and intrinsic rewards by updating the policy in the direction of the angular bisector of the two gradients. However, the foundation of joint optimization is not solid, requiring additional neural networks and computation expenses. Heess et al. (2017) introduce the Distributed Proximal Policy Optimization (DPPO) method for agents to learn complex skills in diverse environments. The research explores how agents with simulated bodies can learn complex locomotion skills in challenging environments. Different reinforcement learning algorithms may converge to different policies for the same task, with policy gradient algorithms tending to converge to the same local optimum while off-policy and value-based algorithms learn sophisticated strategies. The focus is on learning different policies through a single algorithm and avoiding local optima. The research focuses on learning algorithms that avoid local optima and maintain model uncertainty using an ensemble of deep neural networks. A metric is defined to measure policy differences, which is essential for the proposed algorithm. The learned policies are denoted as \u03b8i, with \u0398 representing the parameter space. A metric space is defined with properties like identity, symmetry, and triangle inequality. The Total Variance Divergence is used to measure policy distance in a metric space on parameter space \u0398. The goal is to maximize the uniqueness of a new policy by calculating the divergence between policies using Monte Carlo estimation. To improve sample efficiency in continuous state cases, we propose approximating the domain of possible states between different policies. This approximation allows us to use \u03c1(s|s \u223c \u03b8) as our choice of \u03c1(s), ensuring properties in Definition 1 still hold. In practice, Condition 1 always holds by adding noise on \u03b8. Sampling s from S \u03b8 \u222a S \u03b8j satisfies Definition 1. The objective function of policy differentiation involves exploration in training. Proposition 1 states unbiased estimation of \u03c1 \u03b8 (s) using a single trajectory \u03c4. The next step after ensuring Condition 1 holds by adding noise on \u03b8 is to develop an efficient learning algorithm that considers both reward from the primal task and policy uniqueness. Previous approaches directly combine these rewards with a weight parameter \u03b1, which requires careful selection. The objective is to optimize a weight parameter \u03b1 for intrinsic reward r int in a constrained optimization problem, considering the importance of policy uniqueness. The text discusses the optimization of the weight parameter \u03b1 for intrinsic reward r int in a constrained optimization problem. It compares different methods such as Task Novel Bisector (TNB) and Interior Point Methods (IPMs) for solving the problem efficiently. The selection of \u03b1 is crucial in this process. In the proposed RL paradigm, the learning process is influenced by peers, allowing for a more natural way to bound the feasible region. By using previous trained policies to guide new agents and terminating those that step outside the feasible region, valid samples are collected during the training process. During the training process, valid samples are collected inside the feasible region to ensure uniqueness in the new policy. This eliminates the need to balance intrinsic and extrinsic rewards, making the learning process more robust. The approach, named Interior Policy Differentiation (IPD) method, is demonstrated on the MuJoCo environment in OpenAI Gym. In experiments using MuJoCo environments, including Hopper-v3, Walker2d-v3, and HalfCheetah-v3, different policies can be generated by selecting different random seeds. The study focuses on benchmarking the uniqueness generated from stochasticity in training processes of vanilla RL algorithms and random weight initialization, primarily using PPO. Comparison with TNB and WSR approaches is also conducted. The study compares the uniqueness generated from stochasticity in training processes of vanilla RL algorithms and random weight initialization, primarily using PPO. Implementation details of WSR, TNB, and a new method are provided. The new method aims to train policies sequentially to be unique from previously trained policies. Visual results of the new method are shown in Fig.2. The visualization method displays agents' poses over time, highlighting acceleration patterns. Experimental results show the uniqueness and performance of different policies. The proposed method outperforms others in Hopper and HalfCheetah environments. Walker2d shows improvements in uniqueness but not in performance compared to PPO. In Table 1, a comparison of task-related rewards using three methods to surpass PPO's performance is detailed. Figures 5 and 6 in Appendix C depict the performance and reward curves of each trained policy, while Figure 7 in Appendix C provides more detailed results on uniqueness. Success rate is also used as a metric to compare approaches, with success defined as surpassing the averaged final performance of PPO. Our method consistently outperforms the PPO baseline in terms of success rate, ensuring improved performance in environments like Hopper and HalfCheetah. It prevents policies from getting stuck in local minima by encouraging exploration of different action patterns, leading to noticeable performance enhancements. Our method enhances traditional RL schemes by promoting exploration of different action patterns, leading to improved performance in environments like HalfCheetah. The environment of HalfCheetah lacks explicit termination signals, causing agents to initially act randomly and incur large control costs. Interacting with peers allows agents to receive termination signals, preventing the repetition of trivial samples. Our method enhances traditional RL schemes by promoting exploration of different action patterns, leading to improved performance in environments like HalfCheetah. The agent can receive termination signals from peers to prevent wasting effort acting randomly. The learning process involves the agent first learning to terminate itself quickly to avoid control costs, then learning to behave differently for higher rewards. The difficulty of finding a unique policy increases as the number of policies learned with social influence grows. Performance changes under different scales of social influence are shown in Fig. 4, with a more noticeable decrease in performance in Hopper compared to other environments. In this work, an efficient approach called Interior Policy Differentiation (IPD) is developed to motivate RL to learn diverse strategies inspired by social influence. By defining policy uniqueness and treating the problem as a constrained optimization problem, IPD utilizes Interior Point Methods to help agents avoid local minimum and learn various well-behaved policies. Experimental results show that IPD can be interpreted as implicit curriculum learning and guarantees properties like maximal and minimal policy uniqueness in Hopper. Figure 7 shows the maximal and minimal policy uniqueness in Hopper, Walker2d, and HalfCheetah environments. TNB and WSR can optimize uniqueness directly, sometimes exceeding our method, but this may impact task performance. Hyper-parameter tuning and reward shaping are crucial to balance this trade-off. The calculation of DTV uses deterministic policies without Gaussian noise. MLP with 2 hidden layers is used for actor models in PPO, with the first layer having 32 units. Ablation study on unit numbers in the second layer is detailed in Table 1. In the ablation study, unit numbers in the second layer were analyzed using 10, 64, and 256 hidden units for different tasks. Training timesteps were fixed at 1M for Hopper-v3, 1.6M for Walker2d-v3, and 3M for HalfCheetah. The constraint threshold r0 in the proposed method controls policy uniqueness, with larger thresholds leading to more distinct behaviors. The constraint threshold r0 in the proposed method controls policy uniqueness, with larger thresholds leading to more distinct behaviors. Different choices of threshold values were tested, and the performance of agents under these thresholds is shown in Fig. 9. The cumulative uniqueness is used as constraints instead of forcing every single action of a new agent to be different from others. The cumulative uniqueness is used as constraints in the optimization problem, with different methods like WSR, TNB, and IPD. The Penalty Method handles constraints by incorporating them into a penalty term and solving the unconstrained problem iteratively. The WSR method approximates with a fixed weight term alpha. The WSR method approximates the primal constrained problem by choosing a fixed weight term \u03b1. The final solution heavily depends on the selection of \u03b1. The Feasible Direction Method (FDM) considers constraints by finding a direction p that satisfies g(\u03b8 + \u03bb p) > 0 for small \u03bb. The TNB method selects p using the bisector of gradients \u2207 \u03b8 f and \u2207 \u03b8 g, with a fixed learning stride. The optimization result of TNB relies on the shape of g. The shape of g is crucial for the success of TNB. The barrier term of \u2212\u03b1 log g(\u03b8) is used, where \u03b1 is a small positive number. The objective with barrier term gets closer to the primal objective as \u03b1 gets smaller. Choosing a sequence of decreasing \u03b1 values helps in practice. Directly applying this method is computationally challenging, so bounding collected transitions in the feasible region based on previous trained policies is a more natural approach. During training, termination signals are sent by previous policies to new agents, implicitly bounding the feasible region. Valid samples collected during training are within this region, leading to a unique new policy without the need to balance intrinsic and extrinsic rewards. The learning process becomes more robust without objective inconsistency. The pseudo code of IPD based on PPO is shown in Algorithm.1, with additions highlighted in blue."
}