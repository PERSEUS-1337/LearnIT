{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters while improving performance. This arrangement also introduces additional regularization. The branches are tightly coupled by averaging their log-probabilities, enhancing the learning of better representations. This approach, known as \"coupled ensembles,\" is applicable to various neural network architectures. The study explores deep convolutional network architecture, proposing a reconfiguration of model parameters into parallel branches at the global network level. This approach, known as \"coupled ensembles,\" can be applied to various neural network architectures and has shown improved performance with reduced parameters. The study introduces a new approach called \"coupled ensembling\" to reconfigure model parameters into parallel branches at the global network level. This method, utilized in state-of-the-art models like ResNet and DenseNet, achieves comparable performance with fewer parameters. The study introduces \"coupled ensembling\" to reconfigure model parameters into parallel branches at the global network level. By combining activations through an arithmetic mean, performance on CIFAR and SVHN datasets is significantly improved with reduced parameters. Further ensembling of coupled ensembles leads to additional improvement. The paper discusses related work, introduces coupled ensembles, evaluates the approach, and concludes with future work. The proposed network architecture is similar to Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network. The Multi-Column Deep Neural Network (MCDNN) BID3 is an ensemble network with branches that differ from traditional ensemble networks. They train a single model with branches, have a fixed parameter budget, combine branch activations through log-probabilities, and use the same input for all branches. This approach has been successful in various vision applications. Recently, modifications using grouped convolutions have been proposed for multi-branch architectures in vision applications. These modifications involve stacking template building blocks to form the model at the global level, rather than just at the building block level of base architectures like ResNet and Inception. Additionally, a generic modification involves replicating an \"element block\" as parallel branches to create the final composite model. Shake-Shake regularization further enhances performance by introducing a stochastic mixture of branches. Regularization techniques like BID5 and BID26 have been proposed for multi-branch architectures in vision applications. BID5 achieves good results on CIFAR datasets but requires more epochs for convergence and depends on batch size. BID26 explores parallel paths in ResNet, while our method rearranges architecture parameters without introducing additional choices. Ensembling neural networks is a reliable technique to improve model performance. Ensembling neural networks is a reliable technique to improve model performance by combining outputs from multiple trainings of the same architecture. The proposed model architecture consists of parallel branches trained jointly, leading to increased performance. This approach is similar to ResNet and ResNeXt's residual block and Inception's inception module but is done at the global network level. Arranging parameters into parallel branches enhances performance. The proposed model architecture consists of parallel branches trained jointly, leading to increased performance. Ensembling approach can be applied for fusion of independently trained coupled ensemble models, resulting in significant performance improvement. Snapshot ensembles use checkpoints during training for efficiency, but increase model size and prediction time. The approach aims to maintain model size while improving performance or achieving same performance with smaller model size. The model architecture consists of parallel branches trained jointly, leading to increased performance. Each branch produces a score vector for target classes. Different element blocks like DenseNet-BC and ResNet are used. The branches are combined using a fuse layer operation, such as averaging log probabilities. The classification task involves assigning samples to one class from a finite set. This approach can be generalized to other tasks like segmentation and object detection. Neural network models output a score vector for target classes, followed by a fully connected (FC) layer and SoftMax (SM) layer. Different network architectures for image classification use this setup. The internal differences before the last FC layer are not relevant. The resulting \"element block\" takes an image input and produces a vector of N values, parametrized by tensor W. In the context of neural network models for image classification, a vector of N values is output, parametrized by tensor W. Fusion of independently trained models involves averaging individual predictions from separate model instances. A \"super-network\" with parallel branches and an AVG layer can be used for this fusion. Alternatively, an averaging layer can be placed before the SoftMax layer for a \"factorized\" approach. Each branch in the model produces a score vector for target categories, which are fused through a \"fuse layer\". The model consists of multiple branches, each producing a score vector for target categories. These score vectors are combined through a \"fuse layer\" during training, leading to improved performance with fewer parameters. Three options are explored for combining score vectors: Activation average, Probability average, and Log Likelihood average. Averaging the log probabilities of the target categories results in a single prediction from the composite model. The proposed architecture combines score vectors from multiple branches through a \"fuse layer\" during training to improve performance with fewer parameters. The parameter vector W of the composite branched model is the concatenation of parameter vectors from element blocks. The architecture is evaluated on CIFAR-10, CIFAR-100, and SVHN datasets, with all hyperparameters set according to the original descriptions of the \"element block.\" During training, input images for CIFAR-10, CIFAR-100, and SVHN are normalized by subtracting the mean image and dividing by the standard deviation. Data augmentation includes random horizontal flips and crops for CIFAR datasets, while no augmentation is used for SVHN. A dropout ratio of 0.2 is applied to DenseNet when training on SVHN. Testing involves normalizing input similarly to training. Error rates are presented as percentages averaged over the last 10 epochs. DenseNet-BC uses PyTorch implementation, and execution times are measured on a single NVIDIA 1080Ti GPU with optimal micro-batch 2. The experiments in Section 4.3 and 4.4 use the CIFAR-100 dataset with DenseNet-BC, L=100, k=12. Results show that a jointly trained branched configuration outperforms averaging predictions from 4 separate models. The error rate is lower for the branched configuration (17.61 vs. 18.42) despite having the same number of trainable parameters. The experiments show that a single branch model with the same number of parameters as a multi-branch configuration has higher error rates. The branched configuration is more efficient in terms of parameters and shows better performance as the number of branches increase. In this section, the performance of a branched model with different \"fuse layer\" choices is compared. The model with e = 4 is trained with fusion after the FC, LSM, or LL layer. Results are presented in Table 1, showing the top-1 error rate on the CIFAR-100 test set. The branched model outperforms a single branch model with the same parameters. The performance of models with different \"fuse layer\" choices during inference is compared in Table 1. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" shows similar performance to a DenseNet-BC model with significantly more parameters. The average error rate of \"element blocks\" trained jointly in coupled ensembles with LSM fusion is also discussed. The error rate of \"element blocks\" trained jointly in coupled ensembles with LSM fusion is significantly lower than when trained individually. This coupling forces them to learn complementary features and better representations. Averaging the log probabilities helps update all branches consistently, providing a stronger gradient signal. Ensemble combinations outperform single branch networks, with the best single branch model having an error rate of 20.01 with a parameter budget of 3.2M. Using 4 branches instead of 1 reduces the error rate to 17.61. Training with Avg. FC yields better results than Avg. SM due to FC values transmitting more information. All experiments use Avg. LSM for training in branched models. In this section, the optimal number of branches e for a given model parameter budget is investigated using DenseNet-BC on CIFAR-100. Results are shown in table 3 for different configurations of branches e, depth L, and growth rate k. DenseNet-BC parameter counts are quantified based on L and k values, as well as the e value in the coupled ensemble version. This is crucial in moderate size models. The study investigates the optimal number of branches for DenseNet-BC on CIFAR-100 with different configurations. Results show that using 2 to 4 branches improves performance significantly compared to a single branch. However, using 6 or 8 branches performs worse possibly due to thin element blocks. The study found that using 6 or 8 branches in DenseNet-BC performs worse due to thin element blocks. Model performance is robust to variations in L, k, and e. Coupled ensembles show improved performance but increase training and prediction times. The study evaluated DenseNet-BC and ResNet BID8 with pre-activation as element blocks. The study evaluated the performance of coupled ensembles with ResNet pre-act as element block and e = 2, 4, showing significantly better results than single branch models. Different network sizes for DenseNet-BC were considered, ranging from 0.8M to 25.6M parameters, with results reported for extreme cases. The experiments demonstrated the effectiveness of the approach with other architectures. The study compared different network sizes for DenseNet-BC, finding that the trade-off between L and k was not critical for a given parameter budget. Experimenting with single-branch and multi-branch versions of the model, error rates were higher than reported by BID11 for single branch DenseNet-BC. The Torch7 and PyTorch implementations were found to be equivalent, with differences possibly due to conservative error rate measures and statistical variations. The study compared different network sizes for DenseNet-BC and found that the coupled ensemble approach outperformed DenseNet-BC's reported performance. Larger models of coupled DenseNet-BCs performed better than current state-of-the-art implementations. The coupled ensemble approach is limited by network size and training time constraints. The study compared different network sizes for DenseNet-BC and found that the coupled ensemble approach outperformed DenseNet-BC's reported performance. Larger models of coupled DenseNet-BCs performed better than current state-of-the-art implementations. The ensembles of coupled ensemble networks showed significant gains by fusing two models, but further fusion did not yield much improvement. The study compared different network sizes for DenseNet-BC and found that the ensembles of coupled ensemble networks outperformed all state-of-the-art implementations. The proposed approach involves replacing a single deep convolutional network with multiple \"element blocks\" coupled via a \"fuse layer\" for improved performance on CIFAR-100. The proposed approach involves using multiple \"element blocks\" coupled via a \"fuse layer\" to improve performance on CIFAR-100 by averaging score vectors at test time. This leads to the best performance within a given parameter budget, with better individual block performance compared to training independently. However, there is a slight increase in training and prediction times due to sequential processing of branches and less efficient data parallelism on GPUs for smaller models. The proposed approach involves using multiple \"element blocks\" coupled via a \"fuse layer\" to improve performance on CIFAR-100 by averaging score vectors at test time. It is possible to extend data parallelism to the branches before the averaging layer, either through parallel implementation of multiple 2D convolutions or spreading branches over multiple GPUs. Preliminary experiments on ImageNet show that coupled ensembles have lower error rates with the same parameter budget compared to single branch models. Future experiments will expand on these findings. The proposed approach involves using multiple \"element blocks\" coupled via a \"fuse layer\" to improve performance on CIFAR-100 by averaging score vectors at test time. The e model instances do not need to share the same architecture. Different options are available for placing the averaging layer in the train version. The branches are defined by parameter vectors, and the global network is defined by a concatenation of all parameter vectors. When training and prediction are done in separate modes, dedicated modes are used. The proposed approach involves using multiple \"element blocks\" coupled via a \"fuse layer\" to improve performance on CIFAR-100 by averaging score vectors at test time. The global network architecture is determined by global hyper-parameters specifying train versus test mode, number of branches, and placement of the AVG layer. Different options exist for replicating element blocks or using a list of element blocks with their own hyper-parameters. The approach involves using multiple \"element blocks\" connected via a \"fuse layer\" to enhance CIFAR-100 performance by averaging score vectors at test time. For larger models, data batches are split into \"micro-batches\" with b/m elements each, where b is the batch size. Gradient accumulation and averaging over micro-batches are used to approximate the gradient calculation. The BatchNorm layer uses micro-batch statistics, which may not be exact but does not significantly impact results. This ensures consistent settings for comparing different models. To ensure consistent settings for comparing different models, parameter updates are performed using gradient for a batch, while forward passes are done with micro-batches. Memory requirements depend on network depth and mini-batch size. The micro-batch \"trick\" adjusts memory needs while maintaining a default mini-batch size. The multi-branch version requires more memory only if branch width is reduced. Hyper-parameter search experiments were conducted through cross-validation. In hyper-parameter search experiments, it was found that reducing both width and depth of the network was the best option. Training with 25M parameters was done within 11GB memory using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch versions. Splitting the network over two GPU boards did not significantly increase speed or improve performance. Top-1 error rate remained the same. The experiments compared the performance of coupled ensembles with two branches to single-branch architectures. Results showed that even with only two branches, there was a significant improvement. The study also evaluated variations in depth and growth rate, finding stable performance. The best combination for the test set was predicted to be (L = 82, k = 8, e = 3), with a slightly better alternative of (L = 70, k = 9, e = 3). Comparisons were made on parameter usage and performance against other model architectures. The parameter usage and performance of branched coupled ensembles with model architectures recovered using meta learning techniques were compared. Issues of reproducibility and statistical significance in performance measures were identified, including variations in framework, random seed, CuDNN non-determinism, and fluctuations in batch normalization. Fluctuations in batch normalization are observed even with all hyper-parameters set to 0. The choice between the model obtained after the last epoch or the best performing model can impact evaluation measures due to random initialization. Different random seeds can lead to different local minima in neural networks. The dispersion in local minima of neural networks can complicate comparisons between methods, as differences smaller than the dispersion may not be significant. Statistical tests may not help much as even models with the same seed can show significant differences. Experiments show the dispersion in moderate scale models, as conducting a large number of trials for larger models is often not feasible. In the study, the relative importance of different effects in DenseNet-BC with L = 100, k = 12 on CIFAR 100 was quantified. Different combinations were tried using Torch7 and PyTorch, as well as using the same seed or different seeds. Various performance measures were used, and results were presented in terms of minimum, median, maximum, and mean\u00b1standard deviation over 10 measures. The study compared the error rate of models in Torch7 and PyTorch on DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results showed no significant difference between implementations or using the same seed. Reproducing exact results was not possible due to dispersion. Standard deviation was slightly smaller when computed on the last 10 epochs. The study compared error rates of models in Torch7 and PyTorch on DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results showed no significant difference between implementations or using the same seed. Reproducing exact results was not possible due to dispersion. Standard deviation was slightly smaller when computed on the last 10 epochs. The mean of measures on 10 runs is significantly lower when taken at the best epoch compared to single last epoch or last 10 epochs. A method for ensuring reproducibility and fair comparisons is proposed. Choosing the measure as the minimum error rate for all models computed during training may introduce bias for absolute performance estimation. Using the error rate at the last iteration or the 10 last iterations shows no difference in the mean, but the standard deviation is smaller for the latter. Using the 10 or 25 last epochs also does not significantly impact learning. Different values can be used for better results. In CIFAR experiments, the average error rate of models from the last 10 epochs is used for more robust results. For SVHN experiments, the last 4 iterations are used due to fewer epochs. Comparisons between single-branch and multi-branch architectures show a clear advantage for multi-branch networks under a constant parameter budget. The training time of multi-branch networks is longer than single-branch networks, but can still improve over single-branch ones at a constant training time budget. Ways to reduce training time include reducing iterations, parameter count, or increasing width while reducing depth. Results for these options are shown for CIFAR 10 and 100, comparing single branch DenseNet-BC L = 190, k = 40, e = 1 with multi-branch configurations. The study compares different configurations of DenseNet-BC models with varying parameter budgets and training times. Options include reducing training epochs, depth, or matching parameter count and training time of a single-branch baseline. Results show that these options perform better than the single-branch baseline, with slight variations in performance compared to the full multi-branch baseline. In this section, the performance of single branch models and coupled ensembles in a low training data scenario is compared. Coupled ensembles significantly outperform the single branch model for a fixed parameter budget. Experiments were conducted on two datasets, STL-10 and a subset of CIFAR-100. Results show that coupled ensembles perform better than single branch models. In preliminary experiments on ILSVRC2012, single-branch and multi-branch coupled ensembles were compared using images of size 256\u00d7256. Data augmentation included random flips and crops of size 224\u00d7224. DenseNet-169-k32-e1 was used as a baseline single-branch model, while DenseNet-121-k30-e2 was used for the coupled ensemble. Results in table 11 show that the coupled ensemble approach with two branches significantly outperforms the baseline. The coupled ensemble approach with two branches outperforms the baseline, even with a constant training time budget."
}