{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets of the phrase's meaning and outperform strong baselines on benchmark datasets. The curr_chunk discusses the use of various NLP models like word2vec, GloVe, skip-thoughts, ELMo, and BERT for unsupervised tasks. It highlights the limitations of single embeddings for representing multiple senses or topics in phrases and sentences. To address this issue, word sense induction methods are proposed. The curr_chunk discusses word sense induction methods and multi-mode word embeddings to represent target words in a distributional semantic space. It uses real property as an example to show different senses of the word. Unlike topic modeling, these approaches solve distinct clustering problems for each target word. The curr_chunk discusses challenges in extending multi-mode representations to phrases or sentences due to efficiency issues. It highlights the difficulty in clustering unique sequences and the large number of parameters required. The target phrase \"real property\" is used as an example to illustrate clustering centers for different senses of the word. The curr_chunk discusses using a neural encoder and decoder to compress the embedding of co-occurring words in target phrases. This approach aims to overcome the challenge of sparseness in co-occurring statistics and reduce the number of parameters needed for clustering unique sequences. Unlike previous methods, the model predicts cluster centers from the word sequence to reconstruct the co-occurring distribution effectively. The proposed model learns a mapping between target sequences and cluster centers during training, allowing for direct prediction of cluster centers at test time. A nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed word embeddings. Gradients are back-propagated to update cluster centers, encoder, and decoder weights jointly. Experimental results demonstrate the model's ability to capture the compositional meanings of words. The proposed model captures compositional meanings of words better than word embedding averaging in unsupervised tasks. It can measure asymmetric relations like hypernymy without supervision and outperforms single-mode alternatives in sentence representation. Training setup, objective function, and architecture are detailed in Sections 2.1, 2.2, and 2.3. The model represents sentences using multiple codebook embeddings. The model represents sentences using multiple codebook embeddings predicted by a sequence to embeddings model. The loss function encourages the model to generate codebook embeddings that can reconstruct co-occurring words but not negatively sampled words to avoid predicting common topics. The sequence of words in the corpus is expressed as a target sequence, with neighboring words related to the sequence. The training signal is to reconstruct a set of neighboring words within a fixed window size. The model uses codebook embeddings to represent sentences, with a focus on clustering words that could potentially co-occur instead of actual co-occurring words. Training involves separate models for phrases and sentences due to different training signals. The goal is to predict possibly co-occurring words by observing similar sequences, emphasizing semantics over syntax. The model uses codebook embeddings to represent sentences, focusing on clustering words that could potentially co-occur. It considers word order information in the input sequence but ignores the order of co-occurring words. The distribution of co-occurring words is modeled in a pre-trained word embedding space, with predicted cluster centers for the input sequence. The number of clusters is fixed to simplify the prediction model design for downstream tasks. The experimental section discusses the effect of different cluster numbers on the prediction model design. The reconstruction loss of k-means clustering in the word embedding space is defined using a permutation matrix. Non-negative sparse coding relaxes constraints by allowing positive coefficients. Using NNSC loss helps neural architectures generate diverse cluster centers compared to kmeans loss. The NNSC loss is smoother and easier to optimize for neural networks compared to kmeans loss. By using NNSC, the reconstruction error is defined with a hyper-parameter \u03bb controlling sparsity. The proposed loss is efficient in minimizing L2 distance, unlike methods that estimate permutation between prediction and ground truth words, which can be computationally expensive. The proposed loss function is efficient in minimizing L2 distance by using a pre-trained embedding space. It estimates M Ot on the fly using convex optimization and back-propagates gradients to achieve end-to-end training. The neural network architecture is similar to Word2Vec but can encode compositional meaning and decode multiple embeddings. Our neural network architecture, similar to a transformation-based seq2seq model, uses an encoder to map input sequences into contextualized embeddings. Unlike typical seq2seq models, our decoder outputs a sequence of embeddings instead of words, allowing for predicting all codebook embeddings in a single pass. The <eos> embedding is treated as the sentence representation, and different aspects are captured by passing it through different linear layers. The decoder in our neural network architecture outputs a sequence of embeddings for predicting all codebook embeddings in a single pass. Different aspects are captured by passing the <eos> embedding through different linear layers before becoming the input of the decoder. The attention on contextualized word embeddings significantly affects sentence representation, while the performance of phrase representation is not as affected. The framework is flexible, allowing for the replacement of encoder and decoder with other architectures and the incorporation of additional input features. The model visualizes cluster centers to summarize target sequences effectively. Codebook embeddings enhance semantic facets of phrases and sentences. Evaluating topics using codebook embeddings improves unsupervised semantic tasks. Pre-trained GloVe embeddings are used for sentence and phrase representation. The model is trained on Wikipedia 2016 with stop words removed from co-occurring words. Our models do not require additional resources like PPDB or multi-lingual resources, making them practical for domains with low resources such as scientific literature. The transformers in our models have the same number of dimensions as GloVe embeddings (300). Training all models on a single GPU within a week due to computational limitations results in underfitting after a week. BERT is a powerful model trained on a masked language modeling loss, providing effective pretrained embeddings for various tasks. Despite differences in training parameters and resources compared to our models, we still provide unsupervised performance metrics for reference. Semeval 2013 task 5(a) English and Turney 2012 are standard benchmarks for evaluating phrase similarity. BERT is a powerful model trained on masked language modeling loss, providing pretrained embeddings for tasks like Semeval 2013 and Turney 2012 for evaluating phrase similarity. BiRD and WikiSRS contain ground truth phrase similarities, while our model evaluates phrase similarity using transformer encoder embeddings and cosine similarity. Our method, Ours Emb, computes the similarity between two phrase embeddings by calculating the reconstruction error from normalized codebook embeddings. We use a symmetric distance SC and negative distance to represent similarity when ranking for similar phrases. Performance comparison with 5 baselines including GloVe Avg, Word2Vec Avg, BERT CLS, BERT Avg, and FCT LM Emb is conducted. Our models significantly outperform all baselines in 4 datasets, including Turney. The effectiveness of non-linearly composing word embeddings is highlighted, showing better performance than GloVe, Word2Vec, and FCT baselines. Results also suggest that multi-mode embeddings may not always improve performance in word similarity benchmarks. The performances of Ours (K=10) remain strong compared with baselines in sentence similarity tasks. The similarity performance is not sensitive to the number of clusters, alleviating the problem of selecting K in practice. The benchmark STS Low compares model performances on less similar sentence pairs. Our method is also compared with word mover's distance (WMD). In comparison to word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos), our method incorporates a weighting technique based on word probabilities. This method, referred to as GloVe SIF, removes the first principal component to enhance performance. Additionally, we also present results without post-processing, known as GloVe Prob_avg, showing strong performance in sentence similarity tasks. The importance of considering word embeddings in addition to sentence embeddings for measuring sentence similarity is highlighted. Multi-facet embeddings allow for estimating word importance by computing cosine similarity with predicted codebook embeddings. An importance weighting technique is applied to all words in the query sentence to enhance the original weighting. The importance weighting technique is applied to word embeddings to enhance original weighting vectors on GloVe Avg, GloVe Prob_avg, and GloVe SIF, resulting in improved performance in sentence similarity tasks compared to WMD and BERT Avg. Our method with attention weighting demonstrates benefits in STSB Low and when not relying on training distribution assumptions. The variant uses a bi-LSTM as the encoder and a LSTM as the decoder, outperforming ST Cos. It applies the model to HypeNet for hypernymy detection based on co-occurring words. The variant uses a bi-LSTM as the encoder and a LSTM as the decoder, outperforming ST Cos in hypernymy detection based on co-occurring words. Cluster embeddings of co-occurring words to reconstruct hypernyms better. Asymmetric scoring function defined for detecting hypernyms. Our methods outperform baselines in detecting hypernym direction. Summary A with normalized embeddings best reconstructs word distribution in the document. The extractive summarization method optimizes sentence embeddings by selecting sentences greedily. Multiple codebook embeddings capture different aspects of sentences. Comparisons are made with alternative ways of modeling sentence aspects, such as average word embeddings and using all words in the sentences. The method optimizes sentence embeddings by selecting sentences greedily. Codebook embeddings capture different aspects of sentences, avoiding the problem of sentence length normalization. Comparisons are made with alternative ways of modeling sentence aspects, such as average word embeddings and using all words in the sentences. The results are compared using F1 of ROUGE in Table 5, with different methods choosing 3 sentences. Unsupervised methods do not use sentence position information, which is a strong signal in news corpora like CNN/Daily Mail. In evaluating unsupervised sentence embeddings, the focus is on comparing methods that do not assume the first few sentences form a good summary. Predicting more aspects with higher cluster numbers yields better results, with K=100 giving the best performance after selecting 3 sentences. Larger cluster numbers are desired in this application, allowing for a relatively large K to be set due to alleviating computational and sample efficiency challenges. Neural networks have been widely applied for discovering semantically coherent topics. Sparse coding on word embedding space is used to model multiple aspects of a word, while parameterizing word embeddings with neural networks helps save storage space. Words are represented as single or multiple regions in Gaussian embeddings to capture asymmetric relations. The challenges of extending methods to longer sequences are not addressed in previous studies. Designing a neural decoder for sets rather than sequences involves matching steps and computing distance loss. Various loss options like Chamfer distance and more sophisticated matching losses are used. The goal is to measure distances between ground truth and predicted sets, while the set decoder reconstructs sets using fewer bases. Our goal is to efficiently predict clustering centers that reconstruct observed instances, overcoming challenges in learning multi-mode representation for long sequences. Using a neural encoder to model target sequence meaning and a neural decoder to predict codebook embeddings. The proposed model uses a neural decoder to predict codebook embeddings as representations of sentences or phrases. It outperforms BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings work best for sequences with many aspects, while single-facet embeddings perform well for sequences with one aspect. In the future, the goal is to train a single model to generate multi-facet embeddings for phrases and sentences, and apply this method to other unsupervised learning tasks. The model is kept simple to converge training loss quickly, without fine-tuning hyper-parameters. The transformer architecture is similar to BERT, with a sparsity penalty weight of 0.4 and a maximal sentence size of 50. The weights on coefficient matrix \u03bb is set to 0.4. The maximal sentence size is 50, and the maximal number of co-occurring words is 30. The number of dimensions in transformers is 300. For sentence representation, the number of transformer layers on the decoder side is 5 with a dropout on attention of 0.1 for K = 10, and 1 for K = 1. For phrase representation, the number of transformer layers on the decoder side is 2 with a dropout on attention of 0.5. The window size d t is 5. The code will be released to reveal more hyper-parameter settings. The number of codebook embeddings K is determined by the performance of training data, with larger K needing longer training time. Skip-thoughts use a hidden embedding size of 600 and are retrained for 2 weeks. The model has fewer parameters than BERT base and requires less computational resources. In comparison to BERT base, BERT Large performs better in similarity tasks but worse in hypernym detection. Despite the performance gains of BERT in similarity tasks, the method being discussed is still superior, especially in phrase similarity tasks. The hypothesis is that BERT's training method may not be as effective for short sequences like phrases. In Section 3.4, comparisons were made between different summarization methods with the same number of sentences selected. The performance of W Emb (*) methods may suffer due to selecting shorter sentences. A plot of R-1 performance against sentence length showed that Ours (K=100) outperformed W Emb (GloVe) and Sent Emb (GloVe) when summaries were of similar length. Additionally, W Emb (*) generally outperformed Sent Emb (*) when comparing summaries of similar length, although this comparison may not be entirely fair. In practice, it is preferable to avoid choosing many short sentences in extractive summarization for fluency. The figure suggests that Ours (K=100) is the best choice for summaries under 50 words, while W Emb (BERT) is better for longer summaries. Combining our method with BERT may yield the best performance, as shown in the visualization of predicted embeddings from randomly selected sentences in the validation set. The format of the file for visualizing predicted embeddings from randomly selected sentences in the validation set is similar to Table 1, with the first line being the preprocessed input sentence and embeddings visualized by the nearest five neighbors in a GloVe embedding space."
}