{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models used for high-dimensional continuous measures. This paper introduces a scalable method for unbalanced optimal transport (OT) within the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the transformation of a source measure to a target measure. The proposed algorithm utilizes stochastic alternating gradient updates, akin to GANs, and is validated through numerical experiments in population modeling. Optimal transport involves finding a cost-optimal way to transform one measure to another using mass variation and transport. Classical optimal transport pushes a source to a target distribution without allowing for mass variations, while modern approaches use the Kantorovich formulation for optimal probabilistic coupling between measures. Regularizing the objective with an entropy term has been shown to be effective. Optimal transport involves transforming measures efficiently using the Sinkhorn algorithm. Stochastic methods based on the dual objective have been proposed for various applications, including computer graphics, domain adaptation, and image translation. Generative models like GANs can also be used to learn transport maps. In optimal transport, the transformation of a source population \u00b5 to a target population \u03bd is modeled. Different sub-populations may grow at different rates. Various methods, including GANs, have been used to tackle this problem, but they struggle with handling mass variation between source and target populations. New formulations and numerical methods are being developed to address this issue. In optimal transport, a class of scaling algorithms has been developed for approximating solutions to optimal entropy-transport problems. These algorithms have been used in various applications such as computer graphics, tumor growth modeling, and computational biology. Inspired by the success of GANs, a novel framework for unbalanced optimal transport is presented to address mass variation between source and target populations. In a novel framework for unbalanced optimal transport, a Monge-like formulation is proposed to learn a stochastic transport map and scaling factor for cost-optimal transport. The methodology is demonstrated using MNIST and USPS handwritten digits datasets for population modeling. The curr_chunk discusses the application of a new scalable method for solving the optimal-entropy transport problem in various datasets, including MNIST, USPS, CelebA, and a single-cell RNA-seq dataset from zebrafish. It introduces Algorithm 2 in the Appendix as an alternative to BID8 for large or continuous datasets. The notation and functions used in the algorithm are also briefly explained. Optimal transport (OT) involves projecting measures onto X and Y using functions \u03c0 X and \u03c0 Y. The Monge problem seeks deterministic transport maps, while the Kantorovich OT problem is a convex relaxation that considers probabilistic transport plans. The conditional probability distributions \u03b3 y|x represent stochastic maps from X to Y, offering a \"one-to-many\" approach compared to the Monge problem. The relaxed problem provides a numerical method for solving OT. The relaxed problem in optimal transport is a linear program that can be solved efficiently using the Sinkhorn algorithm. Entropic regularization simplifies the dual optimization problem, leading to stochastic algorithms for computing transport plans. Various formulations extend classical optimal transport to handle mass variation, with numerical methods based on optimal-entropy transport. In the context of optimal transport, the proposed algorithm for unbalanced OT directly models mass variation and can be applied to high-dimensional continuous measures. This algorithm is based on a Monge-like formulation and aims to learn a stochastic measure \u03b3 that minimizes a cost function. The state-of-the-art in discrete settings involves iterative scaling algorithms that generalize the Sinkhorn algorithm for computing regularized OT plans. However, there are currently no practical algorithms for unbalanced OT between continuous measures, especially in high-dimensional spaces. The unbalanced Optimal Transport (OT) algorithm aims to learn a stochastic transport map and scaling factor to push a source measure to a target measure in a cost-optimal way. It involves a Monge-like formulation with a focus on mass transport and variation costs, utilizing a deterministic or stochastic transport map. This approach is particularly useful for high-dimensional continuous measures where traditional algorithms may not be practical. The unbalanced Optimal Transport (OT) algorithm utilizes a stochastic transport map and scaling factor to push a source measure to a target measure in a cost-optimal way. It is a more suitable model for practical problems like cell biology, where one cell can give rise to multiple cells. The transport map T models the movement of points from source to target, while the scaling factor \u03be represents growth or shrinkage of these points. Different transformation models are optimal depending on the scenario. The unbalanced Optimal Transport (OT) algorithm uses a stochastic transport map and scaling factor to move points from a source measure to a target measure. Different transformation models are optimal based on the costs of mass transport and variation. An unbalanced transport map with a scaling factor can address class imbalances by adjusting the weights of samples in the source distribution to balance classes with the target distribution. The relaxation of the optimal-entropy transport problem involves specifying a joint measure \u03b3 \u2208 M + (X \u00d7 Y) using an appropriate choice of \u03c8. This formulation differs from deterministic transport maps as it restricts the search space for joint measures \u03b3. The asymmetric Monge formulation requires that all mass transported to Y must come from within the support of \u00b5. The relaxation of the optimal-entropy transport problem involves specifying a joint measure \u03b3 \u2208 M + (X \u00d7 Y) using an appropriate choice of \u03c8. Equivalence can be established by restricting the support of \u03b3 to supp(\u00b5) \u00d7 Y. Theorem 3.4 states that solutions of the relaxed problem converge to solutions of the original problem under certain conditions. The relaxation of unbalanced Monge OT involves learning the transport map and scaling factor using stochastic gradient methods. The divergence term is minimized by writing it as a penalty with an adversary function. The objective can be optimized using alternating stochastic gradient updates with neural networks parameterizing T, \u03be, and f. The optimization procedure involves alternating stochastic gradient updates with neural networks parameterizing T, \u03be, and f. The objective is to minimize the divergence between transported samples and real samples from \u03bd, measured by the adversary f. Additionally, cost functions encourage finding the most cost-efficient strategy. The probabilistic Monge-like formulation involves solving a non-convex optimization problem using neural networks to learn a transport map T and scaling factor \u03be. This approach enables efficient computation of transport and scaling from a source domain to a target domain with practical applications. The scaling algorithm of BID8 solves a convex optimization problem and is proven to converge, but is limited to discrete problems. A new stochastic method proposed in the Appendix can handle transport between continuous measures and overcomes scalability limitations. However, the output is in the form of the dual solution, which is less interpretable compared to Algorithm 1. In Section 4, the advantage of directly learning a transport map and scaling factor using Algorithm 1 is demonstrated. The problem of learning a scaling factor to balance measures \u00b5 and \u03bd arises in causal inference, where the goal is to eliminate selection biases in treatment inference by scaling the importance of different members from a control population based on their likelihood to be present in the treated population. Algorithm 1 demonstrates unbalanced optimal transport between modified MNIST datasets to mimic class distribution changes in the target population. Algorithm 1 demonstrates unbalanced optimal transport between modified MNIST datasets to mimic class distribution changes in the target population. The scaling factor learned by Algorithm 1 reflects class imbalances and can model growth or decline of different classes in a population. Experiments show that the scaling factor roughly reflects the imbalance ratio between source and target distributions, as illustrated in FIG4. Unbalanced OT is applied from the MNIST dataset to the USPS dataset to simulate population evolution over time. The evolution of the MNIST distribution to the USPS distribution is modeled using Algorithm 1, with the transport cost being the Euclidean distance between the original and transported images. The unbalanced transport is visualized in FIG1, showing how MNIST digits change in prominence in the USPS dataset. Despite the limitations of the Euclidean distance measure, many MNIST digits retain their likeness during the transport. The model analyzes which MNIST digits increase or decrease in prominence in the USPS dataset. The model analyzed MNIST digits for prominence changes in the USPS dataset. Higher scaling factors resulted in brighter digits covering more pixels. Algorithm 1 was applied to perform unbalanced OT on the CelebA dataset, simulating the transformation from young to aged faces using a VAE for semantic similarity. The study applied Algorithm 1 to perform unbalanced optimal transport on the CelebA dataset, transforming young faces into aged faces in the latent space. The transported faces generally retained key features, with exceptions like gender swaps. Interestingly, young faces with higher scaling factors were more likely to be male. The study applied Algorithm 1 to perform unbalanced optimal transport on the CelebA dataset, transforming young faces into aged faces in the latent space. The model predicts a growth in the prominence of male faces compared to female faces as the population evolves from young to aged. There is a strong gender imbalance between the young and aged populations, with the aged population being predominantly male. This phenomenon was confirmed by checking the ground truth labels. In a study, Algorithm 1 was used to analyze single-cell gene expression data from two stages of zebrafish embryogenesis. The cells with higher scaling factors were found to be enriched for genes associated with differentiation and development of the mesoderm, indicating potential for biological discovery. The text presents a stochastic method for unbalanced optimal transport based on the regularized dual formulation of BID7. The dual formulation involves a constrained optimization problem that can be challenging to solve, but adding a strongly convex regularization term can help. This term encourages transport plans with high entropy, leading to a smoothing effect. The dual of the regularized problem is derived using the Fenchel-Rockafellar theorem. The relationship between primal optimizer \u03b3 * and dual optimizer (u * , v * ) is given by DISPLAYFORM5. Rewriting in terms of expectations, assuming access to samples from \u00b5, \u03bd, and normalized measures. Parameterizing u, v with neural networks u \u03b8 , v \u03c6 and optimizing using stochastic gradient descent. Algorithm 2 describes this generalization from classical OT to unbalanced OT, with (9) becoming the dual of the entropy-regularized classical OT problem. The dual solution (u*, v*) learned from Algorithm 2 can be used to reconstruct the primal solution \u03b3* indicating mass transport between points in X and Y. An \"averaged\" deterministic mapping from X to Y can be learned using \u03b3*. A stochastic algorithm for learning this map from the dual solution is presented in Algorithm 3. The objectives in (6) and (3) are equivalent if reformulated in terms of \u03b3 instead of (T, \u03be). The formulations are equivalent if the search space of (3) contains only joint measures specified by some (T, \u03be). This relation is formalized by Lemma 3.3, showing L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) for any solution (T, \u03be) and defined \u03b3. This implies that \u03be is the Radon-Nikodym derivative of \u03c0 X # \u03b3 with respect to \u00b5. DISPLAYFORM3 states that L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) by taking the infimum over the left-hand side. A family of measurable functions {T x : Z \u2192 Y} x\u2208X exists due to the disintegration theorem. The Radon-Nikodym derivative \u03be is related to (T, \u03be) satisfying certain conditions. The Radon-Nikodym theorem implies that (T, \u03be) satisfy DISPLAYFORM10, leading to the conclusion that W c1,c2,\u03c8 (\u00b5, \u03bd) \u2265 L \u03c8 (\u00b5, \u03bd). This result is crucial for proving the uniqueness of the joint measure \u03b3 in certain conditions. The uniqueness of the joint measure \u03b3 can be proven under certain conditions by showing that W c1,c2,\u03c8 (\u00b5, \u03bd) \u2265 L \u03c8 (\u00b5, \u03bd). The minimizers of L \u03c8 (\u00b5, \u03bd) are unique, and for certain cost functions and divergences, L \u03c8 defines a proper metric between positive measures \u00b5 and \u03bd. The Hellinger-Kantorovich BID27 or the Wasserstein-Fisher-Rao BID9 metric corresponds to the entropy functions for the KL-divergence. Theoretical analysis shows that solutions of the relaxed problem converge to solutions of the original problem. The joint measure \u03b3 is unique under certain conditions. The Hellinger-Kantorovich BID27 or the Wasserstein-Fisher-Rao BID9 metric corresponds to the entropy functions for the KL-divergence. Theoretical analysis shows that solutions of the relaxed problem converge to solutions of the original problem. The joint measure \u03b3 is unique under certain conditions. For all k, the sequence of minimizers \u03b3 k is bounded and equally tight, leading to the weak convergence of a subsequence \u03b3 k to some \u03b3, which is a minimizer of W c1,c2,\u03b9= (\u00b5, \u03bd). In this section, the convex conjugate form of \u03c8-divergence is presented to rewrite the main objective as a min-max problem. Lemma B.2 states that for non-negative finite measures P, Q over T \u2282 R d, a certain equality holds under specific conditions. The proof of this lemma is provided, with references to related works for generative modeling. The optimal solution for the problem is obtained when dP dQ belongs to the subdifferential of \u03c8 * (f). Proposition B.1 provides conditions for the problem to be well-posed, with the choice of cost functions c1 and c2 being crucial. It is recommended to use a cost function that measures correspondence between X and Y for c1, and a convex function for c2 that vanishes at 1 and tends to infinity at the extremes. The choice of cost functions c1 and c2 is crucial for the well-posedness of the problem. It is recommended to use a cost function that measures correspondence between X and Y for c1, and a convex function for c2 that vanishes at 1 and tends to infinity at the extremes. Any \u03c8-divergence can be used to train generative models to match a generated distribution P to a true data distribution Q. The discriminative objective function corresponds to D \u03c8 (P |Q) with specific constraints to minimize divergence between probability measures P and Q. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. Otherwise, P = Q in general when D \u03c8 (P |Q) is minimized. The discriminative objective function D \u03c8 (P |Q) aims to minimize divergence between probability measures P and Q. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. Choice of activation layers for neural networks can enforce mapping to the correct range. Activation layers for neural networks can enforce mapping to the correct range. In our experiments, fully-connected feedforward networks with 3 hidden layers and ReLU activations were used. The output activation layers were a sigmoid function for pixel brightness and a softplus function for the scaling factor weight."
}