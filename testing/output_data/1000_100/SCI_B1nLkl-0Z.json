{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions, such as Q-values, are essential in reinforcement learning algorithms like SARSA and Q-learning. A new concept of action value, defined by a Gaussian smoothed version of expected Q-value, is proposed. This smoothed Q-value still satisfies a Bellman equation, making it learnable from sampled experience. Gradients of expected reward can be obtained from the gradient and Hessian of the smoothed Q-value function, allowing for new algorithms to train a Gaussian policy directly from a learned Q-value approximator. This approach enables learning both mean and covariance during training, facilitating proximal optimization techniques. The ability to learn mean and covariance during training allows for strong results on continuous control benchmarks. Model-free reinforcement learning involves policy evaluation and improvement processes. Different notions of Q-value lead to distinct families of RL methods, such as SARSA, Q-learning, Soft Q-learning, and PCL. In this work, a new notion of action value called the smoothed Q-value function Q \u03c0 is introduced. Unlike previous notions, it associates a value with a distribution over actions rather than a specific action at each state. The smoothed Q-value is defined as the expected return of taking an action sampled from a normal distribution centered at a, followed by actions sampled from the current policy. This approach allows for a Gaussian-smoothed or noisy interpretation of the Q-value. The smoothed Q-value is a Gaussian-smoothed version of the expected Q-value, with properties that make it useful for RL algorithms. It satisfies Bellman consistency, allows for bootstrapping, and can be used for optimization objectives in Gaussian policies. The gradient of the objective with respect to policy parameters can be derived from the smoothed Q-value function, leading to the proposal of the Smoothie algorithm. Smoothie is an algorithm that trains a policy using derivatives of a smoothed Q-value function, avoiding high variance in updates. It utilizes a non-deterministic Gaussian policy for better exploration and can incorporate proximal policy optimization techniques with a penalty on KL-divergence. The DDPG algorithm significantly improves stability and performance in standard continuous control benchmarks, competing with or surpassing state-of-the-art results, especially for challenging tasks in the low-data regime. The goal is to find an agent that maximizes cumulative discounted reward in a Markov decision process framework. The agent's behavior is modeled using a stochastic policy \u03c0 that produces a distribution over feasible actions at each state s. The optimization objective is expressed in terms of the expected action value function Q \u03c0 (s, a) and the policy gradient theorem expresses the gradient of O ER (\u03c0 \u03b8 ) w.r.t. \u03b8. Many reinforcement learning algorithms trade off variance and bias. In this paper, new RL training methods are developed for multivariate Gaussian policies over continuous action spaces. The policies are parametrized by mean and covariance functions mapping the observed state to a Gaussian distribution. The focus is on estimating Q \u03c0 (s, a) accurately using function approximation, with a trade-off between variance and bias in reinforcement learning algorithms. The paper introduces new RL training methods for Gaussian policies in continuous action spaces, focusing on estimating Q \u03c0 (s, a) accurately using function approximation. BID21 presents a deterministic policy gradient for Gaussian policies with a covariance approaching zero, leading to a deterministic policy where the gradient can be expressed for optimization. The paper introduces new RL training methods for Gaussian policies in continuous action spaces, focusing on estimating Q \u03c0 (s, a) accurately using function approximation. It re-expresses the Bellman equation as a value function approximator Q \u03c0 w can be optimized by minimizing the Bellman error for transitions sampled from a dataset. Algorithms like DDPG alternate between improving the value function by gradient descent and improving the policy based on policy gradient identity. To improve sample efficiency, off-policy distribution is used based on a replay buffer. This substitution does not hold exactly but works well in practice. The paper introduces new RL training methods for Gaussian policies in continuous action spaces, focusing on estimating smoothed action value functions for optimizing parameters. Smoothed Q-values do not assume the first action is fully specified, but rather the mean of the distribution is known. This approach differs from prior work by directly learning a function approximator for Q \u03c0 (s, a) instead of drawing samples. The paper introduces new RL training methods for Gaussian policies in continuous action spaces, focusing on estimating smoothed action value functions for optimizing parameters. To approximate the expectation and derivative in (10), a function approximator for Q \u03c0 (s, a) is directly learned. Gaussian policies \u03c0 \u2261 (\u00b5, \u03a3) enable direct bootstrapping of smoothed Q-values, allowing for Bellman consistency. By parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 \u2261 (\u00b5 \u03b8 , \u03a3 \u03c6 ) with mean and covariance parameters \u03b8 and \u03c6, the objective w.r.t. mean parameters can be optimized using the policy gradient theorem. The second derivative of Q \u03c0 w.r.t. actions allows for exact computation of the derivative of Q \u03c0 w.r.t. \u03a3. The full derivative w.r.t. \u03c6 can be optimized using two approaches for Q 2 optimization. The first approach involves treating target values as fixed, achieving a fixed point with Bellman equation recursion. The second approach uses a single function approximator for Q \u03c0 w (s, a), resulting in a simpler training procedure. The second approach involves using a single function approximator for Q \u03c0 w (s, a) in the training procedure. It utilizes a sampling distribution with full support to draw a phantom action and optimize Q \u03c0 w (s, a) by minimizing a weighted Bellman error. Keeping track of probabilities q(\u00e3 | s) is found to be unnecessary in practice. The replay buffer provides a near-uniform distribution of actions conditioned on states, making it unnecessary to track probabilities q(\u00e3 | s). Policy gradient algorithms, especially in continuous control problems, are unstable, leading to the development of trust region methods and penalty on KL-divergence from a previous policy. These stabilizing techniques have not been applicable to algorithms like DDPG. The paper proposes a new formulation easily amenable to trust region optimization for algorithms like DDPG. It augments the objective with a penalty term involving the previous policy parameterization. The optimization is straightforward due to the analytical expression of KL-divergence between Gaussians. This method builds on previous work using Q-value functions to learn a stable policy, similar to methods that utilize gradient information from Q-value functions. The proposed method is a generalization of the deterministic policy gradient, with updates for training the Q-value approximator and policy mean being identical to DDPG. Stochastic Value Gradient (SVG) also trains stochastic policies but lacks updates for the covariance. The key difference is that SVG estimates the gradient with a noisy Monte Carlo sample, while the proposed method estimates the smoothed Q-value function. Covariance updates are essential for optimizing expected reward. The proposed method introduces expected policy gradients (EPG) as a generalization of DDPG, providing updates for the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. Unlike previous methods, EPG directly estimates the integral of the expected Q-value function, simplifying the updates and relying on neural network function approximators to estimate the smoothed Q-value function. The novel training scheme proposed in this paper introduces expected policy gradients (EPG) to update the mean and covariance of a stochastic Gaussian policy using neural network function approximators. This approach differs from recent advances in distributional RL by focusing on the averaged return of a distribution of actions rather than the distribution of returns of a single action. Further investigation is needed to explore the applicability of this new perspective to a wider class of policy distributions. Smoothie is a new RL algorithm that maintains a parameterized Q \u03c0 w and trains a Gaussian policy \u00b5 \u03b8 , \u03a3 \u03c6 using gradient and Hessian updates. Algorithm 1 outlines the procedure with inputs like environment, learning rates, discount factor, and target network lag. Smoothie is evaluated against DDPG, a baseline algorithm utilizing gradient information of a Q-value approximator. Smoothie, a new RL algorithm, utilizes gradient information of a Q-value approximator and is compared to DDPG, a standard algorithm known for good performance on continuous control benchmarks. To evaluate Smoothie, a simple synthetic task with a reward function of two Gaussians is used. Smoothie learns both the mean and variance of the policy, while DDPG only learns the mean. DDPG struggles to escape local optima, unlike Smoothie. Smoothie successfully solves the task by adjusting the policy mean and covariance during training. The smoothed reward function guides the policy towards the better Gaussian, allowing for efficient exploration and convergence to the global optimum. During training, Smoothie adjusts its policy mean and variance to escape local optima and converge to the global optimum. The reward function is smoothed to aid exploration. In standard continuous control benchmarks, feed forward neural networks are used for policy and Q-values, with the covariance parameterized as a diagonal. DDPG utilizes an Ornstein-Uhlenbeck process for exploration. Smoothie is competitive with DDPG, even outperforming it in tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient. Results compared in FIG2 after hyperparameter search for actor learning rate, critic learning rate, and reward scale. Smoothie performs competitively or better than DDPG across all tasks, with a slight advantage in Swimmer and Ant, and significant improvements in Hopper, Walker2d, and Humanoid. The average reward for Hopper is doubled, and the results for Humanoid are the best published for a method training on millions of environment steps. Smoothie outperforms TRPO in terms of performance with fewer environment steps. The introduction of a KL-penalty improves Smoothie's performance, especially on harder tasks. Smoothie addresses the instability in DDPG training and shows significant performance improvements in Hopper and Humanoid tasks without sacrificing sample efficiency. The new Q-value function, Q \u03c0, presented in Smoothie, allows for successful learning of both mean and covariance during training, matching or surpassing DDPG performance. Learning Q \u03c0 is more sensible than Q \u03c0 as it smooths the reward surface and has a direct relationship with the expected return objective. Future work should investigate these claims further. The new Q-value function, Q \u03c0 in Smoothie, improves training by learning mean and covariance, outperforming DDPG. It is more sensible than Q \u03c0, smoothing the reward surface and directly related to the expected return objective. Further research is needed to explore these claims. The specific identity mentioned can be derived using standard matrix calculus."
}