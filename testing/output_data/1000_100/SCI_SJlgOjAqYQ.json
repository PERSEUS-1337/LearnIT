{
    "title": "SJlgOjAqYQ",
    "content": "The study examines global translation-invariance in deep learning models trained on the MNIST dataset. Both convolutional and capsules neural networks show poor performance in this aspect, but data augmentation improves their performance. While the capsule network performs better on the MNIST testing dataset, convolutional neural networks generally excel in translation-invariance. CNNs are known for their state-of-the-art performance in computer vision tasks, attributed to reduced computation cost and generalization with local invariance in subsampling layers. Capsule networks aim for 'rate-coded' equivariance by including pose, color, lighting, and deformation of visual entities in groups of neurons. They show robustness in dealing with different viewpoints, but it is unclear if they can generalize for global translation invariance. Capsule networks represent viewpoint changes as linear effects on pose matrices between different layers. In this paper, a method is introduced to test global translation-invariance in convolutional and capsule neural network models trained on the MNIST dataset. The testing dataset consists of images generated by shifting the centre of mass of a digit one pixel at a time. The goal is to analyze translation invariance in deep learning models for better generalization. The study evaluates global translation-invariance in deep learning models using images of shifted digit centre of mass. Testing includes 2520 images covering all possible translations. Models are trained on MNIST dataset and tested on both MNIST and GTI datasets. The GTI dataset offers robustness against random noise and smaller training size compared to traditional methods. The CNN model used in the study has nine layers with specific configurations, including convolutional and fully connected layers. The total number of parameters is 361578, much smaller than Capsule networks. The activation function used is ReLU for all layers except the last one, which uses softmax. The CNN model has nine layers with ReLU activation except for the last layer using softmax. Adam optimizer with default parameters is used, and cross entropy loss is the objective function. Results show high accuracy on MNIST data but low accuracy on GTI testing dataset due to global translational invariance issues. Images with digit's center predicted correctly, while those at the corner are assigned incorrect classes. Max-pooling layers preserve local invariance in feature maps. The CNN model with nine layers and ReLU activation, trained on MNIST images, struggled to predict images shifted towards the corner in the GTI dataset, indicating 'place-code' equivariance. To improve performance, data augmentation was applied by shifting images from the center in x and y-direction, resulting in a 98.05% accuracy on the GTI testing dataset. This augmentation implies 'place-code' equivariance in CNN, activating neurons at the corner of feature maps when objects are at the edge. CapsNet with the same architecture was tested on the GTI dataset. The CapsNet model, with 8.2M parameters, was tested on the GTI dataset using the same architecture as the CNN model. Training involved Adam optimizer with exponential decay of learning rate. CapsNet showed robustness in viewpoint invariance but struggled with global invariance. Data augmentation in the MNIST training dataset improved CapsNet accuracy on the GTI dataset, especially with shifting images. The CapsNet model, with 8.2M parameters, was tested on the GTI dataset. CNN outperformed CapsNet on the GTI dataset, even with wider receptive fields in CapsNet's convolutional layers. CapsNet struggled with translational invariance, leading to lower performance. The introduction of a GTI testing dataset aimed to assess CNN and CapsNet's abilities. The CapsNet model, with 8.2M parameters, was tested on the GTI dataset to assess its ability to handle global translational invariance. Despite struggling with this aspect without data augmentation, CapsNet architecture shows potential over CNN due to capsules' ability to learn viewpoints regardless of information source. Testing involved comparing CNN and CapsNet accuracy on MNIST with varying levels of random shifting, proving CapsNet's potential for computer vision tasks with clear and labeled images."
}