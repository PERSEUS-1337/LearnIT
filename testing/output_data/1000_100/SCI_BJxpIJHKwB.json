{
    "title": "BJxpIJHKwB",
    "content": "Few shot image classification involves learning a classifier from limited labeled data. Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) addresses the challenge of generating diverse classification weights for query samples with very few training samples. AWGIM generates different weights for each query sample by allowing them to attend to the entire support set, ensuring adaptability by maximizing mutual information between weights and query/support data. This approach is a novel contribution in the field. This is the first attempt to unify information maximization into few shot learning, proving effective contributions in extensive experiments. AWGIM achieves state-of-the-art performance on benchmark datasets, addressing the challenge of learning from limited data in deep learning methods. Few shot learning enables deep models to learn from very few samples, with meta learning being the most popular approach for this problem. Meta learning is a popular approach for few shot learning, with methods like gradient-based and metric-based approaches. Weight generation methods have shown effectiveness in generating classification weights for different tasks based on limited labeled data. However, fixed classification weights for query samples within a task may not be optimal in few shot scenarios. In this work, Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) is introduced to address sub-optimal weights for query samples in few shot scenarios. AWGIM generates classification weights specifically for each query sample by maximizing mutual information between the generated weights and query, support data. This approach, the first to introduce Variational Information Maximization in few shot learning, minimizes computational overhead due to the nature of few shot problems. AWGIM minimizes computational overhead in few shot learning by maximizing mutual information between generated weights and query, support data. It shows state-of-the-art performance on benchmark datasets and includes detailed analysis of each component's contribution. Previous methods in few shot learning include meta learning approaches like optimal initialization and meta-learner LSTM. In few-shot learning, various methods have been explored to optimize activations and classification weights for better task performance. This includes metric-based methods for learning similarity metrics between query and support samples, considering spatial information or local image descriptors, generating classification weights directly, and using graph neural network denoising autoencoders. Fast weights generation from loss gradients for each task has also been proposed. Yu (2017) proposed generating \"fast weights\" from loss gradients for each task, not considering different weights for query examples or maximizing mutual information. Other few-shot classification methods include using generative models to create more data, closed-form solutions for classification, and integrating label propagation on a transductive graph. Attention mechanisms have shown success in computer vision and natural language processing by modeling interactions between queries and context. Self attention and cross attention are based on whether keys and queries point to the same entities. In this work, attention is used for few-shot image classification by maximizing mutual information. Mutual information measures the decrease of uncertainty in one random variable when another is known. It is defined as the Kullback-Leibler divergence between joint distribution and product of marginal distributions. Mutual information is maximized in applications like Generative Adversarial Networks and visual question generation. The attentive path equips query samples with task knowledge using an attention mechanism. Weight generator g generates classification weights specific for query samples, maximizing mutual information and forcing g to be sensitive to different query samples. The proposed model in this section describes the problem formulation and objective function for meta-learning in few-shot classification tasks. The model is trained episodically, with support and query sets used for prediction. The meta-loss is estimated during meta-training to optimize the model, and performance is evaluated during meta-testing on the query set. The proposed approach for meta-learning in few-shot classification tasks involves evaluating the performance on a query set Q with labeled support set S. The model learns transferable knowledge across tasks and quickly adapts to new tasks. The framework includes a feature extractor for image embeddings and a meta-learner to generate classification weights. Latent Embedding Optimization (LEO) is a related method for weight generation. LEO is a meta-learning approach that avoids updating high-dimensional weights in the inner loop by learning a lower-dimensional latent space. Unlike AWGIM, LEO does not require inner updates to adapt the model, instead focusing on generating classification weights efficiently. The proposed method involves using a feature extractor to process images in a sampled task, generating classification weights through contextual and attentive paths, and maximizing label prediction. LEO is compared to AWGIM, showing how LEO can be seen as a special case of AWGIM under certain conditions. The encoding process involves two paths - contextual and attentive, with the contextual path focusing on learning representations for the support set. Existing methods generate classification weights based on the support set only, which may not be optimal due to the difficulty of estimating accurate weights from limited labeled data. The attentive path introduces adaptive classification weights by utilizing a multi-head self-attention network on the support set to encode global task information. This approach allows for the weights to be aware of the task context and adaptive to different query samples. The cross attention network is applied on each query sample and task-aware support set to produce comprehensive representations. Multi-head attention is used to learn expressive representations from different subspaces. The tensors are replicated and concatenated to create latent representations for the support set. The cp\u2295ap method involves generating specific classification weights for each query sample based on task-context and individual adaptiveness. These weights are decoded by a generator and sampled from a Gaussian distribution. The prediction for query data is computed using the sampled classification weights, which are obtained by computing the mean value on K classification weights for each class. The support data is replicated and reshaped for prediction as well. The method involves generating query-specific classification weights based on task-context and adaptiveness. These weights are decoded by a generator and used for prediction. Reconstruction tasks are used as auxiliary tasks. The analysis is performed for one query sample. The classification weights are expected to be query-specific but experiments show that a weight generator conditioned only on the support data performs better. The method involves generating query-specific classification weights based on task-context and adaptiveness. However, experiments show that a weight generator conditioned only on the support data performs better, indicating a limitation in preserving information from the attentive path during weights generation. To address this, the proposal is to maximize mutual information between generated weights and support/query data using Variational Information Maximization. The method involves maximizing mutual information between generated weights and support/query data using Variational Information Maximization. This is achieved by approximating the true posterior distribution and maximizing the log likelihood of label for both support and query data. The method involves maximizing mutual information between generated weights and support/query data using Variational Information Maximization. This is achieved by approximating the true posterior distribution and maximizing the log likelihood of label for both support and query data. Gaussian distributions are used to approximate the mean, with L2 loss used to reconstruct x cp and x ap. Hyper-parameters \u03bb 1 , \u03bb 2 , \u03bb 3 trade-off different terms. The loss function includes terms to force generated classification weights to carry information about support data and specific query samples. Comparing to LEO, this method involves specific query samples, making reconstructing x ap possible. The proposed method involves maximizing mutual information between generated weights and support/query data using Variational Information Maximization. It includes approximating the true posterior distribution and maximizing the log likelihood of labels for support and query data. The encoding process in contextual path results in computational complexity O((N K) 2) due to self-attention. AWGIM avoids inner updates without compromising performance, reducing training and inference time significantly. Empirical evaluation is conducted on miniImageNet and tieredImageNet. We compare our model on miniImageNet and tieredImageNet datasets, subsets of ILSVRC-12 dataset. miniImageNet has 100 classes with 600 images each, while tieredImageNet has 608 classes and 779,165 images. We use LEO image features and a 28-layer Wide Residual Network for training. The authors trained a 28-layer Wide Residual Network on the meta-training set, representing each image with a 640-dimensional vector. They conducted N-way K-shot experiments, sampling classes for support and query sets. During meta-testing, they sampled tasks and reported average accuracy using TensorFlow. Various parameters were set for feature embeddings, attention module, and MLPs. The attention module is set with specific parameters for feature embeddings and MLPs. Results show accuracy comparison with other approaches on tieredImageNet. Table 2 shows accuracy comparison of different models on tieredImageNet tasks. MetaOptNet Resnets is used for optimization with specific hyperparameters and training iterations. The study compares the performance of AWGIM with various state-of-the-art methods on two datasets. Results for different models on tieredImageNet tasks are shown in Table 2, using MetaOptNet Resnets for optimization. The top half of the tables categorizes methods into metric-based, gradient-based, and graph-based approaches. AWGIM, a proposed classification weights generation method, outperforms all other methods on tieredImageNet and is competitive on miniImageNet. Comparison with LEO shows AWGIM's superiority in all settings. Ablation analysis in Table 3 highlights the effectiveness of AWGIM's components. Random shuffling of generated weights confirms their optimality. The effectiveness of AWGIM's components is highlighted in Table 3 through ablation analysis. The generator in LEO does not have an inner update, while the generator in AWGIM achieves similar or slightly better results, indicating the effectiveness of self-attention in modeling task-context. The generator in AWGIM, utilizing information maximization, outperforms LEO by slightly improving performance. Attention modules are replaced with 2-layer MLPs, termed \"MLP encoding\", achieving accuracy close to LEO even without attention. Ablation analysis on \u03bb 1 , \u03bb 2, and \u03bb 3 emphasizes the importance of maximizing information for task-contextual encoding. In AWGIM, the generator's accuracy is similar to \"generator conditioned on S only\", indicating that classification weights are not tailored for different query samples. Maximizing mutual information between weights and support is crucial, as \u03bb 1 = \u03bb 2 = 0 significantly degrades accuracy. The importance of support label prediction for information maximization is highlighted, with \u03bb 1 = 0 noticeably affecting performance. Classification weights are generated specifically for each query sample in AWGIM, as shown by shuffling weights between query samples to study their adaptability. In AWGIM, classification weights are tailored for each query sample. Shuffling weights between query samples shows adaptability, with random shuffle between classes degrading accuracy in limited support data scenarios. Random shuffle in class has minimal impact. With more labeled data, both shuffling methods yield similar results, worse than original weights, indicating more diverse and specific classification weights for each query sample. In this work, Attentive Weights Generation via Information Maximization (AWGIM) is introduced for few-shot image classification. AWGIM learns to generate optimal classification weights for each query sample by maximizing the mutual information between the generated weights and query, support data. This approach is the first to utilize mutual information techniques for few-shot learning and demonstrates state-of-the-art performance on benchmark datasets. Multi-head attention is utilized in the AWGIM approach for few-shot image classification. The attentive path involves attention mechanisms and a self-attention network to encode global task information. Classification weights are generated following a Gaussian distribution with diagonal covariance. Experimental analysis is conducted to evaluate the performance of the approach. During meta-training, AWGIM is adapted for few-shot regression tasks by setting N=1, modifying the loss to mean square error, and generating weight and bias parameters for a three-layer MLP. Sinusoidal and linear regression tasks are constructed, with multi-head attention improving performance in experiments on the miniImageNet dataset. In experiments comparing AWGIM with LEO on convergence speed, AWGIM outperforms LEO in terms of speed and accuracy on 5-way 1-shot miniImageNet tasks. AWGIM shows faster convergence and minimal computational overhead, with results plotted in Figure 3. Inference time measurements show AWGIM's efficiency compared to MLP encoding. The experiments on AWGIM show negligible overhead compared to MLP encoding due to small values of N, K, and |Q|. Visualization of classification weights using t-SNE reveals efficient processing by GPU. Inputs to the generator g are also plotted for comparison. The t-SNE visualization shows that the classification weights generated by the model are clustered closer for each class, indicating adaptability to different query samples. This aligns with the results showing distinct classification weights for query samples from different classes."
}