{
    "title": "HymuJz-A-",
    "content": "The text discusses the limitations of modern machine vision algorithms in learning visual relations, as demonstrated through experiments that strain convolutional neural networks. It also highlights the challenges faced by relational networks in solving visual question answering problems. The argument is made that feedback mechanisms like working memory and attention are crucial for abstract visual reasoning, inspired by the success of biological vision. Attention and computational components are crucial for abstract visual reasoning. A deep convolutional neural network accurately classified a complex image as a flute after training on millions of photographs. This network surpassed human accuracy on the ImageNet classification challenge. In contrast, a simple binary image with two curves is immediately intuitive to humans. The CNN accurately detects a flute in an image but struggles with recognizing the concept of \"sameness\" in another image. This difficulty in understanding simple relations has been overlooked in contemporary computer vision algorithms, overshadowed by the success of novel neural networks. The limitations of relational networks (RNs) in visual question answering (VQA) benchmarks have been overshadowed by their success on toy datasets. However, RNs face similar challenges as CNNs in tasks like recognizing visual relations, despite the widespread ability of animals to do so. There has been a lack of systematic exploration of these limitations in existing models. Previous work has shown that black-box classifiers and CNN architectures struggle with visual-relation problems, even with extensive training data. Results have been inconclusive, raising questions about the choice of hyperparameters in neural network implementations. In a systematic analysis, CNNs and visual reasoning networks struggle with visual-relation tasks, indicating limitations in current computer vision models. The need for brain mechanisms like working memory and attention is highlighted to enhance models for complex visual reasoning. Our contributions include a systematic performance analysis of CNN architectures on twenty-three SVRT problems, revealing a dichotomy between visual-relation problems. We introduce a novel visual-relation challenge demonstrating CNNs solve same-different tasks through rote memorization. Additionally, a simple modification of the sort-of-CLEVR challenge disrupts state-of-the-art relational network architectures. This prompts a reevaluation of visual question answering challenges and encourages inspiration from neuroscience and cognitive science for designing visual reasoning architectures. In a study on CNN architectures for visual-relation problems, different classes are distinguished by stimuli obeying an abstract rule. Positive examples in the problems feature specific patterns, while negative examples do not. Nine CNNs were trained on each of the twenty-three SVRT problems, with lower accuracies observed on same-different problems. The study tested CNNs of varying depths and receptive field sizes on visual-relation problems, showing lower accuracies on same-different problems compared to spatial-relation problems. Training was done on 2 million examples split into training and test sets for each of the twenty-three problems. The study tested CNNs on visual-relation problems with different depths and receptive field sizes, achieving lower accuracies on same-different (SD) compared to spatial-relation (SR) problems. The problems were sorted by accuracy and colored red for SD and blue for SR based on problem descriptions. CNNs performed much worse on SD problems than on SR problems. The study found that CNNs struggled more with same-different (SD) problems compared to spatial-relation (SR) problems. Some SD problems resulted in low accuracy, indicating a challenging task for CNNs. Larger networks performed better on SD problems, while smaller networks performed equally well on SR problems. This aligns with previous evidence of a visual-relation dichotomy. Experiment 1 supports previous findings that feedforward models struggle with visual-relation problems BID6 BID12 BID4 BID22. The SVRT challenge, while useful, has limitations in its sample selection. Sample PSVRT images illustrate Same-Different and Horizontal-Vertical categories. The images in the SVRT challenge are categorized as Horizontal or Vertical based on displacement orientation. Different problems in the challenge have unique image structures, making direct comparisons challenging. For example, Problem 2 requires one large and one small object, conflicting with Problem 1 where items must be identically-sized and positioned. The SVRT challenge presents difficulties in comparing visual-relation problems due to varying image structures and object requirements. The use of closed curves in SVRT images hinders quantification of image variability and task difficulty. To address these issues, a new dataset called PSVRT was created. To address issues with SVRT, a new dataset called PSVRT was created, consisting of two idealized problems: Spatial Relations (SR) and Same-Different (SD). SR classifies images based on horizontal or vertical arrangement, while SD classifies based on identical items. An image generator produces gray-scale images using square binary bit patterns on a blank background. Parameters control image variability: item size, image size, and number of items. The item size, image size, and number of items in a single image control image variability. The number of items also affects spatial variability. Parameters determine the SD and SR category labels based on item similarity and orientation. The Parametric SVRT test, or PSVRT, quantifies image dataset possibilities using permutations. Images are generated by sampling class labels and items based on similarity and orientation. The number of items and image size control variability in the images. In this experiment, the difficulty of learning PSVRT problems was examined by training a baseline architecture for different image variability parameters. The training sessions measured the number of examples needed to reach 95% accuracy, with TTA used as a measure of problem difficulty. The network was trained from scratch in all conditions to estimate the difficulty of fitting the problems. The study examined the difficulty of learning PSVRT problems by training a baseline architecture under different image variability parameters. Three sub-experiments were conducted by varying n, m, and k separately. The baseline CNN was trained from scratch in each condition with 20 million training images. The best-case result for each experimental condition was reported. The study trained a baseline CNN with varying image variability parameters and observed a strong dichotomy in learning curves, with sudden increases in training accuracy when reaching 95% accuracy. The network architecture included convolution and fully-connected layers, dropout, ADAM optimizer, and Xavier weight initialization. Additionally, experiments were repeated with a larger network size for comparison. The study observed a sudden rise in training accuracy to 100% after reaching 95%, termed the \"learning event\". This event almost always occurred within 20 million training images, leading to a bimodal distribution of final accuracy. No straining effect was found across image parameters in all random initializations. In SR, no straining effect was found across image parameters in all random initializations. However, in SD, a significant straining effect was observed with image size and number of items. Increasing image size led to longer time to accuracy and decreased likelihood of learning. The network struggled to learn with 3 or more items in an image. The relaxation of the same-different rule in the experiment quadrupled the number of matching images, causing a strain on CNNs due to increased image variability with larger sizes and more items. This strain reflects how image size and item number contribute to variability exponentially. Increasing the number of items leads to an exponential-rate increase in image variability for CNNs, with no visible straining effect from increasing item size. Learnability remains stable over a range of item sizes, suggesting the possibility of feedforward feature detectors that can generalize to coordinated item variability. The study found that CNNs do not learn invariant rule-detectors but rather a collection of templates tailored for specific data sets. The features learned by CNNs are sensitive to image variations and not invariant to irrelevant details, leading to decreased performance with increasing image variability. The Relational Network (RN) is an architecture designed to detect visual relations and outperforms baseline CNN on various visual reasoning tasks, including \"sort-of-CLEVR\" VQA task with simple 2D items. The RN was trained to answer relational and non-relational questions using shapes and colors. The sort-of-CLEVR task has limitations due to low item variability and lack of concept learning. To address these issues, the model was trained on a two-item same-different task and PSVRT stimuli to measure performance without these handicaps. The study used relational networks to measure the ability of an RN to transfer the concept of same-different from a training set to a novel set of objects. The architecture details included a convolutional network with four layers and a relational network with multiple layers. The study utilized a CNN+RN architecture with ReLu activations, dropout, softmax output, and cross-entropy loss for training. The model reproduced results from BID22 on the sort-of-CLEVR task. Twelve versions of the dataset were created, each missing a color+shape combination. The CNN+RN architecture was trained to detect sameness in scenes with two items. The CNN+RN architecture was trained to detect sameness in scenes with two items, reaching 95% training accuracy. However, it did not generalize well to left-out color+shape combinations on the sort-of-CLEVR task, with validation accuracy remaining at chance levels. The CNN+RN architecture trained on a two-item same-different problem struggled to generalize to left-out color+shape combinations on the sort-of-CLEVR task. Replacing simple shapes with PSVRT bit patterns did not significantly improve performance, behaving similarly to a vanilla CNN after training on 20M images. The CNN+RN architecture showed a leap in accuracy to over 95% for images sized 120 or below, but did not learn for sizes 150 and 180. This cutoff point may be due to the representational capacity of the RN architecture. Learning templates for arrangements of objects becomes intractable due to the combinatorial explosion in the number of templates needed, indicating that visual-relation problems can exceed the capacity of CNNs. The limitations of representing complex structures with feedforward networks have been recognized by cognitive scientists. Current computer vision research overlooks the ability of biological visual systems to excel at detecting relations. Humans can learn and generalize complicated visual rules with just a few examples, unlike CNNs. For example, humans could learn a complex rule involving shapes from only 6 examples, while the best performing network struggled with the same problem. Visual reasoning ability is not limited to humans, as birds and primates can also be trained to recognize same-different relations. A study with ducklings showed they could quickly learn abstract concepts of same and different from a single example or possess these concepts innately. The behavior of ducklings in learning abstract concepts contrasts with the CNN+RN of Experiment 3, which struggled to transfer the concept of same-different to novel objects. There is evidence that visual-relation detection relies on reentrant/feedback signals beyond feedforward processes. Despite feedback connections in the visual cortex, certain visual recognition tasks can be achieved with minimal cortical feedback. The processing of spatial relations between objects in cluttered scenes requires attention and working memory, as suggested by neuroscience evidence. This goes beyond feedforward processes in the visual cortex and involves feedback signals. The role of working memory in prefrontal and premotor cortices is highlighted when solving Raven's progressive matrices, which require spatial and same-different reasoning. Attention and working memory play a computational role in detecting visual relations by allowing flexible representations to be dynamically constructed at run-time, avoiding capacity overload in feedforward neural networks. Humans can easily detect same objects under transformations or spatial relations between objects. Humans can effortlessly construct structured descriptions about the visual world around them, showing superior ability over modern computers in detecting visual relations. Exploration of attentional and mnemonic mechanisms is crucial for computational understanding of visual reasoning."
}