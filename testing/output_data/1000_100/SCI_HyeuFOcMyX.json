{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences in language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance. By manipulating the planner codes, translations with different structures can be achieved. This approach helps ensure grammatical and logical correctness in speech generation. In neural machine translation, planning ahead is crucial for generating coherent sentences. Unlike humans, NMT models lack a planning phase, leading to uncertainty in word prediction. To address this, researchers aim to incorporate a planning phase to guide sentence structure before decoding actual words. In neural machine translation, planning ahead is essential for coherent sentence generation. Researchers propose inserting planner codes at the beginning of output sentences to govern their structure. These codes help disambiguate uncertain information, such as word order, improving the overall translation process. In this work, simplified POS tags are used to annotate sentence structure. Planner codes are learned through a network that reconstructs sentences. Experiments show improved translation performance with structural planning, allowing control over output sentence structure. Structural annotation is extracted by simplifying POS tags, and a code learning model is explained. The goal is to reduce uncertainty in decoding by providing a \"big picture\" structural annotation for sentences. In this work, a two-step process is used to extract coarse structural annotations from simplified POS tags of target sentences. The goal is to reduce uncertainty in sentence structure decoding during translation. Planner codes are also learned to aid in controlling the output structure. The text discusses using codes C Y to remove uncertainty in sentence structure decoding during translation. The architecture of the code learning model involves computing discrete codes C 1 , .., C N based on simplified POS tags S 1 , ..., S T. These codes are then discretized into approximated one-hot vectors using the Gumbel-Softmax trick. The information from X and C is combined to initialize a decoder LSTM for predicting the tag sequence S 1 , ..., S T. The code learning model architecture involves computing planner codes C for target sentences in training data using an encoder. These planner codes are then connected to target sentences with an \"eoc\" token to train a regular NMT model. Beam search is used during decoding to search for planner codes before emitting real sentences. The planner codes are used with beam search during decoding sentences in NMT models. Recent methods aim to improve syntactic correctness, such as BID19 restricting the search space and BID2 using a multi-task approach. Other works incorporate syntactic structures explicitly, like interleaving CCG supertags in the target side. Aharoni and Goldberg (2017) train NMT models to generate linearized constituent parse trees. BID20 proposes a model to generate words and parse actions simultaneously. The curr_chunk discusses the evaluation of models on translation tasks using code learning approaches, with details on the datasets used and the training process. The models are trained with specific settings and tested on different language pairs. The evaluation of models on translation tasks using code learning approaches involves testing different settings of code length N and code types K. There is a trade-off between accurately reconstructing the source sentence and guessing the correct code. The setting of N = 2, K = 4 is found to have a balanced trade-off. The NMT model used has 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with specific hidden layer units for different tasks. Key-Value Attention is applied in the model. The NMT models for translation tasks use Key-Value Attention in the first decoder layer and a residual connection BID3 to combine hidden states. Dropout with a rate of 0.2 is applied outside of the recurrent function. The models are trained using the NAG optimizer with a learning rate of 0.25, annealed by a factor of 10 after 20K iterations without improvement. Conditioning word prediction on generated planner codes improves translation performance. However, applying greedy search on the JaEn dataset results in a lower BLEU score compared to the baseline. Beam search followed by greedy search was also attempted, but the results were inconclusive. The study explores the importance of simultaneously exploring diverse candidates with different structures in Ja-En translation tasks. Planning ahead to generate more varied candidates improves beam search but not greedy search. Manual selection of planner codes can also be beneficial. The model produces different translations based on different planner codes. The study discusses the process of AP code in Ja-En translation tasks, showing how manipulating planner codes can result in translations with diverse structures. The distribution of assigned planner codes for English sentences in the dataset is illustrated, indicating a skewed distribution. In this paper, a planning phase is added to neural machine translation to generate planner codes that control the output sentence structure. An end-to-end neural network with a discretization bottleneck is designed to predict simplified POS tags of target sentences. Experimental results show that this method improves translation performance and confirms the effectiveness of planner codes. The planning phase in neural machine translation improves translation performance by generating planner codes to control sentence structure. This framework can be extended to plan other latent factors like sentiment or topic."
}