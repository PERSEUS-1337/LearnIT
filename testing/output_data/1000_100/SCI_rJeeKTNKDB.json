{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive, improving biochemical properties by modifying compounds through a graph-to-graph translation problem. The task involves translating molecular graphs into better forms, similar to machine translation. The model is trained on a corpus of molecular pairs where the paraphrase has improved chemical properties. Generating molecular graphs is challenging due to the vast candidate space and complex dependencies. Prior work utilized valid chemical substructures to build graphs, representing molecules as junction trees. The model is trained on a corpus of molecular pairs to improve chemical properties by generating molecular graphs using valid chemical substructures. A multi-resolution, hierarchically coupled encoder-decoder approach is proposed for graph generation, interleaving the prediction of substructure components with their attachments in an auto-regressive manner. The encoder-decoder approach for graph generation involves predicting substructure components and their attachments in a hierarchical manner. The encoding of molecules occurs at three levels to match the decoding process, with graph convolution supporting attachment and substructure predictions. The decoding process is efficient, breaking down generation steps into smaller hierarchical steps to avoid combinatorial explosion. Our method extends the encoder-decoder approach for graph generation by handling conditional translation and interleaving tree and graph decoding steps to avoid generating invalid junction trees. Additionally, we propose an autoregressive decoder to address inconsistent local substructure attachments during training. The new model is evaluated on multiple molecular optimization tasks, outperforming previous state-of-the-art methods. Our model significantly outperforms previous state-of-the-art graph generation methods in discovering molecules with desired properties, showing improvements on QED and DRD2 optimization tasks. It runs 6.3 times faster during decoding and benefits from hierarchical decoding and multi-resolution encoding. Additionally, conditional translation can generalize even with minimal training data on molecular pairs with desired target property combinations. Methods generate molecules based on their SMILES strings. Generative models output adjacency matrices and node labels of graphs at once or decode molecules sequentially node by node. Some approaches use reinforcement learning or hypergraph grammar for molecule generation. Our work is related to generating molecules based on substructures, adopting a two-stage procedure for realizing graphs. Our method adopts a two-stage procedure for realizing graphs. The first step generates a junction tree with substructures as nodes, capturing their coarse relative arrangements. The second step resolves the full graph by specifying how the substructures should be attached to each other. Our approach jointly predicts substructures and their attachments with an autoregressive decoder, unlike previous methods that introduced local independence assumptions and applied steps stage-wise during decoding. Our method represents molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees, closely related to previous works that learn to represent graphs hierarchically. Defferrard et al. (2016) utilized graph coarsening algorithms for constructing multiple layers of graph hierarchy, while Ying et al. (2018) focused on predicting attachments in a hierarchical manner. Our method focuses on graph generation, encoding molecules into multiple sets of vectors for different resolutions. These vectors are aggregated by decoder attention modules in each generation step. The goal is to learn a function that maps a molecule into another with improved chemical properties, using an encoder-decoder with neural attention. The decoder adds substructures and determines their attachment in each step of graph generation. The model focuses on hierarchical graph generation for molecules, representing them with a hierarchical graph with substructure, attachment, and atom layers. Nodes in the graph are encoded into substructure, attachment, and atom vectors for prediction steps. The model focuses on hierarchical graph generation for molecules, representing them with substructure, attachment, and atom vectors for prediction steps in the decoder. Substructures are defined as subgraphs of molecules, including rings and bonds, with a vocabulary constructed from the training set. The substructure tree characterizes the relationship between substructures in the molecular graph. The model generates molecules using a substructure tree, predicting new substructures and their attachments in a depth-first order. It focuses on hierarchical graph generation and topological predictions for new structures. The model predicts new substructures and their attachments in a depth-first order using topological, substructure, and attachment predictions. The model predicts atom pairs in two steps: predicting attaching atoms to form a vocabulary for each substructure, and finding corresponding atoms in the substructure. The predictions give an autoregressive factorization of the distribution over the next substructure and its attachment. During training, teacher forcing is applied to the generation process, with a depth-first traversal over the ground truth substructure tree. The attachment enumeration is manageable due to small substructure sizes. The encoder represents a molecule as a hierarchical graph with three components: atom layer, bond layer, and substructure layer. The atom layer in the molecule is associated with labels indicating atom type and charge, while the bond layer shows bond types. The attachment layer represents attachment configurations of substructures in the molecule's substructure tree, providing information for attachment prediction. The substructure layer mirrors the substructure tree. The substructure layer provides essential information for substructure prediction in the decoding process. Edges connect atoms and substructures between different layers to propagate information. A hierarchical graph HX is created for molecule X, encoded by a hierarchical message passing network (MPN). The encoder contains three MPNs for each layer, using the MPN architecture from Jin et al. (2019). The MPN encoding process is denoted as MPN \u03c8(\u00b7) with parameter \u03c8. The atom layer of HX is encoded first, with inputs as embedding vectors. The atom layer of molecule X (HgX) is encoded using a hierarchical message passing network (MPN). The network propagates message vectors between atoms for T iterations to output atom representations. The attachment layer in the MPN combines atom embeddings and message vectors, while the substructure layer computes substructure representations through message passing. The hierarchical encoder in the MPN computes input features for nodes and runs message passing to obtain substructure representations. The decoder also uses the hierarchical MPN architecture to encode the hierarchical graph at each step, predicting future nodes and edges based on previously generated outputs. The training set includes molecular pairs where compounds can have multiple outputs for property improvement. The model generates diverse outputs by using a variational translation model with an additional input z, sampled from a Gaussian prior during testing. Training is done using variational inference, sampling z from the posterior distribution. The structural changes from molecule X to Y are summarized in a vector \u03b4 X,Y, and z is computed and sampled using a reparameterization trick. The latent code z is then passed to the decoder for output generation. The model uses a variational translation approach with an additional input z for diverse output generation. During testing, users can specify criteria g X,Y to control the outcome of the translation process. This allows for conditional translation where desired properties can be changed. The model allows users to specify criteria during testing to control the outcome of the translation process. It follows an experimental design by Jin et al. (2019) and evaluates the translation model on single-property optimization tasks. A novel conditional optimization task is constructed where the desired criteria are inputted to the translation process to prevent arbitrary compound generation. The molecular similarity between input X and output Y must be above a certain threshold at test time. The model allows users to specify criteria during testing to control the outcome of the translation process. It follows an experimental design by Jin et al. (2019) and evaluates the translation model on single-property optimization tasks. A novel conditional optimization task is constructed where the desired criteria are inputted to the translation process to prevent arbitrary compound generation. The model is trained under an unconditional setting for tasks involving logP optimization, where the goal is to improve drug-likeness and activity levels of compounds. Different criteria can be encoded as a vector g to train the model under conditional translation setup. The evaluation metrics for the model include translation accuracy and diversity. Each test molecule is translated multiple times with different latent codes, and the final translation is selected based on property improvement and similarity constraints. The success rate of translation is reported for different tasks, along with the average pairwise Tanimoto distance between successfully translated compounds. The HierG2G method is compared against baselines like GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. A direct comparison is made with an atom-based translation model (AtomG2G) for molecule generation. AtomG2G is a molecule generation model that predicts new atoms and bond types in an autoregressive manner, achieving state-of-the-art results on translation tasks compared to JTNN. The model's autoregressive decoder allows for more expressive mappings and higher output diversity. Our hierarchical model outperforms AtomG2G on three datasets, with over 10% improvement on the DRD2 task. When compared to other translation methods like Seq2Seq, JTNN, and AtomG2G, our model shows higher translation accuracy and output diversity. Training our model on a specific criteria resulted in a low success rate, but transferring knowledge from other pairs improved the success rate significantly. The ablation study conducted on the hierarchical model focused on the impact of different architecture choices on translation accuracy. By replacing the hierarchical decoder with an atom-based decoder, a decrease in performance was observed on the QED and DRD2 tasks. Additionally, reducing the number of hierarchies in the encoder and decoder while maintaining hierarchical decoding process also affected translation accuracy. The study suggests that the DRD2 task benefits more from structure-based decoding due to the importance of specific functional groups in biological target binding. The hierarchical decoding process was analyzed through ablation studies, showing a slight drop in translation accuracy when removing the top substructure layer. Further removal of the attachment layer significantly degraded performance on both datasets. Despite a decrease in translation performance when using the original GRU MPN instead of LSTM MPN, the model still outperformed JTNN. The developed hierarchical graph-to-graph translation model generates molecular graphs using chemical substructures as building blocks, with a focus on being fully autoregressive and coherent. The model presented in the study outperforms previous models in various settings by utilizing a fully autoregressive approach to learn coherent multi-resolution representations. The message passing network MPN \u03c8 over graph H is defined, along with an attention layer using a bilinear attention function. The AtomG2G decoding process is illustrated, showing the prediction of new atoms and bond types in a sequential manner. The model predicts bond types between nodes in Q sequentially for |Q| steps and adds new atoms to the queue. AtomG2G is an atom-based translation method comparable to HierG2G. Training set sizes and substructure vocabulary are listed for each dataset. Multi-property optimization combines QED and DRD2 tasks. Test set includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters are set for HierG2G's hidden layer dimension. For HierG2G, the hidden layer dimension is set to 270, embedding layer dimension to 200, latent code dimension to 8, and KL regularization weight to 0.3. AtomG2G has hidden and embedding layer dimensions set to 400, with \u03bb KL = 0.3 and T = 20 message passing iterations. CG-VAE models were used for molecule generation and property prediction tasks. Three CG-VAE models were trained for logP, QED, and DRD2 optimization tasks. At test time, compounds are translated following a specific procedure. The procedure for translating compounds involves embedding them into a latent representation and performing gradient ascent to maximize property scores. Keeping the KL regularization weight low is crucial for meaningful results. Ablation studies were conducted by changing the decoder to an atom-based decoder and modifying the input of the decoder attention. The hidden layer and embedding layer dimensions were set to 300 to match the requirements. In experiments, the number of hierarchies in the encoder and decoder MPN was reduced. In the two-layer model, molecules are represented by c X = c G X \u222a c A X, with predictions based on hidden vector h A k. In the one-layer model, molecules are represented by c X = c G X, with predictions based on atom vectors v\u2208S k h v. The hidden layer dimension is adjusted to match the original model size."
}