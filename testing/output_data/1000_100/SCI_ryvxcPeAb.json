{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, adversarial perturbations are decomposed into model-specific and data-dependent components, with the latter contributing mainly to transferability. Adversarial examples are crafted using noise reduced gradient (NRG) to approximate the data-dependent component, enhancing transferability significantly across various ImageNet classification models. Low-capacity models show stronger attack capability compared to high-capacity models with similar test performance. These findings offer a principled approach to constructing successful adversarial examples and insights for developing effective defense strategies against black-box attacks in the era of neural networks. Recent works have shown that adversaries can manipulate neural network models to produce incorrect outputs, creating adversarial examples. Understanding this phenomenon and effectively defending against such attacks are still open questions. Adversarial examples can transfer across different models, posing a threat to black-box systems. Adversarial vulnerability in neural networks was first studied in BID15, with different methods proposed to generate adversarial examples. BID5 argued that linear nature and high dimensionality are the primary causes of adversarial instability, leading to the development of the fast gradient sign method (FGSM). BID11 introduced the DeepFool method based on iterative linearization of the classifier. BID8 analyzed the transferability of adversarial examples and proposed ensemble-based approaches for black-box attacks. BID6 and BID16 showed that iterative gradient sign method is effective for white-box attacks but not for black-box attacks. High-confidence adversarial examples were demonstrated in BID3. In BID3, high-confidence adversarial examples were shown to have stronger transferability in black-box attacks. Various defense mechanisms have been proposed, such as defensive distillation (BID12) and adversarial training (BID5, BID6, BID16). BID9 used image transformations to mitigate adversarial perturbations. Some works focused on detecting adversarial examples (BID7, BID4), but they can be easily overcome by designing stronger adversarial examples (BID3, BID0). This work explains the transferability of adversarial examples to enhance black-box attacks. The transferability of adversarial examples is enhanced by utilizing the data-dependent component of gradient, known as the noise-reduced gradient (NRG) method. This approach focuses on the smooth and ground truth approximating part of the perturbation, leading to improved performance in constructing adversarial examples across different models. Benchmarking on the ImageNet validation set validates the effectiveness of the proposed noise reduced gradient method. The proposed noise reduced gradient significantly boosts success rates of black-box attacks on ImageNet validation set. Model-specific factors like capacity and accuracy influence attack success, with higher accuracy and lower capacity models showing stronger capability. This phenomenon is explained by transferability, offering guidance for attacking unseen models. The model function f : R d \u2192 R K is defined by minimizing empirical risk over the training set. In high-dimensional spaces, deep neural networks are vulnerable to imperceptible adversarial perturbations. Adversarial examples exist in various models like SVM and decision trees. Non-targeted attacks aim to misclassify x, while targeted attacks aim to produce a specific wrong label. Black-box attacks involve attacking models without knowledge of their internal workings. In black-box attacks, the adversary has no knowledge of the target model and cannot query it directly. Instead, they create adversarial examples on a local model and use them to fool the target model. Crafting adversarial perturbations involves optimizing a loss function to measure the discrepancy between prediction and ground truth. In image data, the constraint is x adv \u2208 [0, 255] d, with d as the number of pixels. BID2 introduced a loss function that manipulates output logit directly. Distortion is measured using human eyes, but commonly \u221e and 2 norms are used. Ensemble-based approaches suggest using a large ensemble of source models to improve adversarial examples. The objective involves averaging predicted probabilities of each model with ensemble weights. In image data, the constraint is x adv \u2208 [0, 255] d, with d as the number of pixels. BID2 introduced a loss function that manipulates output logit directly. Distortion is measured using human eyes, but commonly \u221e and 2 norms are used. Ensemble-based approaches suggest using a large ensemble of source models to improve adversarial examples. The objective involves averaging predicted probabilities of each model with ensemble weights. The normalized-gradient based optimizer is used to solve non-targeted and targeted attacks, with various optimizers available. Fast Gradient Based Method (FGBM) and Iterative Gradient Method are two approaches empirically shown to be effective for solving adversarial attacks. The transferability of adversarial examples between models is crucial for black-box attacks and defense strategies. Research suggests that similarity in decision boundaries between source and target models, especially in the direction of transferable adversarial examples, enables this transfer. Additionally, it is proposed that transferable adversarial examples occupy a contiguous subspace. The transferability of adversarial examples between models is influenced by the similarity in decision boundaries. Models A and B, with high performance on the same dataset, learn a similar function on the data manifold. Perturbations can be decomposed into data-dependent and model-specific components, with the former contributing to transferability between models. The transferability of adversarial examples between models is influenced by the similarity in decision boundaries. Perturbations can be decomposed into data-dependent and model-specific components, with the former contributing to transferability between models. The model-specific component contributes little to the transfer from one model to another. The NRG method aims to increase success rates of black-box adversarial attacks by reducing model-specific noise in the data-dependent component. This noise-reduced gradient approach removes noisy model-specific information to approximate the data-dependent component, enhancing transferability between models. The noise-reduced gradient (NRG) method reduces model-specific noise in the data-dependent component to improve success rates of black-box adversarial attacks. By using NRG instead of the ordinary gradient, the optimizer is driven towards more data-dependent solutions, enhancing transferability between models. The noise-reduced gradient (NRG) method, also known as nr-IGSM, is a technique used to improve success rates of black-box adversarial attacks by reducing model-specific noise in the data-dependent component. It is a special case of the noise-reduced fast gradient sign method (nr-FGSM) and can be applied to any general optimizer. To analyze its effectiveness in enhancing transferability, NRG is tested on ImageNet classification models using the ImageNet ILSVRC2012 validation set. For each attack experiment, 5,000 images are randomly selected that can be correctly recognized by all models. For targeted attack experiments, various pre-trained models from PyTorch are used, including resnet and vgg models. Adversarial examples are generated and evaluated based on Top-1 success rate for targeted attacks. The performance of targeted attacks is evaluated based on Top-1 success rate. The cross entropy 2 is chosen as the loss function, and distortion is measured using \u221e norm and scaled 2 norm. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with fast gradient based methods, showing that nr-FGSM outperforms original FGSM consistently. The noise-reduced gradient technique, demonstrated by nr-FGSM, outperforms the original FGSM consistently and dramatically. Even white-box attacks show improvements, indicating the effectiveness of noise-reduced gradient for enhancing transferability. Comparing IGSM and nr-IGSM under the same number of gradient calculations, nr-IGSM generates adversarial examples that transfer more easily, guiding the optimizer to explore more data-dependent solutions. Large models like resnet152 are more robust to adversarial transfer compared to small models. Transfer among models with similar architectures is influenced by model-specific components. IGSM generally generates stronger adversarial examples than FGSM, except for attacks against alexnet. This contradicts previous claims but aligns with the idea that higher confidence adversarial examples are more likely to transfer to target models. Inappropriate hyperparameters may affect transferability. The choice of hyperparameters in BID6 is too small, leading to underfitting. The alexnet model differs significantly from source models, causing IGSM to overfit more than FGSM. Trust in the objective function is questioned as it may lead to overfitting. The noise reduced gradient technique removes model-specific information, improving cross-model generalization. The NRG method is applied to ensemble-based approaches. In this section, the NRG method is applied to ensemble-based approaches for evaluating model ensembles with a reduced set of 1,000 images. Non-targeted attacks using FGSM, IGSM, and their noise reduced versions are tested, with IGSM showing saturated success rates. For targeted attacks, generating adversarial examples with specific labels is challenging, and single-model approaches are ineffective. Targeted attacks were not considered in the previous section. Targeted adversarial examples are sensitive to the step size used in optimization procedures. A larger step size is necessary for generating strong targeted adversarial examples. NRG methods outperform normal methods by a large margin in both targeted and non-targeted attacks. Top-5 success rates of ensemble-based approaches are reported in Table 3. In this section, the sensitivity of hyper parameters m and \u03c3 in NRG methods for black-box attacks is explored using the nr-FGSM approach. Larger m leads to higher fooling rates due to better gradient estimation, while an optimal value of \u03c3 improves performance. Extremely small \u03c3 is ineffective in removing noisy model-specific information. In this experiment, the optimal \u03c3 for adversarial perturbations varies between different source models, being around 15 for resnet18 and 20 for densenet161. The study also examines the robustness of adversarial examples to image transformations like rotation and scaling, which is crucial for real-world applications. Destruction rate is used to quantify the impact of transformations on adversarial images. The study compares the robustness of adversarial examples generated by NRG methods versus vanilla methods using Densenet121 and Resnet34 models. Various image transformations are considered, and the decision boundaries of different models are analyzed to understand the performance differences. Nine target models are evaluated, and the perturbation direction is estimated for each model. The study compares the robustness of adversarial examples generated by NRG methods versus vanilla methods using Densenet121 and Resnet34 models. The perturbation direction is analyzed for different models, showing sensitivity along sign \u2207f and sign (\u2207f \u22a5 ). Removing \u2207f \u22a5 penalizes the optimizer along the model-specific direction, preventing source model-overfitting solutions that transfer poorly to other target models. The study shows that adversarial transfer varies among different models, with larger distances for complex models like resnet152 compared to smaller models like resnet50. Adversarial examples crafted from alexnet generalize poorly across models, while attacks from densenet121 consistently perform well on various target models. This suggests that different models exhibit varying performances in attacking the same target model. Different models can exhibit different performances in attacking the same target model. The study aims to find the principle behind this phenomenon to guide the selection of a better local model for generating adversarial examples to attack a remote black-box system. Results show that models with smaller test error and lower capacity have stronger attack capabilities. The study aims to explain the phenomenon of stronger attack capabilities in models with smaller test error and lower capacity. Adversarial perturbations can be decomposed into two components: model-specific factors and data-dependent factors. The study explains that adversarial perturbations have model-specific and data-dependent components, with the latter contributing more to transferability. The proposed noise-reduced gradient (NRG) methods are more effective for crafting adversarial examples. Models with lower capacity and higher test accuracy are better for black-box attacks. Future research will focus on combining NRG-based methods with adversarial training for defense. The data-dependent component of transferability suggests defensibility against black-box attacks, while white-box attacks are more challenging to defend against due to their high-dimensional nature. In future research, stable features for transfer learning can be learned by incorporating the NRG strategy to reduce model-specific noise. The influence of hyperparameters on the quality of adversarial examples generated using IGSM for targeted black-box attacks is explored. The success rates are evaluated on 1,000 randomly selected images using resnet152 and vgg16 bn as target models. The optimal step size \u03b1 is crucial, with very large or small values yielding harm. The influence of step size on attack performance is explored, with both too large and too small values being detrimental. Small step sizes may lead to worse performance with more iterations, while larger step sizes encourage exploration of model-independent areas. Additional experiments on MNIST dataset with different model depths show varying success rates in cross-model attacks. The results in TAB5 show that low-capacity models have stronger attack capability than large-capacity models. The success rates of FGSM and IGSM attacks against resnet152 are shown in Figure 9, with \u03b5 = 15 chosen as the distortion. The contribution from architecture similarities is not considered in the analysis."
}