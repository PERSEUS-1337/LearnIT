{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. Extensive empirical investigations on the NSynth dataset show that GANs outperform WaveNet baselines in generating audio faster and more efficiently. Neural audio synthesis faces challenges in modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine-scale details but require external signals for global structure, leading to slow sampling speeds. Efforts to speed up generation introduce overhead, such as training secondary networks or customizing low-level kernels. Generative Adversarial Networks (GANs) have been successful in generating high resolution images efficiently. However, adapting GAN architectures for audio synthesis has not achieved the same level of fidelity as in images. Sound consists of locally coherent waves, and there is potential for audio GANs to unlock domain transformations similar to those seen in images. Frame-based techniques for audio waveform estimation involve transposed convolutions or STFTs with a specific frame size and stride. The alignment between audio periodicity and output stride precesses in time, making it challenging to preserve phase coherence. STFT unwraps phase over a 2\u03c0 boundary to calculate instantaneous radial frequency, showing the constant relationship between audio and frame frequency. The NSynth dataset contains individual notes from musical instruments aligned and cropped to reduce variance, similar to the CelebA dataset for faces. GAN researchers have made progress in image modeling by starting with focused datasets and gradually expanding to less constrained domains. The NSynth dataset focuses on individual notes from musical instruments, aligned and cropped for fine-scale details like timbre and fidelity. Various approaches have been explored, including autoregressive WaveNet autoencoders and adversarial training for noncausal convolutional generation in audio waveforms. Convolutional filters trained on audio waveforms learn logarithmically-scaled frequency selective filter banks. Maintaining regularity of periodic signals over short to intermediate timescales is crucial for human perception. Synthesis networks face challenges in learning frequency and phase combinations to produce coherent waveforms. Phase precession is observed in short-time Fourier transform (STFT) and convolutional networks with overlapping filterbanks. A new approach inspired by the phase vocoder BID7 involves unwrapping the phase of a pure tone to generate coherent waveforms. The derivative of the unwrapped phase represents the instantaneous angular frequency, a measure of true signal oscillation. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Key findings include generating log-magnitude spectrograms and phases directly with GANs for more coherent waveforms, estimating IF spectra for even more coherence, and the importance of preventing harmonics from overlapping by increasing STFT frame size and switching to mel frequency scale. The study focuses on using GANs to generate coherent audio by improving performance through size and switching to mel frequency scale. GANs outperform WaveNet on the NSynth dataset, allowing for faster generation. Global conditioning on latent and pitch vectors enables smooth timbre interpolation and consistent timbral identity across pitch. The NSynth dataset contains 300,000 musical notes from 1,000 instruments with labels for pitch, velocity, instrument, and acoustic qualities. Each sample is four seconds long and sampled at 16kHz. The study focuses on using GANs to generate coherent audio by improving performance through size and switching to mel frequency scale. Each sample is four seconds long, and sampled at 16kHz, giving 64,000 dimensions. Human evaluations on audio quality were included, training on acoustic instruments and fundamental pitches ranging from MIDI 24-84. The dataset contains 70,379 examples from strings, brass, woodwinds, and mallets. A new test/train 80/20 split was created from shuffled data. Progressive training methods of BID16 were adapted to generate audio spectra. Hyperparameter configurations and learning rates were explored. The model samples a random vector from a Gaussian distribution and uses transposed convolutions to generate output data. A discriminator network estimates the difference between real and generated distributions. Gradient penalty and pixel normalization are used for continuity. Progressive training shows better convergence time and sample diversity. The method involves conditioning on additional information. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. To encourage the generator to use the pitch information, an auxiliary classification BID24 loss is added to the discriminator. Spectral representations are computed using STFT magnitudes and phase angles with specific parameters. The resulting \"image\" size is (256, 512, 2) with magnitude and phase channels. Magnitudes are scaled to match the tanh output nonlinearity of the generator network. The phase angle and frequency resolution of spectral images are adjusted to match the generator network's nonlinearity. Different variants include \"phase\" models, \"instantaneous frequency\" models, high frequency resolution variants, and \"IF-Mel\" variants. Converting back to linear STFTs involves using an approximate inverse linear transformation. Converting back to linear STFTs involves using an approximate inverse linear transformation. WaveGAN, the current state of the art in waveform generation with GANs, is adapted to accept pitch conditioning and retrained on a subset of the NSynth dataset. Strong WaveNet baselines are created by adapting prior work on the NSynth dataset. We create strong WaveNet baselines by adapting the architecture to accept pitch conditioning signals. Evaluating generative models is challenging due to the subjective nature of audio quality, so we use human evaluation as a gold standard. We used Amazon Mechanical Turk to conduct a comparison test on models presented in TAB0 for synthesizing coherent waveforms. Participants evaluated audio quality on a Likert scale, providing 3600 ratings. The diversity of generated examples was measured using the Number of Statistically-Different Bins (NDB) metric. The training examples are clustered into k = 50 Voronoi cells by k-means in log-spectrogram space. NDB is reported as the number of cells where the number of training examples is statistically significantly different from the number of generated examples. Inception Score (IS) is a metric for evaluating GANs by measuring the mean KL divergence between imageconditional output class probabilities and the marginal distribution. We replace Inception features with a pitch classifier for evaluating GANs. Pitch Accuracy (PA) and Pitch Entropy (PE) measure distinct pitches and output distribution. Fr\u00e9chet Inception Distance (FID) correlates with perceptual quality and diversity on synthetic distributions using pitch-classifier features. The study compares different model and representation variants for audio quality. Human evaluation shows that quality decreases as output representations move from IF-Mel to Waveform. IF-Mel is judged slightly inferior to real data. WaveNet baseline produces high-fidelity sounds but occasionally breaks down. Sample diversity correlates with audio quality, with NDB following the same trend as human evaluation. High frequency resolution improves NDB score. WaveNet baseline receives the worst NDB score. Generative model assigns high likelihood to training data. The generative model assigns high likelihood to training data, leading to a lack of diversity in autoregressive sampling. FID scores are lower for models with high frequency resolution. Mel scaling has less effect on FID compared to listener study. Phase models have high FID, reflecting poor sample quality. Classifier metrics like IS, Pitch Accuracy, and Pitch Entropy show good performance in high-resolution models. The models generate examples with similar accuracy to real data, but the distribution of generated examples may not match the training distribution. Metrics provide a rough measure of which models are less reliably generating pitches. Listen to accompanying audio examples for a better understanding. The WaveGAN and PhaseGAN models show phase irregularities, creating blurry waveforms, while the IFGAN model is more coherent with small variations. Real data and IF models have consistent waveforms, resulting in strong colors for each harmonic, while PhaseGAN has speckles and WaveGAN is irregular. Visualizations show real data and IF models producing consistent waveforms, PhaseGAN with some discontinuities, and WaveGAN being irregular. The WaveGAN and PhaseGAN models exhibit phase irregularities, resulting in blurry waveforms, while the IFGAN model shows more coherence with slight variations. GANs allow conditioning on the same latent vector for the entire sequence, unlike autoregressive models like WaveNet. WaveNet autoencoders learn local latent codes controlling generation on a millisecond scale but have limitations in scope. Interpolating between examples in the raw waveform and the latent code of a WaveNet autoencoder and IF-Mel GAN shows differences in sound mixing. WaveNet's linear interpolation does not align with the complex prior on latents, leading to sound distortions. In contrast, the GAN model with a spherical gaussian prior produces smoother global interpolations. The autoencoder only has local conditioning distributed in time, leading to less realistic sound examples with linear interpolation. In contrast, the IF-Mel GAN with global conditioning produces high-fidelity audio examples with smooth perceptual changes during interpolation. Spherical interpolation stays aligned with the prior, resulting in sounds without additional artifacts. In latent space, the timbre of sounds morph smoothly between instruments while pitches follow Bach's Suite No. 1 in G major. The GAN maintains its timbral identity across pitches, creating a unique instrument identity. The Bach prelude rendered with a single latent vector shows consistent harmonic structure. GANs with upsampling convolutions allow parallel processing for training and generation. Autoregressive models allow for parallel processing of training and generation on modern GPU hardware, significantly reducing latency for audio synthesis. This opens up the possibility for real-time neural network audio synthesis on devices, expanding the range of expressive sounds that can be explored. Previous applications of WaveNet autoencoders relied on prerendering all possible sounds due to long synthesis latency. The curr_chunk focuses on speech synthesis datasets and the use of recurrent and autoregressive models for variable length inputs and outputs. Comparisons are made to audio generation for music, highlighting the slow generation of autoregressive models and the potential of GANs for audio synthesis. The work also builds on recent advances in GAN literature. The curr_chunk discusses modifications to GAN loss functions for improved training stability and generation quality. It also introduces progressive training and architectural tricks to enhance model quality. Additionally, it mentions the NSynth dataset and advancements in achieving timbre transformations in audio sources. The curr_chunk discusses improving audio generation with GANs by training a regression model on pitch and instrument labels. It highlights the limitations of existing methods and presents a new approach that outperforms WaveNet in speed and quality on the NSynth dataset. Further research is needed to apply these findings to a wider range of audio signals. The curr_chunk explores expanding GANs to generate various audio signals, addressing issues like mode collapse and diversity. It suggests combining adversarial losses with encoders or regression losses for better data distribution capture. Different models were trained with varying optimizer settings, learning rates, and classifier loss weights, with the best performance seen at a learning rate of 8e-4 and classifier loss of 10. The curr_chunk discusses the implementation details of the GAN models used for generating audio signals. It includes information on network architecture, normalization techniques, and training procedures. The models were trained with specific settings and achieved the best performance with a learning rate of 8e-4 and classifier loss of 10. The implementation details of the GAN models for generating audio signals include training with a single V100 GPU, batch size of 8, and training on 1.6M examples per stage for progressive models. The WaveNet baseline uses a decoder with 30 layers of dilated convolution and a conditioning stack operating on a one-hot pitch conditioning signal. The conditioning stack in the WaveNet model consists of 5 layers of dilated convolution followed by 3 layers of regular convolution, all with 512 channels. The model uses mulaw encoding for the 8-bit version and a quantized mixture of 10 logistics for the 16-bit version. WaveNets converged in 2 days with 32 V100 GPUs trained with synchronous SGD."
}