{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, despite individual incentives conflicting with the common good. The study focuses on intertemporal social dilemmas and demonstrates that cooperation can be learned through a model-free approach using a modular architecture for deep reinforcement learning agents. The study explores cooperation among self-interested agents through deep reinforcement learning in challenging environments, considering cultural and ecological evolution. Altruism can be favored by selection when individuals cooperate and interact with other cooperators, despite natural selection promoting selfish interests. Various mechanisms such as kin selection, reciprocity, and group selection contribute to the emergence of cooperation among agents. The study delves into cooperation among self-interested agents in multi-agent deep reinforcement learning, focusing on resolving social dilemmas. Different approaches have been proposed to address the trade-off between collective welfare and individual utility, including opponent modeling, long-term planning, and intrinsic motivation functions. These strategies aim to promote cooperation and achieve collectively optimal outcomes in challenging environments. Evolution can be applied to remove hand-crafted intrinsic motivation in deep learning, similar to other applications in optimizing hyperparameters, implementing black-box optimization, and evolving neuroarchitectures, regularization, loss functions, and reward functions. Success is not guaranteed in the ISD setting, but evolutionary simulations of predator-prey dynamics have shown promise with enforced subpopulations. Evolutionary simulations of predator-prey dynamics have shown promise in addressing intertemporal social dilemmas by evolving populations of neurons for neural networks. The system distinguishes between fast learning and slow evolution processes, with intrinsic motivation modeled as an additive term in agent rewards. This intrinsic reward function is implemented as a two-layer neural network, with evolution bridging the intertemporal dilemma. Evolutionary theory suggests evolving intrinsic reward weights across a population can help bridge intertemporal dilemmas. To achieve this, evolutionary dynamics must be structured. A \"Greenbeard\" strategy is implemented where agents choose partners based on signals of cooperativeness, termed assortative matchmaking. However, this method has limitations and is not a general solution for multi-agent reinforcement learning. The shared reward network evolution approach introduces a modular training scheme for agents in deep reinforcement learning. Agents consist of policy and reward network modules that evolve separately. The policy network is trained using modified rewards from the reward network on a fast timescale, while both modules evolve on a slow timescale. Fitness for the policy network is based on individual rewards, while fitness for the reward network is based on collective returns from the group of co-players. In a modular training scheme for agents in deep reinforcement learning, policy and reward networks evolve separately to prevent overfitting. Different parameters were explored, including environments, reward network features, matchmaking, and reward network evolution. The study focuses on intertemporal social dilemmas within a MARL setting. The study focuses on intertemporal social dilemmas within a MARL setting, where individually selfish actions have negative impacts on the group over a longer time horizon. Two dilemmas are considered, one involving collecting apples in a field with a spawning rate affected by the cleanliness of a separate aquifer. In the Harvest game, agents face a dilemma between harvesting all apples quickly for short-term gain or preserving apples for long-term group yield. The apple spawn rate depends on nearby apples, falling to zero if none are left in a certain radius. Agents must balance individual temptation with group benefit to maximize total reward. The total reward for player i in the Harvest game is the sum of extrinsic and intrinsic rewards. Extrinsic reward is obtained from the environment when taking action a from state s, while intrinsic reward is calculated based on social preferences using a neural network with evolved parameters. In the Harvest game, player rewards consist of extrinsic and intrinsic components. The intrinsic reward is determined by social preferences through a neural network with evolved parameters. Each agent has a feature vector that can be transformed into intrinsic reward by other agents. Social preferences should not be influenced by the temporal alignment of rewards, but rather by comparing temporally averaged reward estimates between players. Two methods of aggregating rewards over time were considered. The architecture for adjusting policy in the Harvest game includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. Two variants of deriving intrinsic reward are retrospective and prospective, based on past or future expectations of rewards for other agents. The retrospective method updates temporally decayed rewards for agents, while the prospective method uses value estimates with a stop-gradient before the reward network. The variant uses value estimates for f ij and has a stop-gradient before the reward network module to prevent gradients from flowing back into other agents. Training involved a population of 50 agents with policies {\u03c0 i}, sampled 5 players for each of 500 arenas running in parallel. Episode trajectories lasted 1000 steps and weights were updated using V-Trace. The set of weights evolved included learning rate, entropy cost weight, and reward network weights \u03b8. The policy network parameters were inherited in a Lamarckian manner, with agents observing their last actions and rewards as input to the LSTM. The objective function consisted of the value function gradient, policy gradient, and entropy regularization, weighted by hyperparameters. Evolution was based on a fitness measure calculated as a moving average of total episode return. Matches were determined by random or assortative matchmaking based on recent cooperativeness metrics. This ensured cooperative agents played with similar players, while defectors played with other defectors. Cooperativeness was calculated differently for Cleanup and Harvest scenarios. Cooperative metric-based matchmaking was used for individual reward networks or no reward networks, but not for the multi-level selection model. The reward network was evolved separately within its own population to allow for independent exploration of hyperparameters. Reward networks could be randomly assigned to any policy network, forcing them to generalize to a wide range of policies. In a unique approach, 5 policy networks were paired with a shared reward network in each episode. The evolution of policy network weights and optimization-related hyperparameters was based on individual agent return, while the reward network parameters evolved based on total episode return across co-players. This method differs from previous work by focusing on social features for reward network evolution to address inherent tensions in Inter-System Dependencies (ISDs) and promote social cooperation. The text discusses the use of shared reward networks for social cooperation among multiple agents in a social setting. It contrasts with hand-crafted aggregation methods and shows the importance of using intrinsic reward networks. The performance of different matchmaking strategies and the impact of reward networks on individual agents are also analyzed. In experiments on social cooperation, individual reward network agents showed little improvement over PBT on Cleanup and Harvest tasks. Assortative matchmaking with individual reward networks led to high performance, indicating the importance of conditioning internal rewards on social features. Shared reward network agents performed as well as assortative matchmaking, even with random matchmaking, suggesting that honest signals of other agents may not be necessary for success. Agents don't need immediate access to honest signals of cooperativeness to resolve dilemmas; having the same intrinsic reward function is sufficient. Prospective reward network evolution is less stable than retrospective evolution. Social outcome metrics are plotted to understand agent behavior better. Sustainability is measured by the average time step agents receive positive rewards, showing more sustainable behavior with reward networks. Equality is calculated using the Gini coefficient, with prospective networks leading to lower equality. Tagging frequency is higher with prospective or individual reward networks compared to retrospective shared rewards. The final weights of the retrospective shared reward networks evolved differently for each game, suggesting varying social preferences were needed. Cleanup required a less complex reward network, while Harvest needed a more intricate one to prevent over-exploitation of resources. Random matchmaking led to arbitrary positive values for the first layer weights. In a study on evolutionary theory and cooperation, it was found that random matchmaking did not lead to cooperative behavior due to lack of specialization. Assortative matchmaking, however, promoted cooperation when honest signals were present. A new evolutionary paradigm based on shared reward networks was proposed to achieve cooperation in various situations. Evolving intrinsic social preferences was seen to enhance cooperation by ameliorating intertemporal challenges. Evolution improves credit assignment between selfish acts and negative group outcomes, mitigates social dilemmas by exposing social signals related to selfishness, and promotes mutual cooperation through mechanisms like competitive altruism and inequity aversion. Human cooperation increases with communication. The curr_chunk discusses how modularity can be seen in nature through various examples, such as microorganisms forming multi-cellular structures and prokaryotes incorporating plasmids for cooperation. It also mentions how a reward network in humans can represent a shared cultural norm. The curr_chunk explores alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity. It suggests studying an emergent assortative matchmaking model and combining an evolutionary approach with multi-agent communication for cooperative behaviors. The games mentioned have episodes lasting 1000 steps with different playable area sizes. The games Cleanup and Harvest have different grid sizes for gameplay. Agents can only observe a 15\u00d715 RGB window around them. Actions include moving, rotating, tagging, and cleaning waste. Training involved joint optimization of network parameters and hyperparameters/reward network parameters. Gradient updates were applied for every trajectory up to 100 steps with a batch size of 32. Optimization was done via RMSProp with specific parameters. The baseline cost weight was fixed at 0.25, and the entropy cost evolved using PBT. Learning rates were initially set to 4 \u00d7 10 \u22124 and allowed to evolve. PBT uses genetic algorithms to search over hyperparameters, resulting in an adaptive schedule. A mutation rate of 0.1 was used, with perturbations for entropy cost, learning rate, and reward network parameters. A burn-in period of 4 \u00d7 10 6 agent steps was implemented for an accurate assessment of fitness before evolution."
}