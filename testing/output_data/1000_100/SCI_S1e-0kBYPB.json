{
    "title": "S1e-0kBYPB",
    "content": "To ensure widespread public acceptance of AI systems, we need methods to explain decisions made by black-box models like neural networks. Current explanatory methods face issues with different explanation perspectives leading to varied instance-wise explanations. Additionally, post-hoc explainers are mainly validated on simple models, not on real-world neural networks that may rely on unreasonable correlations. A verification framework for explanatory methods is introduced to address these challenges. A verification framework for explanatory methods is introduced to address challenges in explaining decisions made by black-box models like neural networks. The framework is based on a non-trivial neural network architecture trained on a real-world task, providing guarantees on its inner workings. The efficacy of the evaluation is validated by highlighting failure modes of current explainers. The goal is to offer a publicly available evaluation for the feature-selection perspective on explanations. The text discusses two main perspectives on explanations in machine learning: feature-additivity and feature-selection. These perspectives lead to different explanations when explaining predictions on a single input. The text highlights that comparing explanatory methods adhering to different perspectives may not be coherent due to their fundamentally different explanation targets. The text discusses the strengths and limitations of current explanatory methods in machine learning. It raises questions about the reliability of these methods when explaining models with less dramatic biases. The evaluation of explainers often assumes that the target models behave reasonably, without relying on irrelevant correlations. The text addresses the limitations of current explanatory methods in machine learning, questioning their reliability when explaining models with subtle biases. It proposes a framework to evaluate explainers under the feature-selection perspective, aiming to identify tokens that have zero contribution to the model's predictions. The framework aims to evaluate explainers by identifying tokens with zero contribution to model predictions in multi-aspect sentiment analysis tasks. It tests if explainers rank zero-contribution tokens higher than relevant ones, highlighting the need for critical evaluation of explainers. The framework introduces evaluation tests for explainers, penalizing errors only when guaranteed. L2X, a feature-selection explainer, is evaluated and compared to popular explainers LIME and SHAP, showing that LIME and SHAP perform better most of the time. In Section 5, it is detailed why LIME and SHAP generally outperform L2X. Error rates of these explanatory methods are provided to highlight potential failures in feature selection explanations. The evaluation test will be made available for community use, with a generic methodology that can be applied to various tasks. Feature-based explanatory methods are common, explaining predictions based on input unit features like tokens for text and super-pixels for images. Feature-based explainers provide explanations by analyzing input unit features such as tokens for text and super-pixels for images. There are two main types of explanations: feature-additive, which assigns weights to input features based on their contribution to the model's prediction, and feature-selective, which identifies a subset of features responsible for the prediction. Other types of explanations include example-based, which pinpoint relevant instances in the training set that influenced the model's prediction, and human-level explanations that mimic how humans explain things in the real world. In this work, the focus is on verifying feature-based explainers that mimic human explanations in real-world scenarios. Evaluations commonly involve testing explainers on interpretable target models like linear regression and decision trees, but these simple models may not fully represent complex neural networks used in practice. Synthetic setups are also used for evaluation to create artificial tasks. Synthetic setups are used for evaluation by creating artificial tasks with controlled important features. These setups may prompt target models to learn simpler functions than those needed for real-world applications. Another evaluation setup involves assuming a reasonable behavior for the target model, where intuitive heuristics are identified. Crowd-sourcing evaluation is often used to verify if the features produced by the explainer align with the model's predictions. Neural networks may discover surprising artifacts, making evaluation unreliable for assessing explainer faithfulness to the model. Human evaluation involves predicting model behavior based on explanations, determining explainer effectiveness. The evaluation of explanations for neural network models is typically expensive and labor-intensive. In contrast, a new automatic evaluation framework provides guarantees on the inner-workings of a non-trivial neural network model. This framework challenges explainers to provide consistent explanations for real data models compared to randomized data models, emphasizing fidelity to the target model. Perspective 1 (Feature-additivity) explains that the explanation of a model's prediction consists of contributions from each feature of the instance, approximating the prediction. Various explanatory methods follow this perspective, such as LIME and Shapley values from game theory. Lundberg & Lee unified these methods by showing that Shapley values satisfy three desired constraints. The contribution of each feature in an instance is averaged over a neighborhood, with the choice of neighborhood being critical. Another perspective involves selecting a subset of features that lead to a similar prediction as the original model. Various methods like LIME and Shapley values follow these perspectives. The perspective on feature importance in machine learning models is discussed by Chen et al. (2018), Carter et al. (2018), and Ribeiro et al. (2018). L2X (Chen et al., 2018) focuses on maximizing mutual information between features and predictions, but assumes the number of important features per instance is known, which is often not the case. The model may not always rely on a subset of features, as shown in a hypothetical sentiment analysis regression model. The hypothetical model discussed in the curr_chunk behaves similarly to real-world neural networks, which can exhibit biases due to dataset limitations. For instance, natural language inference models may heavily rely on specific tokens that are not always indicative of the correct target class. In Figure 1, differences between perspectives are highlighted, showing how certain features contribute to model predictions. The feature-additive perspective aims to explain the model's behavior on a neighborhood of the instance, while the feature-selective perspective focuses on pointwise features used by the model in isolation. The ranking of features can vary between instances, with different features being considered important. Both perspectives provide insights into the model's behavior, but one perspective may be preferred over the other. In a verification framework for feature-selection perspective of instance-wise explanations, the RCNN model is leveraged and the dataset is pruned to identify irrelevant and relevant features. Metrics are introduced to measure explainers' ability to rank features accurately. The RCNN model consists of a generator and an encoder. The RCNN model (Lei et al., 2015) includes a generator and an encoder, trained jointly with supervision only on the final prediction. The generator selects a subset of tokens from input text x, passed to the encoder for prediction. Regularizers encourage the generator to select short sub-phrases and fewer tokens. Gradients for the generator are computed to handle non-differentiability during training. The RCNN model includes a generator and an encoder trained with supervision only on the final prediction. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability introduced by intermediate sampling. The model may have learned an internal communication protocol called a handshake, encoding information from non-selected tokens. The goal is to eliminate instances where non-selected tokens have zero contribution to the prediction. The model aims to eliminate instances with non-selected tokens that have zero contribution to the prediction. The proof is in Appendix B. In an example, the model selects \"very\" with a score of 1, but selecting only \"very\" results in a score of 0.5. Equation 7 captures this handshake scenario. Non-selected tokens are considered irrelevant or zero-contribution. The presence of S Sx = S x does not always indicate a handshake, as some tokens may not significantly affect the prediction. After pruning the dataset to remove non-contributing tokens, the model ensures that at least one selected token is clearly relevant for the prediction. This step helps differentiate between noise and features that have zero contribution to the prediction. After pruning non-contributing tokens, the model verifies the relevance of selected tokens for prediction by checking the absolute change in prediction when a token is removed. This process distinguishes between clearly relevant tokens and those that are uncertain (SDK). Simply because a token alone does not significantly impact prediction does not mean it is irrelevant, as it may be important in combination with other tokens. The procedure ensures that tokens with a high impact on prediction are ranked higher than non-selected tokens. The dataset is pruned to keep datapoints with at least one clearly relevant token per instance. The method does not provide an explainer but guarantees that all tokens in N x are ranked lower than any token in SR x. The first most important token has to be in S x. The error metrics are defined based on the ranking of features provided by an explainer. Metric (A) measures the percentage of instances where the most important token identified by the explainer is not selected. Metric (B) assesses the instances where a non-selected token is ranked higher than a relevant token. Metric (C) calculates the average number of non-selected tokens ranked higher than any relevant token. These metrics highlight failures in the explanation provided by the explainer. The study evaluates error metrics (B) and (C) on the RCNN model trained on the BeerAdvocate corpus, focusing on appearance, aroma, and palate aspects. Three separate RCNNs are trained for each aspect, with datasets gathered accordingly. The study evaluates error metrics on the RCNN model trained on the BeerAdvocate corpus for appearance, aroma, and palate aspects. Three datasets were collected, one for each aspect. A threshold of 0.1 was chosen to identify clearly relevant tokens, with statistics provided in Appendix A. On average, 1 or 2 clearly relevant tokens were obtained per datapoint, indicating a strict threshold. The percentages of eliminated datapoints are also provided. The study evaluates popular explainers LIME, SHAP, and L2X on the RCNN model trained on the BeerAdvocate corpus. LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature-selection explainer. The study compared LIME, SHAP, and L2X explainers on the RCNN model trained on the BeerAdvocate corpus. L2X's limitation is the need to know the number of important features per instance, which is not practical in real-world scenarios. The average number of tokens highlighted by human annotators was used as K for testing L2X. Results showed that both LIME and L2X explainers sometimes ranked irrelevant features as most important, indicating a significant failure. The study compared LIME, SHAP, and L2X explainers on the RCNN model trained on the BeerAdvocate corpus. L2X's limitation is the need to know the number of important features per instance, which is not practical in real-world scenarios. Results showed that both LIME and L2X explainers sometimes ranked irrelevant features as most important, indicating a significant failure. In the evaluation dataset, SHAP and L2X had mistakes in the predicted ranking, with L2X placing more zero-contribution tokens ahead of relevant ones. Figure 4 and Figure 6 show explainers' rankings on instances from the palate aspect, with the heatmap indicating the ranking of tokens. The heatmap displays the ranking of tokens by LIME and SHAP explainers, with L2X prioritizing \"taste\", \"great\", \"mouthfeel\", and \"lacing\" over \"gorgeous\". Both explainers attribute importance to nonselected tokens, with LIME and SHAP even ranking \"mouthfeel\" and \"lacing\" as most important. The work introduces a distinction between two explanation perspectives and presents an evaluation test for post-hoc explanatory methods. It offers the first automatic verification framework for real-world neural networks, highlighting errors in popular explanatory methods. The methodology is adaptable to various tasks beyond natural language processing, such as computer vision. The study evaluates post-hoc explainers by selecting super-pixels and making predictions based on blurred images. The evaluation aims to highlight fundamental limitations of explainers, with statistics provided in Table 2. The methodology is domain-agnostic and aims to represent the core algorithm of current explainers. The column %(S Sx = S x ) indicates the percentage of instances removed from the BeerAdvocate dataset due to a potential handshake. %(|SR x | = 0) shows the percentage of datapoints further eliminated if a selected token has an absolute effect of at least 0.1 on the prediction. The model's prediction should change if a non-selected token influences it, indicating a handshake. If all non-selected tokens can be removed without changing the prediction, there was no handshake for the instance x. The absence of a handshake for instance x means that tokens in Nx have zero contribution. This leads to the conclusion that there is no handshake in x. The proof is completed by ensuring S Sx = S x. The beer review describes a dark, fizzy drink with fruity aromas and minimal taste. The beer review describes a dark, fizzy drink with fruity aromas and minimal taste. It has a smooth finish and comes in a nice brown \"grolsch\" like bottle. The initial alcohol content is small, with hints of fruit flavors. Overall, it is better than most American lagers and easy to drink quickly. The beer, poured from a dark bottle, has a yellow color with a lot of head and fruity aromas of apple and blueberry. It is fizzy with minimal mouthfeel and tastes of fruit with a slight warming sensation. Overall, it is better than most American lagers and easy to drink quickly."
}