{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this bias, a new balanced face image dataset with 108,501 images representing 7 race groups was created. Models trained on this dataset showed improved accuracy across different race and gender groups. Several large-scale face image datasets have been proposed, fostering research in automated face detection, alignment, recognition, generation, modification, and attribute classification. These datasets have led to the development of models with consistent accuracy across race and gender groups. Additionally, commercial computer vision APIs have been compared, showing balanced accuracy across gender, race, and age groups. Existing public face datasets are biased towards Caucasian faces, with other races like Latino being underrepresented. This bias can lead to models that do not apply to all subpopulations and raise ethical concerns about fairness in automated systems. Several commercial computer vision systems have been criticized for their asymmetric accuracy across sub-demographics, with biases in training data leading to better performance on male and light faces. Biases in image datasets can occur due to biased selection, capture, and negative sets. To mitigate race bias, a novel face dataset with balanced race composition is proposed. The text proposes a novel face dataset emphasizing balanced race composition, containing 108,501 facial images from various sources. It defines 7 race groups and shows that existing face attribute datasets do not generalize well to unseen data with more nonWhite faces. The new dataset performs better on novel data across racial groups and is the first large-scale face attribute dataset in the wild. The dataset is the first large-scale face attribute dataset in the wild to include Latino, Middle Eastern, East Asian, and Southeast Asian groups. This inclusion expands the applicability of computer vision methods to various fields. Face attribute recognition aims to classify human attributes like gender, race, age, emotions, and expressions from facial appearance. Most existing datasets are constructed from online sources. Face attribute recognition is crucial for various computer vision tasks like face verification and person re-identification. It is essential for these systems to perform equally well across different gender and race groups to maintain public trust. Instances of racial bias, such as Google Photos mistaking African American faces for Gorillas, highlight the importance of addressing these issues in machine learning and computer vision research. Most commercial service providers have stopped providing race classifiers due to potential biases in models. Face attribute recognition is used for demographic surveys in marketing and social science research to understand human behaviors. Social scientists use images to infer demographic attributes and analyze behaviors, such as demographic analyses of social media users. Unfair classification can have significant costs by over or underestimating specific attributes. The cost of unfair classification is significant as it can over or underestimate specific attributes, leading to policy implications. AI and machine learning communities are increasingly focusing on algorithmic fairness and dataset biases. Research in fairness aims to ensure fair outcomes regardless of protected attributes like race and gender. Studies in algorithmic fairness either audit existing bias in datasets or focus on producing fair outcomes. The main task of interest in our paper is balanced gender classification from facial images, addressing biases in commercial gender classification systems. Biased results may stem from skewed datasets or underlying associations between scene and race in images. Balancing all possible attribute co-occurrences is challenging, except in lab-controlled settings. The contribution of the paper is to collect diverse face images from non-White race groups to improve generalization performance. The dataset includes Southeast Asian and Middle Eastern races, addressing discrimination in existing databases. Seven race groups are defined in the dataset. The paper collected diverse face images from non-White race groups to enhance generalization performance. The dataset included Southeast Asian, Middle Eastern, and Latino races, with seven race classifications used for experiments. Ethnicity and race distinctions were discussed, with Latino considered a race based on facial appearance. Categories like Hawaiian and Pacific Islanders and Native Americans were excluded due to limited examples. The experiments in this paper focused on 7 race classifications and discussed the measurement of dataset bias based on skin color or race. Using skin color as a proxy for race has limitations due to factors like lighting conditions and within-group variations. Race is a multidimensional concept, while skin color is one dimensional. The skin color provides limited information to differentiate race groups, leading to the use of race annotations by human annotators. Skin color, measured by ITA, is also utilized in datasets sourced from public figures, which may introduce biases in age and attractiveness. The curr_chunk discusses the bias in image datasets due to limited selection of faces by professional photographers and web search queries. The goal is to minimize selection bias and increase diversity in the dataset by using a balanced race dataset. The dataset was incrementally increased for efficient collection. The dataset size was incrementally increased by detecting and annotating 7,125 faces randomly sampled from the YFCC100M dataset. Demographic compositions of each country were estimated to adjust the number of images sampled, ensuring diversity and avoiding dominance by the White race. Faces from the U.S. and European countries were excluded later in the data collection process. The minimum size of a detected face was set to 50 by 50 pixels to maintain recognizability of attributes. Only images with \"Attribution\" and \"Share Alike\" Creative Commons licenses were used. We used images with \"Attribution\" and \"Share Alike\" Creative Commons licenses for annotation of race, gender, and age group using Amazon Mechanical Turk. Annotations were refined by training a model and manually verifying discrepancies. Skewed race composition was measured in datasets with race annotations. Most face attribute datasets show bias towards White race, while gender distribution is relatively balanced. Model performance was compared using ResNet-34 architecture trained on different datasets. Face detection was done using dlib's CNN-based face detector, and attribute classification was performed in PyTorch. The dataset was compared with UTKFace, LFWA+, and CelebA datasets. Our dataset was compared with UTKFace, LFWA+, and CelebA datasets. UTKFace and LFWA+ have race annotations for comparison, while CelebA was used only for gender classification. FairFace defines 7 race categories but only 4 were used for comparison. Cross-dataset classifications were performed using models trained from these datasets. FairFace is the only dataset with 7 races, so our fine racial groups were merged for compatibility with other datasets. CelebA was included for gender classification. The study compared the dataset with UTKFace, LFWA+, and CelebA datasets. CelebA was used for gender classification. The model performed well on the LFWA+ dataset due to its diversity and generalizability. To test generalization, three novel datasets were used, including Geo-tagged Tweets. The study utilized three different datasets for analysis: Geo-tagged Tweets, Media Photographs, and Protest Dataset. Faces were randomly sampled from each dataset for further examination. The authors collected a diverse image dataset for a protest activity study, sampling 8,000 faces annotated for gender, race, and age. The FairFace model outperformed others in accuracy for race, gender, and age classification. The FairFace model outperforms other models in accuracy for race, gender, and age classification on novel datasets from different sources. Even with fewer training images, the model surpasses larger datasets like CelebA, indicating dataset size is not the sole factor for performance improvement. The model also shows more consistent results across different race groups, measured by standard deviations of classification accuracy. The FairFace model achieves the lowest maximum accuracy disparity in gender classification across different demographic groups. It shows less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White categories. Other models exhibit biases towards male or female categories due to dataset composition. The FairFace model achieves low accuracy disparity in gender classification across demographic groups, with less than 1% difference between male and female, and White and non-White categories. Other models show biases towards males and perform inaccurately on females and non-White groups, indicating unbalanced training data. Data diversity was measured using t-SNE visualization of facial embeddings from various datasets. The FairFace dataset contains diverse faces that are loosely separated by race groups. The dataset was trained from biased datasets, resulting in non-typical examples. Comparisons with other datasets like LFWA+ and UTKFace show differences in face clustering and diversity. Pairwise distance distributions were analyzed to measure face diversity, with UTKFace having more tightly clustered faces. The CDF functions for 3 datasets show differences in face clustering and diversity. UTKFace has tightly clustered faces, while LFWA+ shows diverse faces despite majority being white. Face embedding trained on similar white-oriented dataset may affect diversity perception. Previous studies highlight inconsistent classification accuracies in face analytic models across demographic groups. FairFace images were used to test gender classification APIs. FairFace images were used to test gender classification APIs from Microsoft Face API, Amazon Rekognition, IBM Watson Visual Recognition, and Face++. The dataset is diverse in race, age, expressions, head orientation, and photographic conditions, making it a better benchmark for bias measurement. 7,476 random samples were used, excluding children under 20 due to ambiguity in gender determination. Experiments were conducted in August 2019. Table 6 displays gender classification accuracies of various APIs tested on FairFace images. Not all faces were detected by the APIs except for Amazon Rekognition, which detected all faces. Two sets of accuracies were reported: one treating mis-detections as mis-classifications and the other excluding mis-detections. The study compared gender classification accuracy of various APIs on FairFace images, with Amazon Rekognition detecting all faces. Results showed male category bias, higher error rates for dark-skinned females, and significant gender bias in face detection by Microsoft's model. Skin color alone is not a sufficient guideline to study model bias. The paper introduces a new face image dataset balanced on race, gender, and age, which outperforms existing datasets in classification performance. The dataset, derived from the Yahoo YFCC100m dataset, allows for both academic and commercial usage. The paper introduces a new face image dataset balanced on race, gender, and age, derived from the Yahoo YFCC100m dataset. It can be used for training new models and verifying classifier accuracy. Algorithmic fairness is crucial in AI system design, especially as they impact decision making in society. The dataset aims to mitigate race and gender bias in computer vision systems for better societal acceptance."
}