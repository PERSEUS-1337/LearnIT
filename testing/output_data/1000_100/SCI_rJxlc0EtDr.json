{
    "title": "rJxlc0EtDr",
    "content": "Recent research on neural network architectures with external memory has focused on the bAbI question and answering dataset, which presents challenging tasks requiring reasoning. A classic associative inference task from human neuroscience literature was used to assess the reasoning capacity of existing memory-augmented architectures. Current architectures struggle with reasoning over long distance associations, as shown in tasks involving finding the shortest path between nodes. A novel architecture called MEMO was developed to address this issue by introducing two new components. MEMO is a novel architecture that introduces a separation between memories/facts stored in external memory and utilizes an adaptive retrieval mechanism for reasoning tasks. It can solve both novel reasoning tasks and all 20 tasks in bAbI. This architecture allows for connecting facts acquired at different points in time, enabling the inference of relationships between separate episodes. The hippocampus supports inferential reasoning by recombining single experiences to infer relationships. Memories are stored independently to minimize interference, allowing for recall of specific events. However, this separation conflicts with the hippocampus's role in generalization. Recent research explores how separated memories can be chained together. Recent research has shown that the integration of separated experiences occurs during retrieval through a recurrent mechanism, allowing for inference. This paper explores how neuroscience models can enhance inferential reasoning in neural networks, such as those augmented with external memory like the Differential Neural Computer and end-to-end memory networks. Additionally, more powerful attention mechanisms and the use of context have enabled traditional neural networks to tackle complex tasks. The introduction of new tasks like Paired Associative Inference (PAI) derived from neuroscientific literature aims to enhance inferential reasoning in neural networks. PAI forces networks to learn abstractions to solve unseen associations, moving beyond repetitive tasks like bAbI. This approach challenges neural networks to appreciate distant relationships among elements distributed across multiple facts or memories. MEMO is a new approach that retains all facts in memory and uses a linear projection with a recurrent attention mechanism for flexible weighting of individual elements. It differs from other models by allowing greater flexibility in memory usage for memory-based reasoning tasks. The curr_chunk discusses the problem of prohibitive computation time in neural networks and proposes adapting the amount of compute time to the complexity of the task. It draws inspiration from a model of human associative memory called REMERGE. The curr_chunk introduces a model inspired by adaptive computation time to address the issue of lengthy computation in neural networks. The network outputs a halting policy to determine when to terminate the process based on a fixed point operator. The binary halting random variable is trained using reinforcement learning to adjust weights. The curr_chunk introduces a new task emphasizing reasoning and an investigation into memory representations supporting inferential reasoning. The approach utilizes reinforcement learning to adjust weights for optimal computation steps, with a focus on minimizing required computation. The curr_chunk discusses memory representation for inferential reasoning and introduces a REINFORCE loss component for optimal task-solving iterations. Empirical results on paired associative inference, shortest path finding, and bAbI tasks are presented. The setup involves predicting answers based on knowledge inputs and queries. The curr_chunk explains how a network predicts answers based on knowledge inputs and queries. It involves embedding words, calculating weights over memory elements, and producing outputs. The process includes using matrices for key, values, and query, as well as positional encoding and element-wise multiplication. The curr_chunk discusses the training process of EMN and MEMO networks for predicting answers based on knowledge inputs and queries. EMN uses matrices for key, values, and query with positional embeddings, while MEMO embeds inputs differently by deriving a common embedding for each input matrix and adapting them to be key or value without using positional embeddings. MEMO utilizes multiple heads to attend to memory, allowing flexibility in capturing different parts of the input sentence. Each head has a unique perspective on the common inputs, enabling the network to learn how to weight each item during memory lookup. This approach contrasts with hand-coded positional embeddings used in EMN and has been proven critical for flexibility. MEMO utilizes multi-head attention mechanism for flexible recombination of stored items. It differs from EMN by using DropOut and LayerNorm for generalization and learning dynamics. The attention mechanism involves matrices for transforming logits and queries, and an output MLP for producing answers. This approach incorporates features from Vaswani et al. (2017) for normalization factors. MEMO utilizes a multi-head attention mechanism that differs from previous methods by preserving the query separate from keys and values, leading to linear complexity. It can output potential answers and learn the number of computational steps required. This decision is made by collecting information at each step, processing it with GRUs and an MLP to define a binary policy. The network in MEMO utilizes GRUs and an MLP to define a binary policy and approximate its value function. The input to the network is formed by the Bhattacharyya distance between attention weights of current and previous time steps, and the number of steps taken so far. The network is trained using REINFORCE with parameters adjusted using n-step look ahead values. The network in MEMO uses GRUs and an MLP to define a binary policy and approximate its value function. The objective function is to minimize L Hop, a term that encourages minimizing the expected number of hops in computation. This term helps the network prefer representations that require less computation. Using REINFORCE for training discrete random variables can have high variance, but for a binary halting random variable, the variance is just p(1 \u2212 p). The variance of a binary halting random variable is p(1 \u2212 p), bounded by 1/4 for successful learning. The reward structure is defined by the target answer a and the prediction \u00e2. The final layer of M LP R is initialized with bias init to increase the probability of producing a 1. A maximum number of hops, N, is set for the network, with no gradient sharing between the hop network and the main MEMO network. Memory-augmented networks like the Differential Neural Computer (DNC) operate sequentially on inputs, learning to read and write to a memory store. While DNC struggled with scalability, incorporating sparsity improved performance on larger tasks. Other memory-augmented architectures have since been developed, such as the Dynamic Memory Network, sharing similarities with EMNs. The Dynamic Memory Network (Kumar et al., 2016) operates on sequential inputs, similar to EMNs. The Recurrent Entity Network (Henaff et al., 2016) uses a parallel architecture like DNC but enables simultaneous updates. The Working Memory Network (Pavez et al., 2018) is based on EMNs but includes a working memory buffer. RelationNet (Santoro et al., 2017) enables relational reasoning over memory contents. These models perform well on tasks like the bAbI task suite. Machine learning algorithms often do not adjust computational budget based on task complexity. Conditional computation methods like Adaptive Computation Time (ACT), Adaptive Early Exit Networks, and the use of REINFORCE have been developed to adjust the number of computational steps based on task complexity. These approaches allow for dynamic modulation of computational resources in neural networks. Conditional computation methods like Adaptive Computation Time (ACT) and Adaptive Early Exit Networks have been developed to adjust computational steps based on task complexity. Different techniques, such as jump technique in recurrent neural networks and neural networks augmented with external memory, have been applied to reduce processed inputs. Our method introduces the idea of using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory. Graph Neural Networks (GNNs) consist of an iterative message passing process propagating node and edge embeddings throughout a graph. Graph Neural Networks (GNNs) use neural networks to aggregate functions over graph components for various learning tasks. Unlike GNNs, our method adapts computation steps dynamically and does not require message passing between memories. The paper introduces a task derived from neuroscience to probe the reasoning capacity of neural networks, focusing on paired associative inference (PAI) task. This task involves appreciating distant relationships among elements distributed across multiple memories, without requiring message passing between memories. The PAI task involves associating images in pairs and testing memory retrieval through direct and indirect queries. The task requires inference across multiple episodes to determine the correct match. The PAI task involves associating images in pairs and testing memory retrieval through direct and indirect queries. With CUE, image A, and two possible choices: image C, the MATCH, originally paired with B; or another image C, the LURE, forming a different triplet A \u2212 B \u2212 C. The right answer, C, can only be produced by appreciating that A and C are linked because they both were paired with B. This is analogous to the insight that two people walking the same little girl are likely to have some form of association. For specific details on how the batch was created, refer to the appendix. MEMO was compared with other memory-augmented architectures: End to End Memory Networks (EMN), DNC, and the Universal Transformer (UT). Table 1 reports the summary of results of our model (MEMO) and the other baselines. Table 1 summarizes the results of the MEMO model compared to other baselines on the hardest inference queries for PAI tasks. MEMO and DNC achieved the highest accuracy on the smaller set, while MEMO was the only architecture successful on longer sequences. Further analysis showed that MEMO required fewer steps than DNC to achieve the same accuracy on a length 3 PAI task. Attention weights were analyzed to understand how MEMO approached the task of associating a CUE with the MATCH. The MEMO model successfully associated a CUE with a MATCH in an inference query, avoiding interference from a LURE. The network retrieved memories in different slots to form associations A-B and B-C. In the second hop, appropriate probability masses were assigned to the associations, with some mass also associated with the LURE. In the second hop, MEMO assigned probability masses to slots for correct inference, confirmed in the last hop. Different patterns of memories activation were observed with varying hops, indicating algorithm dependence on network steps. This may relate to knowledge distillation in neural networks. The analysis confirmed that specific memory representations and recurrent attention mechanism are essential for successful inference, as individual components were not sufficient. Direct queries test episodic memory and can be solved with a single memory look-up. The adaptive computation mechanism was found to be more data efficient compared to ACT for the task. The analysis showed that memory representations and attention mechanisms are crucial for successful inference. Synthetic reasoning experiments on randomly generated graphs were conducted, showing high accuracy in predicting the shortest path between nodes for DNC, Universal Transformer, and MEMO models. MEMO outperformed EMN and DNC in predicting nodes on complicated graphs with high connectivity. Universal Transformer showed different performance in predicting the first and second nodes of the shortest path. In comparison to MEMO, EMN, UT, and DNC, MEMO demonstrated superior performance in computing operations requiring direct reasoning. Results for the best 5 hyper-parameters for MEMO are reported, along with accuracy on the bAbI question answering dataset. MEMO successfully solved all tasks in the 10k training regime with lower error compared to other models. In an investigation of memory representations for inferential reasoning, the combination of memory representations and recurrent attention was crucial for achieving state-of-the-art performance on the bAbI task. The use of layernorm in the recurrent attention mechanism was also essential for stable training and improved performance. Test results showed superior performance compared to DNC and Universal Transformer on the bAbI task. MEMO, an extension to existing memory architectures, achieved state-of-the-art results in inferential reasoning tasks such as paired associative inference and graph traversal. It also matched the performance of current state-of-the-art results on the bAbI dataset by flexibly weighting individual elements in memory with a recurrent attention mechanism. The study introduced MEMO, an extension to memory architectures, which utilized a recurrent attention mechanism. The dataset used was ImageNet, with three sets of training, validation, and test images. Three distinct datasets were created with sequences of length three, four, and five items. Each dataset contained a large number of training, evaluation, and testing images. The batch construction involved selecting N sequences from the pool to create memory, query, and target entries. The study introduced MEMO, an extension to memory architectures, utilizing a recurrent attention mechanism with ImageNet dataset. Three datasets were created with sequences of different lengths. Memory content was created with pairwise associations between items in the sequence. Queries consisted of cue, match, and lure images. Direct queries test episodic memory without requiring inference. The study introduced MEMO, an extension to memory architectures, utilizing a recurrent attention mechanism with ImageNet dataset. Three datasets were created with sequences of different lengths. Memory content was created with pairwise associations between items in the sequence. Queries consisted of cue, match, and lure images. 'Direct' queries test episodic memory by retrieving an episode, while 'indirect' queries require inference across multiple episodes. The network is presented with concatenated image embedding vectors for the cue, match, and lure, with randomized positions for the match and lure to avoid degenerate solutions. The task can only be solved by appreciating the correct connection between the images and avoiding interference from other items. The study introduced MEMO, an extension to memory architectures, utilizing a recurrent attention mechanism with ImageNet dataset. Memory content was created with pairwise associations between items in the sequence. Queries consisted of cue, match, and lure images. The network needed to predict the class of the matches, with longer sequences providing more 'direct' and 'indirect' queries that require different levels of inference. The inputs for EMN and MEMO were used accordingly. The study introduced MEMO, an extension to memory architectures, utilizing a recurrent attention mechanism with ImageNet dataset. For EMN and MEMO, memory and query inputs were used accordingly. In the case of DNC, stories and query were embedded similarly to MEMO, followed by blank inputs for pondering steps. For UT, stories and query were embedded like MEMO, and the encoder output was used as the model output. Graph generation for shortest path experiments followed a method similar to Graves et al. (2016). Graphs are generated by sampling two-dimensional points from a unit square to create nodes. Each node has K nearest neighbors as outbound connections, with K randomly chosen for each node. The task is divided into a graph description, a query, and a target. The graph description consists of tuples representing connections between nodes. Queries are represented as tuples indicating the start and end of a path to find, with the target being the sequence of node IDs along the path. During training, mini-batches of 64 graphs, queries, and target paths are sampled. In the experiments, 64 graphs are used with associated queries and target paths. Queries are represented as a 64 \u00d7 2 matrix, targets as a 64 \u00d7 (L \u2212 1) matrix, and graph descriptions as a 64 \u00d7 M \u00d7 2 matrix. The networks are trained for 2e4 epochs with 100 batch updates each. For EMN and MEMO, the graph description is set as the memory contents, and the query is used as input. The model predicts answers for nodes sequentially, with the first node predicted before the second. In the experiments, 64 graphs are used with associated queries and target paths. Queries are represented as a 64 \u00d7 2 matrix, targets as a 64 \u00d7 (L \u2212 1) matrix, and graph descriptions as a 64 \u00d7 M \u00d7 2 matrix. The networks are trained for 2e4 epochs with 100 batch updates each. For EMN and MEMO, the graph description is set as the memory contents, and the query is used as input. The model predicts answers for nodes sequentially, with the first node predicted before the second. In contrast, MEMO uses the predicted answer for the first node as the query for the second node, while EMN uses the ground truth answer of the first node as the query for the second node. The weights for each answer are not shared. The Universal Transformer embeds the query and graph description, concatenates the embeddings, and uses the encoder output as the answer. The answer from each round is used as the initial query for the next round of hops. The weights for each answer are not shared. For DNC, the query and graph description are embedded as in EMN and MEMO. The model outputs the sequence of nodes for the proposed shortest path using pondering steps. Training is done using Adam with cross-entropy loss. Evaluation is based on accuracy over target nodes. Average values and standard deviation are reported over the best hyperparameters. Training is conducted for a fixed number of steps. The training regime for DNC and UT allows for a 'global view' on the problem, enabling better performance in the second node. In contrast, MEMO has a 'local view' where the answer to the second node depends on the first node. Comparison between MEMO and EMN was conducted under different conditions. In a comparison between MEMO and EMN models, using the ground truth answer of the first node as the query for the second node resulted in improved performance. However, when EMN was trained using the same regime as MEMO, its performance dropped significantly. These results were consistent across different scenarios with varying numbers of nodes and outbound edges. The text describes the pre-processing steps for the dataset, including converting text to lowercase, handling punctuation, and separating queries from stories. During training, mini-batches of 128 queries and corresponding stories are sampled, resulting in matrices of specific sizes. Padding with zeros is done for queries. The text discusses the maximum number of stories and sentence size, padding with zeros for queries and stories that do not reach the max size. Different models use stories and queries as inputs in various ways. Optimization steps are taken using Adam, and training stops after a fixed number of time-steps. The text discusses training optimization steps using Adam and stopping training after a fixed number of time-steps. The model incorporates temporal context by adding a time encoding vector to the memory store. Training involves 2e4 epochs with 100 batch updates, and evaluation includes sampling a batch of 10,000 elements. The mean accuracy and task-specific accuracy for the 20 tasks of bAbI are computed. The model is trained using cross entropy loss to predict class IDs. MEMO was trained using cross entropy loss to predict class ID, node ID, and word ID in different tasks. The halting policy network parameters were updated using RMSProp. The temporal complexity of MEMO is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where various parameters are fixed for all experiments. MEMO has linear complexity with respect to the number of sentences in the input, while the Universal Transformer has quadratic complexity. The spatial complexity of MEMO is O(I \u00b7 S \u00b7 d), where I, S, and d are fixed parameters. The halting unit h in MEMO is defined using a binary policy \u03c0 t. This differs from the original ACT implementation. The halting unit in MEMO uses a binary policy \u03c0 t, which differs from the original ACT implementation. It includes trainable weights and biases, enabling more powerful representations. The halting probability is defined as T = min{t : where , with a fixed value of 0.01. The answer provided by MEMO+ACT is defined as a t corresponding to the answer provided at hop t. The architecture used is the same as described in Graves et al. (2016), with specific layer sizes detailed in Table 13. The architecture used in this study is based on Graves et al. (2016) and Dehghani et al. (2018), with hyperparameters detailed in Table 15 and available at a specific GitHub link. Hyperparameter search was also conducted for training tasks."
}