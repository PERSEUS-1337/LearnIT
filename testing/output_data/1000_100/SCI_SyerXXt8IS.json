{
    "title": "SyerXXt8IS",
    "content": "Auto-generating stronger input features for ML methods with limited training data. Biological neural nets (BNNs) excel at fast learning, particularly in the insect olfactory network. The network utilizes competitive inhibition, sparse connectivity, and Hebbian updates for rapid odor learning. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, outperforming other methods like PCA and NNs. \"Insect cyborgs\" combining BNN and ML methods show significantly improved performance on MNIST and Omniglot datasets, reducing test set errors by 20% to 55%. The study highlights the potential value of BNN-inspired feature generators in the ML context, aiming to improve ML methods' ability to learn from limited data by automatically generating new class-separating features. This approach is based on the rapid learning capabilities of BNNs, which can learn effectively even from a few samples. The insect olfactory network, specifically the Antennal Lobe (AL) and Mushroom Body (MB), serves as an effective feedforward network for learning new odors with just a few exposures. It incorporates competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. MothNet, a computational model based on this network, demonstrates rapid learning capabilities, outperforming standard ML methods with limited training samples per class. The MothNet architecture utilizes sparsity in the Mushroom Body (MB) and Hebbian updates for weight adjustments in the connections. It aims to serve as a front-end feature generator for a machine learning classifier. The MothNet architecture serves as a feature generator for a machine learning classifier by combining MothNet with a downstream ML module. The AL-MB model acts as an automatic feature generator, improving ML method accuracies on a non-spatial dataset derived from vMNIST. The MothNet architecture, known as AL-MB model, enhances machine learning accuracy by providing stronger features compared to other methods like PCA, PLS, NNs, and transfer learning. The vMNIST dataset, with 85 pixels-as-features, outperforms baseline ML methods like Nearest Neighbors, SVM, and Neural Net. The full network architecture details of MothNet are provided in [11]. The MothNet architecture, known as AL-MB model, enhances machine learning accuracy by providing stronger features compared to other methods like PCA, PLS, NNs, and transfer learning. Full Matlab code for these cyborg experiments, including comparison methods and details on ML methods and hyperparameters, can be found at [12]. Cyborg vs baseline ML methods were tested on vMNIST dataset. Trained ML accuracies of the baselines and cyborgs were compared to assess performance. The MothNet architecture, known as AL-MB model, enhances machine learning accuracy by providing stronger features compared to other methods like PCA, PLS, NNs, and transfer learning. Trained ML accuracies of the baselines and cyborgs were compared to assess gains. Different feature generators were tested on the vMNIST dataset to compare their effectiveness with MothNet features. The MothNet architecture, AL-MB model, significantly improved ML accuracy by capturing new class-relevant features. Testing on vMNIST dataset showed gains ranging from 10% to 88% compared to other feature generators like PCA, PLS, and NN. MothNet features improved ML accuracy across all models, with gains ranging from 10% to 88%. NN models benefited the most, with a 40% to 55% reduction in test error. Even when ML baseline accuracy exceeded MothNet's ceiling, MothNet features still enhanced accuracy by providing clustering information that ML methods utilized effectively. The MothNet features significantly improved ML accuracy across all models, with gains ranging from 10% to 88%. NN models benefited the most, with a 40% to 55% reduction in test error. The readouts contained clustering information that ML methods leveraged more effectively than MothNet itself. Gains were significant in almost all cases with N > 3. MothNet features were found to be far more effective than other methods such as Nearest Neighbors and SVM. The MothNet features significantly improved ML accuracy across all models, with gains ranging from 10% to 88%. Cyborgs with a pass-through AL layer still showed significant improvements in accuracy over baseline ML methods. The high-dimensional, trainable layer (the MB) was found to be most important, with competitive inhibition of the AL layer adding value in generating strong features. NNs benefitted the most from the AL layer, with gains of up to 40% of the total gain. The bio-mimetic feature generator MothNet significantly improved ML methods on vMNIST and vOmniglot by making class-relevant information accessible. MothNet features outperformed standard methods like PCA, PLS, NNs, and pre-training. The competitive inhibition layer enhances classification by creating attractor basins for inputs, increasing the effective distance between samples of different classes. The sparse connectivity from AL to MB provides computational and anti-noise benefits. The insect MB, similar to sparse autoencoders, differs in various aspects such as not seeking to match the identity function, having a greater number of active neurons than the input dimension, no pre-training step, and requiring few samples for structure improvement. It also differs from Reservoir Networks as MB neurons lack recurrent connections. The Hebbian update mechanism in MB is distinct from backprop, with weight updates occurring on a local basis. The dissimilarity of optimizers (MothNet vs ML) is seen as advantageous. The dissimilarity of optimizers (MothNet vs ML) was beneficial in increasing total encoded information."
}