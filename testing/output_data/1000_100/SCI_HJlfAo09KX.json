{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer neural network. Under Gaussian inputs, the empirical risk function shows strong convexity and smoothness, allowing gradient descent to converge linearly to the ground truth. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks. Neural networks have gained research interest for their success in practical domains like computer vision and artificial intelligence. Efforts are being made to understand the theoretical underpinnings behind their success, including which functions can be represented by deep neural networks and why they generalize well. Model recovery, where training samples are generated from a distribution based on a neural network, is a key area of research. The goal is to recover the underlying model parameter W from samples generated from a distribution based on a neural network model. Two types of data generations are studied: regression problems and classification problems. In regression, samples are generated using a weight vector and Gaussian input, while in classification, labels are drawn under a conditional distribution. In the context of recovering model parameters from samples generated by a neural network model, previous studies focused on using gradient descent with squared loss for regression and classification settings. Statistical guarantees were provided for model recovery using the squared loss, showing the positive definiteness of the Hessian in the local neighborhood of the ground truth. This guarantee necessitates a fresh set of samples at each iteration for gradient descent to converge to the ground truth. The study aims to develop a strong statistical guarantee for the loss function in eq. (2) for classification problems, specifically focusing on recovering one-hidden-layer neural networks using the cross entropy loss function. This is a more challenging but practical approach compared to using the squared loss function. The research provides the first performance guarantee for this scenario, showing guaranteed linear convergence without the need for per-iteration resampling. The study focuses on developing a statistical guarantee for the cross entropy loss function in recovering one-hidden-layer neural networks for classification problems. It shows guaranteed linear convergence with a sample complexity of O(dK 5 log 2 d) and a convergence rate of O(dK 9/2 log n/n) in the Frobenius norm. The study demonstrates guaranteed linear convergence in recovering one-hidden-layer neural networks for classification problems, with a sample complexity of O(dK 5 log 2 d) and a convergence rate of O(dK 9/2 log n/n) in the Frobenius norm. The convergence guarantee does not require fresh samples at each iteration due to uniform strong convexity in the local neighborhood. The tensor method proposed in BID38 provides an initialization near the ground truth, with a computational complexity of O(ndK 2 log(1/ )). The proof replaces the homogeneous assumption on activation functions with a condition on the curvature of activation functions around W, applicable to a wider range of activation functions like sigmoid and tanh. Various new techniques are developed to analyze the cross-entropy loss function, leveraging statistical information on geometric curvatures. Our technique guarantees uniform concentrations for the classification problem using the squared loss, focusing on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization. The parameter recovery viewpoint is crucial for non-convex learning in signal processing problems like matrix completion and blind deconvolution. The statistical model for data generation removes worst-case instances, allowing us to focus on average-case performance with benign geometric properties enabling global convergence. In the study of one-hidden-layer network models, landscape analysis reveals that with a large network size compared to data input, there are no spurious local minima, and all local minima are global. However, for cases with multiple neurons in the under-parameterized setting, spurious bad local minima exist even at the population level. Various characterizations for the local Hessian in regression problems with different activation functions have been provided. In the model recovery problem, BID28 and BID21 showed convergence results for gradient descent with different activation functions and sample complexities. BID38's study provides additional insights into the ground truth. Our study analyzes the cross entropy loss function with a different form and addresses the model recovery classification problem under the multi-neuron case. Previous research focused on different neural network structures and loss functions, making direct comparisons challenging. The paper is organized into sections discussing problem formulation, main results on local geometry, and local linear convergence of gradient descent. The paper discusses local geometry and local linear convergence of gradient descent. It also covers the initialization method, numerical examples, and conclusions. Notable notations include boldface letters for vectors and matrices, spectral norms, and the gradient and Hessian of a function. The paper discusses the generative model for training data and the gradient descent algorithm for learning network weights. It focuses on the classification setting using a one-hidden layer neural network model with a sigmoid activation function. The goal is to estimate the network weights by minimizing the empirical risk function, which is the cross entropy loss. The paper introduces a gradient descent algorithm with a well-designed initialization scheme to estimate network weights for a one-hidden layer neural network model. The algorithm uses a fixed set of training samples throughout execution, unlike other methods that resample at each iteration. The paper introduces a gradient descent algorithm with a well-designed initialization scheme to estimate network weights for a one-hidden layer neural network model. It characterizes the local strong convexity of the loss function and introduces an important quantity, \u03c1(\u03c3), for the sigmoid activation function. The algorithm uses a fixed set of training samples throughout execution. Theorem 1 guarantees that the Hessian of the empirical risk function is positive definite in a local neighborhood of W for a classification model with sigmoid activation function, under certain conditions. The proof is outlined in Appendix A and applies to all column permutations of W. The bounds in Theorem 1 depend on the dimension parameters of the network (n and K), as well as the activation function and the ground truth. For the classification problem, due to the nature of quantized labels, W is no longer a critical point of the empirical risk function. By the strong convexity of the empirical risk function in the local neighborhood of W, there can exist at most one critical point in B(W, r). Theorem 2 guarantees the existence of a unique critical point Wn in B(W, r) for a classification model with sigmoid activation function. Gradient descent converges linearly to Wn with a rate of O(K^(9/4) * d * log n / n). This critical point is provably close to the ground truth W and can be recovered. The tensor method proposed in BID38 is used for initialization in order to achieve consistent recovery of W as n approaches infinity. Gradient descent converges linearly to Wn at a linear rate when initialized in the basin of attraction. The computational complexity for achieving -accuracy is O(ndK^2 log(1/\u03b5)), which is linear in n, d, and log(1/\u03b5). The tensor method proposed in BID38 is used for initialization to achieve consistent recovery of W as n approaches infinity. The algorithm includes estimating the direction of each column of W and approximating the magnitude and sign of w i. Technical assumptions are made for the classification problem. The algorithm proposed in BID38 uses the tensor method for initialization to recover W consistently as n approaches infinity. Assumptions are made for the regression problem, including conditions on the activation function and the curvature around the ground truth. The performance guarantee for the initialization algorithm is presented in Theorem 3 for the classification model. The proof of Theorem 3 shows that Algorithm 2's output W 0 satisfies DISPLAYFORM1 with high probability. The proof involves accurate estimation of the direction and norm of W. Gradient descent is used to verify strong convexity of the empirical risk function. Multiple initializations in a local region lead to convergence to the same critical point W n. The variance of gradient descent output is calculated with multiple random initializations. An experiment is successful if the standard deviation is less than 10^-2. Gradient descent converges to the same local minima with high probability when sample complexity is sufficient. The statistical accuracy of the local minimizer for gradient descent is shown to be high when initialized close to the ground truth. Average estimation error decreases with increasing sample size, matching theoretical predictions. Cross entropy loss with gradient descent outperforms squared loss in terms of error rates. In a classification problem, cross entropy loss outperforms squared loss. The study focuses on model recovery of a neural network using cross entropy loss, characterizing sample complexity for local strong convexity. Gradient descent converges linearly to the ground truth with proper initialization. Future work aims to extend analysis to different activation functions and network structures. The proof of Theorem 1 involves showing smoothness and convexity properties of the Hessian of the population loss function in a neighborhood of W. Lemmas 1, 2, and 3 establish these properties, leading to the conclusion that the Hessian of the empirical loss function also exhibits strong convexity and smoothness in the same neighborhood. Lemma 1 states that for sigmoid activations, if W F \u2264 1, then a certain condition holds for a constant C when W \u2212 W F \u2264 0.7. Lemma 2 establishes local strong convexity and smoothness of the population loss function for sigmoid activations. Lemma 3 shows that under certain conditions, the Hessian of the empirical loss function is close to the Hessian of the population loss function. The proof of Theorem 1 combines Lemmas 3 and 1 to show that as long as the sample size n is set appropriately, the function is strongly convex in a certain region. The proof of Theorem 2 shows that the gradient concentrates around a critical point in that region, and gradient descent converges linearly to that point. The gradient of the function concentrates around a critical point W, with gradient descent converging linearly to W. Lemma 4 establishes that the gradient uniformly concentrates around W. For sigmoid activation function and appropriate sample size n, there exists a unique critical point Wn in a certain region. Theorem 1 and the Cauchy-Schwarz inequality are used to establish the existence of a critical point in B(W, r). Gradient descent converges linearly to the local minimizer Wn as long as the learning rate \u03b7 is set appropriately. The proof involves showing the accuracy of the estimation of the direction of W and follows similar arguments to previous works. The proof in part (b) does not require the homogeneous condition for the activation function, instead, it is based on a mild condition in Assumption 2. A tensor operation is defined for matrices A, B, and C. BID38 shows that for the regression problem, if the sample size is large enough, a certain result holds with high probability. The main idea of the proof is to bound the estimation error via Bernstein inequality. The estimation error of P 2 and R 3 is bounded using Bernstein inequality. For the classification problem, Bernstein inequality is applied to all neurons together. The label y i is naturally bounded in classification, while in regression, the output y i needs to be upper bounded. A different proof for estimating w i is provided, not requiring homogeneous conditions on the activation function. The quantity Q1 is defined based on the first non-zero index of Ml1=0. It contains information of wi and can be estimated through an optimization problem. By solving this problem, an estimate ai of wi can be obtained. The sign of \u03b2i can correctly estimate si, and further calculations lead to estimating wi. The sub-gaussian and sub-exponential norms of random variables are defined as X \u03c82 and X \u03c81 respectively. These definitions are useful in proving the desired result related to the inverse function and sample size requirements. The sub-gaussian and sub-exponential norms of random variables, denoted as X \u03c82 and X \u03c81 respectively, are defined. The gradient and Hessian of E[(W; are calculated, and \u2206 j,l is evaluated. The hessian block is written concisely, and \u2206 j,l is calculated and plugged back to obtain the desired result. The text discusses upper bounding E T 2 j,l,k using Cauchy-Schwarz inequality and presents Lemma 5 for sigmoid activation function. It further explores the Hessian of the population risk at ground truth. The text discusses upper bounding the Hessian of the population risk at ground truth and applies Lemma 1 to obtain a uniform bound in the neighborhood of W. It further provides an upper bound for \u2207 2 f (W) and concludes with the implications of Lemma 1 within a certain neighborhood. The text presents a proof of Lemma 3 by adapting analysis from a previous study. It introduces the covering number N of a Euclidean ball and defines a cover set W. It then discusses events A t, B t, and C t, and proceeds to bound the terms P(B t) separately. The upper bound for P(B t) is discussed, along with a technical lemma from a previous study."
}