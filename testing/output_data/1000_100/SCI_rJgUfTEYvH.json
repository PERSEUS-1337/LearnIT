{
    "title": "rJgUfTEYvH",
    "content": "Generative models for video prediction face the challenge of uncertain futures. Existing models are either computationally expensive or do not optimize data likelihood directly. This work introduces multi-frame video prediction using normalizing flows, enabling direct data likelihood optimization and high-quality stochastic predictions. Flow-based generative models offer a competitive approach to generative modeling by modeling latent space dynamics effectively. Flow-based generative models are a competitive approach to video generative modeling, benefiting from exponential progress in computational hardware and advancements in machine learning. The field has seen improvements in various capabilities like image classification, machine translation, and game-playing agents. However, the application of machine learning has been limited to scenarios with ample supervision or accurate simulations of the environment. Utilizing large unlabeled datasets with predictive generative models can be an appealing alternative to supervised learning, especially for complex generative models that aim to predict future events by building an internal representation of the world. This approach, exemplified by a predictive generative model predicting future frames in videos, allows for a rich understanding of real-world phenomena without the need for labeled examples. Training a large generative model on abundant video sequences can lead to learning representations useful for downstream tasks. In this paper, the focus is on stochastic prediction, specifically conditional video prediction, where raw RGB video frames are synthesized based on a short context of past observations. The challenge lies in the uncertainty of the future, with various probabilistic models being studied to represent uncertain futures. These models are either computationally expensive or do not directly optimize the likelihood of the data. The paper focuses on conditional video prediction using flow-based generative models to accurately synthesize realistic video frames. This approach extends flow-based models to predict future values of the latent state, addressing the challenges of modeling high-dimensional video sequences. VideoFlow is a flow-based video prediction model that achieves competitive results in stochastic video prediction, outperforming VAE-based models on the BAIR dataset. It produces high-quality results without common artifacts like blurry predictions and offers faster test-time image synthesis. VideoFlow is a flow-based video prediction model that achieves faster test-time image synthesis, making it practical for real-time applications like robotic control. Previous research focused on deterministic predictive models with architectural changes and different generation objectives. VideoFlow is a flow-based video prediction model that addresses stochastic environments by effectively reasoning over uncertain futures in real-world videos. Previous research focused on deterministic predictive models, but incorporating stochasticity is crucial for generating accurate predictions in such scenarios. VideoFlow is a flow-based video prediction model that incorporates stochasticity through variational auto-encoders, generative adversarial networks, and autoregressive models to optimize log-likelihood for accurate predictions in uncertain environments. The video prediction model VideoFlow incorporates variational auto-encoders, generative adversarial networks, and autoregressive models to optimize log-likelihood for accurate predictions in uncertain environments. The proposed VAE model produces better predictions than autoregressive models, especially for longer horizons, with faster sampling and high-quality long-term predictions. Flow-based generative models encode data into stochastic variables through a sequential process, allowing for exact latent variable inference and log-likelihood evaluation. By transforming data through invertible functions, we can compute the log-likelihood exactly and generate samples from the data distribution. Our model proposes a generative flow for video using a multi-scale flow architecture. The latent space is divided into separate variables per timestep, with each timestep represented by an invertible transformation of a video frame. The multi-scale architecture encodes information about each frame at different scales, allowing for exact latent variable inference and log-likelihood evaluation. In this subsection, invertible transformations with simple Jacobian determinants are chosen, such as triangular, diagonal, or permutation matrices. Techniques like Actnorm, Coupling, SoftPermute, and Squeeze are applied to the input data for efficient processing in the flow architecture. We reshape the input to allow for a larger receptive field and infer latent variables at different levels using a multi-scale architecture. The architecture consists of flows at multiple levels to obtain latent variables for each frame of the video. Techniques like Actnorm, Coupling, SoftPermute, and Squeeze are applied for efficient processing in the flow architecture. The architecture for inferring latent variables in a video involves an autoregressive factorization for the latent prior, with conditional priors specified for each timestep. A deep 3-D residual network is used to predict the mean and log-scale of a conditionally factorized Gaussian density. The log-likelihood objective has two parts, detailed in Section B and C of the appendix. The log-likelihood objective of Equation (1) has two parts: the invertible multi-scale architecture contributes via the sum of log Jacobian determinants of transformations mapping video frames to latent variables, and the latent dynamics model contributes log p \u03b8 (z). Parameters are learned by maximizing this objective, with the prior modeling temporal dependencies and the flow acting on separate frames. Realism comparison with SAVP-VAE and SV2P shows varying fooling rates. VideoFlow model generates trajectories conditioned on specific frames for different shapes. The use of 2-D convolutions with autoregressive priors in the flow model allows for synthesizing long sequences without introducing temporal artifacts. 3-D convolutional flows were found to be computationally expensive compared to autoregressive priors, leading to limitations in memory and training consistency. The generated videos can be viewed online, with a blue border representing conditioning in the visuals. The generated videos can be viewed online with a blue border representing conditioning. VideoFlow models the Stochastic Movement Dataset, where shapes move in eight directions with constant speed. By looking back at just one frame, the model can predict the position of the shape at the next step. The model predicts future trajectory of shapes in videos, outperforming baselines in generating realistic trajectories. It is assessed using a real vs fake Amazon Mechanical Turk test. The model generates realistic trajectories in videos without actions, using the BAIR robot pushing dataset. Training baseline models like SAVP-VAE, SV2P, and SVG-LP to generate target frames. Realism and diversity are measured using a 2AFC test and pairwise cosine distance in VGG perceptual space. VideoFlow outperforms baseline models like SAVP-VAE and SV2P in bits-per-pixel, attributed to their optimization objectives. The models do not directly optimize the variational bound on log-likelihood due to a \u03b2 = 1 term in their objective and scheduled sampling. Sampling 100 videos from each model, VideoFlow selects the closest to ground-truth based on PSNR, SSIM, and VGG perceptual metrics. VideoFlow outperforms baseline models like SAVP-VAE and SV2P in bits-per-pixel by selecting the closest generated video to the ground-truth based on PSNR, SSIM, and VGG perceptual metrics. The BAIR robot-pushing dataset is highly stochastic, and the models aim to generate realistic future scenarios. In prior work, researchers have effectively tuned pixel-level variance as a hyperparameter to improve sample quality in video models. By removing pixel-level noise, higher quality videos can be achieved at the cost of diversity. Sampling videos at a lower temperature can help remove pixel-level noise, similar to procedures in previous studies. In video models, tuning pixel-level variance as a hyperparameter can enhance sample quality by reducing noise, albeit at the expense of diversity. Sampling frames with a lower temperature can help eliminate pixel-level noise, as demonstrated in previous studies. For networks with additive coupling layers, adjusting the standard deviation of the latent gaussian distribution by a factor of T enables sampling frames with a temperature of 1.0 or an optimal temperature determined through validation set tuning. Low-temperature sampling negatively impacts the performance of SV2P and SAVP-VAE models. The best-performing SAVP-VAE models exhibit disappearing arms, outperforming those without this feature. The model with the optimal temperature excels on VGG-based similarity metrics compared to SAVP-VAE and SVG-LP models. Our model with temperature T = 1.0 performs well on VGG-based similarity metrics and competes with state-of-the-art video generation models. VideoFlow, which models the conditional probability of frames, underperforms on PSNR. Diversity and quality of generated samples are assessed by computing mean distance in VGG perceptual space and fooling rates. VideoFlow outperforms diversity values from prior work and competes in realism. At T = 0.6, it has the highest fooling rate and is on par with VAE models in diversity. Lower temperatures result in less random arm behavior and clear background objects, leading to higher realism scores. Higher temperatures produce more stochastic arm motion, increasing diversity but decreasing realism. Interpolations between different shapes are shown in Figure 6. Interpolations between the initial and final frames of two videos in the BAIR robot pushing dataset show cohesive arm motion. Using multi-level latent representation, background objects at lower levels and arm motion at higher levels are interpolated. Different shapes with fixed type but varying size and color smoothly interpolate in the latent space. During training, colors of shapes are sampled from a uniform distribution. Interpolated colors in the space match those in the training set. VideoFlow generates frames into the future, detecting temporal consistency. In the presence of occlusions, background objects become noisier while the arm remains sharp. Our VideoFlow model has a bijection between the latent state z t and frame x t, leading to potential object occlusion issues. To address this, we plan to incorporate longer memory in the model, such as using recurrent neural networks or memory-efficient backpropagation algorithms. The model is trained on 3 frames to detect the plausibility of a temporally inconsistent frame in the future. VideoFlow model predicts future frames by obtaining a distribution over the 4th frame and assigning decreasing likelihood to frames further in the future. Inspired by the Glow model, VideoFlow uses a latent dynamical system model for video prediction, achieving competitive results with VAE models. VideoFlow achieves competitive results in stochastic video prediction, optimizing log-likelihood directly for faster synthesis compared to autoregressive models. Future work includes incorporating memory for long-range dependencies and applying the model to challenging tasks. The dataset consists of 8-bit videos with added uniform noise to prevent distribution distortion. Additive noise is necessary to prevent infinite densities at datapoints in q(x) and allows for optimization of log-likelihood. Lowering temperature from 1.0 to 0.0 decreases VAE model performance. VideoFlow benefits from low-temperature sampling by balancing noise removal and reduced stochasticity. VAE models show a slightly blurry background with reduced stochasticity as temperature decreases. The VideoFlow model shows high-quality video generation as bits-per-pixel decreases, with training progression correlated to video quality. Hyperparameters include a learning rate schedule, linear warmup, linear-decay schedule, and training with the Adam optimizer. Models were tuned using VGG cosine similarity metric with ground-truth data. The VideoFlow model demonstrates high-quality video generation as bits-per-pixel decreases, with training progression linked to video quality. Hyperparameters include learning rate schedule, linear warmup, linear-decay schedule, and training with Adam optimizer. Models were tuned using VGG cosine similarity metric with ground-truth data. For the SAVP-VAE model, linear decay on learning rate is applied for the last 100K steps. SAVP-GAN tunes gan loss multiplier and learning rate on a logscale. Results show a weak correlation between VGG perceptual metrics and bits-per-pixel. VideoFlow model remains competitive with SVG-LP on VGG perceptual metrics even with 4x parameter reduction."
}