{
    "title": "rJfW5oA5KQ",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in learning complex real-world distributions, but recent works have highlighted issues such as lack of diversity and mode collapse. Theoretical work by Arora et al. (2017a) suggests a dilemma regarding GANs' statistical properties: powerful discriminators can lead to overfitting, while weak discriminators may fail to detect mode collapse. In contrast to previous works on GANs, this paper demonstrates that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by using discriminators with strong distinguishing power against specific generator classes. The discriminators, designed for various generator classes, can approximate the Wasserstein distance and/or KL-divergence, ensuring that the learned distribution closely matches the true distribution and avoids dropping modes. Preliminary experiments on synthetic datasets show a strong correlation between the test Integral Probability Metric (IPM) and KL divergence. Experiments show that the lack of diversity in GANs may be due to sub-optimal optimization rather than statistical inefficiency. Various techniques have been proposed to enhance the quality and stability of GAN training. Recent studies question whether GANs truly learn the target distribution, with some showing a correlation between Integral Probability Metric (IPM) and KL divergence. Recent work has highlighted concerns about mode collapse and lack of diversity in distributions learned by GANs. The paper suggests that designing discriminators with strong distinguishing power can alleviate mode collapse, focusing on the Wasserstein GAN formulation. The F-Integral Probability Metric is used to compare distributions, and WGAN aims to learn the data distribution using a family of generators and discriminators. The F-Integral Probability Metric is used in Wasserstein GANs to learn data distribution with a family of generators and discriminators. Mode collapse is a concern due to the weaker nature of IPM compared to the Wasserstein-1 distance. The issue arises from the learned distribution generating high-quality but low-diversity examples. The problem in Wasserstein GANs arises from the weaker nature of IPM compared to the Wasserstein-1 distance. Increasing the strength of the discriminator to larger families like all 1-Lipschitz functions is a proposed solution, but the Wasserstein-1 distance lacks good generalization properties. The paper addresses the dilemma in GAN theories where powerful discriminators lead to overfitting, while weak discriminators result in diversity issues due to the limitations of IPM in approximating the Wasserstein distance. The proposed solution involves designing a strong discriminator class against a specific generator class to resolve this conundrum. The paper focuses on designing a discriminator class F with restricted approximability against a generator class G to address overfitting and diversity issues in GAN theories. The goal is for the discriminator to approximate the Wasserstein distance W1 for the data distribution p and any q \u2208 G. In this paper, the focus is on designing a discriminator class F with restricted approximability against a generator class G to address overfitting and diversity issues in GAN theories. The discriminator aims to approximate the Wasserstein distance W1 for the data distribution p and any q \u2208 G. The framework allows for non-realizable cases and resolves mode collapse by ensuring that if the IPM between p and q is small, they are also close in Wasserstein distance, preventing significant mode-dropping. Additionally, population-level guarantees can be translated into empirical-level guarantees through capacity bounds such as Rademacher complexity. The paper introduces a theoretical framework for analyzing Wasserstein GANs with polynomial samples, focusing on designing a discriminator class F with restricted approximability against a generator class G to address overfitting and diversity issues. The framework ensures that if the IPM between p and q is small, they are also close in Wasserstein distance, preventing mode collapse. Techniques for designing F are developed for various generator classes, including mixtures of Gaussians and distributions generated by invertible neural networks. In Section 4, the paper studies distributions generated by invertible neural networks. A special type of neural network discriminator with one additional layer than the generator has restricted approximability, ensuring diversity guarantees. The paper discusses distributions produced by invertible neural networks, highlighting the limitations of the invertibility assumption in generating distributions supported on the entire space. It mentions the challenge of matching the support of estimated distributions with the support of natural image distributions residing on low-dimensional manifolds. The KL-divergence is noted as not suitable for measuring statistical distance in cases where distributions have low-dimensional supports. The paper explores the approximation of Wasserstein distance by IPMs for generators with low-dimensional supports, showcasing the advantage of GANs over MLE in learning such distributions. It introduces tools for approximating the log-density of a neural network generator and demonstrates the correlation between IPM and Wasserstein distance for low-dimensional distributions with measure-zero support. The theory suggests that the test IPM could be an alternative for measuring diversity and quality of learned distributions when KL-divergence or Wasserstein distance is not measurable in complex settings. Real datasets often require careful optimization to balance learning of generators and discriminators, leading to sub-optimal IPM. Lack of diversity in experiments may be due to optimization sub-optimality rather than statistical inefficiency. Various empirical tests have been developed to assess diversity, memorization, and generalization in GANs. Mode collapse can be caused by a weak discriminator, leading to a lack of diversity. Several architectures and algorithms have been proposed to address mode collapse with varying success. The curr_chunk discusses the lack of provable solutions in training GANs with quadratic discriminators, highlighting the work of Zhang et al. in showing the IPM as a proper metric. The strength of the work lies in developing statistical guarantees in Wasserstein distance for distributions like injective neural network generators. The curr_chunk discusses the challenges of GAN training with non-parametric setups and the use of invertible generator structures. It suggests that successful GAN training implies learning in KL-divergence when data can be generated by an invertible neural net, indicating real data cannot be generated by such networks. The curr_chunk explains that data cannot be generated by an invertible neural network. It also discusses bounding the closeness between learned and true distributions using injective neural networks and Wasserstein distance. The notion of IPM includes statistical distances like TV and Wasserstein-1 distance. The F-IPM is referred to as the neural net IPM when F is a class of neural networks. The chunk also mentions distances like KL divergence and Wasserstein-2 distance. The Wasserstein-2 distance is defined for distributions with finite second moments. The Rademacher complexity of a function class is discussed, along with the training IPM loss for Wasserstein GAN. Generalization of the IPM is governed by the Rademacher complexity over the distribution space. Theorem 2.1 on generalization is mentioned, along with notation for Gaussian distributions and a comparison between quantities. The curr_chunk discusses designing discriminators with restricted approximability for simple parameterized distributions like Gaussian distributions. It proves that one-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees. The set of Gaussian distributions with bounded mean and well-conditioned covariance is considered, and the discriminators induce restricted approximability with respect to the Gaussian distributions. The curr_chunk discusses extending the concept of designing discriminators with restricted approximability to exponential families and mixture of Gaussians. It shows that linear combinations of sufficient statistics can serve as discriminators with restricted approximability. The proof for the extension to mixture of Gaussians is deferred to the appendix. The curr_chunk discusses discriminators in exponential families, where the log partition function must satisfy certain conditions. It also mentions the Rademacher complexity bound and assumptions regarding the curvature. The curr_chunk discusses designing discriminators with restricted approximability for neural net generators in GANs. It covers invertible neural networks generators with proper densities and extends the results to injective neural networks generators in Section 4.2. In this section, invertible neural networks are considered as generators in GANs. The distribution of the neural networks is parameterized by \u03b8 and \u03b3, allowing for non-spherical variances to impact the output distribution differently. The ability to model data around a \"k-dimensional manifold\" with noise is highlighted. The focus is on the set of invertible neural networks G\u03b8 with standard-layer feedforward nets. In this section, invertible neural networks are used as generators in GANs. The networks are parameterized by \u03b8 and \u03b3, allowing for non-spherical variances to affect the output distribution. The focus is on invertible neural networks G\u03b8 with standard-layer feedforward nets, where the networks are assumed to be invertible with activation functions satisfying certain conditions. The text discusses the use of invertible neural networks in GANs, parameterized by \u03b8 and \u03b3, with activation functions satisfying specific conditions. It highlights the ability of neural networks to implement pseudo-random functions indistinguishable from random functions. The proof involves the change-of-variable formula and the observation that the parameterized family of neural networks likely contains good approximations of log p \u2212 log q. The text discusses the use of invertible neural networks in GANs, parameterized by \u03b8 and \u03b3, with activation functions satisfying specific conditions. It highlights the ability of neural networks to implement pseudo-random functions. The log-det of the Jacobian involves computing the determinant of weight matrices, which can be simplified by adding a bias on the final output layer. Theorem 4.2 states that the discriminator class has restricted approximability with respect to the set of invertible-generator distributions. The proof of Theorem 4.2 in GANs involves using a lemma that connects KL divergence to IPM. It requires log densities to exist and belong to a discriminator family. The lemma states conditions for F and functions in F to be L-Lipschitz. The proof sketch outlines using Wasserstein distance to bound quantities for any p, q \u2208 G. In the context of GANs, transportation inequalities by Bobkov-G\u00f6tze and Gozlan are used to show that if a function is 1-Lipschitz, then the output is sub-Gaussian. In the case of an invertible generator, the upper bound is obtained by ensuring Lipschitz continuity. Two workarounds are proposed to address global Lipschitz constraints, leading to a generalization bound. In the context of GANs, transportation inequalities by Bobkov-G\u00f6tze and Gozlan are used to show that if a function is 1-Lipschitz, then the output is sub-Gaussian. This leads to a generalization bound for invertible generator models. Additionally, injective neural network generators are considered for generating low-dimensional distributions. In the context of GANs, transportation inequalities by Bobkov-G\u00f6tze and Gozlan are used to show that 1-Lipschitz functions lead to sub-Gaussian outputs. This extends to invertible generator models. Injective neural network generators are also explored for creating low-dimensional distributions. Furthermore, a novel divergence method is designed to approximate the Wasserstein distance between distributions generated by neural nets. An additional variable \u03b2 is introduced in the optimization to approximate the Wasserstein distance. Theorem 4.5 states that if d(p^n, q^n) is small for n poly(d), then W(p, q) is also small, preventing mode collapse in neural network generators as long as the discriminator family F has restricted approximability with respect to the generator family G. The IPM W F (p, q) is upper and lower bounded by the Wasserstein distance W 1 (p, q) due to the restricted approximability of discriminator family F with respect to generator family G. Synthetic experiments confirm that the restricted approximability may hold in GAN training, suggesting that optimization difficulty, rather than model capacity, may be the main challenge in practice. The difficulty of GAN training may stem from optimization challenges rather than statistical inefficiency. Experiments show good statistical behaviors on typical discriminator classes. Synthetic experiments with WGANs demonstrate correlations between IPM and Wasserstein distance, as well as IPM and KL divergence. Training GANs on various curves in two dimensions reveals insights into the optimization process. In experiments with WGANs, correlations between IPM and Wasserstein distance are shown. Training GANs on unit circle and Swiss roll curves in two dimensions provides insights into optimization processes. Generators and discriminators use standard two-hidden-layer ReLU nets. The generator and discriminator architectures for the WGANs are 2-50-50-2 and 2-50-50-1 respectively. The RMSProp optimizer is used with learning rates of 10^-4 for both. Two metrics, neural net IPM WF(p, q) and Wasserstein distance W1(p, q), are compared between the ground truth distribution p and the learned distribution q. The empirical Wasserstein distance W1(p, q) is used as a proxy for the true Wasserstein distance. Results are shown in FIG1 for the Swiss roll experiment and FIG4 for the unit circle. The learned generator closely matches the ground truth distribution in both the Swiss roll and unit circle experiments at iteration 10000. The neural net IPM and Wasserstein distance show a strong correlation. Sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence are presented. Discriminators with restricted approximability are designed for good generalization and mode coverage. The techniques presented are tailored to the generator class to avoid mode collapse and have good generalization properties. The goal is to extend these techniques to other distribution families with tighter sample complexity bounds by designing better discriminators and exploring approximation theory in the context of GANs. Theorems and proofs are provided to support the effectiveness of the discriminator family in achieving these goals. The proof of Theorem 3.1 establishes upper and lower bounds for the discriminator family's restricted approximability. It shows that a linear discriminator can be represented as the sum of two ReLU discriminators. The mean distance and covariance distance between two Gaussians are computed, demonstrating the effectiveness of the discriminator family in GANs. The proof establishes bounds for the discriminator family's restricted approximability, showing a linear discriminator can be represented as the sum of two ReLU discriminators. The W 2 distance between two Gaussians is used to bridge the KL and F-distance, with a lower bound of c = 1/(2 \u221a 2\u03c0) established. The proof provides bounds for the discriminator family's restricted approximability, showing a linear discriminator can be represented as the sum of two ReLU discriminators. The W 2 distance between two Gaussians bridges the KL and F-distance, with a lower bound of c = 1/(2 \u221a 2\u03c0) established. The growth of \u2207 log p 1 (x) 2 is bounded, leading to a bound for log p 1 (X) and log p 2 (X). The exponential family property and assumptions on \u2207 2 A are utilized to derive further bounds. The proof establishes bounds for the discriminator family's restricted approximability and generalization. It shows that a linear discriminator can be represented as the sum of two ReLU discriminators. The Rademacher complexity is computed for any \u03b8 \u2208 \u0398, and a mixture of k identity-covariance Gaussians on R d is considered. The family F is suitable for learning mixture of k Gaussians, with bounds provided for restricted approximability and generalization. The Gaussian concentration result will be utilized. The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) is used for convenience in proving the upper and lower bounds for restricted approximability of the discriminator family F. The lower bound is established by considering the regularity properties of the distributions in the Bobkov-Gotze sense. Each mixture component is shown to be 1-sub-Gaussian, leading to the conclusion that X satisfies the Bobkov-Gozlan condition with \u03c3^2 = D^2 + 1. The Bobkov-Gozlan condition is satisfied by X with \u03c3^2 = D^2 + 1. The Rademacher complexity of f \u03b8 is bounded, showing that Y \u03b8 is Lipschitz in \u03b8. The expected supremum over a covering set is then bounded. The log-sum-exp part is D-Lipschitz in X, allowing for reuse of analysis from the Bobkov-Gotze part. This leads to \u03b5 i log DISPLAYFORM8 being 1)-subGaussian, giving sub-Gaussian maxima bounds. Theorem D.2 provides upper bounds for f-contrast by Wasserstein, with various bounds depending on the distributions and gradients of f. The proof involves a truncation argument and bounding terms using Cauchy-Schwarz inequality. It also extends a previous proposition by Polyanskiy & Wu (2016) for completeness. The triangle inequality is used to bound the left-hand side. The inverse of x = G \u03b8 (z) can be computed as DISPLAYFORM0 \u03b8 is a -layer feedforward net with activation \u03c3 \u22121. The problem of representing log p \u03b8 (x) by a neural network is considered, with the log density having the formula DISPLAYFORM1. The inverse network implementing G \u22121 \u03b8 has layers, d2 + d parameters in each layer, and \u03c3 \u22121 as the activation function. By adding branches, the log determinant of the Jacobian can also be computed. The log determinant of the Jacobian can be computed by defining h = W \u22121 (x \u2212 b) and recursively backward. This leads to a neural network that computes log p \u03b8 (x) with no more than + 1 layers and O(d2) parameters. A restricted approximability bound is stated in terms of the W2 distance, and a lower bound is proven for any \u03b8 1 , \u03b8 2 \u2208 \u0398. The log determinant of the Jacobian can be computed by defining h = W \u22121 (x \u2212 b) and recursively backward. This leads to a neural network that computes log p \u03b8 (x) with no more than + 1 layers and O(d2) parameters. A restricted approximability bound is stated in terms of the W2 distance, and a lower bound is proven for any \u03b8 1 , \u03b8 2 \u2208 \u0398. We show that p \u03b8 satisfies the Gozlan condition for any \u03b8 \u2208 \u0398 and apply Theorem D.1. The mapping is L-Lipschitz, and the random variable is L2-sub-Gaussian, satisfying the Gozlan condition with \u03c32 = L2. The Wasserstein distances are used to upper bound W F through Theorem D.2. The log determinant of the Jacobian can be computed by defining h = W \u22121 (x \u2212 b) and recursively backward. This leads to a neural network that computes log p \u03b8 (x) with no more than + 1 layers and O(d2) parameters. A restricted approximability bound is stated in terms of the W2 distance, and a lower bound is proven for any \u03b8 1 , \u03b8 2 \u2208 \u0398. We show that p \u03b8 satisfies the Gozlan condition for any \u03b8 \u2208 \u0398 and apply Theorem D.1. The mapping is L-Lipschitz, and the random variable is L2-sub-Gaussian, satisfying the Gozlan condition with \u03c32 = L2. The Wasserstein distances are used to upper bound W F through Theorem D.2. Recall that DISPLAYFORM0 where h 1 , . . . , h (= z) are the hidden-layers of the inverse network z = G \u22121 \u03b8 (x), and C(\u03b8) is a constant that does not depend on x. We first show the W 2 bound. Clearly log p \u03b8 (x) is differentiable in x. As \u03b8 \u2208 \u0398 has norm bounds, each layer h k is C(R W , R b , k)-Lipschitz in x, so term II is altogether DISPLAYFORM1 For term I, note that h is C-Lipschitz in x, so we have DISPLAYFORM2 Putting together the two terms gives DISPLAYFORM3 Further, under either p \u03b81 or p \u03b82 (for example p \u03b81 ), we have DISPLAYFORM4 Therefore we can apply Theorem D.2(c) and get DISPLAYFORM5 We now turn to the W 1 bound. The bound eq. (22) already implies that for X 2 \u2264 D, DISPLAYFORM6 Choosing D = K \u221a d, for a sufficiently large constant K, by the bound X 2 \u2264 C( Z 2 + 1) we have the tail bound P( X 2 \u2265 D) \u2264 exp(\u221220d). On the other hand by the bound | log p \u03b8 (x)| \u2264 C(( x 2 + 1) 2 /\u03b4 2 + \u221a d( x 2 + 1)) we get under either p \u03b81 or p \u03b82 (for example The log determinant of the Jacobian can be computed by defining h = W \u22121 (x \u2212 b) and recursively backward. This leads to a neural network that computes log p \u03b8 (x) with no more than + 1 layers and O(d2) parameters. A restricted approximability bound is stated in terms of the W2 distance, and a lower bound is proven for any \u03b8 1 , \u03b8 2 \u2208 \u0398. We show that p \u03b8 satisfies the Gozlan condition for any \u03b8 \u2208 \u0398 and apply Theorem D.1. The mapping is L-Lipschitz, and the random variable is L2-sub-Gaussian, satisfying the Gozlan condition with \u03c32 = L2. The Wasserstein distances are used to upper bound W F through Theorem D.2. The log determinant of the Jacobian can be computed by defining h = W \u22121 (x \u2212 b) and recursively backward. This leads to a neural network that computes log p \u03b8 (x) with no more than + 1 layers and O(d2) parameters. A restricted approximability bound is stated in terms of the W2 distance, and a lower bound is proven for any \u03b8 1 , \u03b8 2 \u2208 \u0398. We show that p \u03b8 satisfies the Gozlan condition for any \u03b8 \u2208 \u0398 and apply Theorem D.1. The mapping is L-Lipschitz, and the random variable is L2-sub-Gaussian, satisfying the Gozlan condition with \u03c32 = L2. The Wasserstein distances are used to upper bound W F through Theorem D.2. Additionally, there are two lemmas dealing with discretization error and expected max over a finite set, leading to a bound for \u03b5 \u2264 min {R W , R b } and \u03bb \u2264 \u03bb 0 \u03b4 2 n. The text discusses the computation of the log determinant of the Jacobian in a neural network with restricted approximability bounds and Lipschitzness of hidden layers. The goal is to show bounds for the generalization error and verify results for each layer in the network. The text discusses verifying results for each layer in a neural network with restricted approximability bounds and Lipschitzness. It shows bounds for generalization error and analyzes the Lipschitz properties of the layers. The text discusses the Lipschitz properties of the layers in a neural network with restricted approximability bounds. It shows bounds for generalization error and analyzes the sub-exponentiality of the terms in the network. The text discusses bounding the number of \u0398 by independent covering numbers and volume arguments. It introduces a truncated version of the convolution of p and a Gaussian distribution for p \u03b2 \u03b8, adding Gaussian noise and truncating the distribution to a high-probability region. The text introduces regularity conditions for the family of generators G, including bounds on partial derivatives of f and assumptions on the inverse activation function being Lipschitz. It also mentions asymptotic notation for notational convenience. The main theorem states that for certain F, d, F approximates the Wasserstein distance. The proof involves a parameterized family F that can approximate the log density of p for every p in G. The main theorem involves a parameterized family F that approximates the Wasserstein distance for certain F and d. The proof includes neural networks N1, N2 that approximate the log density of p for every p in G using Laplace's method of integration. The proof of Theorem E.1 involves neural networks N1 and N2 from family F approximating log densities of p and q. By setting f = N1(x) - N2(x), a lower bound is proven using the Bobkov-G\u00f6tze theorem. For the upper bound, setting \u03b2 = W^(1/6) is sufficient. The optimal coupling C of p, q and induced coupling Cz on latent variable z are considered. The proof of Theorem E.1 involves neural networks approximating log densities of p and q. The optimal coupling of p, q and induced coupling on latent variable z are considered. The rest of the section is dedicated to the proof of Theorem E.2. The proof of Theorem E.1 involves neural networks approximating log densities of p and q, with the optimal coupling of p, q and induced coupling on latent variable z. The algorithm approximates the integral and the Lipschitz constant of the neural network is discussed. The proof of Theorem E.2 is presented. The algorithm presented approximates the integral and can be implemented by a small, Lipschitz network. It involves calculating gradients and approximate eigenvector/eigenvalue pairs to achieve the desired results. The Algorithm 1 approximates the integral using Lemma E.8 with a different division of the integral. The output of the \"invertor\" circuit is denoted as \u1e91 = N inv (x). A set B is defined as {z : | z \u2212 \u1e91, e i | \u2264 \u03b4}. Matrices E i are chosen from a \u03b2 2 -net S with spectral norm bounded by O(1/\u03b2 2 ). There exist matrices E 1 , E 2 , ..., E r such that if M \u2208 S, at least one of the matrices M + E i has eigenvalues that are \u2126(\u03b2)-separated. The proof of Theorem E.9 utilizes the same approximation method. The integral in DISPLAYFORM17 can be evaluated similarly to Theorem E.9, with a bound on T \u03b2 holding universally on a neighborhood of radius D x. Part (3) is easily derived from FORMULA0 and FORMULA1 in DISPLAYFORM18. Synthetic WGAN experiments are conducted with invertible neural net generators and discriminators designed with restricted approximability. The goal is to show that the empirical IPM W F (p, q) correlates well with the KL-divergence between p and q on synthetic data. Data is generated from a ground-truth invertible neural net generator. The text discusses the use of invertible neural net generators and discriminators in training, with a focus on designing the discriminator architecture for restricted approximability. Training involves generating batches from both ground-truth and trained generators to solve a min-max problem. The discriminator's log \u03c3 \u22121 function is modeled as a trainable neural network due to its non-differentiability. Constraints are added to all parameters in accordance with Assumption 1. The text discusses training with invertible neural net generators and discriminators, focusing on designing the discriminator architecture for restricted approximability. Training involves generating batches from both ground-truth and trained generators to solve a min-max problem in the Wasserstein GAN formulation. Evaluation metrics include KL divergence and training loss. During training, the GAN loss is carefully balanced between discriminator and generator steps, potentially deviating from the true W F. A separately optimized WGAN loss is occasionally reported, where the discriminator is trained without regularization. This approach aims to find an approximate maximizer for contrast in F, indicating KL divergence. The theory suggests that WGAN can learn the true generator in KL divergence, with the F-IPM serving as an evaluation metric for it. In experiments, a two-layer neural net is used as the generator in a 10-dimensional space. The discriminator is trained with either Vanilla WGAN or WGAN-GP, with results showing that WGAN training with a specific discriminator design can accurately learn the true distribution in terms of KL divergence. The study demonstrates that GANs are successful in finding the true distribution without mode collapse, as indicated by a KL lower than 1. The W F (eval) and KL divergence are closely related, with gradient penalty improving optimization. Using vanilla fully-connected discriminator nets also shows correlation with KL-divergence, highlighting the effectiveness of the discriminator design in learning the true distribution. The KL-divergence between the true distribution and learned distribution, estimated IPM, and training loss are closely related. The inferior performance of the WGAN-Vanilla algorithm in KL divergence is attributed to training performance rather than statistical properties. This phenomenon is likely to occur in training GANs with real-life data as well. In this section, the correlation between perturbations and KL divergence is tested by comparing it with neural net IPM on pairs of perturbed generators. The results show a clear positive correlation between the KL divergence and neural net IPM, supporting the theory that the neural net distance scales linearly in the KL divergence. In Section G.2, experiments were redone using vanilla fully-connected discriminator nets with a three-layer net. Results showed that generators converged well in KL divergence, but correlation was slightly weaker compared to restricted approximability. Vanilla discriminator structures may be satisfactory for good generator performance. The KL-divergence between true and learned distributions, estimated IPM, and training loss were analyzed during training. The correlation between KL and neural net IPM was found to be 0.7489 with vanilla fully-connected discriminators."
}