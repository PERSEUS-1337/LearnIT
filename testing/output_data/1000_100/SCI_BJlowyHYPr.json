{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new type of recurrent neural model designed for forecasting data streams from geospatial point-cloud sources. It utilizes a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. This operator can be integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM is applied to mobile service traffic and air quality forecasting, showing accurate long-term predictions with real-world datasets. Point-cloud stream forecasting predicts future values from geospatial point-cloud data sources like mobile network antennas and air quality sensors. It outperforms traditional spatiotemporal forecasting on grid-structural data. Point-cloud stream forecasting operates on irregular sets of points with complex spatial correlations, unlike grid-structural data. Vanilla LSTMs have limited spatial feature capabilities, while ConvLSTM and PredRNN++ are restricted to grid-structural data. Different approaches to geospatial data stream forecasting include predicting over grid-structured data with ConvLSTMs and mapping point-cloud input to a grid for scattered point-clouds. The proposed PointCNN leverages spatial-local correlations of point clouds, regardless of input order. However, existing architectures are limited in discovering temporal dependencies. The concept of forecasting over point cloud-streams is detailed, introducing the DConv operator in the CloudLSTM architecture. CloudLSTM and its variants are presented, along with combining CloudLSTM with Seq2seq learning. The CloudLSTM architecture is introduced to combine with Seq2seq learning and attention mechanisms for precise forecasting over point-cloud streams. A point-cloud is defined as a set of points, each containing value features and coordinates. The model aims to embrace five key properties for ideal point-cloud stream forecasting. The CloudLSTM architecture combines Seq2seq learning and attention mechanisms for accurate point-cloud stream forecasting. It aims to adhere to five key properties: order invariance, information intactness, interaction among points, robustness to transformations, and location variance. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of CloudLSTM, aiming to satisfy key properties such as order invariance and information intactness. DConv generalizes convolution on point-clouds, maintaining the input's information intactness by outputting the same number of elements as the input. The Dynamic Point Cloud Convolution (DConv) operator, a core module of CloudLSTM, maintains order invariance and information intactness. It generalizes convolution on point-clouds, preserving input information by outputting the same number of elements. The DConv sums element-wise products over features and points in a subset to obtain values and coordinates for the output point cloud. The Dynamic Point Cloud Convolution (DConv) operator in CloudLSTM maintains order invariance and information intactness by aggregating coordinate features with learnable weights shared across anchor points. Scalar weights and biases are defined for input and output channels, with a sigmoid function limiting predicted coordinates. The Dynamic Point Cloud Convolution (DConv) operator in CloudLSTM normalizes raw point-cloud coordinates to (0, 1) for robustness. The K nearest points vary per channel due to different types of measurements, such as mobile app traffic consumption and air quality indicators. Spatial correlations differ based on human mobility, affecting data consumption patterns and air quality readings. The CloudLSTM introduces the DConv operator, which weights its K nearest neighbors across all features to produce values and coordinates in the next layer. DConv is a symmetric function that does not depend on input order, ensuring consistent output. It is performed on every point in a set and generates the same number of features and points in its output. DConv is a symmetric function that operates on neighboring point sets to capture local dependencies and improve robustness to global transformations. Normalization over coordinate features enhances this robustness. DConv learns the layout and topology of the cloud-point for the next layer, enabling dynamic positioning tailored to each channel and time step, essential for spatiotemporal forecasting neural models. The DConv operator, based on PointCNN and Deformable Convolution, is designed for pointcloud structural data, ensuring order invariance while avoiding information loss through aggregation over points. It can be efficiently implemented using 2D convolution and is detailed in Appendices A and B for complexity analysis. The DConv operator maintains permutation by aligning distance rankings between points, ensuring order invariance without information loss. It can be seen as a DefCNN over point-clouds, with differences in deforming input maps and selecting neighboring points for operations. DConv allows adaptive receptive fields and can be integrated into LSTMs for learning spatial and temporal correlations over point-clouds. The Convolutional Point-cloud LSTM (CloudLSTM) integrates spatial and temporal correlations over point-clouds using input, forget, and output gates, memory cell, and hidden states. It combines CloudLSTM with Seq2seq learning and soft attention mechanism for enhanced performance. The Seq2seq CloudLSTM model integrates Seq2seq learning and the soft attention mechanism for forecasting. The architecture includes an encoder and decoder with CloudLSTMs, connected via a context vector. Data is processed by CloudCNN layers before generating predictions. In this study, a two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell is employed for natural language processing tasks. The DConv is also explored in vanilla RNN and Convolutional GRU, leading to CloudRNN and CloudGRU. These models share a similar Seq2seq architecture with CloudLSTM but do not use the attention mechanism. Performance is evaluated using traffic datasets from mobile services recorded at network antennas. The study employs the CloudLSTM model to forecast mobile service demands and air quality indicators, comparing it with 12 baseline deep learning models. The models are implemented using TensorFlow and TensorLayer, trained on a computing cluster with NVIDIA Tesla GPUs, and optimized using the Adam optimizer. Experimental results are reported after discussing the datasets and baseline models used. In the experimental results, forecasting tasks on spatiotemporal point-cloud streams in 2D geospatial environments are conducted. Data from two scenarios for each use case is analyzed, with a focus on mobile traffic forecasting in European metropolitan areas. The data includes traffic volume from devices associated with antennas in urban regions, treated as 2D point clouds. Coordinate features are omitted in the final output for fixed data sources, but are necessary for crowd mobility forecasting. The dataset for mobile traffic forecasting consists of 24,482 traffic snapshots from 38 different mobile services in urban regions. Air quality forecasting is done using a dataset with six air quality indicators collected by 437 monitoring stations in China over a year. The dataset includes 8,760 snapshots for each cluster, with missing data filled using linear interpolation. Measurements are transformed into input channels of the point-cloud S, normalized to (0, 1) range. Point-clouds are transformed into grids for baseline models. Training plus validation and test sets ratio is 8:2. Performance of CloudLSTM is compared with baseline models like PointCNN. CloudCNN and PointLSTM are original benchmarks introduced for feature extraction from point clouds. CloudLSTM is compared with baseline models like PointCNN, MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++. The accuracy of CloudLSTM is evaluated in terms of Mean Absolute Error. For mobile traffic prediction, CloudLSTM is evaluated using MAE, RMSE, PSNR, and SSIM metrics over a time horizon of 30 minutes and 3 hours. Various neural networks like LSTM, ConvLSTM, PredRNN++, CloudRNN, and CloudGRU are used for forecasting. In the air quality forecasting use case, models receive 12 hours of measurements as input and forecast indicators for the following 12 hours. The prediction steps are extended to 72 hours for RNN-based models. Performance is evaluated for 4,888 instances across the test set, showing that RNN-based architectures outperform CNN-based models and MLP. CloudLSTM, CloudRNN, and CloudGRU variants achieve lower MAE/RMSE and higher PSNR/SSIM on urban scenarios. Among the RNN-based architectures, CloudLSTM performs the best, followed by CloudGRU and then CloudRNN. The CloudLSTM model shows consistent forecasting performance regardless of the number of neighbors used. Additionally, the attention mechanism improves forecasting by capturing better dependencies between input sequences and vectors in decoders. This effect has been observed in other NLP tasks as well. The complete evaluation is provided in Appendix H, including long-term forecasting performance up to 36 time steps. In city 1, the MAE remains stable for most models, indicating reliable long-term forecasting. For city 2, low K values may impact CloudLSTM's performance in the long term. CloudLSTMs outperform ConvLSTM in air quality forecasting, achieving up to 12.2% and 8.8% better MAE and RMSE, respectively. The CloudCNN outperforms the PointCNN in predicting mobile traffic, with lower K values yielding better performance. Results show the effectiveness of CloudLSTM models for spatiotemporal point-cloud stream data. Performance evaluations for long-term forecasting up to 72 future time steps are conducted on RNN-based models. CloudLSTM, a neural model for spatiotemporal forecasting on pointcloud data streams, utilizes DConv for spatial feature learning and permutation invariance. Comparisons show D-Conv's significant contribution to performance improvements over other models like ConvLSTM and PredRNN++. Attention mechanisms in CloudLSTM have minimal impact on performance. Core operator, RNN structure, and attention are ranked by their contribution, with core operator being the most influential. The DConv is a flexible method that can be combined with various RNN models and attention mechanisms. It can be efficiently implemented using a standard 2D convolution operator, transforming input and output tensors for processing. The algorithm for DConv implementation involves finding nearest neighbors and performing convolution steps. The DConv method can be implemented efficiently using a standard 2D convolution operator. The complexity of DConv is analyzed by separating the operation into finding nearest neighbors and performing weighting computation steps. The complexity of finding K nearest neighbors for one point is close to O(K \u00b7 L log N) if using KD trees, while the complexity of computing weighting is determined by Eq. 2. The complexity of DConv is O((H + L) \u00b7 K) for computing one feature of the output point set. DConv introduces extra complexity by searching K nearest neighbors for each point, O(K \u00b7 L log N). Normalization of coordinate features enables transformation invariance with shifting and scaling. By normalizing the coordinates, the model becomes invariant to shifting and scaling transformations. The proposed CloudLSTM is combined with an attention mechanism. The context tensor for encoder state i is represented with a score function e i,j. The proposal is compared against baseline models like MLP, CNN, and 3D-CNN in mobile traffic forecasting. DefCNN learns convolutional filter shapes and is similar to the DConv operator. The ConvLSTM, PredRNN++, CloudRNN, and CloudGRU models are discussed in relation to spatiotemporal forecasting. The CloudRNN and CloudGRU share a Seq2seq architecture with CloudLSTM but do not use the attention mechanism. The detailed configuration and number of parameters for each model are shown in Table 3. Increasing the number of layers did not improve performance for ConvLSTM, PredRNN++, and PointLSTM. In the context of spatiotemporal forecasting, the performance of ConvLSTM, PredRNN++, and PointLSTM did not improve with increased layers. 3x3 filters are commonly used in image applications with a receptive field of 9, equivalent to K=9 in CloudLSTMs. Different configurations of 2-stack Seq2seq CloudLSTM with varying channels and K values were tested, along with an attention mechanism. All architectures were optimized using the MSE loss function for forecasting mobile traffic volume and air quality indicators. The performance of ConvLSTM, PredRNN++, and PointLSTM did not improve with increased layers in spatiotemporal forecasting. Evaluation metrics such as MAE, RMSE, PSNR, and SSIM were used to assess model performance. PSNR is calculated based on the average and maximum traffic recorded for all services/quality indicators at all antennas/monitoring stations. Anonymized locations of antenna sets in cities were shown, with data collected via deep packet inspection at the packet gateway. Data is collected via deep packet inspection at the packet gateway using proprietary traffic classifiers to associate flows to specific services. The anonymized locations of antenna sets in cities are shown, and all measurements were conducted under the supervision of the national privacy agency in compliance with regulations. The dataset used for the study only provides mobile service traffic information at the antenna level, ensuring full anonymization and no personal subscriber information. The dataset used for analysis is fully anonymized, ensuring no privacy concerns. Raw data cannot be made public due to a confidentiality agreement. The analysis includes 38 different services, with streaming dominating traffic consumption. Other services like web, cloud, social media, and chat also consume significant fractions of mobile traffic. Gaming accounts for only 0.5% of the demand. The air quality dataset from 43 cities in China, collected by Microsoft Research, contains 2,891,393 records from 437 monitoring stations over a year. Stations are divided into two clusters, A with 274 stations and B with 163. Missing data was filled using linear interpolation. Evaluation of forecasting accuracy for mobile services with Attention CloudLSTMs is presented in MAE graphs. The MAE evaluation on service and category basis shows that CloudLSTMs perform similarly in both cities. Services with higher traffic volume have higher prediction errors due to frequent fluctuations. Long-term air quality forecasting MAE increases with time for all models. Larger K in CloudLSTM improves performance over time. The performance of CloudLSTM improves with larger K, leading to slower MAE growth over time. Evaluation includes visualizing hidden features for insights into model knowledge. Scatter distributions of hidden states in CloudLSTM and Attention CloudLSTM are shown in Fig. 10. Each Ht has 1 value feature and 2 coordinate features per point. In Fig. 10, scatter distributions of hidden states in CloudLSTM and Attention CloudLSTM are visualized. The performance of CloudLSTM improves with larger K, showing slower MAE growth over time. In Fig. 11 and 12, NO2 forecasting examples in City Cluster A and B are displayed, generated by RNN-based models, with Attention CloudLSTMs offering better predictions capturing trends in point-cloud streams. The CloudLSTM model uses DConv with Sigmoid functions to improve forecasting accuracy by refining the positions of input points at each time step. This allows the model to work well with outlier points, as demonstrated using DBSCAN to identify outliers in air quality datasets. The CloudLSTM model, utilizing DConv with Sigmoid functions, excels in forecasting accuracy by handling outlier points effectively. Through DBSCAN, outliers in air quality datasets are identified, with CloudLSTM showing the lowest prediction error compared to other models. Additionally, CloudCNN with DConv operator outperforms CNN, 3D-CNN, DefCNN, and PointCNN in forecasting performance. Further experiments demonstrate the robustness of CloudLSTM to outliers in controlled scenarios. The CloudLSTM model, using DConv with Sigmoid functions, excels in forecasting accuracy by effectively handling outlier points. Outliers are moved away from the center by different distances on both x and y axes, with the model showing robustness to outliers in forecasting performance. CloudLSTM outperforms PointLSTM significantly, demonstrating its ability to handle outliers effectively. Our CloudLSTM model outperforms MLPs and LSTMs that rely on k-nearest neighbors for forecasting, showing superior performance on the air quality dataset. The number of neighbors K impacts the model's receptive field, with a small K focusing on local spatial dependencies and a large K considering global spatial correlations between points. The CloudLSTM model outperforms MLPs and LSTMs in forecasting by extracting local spatial dependencies through DConv kernels and merging global spatial dependency via stacks of time steps and layers. Seasonal information in mobile traffic series can be utilized to enhance forecasting performance, but directly feeding the model with data spanning multiple days is impractical due to the length of sequences. To efficiently capture seasonal information in mobile traffic data, sequences are concatenated to form inputs with a length of 90. Experiments on a subset of antennas show improved forecasting performance with seasonal information. However, this increases model complexity, indicating the need for future work in this area. Future work will focus on a more efficient way to fuse seasonal information in mobile traffic data, with a marginal increase in model complexity."
}