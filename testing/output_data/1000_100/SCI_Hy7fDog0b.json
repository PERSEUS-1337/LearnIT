{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions, but current training techniques require fully-observed samples. In scenarios where obtaining full samples is costly or impossible, learning implicit generative models from partial, noisy observations is challenging. A new method called AmbientGAN is proposed to train Generative Adversarial Networks (GANs) using lossy measurements. This approach shows promising results on benchmark datasets, with qualitative and quantitative improvements of 2-4 times. Generative models trained with our method achieve 2-4 times higher inception scores than baselines by training directly from noisy or incomplete samples. The generator output is passed through a simulated random measurement function, and the discriminator distinguishes between real and generated measurements. The approach addresses the challenge of training generative models when full samples are costly or unavailable, showing that even projections or different types of measurements can be used for recovery. Our approach, AmbientGAN, trains generative models by distinguishing real measurements from simulated measurements of generated images. It is effective in constructing models from noisy observations and low-dimensional projections with information loss. The method demonstrates good visual quality and higher inception scores compared to baseline methods. The curr_chunk discusses theoretical and empirical results of measuring image quality quantitatively using noisy and blurred versions of images. It highlights the importance of finding a generative model that matches the true distribution in a GAN game. The empirical work explores measurement models for image quality using noisy and blurred images. Results show improvements in sample quality by incorporating the measurement process into GAN training. The study compares Wiener deconvolution to their models for generating cleaner samples from 2D images in the MNIST dataset. They use pairs of 1D projections to measure image quality, finding that AmbientGAN recovers underlying structure well. Different approaches to neural network generative models are discussed, including autoregressive and adversarial methods, with some successful combination approaches. The adversarial framework is powerful for modeling complex data distributions like images, video, and 3D models. Generative models have various applications, including solving inverse problems and translating images between domains using GANs. Training stability can be improved by operating generators and discriminators on different low-dimensional projections of data. Our work is closely related to BID10, where 3D object shapes are created from 2D projections. The AmbientGAN framework involves creating 2D projections using weighted sums of voxel occupancies. Measurements are performed on samples from a real underlying distribution, with each measurement output by a stochastic measurement function parameterized by \u03b8. Our task is to create an implicit generative model of a distribution p_r_x using a set of IID realizations {y1, y2, ..., ys} from the distribution p_r_y. We combine the measurement process with adversarial training, similar to a standard GAN setup, by using a random latent vector Z from a distribution p_z to sample from p_r_x. The goal is to learn a generator G to match distribution p_g x with p_r x using random measurements on generated objects X g. A discriminator distinguishes real from fake measurements, with a quality function q(\u00b7) defining the objective. The AmbientGAN objective is defined based on the discriminator output using quality functions. The model is implemented with differentiable neural networks and can be trained using a gradient-based GAN training procedure. Stochastic gradients are computed for parameters in G and D by backpropagation using sampled inputs. Updates alternate between parameters of D and G, and the approach is compatible with various GAN improvements. The AmbientGAN learning framework is versatile and can incorporate additional information like per sample labels. Different versions of GANs are used in experiments, including DCGAN, Wasserstein GAN, and Auxiliary Classifier Wasserstein GAN. The measurement models used are tailored for 2D images but can be adapted for other data formats. The measurement models for 2D images include Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, and Pad-Rotate-Project. These models manipulate the image data in various ways to generate measurements for further processing. In image processing, different measurement models like PadRotate-Project-\u03b8 and Gaussian-Projection are used to generate measurement vectors for further analysis. The goal is to recover the true underlying distribution of the image data based on the observed measurements, ensuring consistency with the AmbientGAN training procedure. The AmbientGAN training procedure ensures consistency in recovering the true underlying distribution of image data using various measurement models like Gaussian-Projection and Convolve+Noise. The uniqueness of the true distribution given the measurement distribution is a key assumption, which is satisfied under different measurement models, allowing for recovery with the AmbientGAN framework. Theorem 5.4 assumes a finite discrete set of pixel values, which is common in practical scenarios. It provides a sample complexity result for learning distributions in the AmbientGAN framework under the Block-Pixels measurement model. If the probability of blocking a pixel is less than 1, a unique distribution can induce the measurement. The text discusses the use of different datasets for experiments, including MNIST, CelebA, and CIFAR-10. Various generative models were used, with more details provided in the appendix. The text discusses the use of different datasets for experiments, including MNIST, CelebA, and CIFAR-10, with various generative models used. The architectures of the models are based on previous works, and specific discriminator architectures are used for different types of measurements. Baseline approaches were implemented to evaluate the performance of the AmbientGAN framework on creating an implicit generative model for p r x. The \"ignore\" baseline ignores measurements and learns a generative model directly on them. A stronger baseline involves inverting measurement functions to obtain full-samples for generative model learning. In the AmbientGAN setting, we aim to approximate an inverse function to obtain x i from measurement y i = f \u03b8i (x i ). Different methods are used for various measurement models, such as blurring images for Block-Pixels measurements. This allows us to train a generative model using the estimated inverse samples. In the AmbientGAN setting, different methods are used for various measurement models to approximate an inverse function. Total variation inpainting is implemented to fill in pixels from the surrounding. For Convolve+Noise measurements, Wiener deconvolution is used with a Gaussian kernel. Block-Patch measurements utilize the Navier Stokes based inpainting method BID2. Other measurement models present challenges in obtaining an approximate inverse function. Keep-Patch and Extract-Patch measurements pose difficulties due to lack of information. Pad-Rotate-Project-\u03b8 measurements require sampling multiple angles for inverting the Radon transform, but limited projections hinder this process. In the AmbientGAN setting, various methods are used to approximate an inverse function for different measurement models. Inverting Pad-Rotate-Project measurements is challenging due to limited information about angles. Results with AmbientGAN models are reported for a subset of experiments, showing samples generated by baselines and our models. Samples are heavily degraded in the measurement process, making it difficult for baselines to produce good results. Additional results are provided in the appendix. Our models outperform baselines in creating high-quality images by using Gaussian kernel and IID Gaussian noise for measurements. Results on celebA with DCGAN show that baselines drown measurements in noise, while our models produce coherent faces by observing parts of one image at a time. Pad-Rotate-Project and Pad-Rotate-Project-\u03b8 models result in signal degradation, losing most information in samples. Our experiments use two measurements at a time, with results on MNIST showing the limitations of the first model in learning rotations. The first model in MNIST with DCGAN learns up to rotation and reflection, generating digits with consistent orientations per class. The second model includes rotation angles, producing upright digits. Despite lower visual quality, our method shows the ability to generate digit images from 1D projections. However, on celebA dataset, the model trained with Pad-Rotate-Project-\u03b8 measurements only captures a crude outline of a face, emphasizing the challenge of learning complex distributions with limited projections. In learning complex distributions with 1D projections, better understanding of distribution recovery and GAN training is needed. Inception scores are used to quantify generative model quality in the AmbientGAN framework. Different models were trained on MNIST with varying pixel blocking probabilities, showing the relationship between inception scores and pixel blocking. In Fig. 7 (left), scores as a function of p show that AmbientGAN models outperform baseline models as pixel blocking increases. For Convolve+Noise measurements on MNIST with varying noise levels, AmbientGAN models maintain high inception scores compared to Wiener deconvolution and \"ignore\" baseline. In Fig. 7 (right), the inception score is plotted against \u03c3, showing the deterioration of baseline models with increasing noise levels. The Pad-Rotate-Project model achieves an inception score of 4.18, while the model with Pad-Rotate-Project-\u03b8 measurements scores 8.12. The vanilla GAN model reaches a score of 8.99. The second model trained on 1D projections performs close to the fully-observed case. Total variation inpainting is slow and performs similarly to unmeasured-blur baseline on MNIST. Inpainting baselines are not run on CIFAR-10. Our approach outperforms baselines in terms of inception score on CIFAR-10 dataset. We aim to learn a distribution from incomplete, noisy data to construct new generative models. The data distribution, parameter distribution, and measurement distribution are key components in our approach. The text discusses the uniqueness of probability distributions in matching measurement distributions in the context of the vanilla GAN model. The underlying distribution must match all 1D marginals to converge to the true distribution. The text discusses the uniqueness of probability distributions in matching measurement distributions with Convolve+Noise model. If certain conditions are met, there is a unique distribution that can induce the measurement distribution. This implies a bijective map between X and Z, where Z = h(X) and h is a continuous transformation. The text discusses the uniqueness of probability distributions in matching measurement distributions with Convolve+Noise model. It explains the bijective map between X and Z, where Z = h(X) and h is a continuous transformation. The proof involves relating the pdfs of X and Z, and using a reverse map to determine the true underlying distribution. The text discusses the empirical version of the vanilla GAN objective and the optimal discriminator for the empirical objective. It also presents a proof related to the Empirical Risk Minimization (ERM) version of the loss and Theorem 5.4 regarding image pixel values in a finite set. The text discusses the Block-Pixels measurement model with a probability of blocking a pixel. It introduces a distribution over measurements using a transition matrix A. If A is invertible, the distribution p x can be recovered from p y. The sample complexity is determined by the minimum eigenvalue of A, which is guaranteed to be greater than 0. The text discusses the sample complexity in the context of Block-Pixels measurement. It introduces a distribution over measurements using a transition matrix A. The optimal generator must satisfy certain conditions, and the probability of success is determined by the minimum eigenvalue of A. The specific case of Block-Pixels measurement involves dividing images into classes based on the number of zero pixels, and considering the transition matrix A for these classes. The transition matrix for Block-Pixels measurement is lower triangular, with diagonal entries representing unaffected images. The minimum eigenvalue is (1 - p) n, proving A is invertible. The DCGAN model on MNIST follows a specific architecture. The DCGAN model on MNIST uses a 100-dimensional noise input sampled uniformly on [-1, 1], with two linear and two deconvolutional layers for the generator, and two convolutional layers followed by two linear layers for the discriminator. Batch-norm is applied. The WGANGP model on MNIST has a generator with a 128-dimensional latent vector sampled uniformly on [-1, 1], using one linear and three deconvolutional layers, and a discriminator with three convolutional layers followed by one linear layer. Batch-norm is not used. The unconditional DCGAN model on celebA has a 100-dimensional latent vector. The DCGAN model on celebA uses a 100-dimensional latent vector with Uniform distribution. The generator has one linear layer and four deconvolutional layers, while the discriminator has four convolutional layers and a linear layer. Batch-norm is applied in both. The ACWGANGP model on CIFAR-10 uses a 128-dimensional latent vector sampled from a standard Gaussian distribution. The generator includes a linear layer, three residual blocks, and conditional batch normalization. The discriminator consists of one residual block with convolutional layers. The discriminator in the current model consists of one residual block with convolutional layers followed by three residual blocks and a final linear layer. The experiment demonstrates the robustness of the AmbientGAN approach to mismatches in the parameter distribution of the measurement function. The AmbientGAN approach is tested using the Block-Pixels measurement model on the MNIST dataset. The generator's performance is evaluated by training models with different blocking probabilities, showing robustness at p = 0.5. The learned generator captures data distribution well and is used for compressed sensing, improving sensing over sparsity-based approaches. Using an AmbientGAN trained with corrupted samples, we achieved a reduction in the number of measurements for compressed sensing compared to a regular GAN trained with fully observed samples. The reconstruction error vs the number of measurements plot showed similar results between Lasso and AmbientGAN."
}