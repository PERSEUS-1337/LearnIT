{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complexity of deep networks makes it impossible to explain hidden features in a way that humans can understand. However, a new method using \\textit{M-of-N} rules has been proposed to map the landscape of rules describing hidden features in Convolutional Neural Networks (CNNs), showing a balance between comprehensibility and accuracy. The landscape of rules in deep networks shows an optimal tradeoff between comprehensibility and accuracy. Each latent variable follows an optimal \\textit{M-of-N} rule. Rules in the first and final layers are highly explainable, while those in the second and third layers are less so. This sheds light on the feasibility of rule extraction from deep networks and the importance of decompositional knowledge extraction for explainability in AI. Knowledge extraction aims to increase the explainability of neural networks by uncovering the implicit knowledge learned in their weights. This involves translating trained neural networks into symbolic rules or decision trees, similar to those in symbolic AI and logic programming. Rule extraction techniques in symbolic AI, ML, and logic programming have been developed over the years. These techniques typically take a decompositional or pedagogical approach to generate rules from network parameters or behavior. The complexity of extracted rules remains a major issue, making knowledge extraction challenging, especially with decompositional methods. The distributed nature of neural networks, with patterns of activity over many neurons, plays a crucial role in their capabilities. Some argue that explaining latent features using symbolic knowledge extraction is a dead end, suggesting distillation methods instead. However, the efficacy of distillation for improving robustness is debated. Other practical approaches focus on obtaining guarantees rather than opening the black box. In this paper, a method for empirically examining the explainability of latent variables in neural networks is developed. Rule extraction is used to search through a space of rules describing a latent variable, measuring error and complexity. The relationship between rule complexity and accuracy in capturing network behavior is mapped out. Applying this method to a 4-layer CNN trained on fashion MNIST reveals varying accuracy of rules across layers. The discovery of a 'critical point' on the rule extraction landscape reveals an ideal M-of-N rule for each latent variable in neural networks. The accuracy of rules varies across layers, with convolutional layers showing more complex rules compared to fully connected layers. Rules in the first and final layers have near 0% error, while those in the second and third layers have up to 15% error. Previous algorithms for knowledge extraction are briefly discussed in Section 2. In Section 2, previous algorithms for knowledge extraction are briefly discussed. Section 3 defines accuracy and complexity for M-of-N rules and outlines the extraction process. Experimental results of rule extraction for accuracy/complexity mapping are presented in Section 4, followed by a conclusion in Section 5. The KBANN algorithm was one of the first attempts at knowledge extraction, using a decompositional approach for extracting symbolic rules. More advanced algorithms generate binary trees with M-of-N rules at each node. The more recent algorithms for knowledge extraction focus on selecting an M-of-N rule based on maximum information gain with respect to the output. These methods treat the model as a black box and can be queried to generate data for rule extraction. Other extraction methods combine pedagogical and decompositional approaches, while some opt for visually oriented techniques. Various methods have been developed to solve the 'black-box' problem of neural networks. The problem of neural networks in deep networks lies in the complexity of explaining arbitrary hidden features due to multiple layers. Decompositional techniques may produce overly complex rules, making end-to-end explanation impractical. However, some layers in deep networks may have explainable rules, as shown in experiments. In deep networks, some layers may have explainable rules that can clarify the network's behavior in terms of certain features. Rule extraction could aid in modular explanation of network models and offer insights into latent feature disentanglement. Logical rules in programming are defined as implications A \u2190 B, where A is the head and B is the body of the rule. In neural networks, rules are used to explain network behavior based on neuron states. Literal definitions are based on neuron activation values, with M-of-N rules commonly used for rule extraction. M-of-N rules provide a compact representation for rule extraction in neural networks, allowing for a more general and structured explanation of neuron behavior. They offer a flexible approach by requiring only a subset of variables to be true, reflecting input/output dependencies and resembling weightless perceptrons. M-of-N rules are like weightless perceptrons in neural networks, with the output neuron representing the head and visible neurons representing the body of the rule. Setting bias and weights accordingly allows for encoding M-of-N rules. This approach, often forgotten, is brought back for explainability in neural networks. Splitting values for continuous neurons are chosen using information gain. Values for continuous neurons are selected based on information gain. A literal for a target neuron is generated by choosing a split that maximizes the decrease in entropy of network outputs. Input literals are then created by maximizing information gain with respect to the target literal. Each target literal in a layer has its own set of input literals, corresponding to the same input neurons but with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches. In rule extraction, the focus is on the accuracy and comprehensibility of rules derived from feature maps in a neural network. Accuracy is measured by the expected difference between rule predictions and network outputs. By determining the truth of literals using the network state, rules can be defined based on the relationship between variables. When extracting rules from a neural network, accuracy is measured by the discrepancy between rule predictions and network outputs. Comprehensibility is assessed based on the complexity of a rule, determined by the length of its body in disjunctive normal form. The complexity of rules extracted from a neural network is measured by the length of its body in disjunctive normal form (DNF). The complexity is normalized relative to a maximum complexity, with an example given for a perceptron rule. A 1-of-2 rule is the most complex for 2 variables, with a complexity of 1. The complexity of rules extracted from a neural network is measured by the length of its body in disjunctive normal form (DNF), with the longest DNF having a complexity of 1. A loss function for a rule is defined using a parameter \u03b2 to balance soundness and complexity. By using a brute force search procedure with various values of \u03b2, the relationship between rule complexity and accuracy is determined. For \u03b2 = 0, the rule with minimum loss will have minimum error regardless of complexity, while for large \u03b2, the rule with minimum loss will have 0 complexity. Neurons and splits are generated to create sets of literals for rules. Neurons and splits are generated to create sets of literals for rules by negating literals corresponding to neurons with negative weights. The search procedure minimizes rule complexity by reordering variables based on weight magnitude and considering M-of-N rules. Information gain is maximized to define literals for each neuron and input. The most accurate M-of-N rules are defined by the literals corresponding to neurons with the strongest weights. Ordering literals by information gains rather than weights may not be necessary, as high accuracy is achieved when ordering by weight in experimental results from the softmax layer. In order to expedite rule extraction, the algorithm was implemented in Spark and run on IBM cloud services. The accuracy of the extracted rules was evaluated using examples from the training set, not the test set, to better represent the network's learned behavior. Running the search in parallel allowed for mapping the accuracy/complexity graph for 50 hidden neurons in a few hours. To expedite rule extraction, the algorithm was implemented in Spark and run on IBM cloud services. The accuracy of the extracted rules was evaluated using examples from the training set. Increasing the number of examples greatly increases the time taken, so only 1000 examples were used. The extraction process for the first hidden feature in the CNN trained on the fashion MNIST dataset is demonstrated by selecting 1000 random input examples to compute neuron activations and predicted labels. Each neuron in the first layer corresponds to a different 5x5 patch of the input, with 784 neurons in total. The optimal splitting value of each neuron is found by computing information gain, with neuron 96 having the maximum information gain for the first feature. The neuron with the maximum information gain for the first hidden feature in the CNN is neuron 96, with an information gain of 0.015 when split on the value 0.0004. This neuron corresponds to the image patch centered at (3, 12). Using this split, the input splits are defined by choosing values that result in the maximum information gain with respect to variable H. The algorithm extracts three different rules explaining H for various error/complexity tradeoffs, visualized in Figure 1. Figure 1 shows rules extracted for the first feature in a neural network. The most complex rule is a 5-of-13 rule with a 0.025 error rate, while a simpler 3-of-4 rule has a 0.043 error rate. A heavy penalty results in a trivial 1-of-1 rule with a 0.13 error rate. The technique is applied to the DNA promoter dataset for demonstration. In the DNA promoter dataset, a feed forward network with a single hidden layer of 100 nodes shows an exponential relationship between complexity and error. The output layer achieves 100% fidelity using a specific rule. Rules for the hidden layer are defined based on information gain from the output, with M-of-N rules extracted from the input layer. The network's output can be predicted by a rule based on the sets of literals in the rules explaining H39 and H80. Errors propagate when stacking rules that don't perfectly approximate each layer, requiring a single set of splits for each layer to replace the network with hierarchical rules. Conducting experiments layer by layer independently introduces more error. In order to provide a baseline for the best achievable rules when doing rule extraction with M-of-N rules, experiments were conducted layer by layer independently. This approach allows for examining the rule extraction landscape of a neural network trained on a practical example, such as a basic CNN on fashion MNIST in tensorflow. The CNN architecture included convolutional and max pooling layers, as well as a fully connected layer with rectified linear activation function. In the experiments, rules were extracted layer by layer from a neural network trained on a basic CNN for fashion MNIST. Different values of \u03b2 were used to produce sets of extracted rules with varying error/complexity trade-offs. The search procedure was repeated for each layer, resulting in 5 different sets of rules for each \u03b2 value. In the experiments, rules were extracted layer by layer from a neural network trained on a basic CNN for fashion MNIST. Different values of \u03b2 were used to produce sets of extracted rules with varying error/complexity trade-offs. The Complexity/Error trade-off for the extracted rules from each layer varied, with the first and final layers achieving near 0 error, while the second and third layers showed different accuracy/complexity trade-offs. The optimal accuracy/complexity tradeoff was not solely determined by the number of input nodes. The results of rule extraction from different layers of a neural network trained on fashion MNIST show that there is a critical point where error increases rapidly as complexity penalty increases. This indicates a natural set of rules for explaining latent features, with a trade-off between accuracy and complexity. Existing rule extraction algorithms do not explicitly consider this critical point. This paper introduces a rule extraction algorithm that considers rule complexity as a key factor, unlike current algorithms. Empirical evaluation of extraction algorithms is crucial for validation, highlighting both limitations and potential. Some layers of neural networks may have complex rules with high error rates, while others may exhibit simpler explanations. The black box problem of neural networks has been a longstanding issue, but as they become more integrated into society, the need for explainability grows. Rule extraction algorithms have shown success in capturing output neuron behavior accurately, especially in the final layer of CNNs. Selective use of decompositional algorithms depending on the layer being explained is suggested, as decompositional rule extraction may not be a general method of explainability. The distributed nature of neural networks makes rule extraction unfeasible for interpreting and explaining them. A novel search method for M-of-N rules was applied to explain the latent features of a CNN, showing that latent features can be described by an 'optimal' rule with a trade-off between error and complexity. The discrepancy in this trade-off between neurons in different layers suggests varying levels of interpretability. Rule extraction as a general technique may not adequately describe all latent variables in neural networks due to their complex architectures. However, simplifying explanations without sacrificing accuracy can be useful for understanding networks with easily understandable features. Decompositional rule extraction remains important for network behavior understanding, with potential for further research on different transfer functions, data sets, architectures, and regularization methods."
}