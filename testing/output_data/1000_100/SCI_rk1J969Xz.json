{
    "title": "rk1J969Xz",
    "content": "Estimating image location solely based on image contents is challenging due to the need for contextual information. This study introduces a global meshing strategy and training procedures to improve geolocation inference models. Delaunay triangles are shown to be effective for geolocation in low volume scenarios. The study introduces a global meshing strategy using Delaunay triangles to improve geolocation inference models in low volume scenarios. Incorporating time of posting, user albuming, and other meta data can enhance geolocation accuracy by up to 11% for country-level and 3% for city-level localities. Advancements in deep learning have expanded the capabilities of machine learning beyond image classification. Recent methods allow for deeper analysis of contextual information in images, enabling researchers to ask more complex questions like determining the geographic location of a picture. However, estimating the origin of ground-level images is challenging due to uneven distribution of images with geographic information, complicating model design. The work focuses on content-based image geolocation, identifying the geographic origin of ground-level images amidst challenges like conflicting data and ambiguous geographic terms. With the rise of image-based social data, inferring geographic context from images has become a significant problem. The paper addresses challenges in geolocating ground-level images, especially in mixed-media data where geolocation information may be unreliable or incomplete. Various approaches have been explored for geolocation from image content, building on recent work in global geolocation from ground-based imagery. One-hot encoding is a common approach for geolocation of imagery. Instance-level scene retrieval is used to assign geolabels based on image similarity. Prior work includes data sampling strategies for large-scale classification in social media applications, such as weighted sampling of minority classes. Biasing class selection during deep learning model training with random noise is a known method to address rare class examples. Researchers in Kordopatis-Zilos et al. (2016) address concerns regarding sampling in social media geolocation models. They explore different models for geolocation based on image content, time information, and user-album inputs. Their work highlights the importance of considering alternative approaches for geolocation and utilizing time and user information to enhance accuracy. The data used in their study is derived from YFCC100M BID17. The collected data for geolocation models is derived from YFCC100M BID17, with training and validation on a subset of geolabeled imagery. User-id and posted-time are important metadata associated with each image. The accuracy of GPS location is assumed to be precise based on previous results. The global-scale geolocation model approach described in this work is similar to previous studies. The global-scale geolocation model approach in this work involves subdividing the globe into a grid for classification. An image classifier is trained to predict longitudes based on the time of posting, showing differences in prior distributions. For example, images posted at 01:00 UTC are more likely to have a longitude close to zero compared to those posted at 21:00 UTC. The Delaunay triangle-based meshing architecture is used for classifying imagery based on ground truth GPS data. This approach differs from PlaNet's quad-tree mesh, with triangular meshes being more adaptive to Earth's geometric features. Triangular meshes can capture water/land interfaces without additional refinement but lack the refinement level information of quad-tree meshes. The quad-tree approach allows for controlled granularity by adaptively refining cells based on the number of examples they contain. Meshes are initialized with a structured grid and can be tuned by modifying the refinement level. The geolocation classification mesh parameters are shown in Table 1. The table displays meshing parameters for three meshes used in the study, covering a range of structures. Fine P mesh replicates PlaNet's parameters but with different methodology and dataset. Coarse mesh and fine mesh classification structures are shown for models developed in the paper, with red triangles indicating training criteria met in certain regions. Fine and fine P meshes were generated with all available imagery. The fine mesh better represents geographic regions compared to the coarse mesh. The Inception v4 neural network architecture is used to develop a mesh-based geolocation model. Cells are labeled based on the cell centroid in latitudelongitude, with geolocation specified as the center-of-mass of the training data in each cell. The geolocation for each cell is computed using the lat/lon centroid of the image population. Models are evaluated by calculating the distance between the predicted cell and the ground-truth GPS coordinate using a great-circle distance formula. Error thresholds of 1 km, 25 km, 200 km, 750 km, 2500 km are utilized to represent street, city, and other geographic regions. Error thresholds of 1 km, 25 km, 200 km, 750 km, 2500 km are used to represent different localities. YFCC100M images have metadata including user id and posting time, which is utilized in model M2. The output N Softmax 1 provides evidence for geolocation class, with a consideration for time dependence. The operational research hypothesis for model M2 is that there is time dependence after conditioning on image content. Variables related to time are added to the geolocation model output from M1 to create input for M2. The top 10 probabilities from the softmax layer of M1 are used, with the rest set to 0. Model M3 uses Bidirectional LSTMs to geolocate multiple images from a single user by leveraging information from other images posted by the user. In M3, images by a user are organized sequentially in time with no specific further organization. All images are grouped into albums of size 24, padded with zeros if necessary. During training, a user is limited to a single random album per epoch. Album averaging assigns images in an album to a mesh cell based on highest average probability, borrowing information across related images to increase accuracy. Location of an image is determined as the maximum average probability across all images associated with the posting. The maximum average probability across all images associated with the posting user is used to determine the location of an image. LSTM on a time-ordered sequence of images was considered, but did not significantly improve performance. The output of M1 is filtered to show only the top 10 mesh cell probabilities, which are then re-normalized. Training of M2 and M3 was done on the validation data of M1. Time inputs are concatenated to the filtered and normalized outputs, and a new training step is implied. The study investigates meshing parameters for geolocation inference, showing a trade-off between fine and coarse granularity. Results indicate better performance of the coarse mesh for large granularity and the fine mesh for finer granularity. Comparing training data centroid vs. cell centroid for class labels, a significant improvement is seen for the coarse mesh. The BID20 model is used with indoor-outdoor labels to filter geolocation inference on outdoor imagery without re-training the model. The geolocation model was not re-trained on outdoor imagery but used as a filtering operation during inference. Results show a 4-8% improvement in accuracy for region/country localities. The Im2GPS testing data was used to test the model, demonstrating generality in approach. The M1 classification model's performance is comparable to BID19 with less training data and classification regions. The use of time improves geolocation accuracy, with a slight gain in accuracy for both coarse and fine mesh models. There is a measurable difference in error between using time (M2) and not using time (M1). This difference is statistically significant and consistent across error calculations. The difference in geolocation accuracy between using time (M2) and not using time (M1) is statistically significant (p-value < 10^-16). The distribution of errors is mean shifted, with lower bias in time input models. Cross-entropy was optimized for both models to minimize class biases. Median errors for coarse mesh are 1627 km for M1 and 1262 km for M2. KL-Divergence is calculated for observed class proportions in validation for each model. The KL-divergence of the model output class frequencies compared to the true class frequencies in validation are in TAB5. \"User-Averaging\" is used for predicting individual images with M1 or M2, but biases cell count frequency. Using albums to borrow information across users reduces bias and improves accuracy. The training method includes bias reducing cross-entropy optimization, resulting in LSTMs on user albums having the lowest class bias. Different models were compared at various spatial resolutions, with time meta information concatenated with M1 output. Albums are created using user-id and contain 24 images. Conditioning on latent variables can improve geolocation models, and using time of day in the models was found to increase accuracy. Time of day was observed to increase accuracy and lower bias in geolocation models. Using time information improved both meshes. Accounting for indoor/outdoor scenes in images explained variation in validation accuracy, with outdoor-only results being better. Future work could involve concatenating the probability that an image is outdoor to the input of the model. Increasing the granularity of a grid improves accuracy at street and city levels but reduces accuracy at country and regional levels. Street level geoinferencing is not practical with a coarse mesh. A coarse mesh is superior for 200 km resolutions, and using a Delunary triangle-based mesh allows for training accurate models with fewer examples. Training data was divided into sets for models M1, M2, and M3 to prevent overlap. A training procedure was developed for large meshes, starting with pre-training on ImageNet and using Adagrad. The training procedure for large meshes involved pre-training on ImageNet with Adagrad, increasing training examples each epoch by 6%, oversampling minority classes initially, reducing class bias after each training cycle, and training the final model with SGD using a decreasing learning rate. Hyperparameters were empirically determined. The M2 model is trained using He initializations, Adaboost, and ADAM with specific learning rates. Early stopping is employed to monitor validation accuracy. The generality of the M1 classification model is demonstrated through a query-by-example on random Im2GPS data. An image of the Church of the Savior on Spilled Blood is used as an example, showing that it was not present in the training or random dataset. Each image is assigned a categorical indicator variable, and there is a latent class distribution assumed to be constant across training, testing, and application. The last layer in trained networks is a fit logit for class i, with the output being a softmax. When comparing models, accuracy is preferred but unbiased models are also considered in classification distribution. A low KL-divergence between q and p is expected if training is done well. Entropy of p and q is also taken into account for completeness."
}