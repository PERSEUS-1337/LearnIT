{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization in deep neural networks by preventing overfitting. This paper discusses novel observations about dropout in DNNs with ReLU activations, leading to the proposed method \"Jumpout\" which samples the dropout rate using a monotone decreasing distribution. Jumpout is a new method that samples the dropout rate using a monotone decreasing distribution, training the local linear model at each data point to work better for nearby data points. It adaptively normalizes the dropout rate at each layer and rescales the outputs for a better trade-off, showing improved performance on various datasets compared to original dropout. Deep learning has achieved remarkable success on various machine learning tasks, but overfitting can weaken generalization performance. Dropout is a technique that randomly sets hidden neuron activations to mitigate overfitting without significant computational overhead. However, tuning dropout rates can impact model convergence and final performance. Dropout rates should be tuned separately for each layer and training stage to avoid impacting model convergence and final performance. In practice, a single constant dropout rate is often used to reduce computation, but this may limit the generalization of the DNN to noisy samples. The effectiveness of dropout in DNN architectures is impacted by the varying fractions of activated neurons, leading to too much or too little perturbation in different layers and samples. Dropout is incompatible with batch normalization, as rescaling undropped neurons to match the original activation gain breaks normalization parameters consistency between training and test phases. This incompatibility may result in poor behavior when used together. The proposed \"jumpout\" is an improved version of dropout, addressing drawbacks like dropout being dropped out in training and its decreasing popularity. It modifies dropout to enhance generalization performance for DNNs with ReLU activations by changing activation patterns and linear models during training. The proposed \"jumpout\" is an improved version of dropout, addressing drawbacks like dropout being dropped out in training and its decreasing popularity. It modifies dropout to enhance generalization performance for DNNs with ReLU activations by changing activation patterns and linear models during training. Each linear model is trained to work for data points in nearby polyhedra, but with a fixed dropout rate, the smoothing effect may not achieve the goal of local smoothness. In jumpout, the dropout rate is a random variable sampled from a decreasing distribution, ensuring a higher probability of smaller dropout rates. This leads to a decreased probability of smoothing polyhedra as points move farther away. Additionally, jumpout adapts the dropout rate for each layer and training stage, addressing variations in the fraction of activated neurons. In jumpout, the dropout rate is adaptively normalized for each layer and training sample, ensuring consistent neural deactivation rates. It addresses the incompatibility between dropout and BN by rescaling outputs to maintain variance. Jumpout randomly generates a mask over hidden neurons like dropout, requiring no extra training and easily integrating into existing architectures. Jumpout is a modification to dropout code that shows similar memory and computation costs but outperforms dropout on various tasks. Previous approaches like \"standout\" and BID24 have proposed adaptive dropout rates for different layers and neurons. BID23 demonstrated the relationship between Rademacher complexity of a DNN and dropout rate vectors. The complexity of a DNN is bounded by a function related to dropout rate vectors. Jumpout adjusts dropout rates based on ReLU activation patterns without relying on additional models. It introduces minimal computation and memory overhead and can be easily integrated into existing architectures. Gaussian dropout and variational dropout were proposed to optimize dropout rates for faster convergence and adaptive neuron uncertainty. Jumpout is a modification of the original dropout method that aims to improve dropout rates for every neuron without adding extra training costs or parameters. It can be combined with other dropout variants and targets different issues in dropout. The study focuses on a feed-forward deep neural network architecture with hidden nodes and activation functions. The formalization can represent various DNN architectures, including fully-connected networks with bias terms. The study discusses the architecture of a feed-forward deep neural network with hidden nodes and activation functions. The convolution operator is a matrix multiplication with sparse weight matrices. Average-pooling and max-pooling can be represented as matrix operations. Residual network blocks can be represented by appending an identity matrix to retain input values. DNNs with shortcut connections can be written in a specific form. The DNN in Eqn. FORMULA0 can be represented as a piecewise linear function using ReLU activation. The activation pattern modifies the weight matrix, eventually leading to a linear model. The gradient \u2202x is the weight vector of the linear model. The gradient \u2202x is the weight vector of the linear model associated with activation patterns on all layers for a data input x. DNNs with ReLU activations are focused on for their computational efficiency and good performance. Dropout improves generalization by considering nearby convex polyhedra of local linear models. Three modifications to dropout lead to jumpout, improving DNN performance by promoting neuron independence and training ensemble networks. Dropout smooths local linear models for enhanced generalization. Dropout improves generalization performance by smoothing local linear models in DNNs with ReLUs. The input space is divided into convex polyhedra, leading to each data point having its own distinct local linear model. Nearby polyhedra may correspond to different linear models due to the multiplication of weight matrices. The proposed method suggests sampling a dropout rate from a truncated half-normal distribution to address the issue of fragility and lack of smoothness in deep neural networks with ReLUs. This aims to improve generalization performance by smoothing local linear models within convex polyhedra in the input space. The method proposes sampling a dropout rate from a truncated half-normal distribution to improve generalization performance in deep neural networks with ReLUs. The dropout rate is sampled from a Gaussian distribution with mean zero, and the absolute value is taken to determine the rate within specified limits. The standard deviation controls generalization enforcement, with smaller dropout rates sampled more frequently to contribute to linear models. Other distributions could also be explored for this purpose in future work. The Gaussian-based dropout rate distribution encourages smoothness in generalization performance of local linear models in deep neural networks. Tuning dropout rates separately for each layer is ideal but computationally expensive, leading to the common practice of setting a global dropout rate. However, using a single global dropout rate may not be optimal as it affects the proportion of active neurons. The proportion of active neurons in each layer varies significantly during training stages, affecting the effectiveness of dropout rates. To address this, the dropout rate is normalized by the fraction of active neurons to ensure consistent behavior across layers and training stages. This approach helps maintain a more consistent activation pattern and improves the overall performance of deep neural networks. The dropout rate is tuned as a single hyper-parameter to achieve a consistent activation pattern. The scaling factor in standard dropout keeps the mean of neurons the same between training and test, causing incompatibility with batch normalization. Combining dropout layers with BN layers can help address this issue. Combining dropout layers with BN layers involves a linear computational layer followed by a BN layer, ReLU activation layer, and dropout layer. Neuron values after ReLU are treated as random variables. Applying a dropout rate affects the mean and variance in the BN layer. Dropout changes the scales of mean and variance of neurons during training, leading to inconsistency with the test phase in BN layers. To counteract this, rescale the output y to recover the original scale of the mean and variance. Rescaling factors depend on the dropout rate p. During training, dropout alters neuron scales, causing inconsistencies with BN layers in the test phase. Rescaling factors for output y depend on the dropout rate p. Different rescaling methods like (1 \u2212 p) \u22120.5 and (1 \u2212 p) \u22120.75 are applied to y in the CIFAR10(s) network. The rescaling factor (1 \u2212 p) \u22120.75 shows a good balance between mean and variance adjustments. During training, dropout alters neuron scales, causing inconsistencies with BN layers in the test phase. Rescaling factors for output y depend on the dropout rate p. The rescaling factor (1 \u2212 p) \u22120.75 gives nice trade-offs between mean and variance rescaling, making both the mean and variance sufficiently consistent for cases of using dropout and not using dropout. Using dropout with batch normalization (BN) in convolutional networks can potentially improve performance, with larger dropout rates leading to more improvement. However, using the original dropout with BN can significantly decrease accuracy when the dropout rate exceeds 0.15. In contrast, using a rescaling factor of (1 \u2212 p) \u22120.75 with dropout shows consistent performance improvement even with increasing dropout rates. The improved dropout method \"Jumpout\" combines three modifications to overcome original dropout drawbacks. Jumpout dynamically samples a random dropout rate from a decreasing distribution and normalizes it based on active neurons for consistent regularization and generalization effects. Additionally, it scales outputs by (1 \u2212 p) \u22120.75. Jumpout further scales the outputs by (1 \u2212 p) \u22120.75 during training to synergize with batchnorm operations. It requires three hyperparameters, with \u03c3 controlling the standard deviation of the half-normal distribution. The auxiliary truncation hyperparameters (p min , p max ) bound the samples from the distribution, typically set at p min = 0.01 and p max = 0.6. Tuning \u03c3 achieved good performance, with the input h j considered as the features of layer j for one data point. In practice, the average q + j over data points is used as the estimate for the mini-batch, providing comparable performance with less computation and memory. Jumpout has similar memory cost as dropout, with minimal computation requirements. Dropout and jumpout are applied to various DNN architectures, comparing their performance on benchmark datasets. In experiments on various datasets, different DNN architectures were used, including convolutional layers applied to CIFAR10, WideResNet-28-10 applied to CIFAR10 and CIFAR100, \"pre-activation\" version of ResNet-20 applied to Fashion-MNIST, WideResNet-16-8 applied to SVHN and STL10, and ResNet-18 applied to ImageNet. Standard settings, data preprocessing/augmentation, and hyperparameters were followed. On ImageNet, pre-trained ResNet18 models were used with dropout and jumpout. Training on ImageNet typically does not face overfitting issues with standard data augmentation methods. Jumpout consistently outperforms dropout on all datasets and DNNs tested, including Fashion-MNIST and CIFAR10. Even with high test accuracy, jumpout still brings improvements. On CIFAR100 and ImageNet, jumpout achieves significant improvements without the need to increase model size. This highlights the effectiveness and advantage of jumpout over dropout. The effectiveness of jumpout over dropout is highlighted through consistent outperformance on various datasets and DNNs. A thorough ablation study confirms the benefits of each modification, with jumpout achieving the best performance by combining all three modifications. Learning curves show jumpout's advantages in early learning stages, reaching higher accuracy faster than dropout. Future improvements may include optimizing the learning rate schedule for jumpout to achieve final performance sooner. The study compares jumpout and dropout on CIFAR10(s) dataset. Jumpout reaches final performance earlier than dropout. A rescaling factor of (1 \u2212 p) \u22120.75 shows a good balance between mean and variance rescaling."
}