{
    "title": "HyM8V2A9Km",
    "content": "Sparse reward is a challenging problem in reinforcement learning. Hindsight Experience Replay (HER) addresses this by relabeling goals. However, HER has limited applicability due to a lack of universal goal representation. Augmenting experienCe via TeacheR's adviCE (ACTRCE) extends HER using natural language as the goal representation. ACTRCE efficiently solves difficult reinforcement learning problems in 3D navigation tasks, where non-language goal representations fail. It allows the agent to generalize to unseen instructions and lexicons. Using hindsight advice is crucial for solving challenging tasks, with even a small amount being sufficient for learning to progress practically. Deep reinforcement learning applications often rely on complex reward functions, but designing these can be difficult and lead to biased learning. Sparse and binary reward functions can be used to simplify this process. The reward function in deep reinforcement learning can be simplified using a sparse and binary approach, but this can make learning difficult. Hindsight Experience Replay (HER) addresses this issue by converting failed experiences into successful ones. However, HER relies on the assumption that there exists a goal for every state in the environment, which can be inefficient if represented using the state space. In the context of deep reinforcement learning, the HER framework combines natural language goal representation to efficiently describe and generalize goals across tasks and environments. This approach allows for flexible and compact goal descriptions, reducing redundancy in state representations. In this paper, the HER framework is combined with natural language goal representation to propose an efficient technique called ACTRCE. This method involves a teacher giving advice in natural language to the agent after each episode, helping to alleviate the sparse reward problem in reinforcement learning. The language goal representation, when combined with hindsight advice, allows the agent to efficiently solve problems in challenging environments and generalize to unseen instructions. The curr_chunk discusses the importance of using hindsight advice in solving challenging tasks and the practical aspect of using a small amount of hindsight advice for learning. It also mentions the relevance of language learning in achieving goals described in natural language, which is part of the language grounding problem. The work combines reinforcement learning techniques to address this issue. The curr_chunk introduces reinforcement learning techniques for language grounding, focusing on maximizing expected cumulative return through policy parameters optimization. The curr_chunk discusses Q-learning, an off-policy, model-free RL algorithm based on the Bellman equation. It uses semi-gradient descent to minimize the squared Temporal Difference error. Deep Q-Network builds on Q-learning by using a neural network to approximate the optimal Q-function. The curr_chunk introduces goal-oriented reinforcement learning framework, incorporating a goal space G with a fixed goal g for each episode. The agent's objective is to maximize the expected discounted cumulative return given the goal. It follows a specific family of goal conditioned reward functions where the reward is either 0 or 1 based on the resulting state. The curr_chunk discusses how the reward function in goal-oriented reinforcement learning can be sparse, making it challenging for the agent to learn. The proposed solution involves collecting experiences under a goal, transforming the goal if the terminal reward is zero, and using an off-policy algorithm to learn from the goal-transformed experiences. This approach allows for flexible relabeling with desirable goals using a representation map from the state space to the goal space. The goal representation in goal-oriented reinforcement learning is challenging to construct, as a simple mapping is not sufficient. Using a trivial construction like G = S and f g (s) = [g = s] is redundant and limits the algorithm's applicability. For a goal-oriented MDP with parameters, the objective is to maximize the expected discounted cumulative return for each goal in G. In goal-oriented reinforcement learning, the goal representation mappings are crucial for maximizing the expected return. The use of natural language descriptions for goals can reduce redundancy and provide more abstract representations compared to simple mappings like subspace projections. This approach motivates the use of teachers to give advice in the form of natural language descriptions for each state's goal. In goal-oriented reinforcement learning, natural language descriptions are used to define goals, with teachers providing advice on goal descriptions for each state. Teachers can give positive advice for achieved goals and negative advice for unachieved goals to enhance training. In goal-oriented reinforcement learning, teachers provide advice on goal descriptions for each state, including negative rewards for unachieved goals. Different descriptions of the same goal can be used, such as \"Reach a blue torch\" or \"Reach the largest blue object.\" In goal-oriented reinforcement learning, teachers provide advice on goal descriptions for each state, including negative rewards for unachieved goals. The goal can be described as \"Reach the largest blue object\", with different teachers giving different advice. The approach involves relabeling the original goal with each advice and corresponding rewards, and augmenting the replay buffer with these trajectories. The proposed algorithm formalizes this process, considering both MDP and POMDP settings for teacher advice. The challenge lies in dealing with natural language goal representation, which is explored in two standard ways in the paper. In goal-oriented reinforcement learning, teachers provide advice on goal descriptions for each state. Two standard ways to convert language goals into continuous vectors for neural network training are explored: using a recurrent neural network to embed each word sequentially, or using a pre-trained language component. The latter method allows the agent to better understand unseen lexicons related to the language goals in training, improving natural language understanding and robustness to advice. In goal-oriented reinforcement learning, teachers provide advice on goal descriptions for each state. The architecture of the model consists of 3 modules: a language component converting instructions into a continuous vector, an observation processing component using convolution neural networks, and a fusion component using gated attention to combine goal information and observation. Experimental results demonstrate the effectiveness of the proposed method in KrazyGrid World and ViZDoom environments. The experimental setup includes two environments, KrazyGrid World and ViZDoom. The study compares different goal representations, such as one hot vectors, GRU embeddings, and pre-trained word embeddings. It shows that GRU and pre-trained embeddings scale better with increasing instructions. The study also explores the effectiveness of hindsight language advice in improving sample efficiency. In challenging tasks, significant improvement in sample efficiency is seen with teachers' advice. Even limited advice can lead to notable enhancements, with low burden in practice. The method was tested in KrazyGrid World and ViZDoom environments, with details provided for each. ViZDoom BID14 BID4 is a 3D learning environment based on the game Doom, with language goals involving natural instructions like \"Go to the green torch\". Singleton tasks are individual language goals, while challenging tasks combine multiple tasks using \"and\" and \"or\" compositions. In our experiments, we explore the use of language for goal representation in reinforcement learning. We compare language-based goal representations with non-language representations. The agent completes tasks using \"and\" and \"or\" compositions in different environments. Training details include using the DQN algorithm, sampling 16 environments, updating agents with an average gradient, and further training steps outlined in the appendices. In reinforcement learning experiments, language-based goal representations are compared with non-language representations. The effectiveness of language goal representations increases with task difficulty, providing better learning signals and generalizing to unseen goals. Different goal representations are described, including Language Sentence Representation with GRUs using pre-trained sentence embeddings for robustness. In reinforcement learning experiments, language-based goal representations are compared with non-language representations. Language goal representations using pre-trained sentence embeddings provide better learning signals and generalize to unseen goals. The language component is pre-trained with InferLite BID16, a lightweight sentence encoder trained for natural language inference. The original sentence embedding vector is of dimension 4096, projected down to 256 for consistency with other models. One-Hot Representation is used for non-language baseline, representing each instruction as a one hot vector without language structure. The one-hot goal representation is embedded to a vector with the same dimensions as the GRU representation, learned independently for each goal encountered. All three goal representations are used with hindsight advice for learning singleton tasks and compositional tasks. In reinforcement learning experiments, language-based goal representations using pre-trained sentence embeddings outperform non-language representations. Language representations generalize better to unseen goals and provide stronger learning signals. One-hot representations struggle with generalization, achieving only a 24% success rate in compositional tasks compared to 97% with language representations. Language representations allow for easy generalization to unseen instructions with seen lexicons, as shown in Table 1 under \"ZSL\" (zero-shot learning). With GRU language goal representation, the agent can generalize to unseen instructions 83% of the time, demonstrating strong generalization ability. A visualization analysis was conducted to examine the statistical relations between learned embeddings of goals. The correlation matrices for each goal representation were calculated and plotted, revealing significant differences in learned embeddings. The study compared GRU and InferLite embeddings, noting similar block-like structures for colours and shapes in GRU and InferLite. t-SNE embeddings showed meaningful clustering for language goals and sporadic embeddings for one-hot goals. Pre-trained embeddings were used for model generalization to unseen lexicons at test time. The agent trained with InferLite goal representations showed 83% generalization to unseen instructions. The agent's performance on new instructions with unseen lexicons is above 66% of the time. Understanding synonyms can improve learning in noisy settings. Hindsight advice is crucial for learning, as shown in comparison to the DQN algorithm without such advice. Without hindsight advice, DQN struggles to learn challenging tasks, but even a small amount (1%) of advice can significantly improve learning. Recurrent neural networks are used for embedding language goals in both our method and the baseline. In experiments on KGW, our method quickly learned and achieved good results, while the baseline DQN failed to learn anything. In ViZDoom experiments, our agent trained in 3 configurations showed promising results. In ViZDoom experiments, our agent trained in 3 configurations: 5 objects in easy and hard mode, and 7 objects in hard mode with a larger room size. Only the agent trained with ACTRCE was able to learn in the more difficult 7-objects environment. Figure 4 (e) shows the training instruction average success rate using ACTRCE versus DQN baseline. Table 1 summarizes the agent's Multitask and Zero-Shot generalization performance. In KGW experiments, ACTRCE outperformed baseline DQN in average success rates over all goals. In ViZDoom experiments, the baseline DQN struggled to learn the task, while ACTRCE achieved good performance. Using hindsight advice, the agent learned well with ACTRCE compared to the baseline DQN. The teacher provided advice only in the first {10%, 1%} of frames during training. The experiment was conducted in single target mode with 7 objects using GRU as the sentence embedding. Figure 4 illustrates the training average success rate over frames. The agent learned well with ACTRCE compared to the baseline DQN, even with very little (1%) advice. Previous approaches have used natural language in reinforcement learning, such as translating language advice into a neural network program and exploiting human feedback to shape rewards. The use of language in reinforcement learning has been explored, with approaches like multi-modal embedding for Atari games and few-shot learning problems. Researchers have mapped language instructions to actions in 2D environments and introduced zero-shot sentence extrapolation. Our work builds on previous research that used a gated-attention architecture to combine language and image features for executing written instructions in a 3D environment. We also incorporate experience replay techniques to enhance learning speed and efficiency. The ACTRCE method utilizes natural language as a goal representation for hindsight advice in reinforcement learning. It demonstrates the benefits of using language goals, showing efficient problem-solving in 3D navigation tasks compared to non-language goal representations. The agent can generalize to unseen instructions and even adapt to instructions with unseen lexicons, showcasing its potential with noisy human advice. The ACTRCE algorithm relies on hindsight advice and shows practicality with minimal advice for learning. KrazyGrid World is a 2D grid environment with different tile functionalities and colors. The agent's goal is to reach goals of different colors using a global view of the grid state. Each tile is represented by functionality and color attributes in one hot vectors, with the agent's position also represented. The grid size is 9x9 with 3 goals in the environment. In experiments, a 9x9 grid environment with 3 goals of distinct colors is used. Various numbers of lavas are tested, ensuring at least 1 lava of each color. Episodes end when the agent reaches a lava or goal, or after a maximum of 25 time steps. The goal space is expanded to include compositions of goals, leading to modifications in the environment. Episodes now end when the agent reaches a lava or 2 different goals, or after 50 time steps. An extra action called \"flag\" allows the agent to end the episode when it believes the goal is achieved. ViZDoom BID14 BID4 is a 3D learning environment based on the game Doom, where the agent navigates a room with random objects using actions like turn left, turn right, and move forward. The goal is to follow a natural language instruction like \"Go to the green torch\" within 30 time steps. The episode terminates when the agent reaches the target object or after the maximum time step is reached. The ViZDoom BID14 BID4 environment involves navigating a room with random objects to follow natural language instructions within 30 time steps. Difficulty modes include easy and hard, with different object and agent distributions. Compositional instructions consist of two single object instructions joined by \"and\". No superlative instructions are included to ensure clarity. The ViZDoom BID14 BID4 environment involves navigating a room with random objects to follow natural language instructions within 30 time steps. Difficulty modes include easy and hard, with different object and agent distributions. Compositional instructions consist of two single object instructions joined by \"and\". The instructions are designed to be unambiguous, with mutually exclusive sets of valid objects for each instruction. A basic head-up display (HUD) is added to the environment, showing thumbnail images of reached objects. The episode terminates once the HUD displays two reached objects. The HUD in the ViZDoom environment displays up to 2 reached objects, terminating the episode when the second object is reached. Synonym instructions are generated by replacing words with synonyms. Positive and negative feedback is given based on reaching objects or not. In the ViZDoom environment, positive and negative advice is given based on the number of objects reached. For singleton tasks, advice is given for reaching 0/1 objects, while for compositional tasks, advice is tailored based on the number of objects reached during the episode. In ViZDoom, advice is given based on the number of objects reached. For singleton tasks, instructions are generated for unreached objects using convolution layers and LSTM, followed by fully connected layers for predicting action values. In ViZDoom, advice is given based on the number of objects reached. For singleton tasks, instructions are generated using convolution layers and LSTM, followed by fully connected layers for predicting action values. The grid observation is preprocessed with convolution layers and ReLU activation functions, followed by a bidirectional LSTM for language sentences. The observation is further processed with additional convolution layers and language attention for gating on feature maps. The architecture for processing language input in ViZDoom involves using an embedding matrix to convert vocabulary to a vector, followed by a Gated Recurrent Unit (GRU) with 256 units. The final hidden vector from the GRU is then passed through a fully connected layer with 64 units. The architecture for processing language input in ViZDoom involves using word embeddings. The GRU's last hidden vector is passed into a fully connected layer with 64 output units with sigmoid activation, acting as the attention vector. The gated feature maps are flattened and passed through a fully connected layer with ReLU activation. The LSTM with 256 hidden units predicts the 3 action values. Hyperparameters for KrazyGrid World experiments are tuned, including learning rate, replay buffer size, and training frequency. Double DQN and Huber loss are used for stable gradients. ViZDoom environment utilizes a set of training instructions and test instructions for agent training and evaluation. The implementation of DQN is based on Arnold's work, with a cyclic buffer replay buffer containing recent transitions. Episodes are generated with an epsilon-greedy policy, starting at 1.0 and decaying linearly to 0.01. Training begins after the first 1000 frames have been collected, using Double DQN to reduce Q-value overestimation and Huber loss for stable gradients. The training starts after collecting 1000 frames, using Double DQN to reduce Q-value overestimation and Huber loss for stable gradients. The Adam optimizer with a learning rate of 0.0001 is used. The network is updated every 4 frames on easy and 16 frames on difficult mode. Sampling from the replay buffer involves selecting 32 consecutive frames from a random episode. Running 16 parallel threads helps alleviate the correlation between samples in the mini-batch. The training process involves running 16 parallel threads for gradient updates and synchronizing the training thread model with the shared network. The target network is synchronized every 500 time steps, and one additional thread evaluates the multi-task success rate. Different teacher types are described, focusing on desired goals and exploring environments based on sampled goals. In exploring environments with different teacher types, advice is given based on the achieved goal. Three types of teachers are considered: Optimistic, Knowledgeable, and Discouraging. Each teacher provides advice based on the agent's performance, with varying levels of feedback. The study compares this method to the DQN algorithm for effectiveness. In comparing different teacher types for providing advice, the method ACTRCE was evaluated in KrazyGrid World. Results showed that ACTRCE quickly learned and achieved good results on environments with 3 lavas and 3 goals. However, performance decreased on environments with 6 lavas. The baseline DQN failed to learn anything at all. After 32 million frames of training, ACTRCE's performance dropped from 83% to 63%. Having knowledgeable teachers helped speed up learning, especially when the number of lavas increased. With language advice provided even in difficult settings, ACTRCE learned at a similar rate, outperforming ACTRCE \u2212 by 17%. The concern of only learning easier tasks was raised, prompting the question of whether learning from hindsight can aid in learning difficult tasks. In a transfer learning experiment, agents were pretrained with a pessimistic teacher before training with ACTRCE. Despite different goals, pretraining with pessimistic agents led to faster learning compared to unpretrained agents. In a transfer learning experiment, agents pretrained with a pessimistic teacher learned faster than unpretrained ones. Specifically, in environments with 3 and 6 lavas, pretrained agents outperformed ACTRCE. Learning easier goals can provide signals for harder goals, especially when tasks require similar modules. Experiments on ViZDoom with single target for 5 objects showed DQN's inconsistency on hard mode compared to ACTRCE's low variance. In a transfer learning experiment, pretrained agents with a pessimistic teacher outperformed unpretrained ones in environments with 3 and 6 lavas. Experiments on ViZDoom with single target for 5 objects showed DQN's inconsistency on hard mode compared to ACTRCE's low variance across seeds. A3C baseline from BID4 was less sample efficient than DQN/ACTRCE implementation on an easy task. The average episode length is decreasing with ACTRCE compared to baseline DQN for harder tasks. A cumulative success rate curve is constructed to measure model performance, with a larger area indicating a better model. The Multi-task cumulative success rate curve for ViZDoom tasks using GRU hidden state language encoding shows that ACTRCE outperforms baseline DQN in longer successful trajectories for harder tasks. When two target objects are adjacent, it takes fewer time-steps to reach the second object. When the target objects are not adjacent, the agent needs to carefully navigate to avoid obstacles, requiring more time-steps."
}