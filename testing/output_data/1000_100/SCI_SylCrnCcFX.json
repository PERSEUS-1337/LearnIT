{
    "title": "SylCrnCcFX",
    "content": "Deep networks aim to understand complex mappings through locally linear behavior. The challenge lies in the instability of derivatives, especially in networks with piecewise linear activation functions. A new learning problem is proposed to promote stable derivatives over larger regions. The algorithm involves identifying stable linear approximation regions and expanding them through optimization. This approach is demonstrated on image and sequence datasets using residual and recurrent networks. The derivatives of functions in deep learning models are unstable due to over-parametrization, leading to challenges in understanding complex mappings. Local linear approximations are used to explain model predictions and guide learning through regularization of functional classes. The focus is on derivatives with respect to input coordinates rather than parameters. The over-parametrization of deep learning models leads to unstable function values and derivatives, affecting the robustness of first-order approximations. Gradient stability is different from adversarial examples, with robust estimation techniques focusing on stable function values to protect against adversarial attacks. In this paper, the focus is on deep networks with piecewise linear activations to ensure gradient stability. The special structure of these networks allows for inferring lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable. The paper investigates the case of p = 2 and formulates a regularization problem to maximize the lower bound. The resulting objective is rigid and non-smooth, resembling support vector machines. The paper focuses on deep networks with piecewise linear activations, similar to support vector machines. It proposes a novel perturbation algorithm for evaluating neuron gradients without back-propagation. The algorithms are tested on various network types and datasets, showcasing both quantitative and qualitative results. The paper introduces inference algorithms for identifying stable regions in neural networks with piecewise linear activation functions. It also presents a novel learning criterion to expand regions of stable derivatives and perturbation algorithms for high-dimensional data. The focus is on networks like ReLU BID15 and its variants, which are inherently piecewise linear due to their activation functions. Various network types such as FC, CNN, RNN, and ResNet are considered for empirical evaluation. The proposed approach focuses on identifying stable regions in neural networks with piecewise linear activation functions like ReLU. It utilizes a mixed integer linear representation to encode the active linear piece of the activation function for each neuron. The feasible set corresponding to an activation pattern in the input space ensures stable derivatives, forming linear regions. The text discusses the identification of stable regions in neural networks with piecewise linear activation functions like ReLU. It focuses on expanding local linear regions through learning, in contrast to quantifying the number of linear regions as a measure of complexity. The notion of stability considered differs from adversarial examples. The text discusses stability in neural networks with piecewise linear activation functions like ReLU, focusing on expanding local linear regions through learning. Unlike adversarial examples, the methods for finding exact solutions differ and are computationally challenging on ImageNet scale images. The proposed inference algorithm certifies a 2 margin around a point by forwarding O(D) samples in parallel, scaling to ResNet on 299 \u00d7 299 \u00d7 3 dimensional images. The proposed learning algorithm maximizes the 2 margin of linear regions around each data point, similar to transductive/semi-supervised SVM. It introduces a smooth relaxation of the margin and perturbation algorithms for realistic networks. The text discusses novel perturbation algorithms for gradient stability in deep models, focusing on establishing robust derivatives. The approaches are developed for FC networks with ReLU activations and aim to address the instability of gradient-based explanations. The text introduces notation for a neural network with hidden layers and neurons, using transformation matrices and biases for computations. It discusses the output as a linear transformation of the last hidden layer. The text discusses the piecewise linear property of neural networks and the use of activation patterns to represent functional constraints for neurons. The text discusses activation patterns in neural networks, defining sub-gradients and feasible sets. Linear regions of the activation pattern are characterized as convex polyhedrons. The lower bound of the margin subject to a derivative specification is also discussed. The text discusses directional verification in neural networks using convexity of activation patterns. Feasibility of directional perturbations is checked by examining if a point lies within the feasible set. The feasibility problem on 1-balls is addressed, with a proposition for 1-ball feasibility. The text also mentions the intractability of the feasibility problem for \u221e-balls in high dimensions. The text discusses verifying feasibility of directional perturbations in neural networks using convexity. It mentions the tractability of 1-ball feasibility due to convexity of the set, with efficient certification through binary searches. A proposition for 2-ball feasibility is also presented, involving computing the minimum distance between a point and hyperplanes induced by neurons. The efficiency of computing gradients is highlighted, with reference to a figure for further details. The text discusses certifying the number of complete linear regions in neural networks efficiently. It proposes certifying the number of complete linear regions among data points, which is computationally feasible under certain conditions. The number of complete linear regions is related to the structure of the data manifold, and the text introduces the concept of Complete Linear Region Certificate. In this section, methods are discussed to maximize the margin\u02c6 x,2 in neural networks. A regularization problem is formulated to achieve this, but the rigid objective may hinder optimization. A hinge-based relaxation approach is proposed to alleviate this issue, similar to SVM. If certain conditions are not met, an upper bound can still be obtained due to constraints. The text discusses deriving a relaxation for a regularization problem in neural networks to maximize the margin\u02c6 x,2. It involves solving a smoother problem by relaxing constraints and using a soft regularization approach with a hyper-parameter. The proposed method is compared to maximizing the margin in a linear model scenario. Additionally, a toy dataset is used to visualize the effects of the methods on a fully connected network. The text discusses training a 4-layer fully connected network with binary cross-entropy loss, distance regularization, and relaxed regularization. The regularization methods enlarge linear regions and create smoother prediction boundaries. A generalization to the relaxed loss is made to address scaling issues in large networks. The text discusses the generalization of relaxed loss for training large networks, focusing on learning Robust Local Linearity (ROLL) with a special case when \u03b3 = 100. This simplifies the training process, allows for parallel computation, and induces a strong synergy effect between gradient norms in different layers. The ROLL loss demands heavy computation on gradient norms during back-propagation. The text discusses a parallel algorithm for training large networks without back-propagation by exploiting the functional structure of f \u03b8. It constructs a linear network g \u03b8 with fixed linear activation functions to mimic f \u03b8 behavior. Derivatives of neurons can be computed by forwarding two samples, allowing for amortized and parallelized analysis of complexity. The text proposes a parallel algorithm for training large networks without back-propagation by utilizing the functional structure of f \u03b8. It introduces a perturbation algorithm for computing gradients efficiently, and suggests an unbiased estimator for the loss function. The algorithm aims to address challenges in computing loss for large networks in high-dimensional settings. The proposed algorithm efficiently computes gradients for large networks with affine transformations and piecewise linear activation functions. It can be applied to all deep learning models by enumerating neurons with ReLU-like activation functions. The algorithm does not immediately generalize to maxout/max-pooling nonlinearity but suggests using average-pooling or convolution with large strides instead. In this section, the 'ROLL' approach is compared with a baseline model ('vanilla') in various scenarios on a testing set. Experiments are conducted on a single GPU with 12G memory using evaluation measures such as accuracy, number of complete linear regions, and margins of linear regions. Parameter analysis is done on the MNIST dataset with a 4-layer FC model with ReLU activations. Experiments were conducted on a 4-layer FC model with ReLU activations, and the results are reported in TAB1. The tuned models have specific parameters, and the ROLL loss achieves larger margins compared to the vanilla loss. By trading off 1% accuracy, even larger margins can be achieved. The Spearman's rank correlation between two variables among testing data is high. The lower number of complete linear regions in our approach reflects the existence of larger linear regions. In the ROLL model, linear regions with ACC=98% have the same label. Parameter analysis in Figure 2 shows accuracy decreases with increased C and \u03bb. Higher \u03b3 values indicate less sensitivity to hyper-parameters. Efficiency is validated by measuring running time for gradient descent steps. Comparison is made between vanilla loss, full ROLL loss, and approximate ROLL loss. The approximate ROLL loss computed by perturbations is comparable to the full loss, with minimal computational overhead. Results on ResNet for Caltech-256 show the efficiency of the perturbation algorithm, achieving a 12 times speed-up compared to back-propagation. Training RNNs for speaker identification on a Japanese Vowel dataset from the UCI machine is also discussed. We implement RNNs for speaker identification on a Japanese Vowel dataset from the UCI machine learning repository using the state-of-the-art scaled Cayley orthogonal RNN (scoRNN) with LeakyReLU activation. The results show our approach leads to a model with larger margins on testing data compared to the vanilla loss, with a Spearman's rank correlation of 0.98. Sensitivity analysis on derivatives is also conducted. The study conducts sensitivity analysis on derivatives to identify stability bounds for stable derivatives. The ROLL regularization shows larger stability bounds compared to the vanilla model. Experiments are performed on the Caltech-256 dataset using a 18-layer ResNet model. The ROLL loss function is utilized with random samples for training, validation, and testing sets. The study evaluates the stability of gradients in a high-dimensional input space using a sample-based approach. The goal is to assess stability across different linear regions by measuring gradient distortion. The evaluation is based on labeled data and involves computing expected and maximum distortion within a defined region. The study assesses gradient distortion in a high-dimensional input space using a sample-based approach. The maximum distortion is computed using a genetic algorithm due to the complexity of the optimization process. Implementation details are provided in the appendix, with 8000 samples used to approximate the expected distortion. The study evaluates gradient distortion in a high-dimensional input space using a sample-based approach. Results show that the ROLL loss produces more stable gradients and slightly better precisions compared to the vanilla loss. Only a small number of images change prediction labels in both models. The goal is to create locally transparent neural networks for robust local linearity. The study focuses on constructing locally transparent neural networks using piecewise linear networks and a margin principle similar to SVM. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. The feasible set of activation patterns is equivalent to satisfying linear constraints in the first layer, with subsequent layers following fixed activation indicators. The proof follows by induction. The proof by induction shows that for a given point x and feasible set S(x), if a unit vector \u2206x exists such that x + \u00af\u2206x is in S(x), then f \u03b8 is linear in a certain range. Additionally, it is proven that for a point x and feasible set S(x), if x is within a 1-ball B ,1 (x) with extreme points, then x is a convex combination of those points and thus in S(x). Lastly, the minimum 2 distance between a point x and a convex polyhedron S(x) is discussed in relation to the union of hyperplanes. The proof involves constructing a neural network feasible in Eq. (5) with the same loss as the optimal model in Eq. (4). The network g \u03b8 is built with the same weights and biases as f \u03b8 but with a linear activation function. Each layer in g \u03b8 is represented accordingly. The activation function \u00f4 is fixed given x, applying linearity to\u1e91. Partial derivatives with respect to input axis k can be computed using a zero vector and a unit vector. Derivatives of neurons with respect to x k can be computed with 2 forward passes. The complexity analysis assumes no overhead for parallel computation and a unit operation for batch matrix multiplication. The perturbation algorithm computes gradients for a batch of inputs using two forward passes, taking a total of 2M operations. In contrast, back-propagation requires 2i operations for each neuron, totaling M i=1 2iN i operations. Dynamic programming can be used to compute all gradients efficiently. The dynamic programming approach efficiently computes gradients for fully connected networks, but is inefficient for convolutional layers due to the expensive linear transformation representation. An introductory guide is provided for deriving inference and learning methods for maxout/max-pooling nonlinearity in piecewise linear networks. In piecewise linear networks, deriving inference and learning methods using max-pooling nonlinearity is feasible but not recommended due to inducing new linear constraints. Instead, convolution with large strides or average-pooling is suggested as they do not incur constraints. The target network is assumed to have a single nonlinearity mapping N neurons to 1 output. Activation patterns determine input selection, leading to a degeneration to a linear model when fixed. These patterns create stable derivative regions in the input space but may also result in degenerate cases with similar linear coefficients. The feasible set of a feasible activation pattern at x can be derived as a convex polyhedron with linear constraints. In a fully-connected model with 4 hidden layers and 100 neurons each, training is done with a sigmoid cross entropy loss function for 5000 epochs using the Adam optimizer. The model has an input dimension of 2, output dimension of 1, and induces N-1 linear constraints for each max-pooling neuron with N inputs. The training process involves fixing C = 5 and increasing \u03bb \u2208 {10 \u22122 , . . . , 10 2 } for distance and relaxed regularization until the classifier is not perfect. The tuned \u03bb is 1 in both cases. Data normalization is done with \u00b5 = 0.1307 and \u03c3 = 0.3081. The FC model has 4 hidden layers with 300 neurons each, ReLU activation, and cross-entropy loss. Training involves 20 epochs without approximate learning. The best model is chosen based on validation loss. The model is chosen based on the best validation loss from all epochs using stochastic gradient descent with Nesterov momentum. Parameters like learning rate, momentum, and batch size are specified. A grid search is done on \u03bb, C, \u03b3, and the representation is learned with a single layer scoRNN with LeakyReLU activation functions. The hidden neurons in scoRNN are set to 512. The dimension of hidden neurons in scoRNN is set to 512, with a cross-entropy loss function and AMSGrad optimizer. A grid search is performed on \u03bb, C, \u03b3, and models with similar testing accuracy to the baseline model are reported. Pre-trained ResNet-18 is used with model architecture modifications. The model architecture is revised by replacing max-pooling with average-pooling after the first convolutional layer and enlarging the receptive field of the last pooling layer to 512 dimensions. The model is trained with stochastic gradient descent with Nesterov momentum for 20 epochs, adjusting the learning rate and batch size accordingly. Tuning involves fixing C = 8 for computational efficiency. After revising the model architecture, tuning involves fixing C = 8 for computational efficiency. The training process includes using 18 samples for approximate learning, tuning \u03bb values, fixing \u03bb to 0.001, and training with 360 random samples for improved approximation using a genetic algorithm with 4800 populations and 30 epochs. Samples are evaluated based on the distance of their gradient from the target x, and the top 25% samples are selected in each epoch. The genetic algorithm used in the training process involves sorting samples based on distance, replacing 75% with a linear combination from the top 25%, projecting updated samples to ensure feasibility, and returning the sample with the maximum distance. Mutation was not implemented due to computational reasons. The crossover operator is compared to a gradient step, and images are visualized including original, original gradient, adversarial gradient, and image of adversarial gradient. The integrated gradient attribution method visualizes gradients and integrated gradients by aggregating derivatives, taking absolute values, normalizing, and clipping values. This results in a gray-scaled image that can be visualized by the element-wise product with the original image. The integrated gradient attribution method visualizes gradients and integrated gradients by aggregating derivatives, taking absolute values, normalizing, and clipping values, resulting in a gray-scaled image. The visualization highlights differences in settings using the Caltech-256 dataset on the ROLL model, showing examples at different percentiles of maximum gradient distortions. The Caltech-256 dataset is used to visualize maximum gradient distortions on the ROLL model. The vanilla model shows distortions for 'Projector', 'Bear', and 'Rainbow', while the ROLL model shows distortions for 'Bear' and 'Rainbow'."
}