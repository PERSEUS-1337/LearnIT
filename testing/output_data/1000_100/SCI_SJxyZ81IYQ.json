{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively. This approach preserves semantic content better and follows a natural human language structure. The current text chunk discusses the limitations of the sequential model in image captioning due to its inability to reflect hierarchical structures in natural languages. This contrasts with the previous paragraph, which introduces a new image captioning paradigm focused on extracting explicit semantic representation from images and constructing captions recursively. The text discusses the drawbacks of sequential models in image captioning, highlighting their reliance on n-gram statistics and lack of consideration for hierarchical dependencies. To address these issues, a new paradigm is proposed where semantics and syntax are separated into two stages for generating more accurate and meaningful captions. The correct captions are generated in two stages: first, the semantic content of the image is represented by noun-phrases, then the caption is constructed through recursive composition. The compositional procedure forms higher-level phrases by connecting selected sub-phrases. The compositional procedure for generating captions involves two parametric modular nets for phrase composition and completeness evaluation. This paradigm preserves semantic content, reflects natural language structures, increases caption diversity, and generalizes well to new data. Recent works in image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. This approach has shown to generalize well to new data and maintain good performance with limited training data. The early methods focused on bottom-up and detection-based approaches, extracting visual concepts from images and assembling them into captions using predefined templates or learned anchors. Recent works in image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. The approach involves representing the input image with feature vectors and applying attention mechanisms to extract relevant information. Various adjustments have been made to improve attention computation and capture semantic information in the input image. Some recent approaches focus on directly extracting phrases or semantic words from the image. The recent shift in image captioning involves using CNNs for image representation and RNNs for caption generation. Different approaches have been proposed to improve attention computation and capture semantic information. One approach involves treating noun-phrases as hyper-words and using a hierarchical LSTM model for phrase and word generation. However, these sequential generation methods tend to favor frequent n-grams, leading to issues like incorrect semantic coverage and lack of diversity. In contrast, a proposed paradigm represents the input image with noun-phrases and constructs captions through a recursive composition procedure, preserving the disentanglement between semantics and syntax. The recent shift in image captioning involves using CNNs for image representation and RNNs for caption generation. Different approaches have been proposed to improve attention computation and capture semantic information. One approach involves treating noun-phrases as hyper-words and using a hierarchical LSTM model for phrase and word generation. However, a proposed paradigm represents the input image with noun-phrases and constructs captions through a recursive composition procedure, preserving the disentanglement between semantics and syntax effectively. This procedure requires less data to learn and leads to more diverse captions compared to non-recursive methods. In image captioning, a two-stage framework is proposed for generating captions in a hierarchical structure. The framework involves deriving noun-phrases as a semantic representation and constructing captions through a recursive compositional procedure called CompCap. This approach differs from mainstream models by considering the hierarchical nature of natural language and can generate more diverse captions with less data. In a two-stage framework for image captioning, CompCap considers nonsequential dependencies among words and phrases, representing image semantics with explicit noun-phrases like \"a black cat\" and \"two boys\". This approach differs from conventional methods by focusing on visual understanding tasks. In a study on noun-phrase extraction for image captioning, the approach involves deriving a list of distinct noun-phrases from training captions and treating each as a class for multi-label classification. Visual features are extracted from images using a Convolutional Neural Network and encoded for binary classification of each noun-phrase. The approach involves binary classification of noun-phrases based on visual features extracted from images using a Convolutional Neural Network. The input image is represented by selecting top-scoring noun-phrases and pruning similar concepts through Semantic Non-Maximum Suppression. A recursive compositional procedure called CompCap constructs the caption by scanning ordered pairs of phrases and applying a Connecting Module to generate a sequence of words. The approach involves using a Connecting Module to generate a sequence of words that connect pairs of phrases in a plausible way. The module computes a score for the generated phrase and selects the one with the maximum score as the new phrase. An Evaluation Module is then used to determine if the new phrase is a complete caption. If not, the process is repeated with updated pairs until a complete caption is obtained. The Connecting Module aims to select a connecting phrase given left and right phrases, evaluating the connecting score. An alternative strategy is used to treat the generation of connecting phrases as a classification problem due to dealing with incomplete captions. The Connecting Module selects connecting phrases based on left and right phrases, treating it as a classification problem. It involves mining distinct connecting sequences from training captions and using a two-level LSTM model to encode phrases. The Connecting Module selects connecting phrases based on left and right phrases using a two-level LSTM model to encode phrases. The model involves low-level LSTM controlling attention with visual features and high-level LSTM driving the encoded state evolution. Encoders for phrases share the same structure but have different parameters to encode phrases differently based on their position in the ordered pair. The connecting scores are determined by a softmax layer, with the highest score connecting the phrases. The Evaluation Module (E-Module) determines if a phrase is a complete caption by encoding it into a vector and evaluating its probability. Other properties can also be checked, such as caption quality using a caption evaluator. The framework can be extended for generating diverse captions using beam search or probabilistic sampling to avoid local minima. User preferences can also be incorporated by filtering initial noun phrases. In the Experimental section, the influence on resultant captions can be controlled by filtering initial noun phrases or modulating their scores. Experiments were conducted on MS-COCO BID4 and Flickr30k BID5 datasets, each with a specific number of images and ground-truth captions. The vocabulary was standardized by converting words to lowercase, removing non-alphabet characters, and words appearing less than 5 times. Training captions were limited to 18 words, and a special token UNK was used for replaced words. To collect training data, ground-truth captions are parsed into trees using NLP toolkit BID31. The C-Module and E-Module are separately trained for classification tasks. The recursive compositional procedure is modularized for better generalization. Testing involves two forward passes for each module. Comparisons are made with NIC, AdapAtt, and TopDown methods, all encoding images as feature vectors. CompCap is compared with LSTM-A5 BID19, which predicts semantical concepts as additional visual features. All methods are re-implemented and trained using the same hyperparameters, with ResNet-152 BID16 used to extract image features. ResNet-152 is fixed without finetuning during training, and the learning rate is set to 0.0001 for all methods. Parameters that yield the best performance on the validation set are selected for caption generation during testing. Beam-search of size 3 is used for all methods. CompCap is compared with LSTM-A5 in caption generation using ResNet-152 for image features. CompCap selects 7 top noun-phrases for input representation. Results show CompCap performs best in SPICE metric but lags in CIDEr, BLEU-4, ROUGE, and METEOR compared to baselines. The study compares CompCap with LSTM-A5 in caption generation using ResNet-152 for image features. CompCap selects 7 top noun-phrases for input representation. Results show that while CompCap excels in SPICE metric, it falls behind in CIDEr, BLEU-4, ROUGE, and METEOR compared to baseline methods. The ablation study on the proposed compositional paradigm demonstrates that representing the input image with groundtruth noun-phrases significantly improves all metrics, indicating CompCap's effectiveness in preserving semantic content. CompCap generates better captions with improved semantic understanding of input images. By integrating ground-truth noun-phrases in composing order, metrics, except for SPICE, show further improvement. The compositional paradigm separates semantics and syntax, allowing CompCap to handle out-of-domain content effectively and requiring less data to learn. Studies confirm the effectiveness of this approach. In two studies, CompCap was tested with different training data ratios and datasets, showing competitive results with diverse captions. The approach disentangles semantics and syntax, benefiting from in-domain and out-of-domain data. CompCap can generate diverse captions by varying noun-phrases and composing order. Five metrics were computed to evaluate caption diversity, including novel and unique captions, vocabulary usage, and pair-wise editing distances. This analysis showed competitive results with diverse captions in different training data ratios and datasets. CompCap obtained the best results in all metrics for caption diversity, showing diverse and novel captions. Qualitative samples in FIG2 demonstrate different composing orders and noun-phrases. Error analysis in FIG4 highlights similar errors in failure cases. The analysis in FIG4 shows failure cases with errors in captions generated by CompCap, stemming from misunderstanding of input visual content. Proposed method for image captioning involves a compositional approach, factorizing captioning into two stages. The proposed method for image captioning involves a compositional approach, dividing the captioning procedure into two stages. The first stage extracts an explicit representation of the input image with noun-phrases, while the second stage assembles these noun-phrases into a caption using a recursive compositional procedure. This hierarchical structure of caption generation preserves semantics effectively, requires less data to train, generalizes better across datasets, and produces more diverse captions. The key for suppression is to identify semantically similar noun-phrases based on comparisons of central nouns. The method involves using encoders in the C-Module to get encodings for noun-phrases, computing normalized euclidean distances for semantically similar noun-phrases, and considering them similar if the sum of distances is less than a threshold. The C-Module uses encoders for P (l) and P (r) with independent parameters to encode ordered pairs, leading to better performance. Additional hyperparameters for CompCap can be tuned, such as beam search sizes for pair and phrase selection. Adjusting these hyperparameters individually is shown in FIG6. The size of beam search for pair selection and connecting phrase selection has minor influence on the performance of CompCap, as shown in FIG6."
}