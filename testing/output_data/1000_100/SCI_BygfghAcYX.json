{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is suggested for two layer ReLU networks, providing a tighter generalization bound. The capacity bound correlates with test error behavior with increasing network sizes and partly explains the improvement in generalization with over-parametrization. Additionally, a matching lower bound for the Rademacher complexity is presented, improving over previous capacity lower bounds for neural networks. Increasing the size of neural networks has led to an arms race in pursuit of better performance, even though they are over-parametrized. Surprisingly, larger models help improve generalization error, contrary to traditional wisdom on overfitting. Capacity is typically controlled by limiting parameters or adding regularization, but in neural networks, increasing model size enhances generalization. Increasing the size of neural networks has led to an arms race in pursuit of better performance, even though they are over-parametrized. Larger models help improve generalization error, contrary to traditional wisdom on overfitting. Capacity is typically controlled by limiting parameters or adding regularization, but in neural networks, increasing model size enhances generalization by improving generalization error without explicit regularization. Existing works have proposed various measures to assess the capacity of neural networks and explain generalization behavior. Even when a network is large enough to fit the training data perfectly, test error continues to decrease with network size. Unit capacity and unit impact are key factors in determining network complexity and output influence. Theorem 1 states that average unit capacity and impact decrease faster than 1/ \u221a h as the number of hidden units (h) increases. Existing complexity measures fail to explain why over-parametrization helps and increase with network size, even for two layer networks. In this study, the authors focus on the impact of the number of hidden units on network norms. They simplify the architecture to two layer ReLU networks and prove a tighter generalization bound. The capacity bound decreases with increasing hidden units, correlating with test error. The key insight is to analyze complexity at a unit level, showing that unit level measures decrease faster with increasing hidden units. The study focuses on the impact of hidden units on network norms in two-layer ReLU networks. Unit level measures decrease faster with increasing hidden units, leading to a tighter generalization bound. The closeness of learned weights to initialization in over-parametrized settings is discussed, with the number of hidden units going to infinity resulting in minimizing training error. The large number of hidden units in a layer represent all possible features, making the optimization problem about selecting the right features to minimize training loss. Studies have shown the importance of initialization in over-parametrized settings, with the closeness of learned weights to initialization playing a significant role in minimizing training error. In this paper, the authors empirically investigate the role of over-parametrization in generalization of neural networks on different datasets. They propose tighter generalization bounds for two-layer ReLU networks and introduce a complexity measure that decreases with the number of hidden units, potentially explaining the effect of over-parametrization on generalization. The authors provide a lower bound for the Rademacher complexity of two-layer ReLU networks with a scalar output, improving upon previous bounds. They focus on fully connected ReLU networks for c-class classification tasks, considering input dimension, output dimension, and number of hidden units. The prediction is based on the label with the highest output score. The margin operator \u00b5 is defined as the difference between the score of the correct label and the maximum score among other labels. The ramp loss is defined for any distribution D and margin \u03b3 > 0. The expected margin loss of a predictor f (.) is bounded between 0 and 1. The function class F is denoted as \u03b3 \u2022 F for the composition of the loss function and functions from class F. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels. By bounding the Rademacher complexity of neural networks, we can obtain a bound on the generalization error. The choice of the right function class is crucial to capture only the real trained networks, leading to a decrease in generalization error with increasing complexity. The complexity measure explains the decrease in generalization error with increasing width. Larger function classes may result in weaker capacity bounds. Experiments on CIFAR-10 dataset show behavior of network layers with increasing hidden units. Frobenius norm of weights in larger networks increases due to random initialization. The Frobenius norm of weights increases with network size due to random initialization. The distance to initialization per unit decreases with increasing hidden units. The distribution of angles shifts from orthogonal to aligned in large networks. Unit capacity is defined as the per unit distance to initialization. The behavior of the second layer differs from the first layer with increasing hidden units. In the second layer of the neural network, the Frobenius norm and distance to initialization decrease, indicating limited impact of initialization. As the network size grows, the norm of outgoing weights from hidden units decreases faster than 1/ \u221a h. This decrease in unit impact is defined as \u03b1 i = v i 2, the magnitude of outgoing weights. The hypothesis class of neural networks with bounded unit capacity and impact is studied for generalization behavior. A generalization bound for two-layer ReLU networks is proven by bounding the Rademacher complexity in terms of unit capacity and impact. This analysis provides a better understanding of network behavior. The Rademacher complexity of two-layer neural networks is bounded by decomposing the complexity across hidden units, leading to a tighter generalization bound. This new technique provides a more accurate understanding of network behavior compared to previous methods. The generalization bound for two-layer ReLU networks is improved by covering the space of possible values for \u03b1 and \u03b2. The bound decreases with increasing width for networks learned in practice and matches the Rademacher complexity, showing its tightness. The generalization bound for two-layer ReLU networks is improved by covering the space of possible values for \u03b1 and \u03b2. The additive factor\u00d5( h/m) in the bound is small in the regimes of interest, resulting in a decrease in capacity with over-parametrization. The bound is further extended to p norms in Appendix Section B, showing a finer tradeoff between terms. Comparing with Golowich et al., the key complexity terms differ slightly, but behave similarly for the range of h considered. The study compares V and F networks with different numbers of classes, showing that even with similar behavior, U-U0 can vary significantly. Experimental results on CIFAR-10 and SVHN datasets reveal that larger networks can achieve better generalization, even without regularization. Unit capacity and impact decrease with increasing network size. The study shows that larger networks achieve better generalization without regularization. Unit capacity and impact decrease with network size, as shown in various figures. The effective capacity of the function class decreases with h, improving over other bounds for networks larger than 1024. Our capacity bound decreases with network size, unlike other norm-based bounds which increase significantly for larger networks. This suggests that our measure may point to properties allowing over-parametrized networks to generalize. Additionally, we compare our complexity measure between networks trained on real and random labels. In this section, a lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound. The lower bound is shown on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer. This allows for comparison with existing results and extends the lower bound to the bigger class F W. The lower bound for the Rademacher complexity of neural networks matches the dominant term in the upper bound. It shows that even with additional constraints on the spectral norm, the upper bound provided in Theorem 1 is tight. The lower bound for spectral norm bounded neural networks with scalar output and element-wise activation functions reveals a capacity gap compared to linear models. It excludes networks with rank-1 weight matrices, showing a \u0398( \u221a h)-capacity difference between ReLU and linear networks. This bound does not apply to linear networks and can be extended to more layers by setting intermediate layer weight matrices to the Identity matrix. Theorem 7 in Golowich et al. (2018) provides a lower bound for neural networks with bounded spectral norm, improving on previous results. The paper introduces a new capacity bound for neural networks that decreases with the number of hidden units, potentially explaining better generalization performance of larger networks. The paper introduces a new capacity bound for neural networks that decreases with the number of hidden units, potentially explaining better generalization performance of larger networks. The study also discusses the role of depth and width in controlling network capacity, with future directions for research. The paper provides a lower bound for network capacity, but the absolute values are still larger than the number of training samples, prompting the need for smaller bounds. The study does not address how optimization algorithms converge to low complexity networks or how hyperparameter choices affect the complexity of solutions. In an experiment, a pre-activation ResNet18 architecture was trained on CIFAR-10 dataset with specific settings. The architecture included a convolution layer, 8 residual blocks, and a linear layer. The number of output channels and strides in the blocks varied based on the number of channels in the first convolution layer. 11 architectures were trained with different values for k. In experiments, 11 architectures were trained with varying values for k. SGD was used with specific settings, including mini-batch size 64, momentum 0.9, and initial learning rate 0.1. Data augmentation was performed by random horizontal flip and random crop. Fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets with architectures ranging from 2^3 to 2^15 hidden units. For experiments, 11 architectures were trained with varying values for k using SGD with specific settings. Data augmentation was performed, and fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets with different hidden unit architectures. The training used a fixed step size and stopped when the cross-entropy reached 0.01 or after 1000 epochs. Generalization bounds were calculated with specific margins and adjustments for the number of classes. Random initialization was used as the reference matrix for the bounds. In Bartlett et al. (2017) and BID0, random initialization was used as the reference matrix for estimating distributions with Gaussian kernel density. Figures 6 and 7 display measures on networks trained on SVHN and MNIST datasets. Over-parametrization in MNIST is shown in FIG10, with comparisons of generalization bounds. Theorem 2 is generalized to p norm, with Lemma 11 constructing a cover for the p ball with entry-wise dominance. Theorem 5 provides a bound on generalization error for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1), and U 0 \u2208 R h\u00d7d. The p norm of row 2 norms is discussed, with improvements on the additive term in Theorem 2. A tighter bound for generalization error is provided in Corollary 6, with a vector-contraction inequality for Rademacher complexities. Lemma 7 from Maurer (2016) is used in the proof, relating the norm of a vector to the expected magnitude of its inner product with Rademacher random variables. The Rademacher complexity of a class of networks can be decomposed to that of hidden units, as shown in Lemma 9. The complexity is bounded using induction, with a base case for t = 1 and a proof for t = t + 1. The inequality is derived from the Lipschitzness of the ramp function. The proof of Theorem 1 involves applying Lemma 10 with specific functions and variables, leading to a final inequality that completes the induction proof. The proof of Theorem 1 involves applying Lemma 10 with specific functions and variables, leading to a final inequality that completes the induction proof. The proof is completed by taking the sum of the inequality over j from 1 to h, using a covering lemma to prove the generalization bound without assuming knowledge of network parameter norms. Lemma 11 shows how to cover a ball with a dominating set entry-wise, with bounds on the cover size. Lemma 13 provides a bound on generalization error for given parameters and training set. Lemma 14 provides specific results for the case p = 2, with bounds on various variables and constants. The lemma establishes conditions for the second term in Eq. 12 to be larger than 1, ensuring the validity of the inequality. The generalization error for a function f(x) with specific V and U matrices is bounded by a lemma, with a proof involving an upper bound for the generalization bound. The proof of Theorem 2 follows from this lemma, which provides a generalization bound for any p \u2265 2, with additional constants and logarithmic factors. The generalization error for a function f(x) with specific V and U matrices is bounded by a lemma, with a proof involving an upper bound for the generalization bound. The proof of Theorem 5 directly follows from Lemma 15 and uses notation to hide constants and logarithmic factors. The proof of Theorem 3 starts with specific values for h, d, and m, and involves picking V and using a specific dataset. The dataset is divided into 2k groups with n copies of different elements in an orthonormal basis. A matrix U(\u03be) is defined as the product of a diagonal matrix Diag(\u03b2) and a matrix F(\u03be). The 2-norm of each row of F is bounded by 1, leading to an upper bound of U(\u03be) 2."
}