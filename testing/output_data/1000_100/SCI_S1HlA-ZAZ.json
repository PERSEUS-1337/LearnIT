{
    "title": "S1HlA-ZAZ",
    "content": "We present an end-to-end trained memory system inspired by Kanerva's sparse distributed memory. It has a robust distributed reading and writing mechanism and is analytically tractable for optimal compression. Formulated as a hierarchical conditional generative model, the memory improves generative models on Omniglot and CIFAR datasets. Compared to the Differentiable Neural Computer (DNC), our memory model has greater capacity and is easier to train. The text discusses various approaches to augment neural networks with fast memory stores, highlighting the challenge of efficiently utilizing memory. Different models like Differentiable Neural Computers (DNCs), Matching Networks, Neural Episodic Controller, and Neural Statistician employ different strategies for memory management. While some models store embeddings directly, others summarize datasets to reduce memory requirements. The text discusses memory structures in neural networks, focusing on efficient storage of information. Various models like Hopfield Net, Boltzmann Machine, and Kanerva's sparse distributed memory are explored for their memory capacity and reading/writing mechanisms. These models offer insights into designing memory structures that can store data effectively. The paper introduces a conditional generative memory model inspired by Kanerva's sparse distributed memory. It includes learnable addresses and reparametrised latent variables to optimize memory writing. The model adapts quickly to new data, providing top-down knowledge in addition to bottom-up perception. Our proposal introduces a memory system that enriches priors in VAE-like models through adaptive memory, offering effective compression and storage of complex data. The memory architecture extends VAE by deriving the prior from an adaptive memory store, with parameters \u03b8 for the generative model and \u03c6 for the inference model. The inference model's parameters are represented by \u03c6. Parameterised distributions are implemented as multivariate Gaussian distributions with diagonal covariance matrices. The objective of training a VAE is to maximize its log-likelihood by jointly optimizing \u03b8 and \u03c6 for a variational lower-bound of the likelihood. This includes a negative reconstruction loss term and a regularizer term to encourage the approximated posterior to be near the prior of z. The model introduces the concept of an exchangeable episode. The concept of an exchangeable episode is utilized in training memory-based generative models. The joint distribution is factorized into the marginal distribution and the posterior, allowing for the computation of the posterior as writing data into memory. Maximizing mutual information between memory and data is key in this scenario. The joint distribution of the generative model involves latent variables Y and Z, with memory M represented as a random matrix with a matrix variate Gaussian distribution. The distribution is factorized to show conditional independence, with matrices R, U, and V determining the mean and covariance of M. Independence is assumed between columns but not rows of M. The memory matrix M is represented as a random matrix with a matrix variate Gaussian distribution, with independence assumed between columns but not rows. The addresses A are optimised through back-propagation and normalized to avoid degeneracy. The addressing variable yt computes weights for memory access, with a learned projection transforming yt into a key vector. The weights wt across rows of M are computed using a multi-layer perception (MLP) projection. The code zt is a learned representation that generates samples of xt through a conditional distribution. The prior for zt is memory-dependent, resulting in a richer marginal distribution. In the hierarchical model, M captures global statistics of an episode, while yt and zt capture local statistics for data within an episode. The reading inference model refines the prior distribution of zt with evidence from xt, using a parameterized posterior distribution. Updating memory involves a trade-off between preserving old information and incorporating new information. Updating memory involves a trade-off between preserving old information and writing new information optimally through Bayes' rule. Memory writing is interpreted as inference, computing the posterior distribution of memory. Batch and online inference methods are considered, approximating the posterior distribution of memory using one sample to approximate the integral. The posterior distributions of the addressing and code variables are parameterized. Updating memory involves preserving old information and writing new information optimally through Bayes' rule. The posterior of memory in a linear Gaussian model is analytically tractable, with parameters R and U updated using a specific rule. Prior parameters are trained through back-propagation to learn the dataset's general structure, allowing the posterior to adapt to features. The update rule for memory in a linear Gaussian model involves inverting \u03a3 z with a complexity of O(T 3) and storing/multiplying the row-covariance matrix U with a complexity of O(K 2). On-line updating can reduce costs by using one sample at a time, and updating using the entire episode at once is equivalent to iterative one-sample updates. Diagonal covariance can reduce costs to O(K), but experiments suggest full covariance is useful for coordination. To reduce costs, diagonal covariance can be used to lower it to O(K), while full covariance is beneficial for memory coordination. Training the model involves optimizing a variational lower-bound of the conditional likelihood. Sampling from q\u03c6(yt, zt|xt, M) is done to approximate the inner expectation efficiently. A mean-field approximation is used for memory to avoid expensive Cholesky decomposition of the non-diagonal matrix U. Future work includes investigating low-rank approximation of U for better cost-performance balance. The Gaussian distribution is utilized for distribution-based reading and writing operations in the model. An iterative reading mechanism similar to Kanerva's sparse distributed memory is implemented to decrease errors and converge to stored memory. In our model, an iterative process similar to Kanerva's sparse distributed memory is used for denoising and sampling. By incorporating knowledge about memory in reading, we improve the coupling between x t and y t. Despite the computational cost, intractable posteriors can be efficiently approximated using loopy belief-propagation. This iterative sampling approach enhances the local coupling between x t and y t. Our model uses an iterative process to improve the coupling between x t and y t, with future research focusing on understanding this process. The model implementation details are provided in Appendix C, using encoder and decoder models for evaluating the adaptive memory. Experiments were conducted with Omniglot and CIFAR datasets, adjusting parameters such as filter numbers, memory size, and code size. The Adam optimizer was used with minimal tuning, and the variational lower bound was reported for comparison. Our model was tested on the Omniglot dataset with 1623 classes and 20 examples each, using a 64x100 memory M and a 64x50 address matrix A. We randomly sampled 32 images for each \"episode\" without class labels. The model was also tested on the CIFAR dataset with 32x32x3 color images. For the CIFAR dataset, convolutional coders with 32 features at each layer, a code size of 200, and a 128x200 memory with a 128x50 address matrix were used. The model was compared to a baseline VAE model, showing a modest increase in parameters. The negative variational lower bound, reconstruction loss, and KL-Divergence were monitored during learning. The model achieved better results compared to the VAE on the Omniglot dataset, with lower negative variational lower-bound, improved reconstruction, and KL-divergence. The KL-divergence sharply decreased around the 2000th step, indicating the model learned to utilize memory for a more informative prior. The top-down prior from memory provided most of the information for the code. The VAE achieved a negative log-likelihood (NLL) of \u2264 112.7 at the end of training, worse than state-of-the-art unconditioned generation but comparable to results with IWAE training. The reduction in KL-divergence was crucial for improving sample quality, as observed in experiments with Omniglot and CIFAR datasets. The Kanerva Machine achieved a conditional NLL of 68.3, demonstrating the power of incorporating an adaptive memory into generative models. The weights were well distributed over the memory, showing patterns written into the memory were superimposed on others. The reconstruction of inputs and weights used in reconstruction were widely distributed across memory slots, with denoising through iterative reading. The study compares samples from the VAE and the Kanerva Machine, showing improved sample quality in consecutive iterations. Most samples stabilized after the 6th iteration, indicating convergence in iterative sampling. The study compares samples from VAE and Kanerva Machine, showing improved sample quality in consecutive iterations. Samples stabilized after the 6th iteration, indicating convergence in iterative sampling. However, this approach does not apply to VAEs as their sample quality did not improve after iterations. The model tested the recovery of original images from corrupted inputs through iterative reading. Despite some cases producing incorrect patterns, the model showed interpretability of internal representations in memory. Linear interpolations between address weights were found to be meaningful, as demonstrated in Figure 2 in Appendix A. The model tested the recovery of original images from corrupted inputs through iterative reading, showing interpretability of internal representations in memory. Linear interpolations between address weights were found to be meaningful, as demonstrated in Figure 2 in Appendix A. Comparisons were made with the Differentiable Neural Computer (DNC) and a variant, the Least Recently Used Architecture (LRUA), using the same episode storage and retrieval task with Omniglot data. Training curves and test variational lower-bounds of DNC and Kanerva Machine were analyzed, highlighting differences in sensitivity to initialization and error plateauing. The DNC and Kanerva Machine were compared in training performance. The DNC was sensitive to hyper-parameters and only 2 out of 6 instances reached a test loss close to 100. In contrast, the Kanerva Machine was robust to hyper-parameters and trained fastest with batch size 16 and learning rate 1 \u00d7 10 \u22124, converging below 70 test loss with all configurations. The Kanerva Machine is easier to train due to principled reading and writing operations not dependent on model parameters. The Kanerva Machine is a memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model. It generalizes well to larger episodes and outperforms the DNC in terms of variational lower-bound. The model does not depend on model parameters for reading and writing operations. Our architecture, inspired by Kanerva's model, removes the assumption of uniform data distribution by training a generative model. Memory is implemented as a generative model, allowing retrieval of unseen patterns through sampling. Previous works have provided probabilistic interpretations of Kanerva's model, but our model is the first to generalize it to continuous, non-uniform data. Our model generalizes Kanerva's memory model to continuous, non-uniform data while integrating with deep neural networks for modern machine learning. Unlike other models, our model quickly adapts to new data for episode-based learning. Our model efficiently stores information in a compressed form by leveraging statistical regularity in images through an encoder, learned addresses, and Bayes' rule for memory updates. It employs an exact Bayes' update-rule without compromising neural network flexibility. Combining classical statistical models with neural networks shows promise for novel memory models in machine learning. Kanerva's memory model in machine learning involves a fixed table of addresses pointing to a modifiable memory. The addresses and memory have the same size of K \u00d7 D, where K is the number of addresses and D is the input dimensionality. Inputs are compared with addresses through Hamming distance, calculated as h(a, b) = 1/2(D - a \u00b7 b). Kanerva's memory model involves storing patterns in memory addresses based on Hamming distance. Addresses are selected if distance to input is below a threshold. Reading involves summing memory contents at selected addresses. Sparse and distributed operations ensure correct retrieval even with overwriting. Kanerva showed that even corrupted queries can be discovered from memory. The application of Kanerva's memory model is limited by the assumption of uniform and binary data distribution, which is rarely true in practice. Real-world data often lie on low-dimensional manifolds, making binary representation less efficient in neural network implementations optimized for floating-point numbers. Our model architecture includes a convolutional encoder for converting input images into embedding vectors, with 3 consecutive blocks of convolutional layers. The model architecture includes a convolutional layer with a 4x4 filter and stride 2, followed by a ResNet block without bottleneck. The convolutional layers have 16 or 32 filters, and the output is linearly projected to a 2C dimensional vector. Adding noise to the input helps stabilize training. Gaussian noise with a standard deviation of 0.2 is used. Different likelihood functions are used for different datasets to avoid collapsing. To prevent Gaussian likelihood collapsing, uniform noise U(0, 1/256) is added to CIFAR images during training. The differentiable neural computer (DNC) is wrapped with the same interface as the Kanerva memory for fair comparison. DNC receives addressing variable y_t and z_t during reading and writing stages, using a 2-layer MLP with 200 hidden neurons. During writing, DNC's read-out is discarded, keeping its state as memory; during reading, the state is discarded at each step to prevent storing new information. The DNC uses a 2-layer MLP with 200 hidden neurons as the controller instead of LSTM to avoid interference with external memory. To prevent confusion in auto-encoding, the controller output is removed to ensure DNC only reads from memory. The focus is on memory performance, with tests showing the importance of covariance between memory rows. The DNC uses a 2-layer MLP with 200 hidden neurons as the controller instead of LSTM to avoid interference with external memory. The models using full covariance matrices were slightly slower per-iteration, but the test loss decreased far more quickly. The bottom-up stream in our model compensates for memory by directly sampling z t from p \u03b8 (z t |y t , M ) for the decoder p \u03b8 (x t |z t ). The negative variational lower bound, reconstruction loss, and total KL-divergence during CIFAR training show similar patterns to those in Fig. 2. The relatively small difference in KL-divergence significantly influences sample quality. The Kanerva Machine shows increasing advantage over VAE in training. Linear Gaussian model defined by Eq. 6, joint distribution p(vec(Z), vec(M)) = N(vec(Z), vec(M); \u00b5j, \u03a3j). Posterior distribution p(vec(M)|vec(Z)) = N(vec(M); \u00b5p, \u03a3p) derived using conditional formula for Gaussian. Update rule in eq. 9 to 11 derived from matrix variate Gaussian distribution properties. Model uses samples from q\u03c6(z_t|x_t) for writing to memory and mean-field approximation for reading. An alternative method utilizing the analytic tractability of the Gaussian distribution is described here, using \u03c8 = {R, U, V} to represent memory parameters."
}