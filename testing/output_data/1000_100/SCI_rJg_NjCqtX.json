{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing chemical compound names. A framework using spelling error correction, byte pair encoding tokenization, and neural sequence to sequence model is proposed for auto standardization from non-systematic names to systematic names. The framework achieves a standardization accuracy of 54.04% on the test dataset, a significant improvement compared to previous results. There are over 100 million named chemical substances worldwide, each assigned systematic names by IUPAC. However, many chemicals also have common or trivial names for simplicity, like sucrose being known as sugar. Chemical substances in the pharmaceutical industry often have proprietary names to differentiate products. Chemical information extraction relies on standard chemical names for databases like PubChem and SciFinder. Updating these databases with information from chemical papers is an ongoing task. Chemical information extraction from papers is crucial for updating databases like PubChem and SciFinder. Using systematic names makes it easier to convert to other representations like SMILES and InCHI. OPSIN is a system that can convert systematic names to SMILES with high precision. Non-systematic names can lead to errors, including spelling differences. Non-systematic names can lead to errors such as spelling, ordering, common name, and synonym errors. These errors can occur simultaneously in a single non-systematic name, making the task challenging. The task of converting non-systematic chemical names to systematic names is challenging due to errors like spelling, ordering, common name, and synonym errors. A framework is proposed to address these errors through spelling error correction, BPE tokenization, and a sequence to sequence model. Limited work has been done in this area, with BID2 being a notable citation for their online system ChemHits, which relies heavily on chemical knowledge. The sequence to sequence model is used for converting non-systematic chemical names to systematic names, similar to machine translation. The framework is fully data-driven, achieving 54.04% accuracy without external chemical knowledge. The work is based on a corpus of chemical names from high-impact Chemical Journals. The corpus from Chemical Journals with High Impact factors (CJHIF) contains non-systematic and systematic names of chemical substances. There are 384816 data pairs in the corpus. The Levenshtein distance between non-systematic and systematic names is shown in FIG1. The experiment uses 80%, 19%, and 1% data for training, testing, and development sets respectively. The goal is to correct spelling errors in chemical substance names by separating them into elemental words. The text discusses the process of correcting spelling errors in chemical substance names by separating them into elemental words. Two vocabularies are set up from the dataset: one for systematic elemental words and one for non-systematic elemental words. The non-systematic names are used to build an elemental vocabulary, and common names or synonyms are identified. These vocabularies are combined to create a final elemental vocabulary for efficient correction search using BK-Tree BID0. The text discusses using BK-Tree BID0 to efficiently correct spelling errors in chemical substance names by structuring an elemental vocabulary. BK-Tree is a tree structure widely used for spelling error correction, where the smallest Levenshtein distance between words is rapidly identified. This method allows for correcting non-systematic names and easily inserting new training data, making it scalable. The BK-Tree is used to correct spelling errors in chemical substance names by breaking them into elemental words and inputting them one by one. This method helps correct non-systematic names and reduce noise in training the sequence to sequence model. Each node in the BK-Tree represents an elemental word, with edges showing the Levenshtein distance between them. This allows for efficient correction and scalability in inserting new training data. To apply the sequence-to-sequence model, chemical names are tokenized using Byte Pair Encoding (BPE) BID11. The symbol set is initialized by splitting names into characters and iteratively counting symbol pairs to create new symbols. BPE is chosen for its efficiency in tokenization. After tokenizing chemical names using Byte Pair Encoding (BPE) for efficient tokenization, the split pairs are used to train a sequence-to-sequence model, which consists of two recurrent neural networks (RNN) - an encoder that processes the non-systematic names separated by BPE and generates context. The sequence-to-sequence model uses a BiLSTM encoder to generate a context vector H from source sequences and a decoder to generate target sequences. The decoder calculates the probability of output sequences, with a parameter threshold for BK-Tree in spelling error correction. In the error correction stage, the BK-Tree threshold is the only parameter tested with values of 1, 2, and 3. At the BPE stage, the number of merge operations parameter was tested with values of 2500, 5000, 10000, and 20000. The sequence-to-sequence model uses word embeddings and hidden states of dimension 500, with a vocabulary size based on basic characters and merge operations. The encoder and decoder both have 2 layers. Training involves spelling error correction for non-systematic names, with parameters trained jointly using SGD and a cross-entropy loss function. The loss is computed over a minibatch of size 64 and normalized. Weights are initialized with a random uniform distribution from -0.1 to 0.1. Initial learning rate is 1.0 with decay applied every epoch after epoch 8. Dropout rate is 0.3 and model is trained for 15 epochs. Beam size is set to 5 for decoding. Another experiment replaces sequence to sequence model with Statistical Machine Translation (SMT) model using Moses system BID6. Training sequences are limited to length 80 with a 3-grams language model. Tokenization is done with BPE using 5000 merge operations. Data augmentation is used for neural model learning. Data augmentation is a technique used in neural model learning to handle noisy data, specifically spelling errors. Different error insertion methods are applied with equal probability, including inserting, deleting, exchanging, and replacing characters. The experiment measures standardization quality using accuracy and BLEU score BID10. Accuracy is calculated by the successful standardization of non-systematic names divided by the total number. The experiment results on test dataset show that the combination of spelling error correction, BPE tokenization, and sequence to sequence model performs the best. The framework outperforms the SMT model and the ChemHits system. BPE merge operation with 5000 merges is optimal. Spelling error correction and data augmentation improve the framework's performance. The sequence to sequence model is helpful for standardization, outperforming data augmentation. Overcorrection can occur with large thresholds. Examples show the model correcting non-systematic names, including spelling and synonym errors. The sequence to sequence model corrects non-systematic chemical names, including common name and ordering errors. Visualization of attentions in an example shows the model's ability to find relations and standardize names successfully. In this section, the system analyzes failed standardization attempts by randomly selecting 100 samples and labeling their error types. The most confusing error type is synonym error, while the system performs well at spelling errors. Common errors are challenging due to the difficulty in finding a rule between common names and systematic names. Among the samples, some are nearly correct, some are totally incorrect, and the rest are partially incorrect. The system analyzes failed standardization attempts by randomly selecting 100 samples. Nearly half of the non-systematic names are not successfully standardized. The accuracy for systematic names of different lengths is shown in Figure 6, with the best performance for names between 20 and 40 characters. The model does not consider chemical rules, leading to some names that disobey the rules. Examples of failed attempts are shown in Table 8. The text discusses a framework for converting non-systematic chemical names to systematic names, achieving an accuracy of 54.04% on the dataset. The framework includes spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. This approach outperforms previous rule-based systems, enabling more practical extraction of chemical information. The framework discussed in the text converts non-systematic chemical names to systematic names with an accuracy of 54.04%. It includes spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. The framework is trained end to end, data-driven, and independent of external chemical knowledge, opening up new research possibilities for chemical information extraction."
}