{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. To address this, a unified framework called EdgeGANRob has been proposed, which focuses on extracting shape/structure features from images to improve CNN robustness. Additionally, a robust edge detection approach called Robust Canny has been introduced to reduce sensitivity to adversarial perturbations. EdgeGANRob is compared to a simplified version called EdgeNetRob for further insights. EdgeNetRob boosts model robustness but lowers clean model accuracy. EdgeGANRob improves clean model accuracy compared to EdgeNetRob without sacrificing robustness. Extensive experiments demonstrate EdgeGANRob's resilience across various learning tasks and settings. Convolutional neural networks (CNNs) have achieved state-of-the-art performance but are vulnerable to adversarial examples and data poisoning attacks, reducing generalization accuracy. Recent studies show CNNs learn surface statistical regularities instead of high-level abstractions. Recent studies explore the vulnerability of CNNs to adversarial examples, attributing it to non-robust but highly-predictive features. They suggest training classifiers only on \"robust features\" that are insensitive to perturbations. Human recognition relies on global object shapes, while CNNs are biased towards local patterns. Recent studies suggest that CNNs are vulnerable to adversarial examples due to their bias towards local features. Human recognition relies on global object shapes, while CNNs are more focused on local patterns. Researchers propose improving CNN robustness by emphasizing global shape structure in training. This paper proposes using edges as a shape representation to improve CNN robustness to adversarial attacks. The EdgeGANRob approach leverages structural information in images through a two-stage procedure called EdgeNetRob, which detects edges and trains the classifier on them. The EdgeNetRob approach improves CNN robustness by training on extracted edges, eliminating texture bias. A robust edge detection algorithm, Robust Canny, enhances EdgeNetRob's robustness against attacks. However, EdgeNetRob decreases clean accuracy due to missing texture/color information, leading to the development of EdgeGANRob. The EdgeGANRob model enhances CNN robustness by refilling texture/colors based on edge images before classification. It introduces a unified framework to extract edge/structure information and reconstruct original images using GAN. Additionally, a robust edge detection approach, Robust Canny, is proposed to reduce sensitivity to adversarial perturbations. The inpainting GAN in EdgeGANRob is evaluated for effectiveness in learning tasks on robust edge features. Evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Defense methods against adversarial examples are discussed, highlighting the importance of evaluating against customized white-box attacks. Defense methods should be evaluated against strong adaptive attacks and distribution shifting in real-world applications. CNNs tend to learn superficial statistical cues, but methods like penalizing predictive power of local representations can help mitigate this. Benchmark datasets are proposed for evaluating model robustness under common perturbations. Backdoor attacks work by injecting malicious code. Backdoor attacks inject patterns into training data to manipulate model predictions. Detection methods using robust statistics have been proposed. Neuron pruning can protect models from backdoor attacks. Recent research shows CNNs rely more on textures than global shape structure for image recognition. Adversarially robust models tend to capture global structure of objects. Non-robust features exist in natural images. Ilyas et al. (2019) argue that non-robust features in natural images are highly predictive but not interpretable by humans. They propose using edge features as robust features in a new classification pipeline called EdgeGANRob. The method extracts edge/structure features, reconstructs images with a GAN, and evaluates robustness under three settings. EdgeNetRob is a simplified version of EdgeGANRob that focuses on edge detection and training an image classifier solely on edge maps. This approach aims to reduce sensitivity to local textures but may degrade CNN performance on clean test data due to missing texture/color information. EdgeGANRob is developed to refill texture/colors of edge images, improving clean accuracy. The robustness of the classification system relies on the edge detector used, leading to the proposal of a robust edge detection algorithm called Robust Canny. Traditional edge detection methods are found to be more robust than neural network-based detectors like HED. The proposed Robust Canny edge detector improves the robustness of traditional edge detection methods like Canny by truncating noisy pixels in its intermediate stages. It includes stages such as noise reduction, gradient computation, noise masking, and non-maximum suppression. The Robust Canny edge detector enhances traditional methods by truncating noisy pixels in intermediate stages, including noise reduction, gradient computation, noise masking, and non-maximum suppression. It involves steps like edge thinning, double thresholding, and edge tracking by hysteresis to detect strong and weak edge pixels. The algorithm also incorporates a noise masking stage after computing image gradients to mitigate perturbation noise. By adding a truncation operation, adversarial noise on the gradient map with small magnitude is reduced without affecting final edge map quality. Parameters of Canny (e.g. \u03c3, thresholds \u03b8 l , \u03b8 h ) also impact robustness level, with larger \u03c3 and higher thresholds resulting in better robustness but potentially sacrificing clean accuracy. Careful parameter selection is crucial for a robust edge detector. Details on training a Generative Adversarial Network (GAN) in EdgeGANRob are provided in the experiment section. In EdgeGANRob, a Generative Adversarial Network (GAN) is trained to generate color images from edge maps using a two-step process. The first stage involves training a conditional GAN with adversarial and feature matching losses. In the second stage, the GAN is fine-tuned along with a classifier to improve classification accuracy of the generated RGB images. EdgeGANRob improves robustness under adversarial attack, distribution shifting, and backdoor attack by inpainting GAN. The method focuses on generating realistic images to facilitate classifier fine-tuning for accuracy. Leveraging edge features enhances model generalization by preserving shape structure. EdgeGANRob focuses on shape structure to make it less sensitive to distribution changes during testing, protecting against backdoor attacks by removing malicious patterns through edge extraction. The method's robustness is evaluated against adversarial attacks, showing unique advantages in certain settings. EdgeNetRob, a variant without inpainting GAN, is also considered for comparison as a robust recognition method. The study evaluates the robustness of different methods against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. MNIST and CIFAR-10 datasets were not chosen due to their limitations. The same network architecture is used for classification, and \u221e adversarial perturbation constraints are applied. The study evaluates the robustness of different methods against adversarial attacks on Fashion MNIST and CelebA datasets using standard perturbation budgets. The evaluation includes white-box attacks using the BPDA attack and the need for a robust edge detector for defense is highlighted. Edge detection methods RCF, Canny, and Robust Canny are evaluated by training a classifier on the extracted edge maps. Results on Fashion MNIST show that RCF edges are not robust under strong adaptive attacks. Comparisons with Adversarial training method show a small drop in clean accuracy for EdgeNetRob and EdgeGANRob. EdgeNetRob and EdgeGANRob achieve higher clean accuracy compared to the vanilla baseline model and adversarial training with = 8. EdgeGANRob outperforms EdgeNetRob on the CelebA dataset, highlighting the importance of using GANs on complex datasets. Both models show robustness against strong adaptive attacks, with EdgeNetRob being more time-efficient due to not using adversarial training. Generalization ability under distribution shifting is also tested. In experiments testing generalization ability under distribution shifting, models EdgeNetRob and EdgeGANRob significantly improve accuracy on perturbed images with negative color, radial kernel, and random kernel patterns compared to the state-of-the-art method PAR. Visualization results are provided in Appendix D, with overall results shown in Table 3. Our method outperforms PAR in accuracy on greyscale images, utilizing edge features for CNN generalization under distribution shifting. It also serves as a defense against backdoor attacks, embedding invisible watermarks in images for Fashion MNIST and CelebA datasets. Qualitative results are shown in figures, with specific attack and target pairs chosen for each dataset. Comparisons are made with a baseline method proposed in Tran et al. (2018). Our method successfully attacks the vanilla Net with high poisoning accuracy on CelebA and Fashion MNIST datasets. Spectral Signature does not perform well with invisible watermark patterns, while EdgeNetRob and EdgeGANRob consistently show low poisoning accuracy. EdgeGANRob achieves better clean accuracy compared to EdgeNetRob. The effect of invisible watermark patterns can be removed by the edge detector. Our method combines a robust edge feature extractor with a generative adversarial network to improve model robustness against adversarial attacks and distribution shifting. It also enhances robustness against backdoor attacks. We resize images in CelebA to 128 \u00d7 128 and normalize data to [-1, 1]. On Fashion-MNIST, we use a LeNet-style CNN. The study utilizes different models and attacks for Fashion-MNIST and CelebA datasets. They employ LeNet-style CNN for Fashion-MNIST and ResNet for CelebA. Various attacks such as Projected Gradient Descent (PGD) and Carlini & Wagner \u221e attack (CW) are used with specific parameters. Robust Canny is used for evaluating adversarial robustness with different hyperparameters for each dataset. The study uses different models and attacks for Fashion-MNIST and CelebA datasets, employing LeNet-style CNN for Fashion-MNIST and ResNet for CelebA. Robust Canny is used to evaluate adversarial robustness with specific hyperparameters for each dataset. In a white-box attack scenario, backpropagating gradients through non-differentiable transformations is necessary to construct adversarial samples. The Backward Pass Differentiable Approximation (BPDA) technique can be used to replace non-differentiable transformations with differentiable approximations for stronger attacks. A differentiable approximation of the Robust Canny algorithm is found by breaking the transformation into two stages. The Robust Canny algorithm is broken into two stages: C1 (steps 1-3) and C2 (steps 4-6). C2 is a non-differentiable operation where the output is a masked version of the input. To make R-Canny differentiable for BPDA, the mask is assumed to be constant. Gradients are only backpropagated through C1, not M. Test accuracy changes are shown under radial and random mask transformations in Figure A. The radius of the mask in the Fourier domain is explored with random mask transformations using various probabilities. Additional visualization results for CelebA under different distribution shifts are shown. Qualitative results of EdgeGANRob and EdgeNetRob for backdoor attacks on Fashion MNIST are presented, with EdgeNetRob showing the ability to slightly remove poisoning patterns. The generated images do not share similar patterns."
}