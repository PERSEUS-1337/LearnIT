{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization in deep neural networks. Three novel observations about dropout in DNNs with ReLU activations are discussed: 1) it encourages training on nearby data points, 2) different layers have varying neural-deactivation rates, and 3) there can be inconsistencies with batch normalization. These observations lead to improvements in dropout, resulting in the proposed method \"Jumpout.\" Jumpout samples dropout rates using a decreasing distribution. The proposed method \"Jumpout\" improves dropout in deep neural networks by sampling dropout rates using a decreasing distribution and adaptively normalizing the dropout rate at each layer and training sample. This approach aims to address inconsistencies with batch normalization and encourages training on nearby data points for better generalization. Jumpout improves dropout in deep neural networks by sampling dropout rates using a decreasing distribution and adaptively normalizing the rate at each layer and training sample. This approach aims to address inconsistencies with batch normalization and enhances generalization performance on unseen data. Compared to traditional dropout, Jumpout shows significantly improved performance on various datasets with minimal additional memory and computation costs. Deep learning has achieved remarkable success on various machine learning tasks, but overfitting can weaken generalization performance. Dropout is a technique to mitigate overfitting by randomly setting hidden neuron activations to 0, reducing co-adaptation. However, dropout has drawbacks such as needing to tune dropout rates for optimal performance. High rates can slow convergence, while low rates yield minimal benefits. Dropout in deep neural networks (DNN) helps mitigate overfitting by randomly setting hidden neuron activations to 0. However, tuning dropout rates is necessary for optimal performance. High rates can slow convergence, while low rates offer minimal benefits. Ideally, dropout rates should be tuned separately for each layer and training stage, but in practice, a single dropout rate is often used for all layers throughout training. This fixed rate generalizes the DNN to noisy samples with a specific expected amount of perturbation. When using dropout in deep neural networks, a single dropout rate is often kept constant for all layers throughout training. This fixed rate helps generalize the DNN to noisy samples with a specific expected amount of perturbation. However, this approach may lead to too much perturbation for some layers and samples, and too little for others. Additionally, dropout is not compatible with batch normalization. The effectiveness of dropout in deep neural networks varies due to different fractions of activated neurons, leading to too much or too little perturbation for some layers and samples. Dropout is incompatible with batch normalization, as rescaling undropped neurons breaks the consistency of normalization parameters required between training and test phases. This conflict often results in dropout being omitted in favor of batch normalization in modern DNN architectures. The proposed \"jumpout\" modification aims to address the drawbacks of dropout in deep neural networks with rectified linear unit (ReLU) activations. Dropout is often omitted in favor of batch normalization (BN) due to compatibility issues, but the proposed modifications aim to improve generalization performance. The \"jumpout\" modification aims to improve generalization performance in deep neural networks with ReLU activations by randomly changing ReLU activation patterns during training, leading to better performance across different polyhedra. The \"jumpout\" modification aims to enhance generalization in deep neural networks with ReLU activations by altering activation patterns, improving performance across different polyhedra. Dropout improves generalization by training linear models to work for data points in nearby polyhedra, but with a fixed dropout rate, each model is smoothed to work at a typical distance away, potentially not achieving local smoothness. The \"jumpout\" modification improves generalization in deep neural networks with ReLU activations by altering activation patterns, increasing performance across different polyhedra. Unlike dropout with a fixed rate, jumpout samples a dropout rate from a decreasing distribution, prioritizing smaller rates. This results in a higher probability of smoothing polyhedra to nearby points as they move farther away. Additionally, jumpout ensures a consistent fraction of activated neurons across layers, samples, and training stages. The \"jumpout\" modification improves generalization in deep neural networks with ReLU activations by altering activation patterns and ensuring a consistent fraction of activated neurons across layers and training stages. It adaptsively normalizes the dropout rate for each layer and training sample, addressing incompatibility issues between dropout and BN. In jumpout, the dropout rate is adaptively normalized for consistent neural deactivation across layers and samples during training. The outputs are rescaled to maintain variance, allowing for compatibility with Batch Normalization. Jumpout randomly drops activations like dropout but can be easily implemented without extra training. Jumpout is a method that randomly generates a 0/1 mask over hidden neurons to drop activations, similar to dropout. It can be easily implemented without extra training and shows improved performance compared to dropout on various tasks. Previous approaches like \"standout\" have also addressed the fixed dropout rate problem by adapting dropout rates for different layers and training stages. Jumpout outperforms dropout on various tasks by adjusting the dropout rate based on ReLU activation patterns, unlike previous methods that rely on additional trained models. The complexity of a DNN is bounded by a function related to dropout rate vectors, and jumpout adjusts dropout rates based on ReLU activation patterns without relying on additional trained models. It introduces minimal computation and memory overhead and can easily be integrated into existing model architectures. BID19 introduced Gaussian dropout as a faster convergence optimization method, while BID9 proposed variational dropout to connect global uncertainty with adaptive dropout rates for each neuron. In this paper, various extensions and modifications to the original dropout method are discussed. These include Gaussian dropout for faster convergence, variational dropout for adaptive dropout rates, sparse dropout rates, Swapout combining dropout with random skipping connections, and Fraternal Dropout training two identical DNNs with different dropout masks. These changes aim to improve dropout performance without additional training costs. In this paper, various extensions and modifications to the original dropout method are discussed, including Gaussian dropout for faster convergence, variational dropout for adaptive rates, sparse dropout rates, Swapout combining dropout with random skipping connections, and Fraternal Dropout training two identical DNNs with different dropout masks. Jumpout introduces changes to dropout without extra training costs or additional parameters, targeting different problems and can be applied with other dropout variants. The paper discusses various extensions and modifications to the original dropout method, including Gaussian dropout for faster convergence, variational dropout for adaptive rates, sparse dropout rates, Swapout combining dropout with random skipping connections, and Fraternal Dropout training two identical DNNs with different dropout masks. Additionally, Jumpout introduces changes to dropout without extra training costs or additional parameters, targeting different problems and can be applied with other dropout variants. The study focuses on feed-forward deep neural networks and their formalization, which can generalize many DNN architectures used in practice. The DNN formalization discussed in the paper can represent various DNN architectures used in practice. Equation (1) covers DNNs with bias terms at each layer, convolution operator as matrix multiplication, average-pooling as a linear operator, and max-pooling as an activation function. Residual network blocks can be represented by appending an identity matrix to a weight matrix. The DNN formalization discussed in the paper covers various DNN architectures used in practice, including average-pooling as a linear operator and max-pooling as an activation function. Residual network blocks can be represented by appending an identity matrix to a weight matrix, allowing for the retention and addition of input values through matrix operations. Piecewise linear activation functions like ReLU result in a DNN that can be written as a piecewise linear function. The DNN in Eqn. FORMULA0 can be written as a piecewise linear function using ReLU activation. The activation pattern modifies the weight matrix, resulting in a linear model. The gradient \u2202x is the weight vector of the linear model. The DNN with ReLU activation patterns can be transformed into a linear model by setting rows with 0 activation patterns to zero vectors. This process can be extended to deeper layers, eliminating ReLU functions. The resulting linear model is associated with activation patterns on all layers, defining a convex polyhedron. ReLU units are cost-effective and widely used, especially in tasks requiring good performance. In the context of ReLU activation patterns in DNNs, the focus is on how dropout improves generalization performance by considering local linear models and nearby convex polyhedra. Three modifications to dropout lead to \"jumpout,\" enhancing DNN performance by preventing co-adaptation. The modifications to dropout, known as \"jumpout,\" prevent co-adaptation in DNNs and improve generalization performance by encouraging neuron independence and training smaller networks. Dropout smooths local linear models and acts as an ensemble during test/inference. During test/inference, dropout in DNNs acts as an ensemble of smaller networks, reducing variance. It smooths local linear models by dividing the input space into convex polyhedra, improving generalization performance by dispersing training samples among different regions. During training, large DNNs with thousands of neurons per layer can behave like linear models when the input space is divided into convex polyhedra. Each training data point may have its own distinct local linear model, and nearby polyhedra can correspond to different linear models due to variations in weight matrices and activation patterns. This can result in significant differences between linear models of different polyhedra. During training, large DNNs can behave like linear models within convex polyhedra, with each data point having its own local linear model. Variations in weight matrices and activation patterns can lead to differences between linear models of different polyhedra. To address issues with dropout, a dropout rate is sampled from a truncated half-normal distribution, improving generalization ability. To improve generalization ability, a dropout rate is sampled from a truncated half-normal distribution, ensuring the probability does not become too small or too large. This approach creates a monotone decreasing probability of dropout rates, enhancing performance. Other distributions like Beta distribution could also be explored in the future. By utilizing a Gaussian-based dropout rate distribution with a standard deviation \u03c3 as a hyper-parameter, smaller dropout rates are sampled more frequently to enhance generalization performance. This method encourages smoothness in the performance of local linear models, performing well on points in closer polyhedra but diminishing effectiveness for points farther away. The Gaussian-based dropout rate distribution with a standard deviation \u03c3 encourages smoothness in the performance of local linear models, performing well on points in closer polyhedra but diminishing effectiveness for points farther away. Tuning the dropout rates of different layers separately is ideal for improving network performance, but setting a single global dropout rate is a common approach due to computational constraints. Setting a single global dropout rate is a common approach due to computational constraints, but it is suboptimal as the proportion of active neurons in each layer varies significantly. The effective dropout rate of every layer is determined by the fraction of active neurons in that layer. The effective dropout rate of every layer is normalized by the fraction of active neurons in that layer to better control dropout behavior. This allows for more consistent activation pattern changes and precise tuning of dropout rates as a single hyper-parameter. In standard dropout, neurons are scaled by 1/p during training and kept unchanged during the test phase. The changed activation pattern is more consistent with the original pattern, allowing for precise tuning of dropout rates as a single hyper-parameter. The scaling factor in standard dropout keeps the mean of neurons the same between training and test phases, but the variance can differ significantly, leading to unpredictable behavior in deep neural networks when combined with batch normalization layers. The variance can differ significantly between training and test phases in deep neural networks when combined with batch normalization layers. One possible setting involves combining dropout layers with BN layers to address this issue. When dropout is applied to a neuron after ReLU, its value is treated as a random variable with probabilities of being 1 or 0. The neuron's value then gets multiplied by a weight matrix entry and contributes to the next layer. Dropout affects the mean and variance of neurons during training, but these parameters are not scaled during testing, leading to inconsistency in batch normalization. During training, dropout changes the scales of mean and variance of neurons. This inconsistency with batch normalization during testing can be fixed by rescaling the output and dropped neurons. Rescaling factor depends on dropout rate and the weight matrix. During training, dropout changes the scales of mean and variance of neurons. The rescaling factor for recovering the scale of the variance should be (1 \u2212 p) \u22120.5 if E(y) is small. However, computing information about the weight matrix of the following layer requires additional computation and memory cost. Simple scaling methods cannot resolve the shift in both mean and variance caused by dropout. The rescaling factors of (1 \u2212 p) \u22121, (1 \u2212 p) \u22120.5, and (1 \u2212 p) \u22120.75 are applied to y when p = 0.2 in the CIFAR10(s) network. During training, dropout affects the mean and variance scales of neurons. Rescaling factors like (1 \u2212 p) \u22121, (1 \u2212 p) \u22120.5, and (1 \u2212 p) \u22120.75 are used for y in the CIFAR10(s) network. The rescaling factor of (1 \u2212 p) \u22120.75 shows a good balance between mean and variance rescaling. During training, rescaling factors like (1 \u2212 p) \u22121, (1 \u2212 p) \u22120.5, and (1 \u2212 p) \u22120.75 are used for y in the CIFAR10(s) network to balance mean and variance rescaling. The rescaling factor of (1 \u2212 p) \u22120.75 ensures consistency in both mean and variance for dropout and non-dropout cases. During training, rescaling factors like (1 \u2212 p) \u22121, (1 \u2212 p) \u22120.5, and (1 \u2212 p) \u22120.75 are used for y in the CIFAR10(s) network to balance mean and variance rescaling. The rescaling factor of (1 \u2212 p) \u22120.75 ensures consistency in both mean and variance for dropout and non-dropout cases. In the proposed method, a trade-off point of (1 \u2212 p) \u22120.75 is used to make the mean and variance consistent when using dropout. Comparisons between original dropout and dropout with the rescaling factor show potential performance improvements, especially when used with Batch Normalization in convolutional networks. The proposed \"Jumpout\" layer for DNN with ReLU combines modifications to improve dropout performance, showing potential improvements when used with Batch Normalization in convolutional networks. Using rescaling factors like (1 \u2212 p) \u22120.75 can lead to consistent mean and variance for dropout and non-dropout cases. Comparisons show that larger dropout rates may result in performance improvements, with the best results seen when using the improved dropout method. Our proposed \"Jumpout\" layer for DNN with ReLU improves dropout performance by rescaling factors and adapting the dropout rate based on the number of active neurons. Jumpout samples from a decreasing distribution for a random dropout rate, enforcing consistent regularization and scaling outputs by (1 \u2212 p) \u22120.75. Jumpout improves dropout performance by adapting the dropout rate based on active neurons. It scales outputs by (1 \u2212 p) \u22120.75 and requires three hyperparameters, with \u03c3 being the main one. The auxiliary truncation hyperparameters (p min , p max ) bound samples from the half-normal distribution. In practice, p min = 0.01 and p max = 0.6 work consistently well. Jumpout has three hyperparameters, with \u03c3 being the main one. The auxiliary truncation hyperparameters (p min , p max ) bound samples from the half-normal distribution. In practice, p min = 0.01 and p max = 0.6 work consistently well. The input h j is considered to be the features of layer j corresponding to one data point. For a mini-batch of data points, the average q + j over data points is used as the estimate for the mini-batch. Jumpout has almost the same memory cost as the original dropout. In practice, the average q + j over data points is used as the estimate for the mini-batch in Jumpout, which has similar memory cost as original dropout. Jumpout requires minimal computation for counting active neurons and sampling from the distribution. Dropout and Jumpout are applied to various DNN architectures for performance comparison on benchmark datasets. In this section, dropout and jumpout are applied to different DNN architectures for performance comparison on six benchmark datasets of varying scales. The architectures include small CNNs, WideResNet, ResNet, and pre-activation ResNet models applied to datasets like CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet. Standard settings, data preprocessing/augmentation, and hyperparameters are followed for CIFAR and Fashion-MNIST experiments, while pre-trained ResNet18 models are used for ImageNet experiments. For ImageNet experiments, pre-trained ResNet18 models are used and trained with dropout and jumpout for the same number of epochs to address overfitting. Starting from pre-trained models with high training accuracy helps prevent overfitting issues. Experimental results in TAB1 show that jumpout consistently performs well. The experimental results in TAB1 demonstrate that jumpout consistently outperforms dropout on all datasets and DNNs tested, including Fashion-MNIST and CIFAR10. Even on datasets with high test accuracy, such as CIFAR100 and ImageNet, jumpout shows significant improvements without the need to increase model size. This highlights the effectiveness and advantages of jumpout over traditional dropout methods. Jumpout demonstrates superior performance over traditional dropout methods on various datasets and DNNs, including CIFAR100 and ImageNet, without the need for increasing model size. A thorough ablation study confirms the effectiveness of jumpout's modifications, with the combination of all three modifications achieving the best results. Learning curves and convergence plots further illustrate the advantages of jumpout over dropout. Jumpout demonstrates superior performance over traditional dropout methods on various datasets and DNNs, including CIFAR100 and ImageNet, without the need for increasing model size. The combination of three modifications in jumpout achieves the best results. Learning curves and convergence plots show jumpout's advantages over dropout, with adaptive dropout rates per minibatch leading to faster and more accurate learning. Further improvements may be possible with a better learning rate schedule specifically for jumpout. The final performance of dropout rescaling factors applied to y in CIFAR10(s) is shown in plots, with (1 \u2212 p) \u22120.75 giving nice trade-offs between mean and variance rescaling."
}