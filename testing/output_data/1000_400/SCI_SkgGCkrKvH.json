{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models is crucial for data privacy and on-device learning over networks. Communication compression is proposed to overcome network bandwidth limitations in decentralized training. Choco-SGD achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. The algorithm's practical performance is demonstrated in training deep learning models on decentralized user devices and in datacenters. Distributed machine learning has enabled successful applications in research and industry by offering computational scalability. Distributed machine learning enables successful applications by leveraging computational scalability and data-locality. Recent theoretical results show decentralized schemes can be as efficient as centralized approaches in training deep learning models. Gradient compression techniques have been proposed for standard distributed training. Recent theoretical results suggest that decentralized schemes can be as efficient as centralized approaches in training deep learning models. Gradient compression techniques have been proposed for distributed training to reduce data sent over communication links. Tang et al. (2018) introduced DCD and ECD algorithms for decentralized training, but they have limitations in compression operators and ratios. CHOCO-SGD is a recent focus for study. In response to limitations in existing algorithms for decentralized training of deep neural networks, CHOCO-SGD is studied for its ability to overcome these constraints and achieve high compression ratios. The evaluation focuses on generalization performance on standard machine learning benchmarks, departing from previous works that mainly considered training performance. Two scenarios are studied, including training in a challenging peer-to-peer setting with distributed training data. CHOCO-SGD is evaluated for decentralized training of deep neural networks, showing speed-ups over decentralized baselines with less communication overhead. It improves scalability in datacenter settings and can enhance time-to-accuracy on large tasks like ImageNet training. CHOCO-SGD is a communication efficient algorithm for decentralized training of deep neural networks, showing improvements in scalability in datacenter settings and faster time-to-accuracy on large tasks like ImageNet training. However, decentralized schemes face challenges when scaling to a larger number of nodes, often not reaching the same performance as centralized schemes. This highlights the need for further research on decentralized training schemes that can scale effectively to a large number of peers. Reporting results on decentralized training schemes is valuable for spurring further research. CHOCO-SGD converges at a rate of O(1/\u221anT + n/(\u03c1^4\u03b4^2T)) on non-convex smooth functions, with n nodes, T iterations, \u03c1 spectral gap, and \u03b4 compression ratio. The main term matches centralized baselines with linear speedup in worker numbers. A version with momentum is analyzed for practical performance on on-device training over a peer-to-peer social network. The decentralized training scheme CHOCO-SGD shows linear speedup with the number of workers. Practical performance is analyzed for on-device training over a social network and in a datacenter setting for training deep learning models. Challenges faced by current decentralized learning approaches are also highlighted. The curr_chunk discusses the challenges faced by current decentralized learning approaches and various methods proposed for training in communication restricted settings, including decentralized schemes, gradient compression, asynchronous methods, and multiple local SGD steps. The paper advocates for combining these approaches. In this paper, the focus is on combining decentralized SGD schemes with gradient compression, particularly emphasizing approaches based on gossip averaging. The convergence rate of these methods depends on the spectral gap of the mixing matrix. Previous work has shown convergence rates consistent with centralized mini-batch SGD, with the spectral gap affecting the asymptotic behavior. The convergence rate of decentralized SGD schemes with gradient compression depends on the spectral gap of the mixing matrix. Recent studies have shown that gossip averaging combined with SGD achieves a convergence rate of O(1/\u221anT + n/(\u03c1^2T)), with the spectral gap mainly affecting smaller terms. Quantization, particularly communication compression with quantization, has gained popularity in deep learning, with theoretical guarantees established for both unbiased and biased compression schemes. Error correction schemes have shown to work best in practice. Decentralized optimization with quantization has been a topic of interest in the deep learning community, with studies showing that gossip averaging combined with SGD may not converge correctly in the presence of quantization noise. Recent research has also explored proximal updates and variance reduction in conjunction with quantized updates. The CHOCO-SGD algorithm can handle high compression rates and was introduced by Koloskova et al. It converges at a faster rate than other algorithms, even with increasing compression accuracy. Tang et al. proposed DCD and ECD algorithms for deep learning applications, which converge at the same rate as the centralized baseline but only for a constant compression ratio. The CHOCO-SGD algorithm introduced by Koloskova et al. can handle high compression rates and achieves faster convergence compared to other algorithms. Tang et al. proposed DCD and ECD algorithms for deep learning applications, which converge at the same rate as the centralized baseline but only for a constant compression ratio. The decentralized optimization problem is formally introduced along with the CHOCO-SGD algorithm by Koloskova et al. It involves distributed optimization across n nodes with local data distributions and communication limited to local neighbors in a weighted graph. Distributed optimization across n nodes with local data distributions and limited communication to local neighbors in a weighted graph. Compression of messages through quantization or sparsification is aimed for in the algorithm. In distributed optimization across n nodes in a weighted graph, weights are set based on local node degrees for efficient computation. Compression of messages using compression operators is emphasized to transmit only compressed data. CHOCO-SGD algorithm is outlined in Algorithm 1 for implementation. The CHOCO-SGD algorithm, outlined in Algorithm 1, involves each worker storing a private variable updated by stochastic gradient and gossip averaging steps. Communication between nodes is done using compressed updates, preserving averages even with quantization noise. The CHOCO-SGD algorithm involves nodes communicating with neighbors using compressed updates to update variables, which are publicly available copies of private variables. Communication and gradient computation can be executed in parallel, with each node needing to store at most 3 vectors. The CHOCO-SGD algorithm allows for parallel execution of communication and gradient computation, with each node storing a maximum of 3 vectors. Additionally, a momentum-version of CHOCO-SGD is proposed in Algorithm 2, extending the analysis to non-convex problems. Technical assumptions are made regarding bounded variance of stochastic gradients, leading to asymptotic convergence of CHOCO-SGD. The CHOCO-SGD algorithm converges asymptotically with bounded variance of stochastic gradients. It shows linear speed-up compared to SGD on a single node, with compression and graph topology affecting higher order terms. Experimentally compared to baselines, momentum is leveraged in all algorithms, including the newly developed momentum version of CHOCO-SGD. The newly developed momentum version of CHOCO-SGD, Algorithm 2, is compared experimentally to baselines for compression operators. Momentum is leveraged in all algorithms, with additional factors such as weight decay and local momentum memory. The experiments are conducted using a ring topology with 8 nodes and training the ResNet20 architecture on the Cifar10 dataset. For experiments, a ring topology with 8 nodes is used to train ResNet20 on the Cifar10 dataset. Various algorithms with momentum are implemented, adjusting learning rates and decay factors during training. Compression schemes are applied to every layer of ResNet20 separately, with two unbiased schemes implemented: quantization that rounds weights to b-bit representations and random sparsification. The top-1 test accuracy is evaluated on each node separately over the dataset, with the average performance reported over all nodes. Hyper-parameter tuning is conducted for algorithms with momentum, adjusting learning rates and decay factors during training. Compression schemes are applied to every layer of ResNet20 separately, with various unbiased and biased compression schemes implemented. These include quantization, random sparsification, top fraction selection, and sign compression. The top-1 test accuracy is evaluated on each node separately over the dataset, with the average performance reported over all nodes. Results from the experiments show that unbiased compression schemes like ECD and DCD perform well only at low compression ratios, sometimes failing at high ratios. This contrasts with biased schemes like CHOCO-SGD and DeepSqueeze, which have only been studied with biased schemes but can be scaled down to meet specifications. Results show that unbiased compression schemes like ECD and DCD perform well at low compression ratios but struggle at high ratios. In contrast, biased schemes like CHOCO-SGD show better performance, with state-of-the-art accuracy and significantly reduced bits per weight compared to full precision baseline. CHOCO-SGD, a biased compression scheme, achieves state-of-the-art accuracy with significantly fewer bits per weight compared to full precision baseline. It can generalize well in all scenarios and is particularly effective in challenging decentralized real-world scenarios where each device has access only to local data and communication bandwidth is limited. In decentralized scenarios, sensor networks, mobile devices, or hospitals jointly train a machine learning model with limited communication bandwidth and unknown global network topology. Privacy is maintained by keeping training data private on each device. Training data is permanently split between nodes, and each node has a distinct part of the dataset. This scenario has not been studied before for decentralized settings. In decentralized scenarios, training data is permanently split between nodes, with each node having a distinct part of the dataset. A study on decentralized deep learning compared CHOCO-SGD with sign compression, decentralized SGD without compression, and a centralized baseline where all nodes route their updates to a central coordinator for aggregation. The comparison focused on the best testing accuracy reached after 300 epochs and after communicating 1000 MB. In decentralized deep learning, CHOCO-SGD with sign compression was compared to decentralized SGD without compression and centralized SGD. The study focused on scaling properties, comparing algorithms on different network topologies and tuning learning rates. Results are summarized in Figure 1. The study compared CHOCO-SGD with sign compression to decentralized SGD without compression and centralized SGD on different network topologies. Results showed that CHOCO-SGD slowed down due to graph topology and communication compression, leading to slower convergence but not a generalization issue. The study compared CHOCO-SGD with sign compression to decentralized SGD without compression and centralized SGD on different network topologies. Results showed that CHOCO-SGD slowed down due to graph topology and communication compression, leading to slower convergence but not a generalization issue. Increasing epochs improved decentralized scheme performance, but closing the gap between centralized and decentralized algorithms remained a challenge. The focus in real decentralized scenarios is on reducing communication to minimize user data costs, with CHOCO-SGD performing the best in testing accuracy. In real decentralized scenarios, the focus is on reducing communication to minimize user data costs. CHOCO-SGD performs the best in testing accuracy, with slight degradation as the number of nodes increases. Torus topology is beneficial for large networks due to good mixing properties, while there is not much difference for small networks. Decentralized and Centralized SGD require significantly more bits to reach reasonable accuracy. Experiments on a Real Social Network Graph involve training models on user devices connected by a social network. The chosen network has 32 nodes, and models include ResNet20 for image classification and a three-layer LSTM for language modeling on WikiText-2. The experiments involved training models on a real social network graph with 32 nodes. The models used were ResNet20 for image classification and a three-layer LSTM for language modeling on WikiText-2. The results showed that the decentralized algorithm performed best in terms of training accuracy, followed by the centralized and quantized decentralized. However, the centralized scheme had the highest test accuracy. CHOCO-SGD outperformed the exact decentralized scheme in terms of test accuracy for the same transmitted data. The decentralized algorithm outperforms the centralized and quantized decentralized in training accuracy. However, the centralized scheme achieves the highest test accuracy. CHOCO-SGD surpasses the exact decentralized scheme in test accuracy for the same transmitted data. CHOCO-SGD outperforms centralized SGD in test perplexity, especially with fixed data volume. The benefits of CHOCO-SGD are more pronounced when scaling to more nodes in decentralized optimization methods. Decentralized schemes can outperform centralized ones, even in well-connected environments like datacenters with fast connections. Decentralized optimization methods, like the one described by Lian et al. (2017) and Assran et al. (2019), offer solutions for scaling issues in well-connected environments. Assran et al. (2019) showed significant speedups in training on 256 GPUs with access to all training data. Their algorithm differs from CHOCO-SGD with asynchronous gossip updates and changing communication topology. These properties can be combined with our contribution. Our experiments involve training ImageNet-1k with Resnet-50 on 8 machines. The setup for the experiments involved training ImageNet-1k with Resnet-50 on 8 machines using decentralized communication with compressed communication in a ring topology. Each machine had 4 Tesla P100 GPUs, and mini-batch size on each GPU was 128. The hyperparameters for CHOCO-SGD were directly taken from a previous study. The study utilized decentralized communication with compressed communication (sign-CHOCO-SGD) in a ring topology for training. The mini-batch size on each GPU was 128, and the hyperparameters for CHOCO-SGD were adopted from a previous study. Results showed that CHOCO-SGD outperformed all-reduce in terms of time efficiency, with a slight 1.5% accuracy loss. The approach could potentially be integrated with other schemes for improved training. Our study demonstrates a 20% time gain over the common all-reduce baseline using CHOCO-SGD for decentralized deep learning training in bandwidth-constrained environments. The algorithm shows theoretical convergence guarantees and linear speedup in the number of nodes, with empirical performance on image classification tasks. Decentralized deep learning training in bandwidth-constrained environments shows theoretical convergence guarantees and linear speedup in the number of nodes. The algorithm's performance is empirically studied on image classification tasks and language modeling, enabling training in communication-restricted environments while respecting data locality constraints. Our main contribution is enabling training in strongly communication-restricted environments while respecting data locality constraints. The proof of Theorem 4.1 is presented, derived from a more general statement in Theorem A.2, following a structure similar to Koloskova et al. (2019). The algorithm consists of stochastic gradient updates. Theorem A.2 analyzes CHOCO-SGD for arbitrary stepsizes \u03b7 and derives Theorem 4.1 as a special case. The proof structure follows Koloskova et al. (2019), showing Algorithm 1 as a special case of a more general class of algorithms. Linear convergence is achieved for algorithms with stochastic gradient updates followed by an averaging step. CHOCO-SGD's specific averaging scheme demonstrates linear convergence, as shown in (Koloskova et al., 2019). The decentralized SGD algorithm with arbitrary averaging schemes is discussed, with a focus on linear convergence rates. The use of matrix notation and assumptions for preserving iterates' averages are highlighted, along with an example of exact averaging for consensus algorithms. The text discusses decentralized SGD algorithms with linear convergence rates, focusing on exact averaging for consensus algorithms. It mentions the D-PSGD algorithm and CHOCO-SGD, highlighting the importance of the order of communication and gradient computation parts. The text discusses the consensus averaging scheme in decentralized SGD algorithms, emphasizing the independence of communication and gradient computation parts. The order of these parts can be exchanged without affecting convergence rate. The proof of Theorem 4.1 shows that Algorithm 3 with a constant stepsize satisfies certain conditions for convergence. Proof of Lemma A.1 shows that Algorithm 3 with a constant stepsize \u03b7 satisfies certain conditions for convergence. The rate of convergence for CHOCO-SGD with CHOCO-GOSSIP averaging scheme is discussed, with a dependence on the eigengap \u03c1. The convergence rate for CHOCO-SGD with CHOCO-GOSSIP averaging scheme is discussed, with a worse dependence on the eigengap \u03c1 compared to exact averaging. The proof technique and support for high compression may affect this dependence. The theorem provides guarantees for the averaged parameter vector x in a decentralized setting, but it can be costly. The theorem provides guarantees for the averaged parameter vector x in a decentralized setting, but it can be costly to average all parameters across machines. Similar guarantees can be obtained for individual iterates x_i. Theorem A.4 discusses the convergence of Algorithm 3 in a decentralized setting with constant step sizes. The convergence rate is determined by i and c, and holds for any T. However, the rate is affected by the difference in values between \u03c3 2 and G 2. The convergence rate of Algorithm 3 in a decentralized setting with constant step sizes is discussed in Theorem A.4. The rate holds for any T, but is impacted by the difference in values between \u03c3 2 and G 2. Proof of Theorem A.4 involves L-smoothness and bounding terms, with Corollary A.5 stating convergence of local weights x (t) i under certain assumptions. Lemma B.1 and B.3 provide inequalities for vectors and matrices in Frobenius norm. Algorithm 4 CHOCO-SGD (Koloskova et al., 2019) is introduced as Error Feedback with mixing matrix W. Algorithm 4 CHOCO-SGD (Koloskova et al., 2019) combines error feedback with weight decay and momentum in a decentralized setting. Nesterov momentum can be adapted accordingly. The algorithm saves quantization errors into internal memory for compressed value addition. CHOCO-SGD is an error feedback algorithm in a decentralized setting where quantization errors are saved into internal memory and added to the compressed value at the next iteration. The algorithm transmits the difference in local variable evolution before compressing the value, updating the internal memory, and fine-tuning hyperparameters for model training. In the comparison of different optimization methods, CHOCO-SGD with sign compression is considered alongside decentralized SGD and centralized SGD without compression. Two models are trained: ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. The study compares CHOCO-SGD with sign compression to decentralized SGD and centralized SGD without compression. Two models are trained: ResNet20 for image classification on Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. The LSTM has 28.95 million parameters and uses a three-layer setup with a hidden dimension size of 650. The training involves gradient clipping (0.4) and dropout (0.4) applied only on the output of LSTM. Both models are trained for 300 epochs with a per node mini-batch size of 32. The learning rate of CHOCO-SGD follows a linear scaling rule proportional to the node degree. The BPTT length is set to 30. Gradient clipping (0.4) and dropout (0.4) are fine-tuned for LSTM output. ResNet20 and LSTM are trained for 300 epochs with a mini-batch size of 32 per node. CHOCO-SGD's learning rate scales linearly with node degree. Momentum (0.9) is applied only to ResNet20. Learning rate is warmed up from 0.1 to the initial value over 5 epochs. Initial learning rate is decayed by 10 at 50% and 75% of training epochs. Learning rate per sample\u03b7 is optimized using a linear scaling rule. During training, the initial learning rate is fine-tuned and decayed by a factor of 10 at 50% and 75% of epochs. The optimal learning rate per sample\u03b7 is determined using a linear scaling rule. Grid search is used to find the best performance, adjusting grid points if needed. Tables show the fine-tuned hyperparameters for training ResNet-20 on Cifar10 and LSTM. During training, the learning rate is adjusted and decayed based on epochs. Grid search is used to optimize performance by adjusting grid points. Tables display hyperparameters for training ResNet-20 on Cifar10 and LSTM, as well as for a social network topology with 32 nodes. Learning curves are plotted for the social network topology. The training process involves fixed partitions for nodes, with a mini-batch size of 32 and maximum node degree of 14. Learning rate is adjusted based on node degree. Learning curves are plotted for a social network topology with 32 nodes, where each node accesses a subset of the dataset. Test accuracy and model consensus are depicted in figures. In the datacenter experiment, test accuracy and top-5 accuracy are shown in Figure 8. Figure 9 displays the test accuracy of the averaged model and the distance of local models from the averaged model. Local models reach a consensus towards the end of optimization, with their test performances matching that of the averaged model. Prior to epoch 225, local models diverge from the averaged model, only converging when the stepsize decreases. This behavior was also observed in a previous study by Assran et al. (2019)."
}