{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, and algorithms and convergence proofs are provided for geodesically convex objectives in the case of a product of Riemannian manifolds. The generalization allows adaptivity across manifolds in the cartesian product, with experimentally faster convergence shown. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when optimizing a large number of parameters. Riemannian adaptive methods show faster convergence and lower train loss values compared to standard algorithms, as demonstrated in embedding the WordNet taxonomy in the Poincare ball. This generalization allows adaptivity across manifolds in the cartesian product, with experimentally proven benefits. The development of powerful gradient-based optimization algorithms is crucial for various applications, especially when dealing with a large number of parameters. Recent advancements have led to the creation of successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD. While these algorithms are designed for parameters in a Euclidean space, there is a growing interest in optimizing parameters on a Riemannian manifold, allowing for non-Euclidean geometries. AMSGrad is an algorithm designed for optimizing parameters in a Euclidean space. However, there is a growing interest in optimizing parameters on a Riemannian manifold, allowing for non-Euclidean geometries. This family of algorithms has various applications, including solving Lyapunov equations, matrix factorization, geometric programming, dictionary learning, and hyperbolic taxonomy embedding. In this work, the focus is on generalizing adaptive optimization methods to Riemannian manifolds, which poses challenges due to the lack of intrinsic coordinate systems. The goal is to extend these algorithms to the most agnostic Riemannian setting in an intrinsic manner, addressing issues related to adaptivity and coordinate-wise updates. In this work, the authors explain the challenges of generalizing adaptive optimization methods to Riemannian manifolds without intrinsic coordinate systems. They propose generalizations of the algorithms for a product of manifolds and empirically support their claims with hyperbolic taxonomy embedding tasks. Their initial motivation was to develop Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces. The authors developed Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces, specifically focusing on hyperbolic taxonomy embedding tasks. They believe that the absence of Riemannian adaptive algorithms could hinder the development of competitive optimization-based Riemannian embedding methods in hyperbolic spaces. The recent rise of embedding methods in hyperbolic spaces could benefit from developments in competitive optimization-based Riemannian embedding methods. A manifold M of dimension n is a space that can locally be approximated by a Euclidean space R n, with tangent space T x M defined at each point x \u2208 M as a first-order local approximation. A Riemannian manifold is a space where a Riemannian metric defines the geometry locally, with tangent spaces at each point. The metric induces a global distance function on the manifold, allowing for the calculation of lengths of paths between points. A Riemannian manifold is defined by a Riemannian metric \u03c1, inducing a global distance function on the manifold. Riemannian SGD involves updating along the shortest path using the exponential map. Riemannian SGD involves updating along the shortest path using the exponential map or a retraction map when the former is not known in closed-form. ADAGRAD is an algorithm that rescales updates coordinate-wise based on past gradients, which can be beneficial in sparse gradient scenarios or deep networks. The ADAGRAD algorithm rescales updates based on past gradients, while the ADAM update rule includes momentum and adaptivity terms. The ADAM algorithm combines momentum and adaptivity terms, with the momentum term often leading to significant improvements. BID18 identified a mistake in the convergence proof of ADAM and proposed the AMSGRAD modification or using a time-dependent schedule for \u03b2 2 in ADAMNC. AMSGRAD and ADAMNC were proposed as modifications to the ADAM algorithm by BID18, who identified a mistake in its convergence proof. These modifications involve using an increasing schedule for \u03b2 2 or introducing intrinsic updates on a Riemannian manifold. The formalism requires working with local coordinate systems, known as charts. The RSGD update of Eq. FORMULA1 is intrinsic to the Riemannian manifold, involving exp and grad, which are intrinsic objects. The formalism requires working with local coordinate systems called charts. The gradient of a smooth function on a manifold can be defined intrinsically, but its Hessian is only defined at critical points. The RSGD update is intrinsic as it involves exp and grad, intrinsic objects on the manifold. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free manner. One possible approach is to fix a canonical coordinate system in the tangent space at the initialization point and parallel-transport it along the optimization trajectory. The text discusses the parallel transport on a Riemannian manifold during optimization, highlighting how curvature affects the process and breaks the sparsity of gradients, leading to a loss of adaptivity in optimizing different features at different speeds. Component 5 breaks the sparsity of gradients and the benefit of adaptivity. The interpretation of adaptivity as optimizing different features at different speeds is lost due to the coordinate system used for gradients. The theorems' techniques do not apply to updates defined in Eq. (6). The structure assumes the cartesian product of n Riemannian manifolds. The induced distance function on M is given by the exponential, log map, and parallel transport in each M i. The induced distance function on M is given by the exponential, log map, and parallel transport in each component M i. Designing adaptive schemes in a general Riemannian manifold is challenging due to the absence of intrinsic coordinates. Each component x i in x can be seen as a \"coordinate\", allowing for a simple adaptation of Eq. (3) with Riemannian norms in the adaptivity term. ADAGRAD, ADAM, and AMSGRAD were briefly discussed in section 2. In a general Riemannian manifold, designing adaptive schemes is challenging due to the absence of intrinsic coordinates. ADAM is a combination of ADAGRAD with momentum, while AMSGRAD corrects the convergence proof of ADAM. ADAMNC is a variant of ADAM with a non-constant schedule for parameters \u03b21 and \u03b22. The schedule proposed by BID18 for \u03b22 in ADAMNC allows it to recover the sum of squared-gradients of ADAGRAD. ADAMNC is a modification of ADAM with a non-constant schedule for \u03b21 and \u03b22. The schedule proposed by BID18 for \u03b22 in ADAMNC allows it to recover the sum of squared-gradients of ADAGRAD. The assumptions and notations involve geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03bai \u2264 0. The product manifold of these manifolds is denoted as (M, \u03c1), with X being the set of feasible parameters. The projection operator \u03a0Xi(x) minimizes the distance between y and x in X. Riemannian AMSGRAD is presented in FIG1, with convergence guarantees bounding the regret after T rounds. The algorithm assumes geodesically convex functions on compact sets X i with bounded diameters and gradient constraints. Riemannian AMSGRAD is presented in FIG1 with convergence guarantees bounding regret after T rounds. The algorithm assumes geodesically convex functions on compact sets X i with bounded diameters and gradient constraints. The convergence guarantee for RAMSGRAD is presented in Theorem 1, with comparisons to RADAM and ADAM algorithms. The convergence guarantee for RAMSGRAD is presented in Theorem 1, with comparisons to AMSGRAD and RADAMNC. The regret bound worsens at a speed of approximately 1 + D \u221e |\u03ba|/6 when the curvature is small but non-zero. Theorem 2 shows the convergence guarantee for RADAMNC, with a convergence proof for RADAGRAD when \u03b2 1 := 0. The convergence guarantees for RAMSGRAD and RADAMNC are shown in Theorems 1 and 2 respectively. The role of convexity is highlighted, with a comparison between convexity and geodesic convexity in the theorems. The role of convexity in the convergence guarantees for RAMSGRAD and RADAMNC is discussed, with a comparison between convexity and geodesic convexity. Regret bounds for convex objectives are obtained by bounding the difference between function values, while in the Riemannian case, this term becomes \u03c1 xt (g t , \u2212 log xt (x * )). The cosine law is used to obtain a bound on the difference between current and optimal solutions. The role of convexity in convergence guarantees for RAMSGRAD and RADAMNC is discussed, comparing convexity with geodesic convexity. Regret bounds for convex objectives involve bounding the difference between function values, while in the Riemannian case, this term becomes \u03c1 xt (g t , \u2212 log xt (x * )). The cosine law is used to bound the difference between current and optimal solutions, with a focus on bounding each g t , x t \u2212 x * using a well-chosen decreasing schedule for \u03b1. In Riemannian manifolds, the analogue lemma 6 is generalized for all Alexandrov spaces, including geodesically convex subsets with lower bounded sectional curvature. The curvature dependent quantity \u03b6 from Eq. (10) helps bound \u03c1. Sparse gradients lead to improved bounds, especially when updating only a few words at a time. The choice of \u03d5 does not affect convergence theorems, suggesting potential for better regret bounds by utilizing momentum/acceleration. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed compared to non-adaptive RSGD method. The WordNet noun hierarchy is embedded in the n-dimensional Poincar\u00e9 model of hyperbolic geometry, known for better tree-like graph embedding. Each word is embedded in a space of constant curvature \u22121. The optimization tools could benefit algorithms proposed in previous studies. The Poincar\u00e9 model D n of hyperbolic geometry is used to embed tree-like graphs, with each word in the same space of constant curvature \u22121. The choice of this model is justified by the access to closed form expressions for optimization tools. Key features include rescaled Euclidean gradients, distance functions, geodesics, exponential and logarithmic maps, and parallel transport along unique geodesics. The transitive closure of the WordNet taxonomy graph consists of 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in D n to minimize distances between connected words. Loss function is similar to log-likelihood with sampling of negative word pairs. Edge direction is not considered in the loss function. In the WordNet taxonomy graph, words are embedded in D n to minimize distances between connected words. The loss function used is similar to log-likelihood with sampling of negative word pairs. Edge direction is not considered in the loss function. The training focuses on 5-dimensional hyperbolic spaces and includes a \"burn-in phase.\" The evaluation includes measuring representation capacity and generalization through link prediction on a validation set of 2% edges from the transitive closure edges. We use the same settings as BID15 for training in 5-dimensional hyperbolic spaces. During the \"burn-in phase,\" negative words are sampled based on graph degree raised at power 0.75, improving all metrics. RADAM was found to yield slightly better results than RAMS-GRAD for optimization. During optimization, negatives are uniformly sampled, with n = 5 following BID15. RADAM outperformed RAMS-GRAD, showing convergence to lower loss values when using a first-order approximation of the exponential map. Results for \"exponential\" and \"retraction\" based methods are shown in FIG2. During optimization, negatives are uniformly sampled, with n = 5 following BID15. RADAM outperformed RAMS-GRAD, showing convergence to lower loss values when using a first-order approximation of the exponential map. Results for \"exponential\" and \"retraction\" based methods are shown in FIG2. The need for fewer steps and smaller gradients to escape sub-optimal points on the ball border of D n compared to fully Riemannian methods is highlighted. Retraction-based methods are reported separately as they are not directly comparable to their fully Riemannian analogues. Various learning rates were tested, with RADAM consistently achieving the lowest training loss. RADAM consistently achieves the lowest training loss and outperforms other methods on the MAP metric for both reconstruction and link prediction settings in the full Riemannian setting. In the \"retraction\" setting, RADAM reaches the lowest training loss value and performs on par with RSGD on the MAP evaluation. RAMSGRAD converges faster in terms of MAP for the link prediction task, indicating better generalization capability. After introducing Riemannian SGD by BID1, various first-order Riemannian methods emerged, including Riemannian SVRG, Riemannian Stein variational gradient descent, and Riemannian accelerated gradient descent. Stochastic gradient Langevin dynamics was also generalized to optimize on the probability simplex. Additionally, Riemannian counterparts of SGD with momentum and RMSprop were proposed by BID20, suggesting the transportation of the momentum term using parallel translation. In the geodesically convex case, Riemannian counterparts of SGD with momentum and RMSprop were proposed by BID20, suggesting the transportation of the momentum term using parallel translation. However, their algorithm compromises convergence guarantees by performing coordinate-wise adaptive operations in the tangent space. Another version of Riemannian ADAM for the Grassmann manifold G(1, n) was introduced by BID3, removing the adaptive component and making the adaptivity term a scalar. The text discusses the proposal to generalize adaptive optimization tools for Riemannian manifolds and the lack of adaptivity across manifolds in previous algorithms. The new approach aims to improve convergence rates and outperform non-adaptive methods in tasks like hyperbolic word taxonomy embedding. The text proposes generalizing adaptive optimization tools for Cartesian products of Riemannian manifolds to improve convergence rates and outperform non-adaptive methods in tasks like hyperbolic word taxonomy embedding. The text discusses the application of Cauchy-Schwarz' and Young's inequalities to improve convergence rates in gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. The text presents Lemmas 6, 7, and 8, focusing on inequalities in Alexandrov spaces and their application in optimization algorithms for geodesically convex functions. Lemmas 6 and 7 involve cosine and Cauchy-Schwarz inequalities, while Lemma 8 provides a general inequality for non-negative real numbers. These lemmas are crucial for proving convergence in optimization algorithms like ADAMNC."
}