{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology constructs positive definite kernels from dissimilarity measures on structured inputs like time series, strings, histograms, and graphs. It utilizes random feature maps to build a kernel specified by the distance measure, improving generalizability over universal Nearest-Neighbor estimates. The D2KE methodology proposes using random objects to create a feature embedding for each instance, showing better generalizability than universal Nearest-Neighbor estimates. It subsumes the representative-set method and relates to the distance substitution kernel, while also generalizing Random Features methods to handle complex structured inputs. Our proposed framework for classification experiments compares favorably to existing distance-based learning methods in terms of testing accuracy and computational time across various domains such as time series, strings, and histograms for texts and images. It is often easier to specify a dissimilarity function between instances than to construct a feature representation, especially for structured inputs like real-valued time series or discrete structures such as strings, histograms, and graphs. There are well-developed dissimilarity measures available for complex structured inputs, such as Dynamic Time Warping for time series. Standard machine learning methods are designed for vector representations, but there has been less focus on distance-based methods for structured inputs. Nearest-Neighbor Estimation (NNE) is a common distance-based method that predicts outcomes by averaging nearest neighbors in the input space using dissimilarity measures like Dynamic Time Warping for time series. Distance-based methods for structured inputs have received less attention in machine learning. Nearest-Neighbor Estimation (NNE) is a common method that predicts outcomes by averaging nearest neighbors using dissimilarity measures. However, NNE can be unreliable with high variance when neighbors are far apart, especially in high-dimensional spaces. To address this issue, research has focused on developing global distance-based machine learning methods, drawing upon kernel methods or learning with similarity functions. Research has focused on developing global distance-based machine learning methods by utilizing kernel methods or learning with similarity functions. One approach involves treating the data similarity matrix as a kernel Gram matrix and applying standard kernel-based methods like Support Vector Machines. However, the challenge lies in most similarity measures not providing a positive-definite kernel, leading to non-convex problems. To address this, efforts have been made to estimate a positive-definite Gram matrix that approximates the similarity matrix. A key challenge in global distance-based machine learning methods is the lack of positive-definite kernels in similarity measures, leading to non-convex problems. Efforts have been made to estimate a positive-definite Gram matrix that approximates the similarity matrix, but modifications often result in a loss of information and inconsistency between training and testing samples. The proposed framework, D2KE, aims to address the lack of positive-definite kernels in similarity measures by constructing a family of PD kernels from a dissimilarity measure on structured inputs. This approach offers a more general and richer family of kernels compared to selecting a subset of training samples as a representative set. The D2KE framework proposes constructing a family of positive-definite kernels from a dissimilarity measure on structured inputs. This approach creates novel kernels specifically designed for a given distance measure, improving generalization properties compared to nearest-neighbor estimation. The D2KE framework constructs kernels from a distance measure on structured inputs, improving generalization properties compared to nearest-neighbor estimation. It provides a feature embedding for each instance, enhancing classification and regression models' performance in various domains. Our proposed framework constructs a family of PD kernels via Random Features from a given distance measure for structured inputs, outperforming existing distance-based learning methods in testing accuracy and computational time. It generalizes Random Features methods to complex structured inputs, accelerating kernel machines across a broad range of applications. The framework accelerates kernel machines on structured inputs like time-series and strings using Random Features methods. Existing approaches for Distance-Based Kernel Learning have limitations, but the proposed framework outperforms them in testing accuracy and computational time. The curr_chunk discusses methods for obtaining a positive-definite (PD) kernel through transformations of the distance measure or by finding a Euclidean embedding approximating the dissimilarity matrix. It also mentions a theoretical foundation for an SVM solver in Krein spaces. The curr_chunk discusses specific approaches for building a positive-definite (PD) kernel on structured inputs such as text and time-series by modifying a distance function into a kernel. This type of kernel can result in a diagonal-dominance problem due to the summation over a large number of alignments with a sample itself. Random feature maps have been used to approximate non-linear kernel machines, reducing training and testing times for kernel-based learning algorithms. Various explicit nonlinear random feature maps have been developed for different types of kernels, including Gaussian and Laplacian Kernels, intersection kernels, additive kernels, dot product kernels, and semigroup kernels. Among these, the Random Fourier Features (RFF) method approximates a Gaussian Kernel function efficiently. Numerous explicit nonlinear random feature maps have been constructed for various types of kernels, including Gaussian and Laplacian Kernels, intersection kernels, additive kernels, dot product kernels, and semigroup kernels. The Random Fourier Features (RFF) method approximates a Gaussian Kernel function by multiplying the input with a Gaussian random matrix. Methods have been proposed to accelerate RFF on high-dimensional input data matrices using structured matrices for faster computation and less memory consumption. D2KE is a method that takes structured inputs of different sizes and computes the Random Fourier (RF) features with a structured distance metric. Unlike existing RF methods, D2KE constructs a new PD kernel through a random feature map, making it computationally feasible. The differences between D2KE and existing RF methods are listed in table 1. D2KE constructs a new PD kernel through a random feature map, making it computationally feasible. A recent method, BID49, is limited to single-variable real-valued time-series and cannot handle structured inputs like strings, histograms, and graphs. In contrast, D2KE offers a unified framework for various structured inputs and provides a theoretical analysis on KNN and distance-based kernel methods for estimating target functions. The unified framework of D2KE goes beyond the limitations of BID49 by handling various structured inputs like strings, histograms, and graphs. It provides a theoretical analysis on KNN and distance-based kernel methods for estimating target functions from samples. The dissimilarity measure between input objects is used instead of a feature representation. The dissimilarity measure d(x1, x2) between input objects is crucial for learning a target function f(x). It should be a metric and satisfy certain properties to ensure accurate learning. The ideal feature representation for the learning task should be compact and result in a simple function of the representation. The size of structured inputs may vary widely, such as strings with variable lengths or graphs with different sizes. The dissimilarity measure d(x1, x2) for learning a target function f(x) should be compact and result in a simple function. A small dissimilarity between objects implies a small difference in the function. The target function should have a small Lipschitz-continuity constant with respect to the dissimilarity measure. The target function should have a small Lipschitz-continuity constant with respect to the dissimilarity measure. The covering number N(\u03b4; X, d) measures the size of the space implied by the dissimilarity measure. This affects the estimation error of a Nearest-Neighbor Estimator in structured input spaces. The effective dimension in structured input spaces X with distance measure d and covering number N(\u03b4; X, d) impacts the estimation error of a Nearest-Neighbor Estimator. An example is provided for measuring the space of Multiset, where a ground distance measures the distance between elements. The Hausdorff Distance and covering number N(\u03b4; V, \u2206) of sets in X are also discussed. Equipped with the concept of effective dimension, the estimation error of a k-Nearest-Neighbor estimate of f(x) can be bounded. For a target function f with variance bounded by \u03c3^2, the k-Nearest Neighbor estimate f_n constructed from a training set of size n satisfies a certain inequality. By minimizing this inequality with respect to the parameter k, a proof similar to standard analysis can be obtained. The estimation error of the k-Nearest Neighbor estimate of the target function f can be bounded by minimizing a certain inequality with respect to the parameter k. The proof is similar to standard analysis, with the space partition number replaced by the covering number and dimension replaced by the effective dimension. When the space dimension is large, the estimation error decreases slowly with the number of samples, requiring exponential scaling in the dimension. An estimator based on a RKHS derived from the distance measure offers better sample complexity for problems with higher effective dimension. The curr_chunk introduces a method called D2KE that constructs positive-definite kernels from a given distance measure. It involves a family of kernels derived from a random structured object \u03c9, with a distribution p(\u03c9) over \u2126 and a feature map \u03c6 \u03c9 (x) based on the distance of x to all \u03c9 \u2208 \u2126. This approach aims to convert a distance measure into a positive definite kernel. The D2KE method constructs positive-definite kernels from a given distance measure by using a family of kernels derived from a random structured object \u03c9. The kernel is parameterized by p(\u03c9) and \u03b3, and can be interpreted as a soft version of the distance substitution kernel. The kernel in Equation FORMULA13 is a soft version of the distance substitution kernel, defined by p(\u03c9) and \u03b3. When \u03b3 \u2192 \u221e, the value of Equation FORMULA15 is determined by min \u03c9 \u2208\u2126 d(x, \u03c9) + d(\u03c9, y), which equals d(x, y) if X \u2286 \u2126. Unlike the distance-substitution kernel, our kernel is always positive-definite by construction. Random Feature Approximation is used to evaluate the kernel in Equation FORMULA12. The kernel in Equation FORMULA13 is always positive-definite by construction. Random Feature Approximation is used to approximate the kernel, allowing for its use in large-scale settings with a large number of samples. This approach is particularly useful when standard kernel methods are no longer efficient enough. The RF approximation allows for the use of the kernel in large-scale settings with a large number of samples. A target function can be learned as a linear function of the RF feature map by minimizing domain-specific empirical risk. This approach is efficient when standard kernel methods are no longer effective. Our D2KE approach uses RF based empirical risk minimization for developing a supervised method. The random feature embeddings are computed using a structured distance measure and exponent function parameterized by \u03b3, unlike traditional RF methods. The estimator is analyzed in Section 5 and compared to K-nearest-neighbor. The approach is related to the representative-set method (RSM) through a naive choice of p(\u03c9). The estimator in Algorithm 1 in Section 5 uses a matrix multiplication with a random Gaussian matrix to convert the input matrix into an embedding matrix. This approach is related to the representative-set method (RSM) by setting \u2126 = X and p(\u03c9) = p(x), resulting in a kernel Equation (4) that depends on the data distribution. A Random-Feature approximation to the kernel can be obtained by creating an R-dimensional feature embedding using a part of the training data as samples from p(\u03c9). This is equivalent to a 1/ \u221a R-scaled version of the embedding function in the representative-set method. The Random-Feature approximation method in Algorithm 1 creates an R-dimensional feature embedding using a part of the training data as samples from p(\u03c9). This approach provides a generalization error bound even as R approaches infinity, unlike the representative-set method. The choice of p(\u03c9) significantly impacts the kernel performance, with \"close to uniform\" distributions yielding better results in various domains. In contrast to the representative-set method, the Random-Feature approximation method allows for a larger representative set even as R approaches infinity. The choice of p(\u03c9) greatly influences kernel performance, with \"close to uniform\" distributions often outperforming data distribution choices like p(x). Examples include better performance in time-series domain with DTW using random time series distribution and in string classification with edit distance using random strings distribution. The Random-Feature approximation method outperforms the Representative-Set Method in various classification tasks. For instance, using distributions like random time series, random strings, and random sets of vectors yields significantly better performance. The synthetic nature of these distributions allows for generating unlimited random features, leading to improved results. The chosen distributions p(\u03c9) drawn uniformly from a unit sphere outperform the Representative-Set Method in classification tasks. The synthetic nature of p(\u03c9) allows for generating unlimited random features, resulting in better approximation to the exact kernel. The performance of p(\u03c9) is significantly better even with a small number of random features, capturing more relevant semantic information for estimating f(x) under the dissimilarity measure d(x, \u03c9). In this section, the proposed framework is analyzed from the perspective of error decomposition in RKHS. The population and empirical risks are denoted as L(f) and L(f) respectively, with the estimated function denoted as fR from the random feature approximation. The risk decomposition is discussed in detail, focusing on three terms from right to left. The RKHS framework is analyzed for error decomposition, with population and empirical risks denoted as L(f) and L(f) respectively. The function approximation error is discussed, emphasizing the importance of Lipschitz continuity and additional smoothness constraints. The RKHS framework analyzes error decomposition, emphasizing Lipschitz continuity and smoothness constraints for function approximation. The estimation error is discussed, with the goal of minimizing \u03bb as a function of n for better estimation accuracy. The estimation error in RKHS analysis focuses on minimizing \u03bb as a function of n for better accuracy. It has a better dependency on n (n^-1/2) compared to the k-nearest-neighbor method, especially for higher effective dimensions. Tighter bounds on D \u03bb and improved rates w.r.t. n could be achieved with a more detailed analysis, although it is challenging due to the lack of an analytic form of the kernel. The analysis focuses on minimizing \u03bb as a function of n for better accuracy, with a better dependency on n compared to the k-nearest-neighbor method. Tighter bounds on D \u03bb and improved rates w.r.t. n could be achieved with a more detailed analysis, despite the challenge of lacking an analytic form of the kernel. The analysis focuses on minimizing \u03bb as a function of n for better accuracy, with a better dependency on n compared to the k-nearest-neighbor method. Tighter bounds on D \u03bb and improved rates w.r.t. n could be achieved with a more detailed analysis, despite the challenge of lacking an analytic form of the kernel. In FORMULA3, the focus is on the second term of empirical risk, with uniform convergence and approximation error discussed. The Representer theorem is used to consider the optimal solution of empirical risk minimization, leading to a corollary for guaranteeing a certain difference in empirical risk with probability. The Representer theorem is utilized to find the optimal solution for empirical risk minimization, leading to a corollary ensuring a specific difference in empirical risk with probability. It is stated that having a number of Random Features proportional to the effective dimension can achieve an approximation error. The proposed framework can achieve suboptimal performance according to Claim 1. The proposed framework can achieve suboptimal performance by utilizing random feature approximation based ERM estimator. Claim 1 states that the target function lies close to the population risk minimizer in the RKHS spanned by the D2KE kernel. The method is evaluated in various domains such as time-series, strings, texts, and images, with comparisons among different distance-based methods. The proposed method utilizes random feature approximation based ERM estimator to achieve suboptimal performance. It is evaluated in different domains including time-series, strings, texts, and images, with comparisons among various distance-based methods. Four dissimilarity measures are chosen for the experiments: Dynamic Time Warping (DTW) for time-series, Edit Distance for strings, Earth Mover's distance for texts, and (Modified) Hausdorff distance for images. The study utilized various distance measures for different domains including time-series, strings, texts, and images. C-MEX programs were adapted or implemented for computationally demanding distance measures with quadratic complexity. Four datasets were selected for experiments, including multivariate time-series data from the UCI Machine Learning repository and string data with alphabets of varying sizes. The study used various distance measures for different data types such as time-series, strings, texts, and images. The datasets included multivariate time-series data from the UCI Machine Learning repository, string data with varying alphabet sizes, text data partially overlapped with BID26, and image data from Kaggle with SIFT descriptors. Each dataset was divided into 70/30 train and test subsets. The study compared D2KE with 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, and KSVM for classification tasks on datasets with varying characteristics. The study compared D2KE with 5 state-of-the-art baselines for classification tasks, including KNN, DSK_RBF, DSK_ND, KSVM, and RSM. D2KE has linear complexity in both the number of data samples and the length of the sequence, while the other baselines have quadratic complexity. KSVM and RSM have quadratic complexity in both data samples and sequence length, while D2KE has linear complexity in both. D2KE uses random samples for parameter optimization and achieves performance close to an exact kernel. Linear SVM is used for embedding-based methods, and LIBSVM BID5 is used for precomputed dissimilarity kernels. D2KE outperforms baseline methods in classification accuracy and computation time, showing superiority over KNN and other kernel methods. It achieves performance close to an exact kernel with linear complexity, using linear SVM for embedding-based methods and LIBSVM BID5 for precomputed dissimilarity kernels. In comparison to KNN and other methods, D2KE shows superior performance in classification accuracy and computation time. Our method outperforms DSK_RBF, DSK_ND, KSVM, and RSM, indicating that a representation induced from a positive-definite kernel utilizes data more effectively. Random objects sampled by D2KE perform significantly better, as discussed in section 4. More detailed experimental results for each domain are provided in Appendix C. In this work, a general framework is proposed for deriving a positive-definite kernel and feature embedding function from a given dissimilarity measure between input objects. The framework is especially useful for structured input domains like sequences, time-series, and sets. It subsumes existing approaches and opens up a new direction for creating embeddings of structured objects based on distance to random objects. A potential extension is to develop distance-based embeddings within a deep architecture for structured inputs. The framework proposed in this work allows for the creation of embeddings of structured objects based on distance to random objects. A potential extension is to develop distance-based embeddings within a deep architecture for structured inputs. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous with Lipschitz constant \u03b3, which is used to bound the magnitude of Hoefding's inequality for a given input pair. Hoefding's inequality is used to bound the magnitude of a given input pair. By finding a covering of X with size N, a bound is obtained that holds for all input pairs. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous with parameter \u03b3 \u2264 1. By combining equations, a result is obtained by choosing a specific value for \u03b3. The proof involves optimizing parameters using an approximate kernel and applying a theorem. In the general setup, parameters are searched for on the training set using 10-fold cross-validation. In the proof, optimization of parameters is done using an approximate kernel and a bound on \u03b1 1 /n is established. The general setup involves searching for parameters on the training set through 10-fold cross-validation, using specific methods for different kernels. Random selection is used to obtain data samples for a new method, D2KE, with the best number of samples reported in the range of [4, 4096]. The new method D2KE uses random selection to obtain data samples, with the best number reported in the range of [4, 4096]. Linear SVM is employed for embedding-based methods, while precomputed dissimilarity kernels use LIBSVM. Datasets are collected from various sources including UCI Machine Learning repository, LibSVM Data Collection, and Kaggle Datasets. Datasets for Machine Learning and Data Science research were collected from popular public websites such as UCI Machine Learning repository, LibSVM Data Collection, and Kaggle Datasets. One time-series dataset, IQ, was shared by researchers from George Mason University. Detailed properties of the datasets from four different domains are listed in TAB5. Computation was done on a DELL dual-socket system with Intel Xeon processors at 2.93GHz, 16 cores, and 250 GB of memory, running SUSE Linux. Multithreading with 12 threads was used for distance computations. Gaussian distribution with bandwidth parameter \u03c3 was found to be applicable for all datasets. DTW distance measure was used for time-series data. In all experiments, multithreading with 12 threads was used for distance computations. The Gaussian distribution with bandwidth parameter \u03c3 was applied to all datasets. D2KE outperforms other baselines in classification accuracy and computation time for multivariate time-series. It significantly outperforms KNN, especially on high-dimensional datasets like IQ_radio due to KNN's sensitivity to data noise. Our method, D2KE, outperforms KNN by 26.62% on IQ_radio due to KNN's sensitivity to data noise. Compared to other kernels like DSK_RBF and DSK_ND, our method achieves much better performance. RSM is closest to our method in feature matrix construction, but D2KE performs significantly better with random time series sampling. Our method samples short random sequences to denoise and find patterns in data, unlike RSM which selects a subset of data points leading to potential noise issues. The number of possible random sequences is unlimited, making the feature space more abundant. RSM may have high computational costs for long time-series. The method samples short random sequences to denoise and find patterns in data, with unlimited possibilities for random sequences. Computational costs for long time-series can be high. Levenshtein distance is used as the distance measure for string data. Parameters for \u03b3 and length of random strings are optimized. D2KE consistently outperforms other distance-based methods. Results show that D2KE consistently outperforms other distance-based baselines, including DSK_RBF with Levenshtein distance. The performance of D2KE is particularly strong on large datasets, achieving better results with less computation. Our method, D2KE, offers a clear advantage over baseline methods like DSK_RBF. D2KE achieves better performance on large datasets with less computation compared to other baselines. For text data, we use the earth mover's distance as our distance measure, which has shown strong performance when combined with KNN for document classifications. In text data analysis, D2KE outperforms other methods like DSK_ND and KSVM due to lower computational costs. The earth mover's distance is used as the distance measure for document classification, with Bag of Words representation and google pretrained word vectors. Random documents are generated for parameter optimization, resulting in D2KE outperforming baselines on all datasets. D2KE outperforms other baselines on all datasets by using random word vectors for parameter optimization. It achieves a significant speedup compared to other methods, particularly for datasets with a large number of documents and longer document lengths. D2KE uses random word vectors for parameter optimization, achieving a significant speedup for datasets with many documents and longer lengths. For image data, the modified Hausdorff distance is used as the distance measure between images, with SIFT-descriptors generated and compared using MHD. Random images of each SIFT-descriptor are uniformly sampled from the unit sphere of the embedding vector space. Best parameters for \u03b3 and length of random SIFT-descriptor sequence are searched in specific ranges. D2KE outperforms other baselines in parameter optimization using random word vectors for datasets with many documents. The modified Hausdorff distance is used to compare SIFT-descriptors in image data, with random images sampled from the unit sphere. D2KE performs better than KNN and RSM, despite the quadratic complexity of other methods. The quadratic complexity of DSK_RBF, DSK_ND, and KSVM makes it difficult to scale to large data due to the length of SIFT descriptor sequences. D2KE still outperforms KNN and RSM, showing it can be a strong alternative across applications."
}