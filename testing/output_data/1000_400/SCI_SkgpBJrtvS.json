{
    "title": "SkgpBJrtvS",
    "content": "Knowledge distillation is a common method for transferring representational knowledge between neural networks. However, it overlooks important structural knowledge of the teacher network. A new approach called contrastive learning aims to train a student network to capture more information from the teacher's representation of the data. This new objective outperforms knowledge distillation in various knowledge transfer tasks such as model compression, ensemble distillation, and cross-modal transfer. Contrastive learning is a new approach that aims to train a student network to capture more information from the teacher's data representation. It outperforms knowledge distillation in various knowledge transfer tasks, including model compression, ensemble distillation, and cross-modal transfer. When combined with knowledge distillation, it sets a state of the art in many transfer tasks, sometimes even surpassing the teacher network."
}