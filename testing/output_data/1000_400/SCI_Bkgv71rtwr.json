{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where the source and target domains may not share the same categories. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC), a novel approach that incorporates category-agnostic clusters in the target domain to improve domain adaptation. These clusters provide domain-specific visual cues for better alignment between source and target domains. SE-CC is a novel adaptation technique that utilizes category-agnostic clusters in the target domain to improve domain adaptation. Clustering is used to reveal the underlying data space structure specific to the target domain, enhancing the representation for both closed-set and open-set scenarios. SE-CC is a novel adaptation technique that utilizes category-agnostic clusters in the target domain to improve domain adaptation by matching assignment distribution over clusters to inherent cluster distribution for each target sample. It also enhances the representation with mutual information maximization. Extensive experiments on Office and VisDA datasets show superior results compared to state-of-the-art approaches, especially in scenarios where large quantities of annotated data are not accessible. Convolutional Neural Networks (CNNs) have advanced vision technologies, but require large amounts of annotated data for training. Domain shift occurs when transferring models to new domains, leading to performance drops. Unsupervised domain adaptation can help generalize models using labeled source samples and unlabeled target samples. Existing models often only align data distributions, limiting their applicability to closed-set scenarios. The current focus is on unsupervised domain adaptation to generalize a target model by leveraging labeled source samples and unlabeled target samples. Existing models face limitations in aligning data distributions between source and target domains, hindering their applicability in open-set scenarios. The challenge lies in distinguishing unknown target samples from known ones and learning a hybrid network for both closed-set and open-set domain adaptation. The difficulty of open-set domain adaptation lies in distinguishing unknown target samples from known ones and learning a hybrid network for both closed-set and open-set domain adaptation. One approach is to use an additional binary classifier to assign known/unknown labels to target samples, discarding unknown samples during adaptation. However, this method may not fully exploit the data structure when target sample distributions are diverse or semantic labels are ambiguous. In open-set domain adaptation, distinguishing unknown target samples from known ones is challenging. One approach is to use a binary classifier to discard outliers during adaptation, but this may not fully exploit the data structure when target sample distributions are diverse or semantic labels are ambiguous. To address this, clustering is performed on all unlabeled target samples to explicitly model the diverse semantics of both known and unknown classes in the target domain. This approach aims to create domain-invariant representations for known classes by steering domain adaptation with category-agnostic clusters. In open-set domain adaptation, clustering is used to model the diverse semantics of known and unknown classes in the target domain. The approach aims to create domain-invariant representations for known classes by steering domain adaptation with category-agnostic clusters. This is achieved by decomposing target samples into clusters and refining representations with an additional clustering branch. The new Self-Ensembling with Category-agnostic Clusters (SE-CC) method is introduced to preserve the inherent structure of the target domain. The new Self-Ensembling with Category-agnostic Clusters (SE-CC) method utilizes clustering to decompose target samples into category-agnostic clusters. It refines representations by predicting cluster assignment distributions and minimizing the mismatch with inherent cluster distributions using KL-divergence. The SE-CC method integrates clustering to predict cluster assignment distributions for target samples, minimizing mismatch with inherent clusters using KL-divergence. It enforces feature preservation in the target domain and maximizes mutual information among input features, output distributions, and cluster assignments to enhance representation. The framework is jointly optimized for unsupervised domain adaptation in CNNs. The SE-CC framework integrates clustering to predict cluster assignments for target samples, minimizing domain discrepancy through Maximum Mean Discrepancy (MMD) in CNNs for unsupervised domain adaptation. Different approaches like incorporating a residual transfer module and using a domain discriminator for domain confusion have been explored in the field. Incorporating a residual transfer module and domain discriminator for domain confusion in unsupervised domain adaptation. Open-set domain adaptation addresses scenarios with new and unknown classes in the target domain. Panareda Busto & Gall (2017) made early attempts in this area. The gradient reversal algorithm is used to optimize the domain discriminator in open-set domain adaptation. This approach tackles scenarios with new and unknown classes in the target domain. Previous studies by Panareda Busto & Gall (2017) and Saito et al. (2018b) have explored methods to handle the realistic open-set scenario by exploiting target sample assignments and utilizing adversarial training to separate unknown classes. Baktashmotlagh et al. (2019) have also worked on factorizing the source and target data into shared and private subspaces. Saito et al. (2018b) and Baktashmotlagh et al. (2019) use adversarial training and factorization to separate target samples of unknown class and model target samples from known classes in open-set domain adaptation. Clustering is then applied to decompose unlabeled target samples into category-agnostic clusters for Self-Ensembling to align classification predictions between teacher and student models. In open-set domain adaptation, clustering is used to decompose unlabeled target samples into category-agnostic clusters. This clustering approach is integrated into Self-Ensembling to align classification predictions between teacher and student models. Additionally, a clustering branch is added to the student model to infer cluster assignments for each target sample, enforcing the preservation of the target domain's data structure. The feature representation of the student model is further enhanced by maximizing mutual information among its feature map, classification, and cluster assignment. SE-CC utilizes unlabeled target samples for learning task-specific classifiers in the open-set scenario by leveraging category-agnostic clusters for representation learning. The feature representation is enforced to preserve the underlying data structure in the target domain, enabling effective alignment of sample distributions. In the open-set scenario, SE-CC uses category-agnostic clusters for representation learning, preserving the target data structure during domain adaptation. This enables effective alignment of sample distributions and discrimination between known and unknown classes. The preservation is further utilized to enhance representation learning by maximizing mutual information among input features, clusters, and class probability distributions. This approach has not been fully explored in previous studies. In open-set domain adaptation, the SE-CC model integrates category-agnostic clusters to enhance representation learning by maximizing mutual information among input features, clusters, and class probability distributions. This approach aligns sample distributions and discriminates between known and unknown classes, improving adaptation in both closed-set and open-set scenarios. In open-set domain adaptation, the goal is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown target samples. The Self-Ensembling method is based on the Mean Teacher model and aims to encourage consistency in learning. The Self-Ensembling method, based on the Mean Teacher model, aims to encourage consistent classification predictions between teacher and student models under small perturbations of input images. It penalizes the difference in classification predictions to distinguish unknown target samples from known ones. The Self-Ensembling method encourages consistent classification predictions between teacher and student models by penalizing the difference in classification predictions. It utilizes self-ensembling loss and unsupervised conditional entropy loss during training to drive decision boundaries away from high-density regions in the target domain. The Self-Ensembling method uses unsupervised conditional entropy loss to train the student model for domain adaptation, with a focus on driving decision boundaries away from high-density regions in the target domain. It incorporates supervised cross entropy loss on source data, self-ensembling loss, and conditional entropy loss on unlabeled target data, balanced with tradeoff parameters. The method addresses the challenge of open-set domain adaptation by classifying both inliers and outliers into known and unknown classes. The method addresses the challenge of open-set domain adaptation by classifying both inliers and outliers into known and unknown classes. To improve robustness, clustering is used to model diverse semantics in the target domain, creating category-agnostic clusters integrated into Self-Ensembling for guidance in domain adaptation. To address open-set domain adaptation challenges, clustering is utilized to model diverse semantics in the target domain. Category-agnostic clusters are integrated into Self-Ensembling to guide domain adaptation by aligning cluster assignment distribution with inherent cluster distribution. This ensures domain-invariant feature representations for known classes and more discriminative features for unknown and known classes in the target domain. In the target domain, clustering using k-means is employed to group unlabeled data, revealing underlying structure tailored to the domain. Target samples with similar semantics are grouped together, utilizing features from pre-trained CNNs on ImageNet for clustering. Periodic cluster refreshing did not significantly impact the results. In the target domain, clustering is done using k-means to group unlabeled data based on similar semantics. Target samples are represented by output features of pre-trained CNNs on ImageNet for clustering. The inherent cluster distribution of each target sample is measured through cosine similarities with cluster centroids. The clustering branch in the student model predicts the distribution over category-agnostic clusters for cluster assignment of target samples. It infers cluster assignment distribution via a modified softmax layer based on input features. The clustering branch in the student model predicts cluster assignment distribution for target samples using a modified softmax layer. It is trained with a KL-divergence loss to minimize the mismatch between estimated and inherent cluster distributions. The clustering branch in the student model is trained with a KL-divergence loss to minimize the mismatch between estimated and inherent cluster distributions. The KL-divergence loss enforces the learnt representation to preserve the data structure of the target domain, making it more discriminative for both unknown and known classes. Additionally, inter-cluster relationships are incorporated into the loss as a constraint to maintain the inherent relations among cluster assignment parameter matrices. The goal is to ensure that the cluster assignment parameter matrices of semantically similar clusters are similar. The KL-divergence loss with inter-cluster relationships constraint is used to preserve inherent relations among cluster assignment parameter matrices. Mutual Information Maximization is employed to strengthen target features in an unsupervised manner by maximizing mutual information among input features and output distributions. In a multi-task paradigm, Mutual Information Maximization (MIM) is leveraged in the student model to maximize mutual information among input features and output distributions. A MIM module is designed to estimate and maximize local and global mutual information among input feature maps, output classification distributions, and cluster assignment distributions. Global Mutual Information is encoded from the output feature map of the last convolutional layer in the student model. The output feature map of the last convolutional layer in the student model is encoded into a global feature vector G(x) through a convolutional layer and average pooling. This global feature vector is concatenated with the classification and cluster assignment distributions and fed into the global Mutual Information discriminator for alignment assessment. The global Mutual Information discriminator assesses alignment by discriminating input features based on classification and cluster assignment distributions. It uses a three-layer fully-connected network with a softplus function and Jensen-Shannon MI estimator for estimation. Additionally, local Mutual Information among local input features is exploited. The global Mutual Information is estimated using a softplus function and Jensen-Shannon MI estimator, assessing alignment based on classification and cluster assignment distributions. Local Mutual Information is also utilized by spatially replicating distributions and concatenating them with input features for discrimination. The local Mutual Information discriminator is constructed with three stacked convolutional layers and nonlinear activation. The final output score map denotes the probability of discriminating real input local features with matched classification and cluster assignment distributions. The final objective for the MIM module combines local and global Mutual Information estimations with a tradeoff parameter \u03b1. The SE-CC training objective integrates various losses including cross entropy, self-ensembling, conditional entropy, and KL-divergence. It also includes local and global Mutual Information estimations with tradeoff parameters. Experimental validation was conducted on the Office Saenko et al. VisDA dataset. The SE-CC training objective integrates various losses including self-ensembling, conditional entropy, KL-divergence, and Mutual Information estimation on target data. Experimental validation was conducted on the Office Saenko et al. VisDA dataset, which consists of synthetic-real image transfer tasks across three domains. The testing domain includes video frames in YTBB Real et al. (2017). Ground truth of testing set not publicly available, synthetic images in training domain used as source, COCO images in validation domain used as target for evaluation. Open-set adaptation follows Peng et al. (2018) with 12 known classes for source & target domains, 33 background classes as unknown in source, and 69 COCO categories as unknown in target. Known-to-unknown ratio in target domain set as 1:10. Three metrics - Knwn, Mean, Overall - adopted for evaluation. Knwn: accuracy over known classes, Mean: accuracy over known & unknown classes, Overall: accuracy over all target samples. Closed-set adaptation reports accuracy of all 12 classes. For open-set domain adaptation on Office, different models' performances are compared in Table 1. AODA uses a unique open-set setting without unknown source samples. To compare fairly, a variant of SE-CC (SE-CC \u2666) is included, which learns the classifier without unknown source samples. ResNet152 is used as the backbone for CNNs in both closed-set and open-set scenarios. Our SE-CC model outperforms other state-of-the-art closed-set and open-set adaptation models on Office dataset, showing improved classification accuracy especially in challenging scenarios. Our SE-CC model demonstrates superior performance compared to other closed-set and open-set adaptation models on the Office dataset. It excels in classification accuracy, particularly in challenging scenarios, by leveraging category-agnostic clusters for domain adaptation. This approach ensures domain invariance for known classes while effectively distinguishing target samples from known and unknown classes. The results highlight the importance of aligning data distributions between the source and target domains for improved performance. Our SE-CC model outperforms other open-set adaptation techniques by utilizing category-agnostic clusters to ensure domain invariance for known classes and effectively segregate target samples from known and unknown classes. This approach aligns data distributions between source and target domains, leading to superior performance in domain adaptation. Our SE-CC model, utilizing category-agnostic clusters, outperforms other open-set adaptation techniques by ensuring domain invariance for known classes and segregating target samples effectively. It achieves better performance in domain adaptation compared to AODA, ATI-\u03bb, and FRODA, which exclude unknown target samples. Additionally, SE-CC shows superior performance in closed-set domain adaptation on Office and VisDA datasets, demonstrating the advantage of exploiting underlying data distributions. The SE-CC model demonstrates superior performance in closed-set domain adaptation on Office and VisDA datasets by exploiting the underlying data structure in the target domain through category-agnostic clusters. Ablation studies show how Conditional Entropy and KL-divergence Loss contribute to the overall performance of SE-CC. The SE-CC model shows superior performance in closed-set domain adaptation on Office and VisDA datasets by utilizing category-agnostic clusters. Conditional Entropy, KL-divergence Loss, and Mutual Information Maximization enhance the classifier's decision boundaries, refine features, and maximize mutual information for downstream tasks. Table 5 illustrates performance improvements on VisDA with different designs in SE-CC for open-set domain adaptation. CE is a general method to enhance the classifier for the target domain, regardless of domain adaptation architectures. The SE-CC model utilizes category-agnostic clusters for domain adaptation, showing a performance boost of 4.2% in Mean metric. CE improves Mean accuracy from 65.2% to 66.3%, while KL and MIM contribute 3.0% and 1.2% respectively. The results validate the effectiveness of exploiting target data structure and mutual information for open-set adaptation. The SE-CC model utilizes category-agnostic clusters for domain adaptation, leading to a 4.2% performance boost in Mean metric. It separates unknown target samples from known ones and integrates category-agnostic clusters into Self-Ensembling for improved adaptation in open-set and closed-set scenarios. The SE-CC model integrates category-agnostic clusters into Self-Ensembling for domain adaptation. It aligns cluster assignment distribution with inherent cluster distribution, preserving data structure. Mutual information among input features, classification, and clustering outputs enhances learned features. Experiments on Office and VisDA show performance improvements over state-of-the-art techniques. Detailed frameworks for global and local mutual information estimation are illustrated in Figure 3. The SE-CC model integrates category-agnostic clusters into Self-Ensembling for domain adaptation, aligning cluster assignment distribution with inherent cluster distribution. Experiments on Office and VisDA demonstrate performance improvements over state-of-the-art techniques. Detailed frameworks for global and local mutual information estimation are provided, with implementation in PyTorch and optimization with SGD. The SE-CC model incorporates category-agnostic clusters into Self-Ensembling for domain adaptation, improving performance on Office and VisDA datasets. The settings for cluster number, tradeoff parameters, and hyperparameters are detailed for open-set and closed-set adaptation tasks. Evaluation of the Clustering Branch compares the use of KL-divergence in SE-CC with L1 and L2 distance. In the study, the hyper-parameter search for each transfer is limited to specific ranges. The evaluation of the Clustering Branch shows that KL-divergence outperforms L1 and L2 distance measures. Different variants of the Mutual Information Maximization module are also evaluated. In the study, different variants of the Mutual Information Maximization (MIM) module in the SE-CC are evaluated by estimating mutual information between input features and various outputs. CLS, CLU, and CLS+CLU improve performance by exploiting mutual information between input features and the outputs of classification and clustering branches. CLS+CLU shows the largest performance boost by combining outputs from both branches for mutual information estimation, demonstrating the benefit of exploiting mutual information among input features and combined outputs of downstream tasks. The study evaluates different variants of the Mutual Information Maximization (MIM) module in SE-CC by estimating mutual information between input features and outputs. CLS, CLU, and CLS+CLU improve performance by exploiting mutual information between input features and classification/clustering outputs. CLS+CLU shows the largest performance boost by combining outputs from both branches for mutual information estimation, highlighting the benefit of exploiting mutual information among input features and combined outputs of downstream tasks. SE-CC separates unknown target samples from known ones, preserving the underlying target data structure for both classes. SE-CC separates unknown target samples from known ones, preserving the underlying target data structure for both classes, making it difficult to recognize unknown target samples with ambiguous semantics."
}