{
    "title": "S1e-0kBYPB",
    "content": "To ensure public acceptance of AI systems, we need methods to explain decisions of black-box models like neural networks. Current explanatory methods face issues with different perspectives leading to varied explanations and lack of validation on complex models. A verification framework for explanatory methods is introduced to address these challenges. The verification framework introduced aims to evaluate explanatory methods for neural networks under the feature-selection perspective. It is based on a non-trivial neural network architecture trained on a real-world task, providing guarantees on its inner workings. The framework exposes the failure modes of current explainers and offers a publicly available evaluation tool for explanations based on feature selection. The framework aims to validate explanatory methods for neural networks from a feature-selection perspective. Various post-hoc explanatory methods have been developed to shed light on black-box machine learning models, with two widely used perspectives on explanations: feature-additivity and feature-selection. The framework exposes the failure modes of current explainers and provides a publicly available evaluation tool for explanations based on feature selection. The text discusses two widely used perspectives on explanations in machine learning models: feature-additivity and feature-selection. These perspectives lead to fundamentally different explanations, especially when explaining predictions on a single input. The comparison of explanatory methods adhering to different perspectives may not be coherent due to their different explanation targets. The text compares different perspectives on explanations in machine learning models, highlighting the challenges of coherent comparisons between feature-additivity and feature-selection explainers. It also raises questions about the reliability of explanatory methods when the target model has less dramatic biases due to the unknown decision-making process of neural networks. The text discusses the challenges of evaluating explanatory methods in machine learning models when the target model has less dramatic biases. It questions the reliability of explainers when applied to complex neural networks trained on real-world datasets. In the context of evaluating explanatory methods in machine learning models, P\u00f6rner et al. (2018) propose a framework to generate evaluation tests for explanatory methods under the feature-selection perspective. This framework aims to address the issue of penalizing explainers for pointing to tokens that may not be significant in predicting the model's output. The framework proposed aims to generate evaluation tests for explanatory methods in machine learning models. It involves identifying tokens with zero contribution and those relevant to the model's prediction, testing if explainers rank them correctly. The framework was applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis. It is noted that the test is not sufficient to fully assess explainers' effectiveness due to unknown ground-truth behavior of the target models. The framework introduced focuses on generating evaluation tests for explainers in machine learning models. It tests the explainers' ability to correctly rank tokens with zero contribution and those relevant to the model's prediction. The framework was applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis, providing guarantees on the behavior of the target model for testing explainers. This approach offers a non-speculative evaluation test that penalizes explainers only for guaranteed errors. The framework introduced focuses on generating evaluation tests for explainers in machine learning models, guaranteeing the behavior of the target model for testing explainers. LIME and SHAP perform better than L2X on the evaluation test designed for feature-selection explanatory methods. The comparison between LIME and SHAP against L2X shows that the former explainers generally perform better. The evaluation test designed for feature-selection explanatory methods highlights potential failures in explainers, such as predicting relevant tokens with zero contribution. The methodology for this evaluation will be released for community use in testing future explanatory methods. Our findings show that explainers may predict relevant tokens with zero contribution. A generic evaluation methodology for explanatory methods will be released for community use. Feature-based explainers provide signed weights for input features, with two major types of explanations: feature-additive and another type. Feature-based explainers provide explanations in two major types: feature-additive, which assigns signed weights to input features, and feature-selective, which identifies a subset of features responsible for the prediction. Other types of explanations include example-based, which identifies relevant instances in the training set, and human-level explanations that mimic human reasoning. In this work, the focus is on verifying feature-based explainers, which are the majority of current works. The challenge lies in thoroughly validating their faithfulness to the target model, with four common types of evaluations performed. These evaluations typically involve interpretable target models like linear regression, decision trees, and support vector representations. Despite various evaluation methods proposed, validating the faithfulness of feature-based explainers to complex neural networks remains an open question. Evaluations commonly involve interpretable target models such as linear regression, decision trees, and synthetic setups with controlled feature sets. However, the simplicity of these models may not fully represent the intricacies of practical neural networks. Synthetic setups for evaluation involve creating controlled tasks with specific features. For example, L2X was tested on tasks like 2-dim XOR and nonlinear additive models. Another approach assumes certain heuristics followed by high-performing models, like sentiment analysis relying on adjectives and adverbs. Crowd-sourcing is often used to verify if explainer features align with model predictions. The evaluation of explainers' faithfulness to target models is not reliable, as neural networks may rely on unexpected artifacts despite high accuracy. Another evaluation involves humans predicting model behavior based on explanations provided by different explainers. The evaluation of explainers' faithfulness to target models is unreliable due to unexpected artifacts in neural networks. A different evaluation involves humans predicting model behavior based on explanations from different explainers, which can be costly and labor-intensive. Our automatic evaluation method offers a more efficient alternative for assessing model explanations. Our evaluation framework provides a fully automatic and more efficient alternative for assessing model explanations compared to human-based methods. It focuses on the fidelity of the explainer to the target model, challenging current explanatory methods. Explanatory methods face a more challenging test when applied to real data rather than randomized data. They adhere to the perspective of feature-additivity, where explanations consist of contributions from each feature to approximate the prediction. Various methods follow this perspective, such as LIME and methods unified by Lundberg & Lee. The Shapley values from game theory provide feature contributions in explanatory methods, as shown by Lundberg & Lee. These values are computed by averaging contributions over a neighborhood of the instance. The contribution of each feature in an instance is averaged over a neighborhood of the instance, with the choice of neighborhood being critical. Different perspectives on feature selection for model explanation are discussed by various authors. Perspective 2 focuses on feature selection for model explanation, aiming to identify a small subset of features that lead to similar predictions as the original model. This approach is followed by Chen et al. (2018), Carter et al. (2018), and Ribeiro et al. (2018). L2X (Chen et al., 2018) maximizes mutual information between the subset of features and the prediction. However, it assumes the number of important features per instance is known, which is often not the case. This perspective may not always hold true, as models may rely on all features instead of a subset. In practice, it is not always true that models rely only on a subset of features. A hypothetical sentiment analysis regression model is used to illustrate the differences between perspectives. Real-world neural networks may heavily rely on specific tokens in the input, despite biases in datasets. Real-world neural networks may heavily rely on specific tokens in the input, despite biases in datasets. For example, natural language inference neural networks trained on SNLI may heavily rely on specific tokens like \"outdoors\" for entailment, \"tall\" for neutral, and \"sleeping\" for contradiction. Feature-additive explanation shows the relevance of certain features in the model's decision-making process. The feature-additive explanation highlights the importance of specific features in the model's decision-making process, showing how different features contribute to the overall score. This perspective provides an average explanation of the model's behavior in a neighborhood of instances. The feature-selective explanation perspective focuses on the specific features used by the model on individual instances, revealing differences in feature importance rankings. This perspective offers insights into the model's behavior, with potential real-world applications. The proposed verification framework leverages the RCNN model architecture for validating instance-wise explanations. The proposed verification framework leverages the RCNN model architecture to validate instance-wise explanations by identifying irrelevant and relevant features for each datapoint. Metrics are introduced to measure the accuracy of explainers in ranking these features. The RCNN model consists of a generator and an encoder, both utilizing recurrent convolutional architecture. The RCNN model, consisting of a generator and an encoder, utilizes recurrent convolutional neural networks. The generator selects a subset of tokens from the input text and passes it to the encoder for making predictions. There is no direct supervision on subset selection, with training based on final prediction. Regularizers were also used to encourage model performance. The RCNN model uses a generator to select a subset of tokens from the input text, which is then passed to the encoder for making predictions. Training is based on the final prediction, with regularizers used to improve model performance. The generator is trained jointly with the encoder, with supervision only on the final prediction. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability. The RCNN model uses a generator to select tokens for predictions, with gradients estimated using a REINFORCE-style procedure. The model may learn an internal communication protocol called a handshake, where non-selected tokens contribute information. The goal is to eliminate handshakes by ensuring non-selected tokens have zero contribution to predictions. The goal is to eliminate handshakes in the RCNN model by ensuring non-selected tokens have zero contribution to predictions. The model selects tokens based on importance, with the feature \"good\" being significant. The proof of this concept is shown in Figure 2. Equation 7 captures the handshake concept in the model by assigning a score of 0.5 to non-selected tokens. The dataset is pruned to retain instances where non-selected tokens have no contribution to predictions, ensuring relevance of selected features. After pruning the dataset to retain only instances where non-selected tokens have no contribution to predictions, further pruning is done to ensure at least one selected token is clearly relevant for the prediction. This step aims to differentiate between noise and zero-contribution features in the dataset. After pruning the dataset to remove instances with no contribution from non-selected tokens, further pruning is done to ensure that at least one selected token is clearly relevant for the prediction. This step helps differentiate between noise and zero-contribution features in the dataset by checking the absolute change in prediction when a token is removed. Selected tokens are then partitioned into clearly relevant tokens (SR x) and tokens for which relevance is uncertain (SDK x). The dataset is pruned to retain instances where at least one selected token significantly impacts the prediction, distinguishing between relevant and uncertain tokens. This ensures that tokens with high contribution are ranked higher than those with zero contribution. Our procedure ensures that tokens with high contribution are ranked higher than those with zero contribution. Evaluation metrics focus on explainers providing a ranking over features, guaranteeing that all tokens in N x are ranked lower than any token in SR x. The first most important token has to be in S x. The procedure guarantees that tokens in N x are ranked lower than those in SR x. Error metrics evaluate explainers ranking features, ensuring the most important token is in S x. Error metrics include instances where important tokens are not selected or ranked incorrectly. In this work, the explainer's performance is evaluated using three metrics: (A) percentage of times the most relevant token is incorrectly identified, (B) percentage of instances with errors in the explanation, and (C) average number of irrelevant tokens ranked higher than relevant ones. The evaluation is done on the RCNN model trained on the BeerAdvocate corpus. The RCNN model trained on the BeerAdvocate corpus is evaluated using three metrics: (A) identifying the most relevant token incorrectly, (B) instances with errors in the explanation, and (C) irrelevant tokens ranked higher than relevant ones. The model predicts ratings for appearance, aroma, and palate aspects independently. The RCNN model predicts ratings for appearance, aroma, and palate aspects independently using three separate RCNNs. A threshold of \u03c4 = 0.1 is chosen to identify clearly relevant tokens, with statistics provided in Appendix A such as average review lengths and token counts. The study uses a threshold of 0.1 to identify significant changes in predictions. Various statistics of the datasets are provided in Appendix A, including average review lengths and token counts. The evaluation includes testing three popular explainers: LIME, SHAP, and L2X, with default settings from their original repositories. The evaluation test includes three popular explainers: LIME, SHAP, and L2X. Default settings were used from their original repositories, with some modifications for L2X. LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature-selection explainer. In the evaluation, LIME and SHAP performed better than L2X on most metrics, even though L2X is a feature-selection explainer. L2X's limitation lies in the need to know the number of important features per instance, which is usually unknown in practice. In real-world testing, L2X used the average number of tokens highlighted by human annotators as K, obtaining averages of 23, 18, and 13 for different aspects. Evaluation showed that LIME and SHAP outperformed L2X on most metrics, with LIME and L2X prone to ranking zero-contribution tokens as most relevant features. In real-world testing, L2X used the average number of tokens highlighted by human annotators as K, obtaining averages of 23, 18, and 13 for different aspects. Evaluation showed that LIME and SHAP outperformed L2X on most metrics, with LIME and L2X prone to ranking zero-contribution tokens as most relevant features. The explainers' rankings on an instance from the palate aspect in the evaluation dataset show discrepancies in the placement of zero-contribution tokens compared to clearly relevant features. In the evaluation dataset, the explainers LIME and SHAP tend to attribute importance to nonselected tokens, ranking them as most important. The heatmap displays the ranking of tokens, with discrepancies in the placement of zero-contribution tokens compared to clearly relevant features. The evaluation dataset shows that explainers like LIME and SHAP tend to prioritize nonselected tokens as important, with discrepancies in token rankings. L2X, on the other hand, highlights different key tokens like \"taste\", \"great\", \"mouthfeel\", and \"lacing\". This distinction between explanation perspectives is crucial, and an evaluation test for post-hoc explanatory methods has been introduced in this work. In this work, a distinction between two explanation perspectives is highlighted, and an evaluation test for post-hoc explanatory methods is introduced. The framework offers guarantees on the behavior of real-world neural networks, showcasing error rates for popular explanatory methods. The methodology is adaptable to various tasks and areas beyond natural language processing. The methodology presented is generic and can be applied to various tasks beyond natural language processing. It involves training a neural network to make predictions based on selected features, while evaluating the contribution of non-selected features. The evaluation provides insights into the limitations of post-hoc explainers. The evaluation methodology is generic and can be applied to various tasks beyond natural language processing. It involves training a neural network to make predictions based on selected features and evaluating the contribution of non-selected features, providing insights into the limitations of post-hoc explainers. The evaluation methodology involves training a neural network to make predictions based on selected features and assessing the impact of non-selected features, offering insights into the constraints of post-hoc explainers. The column Sx provides the percentage of instances eliminated from the dataset due to a potential handshake, while |SRx| = 0 shows the percentage of datapoints further eliminated due to the absence of a selected token with an absolute effect of at least 0.1 on the prediction. If the model gives the same prediction when eliminating all non-selected tokens, it indicates no handshake for the instance. The model's prediction changes when a specific token is removed, indicating its importance in a handshake. If all non-selected tokens can be eliminated without changing the prediction, it suggests no handshake occurred. This is confirmed by the equation encoder(generator(Sx)) = encoder(generator(x)) =\u21d2 no handshake in x. The proof concludes that having S Sx = S x satisfies the condition, as shown in Equation 11. The proof shows that having S Sx = S x satisfies the condition for no handshake in x. The beer review describes a dark, fizzy beer with fruity notes and a smooth finish. The beer review describes a dark, fizzy beer with fruity notes and a smooth finish. The beer pours a dark fizzy yellow color with a lot of head that laces well. It smells like fruit, maybe some apple and blueberry, with no mouthfeel. The taste is fruity with a slight warming sensation as you finish the bottle, better than most American lagers. The bottle is nice brown \"grolsch\" like, good for re-use. The beer review describes a dark, fizzy beer with fruity notes and a smooth finish. It pours a dark fizzy yellow color with a lot of head that laces well. Smells like fruit, maybe apple and blueberry, with no mouthfeel. Tastes fruity with a slight warming sensation as you finish the bottle, better than most American lagers. Nice brown \"grolsch\" like bottle, good for re-use."
}