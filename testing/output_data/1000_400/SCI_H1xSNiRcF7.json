{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models for learning from uncertain supervision and inferring soft-inclusions among concepts. The Box Lattice model of Vilnis et al. (2018) showed promising results in modeling soft-inclusions through high-dimensional hyperrectangles. However, the hard edges of the boxes present optimization challenges. This work presents a novel hierarchical embedding model inspired by the Box Lattice model. In this work, a novel hierarchical embedding model is presented, inspired by the Box Lattice model. The approach involves parameterizing density functions using Gaussian convolutions over high-dimensional hyperrectangles, improving optimization robustness in the disjoint case. Performance improvements are demonstrated on various tasks, especially in sparse data scenarios. The hierarchical embedding model, inspired by the Box Lattice model, improves optimization robustness in the disjoint case. Performance enhancements are shown on tasks like WordNet hypernymy prediction and Flickr caption entailment, especially in sparse data scenarios. Embedding methods are crucial in machine learning, converting semantic problems into geometric ones. Word2Vec sparked a renaissance in embeddings, with recent interest in structured or geometric representations. Recent years have seen a resurgence in embeddings, with a focus on structured or geometric representations like Gaussian embeddings, order embeddings, and box embeddings. These geometric structures better capture asymmetry, entailment, and transitive relations compared to simple vector points, providing a strong bias for various tasks. The probabilistic Box Lattice model is highlighted in this work for its unique properties. The focus is on the probabilistic Box Lattice model of box embeddings, which outperforms in modeling transitive relations and complex joint probability distributions. Box embeddings generalize order embeddings by using overlapping boxes instead of vector lattice ordering. Box embeddings (BE) generalize order embeddings by using overlapping boxes instead of vector lattice ordering, but face challenges in gradient-based optimization due to disjoint boxes causing issues with correcting problems in the model, especially with sparse data like market basket models for recommendation. The curr_chunk discusses addressing the issue of disjoint boxes in box embeddings by introducing a new model that uses smoothed density functions with Gaussian convolution. The new approach demonstrates superior results in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. The new approach introduces smoothed density functions with Gaussian convolution to address disjoint boxes in box embeddings, demonstrating superior results in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. It extends existing models like order embeddings and box lattice embeddings. The nonprobabilistic DAG or lattice in a vector space with order given by inclusion of embeddings' forward cones is extended by the probabilistic extension model due to BID9 and the box lattice or box embedding model of BID22. Another hyperrectangle-based generalization of order embeddings, also known as box embeddings, was proposed by BID18. These models, along with hyperbolic space embedding models, optimize energy functions and do not assign conditional probabilities to edges. Methods based on order embeddings consider an edge present only if one box entirely encloses another. Hyperbolic space embedding models have also been proposed for learning hierarchical embeddings, optimizing energy functions without assigning conditional probabilities to edges. The negative curvature of hyperbolic space favors tree structures but may not be suitable for non-treelike DAGs. Smoothing the energy landscape using Gaussian convolution is a common approach in optimization methods and is being incorporated into machine learning models. Our approach involves smoothing the energy landscape of the model using Gaussian convolution, a common technique in optimization methods. We focus on embedding orderings and transitive relations, which is a subset of knowledge graph embedding. Our probabilistic approach aims to learn an embedding model that maps concepts to subsets of event space, with an inductive bias suited for transitive relations and fuzzy concepts of inclusion and entailment. We also provide a brief overview of representing ontologies as geometric objects using order theory. The embedding model maps concepts to subsets of event space, with an inductive bias for transitive relations and fuzzy concepts of inclusion and entailment. It introduces methods for representing ontologies as geometric objects using order theory, including vector and box lattices. Posets and lattices are discussed as formalisms for acyclic directed graph data with transitive relations. A poset generalizes the concept of a totally ordered set to allow for incomparable elements. Posets are useful for representing acyclic directed graph data with transitive relations. A lattice, a type of poset, has unique least upper and greatest lower bounds for any subset of elements. Bounded lattices have additional elements denoting the least upper and greatest lower bounds of the entire set. Bounded lattices are equipped with binary operations for join and meet, satisfying specific properties. The extended real numbers form a bounded lattice under min and max operations. A lattice is a type of poset with unique least upper and greatest lower bounds for subsets of elements. Bounded lattices have additional elements denoting the least upper and greatest lower bounds of the entire set, equipped with binary operations for join and meet. The extended real numbers form a bounded lattice under min and max operations, and sets partially ordered by inclusion also form a lattice. Semilattices have either a meet or join operation, but not both. The \u2227 and \u2228 operations can be swapped to create a valid lattice, known as the dual lattice. A semilattice has either a meet or join operation. In a vector lattice, a vector space is endowed with a lattice structure. The standard partial order for the vector lattice R n is the product order from the underlying real numbers. The Order Embeddings of BID20 embed partial orders as vectors using the reverse product order, corresponding to the dual lattice. Objects become \"more specific\" as they move away from the origin in the vector lattice representation. The toy example in FIG0 demonstrates this concept. Vilnis et al. introduced a box lattice where each concept in a knowledge graph is associated with minimum and maximum coordinate vectors. The Order Embedding vector lattice represents a simple ontology using a probabilistic extension. Vilnis et al. introduced a box lattice for knowledge graphs, where concepts are associated with minimum and maximum coordinate vectors. The box lattice structure includes least upper bounds and greatest lower bounds, denoted by max and min. The box lattice structure involves least upper bounds and greatest lower bounds, denoted by max and min, for pairs of maximum and minimum intervals at each coordinate. The lattice meet is the largest box contained within both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are associated with the volume of boxes under a suitable probability measure, with probability p(x) calculated as n i (x M,i \u2212 x m,i ) under the uniform measure. The uniform measure assigns probabilities to boxes within the unit hypercube, with p(x) calculated as n i (x M,i \u2212 x m,i ). The probability of the empty set is zero. When using gradient-based optimization for learning box embeddings, issues arise when two concepts are incorrectly labeled as disjoint, leading to zero gradient flow. When using gradient-based optimization to learn box embeddings, an issue arises when two concepts are mistakenly labeled as disjoint, resulting in zero gradient flow. The authors propose a surrogate function to address this problem, but a more principled framework is suggested to develop alternate measures that avoid this issue, especially in sparse lattices where most boxes have little to no intersection. When embedding sparse lattices, it is crucial to avoid disjoint boxes to prevent gradient sparsity issues. A surrogate function is proposed by the authors, but a more principled framework is recommended to develop alternate measures for better optimization and model quality. The authors propose a relaxation of the standard box embeddings to address gradient sparsity issues when embedding sparse lattices. They suggest rewriting the joint probability measure of intervals as an integral of the product of indicator functions to better optimize and preserve geometric intuition. The authors propose using kernel smoothing, specifically convolution with a normalized Gaussian kernel, to optimize and preserve geometric intuition in embedding sparse lattices. This approach replaces indicator functions with functions of infinite support, demonstrated in Figure 2 for one dimension. The authors apply the diffusion equation to the original embeddings and use mollified optimization for energy smoothing. They associate smoothed indicator functions with lattice elements x and y, leading to a closed form solution involving the standard normal CDF and softplus function. The solution to equation 2 involves the softplus function and the standard normal CDF. In the zero-temperature limit, the formula converges to the original equation 1. However, multiplication of Gaussian-smoothed indicators does not give a valid operation on a function lattice. Convolution with a zero-bandwidth kernel using a Dirac delta function is not valid for a function lattice due to idempotency issues. To address this, a modification of equation 3 can be made to obtain a function that retains smooth optimization properties. This modification allows for the preservation of probabilities in applications that train on conditional probabilities. By modifying equation 3, a function p can be obtained to preserve probabilities in applications training on conditional probabilities, while maintaining smooth optimization properties. The identity holds true for the hinge function but not the softplus function, with a similar functional form as equation 6 applicable to both. In the zero-temperature limit, equations 3 and 7 are equivalent, but equation 7 is idempotent for specific intervals. In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for specific intervals, inspiring the definition of probabilities using a normalized version of equation 7. Softplus upper-bounds the hinge function, requiring normalization in experiments. In experiments, softplus upper-bounds the hinge function, requiring normalization. Two approaches are used for normalization: dividing by global minimum and maximum sizes or projecting onto the unit hypercube and normalizing. The final probability is the product over dimensions. In experiments, softplus upper-bounds the hinge function, requiring normalization by projecting onto the unit hypercube. The final probability is the product over dimensions, with the softplus overlap showing better behavior for disjoint boxes compared to the Gaussian model. Our approach retains the inductive bias of the original box model, equivalent in the limit, and satisfies the necessary condition that p(x, x) = p(x). A comparison of 3 different functions in FIG2 shows that the softplus overlap performs better for highly disjoint boxes than the Gaussian model while preserving the meet property. Experiments on the WordNet hypernym prediction task evaluate these improvements in practice, with the WordNet hypernym hierarchy containing 837,888 edges. Positive examples are randomly chosen, while negative examples are generated by swapping terms with a random word in the dictionary. The WordNet hypernym hierarchy contains 837,888-edges after transitive closure. Positive and negative examples are randomly chosen for evaluation. The smoothed box model performs nearly as well as the original in terms of test accuracy. Further experiments are conducted to explore performance in a sparse regime. In further experiments, different numbers of positive and negative examples from the WordNet mammal subset were used to compare the box lattice, smoothed approach, and order embeddings (OE) as a baseline. Training data consisted of 1,176 positive examples, with dev/test sets containing 209 positive examples. Negative examples were generated randomly. The models, including OE baseline, Box, and Smoothed Box, performed similarly with balanced data. The training data for the WordNet mammal subset contains 1,176 positive examples, with 209 positive examples in the dev/test sets. Negative examples are generated randomly. The models, including OE baseline, Box, and Smoothed Box, perform well with balanced data. Smoothed Box outperforms OE and Box on imbalanced data, which is crucial for real-world entailment graph learning. The study focuses on the importance of handling imbalanced data in real-world entailment graph learning. Experiments were conducted on the WordNet mammal subset and the Flickr entailment dataset, which consists of 45 million image caption pairs. The study compares F1 scores of different models on imbalanced data and reports KL divergence and Pearson correlation on various subsets of the test data. The study applies the softplus function to the unit cube for volume calculation, showing slight performance gains on unseen captions. It also applies the method to a market-basket task using the MovieLens dataset, predicting users' movie preferences based on ratings above 4 points. The study predicts users' movie preferences using the MovieLens dataset by analyzing pairs of user-movie ratings above 4 points. They compare different models like low-rank matrix factorization and hierarchical embedding methods. The evaluation is done using KL divergence, Pearson correlation, and Spearman correlation on the test set. The study compares various models for predicting users' movie preferences using the MovieLens dataset. They evaluate the models using KL divergence, Pearson correlation, and Spearman correlation on the test set. The results show that the smoothed box embedding method outperforms other baselines, especially in Spearman correlation, which is crucial for recommendation tasks. Additional studies on the model's robustness are also presented. The study presented a method for smoothing energy and optimization landscape of probabilistic box embeddings, showing superior performance in Spearman correlation for recommendation tasks. The model requires fewer hyper-parameters, outperforming current state-of-the-art results on various datasets, particularly effective with sparse data and robust to poor initialization. This work addresses challenges in learning from geometrically-inspired embedding models, with room for further research in this area. The study introduced a method for smoothing energy and optimization landscape of probabilistic box embeddings, showing superior performance in Spearman correlation for recommendation tasks. The model is effective with sparse data and robust to poor initialization. Further research is needed in learning from geometrically-inspired embedding models. The work also includes a proof of the Gaussian overlap formula for lattice elements x and y. The study presents a method for smoothing energy and optimization landscape of probabilistic box embeddings, proving the Gaussian overlap formula for lattice elements x and y. The model performs well with sparse data and is robust to poor initialization. Further research is required in learning from geometrically-inspired embedding models. The MovieLens dataset, with a large proportion of small probabilities, is suitable for optimization by the smoothed model. The MovieLens dataset is suitable for optimization by the smoothed model due to its large proportion of small probabilities. Additional experiments were conducted to test the robustness of the model to initialization, specifically focusing on the potential for disjoint boxes during optimization. The study tested the model's robustness to disjoint boxes during optimization by adjusting the width parameter of the initial distribution of boxes. Results showed that the smoothed model performed well even with disjoint initialization, while the original box model's performance degraded significantly. This suggests that the strength of the smoothed model lies in its ability to optimize smoothly in the disjoint regime. The study tested the model's robustness to disjoint boxes during optimization by adjusting the width parameter of the initial distribution of boxes. Results showed that the smoothed model performed well even with disjoint initialization, while the original box model's performance degraded significantly. This suggests that the strength of the smoothed model lies in its ability to optimize smoothly in the disjoint regime. The methodology and hyperparameter selection methods for each experiment are briefly outlined, with detailed settings and code available at https://github.com/Lorraine333/smoothed_box_embedding. The model is evaluated every epoch on the development set for a large fixed number of epochs in WordNet experiments. Baseline models are trained using BID22 parameters, with hyperparameters determined on the development set. Negative examples are generated randomly based on the ratio for each batch of positive examples. The experimental setup uses the same architecture as BID22 and BID9, with a single-layer LSTM reading captions and producing box embeddings. The model is trained for a large fixed number of epochs and tested. The experimental setup uses a single-layer LSTM to read captions and produce box embeddings parameterized by min and delta. The model is trained for a fixed number of epochs and tested on the development data. Hyperparameters are determined on the development set, and the best development model is used to score the test set."
}