{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as a source of domain knowledge representation. However, ontology information is rarely considered in modeling neural network architectures. We propose ontology-based neural network architectures for sound event classification, demonstrating improved performance by incorporating ontological information. Humans can identify various sounds in their environment, which can be categorized into more abstract classes. Ontologies provide a structure for sound event classification datasets but are underutilized in current models. Neural networks are the state of the art for sound event classification but often do not leverage available ontological information. Ontologies provide structure for sound event classification datasets, but current models rarely utilize this information. Neural networks, although state-of-the-art for sound event classification, often do not consider ontologies in their design. Ontologies represent domain knowledge through categories and relationships, offering a formal way to organize training data and neural network architecture. Ontologies provide structure for sound event classification datasets, organizing training data and neural network architecture based on abstraction hierarchies. Taxonomies can be defined by nouns or verbs, and interactions between objects and materials. Hierarchical relations in classifiers offer benefits like allowing back-off to more general categories and disambiguating classes. Hierarchical relations in sound event classifiers offer benefits such as allowing back-off to more general categories, disambiguating acoustically similar classes, penalizing classification differently, and using domain knowledge to model neural networks. Ontology-based network architectures have shown performance improvement in sound event classification, although they have been more commonly used in computer vision and music. Ontology-based network architectures have shown performance improvement in sound event classification. Authors have proposed various approaches, such as using a deep restricted Boltzmann machine for textual topic classification and employing perceptrons for each node in the hierarchy to improve performance and reduce overfitting. These approaches leverage domain knowledge to model neural networks effectively. In this section, the authors propose ontology-based networks to improve performance in deep learning models. They describe assumptions about the type of ontologies used and present a Feed-forward model with an ontological layer to preserve embedding space consistency. The authors propose ontology-based networks to enhance deep learning models by incorporating ontological constraints. They introduce a Feed-forward model with an ontological layer and extend the learning model to compute ontology-based embeddings using Siamese Neural Networks. The framework is designed to utilize ontology structure and neural network architectures, with a focus on ontologies with two levels but can be generalized to more levels. Training data consists of audio representations associated with a set of labels. The framework proposed by the authors utilizes ontology-based networks to enhance deep learning models by incorporating ontological constraints. The training data consists of audio representations associated with a set of labels, where each possible class is mapped hierarchically. The framework can be generalized to more levels beyond the two levels considered in the study. The ontology-based network framework enhances deep learning models by incorporating hierarchically mapped labels. The architecture includes a Feed-forward Network with an Ontological Layer, where labels from one level can infer labels from another level using a probabilistic formulation. The architecture of the Feed-forward Network with Ontological Layer involves using a probabilistic formulation to infer labels from one level to another. By estimating probabilities of different classes, the model can improve performance in making predictions. The proposed framework introduces an ontological layer in the Feed-forward Network architecture. It utilizes knowledge to improve model performance in predicting classes y2, with a base network learning weights for audio features x to generate outputs for different ontology levels. The Feed-forward Network (FFN) with Ontological Layer includes a base network (Net) and an intermediate vector z, generating outputs for two ontology levels. The base network learns weights for audio features x to produce probability vectors for classes C1 and C2. The ontological layer reflects the relation between super classes and sub classes in the ontology, allowing for predictions of any class in C1 and C2 for any input x. The FFN, once trained, can predict classes in C1 and C2 for any input x using the ontological layer. Equation 3 represents this relationship as a directed graph with the ontological layer defining weights for standard layer connections. Training involves minimizing the loss function L, a combination of two categorical cross-entropy functions for p(y1|x) and p(y2|x). To train the model, a gradient-based method is applied to minimize the loss function L, a combination of two categorical cross-entropy functions for p(y1|x) and p(y2|x). The hyperparameter \u03bb \u2208 [0, 1] is tuned to create embeddings that preserve the ontological structure using a Siamese neural network (SNN). The SNN enforces samples of the same class to be closer while separating samples of different classes, aiming to learn ontology-based embeddings. The ontology-based embeddings were learned using a Siamese neural network (SNN) to preserve the ontological structure. The SNN ensures samples of the same class are closer while separating samples of different classes, with the architecture shown in FIG1. The twin networks have the same base architecture with shared weights, and ontological embeddings are used to compute a similarity metric. The Siamese neural network (SNN) preserves ontological structure by ensuring samples of the same class are closer while separating samples of different classes. The twin networks share weights and use ontological embeddings to compute a similarity metric. The distance between embeddings indicates the difference between samples from the same subclass, different subclass but same superclass, or different superclasses. Output probabilities are calculated for each level. Training the Feed-forward Model with Ontological layer involves using Ontology-based embeddings. The Siamese neural network (SNN) maintains ontological structure by ensuring samples of the same class are closer while separating samples of different classes. It uses ontological embeddings to compute a similarity metric, determining the distance between samples from the same subclass, different subclass but same superclass, or different superclasses. Output probabilities are calculated for each level. The Feed-forward Model with Ontological layer is trained using Ontology-based embeddings to evaluate sound event classification performance in neural network architectures. The dataset for the Making Sense of Sounds Challenge 2 - MSoS aims to classify the most abstract classes in its taxonomy. The dataset for Making Sense of Sounds Challenge 2 - MSoS includes audio files from various sources, with 1500 files in the development set and 500 files in the evaluation set. The dataset has two ontology levels with 97 classes at level 1 and 5 classes at level 2. The files are in a standardized format of single-channel 44.1 kHz, 16-bit .wav files. The development dataset consists of 1500 audio files divided into five categories, with 300 files each. The evaluation dataset has 500 audio files, 100 per category. All files are in a standardized format of single-channel 44.1 kHz, 16-bit .wav files. The dataset was randomly partitioned for training, tuning parameters, and testing. The official blind evaluation set consisted of 500 files across five classes. The Urban Sounds -US8K dataset is designed for urban sound classification, with a taxonomy adjusted to avoid redundant levels. The Urban Sounds -US8K dataset contains 8,732 audio files divided into 10 subsets for classification evaluation. The audio files are real field recordings in a standardized format. The dataset uses 9 folds for training and tuning parameters, with one fold for testing. State-of-the-art Walnet features BID1 are used to represent audio recordings, with a 128-dimensional logmel-spectrogram vector computed for each audio and transformed via a convolutional neural network. The dataset contains 8,732 audio files divided into 10 subsets. State-of-the-art Walnet features BID1 are used to represent audio recordings. A 128-dimensional logmel-spectrogram vector is computed for each audio and transformed via a convolutional neural network (CNN) with a feed-forward multi-layer perceptron network architecture. The network consists of 4 layers: input layer (1024), 2 dense layers (512 and 256), and output layer (128). The dense layers utilize Batch Normalization, a dropout rate of 0.5, and ReLU activation function. The model architecture includes an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. Batch Normalization, a dropout rate of 0.5, and ReLU activation function are used in the dense layers. Baseline models were considered for different data sets, with the addition of an output layer for level 1 or level 2. The baseline models did not include ontological information. The model architecture includes input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and output layer of dimensionality 128. Baseline models for different data sets included output layer for level 1 or level 2 without ontological information. Results of baseline models for MSoS and US8K data sets at level 1 and level 2 were shown in Table 1. Performance of baseline models in MSoS challenge was 0.81 for level 2. Models were trained with different values of \u03bb to analyze the utility of ontological layer, as shown in FIG3. The architecture presented in Section 2.2 was validated by training models with different values of \u03bb. The ontological layer improved classification performance in both level 1 and level 2 datasets. For the MSoS dataset, the best performance was achieved with \u03bb = 0.8, resulting in accuracy of 0.74 and 0.913 for level 1 and 2 respectively, showing a 5.4% and 6% improvement over baseline models. On the US8K dataset, the best performance was with \u03bb = 0.7, with accuracy of 0.82 and 0.86 for level 1. Using the ontological structure, an absolute improvement of 5.4% and 6% was achieved in classification accuracy for level 1 and 2 datasets with \u03bb = 0.8. On the US8K dataset, a smaller improvement of 2.5% and 0.2% was observed for level 1 and 2 with \u03bb = 0.7. The MSoS t-SNE plots show the impact of ontology-based embeddings on class samples at different levels. The MSoS t-SNE plots illustrate the impact of ontology-based embeddings on class samples at different levels. The architecture described in Section 2.3 was tested for sound event classification using Walnet audio features. The ontology-based embeddings resulted in tighter and better defined clusters compared to base network vectors. Different super and sub class pairs were used to train the Siamese neural network for producing the embeddings, which were then passed to the base network architecture. The study utilized t-SNE plots to show how embeddings cluster at various levels. Walnet audio features were processed to train a Siamese neural network for ontology-based embeddings. The network was trained for 50 epochs with tuned hyper-parameters, using 100,000 pairs of input data. Different lambda values were used for classifiers at different levels, with a loss function derived from previous experiments. The study used 100,000 pairs of input data and adjusted lambda values for classifiers at different levels. Results showed MSoS and US8K accuracy in level 1 as 0.736 and 0.818, and in level 2 as 0.886 and 0.856. The architecture outperformed the baseline but slightly underperformed without embeddings. Ontology-based embeddings showed better grouping in t-SNE plots. The study concluded that the architecture performed better than the baseline but slightly underperformed without embeddings. Ontology-based embeddings showed improved grouping in t-SNE plots, with tighter and better-defined clusters for level 2 classes. However, the performance on the US8K dataset was limited due to a similar number of sub classes and super classes. The study proposed a framework for sound event classification using hierarchical ontologies. Two methods were used to incorporate this structure into neural networks, resulting in improved performance compared to the baseline. The Feed-forward Network with Ontological Layer achieved 0.88 accuracy, while using ontological embeddings achieved 0.89 accuracy on the blind evaluation set. The ratio between sub classes and classes impacted the ontology's contribution, with the US8K dataset showing limited performance due to a similar number of sub classes and super classes. In this paper, a framework for sound event classification using hierarchical ontologies was proposed. Two methods were presented to incorporate this structure into neural networks, resulting in improved performance over the baseline. A Feed-forward Network with an ontological layer and a Siamese neural Network were used to relate predictions of different levels in the hierarchy and compute ontology-based embeddings. Results showed clusters of super classes containing different sub classes, with improvements in datasets and the MSoS challenge. The study opens the path for further exploration of ontologies and relations in sound event classification. The study proposed a framework for sound event classification using hierarchical ontologies. Two methods were used to incorporate this structure into neural networks, resulting in improved performance. The embeddings plots showed clusters of super classes with different sub classes, leading to better results in datasets and the MSoS challenge. This research opens the path for further exploration of ontologies and relations in sound event classification."
}