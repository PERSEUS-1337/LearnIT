{
    "title": "r1l73iRqKm",
    "content": "In open-domain dialogue, intelligent agents struggle to incorporate knowledge effectively. Existing models often rely on generic utterances rather than recalled knowledge as context. To address this, a new dataset grounded in knowledge from Wikipedia is introduced. Architectures capable of retrieving, reading, and using this knowledge to generate natural responses are developed. The best dialogue models demonstrate the ability to engage in knowledgeable discussions on various topics, as confirmed by both automatic metrics and human evaluations. Our dialogue models utilize a large dataset grounded in Wikipedia knowledge to generate natural responses. The goal is for machines to engage in knowledgeable discussions on open-domain topics, showcasing their ability to comprehend language, retain and recall knowledge, reason, and produce captivating responses. This research direction aims to enable humans to effectively communicate with machines. Machines must master skills like language comprehension, memory retention, reasoning, and generating captivating responses to enable communication with humans. Current approaches lack effective memory and knowledge utilization, requiring more direct memory mechanisms for intelligent conversations. In the context of enabling communication with humans, machines need to utilize memory and knowledge effectively. This involves encoding, reasoning, and decoding input sequences to engage in open-domain dialogue. The task of conversing intelligently requires more direct memory mechanisms to be employed, especially in setups where conversations can broaden or focus on related themes. In designing architectures for intelligent conversations, Memory Network and Transformer models are combined to retrieve knowledge and generate text outputs. A supervised dataset of human-human conversations was created using crowd-sourced workers, with 1365 discussion topics and 201,999 utterances collected. Transformer Memory Networks were developed to generate outputs in conversations by utilizing a supervised dataset of human-human interactions. The dataset includes diverse discussion topics linked to Wikipedia, with one participant acting as a \"wizard\" linking knowledge to existing articles. This approach allows for training and evaluating conversation models based on their ability to recall and utilize knowledge from text. The Transformer Memory Network architectures were tested using automatic metrics and human evaluations, demonstrating their effectiveness in executing conversations. Our Transformer Memory Network architectures are designed to recall and ground existing text, evaluate models, and engage in knowledgeable conversations with humans. They outperform standard Memory Networks and Transformers in executing conversations, as shown in our new benchmark available in ParlAI. This benchmark aims to encourage further research in utilizing knowledge explicitly in dialogue tasks. Our work focuses on investigating unstructured knowledge across a wide range of topics, potentially spanning all of Wikipedia. Unlike existing dialogue tasks that do not explicitly study the use of knowledge, we aim to encourage improvements in this research direction. Our work explores unstructured knowledge across various topics, potentially covering all of Wikipedia. Unlike existing dialogue tasks, we focus on the importance of retrieving and conditioning knowledge for question answering tasks. The work focuses on natural human dialogues containing a diverse set of utterances, not just questions and answers. Previous research has used Memory Networks for dialogue discussing movies and open-ended discussions from Reddit, linking to structured knowledge. Other studies have utilized unstructured text, such as discussing news articles using Wikipedia summaries as knowledge. The study employed Memory Networks for dialogue discussing movies and open-ended discussions from Reddit, conditioning on a structured knowledge base. Previous research also linked Reddit to structured knowledge, while other studies used unstructured text like Wikipedia summaries for discussing news articles. The comparison was made between Memory Networks BID19 and Transformers in the context of the study. In contrast to previous studies using structured knowledge, our work focuses on open-domain dialogue with Wikipedia articles as grounding. We compare Memory Networks BID19 and Transformers, combining their approaches for multi-turn dialogues. Our paper presents models engaging in chitchat with topic changes, a novel approach not seen before in open-domain settings. Our paper introduces models engaging in full multi-turn dialogue in an open-domain setting, where one participant acts as a knowledgeable expert (wizard) and the other as a curious learner (apprentice). The apprentice delves deeply into a chosen topic to keep the conversation engaging and fun, differentiating it from shallow chit-chat tasks. The apprentice engages in conversation with the wizard, playing the role of a curious learner eager to discuss a chosen topic in depth. The wizard's goal is to inform the apprentice about a topic using an information retrieval system to find relevant Wikipedia paragraphs. The wizard uses an information retrieval system to find relevant Wikipedia paragraphs to inform the conversation with the apprentice. The wizard must craft a relevant reply based on the observed knowledge and present it in an engaging way. The conversation flow involves choosing a topic, sharing information, and constructing replies based on the relevant knowledge shown to the wizard. The conversation flow in wizard-apprentice interactions involves choosing a topic, sharing information, and constructing replies based on relevant knowledge. The goal is to replace the human wizard with a learned agent to interact with a human apprentice. The conversation flow in wizard-apprentice interactions involves choosing a topic, sharing information, and constructing replies based on relevant knowledge. The goal is to replace the human wizard with a learned agent to interact with a human apprentice. Partners end the chat after a minimum of 4 or 5 turns each, randomly chosen beforehand. Data is collected from these conversations to train the learned agent. Topics for dialogue are sourced from a set of 1365 natural, open-domain topics linked to Wikipedia articles. The wizard has access to passages of knowledge relevant to the dialogue context. This knowledge retrieval process is fixed to ensure consistency in data collection. The wizard in dialogue interactions has access to relevant passages of knowledge, retrieved using a fixed retriever system. This system compares articles and queries using TF-IDF weighted bag-of-word and n-gram vectors. The top articles are presented to the wizard for context, along with their titles. This method is used to collect data for training a learned agent, but a better method could potentially be learned and used during testing. During dialogue interactions, the wizard is provided with relevant knowledge passages retrieved using a fixed system. The wizard can select a sentence from the presented articles to respond to the apprentice. This method is used for data collection but a more advanced approach can be implemented during testing. In dialogue interactions, the wizard selects a relevant sentence from articles to respond to the apprentice. Learning dialogue models are developed to replace the wizard, accessing knowledge from sources like Wikipedia to generate dialogue utterances. We develop extensions of Memory Network BID19 and Transformer BID21 models for retrieving relevant information from a large memory based on dialogue history, reading and attending to the knowledge, and generating the next dialogue utterance. Two classes of models are created: retrieval models that select from a set of candidate responses and generative models that generate word-by-word. The input to both models is the current dialogue context at each turn, with the goal of forming a complete dialogue with the user. The models developed are for retrieving information from a large memory based on dialogue history. Two classes of models are created: retrieval models that select from candidate responses and generative models that generate word-by-word. The input to both models is the current dialogue context at each turn, aiming to form a complete dialogue with the user. Knowledge retrieval involves a large knowledge base organized hierarchically into documents, with standard information retrieval techniques used to return a smaller set of candidates for fine-grained selection. The text discusses the use of standard information retrieval techniques to return a smaller set of candidates for fine-grained selection in a large memory-based dialogue system. The system operates on the topic and the last two turns, retrieving the top 7 articles for each lookup and flattening the results into separate sentences. The text discusses using information retrieval techniques to select candidates for fine-grained selection in a memory-based dialogue system. An attention mechanism is used to choose knowledge sentences for the next dialogue turn, with each sentence encoded independently. The dialogue context is encoded with a Transformer, and dot-product attention is performed between memory candidates and the context for utterance prediction. The text discusses using information retrieval techniques in a memory-based dialogue system. Knowledge sentences are encoded independently, and dot-product attention is used for utterance prediction. Different variants of knowledge attention and utterance prediction are considered for retrieval and generative models. The model encodes knowledge sentences and dialogue context with a Transformer, using dot-product attention. Candidate responses are encoded separately. Two versions of the model are considered: Two-stage and End-to-end. The model is trained to minimize cross-entropy loss. The model is trained to minimize cross-entropy loss by selecting the most relevant knowledge and dialogue context for response generation. Generative models use BPE encoding for effective rare word copying from Wikipedia sentences. In the End-to-end version, a shared Transformer encoder encodes all candidates and dialogue history. The model utilizes BPE encoding for rare word copying from Wikipedia sentences and a shared Transformer encoder for encoding candidates and dialogue history in the End-to-end version. The attention prediction over memory is produced by flattening encoded candidates into vectors using normalization. The model is trained to minimize negative log-likelihood of response utterance and can be supervised to choose the same knowledge candidate as the human wizard in the training set. The End-to-end model utilizes BPE encoding and a shared Transformer encoder for dialogue history. It minimizes negative log-likelihood for response utterance and can be supervised to select the same knowledge candidate as the human wizard. In the Two-stage version, separate models are used for knowledge selection and utterance prediction. Knowledge dropout is employed to improve decoder performance. In the Two-stage version, separate models are used for knowledge selection and utterance prediction. Knowledge dropout is employed to improve decoder performance by preventing the model from attending to knowledge during training. This technique helps the generator be more resilient to errors and speeds up training. The experimental setups and results are described, focusing on the ability of models to select knowledge appropriately and predict human-selected knowledge in the dataset. In the experimental setups, models are tested on their ability to select knowledge and predict human-selected knowledge. Transformers outperform baselines when pretrained on a large dataset like Reddit, with marginal impact from multitasking on SQuAD. The study compares an Information Retrieval baseline with a Bag-of-Words Memory Network and finds that Transformers perform best when pretrained on a large dataset like Reddit. The results are shown in a table, and further analysis using other models is provided in the appendix. The best performing Transformer model is used for a two-stage generative Memory Network in the full dialogue task, evaluating models on dialogue generation given knowledge in different settings. In dialogue generation tasks, models are evaluated based on their ability to incorporate knowledge. The addition of knowledge improves model performance, with Transformer Memory Networks showing significant improvements. Generative experiments compare End-to-end and Two-stage Transformer Memory models. The addition of knowledge improves model performance, with Transformer Memory Networks showing significant improvements. Generative experiments compare End-to-end and Two-stage Transformer Memory models, demonstrating substantial improvements when provided with gold knowledge. The experiments show that End-to-end and Two-stage models utilize knowledge in response predictions, outperforming Transformer without knowledge. The Two-stage model excels in perplexity and F1 scores with predicted knowledge, while the End-to-end model performs better with gold knowledge. End-to-end model benefits from additional knowledge selection supervision, improving on every metric. Knowledge dropout also proves to be beneficial. The experiments show that the End-to-end model benefits from additional knowledge selection supervision, improving on every metric. Knowledge dropout also proves to be beneficial in utilizing selected knowledge effectively. The conversation between a human and a model reveals a preference for physical books over e-books due to the sensory experience and ownership. The models are evaluated through human interaction and receive higher ratings compared to retrieval models. The models are evaluated through human interaction by pairing humans with the models to chat about specific topics. Humans rate their dialogue partner on engagingness and a metric called Wiki F1 score is calculated based on the model's knowledge exhibited. The goal is to maximize both engagingness and knowledge metrics. The study evaluates models through human interaction, collecting 546 conversations with ratings from 464 workers. Retrieval models outperform generative models in engagingness. Knowledgeable retriever models show higher Wiki F1 scores in seen and unseen test sets. Generative models show trends towards knowledge use for higher engagingness. The study compares retrieval and generative models in engagingness and knowledge use. Knowledgeable retriever models show higher Wiki F1 scores in seen and unseen test sets, while generative models improve engagingness ratings with knowledge use. Retrieval models are limited to training set responses, resulting in a gap with generative models on unseen data. The study compares retrieval and generative models in engagingness and knowledge use. Retrieval models are limited to producing responses from the training set, resulting in a gap with generative models on unseen data. Transformer Memory Network models are developed to employ large memory systems containing encyclopedic knowledge for open-domain conversations. The models are capable of retrieving and attending to knowledge to output responses in either retrieval or generative modes. Dialogue agents use Transformer Memory Network models to conduct engaging open-domain conversations by retrieving and utilizing encyclopedic knowledge. The Wizard of Wikipedia dataset is collected to train and evaluate these models, showcasing their effectiveness in both automatic and human experiments. This benchmark aims to inspire further exploration in this research direction, with potential future work including bridging the gap between retrieval and generative responses. Our new benchmark dataset aims to inspire further model exploration in the research direction of engaging open-domain conversations using encyclopedic knowledge. Future work includes bridging the gap between retrieval and generative responses, learning to retrieve and reason simultaneously, and investigating the relationship between knowledge-grounded dialogue and existing QA tasks. The goal is to create an engaging and knowledgeable conversational agent. The dataset includes conversations between a wizard and an apprentice, where the wizard uses an information retrieval system to answer questions and provide relevant information. The dataset shows that apprentices ask questions in 13.9% of training set utterances and answer questions 39.5% of the time. In the dataset, apprentices ask questions in 13.9% of training set utterances, answer questions 39.5% of the time, and engage in new or follow-on statements 49.3% of the time. Persona-Chat dataset BID26 was used to select natural topics for conversations, with \u223c1000 personas each describing interests that can be seen as topics of interest. The dataset used personas to create topics of interest for conversations. Each persona consisted of 4-5 sentences describing interests, which were mapped to relevant Wikipedia pages. A total of 1,431 topics were obtained for the task, with 2-3 related topic choices presented per dialogue during data collection. Additional experiments were conducted to test the performance of models trained for the full dialogue task. In total, 1,431 topics were obtained for the task, with 2-3 related topic choices presented per dialogue during data collection. Additional experiments were conducted to test the performance of models trained for the full dialogue task, including knowledge selection tasks and analysis of dialogues produced from human evaluation experiments. Loss clearly benefits generative models. An analysis of dialogues from human evaluation experiments was conducted, with 20 conversations sampled from each experimental setting. Human-human conversations differ significantly from bot conversations, with humans engaging in more small talk and using topics as icebreakers. In contrast, conversations from the Wizard dataset involve one human with access to Wikipedia, leading to more factual discussions. All models attempt to play the role of a wizard and produce factual sentences. The conversation analysis shows that human-human dialogues involve more small talk and icebreakers, while conversations from the Wizard dataset are more factual. Models aim to produce factual sentences like a wizard. The retriever without knowledge tends to go off-topic, while the retriever with knowledge sticks to the chosen topic but struggles with subject changes by humans. The retriever without knowledge tends to go off-topic or change the subject rapidly, while the retriever with knowledge sticks to the chosen topic but struggles with subject changes by humans. Additionally, a two-stage retrieval system was tested on the full Wizard task, outperforming the best retrieval method in terms of F1 but not in terms of Recall@1. The performance of a two-stage retrieval system was compared to the best retrieval method in terms of F1 but not Recall@1. Results suggest that improving knowledge selection could enhance the retrieval system. Human experiments showed higher Wiki F1 scores for the wizard and apprentice, indicating the importance of knowledge in dialogue retrieval tasks. The dataset was used for comparison to human evaluations, showing that knowledge is crucial in dialogue retrieval tasks. The generator without knowledge often provides inaccurate answers but can still maintain a natural conversational flow. The retriever with knowledge performs better, as seen in selected conversations in FIG5. The generator without knowledge exhibits typical seq2seq system behaviors like repetition and inconsistencies in personality. In contrast, the generator with knowledge copies large fragments from Wikipedia, resulting in fewer repetition issues. However, it can sometimes provide inaccurate information. The generator with knowledge, while copying large fragments from Wikipedia, has fewer repetition issues but can sometimes provide inaccurate information. It can act as a conversationalist without inviting a response and often gives formulaic responses. Despite these drawbacks, it is able to generalize to unseen topics successfully."
}