{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization is a bilevel optimization problem where optimal parameters depend on hyperparameters. Regularization hyperparameters for neural networks are adapted by fitting approximations to the best-response function. Scalable best-response approximations are constructed by modeling the best-response as a single network with gated hidden units. This model is fitted using a gradient-based hyperparameter optimization algorithm. Self-Tuning Networks (STNs) are shallow linear networks with L2-regularized Jacobian that use a gating mechanism for hyperparameter optimization. This approach does not require differentiating the training loss with respect to hyperparameters, allowing for tuning of discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. By adapting hyperparameters online, STNs can discover schedules that outperform fixed values, showing superior performance on large-scale deep learning problems. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training, outperforming fixed values on deep learning tasks. Popular hyperparameter optimization methods like grid search and Bayesian optimization work well with low-dimensional spaces but ignore structure for faster convergence. Regularization hyperparameters such as weight decay and dropout are crucial for neural network generalization but challenging to tune. Hyperparameter optimization methods like grid search, random search, and Bayesian optimization are effective for low-dimensional spaces but overlook structure for faster convergence. Formulating hyperparameter optimization as a bilevel optimization problem can lead to better results. Hyperparameter optimization can be formulated as a bilevel optimization problem, where functions L T and L V map parameters and hyperparameters to training and validation losses. The best-response function w * can be approximated with a parametric function \u0175 \u03c6 to speed up the optimization process. This approach involves jointly optimizing \u03c6 and \u03bb, updating \u03c6 to approximate w * in a neighborhood around the current hyperparameters, and then using \u0175 \u03c6 as a proxy for w * in the optimization process. Finding a scalable approximation for \u0175 \u03c6 in the context of neural networks poses a significant challenge. To speed up hyperparameter optimization, the best-response function w * is approximated with a parametric function \u0175 \u03c6. This involves jointly optimizing \u03c6 and \u03bb, updating \u03c6 to approximate w * in a neighborhood around the current hyperparameters, and using \u0175 \u03c6 as a proxy for w * in the optimization process. Constructing a compact approximation by modeling the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of the hyperparameters is proposed, resulting in Self-Tuning Networks. Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering advantages over other optimization methods. They are easy to implement by replacing existing modules in deep learning libraries with \"hyper\" counterparts. This online adaptation ensures computational effort is not wasted and yields hyperparameter schedules that outperform fixed ones. Self-Tuning Networks (STNs) update their hyperparameters online during training, outperforming fixed settings. This adaptation allows tuning of discrete hyperparameters without differentiating the training loss. STNs are evaluated on large-scale deep-learning problems with the Penn Treebank and CIFAR-10 datasets. Self-Tuning Networks (STNs) can adjust hyperparameters during training, improving performance on deep-learning tasks with Penn Treebank and CIFAR-10 datasets. Bilevel optimization involves solving upper and lower-level problems, with applications in various fields including machine learning. Bilevel programs involve solving upper and lower-level problems, with applications in various fields including machine learning. These problems are strongly NP-hard, and most work focuses on restricted settings, but we aim to obtain local solutions in the nonconvex, differentiable, and unconstrained setting. In contrast to previous work focusing on restricted settings, we aim to obtain local solutions in the nonconvex, differentiable, and unconstrained setting by designing a gradient-based algorithm for solving a bilevel problem. The text discusses the use of a gradient-based algorithm for solving Problem 4, highlighting the limitations of simultaneous gradient descent and proposing the use of the best-response function for a more principled approach. This method aims to convert Problem 4 into a single-level problem by substituting the best-response function, allowing for the minimization of Eq. 5 using gradient descent on F* with respect to \u03bb. The text discusses using the best-response function to convert Problem 4 into a single-level problem for optimization. It highlights the conditions required for unique optima and differentiability of the function, providing sufficient conditions for their verification in a neighborhood of a given point. The text discusses the direct and response gradients in optimization problems, highlighting how including the response gradient can stabilize optimization by converting a bilevel problem into a single-level one. This conversion is noted to be beneficial for GAN optimization by Metz et al. (2016). Including the response gradient in optimization can stabilize the process by converting a bilevel problem into a single-level one, as noted by Metz et al. (2016) for GAN optimization. This conversion ensures a conservative gradient vector field, avoiding potential issues. Gradient-based hyperparameter optimization methods aim to approximate the best-response or its Jacobian, but they can be computationally expensive and struggle with discrete hyperparameters. Gradient-based hyperparameter optimization methods aim to approximate the best-response w * or its Jacobian \u2202w * /\u2202\u03bb, but they can be computationally expensive and struggle with discrete and stochastic hyperparameters. Lorraine & Duvenaud (2018) proposed promising approaches to directly approximate w * using a global approximation method with a differentiable function \u0175 \u03c6. Lorraine & Duvenaud (2018) proposed two algorithms for approximating w *: Global Approximation uses a differentiable function \u0175 \u03c6 to approximate w * as a hypernetwork, while Local Approximation locally approximates w * in a neighborhood around the current upper-level parameter \u03bb by minimizing an objective function. In practice, the second algorithm of Lorraine & Duvenaud (2018) approximates w * locally around the upper-level parameter \u03bb using a factorized Gaussian noise distribution with a fixed scale parameter \u03c3. An alternating gradient descent scheme is used to update \u03c6 and \u03bb to minimize specific equations. This approach has shown success with L2 regularization on MNIST but its applicability to different regularizers or larger problems is uncertain. The method requires \u0175 \u03c6, which may be challenging for high-dimensional w, and setting \u03c3 remains unclear. In this section, a best-response approximation \u0175 \u03c6 is constructed for large neural networks, which is memory efficient. The scale of the neighborhood \u03c6 is automatically adjusted, and the algorithm easily handles discrete and stochastic hyperparameters. The resulting networks update their own hyperparameters online during training, known as Self-Tuning Networks. In this section, a best-response approximation \u0175 \u03c6 is constructed for large neural networks, which is memory efficient. The algorithm automatically adjusts the scale of the neighborhood and easily handles discrete and stochastic hyperparameters. The resulting networks update their own hyperparameters online during training, known as Self-Tuning Networks (STNs). The architecture involves an affine transformation of hyperparameters for weight matrix and bias, adding a correction to the usual pre-activation to account for variations. The architecture involves an elementwise multiplication and row-wise rescaling, incorporating additional weight/bias scaled by hyperparameters. It is memory-efficient and enables parallelism for improved sample efficiency in deep learning. The architecture involves elementwise multiplication and row-wise rescaling with additional weight/bias scaled by hyperparameters for improved sample efficiency in deep learning. It enables parallelism by perturbing hyperparameters independently for different examples in a batch. The best-response function can be represented exactly using a linear network with Jacobian norm regularization. In this section, a model is presented where the best-response function can be represented exactly using a linear network with Jacobian norm regularization. The network consists of hidden units modulated conditionally on hyperparameters, using a 2-layer linear network with weights to predict targets from inputs. The model uses a squared-error loss regularized with an L2 penalty on the Jacobian, with a penalty weight mapped to lie in a positive range. The model presented involves a linear network with Jacobian norm regularization and hidden units modulated by hyperparameters. The network uses a squared-error loss with an L2 penalty on the Jacobian, mapped to a positive range. The architecture includes a sigmoidal gating of hidden units to approximate the best-response for deep, nonlinear networks. The architecture in FIG0 involves a linear network with hidden units modulated by hyperparameters. By replacing sigmoidal gating with linear gating for a narrow hyperparameter range, an affine approximation can be used to approximate the best-response function. This approach ensures gradient descent on the approximate objective converges correctly. Using linear gating with affine weights in hyperparameters, an affine approximation to the best-response function for quadratic lower-level objectives ensures convergence to a local optimum. The sampled neighborhood size affects the gradient match between the approximation and the best-response. Adjusting the scale of the hyperparameter distribution during training can impact the flexibility of the approximation to capture the best-response. Varying the entries of \u03c3 can help address issues with the gradient match between the approximation and the best-response. Adjusting the scale of the hyperparameter distribution during training can impact the flexibility of the approximation to capture the best-response. Varying the entries of \u03c3 can help address issues with the gradient match between the approximation and the best-response. To address these issues, adjusting \u03c3 during training based on the sensitivity of the upper-level objective to the sampled hyperparameters is proposed. An entropy term weighted by \u03c4 \u2208 R + enlarges the entries of \u03c3, resulting in an objective similar to variational inference. Adjusting the scale of the hyperparameter distribution during training can impact the flexibility of the approximation to capture the best-response. Varying the entries of \u03c3 can help address issues with the gradient match between the approximation and the best-response. An entropy term weighted by \u03c4 \u2208 R + enlarges the entries of \u03c3, resulting in an objective similar to variational inference. The objective interpolates between variational optimization and variational inference, balancing \u03c3 to avoid a heavy entropy penalty. The algorithm balances between shrinking to decrease the first term and avoiding a heavy entropy penalty. Performance is evaluated at the deterministic current hyperparameter \u03bb 0. The STN training algorithm can tune hyperparameters that other gradient-based algorithms cannot handle, such as discrete or stochastic hyperparameters. The hyperparameters are represented by \u03bb \u2208 R n and mapped to a constrained space by the function r. Training and validation losses, L T and L V, are functions of the hyperparameters. The algorithm uses an unconstrained parametrization \u03bb \u2208 R n for hyperparameters, mapped to a constrained space by function r. Training and validation losses, L T and L V, are functions of hyperparameters and parameters. STNs are trained using a gradient descent scheme alternating between updating \u03c6 for T train steps and updating \u03bb and \u03c3 for T valid steps. The non-differentiability of r due to discrete hyperparameters is not an issue. The derivative of E \u223cp( |\u03c3) [f (\u03bb + ,\u0175 \u03c6 (\u03bb + ))] is estimated during training. The algorithm utilizes an unconstrained parametrization for hyperparameters, with training and validation losses being functions of hyperparameters and parameters. STNs are trained using a gradient descent scheme, updating \u03c6 for T train steps and \u03bb and \u03c3 for T valid steps. The non-differentiability of r due to discrete hyperparameters is not a concern. The derivative of E \u223cp( |\u03c3) [f (\u03bb + ,\u0175 \u03c6 (\u03bb + ))] is estimated during training. The complete algorithm is provided in Algorithm 1 and can be implemented in code. The algorithm involves initializing best-response approximation parameters \u03c6 and hyperparameters \u03bb, and updating them using gradient descent. For regularization schemes where the loss function does not directly depend on \u03bb, reparametrization gradient is used. If the loss function explicitly depends on \u03bb, the REINFORCE gradient estimator is used. The method was applied to convolutional networks and LSTMs, resulting in self-tuning CNNs and self-tuning LSTMs. The method involves updating parameters \u03c6 and hyperparameters \u03bb using gradient descent. It was applied to convolutional networks and LSTMs, resulting in self-tuning CNNs and self-tuning LSTMs. The approach directly affects validation loss and outperforms fixed hyperparameter values. STNs discover schedules for adapting hyperparameters online, leading to improved performance compared to fixed hyperparameters. Hyperparameter optimization methods on CIFAR-10 and PTB datasets involve joint optimization of hypernetwork weights and hyperparameters. STNs do not use a fixed hyperparameter but discover schedules online, outperforming fixed hyperparameters. An ST-LSTM was used to tune output dropout rate on PTB corpus, with the discovered schedule outperforming fixed rates. This improvement is attributed to the schedule, not stochasticity or limited capacity. The schedule discovered by STNs outperforms fixed hyperparameters in optimizing output dropout rate, as shown in Figure 3. This improvement is attributed to the schedule, ruling out stochasticity or limited capacity effects. STNs outperform standard LSTM with perturbations in dropout rate, showcasing their effectiveness in hyperparameter optimization. The schedule discovered by STNs outperforms fixed hyperparameters in optimizing output dropout rate, as shown in Table 2. STNs outperformed perturbation methods in the PTB word-level language modeling task and CIFAR-10 image-classification task. Evidence suggests that the schedule itself, rather than other aspects of STN, was responsible for the performance improvement. Training a standard LSTM from scratch using the schedule resulted in performance close to STN, highlighting the importance of the hyperparameter schedule. The STN schedule discovered by STNs outperforms fixed hyperparameters in optimizing dropout rate. Training a standard LSTM from scratch using the schedule resulted in performance close to STN, highlighting the importance of the hyperparameter schedule. The STN schedule implements a curriculum by using a low dropout rate early in training and gradually increasing it later on for better generalization. The ST-LSTM was evaluated on the PTB corpus with 2-layer LSTM and 650 hidden units per layer. Seven hyperparameters were tuned including variational dropout rates and coefficients for activation regularization. The ST-LSTM model used a 2-layer LSTM with 650 hidden units and 650-dimensional word embeddings. Seven hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. STNs outperformed grid search, random search, and Bayesian optimization, achieving lower validation perplexity more quickly. The STNs outperformed other methods in tuning hyperparameters for ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture. The schedules found for each hyperparameter were nontrivial, with different forms of dropout utilized at various stages of training. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture. Various dropout techniques were used at different training stages, and 15 hyperparameters were considered. STNs were compared to grid search, random search, and Bayesian optimization methods. The study compared STNs to grid search, random search, and Bayesian optimization methods using 15 hyperparameters. STNs found better hyperparameter configurations in less time than other methods, as shown in FIG4 and Table 2. The hyperparameter schedules found by STN are illustrated in FIG3. Bilevel Optimization is also discussed in the context of linear, quadratic, or convex objectives/constraints. The schedules found by the STN are displayed in FIG3. Bilevel Optimization involves linear, quadratic, or convex objectives/constraints. Previous work has used evolutionary techniques and trust-region methods for similar problems. Hypernetworks, introduced by Schmidhuber, map functions to neural net weights, with applications in CNNs. In previous work, Sinha et al. (2013) used evolutionary techniques to estimate best-response functions iteratively. Hypernetworks, introduced by Schmidhuber (1993), map functions to neural net weights, with applications in CNNs. Ha et al. (2016) utilized hypernetworks to generate weights for modern CNNs and RNNs. Gradient-Based Hyperparameter Optimization involves approximating w * (\u03bb 0 ) using w T (\u03bb 0 , w 0 ) after T steps of gradient descent on f with respect to w. Gradient-Based Hyperparameter Optimization involves approximating the best-response using two main approaches. The first approach approximates w * (\u03bb 0 ) by differentiating descent steps to approximate \u2202w * /\u2202\u03bb(\u03bb 0 ). This approach has been used by various researchers. The second approach uses the Implicit Function Theorem to derive \u2202w * /\u2202\u03bb(\u03bb 0 ) under certain conditions, with similar approaches used in different optimization scenarios. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance metrics given hyperparameters. Differentiating gradient descent or training loss with respect to hyperparameters can be challenging, especially as the number of descent steps increases. Implicitly deriving \u2202w * /\u2202\u03bb requires using Hessian-vector products with conjugate gradient solvers to avoid directly computing the Hessian. The number of descent steps increases, requiring Hessian-vector products with conjugate gradient solvers to avoid computing the Hessian directly. Model-Based Hyperparameter Optimization involves Bayesian optimization to model the conditional probability of performance metrics given hyperparameters and dataset. Acquisition function C(\u03bb; p(r|\u03bb, D)) balances exploration and exploitation in choosing the next \u03bb to train on iteratively. Model-free hyperparameter optimization methods like grid search and random search have been advocated, with some extensions like Successive Halving and Hyperband. These approaches do not consider the problem's structure, unlike the method proposed in the current text chunk which aims to balance exploration and exploitation by maximizing an acquisition function. Optimization methods like random search, Successive Halving, and Hyperband adaptively allocate resources using bandit techniques. These model-free approaches perform well in practice and are easy to parallelize. On the other hand, Population Based Training (PBT) considers hyperparameter schedules by training a population of networks in parallel and replacing under-performing networks with better ones. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting their hidden units, allowing for gradient-based optimization to tune various regularization hyperparameters, including discrete ones. This method replaces the population of networks with a single best-response approximation and uses gradients to tune hyperparameters during a single training run. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting their hidden units. This allows for gradient-based optimization to tune various regularization hyperparameters, including discrete ones. STNs discover hyperparameter schedules that outperform fixed hyperparameters and achieve better generalization performance in less time. STNs offer a compelling path towards large-scale, automated hyperparameter tuning for neural networks. The text discusses the benefits of Self-Tuning Networks (STNs) for automated hyperparameter tuning in neural networks. It mentions the support received by the authors and delves into the mathematical aspects of hyperparameter gradients and optimization conditions. The hyperparameter gradient is a sum of validation losses direct and response gradients. By the Implicit Function Theorem, there exists a unique continuously differentiable function that is the unique solution to the optimization problem. The unique solution to the optimization problem involves a continuously differentiable function and the SVD decomposition of the data matrix. The function is simplified by setting u = s Q, leading to a constant Jacobian and a standard L2-regularized least-squares linear problem. The SVD decomposition of X involves orthogonal matrices U and V, with a diagonal matrix D. Simplifying the function y(x; w) by setting u = s Q results in a constant Jacobian. The optimal solutions for regularized and unregularized versions of the problem are given by specific equations involving change-of-basis matrix Q0 and principal components matrix s0. The optimal solution to the unregularized version of Problem 19 is given by Q0s0 = u*, where s0 = D^-1U^t. Best-response functions Q*(\u03bb) = \u03c3(\u03bbv + c) row Q0 and s*(\u03bb) = s0 are chosen. The function f is quadratic, represented by matrices A, B, C, d, and e. The function f is quadratic, represented by matrices A, B, C, d, and e. By assuming \u22022f/\u2202w2 = 0, we find the optimal solution to be Q0s0 = u*, where s0 = D^-1U^t. The best-response functions Q*(\u03bb) = \u03c3(\u03bbv + c) row Q0 and s*(\u03bb) = s0 are chosen. The derivative is set to 0 and second-order sufficient conditions are used to find the solution. The function f is quadratic, represented by matrices A, B, C, d, and e. By assuming \u22022f/\u2202w2 = 0, the optimal solution is found to be Q0s0 = u*. The best-response functions Q*(\u03bb) = \u03c3(\u03bbv + c) row Q0 and s*(\u03bb) = s0 are chosen. Differentiating f and setting the derivatives equal to 0 gives the best-response Jacobian \u2202w*/\u2202\u03bb(\u03bb) as given by Equation 34. Substituting U = C^-1B into the equation gives the solution w*. The model parameters were updated, but hyperparameters were not. Training was stopped when the learning rate fell below 0.0003. Variational dropout was tuned for the input, hidden state, and output of the LSTM. Embedding dropout was also tuned to remove certain words from sequences. The training involved tuning variational dropout on the input, hidden state, and output of the LSTM, as well as embedding dropout. DropConnect was used to regularize the hidden-to-hidden weight matrix. Activation regularization penalized large activations, while temporal activation regularization acted as a slowness regularizer. In the CNN experiments, a single DropConnect rate per mini-batch was sampled, along with activation regularization (AR) and temporal activation regularization (TAR) being used. The hyperparameter ranges for the baselines were [0, 0.95] for dropout rates and [0, 4] for scaling coefficients \u03b1 and \u03b2. The baseline CNN was trained using SGD with initial learning rate 0.01 and momentum 0.9 on mini-batches of size 128, with 20% of the training data held out for validation. In the CNN experiments, the baseline CNN was trained using SGD with initial learning rate 0.01 and momentum 0.9 on mini-batches of size 128. 20% of the training data was held out for validation. The hyperparameters for the baselines included dropout rates in the range [0, 0.95] and scaling coefficients \u03b1 and \u03b2 in the range [0, 4]. The learning rate was decayed by 10 if the validation loss did not decrease for 60 epochs, and training ended if the learning rate fell below 10^-5 or validation loss did not decrease for 75 epochs. For the ST-CNN experiments, hyperparameters were optimized using Adam with a learning rate of 0.003. Training alternated between best-response approximation and hyperparameters following the ST-LSTM schedule. A warm-up period of five epochs was used for model parameters, with an entropy weight of \u03c4 = 0.001. Cutout length was limited to {0, 24} with a specified number of cutout holes. The ST-CNN experiments used a warm-up period of five epochs with an entropy weight of \u03c4 = 0.001. Cutout length was restricted to {0, 24} with a specified number of cutout holes. The model showed robustness to hyperparameter initialization, with low regularization aiding optimization in the early epochs. Connections were drawn between hyperparameter schedules and curriculum learning. The ST-CNN model demonstrated robustness to hyperparameter initialization, with low regularization benefiting early optimization. Curriculum learning involves optimizing a sequence of functions ordered by difficulty, gradually increasing a parameter \u03bb from 0 to 1 to aid optimization and generalization. In this section, hyperparameter schedules are explored as a form of curriculum learning. By gradually increasing \u03bb from 0 to 1 while optimizing C 0 (w), optimization and generalization can be improved. Grid searches were conducted to analyze the effects of different hyperparameter settings during training, showing that greedy schedules can outperform fixed values. A grid search over input and output dropout values was performed, with validation perplexity measured in each epoch. Grid searches were conducted to understand the effects of different hyperparameter settings throughout training. The validation perplexity achieved by various combinations of input and output dropout values at different epochs was measured. It was observed that smaller dropout rates were optimal at the start of training, while larger rates performed better as training progressed. Additionally, a simple example demonstrated the benefits of greedy hyperparameter schedules by constructing a dropout schedule based on values that yielded the best validation perplexity at each epoch. The best validation performance is achieved with larger dropout rates over more epochs. A dropout schedule constructed from the best output dropout values at each epoch leads to better generalization than fixed hyperparameter values. Starting with small dropout values and increasing them later in training results in a fast decrease in validation perplexity and better overall performance. The schedule achieves a fast decrease in validation perplexity by using small dropout values at the start of training and larger dropout values later. PyTorch code listings for best-response layers used in ST-LSTMs and ST-CNNs are provided, along with optimization steps for the training and validation sets."
}