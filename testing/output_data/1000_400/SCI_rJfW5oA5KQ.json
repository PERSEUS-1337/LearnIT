{
    "title": "rJfW5oA5KQ",
    "content": "Recent works have shown that Generative Adversarial Networks (GANs) suffer from lack of diversity or mode collapse, as powerful discriminators cause overfitting while weak discriminators cannot detect mode collapse (Arora et al., 2017a). In contrast to previous findings on GANs suffering from mode collapse, this paper demonstrates that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by using discriminators with strong distinguishing power against specific generator classes. The designed discriminators can approximate the Wasserstein distance and/or KL-divergence, ensuring that the learned distribution closely matches the true distribution and does not drop modes. The paper shows that GANs can learn distributions in Wasserstein distance with polynomial sample complexity using discriminators that approximate the Wasserstein distance and/or KL-divergence. This ensures the learned distribution closely matches the true distribution and does not drop modes, addressing the lack of diversity in GANs. Various ideas have been proposed to improve the quality of learned distributions and training stability in GANs. The great success of Generative Adversarial Networks (GANs) in generating high-quality samples has led to various proposals to enhance distribution quality and training stability. Recent research has highlighted concerns about mode collapse and lack of diversity in GANs' learned distributions. Recent work has shown that distributions learned by GANs can suffer from mode collapse or lack of diversity, missing significant modes of the target distribution. This paper suggests that designing discriminators with strong distinguishing power against specific types of generators can alleviate mode collapse. The focus is on the Wasserstein GAN formulation, particularly the F-Integral Probability Metric between distributions. The paper focuses on the Wasserstein GAN formulation, specifically the F-Integral Probability Metric (F-IPM) between distributions. It sets up generators and discriminators to learn data distribution by solving an objective equation. The use of parametric families like neural networks helps approximate Lipschitz functions for optimization via gradient-based algorithms. In practice, parametric families like neural networks are used to approximate Lipschitz functions for optimizing the F-Integral Probability Metric (F-IPM) via gradient-based algorithms. One main concern with GANs is \"mode-collapse,\" where the learned distribution generates high-quality but low-diversity examples due to the weakness of IPM compared to the Wasserstein-1 distance. The problem in GANs arises from the weakness of IPM compared to Wasserstein-1 distance. Increasing the strength of the discriminator to larger families like all 1-Lipschitz functions is a proposed solution, but it is noted that Wasserstein-1 distance lacks good generalization properties. The Wasserstein-1 distance lacks good generalization properties, as pointed out by Arora et al. Powerful discriminators in GANs lead to overfitting, while weak discriminators result in diversity issues. IPM does not approximate the Wasserstein distance effectively. This paper addresses the challenge in GAN theories where powerful discriminators cause overfitting and weak discriminators lead to diversity issues. The proposed solution involves designing a strong discriminator class F against a specific generator class G, with restricted approximability w.r.t. G and the data distribution p. The paper focuses on designing a strong discriminator class F against a specific generator class G with restricted approximability w.r.t. G and the data distribution p, aiming to approximate the Wasserstein distance W1 for the data distribution p and any q \u2208 G. The paper discusses designing a discriminator class F with restricted approximability to approximate the Wasserstein distance W1 for the data distribution p and any q \u2208 G, aiming to avoid mode collapse. A discriminator class F with restricted approximability can prevent mode collapse by ensuring that if the IPM between p and q is small, then p and q are also close in Wasserstein distance. This allows for passing from population-level guarantees to empirical-level guarantees, relating the Rademacher complexity of F to the statistical properties of Wasserstein GANs. The text discusses the statistical properties of Wasserstein GANs, focusing on the diversity and generalization properties of the distance W F. It introduces a theoretical framework for GANs with polynomial samples and techniques for designing discriminator class F to prevent mode collapse. The paper explores generator classes like mixtures of Gaussians and distributions generated by invertible neural networks. The paper develops techniques for designing discriminator class F with restricted approximability for various generator classes, including mixtures of Gaussians, exponential families, and distributions generated by invertible neural networks. It shows that properly chosen F provides diversity guarantees and discusses using neural networks for Gaussians and linear combinations for exponential families. Additionally, it studies distributions generated by invertible neural networks. The paper discusses designing discriminator class F with restricted approximability for different generator classes, such as Gaussians, exponential families, and distributions from invertible neural networks. It highlights using neural networks for Gaussians and linear combinations for exponential families, and explores the generation of distributions by invertible neural networks. The results suggest that certain neural network discriminators can produce an exponentially large number of modes in the learned distribution q. The paper discusses designing discriminator class F with restricted approximability for different generator classes, such as Gaussians, exponential families, and distributions from invertible neural networks. It highlights using neural networks for Gaussians and linear combinations for exponential families, and explores the generation of distributions by invertible neural networks. The results suggest that certain neural network discriminators can produce an exponentially large number of modes in the learned distribution q. The invertibility assumption has limitations as it only produces distributions supported on the entire space. The KL-divergence is not the proper measurement of statistical distance for cases where both p and q have low-dimensional supports. The KL-divergence is not suitable for measuring statistical distance when distributions have low-dimensional supports. The paper focuses on approximating Wasserstein distance using IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE in learning such distributions. Tools are developed for approximating the log-density of neural network generators, demonstrated in synthetic experiments. The paper demonstrates the advantage of GANs over MLE in learning distributions with low-dimensional supports by approximating Wasserstein distance using IPMs. Tools are developed for approximating the log-density of neural network generators, showing potential for IPM as an alternative measure of diversity and quality in complex settings. The test IPM could be an alternative for measuring diversity and quality in complex settings when KL-divergence or Wasserstein distance is not measurable. The lack of diversity in real experiments may be due to sub-optimality of optimization rather than statistical inefficiency.GANs distributions may not be well-optimized, leading to distinguishable differences from the data distribution. Various empirical tests have been developed to assess diversity, memorization, and generalization in GANs, indicating that lack of diversity is a common issue. Arora et al. (2017a; b) formalized theoretical sources of mode collapse from a weak discriminator. Various architectures and algorithms have been proposed to address mode collapse in GANs, with some success. Feizi et al. (2017) demonstrated guarantees when training GANs with quadratic discriminators. However, there are no proven solutions to this issue in a broader context. Zhang et al. (2017) highlighted the effectiveness of the IPM in addressing mode collapse. Feizi et al. (2017) provided guarantees for training GANs with quadratic discriminators. Zhang et al. (2017) showed the effectiveness of the IPM in addressing mode collapse. This work extends statistical guarantees in Wasserstein distance for distributions like injective neural network generators. Liang (2017) also considers GANs. Our Section 4.1 extends statistical guarantees in Wasserstein distance for distributions like injective neural network generators. Liang (2017) discusses GANs in a non-parametric setup, highlighting that the sample complexity improves with the smoothness of the generator family. The invertible generator structure in Flow-GAN (Grover et al., 2018) addresses issues with GAN training on real datasets. Our theoretical results and experiments demonstrate successful GAN training in terms of the IPM. Our theoretical results and experiments show that successful GAN training implies learning in KL-divergence when the data distribution can be generated by an invertible neural net. This suggests that real data cannot be generated by an invertible neural network. Additionally, if the data can be generated by an injective neural network, we can bound the closeness between the learned distribution and the true distribution in Wasserstein distance. Our theory suggests that successful GAN training involves learning in KL-divergence when data can be generated by an invertible neural network. If data can be generated by an injective neural network, we can bound the closeness between learned and true distributions in Wasserstein distance. The notion of IPM includes statistical distances like TV and Wasserstein-1 distance. KL divergence and Wasserstein-2 distance are also important measures between distributions. The F-IPM, also known as the neural net IPM, considers various distances between distributions such as KL divergence and Wasserstein-2 distance. The Rademacher complexity plays a key role in the generalization of the IPM, particularly in the training IPM loss for Wasserstein GAN. The Rademacher complexity is crucial for generalization in the training IPM loss of the Wasserstein GAN. It governs the generalization of the IPM, considering distances like KL divergence and Wasserstein-2 distance. One-layer neural networks with ReLU are used for discriminators with restricted approximability for simple parameterized distributions like Gaussian distributions and mixtures of Gaussians. One-layer neural networks with ReLU activation can effectively distinguish Gaussian distributions with restricted approximability guarantees. The discriminators are designed for simple parameterized distributions like Gaussian distributions, exponential families, and mixtures of Gaussians. The set of Gaussian distributions with bounded mean and well-conditioned covariance is considered, with given hyper-parameters D, \u03c3 min, and \u03c3 max. The discriminators induce an IPM W F with restricted approximability w.r.t. G. The IPM W F induced by discriminators has restricted approximability w.r.t. G. The set of one-layer neural networks has restricted approximability w.r.t. Gaussian distributions in G. The upper and lower bounds differ by a factor of 1/ \u221a d. The 1/ \u221a d factor is not improvable without using more sophisticated functions than Lipschitz functions of one-dimensional projections of x. The proof is deferred to Section B.1. The IPM induced by discriminators has restricted approximability w.r.t. G. The upper bound is on the order of W1(p, q)/ \u221a d for spherical covariances. Extension to mixture of Gaussians and exponential families is discussed, with linear combinations of sufficient statistics as discriminators with restricted approximability. The discriminator family for an exponential family G is defined as linear functionals over features T(x). If the log partition function log Z(\u03b8) satisfies certain conditions, then there are bounds on the Rademacher complexity. The assumptions require positive lower and global upper bounds on the curvature. The Rademacher complexity bound for an exponential family G is determined by the log partition function log Z(\u03b8) satisfying specific conditions. The curvature must have a positive lower bound and a global upper bound. Geometric assumptions on sufficient statistics are necessary for the Wasserstein distance, which depends on the underlying geometry of x. Discriminators with restricted approximability for neural net generators are designed in this section. In this section, discriminators with restricted approximability for neural net generators are designed. The generators considered are parameterized by invertible neural networks, with a focus on invertible neural networks generators and injective neural networks generators. In this section, invertible neural networks are used as generators with non-spherical variances to model data around a \"k-dimensional manifold\" with noise. The family of neural networks G consists of standard feedforward nets with invertible parameters. The family of invertible neural networks G \u03b8 models data around a k-dimensional manifold with noise. The networks consist of standard feedforward nets with invertible parameters and activation functions. The hidden factors have a standard deviation within the range of \u03b4 to 1, making the neural net invertible. Neural networks G \u03b8 are parameterized by \u03b8 = (W i , b i ) i\u2208[ ] from set DISPLAYFORM2 with activation function \u03c3 being twice-differentiable. The hidden factors' standard deviation \u03b3 i \u2208 [\u03b4, 1] ensures invertibility. Assumptions are needed on generator networks to prevent pseudo-random functions. For any \u03b8 \u2208 \u0398, log p \u03b8 can be computed by a neural network with at most + 1 layers, O( d 2 ) parameters, and specific activation functions. Neural networks G \u03b8 with specific activation functions can compute log p \u03b8 using at most + 1 layers and O( d 2 ) parameters. The family F of neural networks contains functions log p \u2212 log q for p, q \u2208 G. The exact form of family F may not be crucial in practice, as other neural nets may also provide good approximations. The family F of neural networks may not be crucial in practice as other neural nets can also approximate log p \u2212 log q. The proof involves the change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian can be computed by adding a bias on the final output layer, freeing us from structural assumptions on weight matrices. The proof of Lemma 4.1 is deferred to Section D.2. Theorem 4.2 states that the discriminator class F has restricted approximability with respect to the set of invertible-generator distributions G. The proof of this theorem utilizes a lemma that relates the KL divergence to the IPM when log densities exist and belong to the family of discriminators. The proof of Theorem 4.2 utilizes a lemma that connects the KL divergence to the IPM when log densities exist in the discriminator family. Lemma 4.3 states conditions for F and shows a proof sketch, deferring the full proof. The discriminator class chosen in Lemma 4.1 implements log p - log q for any p, q \u2208 G, aiming to bound the Wasserstein distance. The proof of Theorem 4.2 connects the KL divergence to the IPM when log densities exist in the discriminator family. Lemma 4.3 provides conditions for F and a proof sketch, deferring the full proof. It aims to bound the Wasserstein distance by implementing log p - log q for any p, q \u2208 G. To establish the lower bound, transportation inequalities by Bobkov-G\u00f6tze and Gozlan are used, while for the upper bound, two workarounds are suggested. The upper bound (2) would have been immediate if functions in F are Lipschitz globally in the whole space. Two workarounds are provided - either a truncation argument for a W 1 bound or a W 2 bound with Lipschitz constant growing linearly in x 2. This extends the result in (Polyanskiy & Wu, 2016). Combining restricted approximability and generalization bound, training success with small expected IPM implies closeness of estimated distribution q to true distribution p in Wasserstein distance. By combining restricted approximability and generalization bound, successful training with small expected IPM implies that the estimated distribution q is close to the true distribution p in Wasserstein distance. The training error is measured by the expected IPM over the randomness of the learned distributions, and designing efficient algorithms to achieve a small training error is an open question for future work. This section focuses on injective neural network generators that generate distributions in a low dimensional space. In this section, injective neural network generators are discussed, which generate distributions on a low dimensional manifold. The goal is to design a novel divergence between distributions that can be optimized as IPM, despite technical challenges. The neural nets generate distributions on a k-dimensional manifold in R^d, and a variant of IPM is designed to approximate the divergence. In this section, a novel divergence between distributions is designed that can be optimized as IPM. The goal is to approximate the Wasserstein distance using a smoothed F-IPM with a Gaussian distribution. Theorem 4.5 states that for a certain discriminator class, the F-IPM approximates the Wasserstein distance for distributions generated by neural nets. Theorem 4.5 introduces a discriminator class F that approximates the Wasserstein distance for distributions generated by neural nets. It guarantees that mode collapse will not occur as long as the discriminator family F has restricted approximability with respect to the generator. The theorem implies that if the distance between two distributions is small for a polynomial number of iterations, mode collapse will not occur. The discriminator class F must have restricted approximability with respect to the generator to prevent mode collapse. The IPM W_F(p, q) is bounded by the Wasserstein distance W_1(p, q) under restricted approximability. Specific discriminator classes are designed to ensure this, with synthetic experiments confirming the theory's consistency in practice. In synthetic experiments, the theory of restricted approximability in GAN training is confirmed. The correlation between IPM and Wasserstein / KL divergence suggests that optimization difficulty, not statistical inefficiency, may be the main challenge in GAN training. The difficulty of GAN training may stem from optimization challenges rather than statistical inefficiency. Experiments show good statistical behaviors on \"typical\" discriminator classes, with correlations between IPM and Wasserstein / KL divergence. Synthetic experiments with WGANs learning various curves in two dimensions support the theory of restricted approximability in GAN training. In synthetic experiments with WGANs, it was shown that the IPM is correlated with the KL divergence and the Wasserstein distance. The experiments involved training GANs to learn the unit circle and a \"swiss roll\" curve, demonstrating the ability of WGANs to learn distributions well. These results provide evidence for the theory of restricted approximability in GAN training. The Wasserstein distance is used to measure the quality of the learned generator in WGANs. Results show that WGANs can effectively learn distributions like the unit circle and Swiss roll curve. The generator and discriminator architectures consist of two hidden layers with specific neuron configurations. The RMSProp optimizer is used with specific learning rates for both the generator and discriminator. Comparisons are made using two metrics. The generator and discriminator architectures in WGANs have specific neuron configurations. The RMSProp optimizer is used with specific learning rates for both. Two metrics are used for comparisons: neural net IPM W F (p, q) and Wasserstein distance W 1 (p, q). The Wasserstein distance W 1 (p, q) is computed on fresh batches using the POT package. It is a good proxy for the true Wasserstein distance and correlates well with the neural net IPM. The learned generator closely matches the ground truth distribution at iteration 10000, but at iteration 500, the generators have not fully learned the true distributions yet. Sample complexity bounds for learning various distributions using GANs are presented. The generators in GANs have not fully learned the true distributions at iteration 500, with large IPM and Wasserstein distance. Sample complexity bounds for learning distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence are presented, with a focus on designing discriminators with restricted approximability tailored to the generator class. Future work aims to extend techniques to other distribution families with tighter sample complexity bounds. The text discusses designing discriminators tailored to the generator class in GANs to avoid mode collapse and improve generalization. The goal is to extend these techniques to other distribution families with tighter sample complexity bounds. The text discusses designing discriminators tailored to the generator class in GANs to avoid mode collapse and improve generalization. It extends these techniques to other distribution families with tighter sample complexity bounds. Taking expectation over p_n yields a bound, which is then used to establish upper and lower bounds for the discriminator family. The text discusses designing discriminators tailored to the generator class in GANs to avoid mode collapse and improve generalization. It extends these techniques to other distribution families with tighter sample complexity bounds. The mean distance is computed using a linear discriminator as the sum of two ReLU discriminators. Neuron distance between two Gaussians is calculated, and perturbation bounds are used for further analysis. The text discusses using the W 2 distance to bridge the KL and F-distance for Gaussian distributions, with a lower bound established using perturbation bounds and the mean difference. This approach aims to improve generalization in GANs and other distribution families. The text discusses using the W 2 distance to bridge the KL and F-distance for Gaussian distributions, with a lower bound established using perturbation bounds and the mean difference. This approach aims to improve generalization in GANs and other distribution families. The lower bound holds with c = 1/(2 \u221a 2\u03c0) for two Gaussians distributions, and the growth of \u2207 log p 1 (x) 2 is bounded. The Rademacher contraction inequality is used to bound the right-hand side, leading to KL bounds. The text discusses using the W 2 distance to bridge the KL and F-distance for Gaussian distributions, with a lower bound established using perturbation bounds and the mean difference. The Rademacher contraction inequality is used to bound the right-hand side, leading to KL bounds. The exponential family properties are also considered, with bounds derived based on assumptions about the gradient of the function. The text discusses Wasserstein bounds and Rademacher complexity for Gaussian mixtures, using perturbation bounds and mean difference to establish lower bounds. The family F is suitable for learning mixture of k Gaussians, with restricted approximability and generalization properties. The text discusses the family F suitable for learning mixtures of k Gaussians, with restricted approximability and generalization properties. It utilizes Gaussian concentration results and regularity properties to establish upper and lower bounds. The text discusses the implementation of KL divergence by the family F for learning mixtures of k Gaussians. It establishes upper and lower bounds using Gaussian concentration results and regularity properties. The Rademacher complexity of f \u03b8 is bounded to ensure generalization. The text discusses bounding the Rademacher complexity of f \u03b8 for generalization in learning mixtures of k Gaussians. It utilizes Gaussian concentration results and regularity properties to establish upper and lower bounds. The Rademacher process Y \u03b8 is shown to be Lipschitz in \u03b8 and a one-step discretization bound is used. The expected supremum over a covering set is then bounded. The text discusses upper bounding the Rademacher complexity of f \u03b8 for generalization in learning mixtures of k Gaussians. It utilizes Gaussian concentration results and regularity properties to establish bounds. The Rademacher process Y \u03b8 is shown to be Lipschitz in \u03b8. The text discusses upper bounding the Rademacher complexity of f \u03b8 for generalization in learning mixtures of k Gaussians. It utilizes Gaussian concentration results and regularity properties to establish bounds. By the 1-step discretization bound and combining eq. (19) and appendix C.2, we get that Choosing \u03b5 = c/n for sufficiently small c gives that Theorem D.2 (Upper bounding f -contrast by Wasserstein) states conditions for two distributions on R d with positive densities. The text provides a proof for bounding the Wasserstein distance between two distributions on R^d. It involves truncation arguments, Cauchy-Schwarz bounds, and a coupling to establish the desired inequalities. The proof also includes a straightforward extension of a previous proposition for completeness. The proof presented here establishes bounds on the Wasserstein distance between two distributions in R^d using truncation arguments, Cauchy-Schwarz bounds, and a coupling. It also discusses representing log p \u03b8 (x) with a neural network and the computation of the inverse of x = G \u03b8 (z). The text discusses using a neural network to compute the log density of a distribution and the log determinant of the Jacobian. It involves adding layers to the network and computing the inverse of x = G \u03b8 (z). The text discusses using a neural network to compute the log density of a distribution and the log determinant of the Jacobian. It involves adding layers to the network and computing the inverse of x = G \u03b8 (z). The log determinant of the Jacobian is calculated recursively, and a neural network is constructed to compute log p \u03b8 (x) with a limited number of layers and parameters. The theorem is supported by lemmas showing a lower bound and the satisfaction of the Gozlan condition for any \u03b8 \u2208 \u0398. The text discusses using a neural network to compute the log density of a distribution and the log determinant of the Jacobian. It involves adding layers to the network and computing the inverse of x = G \u03b8 (z). The log determinant of the Jacobian is calculated recursively, and a neural network is constructed to compute log p \u03b8 (x) with a limited number of layers and parameters. The theorem is supported by lemmas showing a lower bound and the satisfaction of the Gozlan condition for any \u03b8 \u2208 \u0398. The restricted approximability bound in terms of the W 2 distance is proven by combining three lemmas. The text discusses the use of a neural network to compute the log density of a distribution and the log determinant of the Jacobian. It involves adding layers to the network and computing the inverse of x = G \u03b8 (z). The log determinant of the Jacobian is calculated recursively, and a neural network is constructed to compute log p \u03b8 (x) with a limited number of layers and parameters. The Gozlan condition is satisfied, and the W 2 bound is shown by upper bounding the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The text discusses the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 in the context of using a neural network to compute the log density of a distribution and the log determinant of the Jacobian. The W 2 bound is shown by bounding the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398, and the W 1 bound is also discussed. The text discusses applying Theorem D.2(c) to obtain a bound for X 2, and then focuses on the W 1 bound. By choosing D = K \u221a d, a tail bound is derived. Reparameterizing the log-density neural network leads to a form that belongs to a certain set. The Rademacher complexity of the network is discussed, along with an additional re-parametrization. The text discusses reparametrizing the log-density neural network to belong to a certain set and the Rademacher complexity of the network. An additional re-parametrization is done, leading to a one-step discretization bound. The text discusses creating a parameter K for the sum of log det(W i) and defining a metric for reparametrized \u03b8. It introduces the Rademacher process and one-step discretization bound. Two lemmas are presented for discretization error and expected max over a finite set. Substituting these lemmas into the bound, a final bound is derived for all \u03b5 and \u03bb values. The text discusses deriving a final bound for all \u03b5 and \u03bb values by substituting lemmas into the bound. It focuses on the Lipschitzness of hidden layers and generalization error. The text discusses the Lipschitzness of hidden layers in the inverse network G^-1, showing bounds for each layer and verifying results through induction. It also addresses the Lipschitzness of the log \u03c3^-1 term and the quadratic term, ultimately deriving a final bound for all \u03b5 and \u03bb values. The text discusses verifying results for each layer in the inverse network G^-1, addressing Lipschitzness of log \u03c3^-1 and quadratic terms, and deriving bounds for all \u03b5 and \u03bb values. The text discusses the Lipschitzness of log \u03c3^-1 and quadratic terms in the inverse network G^-1, deriving bounds for all \u03b5 and \u03bb values. The terms in the independent sums are shown to be Cd-sub-Gaussian and subexponential, with bounded means and parameters. The text discusses the Lipschitzness of log \u03c3^-1 and quadratic terms in the inverse network G^-1, deriving bounds for all \u03b5 and \u03bb values. The parameter is 1/\u03b4 2 times the sub-Gaussian parameter of h, with an upper bound on the parameter K. By multiplying by \u03b5 i and summing over n, Y \u03b8 is shown to be mean-zero sub-exponential. The covering number of \u0398 is bounded by the product of independent covering numbers. Jensen's inequality is used to bound the expected maximum, leading to a quantitative statement of the theorem. The covering number of \u0398 is bounded by the product of independent covering numbers. Using Jensen's inequality and a bound from appendix D.6.2, for any \u03bb \u2264 \u03bb 0 \u03b4 2 n, the theorem is stated quantitatively by specifying relevant quantities of the generator class. The distribution p \u03b2 \u03b8 is obtained by adding Gaussian noise with variance \u03b2 2 to a sample from G \u03b8 and truncating it to a high-probability region. The distribution p \u03b2 \u03b8 is obtained by adding Gaussian noise with variance \u03b2 2 to a sample from G \u03b8 and truncating it to a high-probability region in the latent space and observable domain. Regularity conditions are introduced for the family of generators G, including bounds on partial derivatives of f and maximum values of certain functions. The family of generators G is subject to regularity conditions, including bounds on partial derivatives of f and maximum values of certain functions. The main theorem states that for certain F, d, F approximates the Wasserstein distance. Theorem E.1 states that if the generator class G satisfies the assumption E.1 and F is defined as in Theorem E.2, then F approximates the Wasserstein distance. The main theorem states that for certain F and d, F approximates the Wasserstein distance. Theorem E.1 shows that if the generator class G satisfies assumption E.1 and F is defined as in Theorem E.2, then F approximates the Wasserstein distance. The proof relies on a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. Theorem E.2 states that for \u03b2 = O(poly(1/d)), there exists a family of neural networks F that can approximate the log density of p \u03b2 for every distribution p \u2208 G. The approach involves approximating p \u03b2 (x) using Laplace's method of integration, with N in F satisfying certain conditions. Theorem E.1 is proven by approximating p \u03b2 (x) using Laplace's method of integration. Neural networks N1 and N2 in F approximate log p \u03b2 and log q \u03b2, satisfying certain conditions. Theorem E.2 states the existence of neural networks N1 and N2 in F that approximate log p \u03b2 and log q \u03b2. By setting f = N1(x) \u2212 N2(x), a lower bound is proven using the Bobkov-G\u00f6tze theorem. For the upper bound, setting \u03b2 = W1/6 yields the necessary bound. The optimal coupling C of p, q induces coupling Cz on the latent variable z in p, q, leading to the claim. Theorem E.2 establishes the existence of neural networks N1 and N2 in F approximating log p \u03b2 and log q \u03b2. By setting \u03b2 = W 1/6, a necessary bound is obtained. The optimal coupling C of p, q induces coupling Cz on the latent variable z, leading to the generalization claim. The proof of Theorem E.2 is detailed in the following sections. The proof of Theorem E.2 involves establishing neural networks N1 and N2 in F that approximate log p \u03b2 and log q \u03b2, with a necessary bound set at \u03b2 = W 1/6. The proof proceeds with helper lemmas and induction steps to show the existence of estimates \u0125 i. The proof involves establishing neural networks N1 and N2 in F that approximate log p \u03b2 and log q \u03b2 with a bound set at \u03b2 = W 1/6. The induction steps show the existence of estimates \u0125 i and the Lipschitz constant of the neural network is discussed. The algorithm is presented and its approximation capabilities are proven. The algorithm presented approximates the integral and can be implemented by a small, Lipschitz network. It involves calculating gradients and finding matrices with separated eigenvalues. The algorithm approximates the integral using a small, Lipschitz network by calculating gradients and finding matrices with separated eigenvalues. The output of the \"invertor\" circuit is used to calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91), and approximate eigenvector/eigenvalue pairs of H + E i. The algorithm approximates the integral using a small, Lipschitz network by calculating gradients and finding matrices with separated eigenvalues. The output of the \"invertor\" circuit is used to calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91), and approximate eigenvector/eigenvalue pairs of H + E i. The matrices E i are chosen from a \u03b2 2 -net of matrices with bounded spectral norm. There exist matrices E 1 , E 2 , . . . , E r , where r = \u2126(d log(1/\u03b2)), such that at least one of the matrices M + E i has eigenvalues that are \u2126(\u03b2)-separated. The algorithm approximates integrals using a small, Lipschitz network to find matrices with separated eigenvalues. In WGAN experiments, invertible neural net generators and discriminators are used to compute KL divergence and demonstrate correlation with empirical IPM WF(p, q) on synthetic data. In WGAN experiments, invertible neural net generators and discriminators are utilized to compute KL divergence and show correlation with empirical IPM WF(p, q) on synthetic data. The data is generated from a ground-truth invertible neural net generator, where X = G \u03b8 (Z) and Z is a spherical Gaussian. Leaky ReLU with negative slope 0.5 is used as the activation function \u03c3, and the weight matrices of the layers are well-conditioned. The text discusses the use of invertible neural net generators and discriminators in training processes. The generator function is defined as X = G \u03b8 (Z), with Z being a spherical Gaussian. The Leaky ReLU activation function with negative slope 0.5 is used, and weight matrices are well-conditioned. The discriminator architecture is chosen based on restricted approximability guarantee. Constraints are added to parameters, and training involves generating batches from both ground-truth and trained generators to solve a min-max problem. The text discusses using invertible neural net generators and discriminators in training processes. The generator is modeled as a trainable one-hidden-layer neural network mapping reals to reals, with constraints on parameters. Training involves generating batches from both ground-truth and trained generators to solve a min-max problem in the Wasserstein GAN formulation. Evaluation metrics include computing KL divergence between true and learned generators. The RMSProp optimizer is used as the update rule. Evaluation metrics include KL divergence, training loss, and neural net IPM. The KL divergence is considered a strong criterion for distributional closeness. The training loss is the unregularized GAN loss during training, while the neural net IPM is reported separately. During GAN training, the unregularized GAN loss is carefully balanced between discriminator and generator steps. A separately optimized WGAN loss is occasionally reported, where the generator is fixed and the discriminator is trained without regularization. This approach aims to find an approximate maximizer for the contrast, with the loss serving as an approximation of the true W F. The theory suggests that WGAN can learn the true generator in KL divergence, with the F-IPM indicative of this divergence during evaluation. In experiments testing GAN training methods, a two-layer neural net is used as the generator in 10 dimensions. The discriminator is trained with Vanilla WGAN or WGAN-GP, both starting from 6 random initializations. Results are shown in FIG5, indicating the effectiveness of the methods. The discriminator is trained using Vanilla WGAN or WGAN-GP with a fixed ground-truth generator. Results show that WGAN training with a restricted discriminator design can learn the true distribution with low KL divergence. The addition of a gradient penalty improves optimization significantly. The addition of a gradient penalty improves optimization significantly in training the discriminator with Vanilla WGAN or WGAN-GP. The KL divergence is a strong metric between distributions, indicating that GANs are finding the true distribution without mode collapse. The W F curve can serve as a good metric for monitoring convergence, correlating well with the KL-divergence. Testing with vanilla fully-connected discriminator nets also shows good correlation with the KL-divergence. The training loss curve shows the correlation between IPM and KL-divergence. Testing with vanilla fully-connected discriminator nets also demonstrates good correlation with KL-divergence. The inferior performance of the WGAN-Vanilla algorithm in KL divergence does not stem from the statistical properties of GANs but rather from training performance in terms of IPM convergence. Removing optimization effects, the correlation between perturbations and KL divergence is directly tested by comparing it with neural net IPM on pairs of perturbed generators. In this section, the correlation between perturbations and KL divergence is directly tested by comparing it with neural net IPM on pairs of perturbed generators. The KL divergence and neural net IPM between perturbed generators show a clear positive correlation, supporting the theory that the neural net distance scales linearly with the KL divergence. The correlation between perturbations and KL divergence is tested by comparing it with neural net IPM on pairs of perturbed generators. Results show a clear positive correlation, with most points falling around the line W F = 100D kl. Outliers with large KL divergence occur when perturbations are too large, causing weight matrices to become poorly conditioned. Experiments with vanilla fully-connected discriminator nets also show convergence in KL divergence. Results from experiments using vanilla fully-connected discriminator nets show that the generators converge well in KL divergence, although the correlation is slightly weaker compared to the setting with restricted approximability. This suggests that vanilla discriminator structures may be satisfactory for generating good results, but specific designs could enhance the quality of the distance W F. The left-most figure displays the KL divergence between the true and learned distributions, the middle shows the estimated IPM between the distributions, and the right illustrates the training loss. The experiments using vanilla fully-connected discriminator nets show that generators converge well in KL divergence. Specific designs could enhance the quality of the distance W F. The estimated IPM in evaluation correlates well with the KL-divergence. Correlation between KL and neural net IPM is computed with vanilla fully-connected discriminators."
}