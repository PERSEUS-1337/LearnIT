{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging without supervision. A non-statistical framework is proposed, based on identifying a modular organization of the network through counterfactual manipulations. Modularity between groups of channels is achieved to some extent on various generative models, enabling targeted interventions on image datasets for applications like style transfer and assessing robustness to contextual changes. Deep generative models, such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE), have been successful in creating realistic images in various domains. Efforts have been made to produce disentangled latent representations for controlling transformations. This modular organization allows for targeted interventions on image datasets, enabling applications like style transfer and assessing robustness to contextual changes. Efforts have been made to create generative models like GAN and VAE that produce disentangled latent representations for controlling interpretable properties of images. However, current models lack a mechanistic or causal understanding where interpretable properties cannot be attributed to specific parts of the network architecture. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, supporting adaptability and robust decision making. Leveraging trained deep generative architectures for such extrapolations remains an open problem due to non-linearities and high dimensionality. In this paper, a causal framework is proposed to explore modularity in generative models, focusing on the principle of Independent Mechanisms. This principle allows for direct interventions in the network without affecting other causal mechanisms, enabling the modification of individual mechanisms in generating data. The text discusses the use of causal mechanisms in generative models to assess how individual mechanisms can be modified without influencing each other. Counterfactuals are used to evaluate the impact of specific internal variables on deep generative models, with a focus on disentanglement in a causal framework. The analysis is done through unsupervised counterfactual manipulations on VAEs and GANs. In this study, the role of internal variables in deep generative models is assessed using counterfactuals. The focus is on disentanglement in a causal framework, analyzed through unsupervised manipulations on VAEs and GANs. The study shows how these models exhibit modularity in their hidden units, allowing for editing of generated images. The work is related to interpretability in convolutional neural networks, with a different approach needed for generative models due to the high-dimensional nature of changes in intermediate representations. Generative models like InfoGANs, \u03b2-VAEs, and others address disentanglement of latent variables. The concept of intrinsic disentanglement uncovers the internal organization of networks, arguing that many transformations are statistically dependent. This is different from interventions on internal variables proposed by Bau et al. (2018) in GANs. Our approach introduces the concept of intrinsic disentanglement to uncover the internal organization of networks, which differs from interventions on internal variables proposed by Bau et al. (2018) in GANs. Compared to other approaches, our method is more flexible and applies to arbitrary continuous transformations. The approach, introduced independently in 2018, is flexible for arbitrary continuous transformations and free from strong representation theory requirements. Suter et al. (2018) also explore an interventional approach to disentanglement in classical graphical models. A general framework is introduced to define disentanglement and connect it to causal concepts, with a focus on a generative model mapping latent space to a manifold where data points reside. Mathematical details are provided in Appendix A. The generative model M implements a function g M mapping a latent space Z to a manifold Y M where data points live. A representation is a mapping r from Y M to a representation space R. The model is a latent representation of the data, implemented by a non-recurrent neural network with a causal graphical model representation. The latent representation model M maps a latent space Z to a manifold Y M where data points reside. A representation is a mapping r from Y M to a representation space R. The generative model is implemented by a non-recurrent neural network with a causal graphical model representation called Causal Generative Model (CGM). This model includes endogenous variables represented by nodes in the causal graph, allowing the computation of g M through a succession of operations. The latent representation model M maps a latent space Z to a manifold Y M where data points reside. A collection of endogenous variables represented by nodes in the causal graph is chosen to compute the mapping g M. This involves composing endogenous variable assignments with endogenous mappings. The endogenous variables are not statistically independent and can influence each other. A paradigmatic choice for these variables is the output activation maps of each channel in one hidden layer of a convolutional neural network. Mild conditions are used to ensure g M is left-invertible, defining the internal representation of the network. The latent representation model M maps a latent space Z to a manifold Y M where data points reside. Endogenous variables, such as output activation maps of a hidden layer in a neural network, are chosen to compute the mapping g M. Mild conditions ensure g M is left-invertible, defining the network's internal representation. The CGM framework allows defining counterfactuals in the network following Pearl (2014). The CGM framework enables defining counterfactuals in the network by replacing assignments of variables with a vector of assignments. Counterfactuals induce a transformation of the generative model output, with faithfulness ensuring interventions on internal variables result in outputs consistent with the original model. Non-faithful counterfactuals generate examples that may not align with the original model. In the context of generative models, non-faithful counterfactuals can lead to outputs that deviate from the original model, potentially resulting in artifactual outputs or extrapolation to unseen data. The concept of disentangled representation suggests that individual latent variables encode real-world transformations, driving supervised approaches to disentangling representations. The classical notion of disentangled representation posits that latent variables encode real-world transformations, leading to supervised approaches. Unsupervised learning seeks to learn these transformations from unlabeled data, with SOTA approaches enforcing conditional independence between latent factors. SOTA approaches aim to encode real-world transformations from unlabeled data by enforcing conditional independence between latent factors. However, this statistical approach faces challenges such as confounding factors and the ill-posed nature of the problem. Finding an appropriate inductive bias to learn a representation that benefits downstream tasks remains an open question. Finding an appropriate inductive bias to learn a representation that benefits downstream tasks remains an open question. SOTA unsupervised approaches are mostly demonstrated on synthetic datasets, with limited success on complex real-world datasets. Disentangled generative models struggle to match the visual sample quality of non-disentangled models on complex real-world data. Generative models struggle to achieve high visual sample quality on complex real-world data compared to non-disentangled models. A new non-statistical definition of disentanglement is proposed, focusing on transformations in the latent space that act on individual variables while leaving others unchanged. This concept aims to ensure that transformations follow the causal principle of independent mechanisms. The concept of disentanglement in generative models focuses on transformations in the latent space that act on individual variables while leaving others unchanged, following the causal principle of independent mechanisms. This notion, termed extrinsic disentanglement, is agnostic to subjective property choices and statistical independence. The concept of disentanglement in generative models focuses on transformations in the latent space that act on individual variables while leaving others unchanged. This functional definition is agnostic to subjective property choices and statistical independence. However, in the latent space, disentangled transformation still requires statistical independence between factors. To uncover related properties that are disentangled in our definition but may be statistically related, a different representation is needed. The endogenous variables in the graphical model may not be statistically independent due to a common latent cause, but they can still reflect interesting properties that can be intervened on independently. In contrast to latent variables, endogenous variables in a graphical model may not be statistically independent due to a common latent cause. However, they can still reflect interesting properties that can be intervened on independently. Disentanglement is extended to allow transformations of internal variables, ensuring that variables do not have common latent ancestors for faithful counterfactuals. Modularity is a structural property of the internal representation that allows for disentangled transformations. A subset of endogenous variables E is called modular when they do not have common latent ancestors. This property enables the implementation of arbitrary disentangled transformations within the input domain. A modular subset of endogenous variables E allows for disentangled transformations within its input domain. The proof extends Proposition 1 and can be applied to multiple modules. A disentangled representation is defined by partitioning the intermediate representation into modules, enabling valid transformations in the data space. A disentangled representation requires partitioning the set of latent variables into modules, allowing for valid transformations in the data space. This insight is relevant to artificial and biological systems, suggesting that grouping neurons into modules is necessary for meaningful representations. Our framework suggests that a modular structure is essential for meaningful representations in artificial and biological systems. Once a modular structure is identified in the network, a variety of disentangled transformations become available. Transformations that remain within their input domain are considered good candidates for disentanglement, and counterfactual interventions implicitly define transformation. To achieve disentanglement in a modular network structure, counterfactual interventions are used by assigning a constant value to endogenous variables. Sampling from the marginal distribution of these variables helps in defining faithful counterfactuals. The procedure involves selecting a subset of channels in a neural network and generating original examples using latent variables. By memorizing the values of variables for each example, a hybrid example can be created by mixing aspects of the generated images. The procedure involves selecting a subset of channels in a neural network and generating original examples using latent variables. By memorizing the values of variables for each example, a hybrid example can be created by mixing aspects of the generated images. This counterfactual hybridization framework allows assessing the causal effect of a given module E on the output of the generator by generating pairs of vectors from the latent space and collecting hybrid outputs. The hybridization framework assesses the causal effect of a module E on the generator's output by generating pairs of vectors from the latent space and collecting hybrid outputs to estimate an influence map. The approach calculates the mean absolute effect to determine unit-level causal effects, with specificities of taking the absolute value and averaging over many samples. The approach involves interpreting value as a unit-level causal effect, taking the absolute value of effects, and averaging results over different interventions. The hybridization method focuses on selecting subsets to intervene on, using a fine to coarse approach to extract groups for analysis. The hybridization approach focuses on selecting subsets to intervene on, using a fine to coarse approach to extract groups for analysis. Influence maps are used to quantify the individual influence of module E, with a challenge in networks containing many units or channels per layer. Representative EIMs for channels of convolutional layers suggest functional segregation in a VAE trained on the CelebA face dataset. Influence maps are used to group modules at a coarser scale based on similarity. Representative EIMs for channels of convolutional layers show functional segregation in a VAE trained on the CelebA face dataset. Channels are clustered using EIMs as feature vectors, with preprocessing steps including local averaging and thresholding. To group channels in an unsupervised way, clustering is performed using EIMs as feature vectors. Pre-processing steps involve local averaging, thresholding, and NMF algorithm with manually selected rank K to obtain cluster template patterns and weights for each influence map. Each map is assigned a cluster based on the template pattern with the highest weight. The NMF algorithm is used to factorize S into WH, resulting in K cluster template patterns. Each map is assigned a cluster based on the template pattern with the highest weight. The choice of NMF is justified by its success in isolating meaningful parts of images. Additionally, a comparison will be made with the k-means clustering algorithm, and a toy generative model is introduced. The NMF algorithm is used for clustering, justified by its success in isolating image parts. A toy generative model is introduced to further support this approach. The NMF algorithm is utilized for clustering, supported by its effectiveness in isolating image parts. A toy generative model is introduced to reinforce this method. The model involves sampling coefficients from a distribution and enforcing specific conditions on sets of indices to identify areas in the image influenced by individual modules. The NMF algorithm is used for clustering image parts. A generative model is introduced to identify areas influenced by individual modules, ensuring a disentangled representation. The sliding window is applied to enforce similarity between influence maps within the same module. The NMF algorithm is utilized for clustering image parts and a generative model is introduced to identify areas influenced by individual modules, ensuring a disentangled representation. The sliding window is applied to enforce similarity between influence maps within the same module. Additionally, the application of sliding window is justified to enforce similarity between influence maps belonging to the same module, favoring low-rank matrix factorization. The study investigates modularity of generative models trained on the CelebFaces Attributes Dataset (CelebA) using DCGAN, \u03b2-VAE, and BEGAN. The full procedure includes EIM calculations, clustering of channels into modules, and hybridization of generator samples using these modules. Hybridization procedures involve intervening on the output of the intermediate convolutional layer. The study utilized a basic architecture, a plain \u03b2-VAE, and ran a full procedure involving EIM calculations, clustering of channels into modules, and hybridization of generator samples. Setting the number of clusters to 3 led to highly interpretable cluster templates associated with background, face, and hair. Cluster stability analysis confirmed these observations. The study utilized a basic architecture, a plain \u03b2-VAE, and ran a full procedure involving EIM calculations, clustering of channels into modules, and hybridization of generator samples. Setting the number of clusters to 3 led to highly interpretable cluster templates associated with background, face, and hair. Cluster stability analysis confirmed these observations. The analysis showed that NMF-based clustering outperformed k-means clustering, with consistency dropping considerably for 4 clusters. The clustering analysis confirmed that using 3 clusters led to highly interpretable templates for background, face, and hair. Results showed that NMF-based clustering performed better than k-means, with consistency dropping for 4 clusters. The cosine similarity between matching clusters was high at .9, indicating robustness in the clustering approach. Influence maps also reflected the observation that some maps spread over different image locations. The clustering analysis using 3 clusters resulted in interpretable templates for background, face, and hair. The hybridization procedure replaced features in the modules without introducing discontinuities in the overall image structure. The \u03b2-VAE is designed for extrinsic disentanglement. The \u03b2-VAE is designed for extrinsic disentanglement, but further work has shown it may not be optimal compared to other approaches. Investigating intrinsic disentanglement in models without explicit enforcement, like GAN-like architectures, is important as they typically outperform VAE-like approaches in sample quality for complex image datasets. Our approach was applied to models not optimized for disentanglement, including GAN-like architectures and a pretrained BEGAN model known for high-quality face image generation. The results showed good image quality and higher resolution, even with a relatively simple generator. After experiments with basic models, a pretrained BEGAN model was used to test the hypothesis. Interventions on specific layers showed selective transfer of features, with noticeable effects requiring modifications in successive layers. The model, trained on face images with a tighter frame, resulted in clear feature transfer from Original 2 to Original 1. Interventions on specific layers in a pretrained BEGAN model showed selective transfer of features, with noticeable effects requiring modifications in successive layers. The model, trained on face images with a tighter frame, resulted in clear feature transfer from Original 2 to Original 1, with only mild effects on image quality. The study evaluated the quality of counterfactual images compared to original generated images using the Frechet Inception Distance (FID). The hybridization procedure minimally affected image quality. The approach was tested on a BigGAN-deep architecture pretrained on the ImageNet dataset, showing scalability to high-resolution generative models. The study evaluated the quality of counterfactual images generated by a BigGAN-deep architecture pretrained on the ImageNet dataset. It was found that intervening on two successive layers within a Gblock was more effective for generating counterfactuals. Examples showed high-quality counterfactuals with modified backgrounds while keeping a similar object in the foreground. The study found that intervening on two successive layers within a Gblock was more effective for generating high-quality counterfactual images with modified backgrounds while maintaining a similar object in the foreground. The generated counterfactual images were used to probe and improve the robustness of classifiers to contextual changes, comparing the performance of several state-of-the-art pretrained classifiers available on Tensorflow-hub. The study used counterfactual images to test classifier robustness to contextual changes. Different pretrained classifiers were compared, with Inception resnet performing better at intermediate blocks 5-6. High recognition rates were observed when intervening at layers closest to the output. Examples of non-consensual classification results were also provided. The study compared different pretrained classifiers, with Inception resnet performing better at intermediate blocks 5-6. Non-consensual classification results were provided, suggesting that classifiers rely on different aspects of image content for decision-making. The research introduced a mathematical definition of disentanglement and used it to characterize representation in deep generative architectures. This framework helps in understanding internal variables in generative models trained on real-world datasets. The study introduced a framework for characterizing representation in deep generative architectures, revealing interpretable modules of internal variables. This research enhances the interpretability of complex generative models and their applications, such as style transfer and assessing object recognition system robustness. It aims to better utilize deep neural networks by improving interpretability and expanding their potential applications. This research focuses on enhancing the interpretability of deep neural networks by characterizing representation in generative architectures. It aims to make these models more accessible for tasks they were not originally trained for, contributing to more sustainable research in Artificial Intelligence. The study utilizes structural causal models to manipulate parts of the model independently, offering a new perspective on how deep neural networks can be utilized more effectively. The study utilizes structural causal models (SCMs) to manipulate parts of the model independently, offering a new perspective on how deep neural networks can be utilized more effectively. SCMs rely on structural equations (SEs) to represent mathematical models, allowing for the manipulation of variables and exogenous influences. SEs remain valid even after interventions, making them suitable for modeling operations in computational graphs of neural network implementations. The study uses structural causal models (SCMs) with structural equations (SEs) to represent mathematical models in computational graphs of neural network implementations. A Causal Generative Model (CGM) captures the computational relations between input latent variables, generator's output, and endogenous variables. The generator's output can be decomposed into two steps in a feed-forward neural network. The Causal Generative Model (CGM) captures the relations between input latent variables, generator's output, and endogenous variables in a computational graph. The generator's output can be decomposed into two steps in a feed-forward neural network, aligning with the definition of a deterministic structural causal model. The Causal Generative Model (CGM) is represented by an acyclic graph with deterministic structural equations. The model includes endogenous variables, latent inputs, and outputs, following the principles of a deterministic structural causal model. CGMs have specific features allowing for feed-forward networks with cascades of deterministic operations. This setup ensures key properties in the computational graph of generative networks. The Causal Generative Model (CGM) involves an acyclic graph with deterministic structural equations, allowing for feed-forward networks with cascades of deterministic operations. This setup ensures key properties in the computational graph of generative networks, where endogenous variables are assigned once latent inputs are chosen, and outputs are assigned once either latent inputs or a subset of variables are chosen. The latent distribution covers the whole latent space Z, while internal variables and outputs typically live on manifolds of smaller dimension. In an ideal case, the latent distribution covers the whole latent space Z, while internal variables and outputs live on smaller dimension manifolds. Functions assign Y from latent and endogenous variables, called latent and endogenous mappings. The variables are constrained to subsets of their euclidean ambient space. An embedded CGM satisfies these assumptions. In an embedded CGM, variables V k and Y are constrained in subsets of their euclidean space. The output can be computed from inputs by successive assignments along G. The image set Y M of a trained model should approximate the data distribution. The image sets V M, Y M are constrained by parameters M and are difficult to characterize. Y M should approximate the data distribution, and matching it precisely is a key goal for generative models. Transformations on Y M respect its topology using embeddings, allowing inversion of g M. Generative models aim to approximate the data distribution by transforming image sets using embeddings that respect the topology of Y M. Injectivity of the transformation function g M is a key requirement for embedded CGMs, with compactness of the latent space being a determining factor. Generative models based on uniformly distributed latent variables can be considered embedded CGMs if they are injective. Generative models based on uniformly distributed latent variables, if injective, are embedded CGMs. Restricting VAEs' latent space to compact intervals can approximate the original CGM for most samples. The CGM framework allows defining counterfactuals in the network following Pearl (2014). The CGM framework defines counterfactuals in the network by replacing structural assignments for variables. This concept aligns with potential outcomes and induces a transformation of the generative model's output. Counterfactual mapping relates to disentanglement, allowing transformations of internal variables. Intrinsic disentanglement involves endomorphism in the CGM. Our approach relates counterfactuals to disentanglement in a CGM, allowing transformations of internal variables. Intrinsic disentanglement involves endomorphism T in the network, where a transformation T of endogenous variables affects only a subset E. Intrinsic disentanglement in a CGM involves a transformation T that affects variables indexed by E. The value of V2 is computed as in the original CGM before applying transformation T2. Counterfactuals represent perturbations that can be disentangled given their faithfulness. A continuous and injective embedding is achieved when Z is compact and the codomain of gM is Hausdorff. The text discusses intrinsic disentanglement in a CGM involving a transformation T affecting variables indexed by E. It mentions achieving a continuous and injective embedding when Z is compact and the codomain of gM is Hausdorff. Counterfactuals can be disentangled based on their faithfulness, leading to the proof of equivalence between faithful and disentangled transformations. The text discusses the disentanglement of transformations in a CGM involving variables indexed by E. It proves that a transformation is disentangled if it is an endomorphism of YM and the counterfactual mapping is faithful. The absence of a common latent ancestor between E and E ensures unambiguous assignment of values by non-overlapping subsets of latent variables. This guarantees that T is an endomorphism of VM for any choice of endomorphism TE. The text discusses the disentanglement of transformations in a CGM involving variables indexed by E. It proves that a transformation is disentangled if it is an endomorphism of YM and the counterfactual mapping is faithful. The product of the image sets of the two subsets of variables guarantees that T is an endomorphism of VM for any choice of endomorphism TE. The subsets of endogenous variables associated with each Vk are modular, leading to a disentangled representation in the hidden layer. The choice of increasing dimensions and i.i.d. sampling of model parameters ensure injective mapping and counterfactual hybridization results in an influence map covering exactly Ik. The conditions on Ik and thresholding approach guarantee a rank K binary factorization of matrix B. The text discusses disentanglement of transformations in a CGM involving variables indexed by E. It proves that a transformation is disentangled if it is an endomorphism of YM and the counterfactual mapping is faithful. The product of image sets of subsets of variables guarantees T is an endomorphism of VM for any choice of TE. The subsets of endogenous variables associated with each Vk are modular, leading to a disentangled representation in the hidden layer. Conditions on Ik and thresholding approach guarantee a rank K binary factorization of matrix B. The \u03b2-VAE architecture is similar to DCGAN, with hyperparameters specified in Table 1. The method proposed in Berthelot et al. (2017) was used for the CelebA dataset. The \u03b2-VAE architecture, similar to DCGAN, consists of three blocks of convolutional layers with skip connections to enhance image sharpness. The pretrained model used for CelebA dataset and BigGan-deep architecture for ImageNet were not retrained. Consult Berthelot et al. (2017) for architectural details. The pretrained BigGan-deep architecture used for ImageNet consists of ResBlocks with BatchNorm-ReLU-Conv Layers and skip connections. Influence maps were generated by a VAE on the CelebA dataset, showing the impact of perturbations. FID analysis of BEGAN hybrids was also conducted. Influence maps were generated by a VAE on the CelebA dataset, showing the impact of perturbations. FID analysis of BEGAN hybrids indicated that hybrids have a small distance to the generated data and to each other, suggesting that Hybridization produces visually plausible images. Hybrids, normalized by FID, show closeness to generated data and each other. Entropy values vary based on Gblock interventions, with Gblock 6 showing larger values for poorer quality modules. Hybrids from Gblock 4 have smaller entropy values, suggesting object texture is key information. The entropy values of hybrids vary based on Gblock interventions, with Gblock 6 showing larger values for poorer quality modules. Hybrids from Gblock 4 have smaller entropy values, indicating that object texture is a crucial factor for the classifier's decision. The NMF algorithm extracts modules on different Gblocks (from 4 to 6). Entropy values vary based on interventions, with Gblock 6 showing larger values for poorer quality modules. The classification outcome of discriminative models for koala+teddy hybrids is investigated for assessing classifier robustness. The experiment investigates the robustness of classifiers using koala+teddy hybrids. The hybrids resemble a teddy bear in a koala context, testing classifiers' sensitivity to objects over context. Nasnet large is shown to be more robust to contextual changes compared to other classifiers."
}