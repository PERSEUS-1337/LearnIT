{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs involves using the Wasserstein-2 metric proximal on the generators. The approach utilizes the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experimental results show improved speed and stability in training GANs in terms of wall-clock time and FID learning curves. The adversarial game in generative models involves a generator trying to fool a discriminator by recreating the density distribution from a real source. The problem of matching densities can be minimized using discrepancy measures like Wasserstein distance. This approach improves training speed and stability in GANs. Optimal transport, specifically the Wasserstein distance, is used to define a discrepancy measure between densities in generative models like GANs. It not only defines the loss function but also introduces structures for optimization, such as the Wasserstein steepest descent flow. This paper focuses on deriving the Wasserstein steepest descent flow for deep generative models in GANs using the Wasserstein-2 metric function. In this paper, the authors derive the Wasserstein steepest descent flow for deep generative models in GANs using the Wasserstein-2 metric function to compute the proximal operator for the generators. The Fisher-Rao natural gradient in GANs poses challenges due to low dimensional support sets and difficulties with KL divergence. To address this, the gradient operator induced by the Wasserstein-2 metric is proposed for use in computing the proximal operator for GAN generators. The regularization is based on squared constrained Wasserstein-2 distance, which can be approximated by a neural network. The constrained Wasserstein-2 metric simplifies computation in implicit generative models, and a relaxed proximal operator is introduced for further simplification. This method allows for easy implementation as a drop-in regularizer with simple parameter updates. The metric has a simple structure and a relaxed proximal operator is introduced for GAN generators to simplify computation. The method can be easily implemented as a drop-in regularizer with simple parameter updates. The paper introduces the Wasserstein natural gradient and a proximal method in Algorithm 1. Experimental results demonstrate the effectiveness of the proposed methods with various types of GANs. Related work on optimal transport and its proximal operator on a parameter space is briefly reviewed. In experiments with various types of GANs, the proposed methods are shown to be effective. Optimal transport and its proximal operator on a parameter space are briefly presented, and applied to the optimization problems of GANs. Optimal transportation defines distance functions between probability densities, with the Wasserstein-2 distance denoted as W. This distance has a dynamical formulation as a trajectory transporting initial density to final density with minimal kinetic energy. The classic theory of Wasserstein-2 distance involves transporting initial density to final density with minimal kinetic energy. Extending this theory to parameterized density models, the constrained Wasserstein-2 metric function is defined on a parameter space. The constrained Wasserstein-2 metric function on a parameter space involves an infimum over feasible Borel potential functions and continuous parameter paths. This metric differs from the Wasserstein-2 distance on the full density set and can be used for steepest descent optimization schemes. The constrained Wasserstein-2 metric allows for a steepest descent optimization scheme using a Riemannian metric structure. The Wasserstein natural gradient is defined for a loss function F on parameter space \u0398. The Wasserstein gradient operator is defined for a loss function on parameter space \u0398, with the steepest descent flow given by a specific equation. The backward Euler method, also known as the Jordan-Kinderlehrer-Otto scheme, provides a numerical scheme for the optimization process. The backward Euler method, also known as the Jordan-Kinderlehrer-Otto scheme, provides a numerical scheme for the optimization process. The Semi-Backward Euler method is a first-order scheme for the gradient flow of a loss function on parameter space \u0398. The Semi-Backward Euler method for the gradient flow of loss function F on parameter space \u0398 is derived using a Taylor expansion. It is easier to approximate than the forward Euler method and simpler than the backward Euler method (JKO) as it does not require computing and inverting G(\u03b8). The method is implemented in implicit generative models by defining a generator g \u03b8 : R m \u2192 R n. The Semi-Backward Euler method is implemented in implicit generative models by defining a generator g \u03b8 : R m \u2192 R n. It simplifies the computation by approximating variable \u03a6 using a neural network and allows for a simpler formulation of the constrained Wasserstein-2 metric. The update in Proposition 3 introduces a neural network to approximate variable \u03a6 in implicit generative models, leading to a simpler formulation of the constrained Wasserstein-2 metric. This reformulation defines the relaxed Wasserstein metric and introduces an algorithm for the proximal operator on generators. Proposition 4 specifies the constrained Wasserstein-2 metric in implicit generative models, where the derivative of the generator g with respect to \u03b8 must be a gradient vector field of \u03a6 with respect to x. The constrained Wasserstein metric requires the derivative of the generator with respect to the parameter paths to be a gradient vector field of \u03a6 with respect to x. When the sample space is 1 dimensional, the gradient constraint is satisfied, but in general, this is not the case. Finding \u03a6 poses computational difficulties, making fitting the gradient constraint an open problem. To simplify computations, a relaxed Wasserstein metric on the parameter space is considered. The relaxed Wasserstein proximal operator is approximated based on a new metric, with the infimum taken among all feasible continuous parameter paths. The relaxed Wasserstein proximal operator is approximated based on a new metric in parameter space, withdrawing the gradient constraint. This regularization method simplifies the generator by minimizing the expectation of squared differences in high-dimensional sample spaces. Algorithm 1 presents the Relaxed Wasserstein Proximal, illustrating its effectiveness with a toy example. The text discusses the use of Wasserstein proximal operator in GANs, presenting a toy example with a family of distributions. It introduces a proximal regularization for a loss function and compares statistical distance functions between parameters. The text introduces proximal regularization for a loss function and compares statistical distance functions between parameters, including Wasserstein-2 distance, Euclidean distance, and Kullback-Leibler divergence. It discusses how the Wasserstein proximal operator outperforms the Euclidean proximal operator in decreasing the objective function. The text discusses the superiority of Wasserstein proximal over Euclidean proximal in decreasing the objective function for the Wasserstein-1 loss function. It presents numerical experiments using the Relaxed Wasserstein Proximal algorithm for training GANs, showing better speed and stability compared to other methods. The Relaxed Wasserstein Proximal (RWP) algorithm is a faster and more stable method for training GANs compared to other algorithms. It applies regularization to the generator during training, which is a novel approach as most methods focus on regularizing the discriminator. This algorithm introduces two hyperparameters to improve speed and convergence in GAN training. The Relaxed Wasserstein Proximal (RWP) algorithm introduces regularization to the generator during GAN training. It modifies the update rule for the generator by introducing two hyperparameters: the proximal step-size h and the number of iterations. The algorithm is tested on three GAN types using CIFAR-10 and CelebA datasets with the DCGAN architecture. The Relaxed Wasserstein Proximal (RWP) algorithm is tested on three GAN types using CIFAR-10 and CelebA datasets with the DCGAN architecture. The Fr\u00e9chet Inception Distance (FID) is used to measure the quality of generated samples, with specific hyperparameter choices given in Appendix C. The Relaxed Wasserstein Proximal regularization improves convergence speed and stability for GANs on CIFAR-10 and CelebA datasets. It shows a 20% improvement in sample quality for DRAGAN according to FID measurements. Our regularization improves convergence speed and sample quality for GANs on CelebA dataset. Multiple generator iterations may cause initial learning issues for Standard GANs, but can be easily detected and resolved. The defect is expected to be fixed with a more stable loss function or different parameters. Our regularization improves convergence speed and sample quality for GANs on CelebA dataset. The focus is on successful runs, predicting defect rectification with a stable loss function like WGAN-GP or different parameters. Comparing multiple generator updates to discriminator updates in RWP, omitting regularization leads to high FID variance. However, with RWP, FID converges more stably and achieves a lower FID. Samples from the models are provided in Appendix E, and latent space walks show RWP regularization does not cause GAN memorization (see Appendix F for details). The use of RWP regularization in WGAN-GP improves convergence speed and sample quality, leading to a lower FID with more stability. Latent space walks demonstrate that RWP does not cause GAN memorization. Additionally, RWP enhances speed and achieves better results compared to omitting regularization. Multiple generator iterations may initially fail but eventually lead to successful runs. The experiment demonstrates the effect of performing 10 generator iterations per outer-iteration with and without RWP regularization. With RWP, convergence is achieved, and FID is lower, while without RWP, training is highly variable with FID on a rising trend. The Semi-Backward Euler method on the CIFAR-10 dataset shows comparable training to the standard WGAN-GP loss. The experiment on the effect of Semi-Backward Euler (SBE) method on the CIFAR-10 dataset shows convergence and lower FID with RWP regularization. Training without RWP is highly variable with FID increasing. The training of SBE involves approximating three functions and optimizing over the networks. The method is comparable to the standard WGAN-GP. The algorithm and hyperparameter settings for optimizing over three networks in FIG3 are presented. The Semi-Backward Euler method is comparable to norm WGAN-GP, with potential for further investigation. Many studies apply the Wasserstein distance as the loss function in optimal transport for machine learning and GANs. The Wasserstein distance is commonly used as the loss function in machine learning and GANs due to its statistical properties and ability to compare probability distributions. In Wasserstein GAN, the Wasserstein-1 distance function is chosen as the loss function, requiring the discriminator to satisfy the 1-Lipschitz condition. Regularization techniques are often employed to meet this condition. In Wasserstein GAN, the loss function is the Wasserstein-1 distance function, requiring the discriminator to satisfy the 1-Lipschitz condition. Regularization techniques are used to meet this condition. The Wasserstein-2 metric provides a metric tensor structure, forming an infinite dimensional Riemannian manifold called the density manifold. Gradient flows in the density manifold are linked to transport-related partial differential equations, such as the Fokker-Planck equation. The gradient flow in the density manifold is linked to transport-related partial differential equations like the Fokker-Planck equation. Two angles have been developed in learning communities: leveraging gradient flow structure in probability space and studying nonparametric models like the Stein gradient descent method. Additionally, an approximate inference method for computing Wasserstein gradient flow is considered, introducing an approximation towards Kantorovich dual variables. In addition to studying nonparametric models like the Stein gradient descent method, recent research has focused on constrained Wasserstein gradient flow in Gaussian families and elliptical distributions. Our approach applies the Wasserstein gradient to general implicit generative models, introducing constrained Wasserstein gradient and its relaxations. Our approach applies the Wasserstein gradient to general implicit generative models, focusing on regularizing the generator instead of the discriminator. We compute the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space for Wasserstein GAN, leading to better minimization results in terms of FID with faster convergence speeds. The proposed method applies the Wasserstein gradient to implicit generative models, focusing on regularizing the generator. It computes the Wasserstein-2 gradient flow on parameter space for Wasserstein GAN, leading to improved minimization results in terms of FID with faster convergence speeds. The text discusses the use of Wasserstein gradient in implicit generative models to regularize the generator. It computes the Wasserstein-2 gradient flow on parameter space for Wasserstein GAN, resulting in improved minimization results in terms of FID with faster convergence speeds. The Wasserstein gradient operator in (P+, gW) is defined for a loss function F: P+ \u2192 R. Analytical results on the Wasserstein-2 gradient flow are discussed. The Wasserstein-2 metric and gradient operator are constrained on statistical models defined by a triplet (\u0398, Rn, \u03c1). The Riemannian metric g is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. Wasserstein statistical manifold is defined for \u03b8 \u2208 \u0398 with associated metric tensor G(\u03b8). The Riemannian metric g is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The Wasserstein statistical manifold is defined for \u03b8 \u2208 \u0398 with a smooth and positive definite metric tensor G(\u03b8). The constrained Wasserstein gradient operator in parameter space is studied in Theorem 2. The distance function can be expressed as an action function in the Wasserstein statistical manifold. The gradient operator on the Riemannian manifold (\u0398, g\u03b8) is discussed. The distance function in the Wasserstein statistical manifold can be expressed as an action function. The gradient operator on the Riemannian manifold is defined. The derivation of the proposed semi-backward method is presented, along with the proof of a claim involving geodesic paths and reparameterization of time. Equation 6 and 7 are proven, with the maximizer \u03a6* satisfying a specific condition. The geodesic path \u03b8 is reparameterized in time to prove equations 6 and 7. The Semi-backward method is derived as a consistent numerical method in time. Proposition 4 is proven, presenting the implicit model and gradient constraint. The probability density transition equation of g(\u03b8(t), z) satisfies the constrained continuity equation. Proposition 4 is proven using the semi-backward method, showing the implicit model and gradient constraint. The probability density transition equation of g(\u03b8(t), z) satisfies the constrained continuity equation, leading to the proof of equation 10. The text discusses the proof of equations 10 and 11 using gradient and divergence operators, integration by parts, and the push forward relation. It also demonstrates the explicit computation of the Wasserstein and Euclidean proximal operators. The text discusses the explicit computation of the Euclidean proximal operator and provides hyperparameter settings for the Relaxed Wasserstein Proximal experiments. The text discusses hyperparameter settings for the Relaxed Wasserstein Proximal experiments, including optimizer choices, step-sizes, and generator iterations for different datasets like CIFAR-10 and CelebA. The text introduces Wasserstein Proximal as an easy-to-implement regularization method and provides a specific example of using Relaxed Wasserstein Proximal on Standard GANs. The algorithm involves updating the discriminator, performing Adam gradient descent, and repeating the process until a stopping condition is met. The key differences between standard GAN training and using Relaxed Wasserstein Proximal are highlighted. The text introduces Relaxed Wasserstein Proximal as a regularization method for GANs, showcasing samples generated from Standard GAN with RWP on CelebA dataset and WGAN-GP with RWP on CIFAR-10 dataset. It also discusses the concept of latent space walk to detect memorization in generators, showing smooth transitions in GANs with RWP regularization. In FIG5, samples from WGAN-GP with RWP on CIFAR-10 have an FID of 38.3. Latent space walk in GANs with RWP shows smooth transitions. Specific hyperparameters for SBE on WGAN-GP include batch size 64, DCGAN architecture, MLP for \u03a6p, Adam optimizer with LR 0.0002, latent space dimension 100, and discriminator updated 5 times per outer-iteration loop. In the study, a network (MLP) was used for potential \u03a6p with layer-normalization. The Adam optimizer with specific parameters was utilized for the generator, discriminator, and potential. The latent space dimension was set to 100. The discriminator was updated 5 times per outer-iteration loop, along with updates for the generator and potential. Sampling of latent data was also performed during the iterations."
}