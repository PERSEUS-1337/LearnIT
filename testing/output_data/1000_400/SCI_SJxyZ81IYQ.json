{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively. This approach preserves semantic content better and follows a human language-like recursive structure. The task of image captioning involves generating short descriptions for images using an encoder-decoder paradigm. Despite its effectiveness, the sequential model used in current state-of-the-art models fails to reflect the hierarchical structures of natural languages in image captioning. Sequential models used in image captioning lack the ability to reflect the hierarchical structures of natural languages, leading to drawbacks such as reliance on n-gram statistics, favoring frequent n-grams in training, and producing captions that are syntactically correct but semantically irrelevant to the image. Sequential models in image captioning struggle with reflecting hierarchical structures of natural languages, leading to issues like favoring frequent n-grams in training and generating syntactically correct but semantically irrelevant captions. To address these challenges, a new paradigm is proposed where semantics and syntax are separated into two stages. The semantic content of an image is extracted as noun-phrases, like \"a white cat\" or \"two men\", which are then used to construct the caption through recursive composition. The correct captions are generated in two stages: first, the semantic content of the image is represented by noun-phrases like \"a white cat\" or \"two men\", and then the caption is constructed through recursive composition. The compositional procedure involves forming higher-level phrases by connecting selected sub-phrases. The proposed paradigm for caption generation involves a compositional procedure with parametric modular nets, emphasizing the factorization of semantics and syntax. This approach aims to preserve semantic content and improve interpretability and control in caption generation. The proposed paradigm for caption generation emphasizes the factorization of semantics and syntax, preserving semantic content and improving interpretability and control. It also increases caption diversity, generalizes well to new data, and maintains good performance with limited training data. Recent works in image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. This approach has shown improved performance in generating captions while maintaining semantic correctness and generalizing well to new data. Recent works on image captioning utilize convolutional neural networks for image representation and recurrent neural networks for caption generation. Different approaches have been proposed, such as using a single feature vector for image representation, applying attention mechanisms to extract relevant image information, adjusting attention computation to consider generated text, and adding additional LSTM for better attention control. Dai et al reformulated latent states as 2D maps. In recent image captioning research, various approaches have been proposed to improve the generation of captions. These include adjusting attention mechanisms to consider generated text, adding additional LSTM for better attention control, and reformulating latent states as 2D maps. Additionally, some approaches directly extract phrases or semantic words from input images. The authors proposed a bottom-up approach for image captioning, using noun-phrases to construct captions recursively, aiming to address issues with sequential generation of captions and lack of diversity in existing models. The proposed paradigm for image captioning uses noun-phrases to construct captions recursively, addressing issues with existing models such as incorrect semantic coverage and lack of diversity. This approach preserves semantics effectively, requires less data to learn, and leads to more diverse captions compared to other methods. The proposed compositional paradigm for image captioning uses noun-phrases to generate captions by selecting and composing phrases via dynamic programming. It allows for the composition of any number of phrases, leveraging neural networks to learn plausible compositions. The structure of natural language is hierarchical, resembling trees in typical sentence parsing. The proposed two-stage framework for image captioning uses noun-phrases as a semantic representation and constructs captions in a hierarchical structure through a recursive compositional procedure called CompCap. Unlike mainstream models, it focuses on composition rather than n-gram statistics. In a two-stage framework for image captioning, noun-phrases are used as a semantic representation. The caption is constructed hierarchically through a recursive compositional procedure called CompCap, focusing on nonsequential dependencies among words and phrases. This approach explicitly represents image semantics by capturing object categories and associated attributes with noun-phrases like \"a black cat\" or \"two boys\". In our framework, we represent image semantics explicitly using noun-phrases like \"a black cat\" or \"two boys\". This approach captures object categories and associated attributes, extracted from the input image. The number of distinct noun-phrases in a dataset is significantly smaller than the number of images, for example, MS-COCO contains 120K images but only about 3K distinct noun-phrases in the captions. In the study, the approach focuses on object detection and attribute recognition by treating noun-phrase extraction as a multi-label classification problem. Distinct noun-phrases are derived from training captions, encoded using a Convolutional Neural Network, and classified using binary classification for each noun-phrase. The study focuses on object detection and attribute recognition by extracting noun-phrases from captions and treating them as classes. Visual features are extracted from images using a Convolutional Neural Network and encoded for classification. The input image is represented by top-scoring noun-phrases, pruned for semantic similarity, and used in constructing captions through a recursive procedure called CompCap. The study focuses on object detection and attribute recognition by extracting noun-phrases from captions and treating them as classes. Visual features are extracted from images using a Convolutional Neural Network and encoded for classification. The input image is represented by top-scoring noun-phrases, pruned for semantic similarity, and used in constructing captions through a recursive procedure called CompCap. The caption is constructed by connecting pairs of noun-phrases using a Connecting Module (C-Module) to generate longer phrases. The study focuses on object detection and attribute recognition by extracting noun-phrases from captions and treating them as classes. Visual features are extracted from images using a Convolutional Neural Network and encoded for classification. A Connecting Module (C-Module) is applied to generate longer phrases by connecting pairs of noun-phrases. The Evaluation Module (E-Module) assesses the completeness of the resulting caption, updating phrases until a complete caption is obtained. The Connecting Module (C-Module) selects connecting phrases to form complete captions by evaluating connecting scores. The conventional method of using an LSTM to decode intermediate words was found to be ineffective due to the structure of the input captions. The Connecting Module (C-Module) evaluates connecting scores to select phrases for complete captions. An alternative strategy treats phrase generation as a classification problem due to the limited number of distinct connecting phrases in the proposed paradigm. The proposed paradigm treats phrase generation as a classification problem, with a limited number of distinct connecting phrases. Over 1 million samples are collected for the connecting module, containing about 1,000 distinct connecting phrases. The connecting module acts as a classifier, using a two-level LSTM model to encode phrases and output a normalized score. The connecting module acts as a classifier, using a two-level LSTM model to encode phrases and output a normalized score. The encoders for left and right phrases share the same structure but have different parameters. Their encodings go through fully-connected layers and a softmax layer to produce the final output score. The LSTM drives the evolution of the encoded state for the connecting module. Encoders for left and right phrases share the same structure but have different parameters. The softmax output values are used as connecting scores to link phrases. A virtual neg class is added for unconnectable pairs. Scores for phrases are computed based on the C-Module. The Evaluation Module (E-Module) determines if a phrase is a complete caption by encoding it into a vector and evaluating its probability. It can also check other properties like caption quality using a caption evaluator. The E-Module encodes phrases into vectors and evaluates the probability of them being complete captions. It can also assess caption quality using a caption evaluator. Extensions include generating diverse captions through beam search or probabilistic sampling to avoid local minima. The framework allows for the generation of diverse captions through beam search or probabilistic sampling to avoid local minima. It can also be extended to incorporate user preferences or conditions, making it easier to control the resultant captions. Experiments were conducted on MS-COCO BID4 and Flickr30k BID5 datasets. The experiments were conducted on MS-COCO BID4 and Flickr30k BID5 datasets, each containing a large number of images with ground-truth captions. The vocabulary was standardized by converting words to lowercase and removing infrequent words, resulting in vocabularies of 9,487 for MS-COCO and 7,000 for Flickr30k. Training captions were limited to 18 words. The vocabulary for MS-COCO and Flickr30k datasets was standardized by converting words to lowercase and removing infrequent words, resulting in vocabularies of 9,487 and 7,000 words respectively. Training captions were truncated to 18 words. Ground-truth captions were parsed into trees using NLP toolkit BID31 for training the connecting and evaluation modules separately. The recursive compositional procedure is modularized for better generalization and less sensitivity to training statistics. Testing involves two forward passes for each module, with 2 or 3 steps typically needed to generate a complete caption. Several methods were compared in the experiments. The recursive compositional procedure is modularized for better generalization and less sensitivity to training statistics. Testing involves two forward passes for each module, typically requiring 2 or 3 steps to generate a complete caption. Several representative methods are compared with CompCap, including NIC, AdapAtt, TopDown, and LSTM-A5. CompCap is compared with LSTM-A5 BID19, which predicts semantical concepts as additional visual features. All methods are re-implemented and trained using the same hyperparameters, with ResNet-152 BID16 used to extract image features. The last convolutional and fully-connected layer activations are used as regional and global feature vectors. ResNet-152 is fixed during training, and the learning rate is set to 0.0001 for all methods. Best performing parameters on the validation set are selected for testing. During training, ResNet-152 is fixed without finetuning, and the learning rate is set to 0.0001 for all methods. Best performing parameters are selected for testing. CompCap selects 7 noun-phrases with top scores to represent the input image. Beam-search of size 3 is used for pair selection. Quality of generated captions is compared on MS-COCO and Flickr30k test sets using various metrics. CompCap with predicted noun-phrases achieves the best results in SPICE metric but lags behind in CIDEr, BLEU-4, ROUGE, and METEOR compared to baselines. SPICE focuses on semantical analysis, while other metrics favor frequent training n-grams, reflecting the properties of sequential and compositional caption generation methods. The results of the study show that metrics like CIDEr, BLEU-4, ROUGE, and METEOR favor frequent training n-grams, which are more likely to appear in sequentially generated captions. However, a compositional generation procedure preserves semantic content more effectively. An ablation study on the proposed compositional paradigm indicates that using groundtruth noun-phrases from associated captions leads to a significant improvement in all metrics, showing that CompCap effectively preserves semantic content. The proposed compositional paradigm, represented by groundtruth noun-phrases from associated captions, significantly boosts all metrics. CompCap effectively preserves semantic content, generating better captions with improved understanding of input images. By integrating noun-phrases following a composing order, metrics show further improvement, indicating the effectiveness of the approach. CompCap enhances caption quality by focusing on connecting phrase selection, leading to improved metrics except for SPICE. The compositional paradigm disentangles semantics and syntax, making CompCap adept at handling out-of-domain content and requiring less data to learn. Studies confirm the effectiveness of CompCap in improving SPICE and CIDEr scores. In two studies, CompCap was tested on different datasets, showing competitive results when trained on in-domain and out-of-domain data. The ability to disentangle semantics and syntax allows CompCap to handle varying dataset distributions effectively. The study tested CompCap on various datasets, showing competitive results with in-domain and out-of-domain data. CompCap can disentangle semantics and syntax effectively, benefiting from the stable distribution of syntax across datasets. The diversity of captions generated by CompCap was analyzed using five metrics, including the ratio of novel and unique captions. The diversity of captions generated by CompCap was analyzed using five metrics, including the ratio of novel and unique captions, vocabulary usage, and pair-wise editing distances. The diversity was quantified at both the dataset and image levels based on the average distance between captions. CompCap obtained the best results in all metrics for caption diversity, suggesting that the generated captions are diverse and novel. Qualitative samples in FIG2 show captions generated with different composing orders or noun-phrases. Error analysis is included in FIG4 with several failure cases. CompCap achieved the best results in all metrics for caption diversity, indicating that the generated captions are diverse and novel. Error analysis in FIG4 reveals failure cases with different errors from those in FIG1, mainly stemming from a misunderstanding of the visual content. The proposed paradigm aims to address these issues. The proposed paradigm for image captioning addresses issues related to misunderstanding visual content by generating captions in a compositional manner, factorizing the procedure into two stages: extracting explicit representations of input images in the form of noun-phrases and assembling them into captions hierarchically. Our approach for image captioning involves a two-stage process: extracting noun-phrases from input images and assembling them into captions hierarchically. The compositional procedure preserves semantics effectively, requires less training data, generalizes well across datasets, and produces diverse captions by finding semantically similar noun-phrases based on central nouns. The approach for image captioning involves extracting and assembling noun-phrases hierarchically to yield diverse captions. To suppress similar noun-phrases, encoders in the C-Module are used to compare central nouns and compute normalized euclidean distances between their encodings. The C-Module uses encoders to compare noun-phrases and compute normalized euclidean distances between their encodings, determining semantic similarity based on the sum of distances. The C-Module uses encoders for P (l) and P (r) with independent parameters to improve performance. Additional hyperparameters for CompCap can be tuned, such as beam search sizes for pair and phrase selection. Adjusting these hyperparameters is shown in FIG6. Adjusting hyperparameters for CompCap, such as beam search sizes for pair and phrase selection, has minor influence on performance, as shown in FIG6."
}