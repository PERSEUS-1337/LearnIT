{
    "title": "rygFWAEFwS",
    "content": "Stochastic Weight Averaging in Parallel (SWAP) is an algorithm proposed to accelerate DNN training by using large mini-batches to quickly compute an approximate solution and then refining it through averaging the weights of multiple models computed independently and in parallel. The resulting models generalize well and are produced in a substantially shorter time compared to traditional methods like SGD. This approach has shown reduced training time and good generalization performance on datasets like CIFAR10, CIFAR100, and ImageNet. Gradient descent (SGD) and its variants are commonly used to train deep neural networks (DNNs). Increasing the mini-batch size can accelerate DNN training by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss per iteration. In a distributed setting, multiple nodes can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates with one synchronization event per iteration. Training with larger mini-batches requires fewer updates and synchronization events, leading to good overall scaling behavior. In a distributed setting, nodes compute gradient estimates on mini-batch subsets and average them for consensus. Larger mini-batches require fewer updates and synchronization events, leading to good scaling behavior. However, there is a maximum batch size that can result in worse generalization performance, limiting the usefulness of large-batch training strategies. Stochastic Weight Averaging (SWA) can produce models with good generalization performance by averaging weights from final model stages. Large-batch training strategies can be limited by a maximum batch size that hinders generalization performance. Stochastic Weight Averaging (SWA) offers a method to improve generalization by averaging weights from final model stages. Generating multiple independent SGD sequences with small-batches and averaging models from each can achieve similar generalization performance. By generating multiple independent SGD sequences with small-batches and averaging models from each, Stochastic Weight Averaging in Parallel (SWAP) accelerates DNN training by utilizing compute resources efficiently. SWAP achieves generalization performance comparable to models trained with small-batches in a time similar to training with large-batches, as demonstrated on image classification tasks on popular datasets. Our algorithm SWAP accelerates DNN training by efficiently utilizing compute resources. It achieves comparable generalization performance to small-batch models in a time similar to large-batch training. SWAP significantly reduces training times for efficient models and outperforms the state of the art on CIFAR10, training in 68% of the time of the winning entry in the DAWNBench competition. The impact of training batch size on generalization performance is still not fully understood. The impact of training batch size on generalization performance is still unknown. Larger mini-batches may lead to models getting stuck in sharper global minima, which are sensitive to data variations. Studies show conflicting results on the relationship between batch size and model behavior. In studies by Dinh et al., Li et al., and McCandlish et al., different behaviors of models with varying batch sizes were explored. It was found that increasing batch size can lead to a drop in accuracy beyond a certain point, indicating that larger batch sizes may not significantly improve signal to noise ratio. Additionally, using a larger batch size implies fewer model updates, which can impact the distance the model travels in parameter space. In (Hoffer et al., 2017), it is argued that a larger batch size implies fewer model updates, affecting the distance weights travel and generalization performance. Training with large batches for longer times can improve generalization performance but takes more time than small batches. The batch size also influences the optimization process, as shown in (Ma et al., 2017) for convex functions. The batch size affects model generalization and optimization. Larger batches lead to fewer updates but better generalization. However, they take more time than smaller batches. For convex functions, a critical batch size exists where smaller batches converge faster. Adaptive batch size methods exist but may require dataset-specific tuning and inefficiently use computational resources. Post-local SGD is a distributed optimization algorithm that balances gradient precision with communication costs by allowing workers to update their models independently before synchronizing. It refines large-batch training results and has been shown to improve model generalization and achieve significant speedups. Stochastic weight averaging (SWA) is a method that averages models after multiple epochs, while Post-local SGD lets models diverge for a few iterations before averaging. The mechanisms behind SWA and Post-local SGD are believed to be different, pointing to different phenomena in deep neural network optimization. Stochastic weight averaging (SWA) is a method where models are sampled from later stages of an SGD training run and averaged to improve generalization. SWA has been effective in various domains like deep reinforcement learning, semisupervised learning, and Bayesian inference. In this work, SWA is adapted to accelerate DNN training through an algorithm called SWAP, which involves three phases of training with large mini-batch updates. In this work, SWA is adapted to accelerate DNN training through an algorithm called SWAP. The algorithm involves three phases: large mini-batch updates with synchronization, independent refinement of model copies with smaller batch sizes and different randomizations, and averaging weights to produce the final output. During the SWAP algorithm, DNN training is accelerated through three phases: large mini-batch updates with synchronization, independent refinement of model copies with smaller batch sizes and different randomizations, and averaging weights to produce the final output. The last phase involves averaging the weights of resulting models and computing new batch-normalization statistics. Phase 1 stops before reaching zero training loss or 100% accuracy to prevent optimization from getting stuck. Phase 2 involves reduced batch sizes and small-batch training performed independently and simultaneously by workers. During phase 2 of the SWAP algorithm, small-batch training is conducted independently and simultaneously by workers, resulting in different models due to stochasticity. The testing accuracies of the models diverge from each other, even though they have the same learning rates. The optimal stopping accuracy is a hyper-parameter that needs tuning, and each worker produces a different model at the end of the training process. During phase 1, all workers share a common model with the same generalization performance. In phase 2, small-batch training causes models to diverge, even though learning rates are the same. The averaged model outperforms individual models. The large-batch phase with synchronized models is followed by the small-batch phase with diverging independent models. The test accuracy of the averaged weight model is computed by averaging the independent models and evaluating the test loss. The mechanism behind SWAP is visualized by plotting the error on a plane containing the outputs of the algorithm's phases. Orthogonal vectors u, v are chosen to span the plane with \u03b8 1 , \u03b8 2 , \u03b8 3 . The loss value of model \u03b8 = \u03b8 1 +\u03b1u+\u03b2v is plotted at (\u03b1, \u03b2). Training and testing errors for the CIFAR10 dataset are shown in Figure 2, with 'LB' marking phase one output and 'SGD' marking a single worker's output after phase two. The loss value is plotted at location (\u03b1, \u03b2) by generating a weight vector \u03b8 and computing batch-norm statistics. Training and testing errors for the CIFAR10 dataset are visualized in Figure 2, showing the outputs of phase one ('LB'), a worker's output after phase two ('SGD'), and the final model ('SWAP'). The model in phase 2 moves to a different side of the basin, while the final model is closer to the center due to variations in the basin's topology. During phase 2, the 'SGD' worker lies on the outer edges of the basin, while the final model 'SWAP' is closer to the center. The topology variations in the basin cause higher errors for 'LB' and 'SGD' points, but 'SWAP' is less affected. In Figure 3, 'SGD1', 'SGD2', and 'SGD3' points are at different sides of the basin, with 'SWAP' closer to the center and experiencing lower testing errors. In Figure 3a and 3b, worker points 'SGD1', 'SGD2', and 'SGD3' are plotted along with 'SWAP'. The topology changes result in higher testing errors for 'SGD' workers compared to 'SWAP'. The authors suggest that in later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian. The authors argue that weight iterates in later stages of SGD behave like an Ornstein Uhlenbeck process, reaching a high-dimensional Gaussian distribution. This distribution is centered at the local minimum, with a covariance that grows with the learning rate and is inversely proportional to the batch size. Sampling weights from different SGD runs will result in weights spread out on the surface of an ellipsoid, with their average closer to the center. The authors argue that weight iterates in later stages of SGD behave like an Ornstein Uhlenbeck process, reaching a high-dimensional Gaussian distribution centered at the local minimum. Sampling weights from different SGD runs will result in weights spread out on the surface of an ellipsoid, with their average closer to the center. This allows for independent sampling from the same stationary distribution if all runs start in the same basin of attraction. The advantage of SWA and SWAP over SGD is demonstrated by measuring the cosine similarity between the gradient descent direction and the direction towards the output of SWAP, showing a decrease in similarity as training progresses. The advantage of SWA and SWAP over SGD is demonstrated by measuring the cosine similarity between the gradient descent direction and the direction towards the output of SWAP. As training progresses, the cosine similarity decreases, indicating that averaging samples from different sides of the basin allows for faster progress towards the center. The performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated in this section. Best hyper-parameters were found using grid searches for the experiments. In this section, the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated. Best hyper-parameters were found using grid searches for the experiments. Training was done using mini-batch SGD with Nesterov momentum and weight decay. Data augmentation was done using cutout, and a custom ResNet 9 model was used for training. Experiments were run on a machine with 8 NVIDIA Tesla V100 GPUs using Horovod for computation distribution. Statistics were collected over 10 runs with specific settings for SWAP phase one. The experiments evaluated the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. They used mini-batch SGD with Nesterov momentum and weight decay, data augmentation with cutout, and a custom ResNet 9 model. The experiments were conducted on a machine with 8 NVIDIA Tesla V100 GPUs using Horovod for computation distribution. The experiments compared models trained with small-batch only, large-batch only, and SWAP, showing the best test accuracies and training times for each. The experiments compared models trained with small-batch only, large-batch only, and SWAP on CIFAR10 and CIFAR100 datasets. Training with small-batches achieved higher testing accuracy than large-batches but took longer. SWAP showed significant improvement in test accuracies after averaging the models. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch is able to reach the same accuracy in 27 seconds, outperforming the front-runner of the DAWNBench competition for training CIFAR10. The front-runner of the DAWNBench competition trains CIFAR10 to 94% test accuracy in 37 seconds with 4 Tesla V100 GPUs. Using SWAP with 8 Tesla V100 GPUs, the same accuracy is achieved in 27 seconds. Small-batch experiments on ImageNet are run on 8 Tesla V100 GPUs for 28 epochs, while large-batch experiments on 16 Tesla V100 GPUs modify schedules and run for 22 epochs. Our large-batch experiments on ImageNet double the batch size and learning rates, run on 16 Tesla V100 GPUs for 22 epochs. SWAP phase 1 uses these settings for 22 epochs, while phase 2 runs on 8 GPUs for 6 epochs with small-batch settings. Doubling the batch size reduces accuracies, but SWAP restores performance with faster training times. Results are shown in Table 3, achieved without additional tuning. SWAP accelerates generalization performance with reduced training times, achieved by doubling batch size, GPUs, and learning rates. Results in Table 3 show no need for additional tuning. Comparing SWAP with SWA on CIFAR100 dataset. SWAP accelerates generalization performance by doubling batch size, GPUs, and learning rates. SWAP is compared with SWA on CIFAR100 dataset using the same number of models and epochs per sample. SWA samples each model with 10 epochs in-between, while SWAP uses 8 independent workers for 10 epochs each. The study explores if SWA can recover test accuracy of small-batch training on a large-batch training run. The study compares SWAP with SWA on CIFAR100 dataset, using 8 independent workers for 10 epochs each. SWA samples models with 10 epochs in-between, while SWAP doubles batch size, GPUs, and learning rates. Large-batch training run achieves lower accuracy, but SWA does not improve it. SWA is able to reach test accuracy of small-batch training after a large-batch phase. The study evaluates the effect of executing SWA using small batches after a large-batch training run. SWA can reach the test accuracy of a small-batch run but takes more than three times longer to compute the model. The learning rate schedule is illustrated in Figure 6b. Small-batch SWA and SWAP start the cyclic learning rate schedule from the best model found by solely small-batch training. The peak learning rate is selected using a grid-search, and the settings are reused in SWAP. Phase two starts from the model that was previously used. The study evaluates the effect of executing SWA using small batches after a large-batch training run. SWA can reach the test accuracy of a small-batch run but takes more than three times longer to compute the model. The peak learning rate is selected using a grid-search, and the settings are reused in SWAP. Phase two starts from the model that was previously used. Small-batch SWA achieves better accuracy than SWAP at 6.8x more training time. SWAP achieves a test accuracy of 79.11% in 241 seconds, which is 3.5x faster than SWA. SWAP is a variant of Stochastic Weight Averaging (SWA) that improves model generalization performance by using large mini-batches for quick computation and then refining it with small-batch models. By increasing the phase two schedule and sampling multiple models, SWAP achieved a test accuracy of 79.11% in 241 seconds, 3.5x faster than SWA. The algorithm uses large mini-batches for quick computation and refines the solution by averaging weights from models trained with small-batches. The final model shows good generalization performance and is trained faster. This approach is novel and effective, as demonstrated in image classification datasets. Visualizations support the idea that averaged weights are closer to the center of a training loss basin. The algorithm uses large mini-batches for quick computation and refines the solution by averaging weights from models trained with small-batches. The resulting model performs as well as models trained with small-batches only, confirmed in image classification datasets. Visualizations show that averaged weights are closer to the center of a training loss basin. The basin where the large mini-batch converges seems to be the same basin where refined models are found, suggesting regions with good and bad generalization performance are connected through regions of low training loss. Our method involves choosing a transition point between large-batch and small-batch training through grid search. Future work will focus on a principled method for selecting this point. Additionally, we plan to explore using SWAP with other optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad. SWAP's design allows for substituting these schemes for the large-batch stage, such as using local SGD to accelerate the first stage of SWAP. The parameters used in the experiments of Section 5.1 were obtained through independent grid searches. Momentum and weight decay constants were kept at 0.9 and 5 \u00d7 10 \u22124 for all CIFAR experiments. Tables 5 and 6 list the remaining hyperparameters, with a stopping accuracy of 100% indicating the maximum number of epochs were used."
}