{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are used for control policies in reinforcement and imitation learning. They are challenging to explain due to continuous memory vectors and observation features. A new technique called Quantized Bottleneck Insertion is introduced to learn finite representations of these vectors and features, leading to improved interpretability. Results on synthetic environments and Atari games show small finite representations with as few as 3 memory states and 10 observations for a perfect Pong policy. The study presents results of using Quantized Bottleneck Insertion to learn finite representations of RNN policies in reinforcement and imitation learning. The resulting representations are surprisingly small, with as few as 3 memory states and 10 observations for a perfect Pong policy, leading to improved interpretability. This approach addresses the challenge of explaining RNN policies, which are difficult to understand due to their internal memory encoding features of observation history. In this paper, the focus is on comprehending and explaining RNN policies by learning more compact memory representations. The challenge lies in interpreting the high-dimensional continuous memory vectors used in RNN policies, which are updated through complex gating networks. The hypothesis is that these continuous memories may actually be capturing discrete concepts, and quantizing the memory and observation representation could enhance explainability. The paper focuses on understanding RNN policies by compacting memory representations. It suggests that high-dimensional continuous memory vectors in RNNs may capture discrete concepts, leading to the introduction of a Moore Machine representation through Quantized Bottleneck Network (QBN) insertion. The paper introduces a method to transform an RNN policy with continuous memory and observations into a Moore Machine representation using Quantized Bottleneck Networks (QBN). QBNs encode memory states and observation vectors encountered during RNN operation, replacing the original \"wires\" in the policy. The resulting Moore Machine Network (MMN) with quantized memory and observations is nearly equivalent to the original RNN and can be used directly or fine-tuned for improvements. The paper introduces a method to transform an RNN policy into a Moore Machine Network (MMN) using Quantized Bottleneck Networks (QBN). The MMN with quantized memory and observations is nearly equivalent to the original RNN and can be fine-tuned for improvements. Training quantized networks with QBNs is shown to be effective, especially with \"straight through\" gradient estimators. Experiments in synthetic domains and Atari games demonstrate accurate extraction of ground-truth MMNs and insight into RNN memory use. The experiments show that \"straight through\" gradient estimators are effective in extracting ground-truth MMNs from RNNs. Results from synthetic domains and Atari games reveal insights into RNN memory usage, identifying cases of reactive control and open-loop controllers. Our work explores memory usage in RNN policies, identifying cases of reactive control and open-loop controllers. Previous research has focused on understanding Recurrent Networks, but our work is unique in learning finite-memory representations of continuous RNN policies. This is related to extracting Finite State Machines from recurrent networks trained to recognize languages. Our work focuses on learning finite-state representations of continuous RNN policies, which is distinct from previous research on recurrent networks. Existing approaches extract FSMs from RNNs trained to recognize languages, but our method directly inserts discrete elements into the RNN to learn policies. This allows for a more accurate representation of RNN behavior compared to traditional FSM approximations. Our approach involves directly inserting discrete elements into the RNN to learn policies, preserving its behavior while allowing for a finite state characterization. This method enables fine-tuning and visualization using standard learning frameworks, differentiating it from previous approaches that produce FSM approximations separate from the RNN. Our work extends previous approaches by introducing the method of QBN insertion to learn MMNs and transform pre-trained recurrent policies into finite representations. Unlike prior work on fully binary networks, we focus on learning discrete memory and observation representations while allowing arbitrary activations and weights in the network. Our work focuses on learning discrete memory and observation representations in recurrent neural networks (RNNs) for interpretability, unlike prior work on fully binary networks that aim for efficiency. RNNs are commonly used in reinforcement learning to represent policies with internal memory, updating a continuous-valued hidden state at each time step. Our work aims to extract compact quantized memory representations in RNNs for interpretability. RNNs update a continuous-valued hidden state at each time step based on observations, and output actions using a policy function. The transition to a new state is determined by a transition function, often implemented with gating networks like LSTMs or GRUs. The high-dimensional nature of the hidden state and observation features can make interpreting memory challenging. Our goal is to extract compact quantized representations of hidden states and observations in RNNs for interpretability. We introduce Moore Machines and their deep network counterparts to capture key memory features. Moore Machines are finite state machines labeled by output values, corresponding to actions, with hidden states, initial state, observations, and actions. Moore Machines are finite state machines labeled by output values, corresponding to actions, with hidden states, initial state, observations, and actions. Moore Machine Networks (MMN) represent the transition function and policy using deep networks, especially for continuous observations. A Moore Machine Network (MMN) is a Moore Machine where the transition function and policy are represented by deep networks. MMNs provide a mapping from continuous observations to a finite discrete observation space. The quantized state and observation representations use discrete vectors, with a quantization level denoted as k. MMNs can be viewed as traditional RNNs with restricted memory. An MMN is a traditional RNN with restricted memory composed of k-level activation units. Environmental observations are transformed into a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for non-trivial problems, even though RNNs can be learned with relative ease. Incorporating quantized units into the backpropagation process, learning MMNs from scratch can be challenging for non-trivial problems. A new approach involves using trained RNNs to learn quantized bottleneck networks (QBNs) for embedding continuous features into a k-level quantized representation. These QBNs are inserted into the original recurrent net with minimal behavior changes, resulting in a network that consumes quantized features and maintains quantized state. The approach involves using trained RNNs to learn quantized bottleneck networks (QBNs) for embedding continuous features into a k-level quantized representation. QBNs are inserted into the original recurrent net with minimal behavior changes, resulting in a network that consumes quantized features and maintains quantized state. Quantized Bottleneck Networks (QBNs) aim to discretize a continuous space by quantizing the activations of units in the encoding layer. A QBN consists of a multilayer encoder E mapping inputs to a latent encoding E(x) and a multilayer decoder D. The output of QBN is quantized using 3-level quantization, represented as +1, 0, and -1. To support 3-valued quantization, tanh activation is used for the output nodes of E(x) instead of the traditional tanh function. The QBN utilizes 3-valued quantization represented as +1, 0, and -1. To support this, a modified activation function \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(-3x) is used. The introduction of the quantize function makes b(x) non-differentiable, but the straight-through estimator is effective in handling this issue for backpropagation. The straight-through estimator is effective in handling the non-differentiable quantize function in the QBN for backpropagation. The QBN is trained as an autoencoder using L2 reconstruction error. Recurrent policy can generate training sequences for observation, observation feature, and hidden state. The approach involves training a QBN as an autoencoder using L2 reconstruction error and generating training sequences for observation, observation feature, and hidden state with a recurrent policy. Two QBNs, b f and b h, are trained on observed features and states, serving as high-quality k-level encodings. These encodings are inserted as \"wires\" into the original RNN to propagate input to output with some noise. The QBNs b f and b h, trained as high-quality encodings, are inserted as \"wires\" into the original RNN to propagate input to output with some noise. The resulting MMN may not behave identically due to imperfect reconstruction. The QBNs b f and b h are inserted into the RNN to create a quantized representation of features and states. The resulting MMN may not behave identically due to imperfect reconstruction. Fine-tuning can be done by training the MMN on the original RNN data to match the softmax distribution over actions. Training in this way is more stable than simply outputting the same. During fine-tuning, the MMN is trained on the original RNN data to match the softmax distribution over actions. Visualization tools can be used to investigate memory features for a semantic understanding. Another approach is to use the MMN to create a Moore Machine over discrete state and observation spaces for analysis of machine states and their relationships. The MMN can be used to create a Moore Machine over discrete state and observation spaces for analyzing machine states and their relationships. The Moore Machine is constructed by running the MMN to generate a dataset of quantized states, features, and actions, which are then used to create a transition table capturing the transitions seen in the data. The Moore Machine is created using the MMN to generate quantized states, features, and actions from the data. The transition function is constructed from the data to capture transitions, and minimization techniques are applied to reduce the number of states and observations in the resulting Moore Machine. The experiments aim to determine if MMNs can be extracted from RNNs without significant performance loss and to understand the general magnitude of states and observations. The experiments aim to extract Moore Machine Networks (MMNs) from RNNs without performance loss and understand the number of states and observations in minimal machines. Two domains are considered: Mode Counter synthetic environment and benchmark grammar learning problems. These domains help vary memory requirements and types for policies. The Mode Counter Environments (MCEs) allow for varying memory requirements and types for policies, transitioning between modes over time in a partially observable Markov Decision Process. The agent receives a reward for taking the correct action associated with the active mode at each time step. The Mode Counter Environments (MCEs) involve transitioning between modes over time in a partially observable Markov Decision Process. The agent receives a reward for taking the correct action associated with the active mode at each time step. Different parameterizations place varying requirements on memory use to infer the mode and achieve optimal performance. The experiments test different ways of using memory and observations in three MCE instances. In our experiments, we test three different Mode Counter Environments (MCEs) that require varying levels of memory and observation use. The MCE instances include Amnesia, where memory is not needed for optimal actions, Blind, where memory is crucial for determining optimal actions based on observations, and Tracker, where both memory and observations are essential for optimal action selection. The Mode Counter Environments (MCEs) require memory and observation use for optimal action selection. The MCE instances include Amnesia, Blind, and Tracker, each with different memory and observation requirements. The MCEs use a recurrent architecture with feed-forward and GRU layers to achieve 100% accuracy on the imitation dataset. The MCE environments utilize a recurrent architecture with feed-forward and GRU layers to achieve 100% accuracy on the imitation dataset. Training of bottleneck units in the MCE environments is faster compared to RNN training. Encoders consist of 1 feed-forward layer of tanh nodes, with the number of nodes being 4 times the bottleneck size, feeding into 1 feedforward layer of quantized bottleneck nodes. Training of bottleneck units in MCE environments was fast compared to RNN training, as QBNs do not need to learn temporal dependencies. QBNs were trained with bottleneck sizes of Bf \u2208 {4, 8} and Bh \u2208 {4, 8}, embedded into the RNN to create a discrete MMN. Performance of the MMN was measured before and after fine tuning, with most cases not requiring fine tuning due to low reconstruction error. After inserting bottleneck units into the RNN to create a discrete MMN, performance was measured before and after fine-tuning. Most cases did not require fine-tuning due to low reconstruction error. Fine-tuning resulted in perfect MMN performance in all cases except for Tracker (Bh = 4, Bf = 4), which achieved 98% accuracy. Inserting one bottleneck at a time yielded perfect performance, indicating that combined error accumulation of both bottlenecks reduced performance. Moore Machine Extraction also provided the number of states and observations before and after minimization. After inserting bottleneck units into the RNN to create a discrete MMN, fine-tuning resulted in perfect MMN performance in most cases. Inserting one bottleneck at a time showed that combined error accumulation of both bottlenecks reduced performance. Moore Machine Extraction provided the number of states and observations before and after minimization, showing that minimization resulted in exact minimal machines for each MCE domain. After fine-tuning the discrete MMN created from the RNN with bottleneck units, most cases resulted in perfect performance. The MMNs learned via QBN insertions were equivalent to the true minimal machines, showing optimal performance. Examining the ground truth minimal machines revealed insights into memory use, such as transitions not depending on input observations in some cases. The machines for Blind and Amnesia show different memory use patterns. The machine for Blind has transitions independent of input observations, while the machine for Amnesia's actions are solely determined by the current observation. The study evaluates the approach on 7 Tomita Grammars, treating them as environments with 'accept' and 'reject' actions. Each episode involves a random string from the grammar, with a reward given for the correct action on the last symbol. The RNN training for each grammar is discussed. The focus is on policy learning problems with grammars treated as environments with 'accept' and 'reject' actions. RNNs are trained for each grammar using imitation learning and Adam optimizer. Test results show high accuracy except for grammar #6. The training dataset consists of accept/reject strings with lengths in the range [1, 50]. Test results show high accuracy for RNNs, except for grammar #6. MMNs are trained without bottleneck encoders, with bottlenecks for hidden memory state. Fine-tuning MMNs maintains RNN performance in most cases. The MMNs, created by inserting bottlenecks in RNNs, maintain performance without fine-tuning. Results show reduced MM state-space post-minimization while preserving accuracy. MMN learning does not directly lead to minimal machines but is equivalent to them. Tomita grammars over {0, 1} alphabet are solved, with grammar 6 being the exception. In this section, our technique is applied to RNNs learned for six Atari games using the OpenAI gym. Unlike previous experiments with known ground truth minimal machines, the complexity of Atari's input observations makes it unclear what to expect in terms of results. The study applied the technique to RNNs trained on six Atari games in the OpenAI gym. The complexity of Atari's input observations makes it uncertain what results to anticipate. The recurrent architecture for all Atari agents is the same, with input observations being preprocessed images. The agents have a recurrent architecture for processing input image frames from Atari games. The network includes 4 convolutional layers, a GRU layer, and a fully connected layer. The A3C RL algorithm was used for training with specific parameters. The RNN performance on six games is reported in a table. The policy was trained using the A3C RL algorithm with specific parameters. The RNN performance on six games is reported in a table, with training data generated using noisy rollouts to increase diversity. The decoder in the Atari domains was trained using noisy rollouts to increase diversity in the training data. Bottlenecks were trained for different values of B h and B f, with larger values used due to the complexity of Atari games. Each bottleneck was trained to saturation of training loss and inserted into the RNN to create an MMN for each game. The study involved training bottlenecks for Atari games with different values of B h and B f, with larger values used due to the games' complexity. The bottlenecks were inserted into the RNN to create an MMN for each game. The MMNs showed comparable performance to the RNN after fine-tuning for games like Pong, Freeway, Bowling, and Boxing, demonstrating the ability to learn a discrete representation of input and memory without impacting performance. Fine-tuning was required for Boxing and Pong, while it was not necessary for Freeway and Bowling. After fine-tuning, MMNs achieved similar scores to the RNN in complex games like Boxing and Pong. However, for Breakout and Space Invaders, MMNs scored lower due to poor reconstruction in certain game scenarios. For example, in Breakout, the MMN failed to press the fire-button after clearing the first board, resulting in a lower score. This highlights the need for more intelligent approaches in training MMNs. Further investigation revealed that the drop in performance was caused by poor reconstruction in rare parts of the game, such as in Breakout where the MMN failed to press the fire-button after clearing the first board. Minimizing the MMNs resulted in a significant reduction in the number of states and observations, making it easier to analyze manually. This emphasizes the need for more intelligent training approaches for MMNs to capture critical information in rare but important states. After minimizing the MMNs, the number of states and observations drastically decreases, sometimes to just one state or observation. This simplifies manual analysis, especially for moderately complex policies. In Atari games, we observed three types of memory use, with Pong having only three states and 10 observation symbols. Each observation consistently transitions to the same state regardless of the action. In Atari games, three types of memory use were observed. Pong has three states and 10 observation symbols, with each observation transitioning to the same state regardless of the action. Bowling and Freeway have only one observation symbol in the minimal MM, making their policies open-loop controllers that depend on the time-step rather than observations. In Bowling and Freeway, the minimal Markov Model has only one observation symbol, leading to open-loop policies that ignore input images and depend solely on time-step for actions. The policy in Freeway always takes the Up action, while Bowling has a more complex open-loop structure with an initial sequence of actions followed by a repeated loop. The MM extraction approach successfully identifies these policy structures. The MM extraction approach successfully identifies the open-loop policy structures in Bowling and Freeway, providing significant additional insight into memory use in RNN policies. Future work will involve a full semantic analysis of discrete observations and states in Atari policies, enabled by visualization and interaction tools. Our approach involves extracting finite state Moore Machines from Atari policies by training Quantized Bottleneck Networks to produce binary encodings of RNN memory and input features. This allows us to accurately extract ground truth machines in known environments and analyze memory use in RNN policies. Additionally, we can transform the extracted Moore machines into minimal machines for further analysis and usage in various Atari games. Our approach involves extracting finite state Moore Machines from Atari policies by training Quantized Bottleneck Networks to produce binary encodings of RNN memory and input features. This allows us to accurately extract ground truth machines in known environments and analyze memory use in RNN policies. In experiments with six Atari games, the learned MMNs maintain similar performance to the original RNN policies and provide insight into memory usage. The number of required memory states and observations is surprisingly small, and we can identify cases where memory was not used significantly or ignored observations. The study extracted finite state Moore Machines from Atari policies using Quantized Bottleneck Networks to analyze memory usage. The number of required memory states and observations was found to be small, with some policies not utilizing memory significantly. Future work includes developing tools for attaching meaning to observations and analyzing finite-state machine structures for further insight. The study analyzed finite-state machine structures in Atari policies using tools for insight and formal properties. An MCE is parameterized by mode number, transition function, lifespan mapping, and count set. The hidden state is defined by current mode and time-steps in that mode. Mode changes occur based on lifespan reaching, with transition distribution determining next mode. Agent receives continuous-valued observations but does not directly observe state. The agent receives continuous-valued observations o t \u2208 [0, 1] at each step, based on the current mode (m t , c t ). Observations determine the mode when the mode count is in C, otherwise, they are uninformative. The agent must remember the current mode and track how long it has been active to optimize performance. The agent receives continuous-valued observations to determine the mode when the mode count is in C. To optimize performance, the agent must remember the current mode and track its duration. Experiments are conducted with different MCE instances to test memory usage in policies. The experiments test different MCE instances to evaluate memory usage in policies. The optimal policy in each instance varies in terms of using memory to track information from the past or relying solely on current observations to determine the current mode. The experiments evaluate memory usage in policies by testing different MCE instances. The optimal policy varies in using memory to track past information or relying on current observations to determine the current mode. MMN extraction suggests the policy may be ignoring observations and only using memory. The most general instance involves paying attention to observations when c t = 0 and using memory to keep track of the current mode and mode count, which can lead to challenging problems as the number of modes and their life-spans increase."
}