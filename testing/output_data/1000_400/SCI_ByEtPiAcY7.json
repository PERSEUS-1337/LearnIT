{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complexity of deep networks makes it impossible to explain hidden features in a way that humans can understand. However, a new method using \\textit{M-of-N} rules shows promise in mapping the landscape of rules describing hidden features in Convolutional Neural Networks, finding a balance between comprehensibility and accuracy. In this paper, a knowledge extraction method using \\textit{M-of-N} rules is proposed to map the complexity/accuracy landscape of rules in a Convolutional Neural Network (CNN). The results show an optimal trade-off between comprehensibility and accuracy, with rules in the first and final layers being highly explainable, while those in the second and third layers are less so. This sheds light on the feasibility of rule extraction from deep networks and the value of decompositional knowledge extraction for explainability in Artificial Intelligence. The results highlight the feasibility of rule extraction from deep networks and the importance of decompositional knowledge extraction for explainability in Artificial Intelligence. Recent interest in explainable AI has grown due to the lack of explainability in neural network models, particularly deep networks. These models rely on distributed representations, making them less explainable compared to symbolic AI or symbolic machine learning approaches. Knowledge extraction aims to enhance the explainability of neural networks by revealing the implicit knowledge learned in their weights. This involves translating trained neural networks into symbolic rules or decision trees, similar to those in symbolic AI, machine learning, and logic programming. Knowledge extraction seeks to increase the explainability of neural networks by translating them into symbolic rules or decision trees. Rule extraction techniques have been developed over the years, using either decompositional or pedagogical approaches. The main challenge lies in the complexity of the extracted rules. The complexity of rule extraction from neural networks is a major issue due to the distributed representations found in neural networks, where important concepts are represented by patterns of activity over many neurons. The distributed nature of neural networks, with important concepts represented by patterns of activity over many neurons, has led to the conclusion that explaining latent features using symbolic knowledge extraction is not effective. Distillation is proposed as an alternative method, but its efficacy is debated. Some suggest settling for guarantees on the network's performance rather than attempting to open the black box. In this paper, a method for empirically examining the explainability of latent variables in neural networks is developed. Rule extraction is used to search through a space of rules describing a latent variable, measuring error and complexity. Various error/complexity trade-offs are selected to map out a rule extraction landscape. In this study, rule extraction is used to analyze the explainability of latent variables in neural networks. The research explores a landscape of M-of-N rules to capture the behavior of a network, revealing varying levels of accuracy and complexity in describing different layers. A 'critical point' on the rule extraction landscape indicates an ideal rule for each latent variable, with explainability trends differing across layers and architectures. The study explores the landscape of M-of-N rules to describe latent variables in neural networks, revealing varying levels of accuracy and complexity across different layers. Convolutional layers show more complex rules compared to fully connected layers. Rules in the first and final layers have near 0% error, while those in the second and third layers have around 15% error. Previous algorithms for knowledge extraction are briefly discussed in Section 2. The study examines M-of-N rules in neural networks, showing varying accuracy and complexity levels across layers. Convolutional layers have higher complexity than fully connected layers. Rules in the first and final layers have near 0% error, while those in the second and third layers have around 15% error. Previous knowledge extraction algorithms are briefly mentioned in Section 2. The study explores M-of-N rules in neural networks, with convolutional layers being more complex than fully connected layers. Rules in the first and final layers have minimal error, while those in the second and third layers have around 15% error. Previous knowledge extraction algorithms, such as KBANN, used decompositional approaches to extract symbolic rules from hidden variables. These algorithms generate binary trees representing M-of-N rules, with recent methods selecting rules based on maximum information gain from input units. Recent algorithms extract M-of-N rules from neural networks, treating the model as a black box for rule extraction. Some methods combine pedagogical and decompositional approaches, while others use alternative visually oriented techniques. Various methods have been developed to address the 'black-box' problem of neural networks. Some methods combine pedagogical and decompositional approaches, while others use visually oriented techniques to solve the 'black-box' problem of neural networks. Most decompositional rule extraction techniques focus on input/output relationships rather than explaining the latent features of deep networks, which can result in complex rules that are difficult for humans to understand. The use of decompositional techniques to explain deep network features may be impractical due to the complexity of extracted rules. However, some layers of a deep network may have explainable rules that can provide insight into the network's behavior and latent features. This opens the possibility of using rule extraction for modular explanation of network models. In logic programming, a logical rule is an implication of the form A \u2190 B, where A is the head of the rule and B is the body. Disjunctions in the body can be represented as multiple rules with the same head. Most logic programs use a negation by failure paradigm. When explaining a neural network using rules, it can provide insight into the network's behavior and latent features. When explaining a neural network using rules, the literals refer to the states of neurons. For binary neurons, a literal is true if the neuron is active, and false if not. For neurons with continuous activation, a literal is true if the activation is above a threshold. This approach helps capture the complex behavior of neurons in neural networks. In neural networks, a literal is defined by a threshold where X = True if x > a, and X = False otherwise. Instead of adding a rule for each input pattern that activates a neuron, M-of-N rules are used to soften the conjunctive constraint on logical rules by requiring only M variables in the body to be true. This approach is commonly used in rule extraction for neural networks. M-of-N rules provide a compact representation for neural networks, allowing for a softer constraint on logical rules by requiring only a subset of variables to be true. They offer a more general representation than a simple conjunction and reflect the input/output dependencies of neurons. This rule extraction process does not simply generate a lookup table and shares structural similarities with neural networks. M-of-N rules provide a compact representation for neural networks, offering a more general representation than a simple conjunction. They share structural similarities with neural networks, with M-of-N rules being viewed as 'weightless perceptrons'. These rules have been used in knowledge extraction in the past but have since been forgotten. This paper aims to bring M-of-N rules back into the spotlight in the debate on explainability. This paper highlights the importance of M-of-N rules in neural networks for explainability. It discusses setting bias and weights for output and input neurons, respectively. The process involves choosing splitting values for continuous activation values using information gain. The goal is to generate literals for target neurons based on maximizing information gain with respect to output labels. The process involves selecting splits for continuous neurons based on information gain to generate literals for target neurons, maximizing entropy decrease in network outputs. Input literals are then generated by maximizing information gain with respect to the target literal. Each target literal in a layer has its own set of input literals, corresponding to the same input neurons with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches. In rule extraction, the focus is on generating a single rule for each feature map in a layer based on the maximum information gain with respect to the network output. The two key metrics considered are comprehensibility and accuracy, with accuracy defined in terms of the expected difference between the predictions made by the rules and the network. In rule extraction, accuracy is defined as the expected difference between predictions made by rules and the network. This discrepancy can be measured using input configurations to test the output of rules against the network. In rule extraction, accuracy is determined by comparing predictions made by rules and the network using input configurations. The complexity of a rule is defined by the length of its body in disjunctive normal form (DNF), analogous to Kolmogorov complexity. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), similar to Kolmogorov complexity. It is measured relative to a maximum complexity and normalized by taking the logarithm. For example, a perceptron with specific weights and bias can be represented by a rule h = 1 \u21d0\u21d2 1-of{x 1 = 1, \u00ac(x 2 = 1)}. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), normalized by taking the logarithm. An example is a perceptron represented by a rule h = 1 \u21d0\u21d2 1-of{x 1 = 1, \u00ac(x 2 = 1)}. A loss function for a rule is defined as a weighted sum with a parameter \u03b2 determining the trade-off between soundness and complexity. A brute force search procedure with various values of \u03b2 determines the relationship between the allowed complexity of a rule. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), normalized by taking the logarithm. A loss function for a rule is defined as a weighted sum with a parameter \u03b2 determining the trade-off between soundness and complexity. By using a brute force search procedure with various values of \u03b2, the relationship between the allowed complexity of a rule and its maximum accuracy is explicitly determined. For \u03b2 = 0, the rule with the minimum loss will be the one with minimum error regardless of complexity. For large \u03b2 values, the rule with the minimum loss will have 0 complexity, such as a 1-of-1 rule or trivial rules that always predict true or false. Given a neuron h j with n input neurons x i, splits are generated for each neuron to create sets of literals H j and X i. The complexity of rules is determined by the length of their body in disjunctive normal form (DNF), normalized by taking the logarithm. A loss function for a rule is defined as a weighted sum with a parameter \u03b2 determining the trade-off between soundness and complexity. By using a brute force search procedure with various values of \u03b2, the relationship between the allowed complexity of a rule and its maximum accuracy is explicitly determined. For large \u03b2 values, rules with 0 complexity, like 1-of-1 or trivial rules, are considered. Neurons h j with n input neurons x i generate splits to create sets of literals H j and X i. The search procedure for M-of-N rules with variables X i in the body and H j in the head minimizes L(R) by reordering variables based on the weight magnitude. The search procedure focuses on generating splits for neurons based on the weight magnitude of connections. It considers M-of-N rules for each variable and relies on the ordering of variables to define literals. The complexity of rules is determined by their length in disjunctive normal form, with a loss function balancing soundness and complexity. The search for rules with variables in the body and head minimizes the loss function by reordering variables. The search procedure focuses on generating splits for neurons based on weight magnitude. It considers M-of-N rules for each variable and relies on ordering variables to define literals. The complexity of rules is determined by their length in disjunctive normal form, with a loss function balancing soundness and complexity. The search for rules with variables in the body and head minimizes the loss function by reordering variables. Neurons with n input neurons have O(2^n) possible M-of-N rules, making an exhaustive search intractable. The most accurate rules are assumed to use literals corresponding to neurons with the strongest weights. The conditional independence of hidden units is assumed, except for the final layer. The high accuracy of rules extracted from the softmax layer suggests ordering literals by weight is effective. The algorithm for rule extraction in neural networks orders literals by weight to reduce search complexity. Despite the computational challenges with a large number of examples and input neurons, the algorithm was implemented in Spark on IBM cloud services for efficient processing. The accuracy of extracted rules was evaluated using examples from the training set to assess their performance based on network output. The algorithm for rule extraction in neural networks was implemented in Spark on IBM cloud services. The accuracy of extracted rules was evaluated using examples from the training set. The procedure was demonstrated by examining the extraction process for the first hidden feature in a CNN trained on the fashion MNIST dataset using 1000 random input examples. The extraction process for the first hidden feature in a CNN trained on the fashion MNIST dataset involves selecting 1000 random input examples to compute neuron activations and predicted labels. Each neuron corresponds to a 5x5 patch of the input, with the optimal splitting value determined by information gain. The neuron with the maximum information gain for the first feature is neuron 96. In the first layer of a CNN trained on the fashion MNIST dataset, 784 neurons correspond to different 5x5 input patches. Neuron 96, with an information gain of 0.015 at split value 0.0004, is selected for testing. The input splits are defined based on the maximum information gain with respect to variable H, derived from neuron 96. The test input includes 1000 5x5 image patches centered at (3, 12). An optimal M-of-N rule is determined by searching through rules based on input variables defined by the splits. Neuron 96, with an information gain of 0.015 at split value 0.0004, is selected for testing. Input splits are defined based on maximum information gain with respect to variable H. Test input includes 1000 5x5 image patches centered at (3, 12). Optimal M-of-N rule is determined by searching through rules based on input variables defined by the splits. Three different rules are extracted with varying complexity penalties. The first feature extracted various rules for neuron 96, with the most complex rule being a 5-of-13 rule with 0.025 error. Applying penalties for complexity resulted in simpler rules with higher errors. The technique was demonstrated on the DNA promoter dataset using a feed forward network. The technique applied to the DNA promoter dataset involved extracting rules for neuron 96, resulting in simpler rules with higher errors as penalties for complexity were applied. The relationship between complexity and error in the network's layers showed an exponential trend, with an ideal complexity/error tradeoff observed. In the output layer, the rule 1-of-{H39, H80} achieved 100% fidelity. In the output layer, the rule 1-of-{H39, H80} achieved 100% fidelity to the network. The rules for H39 and H80 were extracted from the input layer with no complexity penalty, resulting in some incorrect classifications for H39 but none for H80. The errors in each layer propagate when stacking rules that do not perfectly approximate each layer. The network's output can be predicted by a hierarchical rule system, but errors can propagate when stacking rules that do not perfectly approximate each layer. To replace a network with hierarchical rules, a single set of splits for each layer must be chosen, introducing more error in the process. Conducting experiments layer by layer independently is necessary to address this issue. To replace a network with hierarchical rules, a single set of splits for each layer must be chosen, introducing more error. Conducting experiments layer by layer independently provides an idealized complexity/error curve for rule extraction with M-of-N rules. Testing the layerwise rule extraction on a basic CNN trained on fashion MNIST in tensorflow reveals insights into the rule extraction landscape of neural networks. In a study using a basic CNN trained on fashion MNIST in tensorflow, layerwise rule extraction was tested. The CNN had standard architecture with convolutional and max pooling layers, followed by a fully connected layer. Rules were extracted and tested against the network using random inputs from the training data. In a study using a basic CNN trained on fashion MNIST in tensorflow, layerwise rule extraction was tested. The CNN had standard architecture with convolutional and max pooling layers, followed by a fully connected layer. Rules were extracted and tested against the network using random inputs from the training data. The testing involved selecting 1000 random inputs from the fashion MNIST training data to test extracted rules against the network. Different sets of extracted rules were produced for each layer by repeating the search procedure for 5 different values of \u03b2. The study tested layerwise rule extraction in a CNN trained on fashion MNIST. Different sets of rules were extracted for each layer by repeating the search procedure for 5 values of \u03b2. The extracted rules showed varying error/complexity trade-offs, with the first and final layers achieving near 0 error, while the second and third layers had a similar accuracy/complexity trade-off. The study tested layerwise rule extraction in a CNN trained on fashion MNIST. The extracted rules from each layer varied in accuracy and complexity. The first and final layers achieved near 0 error, while the second and third layers had a similar accuracy/complexity trade-off. The optimal accuracy/complexity trade-off was not solely based on the number of input nodes. The final layer provided more accurate rules with less complexity compared to the first layer. The study on layerwise rule extraction in a CNN trained on fashion MNIST found that despite varying input nodes, the third layer performed similarly to the second layer. The final layer produced more accurate rules with less complexity compared to the first layer. There is a critical point where error increases rapidly as complexity penalty rises, indicating a natural set of rules for explaining latent features. Existing rule extraction algorithms do not explicitly consider complexity. The study on layerwise rule extraction in a CNN trained on fashion MNIST found a critical point where error increases rapidly as complexity penalty rises, indicating a natural set of rules for explaining latent features. This paper is the first to integrate rule complexity into the extraction algorithm, as shown in Figure 1. Empirical evaluation of popular extraction algorithms is crucial for validation, outlining both limitations and potential. This paper integrates rule complexity into the extraction algorithm, as shown in Figure 1. Empirical evaluation of popular extraction algorithms is crucial for validation, outlining both limitations and potential. In some cases, complex rules have a 15% error rate, while in others, simple rules can achieve near 0% error even with low complexity. This suggests that decompositional rule extraction may not be a general method of explainability. The black box problem of neural networks has hindered their integration into society, leading to a growing need for explainability. Extracted rules from the final layer of CNNs can achieve near 0% error with low complexity, suggesting selective use of decompositional algorithms depending on the layer being explained. Knowledge extraction success in making large neural networks interpretable remains mixed. The need for explainability in neural networks has grown as they become more integrated into society. Knowledge extraction from neural networks has had mixed success, with critics questioning the feasibility of decomposing rules due to the distributed nature of neural networks. A novel search method for M-of-N rules was applied to explain the latent features of a CNN, revealing that latent features can be described by an 'optimal' rule representing an ideal error/complexity trade-off for explanation. The search for M-of-N rules to explain latent features in a CNN revealed that an 'optimal' rule can represent an ideal error/complexity trade-off. Rule extraction may not provide adequate descriptions for all latent variables, but simplifying explanations without reducing accuracy can make it a useful tool for network examination. The search for M-of-N rules in CNNs showed that rule extraction may not describe all latent variables adequately. Simplifying explanations without reducing accuracy can be useful for network examination. Further research is needed to explore the effects of different transfer functions, data sets, architectures, and regularization methods on accuracy and interpretability."
}