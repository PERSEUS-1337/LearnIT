{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions but current training techniques require fully-observed samples. Learning implicit generative models with only partial, noisy observations is challenging. A new method called AmbientGAN is proposed for training Generative Adversarial Networks (GANs) using lossy measurements. This method shows significant improvements on benchmark datasets, with models trained using AmbientGAN achieving 2-4 times better performance. AmbientGAN is a new method for training Generative Adversarial Networks (GANs) using lossy measurements, leading to substantial improvements in qualitative and quantitative performance on benchmark datasets. Models trained with AmbientGAN can achieve 2-4 times higher inception scores compared to baselines. This approach addresses the challenge of training generative models directly from noisy or incomplete samples, solving the issue of collecting enough data to start with. This work introduces AmbientGAN, a method for training Generative Adversarial Networks (GANs) using lossy measurements. It addresses the challenge of training generative models directly from noisy or incomplete samples, allowing for higher performance on benchmark datasets. The approach solves the issue of collecting enough data to start with by training a generative model directly from noisy or incomplete samples. AmbientGAN is a method for training GANs using lossy measurements to learn generative models from noisy or incomplete samples. The approach distinguishes real measurements from simulated ones to construct good generative models, even from low dimensional projections with per-sample information loss. The method is demonstrated to be effective on three datasets, producing samples with good visual quality and high inception scores compared to baseline methods. Our method can construct generative models from noisy observations and low-dimensional projections with information loss. Theoretical results show that the distribution of measured images uniquely determines the distribution of original images in a GAN game. The distribution of measured images uniquely determines the distribution of original images in a GAN game, even with noise. Different measurement models, such as dropout and random projection, are considered, with empirical results presented for celebA dataset. In the empirical work, different measurement models are considered for the celebA dataset, including randomly placed occlusions and learning from noisy, blurred images. By incorporating the measurement process into GAN training, better sample quality is achieved. Incorporating the measurement process into GAN training improves sample quality. Learning from noisy, blurred images in celebA dataset and 2D images in MNIST dataset using different measurement models. AmbientGAN recovers underlying structure in both variants. The AmbientGAN model projects images onto lines to sum pixels, with two variants considered. The first variant forgets the line choice, while the second includes it. Both variants recover underlying structure, but the first cannot identify distribution up to rotation or reflection. Neural network-based generative models can be constructed using autoregressive, adversarial, or combination approaches. Adversarial frameworks are powerful for modeling complex data distributions like images, video, and 3D models. Generative models have various applications explored in prior research papers. Generative models, including adversarial frameworks, are effective in modeling complex data distributions such as images, video, and 3D models. Prior research papers explore the utility of generative priors for solving inverse problems and improving the realism of synthetic data using GANs. Operating generators and discriminators on different spaces has been proposed, with some studies showing that training stability can be improved by using discriminators operating on low-dimensional projections of data. Our work is closely related to previous studies on training stability with low-dimensional projections of samples using GANs. We also mention a study where 3D object shapes were created from 2D projections, which is a special case of the AmbientGAN framework. The notation used includes superscripts 'r' for real distribution, 'g' for generated distributions, 'x' for underlying space, and 'y' for measurements. The real underlying distribution is denoted as p^rx over R^n, with lossy measurements observed on samples from this distribution. The process involves creating 2D projections using weighted sums of voxel occupancies. Superscripts 'r' and 'g' represent real and generated distributions, 'x' denotes the underlying space, and 'y' stands for measurements. A real underlying distribution p^rx over R^n is observed with lossy measurements obtained from samples. Each measurement is an output of a stochastic measurement function f\u03b8: R^n \u2192 R^m, parameterized by \u03b8. The distributions p^rx and p\u03b8 induce a distribution over measurements denoted as p^ry. Sampling \u0398 \u223c p\u03b8 and computing f\u03b8(x) for any x and \u03b8 is assumed to be straightforward. Our goal is to create an implicit generative model of a real distribution p^rx using a set of IID realizations {y1, y2, ..., ys} from the distribution pry. By combining the measurement process with adversarial training, we aim to sample from p^rx using a stochastic procedure. This involves utilizing a random latent vector Z \u2208 R^k, sampled from a distribution pz, within the framework of adversarial training. Our approach combines the measurement process with adversarial training to create an implicit generative model of p^rx. We use a random latent vector Z and a generator G to simulate random measurements on generated objects Xg, distinguishing real from fake measurements with a discriminator. The approach combines the measurement process with adversarial training to create a generative model of p^rx. Random measurements are simulated on generated objects Xg using a discriminator to distinguish real from fake measurements. The AmbientGAN objective is defined based on the discriminator output. The measurement function f \u0398 must be differentiable with respect to its inputs for all \u03b8. G and D are implemented as feedforward neural networks. The AmbientGAN objective is defined based on the discriminator output, with the requirement for the measurement function f \u0398 to be differentiable. The model uses feedforward neural networks for G and D, making it end-to-end differentiable and trainable using a gradient-based approach. Stochastic gradients are computed by sampling Z, \u0398, and Y r in each iteration for parameter updates in G and D. The training procedure alternates between updating parameters of D and G, remaining compatible with various GAN improvements. The approach involves sampling Z, \u0398, and Y r to compute stochastic gradients for updating parameters in G and D. Updates alternate between D and G parameters and are compatible with various GAN improvements. Additional information like per sample labels can be easily incorporated. Different measurement models are used for theoretical and empirical results, focusing primarily on 2D images. In experiments, various GAN models like DCGAN, Wasserstein GAN, and Auxiliary Classifier Wasserstein GAN are used. Measurement models for 2D images include Block-Pixels and Convolve+Noise. The AmbientGAN framework is versatile for different data formats and measurement models. In the context of GAN models like DCGAN and Wasserstein GAN, measurement models for 2D images include Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, and Pad-Rotate-Project. These models manipulate the input image in various ways to generate measurements for further processing. The Pad-Rotate-Project measurement function involves padding the image with zeros, rotating it by a random angle, and summing pixels along the vertical axis to create a measurement vector. Another variation, PadRotate-Project-\u03b8, includes the angle in the measurements. Gaussian-Projection projects onto a random Gaussian vector to recover the true underlying distribution for certain measurement models. Gaussian-Projection involves projecting onto a random Gaussian vector to recover the true underlying distribution for certain measurement models. The approach aims to show a unique distribution consistent with observed measurements, providing a consistency guarantee with the AmbientGAN training procedure. Lemma 5.1 provides a consistency guarantee with the AmbientGAN training procedure by assuming uniqueness of the true underlying distribution given the measurement distribution. Theorems following the lemma show this assumption holds under various measurement models, allowing recovery of the true distribution with the AmbientGAN framework. The AmbientGAN framework provides a consistency guarantee for recovering the true underlying distribution under various measurement models such as Gaussian-Projection, Convolve+Noise, and Block-Pixels. The required conditions are easily satisfied for common scenarios like Gaussian blurring with additive noise. Additionally, the framework can be generalized for any continuous and invertible function. The assumption of a finite discrete set of pixel values is also considered, which is common in practical image representations. Sample complexity results are provided for approximately learning distributions in the AmbientGAN. Theorem 5.4 assumes a finite discrete set of pixel values, common in image representations. It provides a sample complexity result for learning distributions in the AmbientGAN framework under the Block-Pixels measurement model. If p < 1, a unique distribution can induce the measurement distribution, with conditions for optimal generator G. The text discusses the use of generative models on different datasets like MNIST, CelebA, and CIFAR-10. Different GAN models are used for the experiments, with details on architectures and hyperparameters provided in the appendix. The CIFAR-10 dataset contains 32 \u00d7 32 RGB images from 10 classes. Various generative models were used for experiments, including conditional DCGAN, unconditional Wasserstein GAN with gradient penalty, unconditional DCGAN for celebA dataset, and Auxiliary Classifier Wasserstein GAN with gradient penalty for CIFAR-10 dataset. Discriminator architectures remain consistent for measurements with 2D outputs. The architecture used for the CIFAR-10 dataset is an Auxiliary Classifier Wasserstein GAN with gradient penalty (ACWGANGP) following the residual architecture in BID12. Different discriminator architectures are used based on the type of output, such as Block-Pixels, Block-Patch, Keep-Patch, Extract-Patch, and Convolve+Noise for 2D outputs, and fully connected discriminators for 1D projections like Pad-Rotate-Project and Pad-Rotate-Project-\u03b8. Baseline approaches were implemented to evaluate the AmbientGAN framework's performance, aiming to create an implicit generative model for p r x from a dataset of IID samples. The AmbientGAN framework was evaluated using baseline approaches on a dataset of IID samples. A crude baseline involves ignoring measurements, while a stronger baseline considers invertible measurement functions to obtain full-samples for generative modeling. The AmbientGAN framework evaluates baseline approaches on IID samples. A stronger baseline involves using invertible measurement functions to obtain full-samples for generative modeling, despite violations of assumptions in the measurement models. The approach aims to approximate an inverse function to \"unmeasure\" and estimate x i for generative model training. In Section 4, methods to obtain approximate inverse functions for different measurement models are described. For Block-Pixels measurements, blurring the image or using total variation inpainting is used. For Convolve+Noise measurements, Wiener deconvolution is used, and for Block-Patch measurements, Navier Stokes based inpainting method BID2 is utilized. For different measurement models, various approaches are used to approximate the inverse function. Total variation inpainting is used for Block-Pixels measurements, Wiener deconvolution for Convolve+Noise measurements, and Navier Stokes based inpainting method BID2 for Block-Patch measurements. Other measurement models present challenges in obtaining an approximate inverse function. Inverting Pad-Rotate-Project measurements is challenging due to the lack of information about \u03b8. Results with AmbientGAN models are reported on a subset of experiments, showing samples generated by baselines and our models for selected parameter settings. Additional results are available in the appendix. Samples generated by baselines and our models are compared for different experiments using datasets of measurements. Results on MNIST are in the appendix, while results on celebA and CIFAR-10 are shown in FIG5 and FIG7. Baselines struggle to invert the measurements process, producing degraded samples, while our models generate images with good visual quality. In the Convolve+Noise experiment with celebA, measurements are drowned in noise. Our models outperform baselines in inverting the measurements process, producing coherent faces with good visual quality. Different measurement models show signal degradation, with our models creating better results on MNIST and celebA datasets. The study demonstrates the use of two measurement models for generating images of digits from 1D projections. The first model shows rotation and reflection, while the second model produces upright digits. Despite lower visual quality, the method proves successful in generating digit images. The study uses two measurement models to generate digit images from 1D projections. The second model produces upright digits, although with lower visual quality. The method successfully generates digit images, but struggles with complex distributions and requires better training methods for GANs. Inception scores are used to evaluate the generative models in the AmbientGAN framework. The difficulty in learning complex distributions with 1D projections and the need for better understanding of distribution recovery under projection measurement model and improved GAN training methods are highlighted. Inception scores are used to evaluate generative models in the AmbientGAN framework, with results shown for CIFAR-10 and MNIST datasets. The model achieved an accuracy of 99.2% on MNIST data. Different models were trained with varying levels of pixel blocking, showing that AmbientGAN models outperformed baselines. Inception scores were computed for each model, with results plotted against the probability of pixel blocking. Additionally, models were trained on MNIST with varying levels of additive noise, showing the impact on the inception score. For Convolve+Noise measurements on MNIST with varying levels of additive noise, AmbientGAN models outperformed baselines. Inception scores were higher for models with higher noise levels, while Pad-Rotate-Project model performed poorly with an inception score of 4.18, compared to Pad-Rotate-Project-\u03b8 model with an inception score of 8.12. The Pad-Rotate-Project model produces poorly aligned digits with an inception score of 4.18, while the Pad-Rotate-Project-\u03b8 model achieves a score of 8.12. The vanilla GAN model trained with fully-observed samples scores 8.99. The second model trained on 1D projections performs close to the fully-observed case. Total variation inpainting method is slow and not run on CIFAR-10 dataset. Generative models require high-quality datasets, but a new approach learns from incomplete, noisy measurements. Inception score vs probability of blocking pixels shows superiority over baselines. Inpainting method slow, not used on CIFAR-10. Inception score plotted against training iteration. Hope to construct new generative models with limited data. Generative models typically need high-quality datasets, but a novel method learns from incomplete, noisy measurements. This approach aims to construct new generative models even when only limited data is available. The text discusses the uniqueness of probability distributions in matching measurement distributions in generative models, even with incomplete and noisy data. The Cramer-Wold theorem is used to show that matching 1D marginals leads to convergence to the true underlying distribution. The Cramer-Wold theorem states that matching 1D marginals leads to convergence to the true underlying distribution. In the Convolve+Noise measurement model, if the support of the convolution kernel and additive noise distribution is empty, there exists a unique distribution that induces the measurement distribution. The Cramer-Wold theorem states that matching 1D marginals leads to convergence to the true underlying distribution. In the Convolve+Noise measurement model, if the support of the convolution kernel and additive noise distribution is empty, there exists a unique distribution that induces the measurement distribution. This unique distribution can be represented by a bijective map between X and Z, where the pdfs of X and Z are related through a Jacobian function. The pdf of the sum of two random variables Y is a convolution of the individual probability density functions. The pdfs of X and Z are related through a Jacobian function, and the pdf of the sum of two random variables Y is a convolution of individual probability density functions. The reverse map from the measurement distribution uniquely determines the true underlying distribution concluding the proof. The reverse map from the measurement distribution uniquely determines the true underlying distribution, as shown in the proof by considering a dataset of measurement samples and defining the empirical version of the vanilla GAN objective. The optimal discriminator and generator for the empirical objective are derived, with the Empirical Risk Minimization (ERM) version of the loss being equivalent to taking the expectation of the data dependent term with respect to the empirical distribution. The optimal discriminator for the empirical objective is derived by fixing it to be optimal, leading to any optimal generator satisfying p g y = p r y. The proof involves replacing the real data distribution with the empirical version, resulting in a unique distribution when p < 1 in the Block-Pixels measurement model. In the Block-Pixels measurement model, if p < 1, there is a unique distribution. By applying random measurement functions to samples from a discrete distribution, the distribution over measurements can be written in terms of a transition matrix A. If A is invertible, the distribution p x is recoverable from p y. The sample complexity is determined by the minimum eigenvalue of A, which is guaranteed to be greater than 0 if A is invertible. The distribution over measurements can be written in terms of a transition matrix A. If A is invertible, the distribution p x is recoverable from p y. The sample complexity is determined by the minimum eigenvalue of A, which is guaranteed to be greater than 0 if A is invertible. The dataset of measurements can be represented as Y j k = I(y k = j). The optimal generator must satisfy p, and in the specific case of Block-Pixels measurement, images are divided into classes based on the number of zero pixels. The optimal generator must satisfy p. In the specific case of Block-Pixels measurement, images are divided into classes based on the number of zero pixels. The transition matrix is lower triangular due to the independent blocking of pixels with probability p. Each image has at least (1 - p) n chance of being unaffected by the blocking. The transition matrix for Block-Pixels measurement is lower triangular, with diagonal entries being strictly positive and at least (1 - p) n. This proves that A is invertible, with the smallest eigenvalue being (1 - p) n. The DCGAN model on MNIST follows a specific architecture. The DCGAN model on MNIST uses a specific architecture with a noise input of 100 dimensions sampled from a uniform distribution. The generator has two linear layers and two deconvolutional layers, while the discriminator has two convolutional layers and two linear layers. Batch-norm is used in both models. The WGANGP model on MNIST has a generator that takes a latent vector of 128 dimensions sampled from a uniform distribution. The discriminator in the WGANGP model on MNIST uses two convolutional layers and two linear layers, with batch-norm applied. The generator takes a 128-dimensional latent vector with IID Uniform sampling. In the unconditional DCGAN model on celebA, the generator has a 100-dimensional latent vector with Uniform sampling, while the discriminator uses four convolutional layers and a linear layer, with batch-norm applied. The ACWGANGP model on CIFAR-10 follows a similar architecture. The ACWGANGP model on celebA uses a latent vector with 100 dimensions and follows a specific architecture. The generator has one linear layer and four deconvolutional layers, while the discriminator has four convolutional layers and a linear layer with batch-norm. On CIFAR-10, the ACWGANGP model follows a residual architecture with a 128-dimensional latent vector sampled from a standard Gaussian distribution. The generator includes a linear layer and three residual blocks, each consisting of conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator has one residual block with two convolutional layers. The ACWGANGP model on celebA uses a latent vector with 100 dimensions and follows a specific architecture. The generator has one linear layer and four deconvolutional layers, while the discriminator has four convolutional layers and a linear layer with batch-norm. On CIFAR-10, the ACWGANGP model follows a residual architecture with a 128-dimensional latent vector sampled from a standard Gaussian distribution. The generator includes a linear layer and three residual blocks, each consisting of conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator has one residual block with two convolutional layers. In our analysis and experiments, we explore scenarios where the parameter distribution is only approximately known, aiming for robust training processes and high-quality learned generators. The AmbientGAN approach is tested for robustness to parameter distribution mismatches in the measurement function. Using the Block-Pixels measurement model on the MNIST dataset, different blocking probabilities are applied to train AmbientGAN models. The study empirically shows that the approach remains robust even with approximate knowledge of parameter distribution. The study tests the robustness of the AmbientGAN approach to parameter distribution mismatches in the measurement function using the Block-Pixels model on the MNIST dataset. Different blocking probabilities are applied to train AmbientGAN models, showing that the approach remains robust even with approximate knowledge of parameter distribution. The generator learned through AmbientGAN captures the data distribution well, and the approach is also explored for compressed sensing. The study demonstrates that the AmbientGAN approach is robust to parameter distribution mismatches in the measurement function. By training an AmbientGAN with Block-Pixels model on MNIST with p = 0.5, the generator captures the data distribution well. Using the learned generator for compressed sensing, a reduction in the number of measurements is observed compared to Lasso, showcasing the effectiveness of AmbientGAN even with corrupted samples."
}