{
    "title": "Byekm0VtwS",
    "content": "The uncertainty in intelligence is crucial for flexibility and creativity. Neuromorphic computing chips with uncertainty can mimic the brain, but current neural networks do not consider this. A proposed uncertainty adaptation training scheme improves neural network performance on uncertain chips, achieving comparable results to original platforms. The uncertainty adaptation training scheme (UATS) improves neural network performance on uncertain neuromorphic computing chips by incorporating uncertainty into the training process. This scheme allows neural networks to achieve comparable inference performances on uncertain chips compared to original platforms, outperforming systems without this training. Uncertainty reasoning is essential for human thinking and intelligence, with two types of uncertainties in intelligent systems: fuzziness and stochasticity. Fuzziness helps the brain efficiently process real-world information by ignoring redundant details, while stochasticity enhances creativity. The brain efficiently processes real-world information by ignoring redundant details through fuzziness and enhances creativity with stochasticity. These characteristics are lacking in most existing AI systems, like deep neural networks using 32-bit or 64-bit floating numbers for weights and activations. Some researchers suggest that 8-bit integers are sufficient for many applications. Additionally, the training results remain consistent regardless of the number of repetitions. The text discusses the use of deep neural networks with 32-bit or 64-bit floating numbers for weights and activations. Some researchers propose using 8-bit integers for many applications. Methods like network quantization and Bayesian networks address issues, while neuromorphic computing chips offer a hardware approach. Emerging nanotechnology devices and crossbar structure-based chips have advanced, utilizing Ohm's law and Kirchhoff's law for efficient vector-matrix multiplication. The emerging nanotechnology device and crossbar structure based neuromorphic computing chips have developed significantly in recent years. The crossbar structure, utilizing Ohm's law and Kirchhoff's law, is efficient for vector-matrix multiplication. Nanoscale nonvolatile memory devices at each cross point provide additional storage capability. The computing in memory (CIM) architecture can alleviate the memory bottleneck in von Neumann architecture, making neuromorphic computing chips more energy and area efficient for AI applications. The computing in memory (CIM) architecture in neuromorphic computing chips addresses the memory bottleneck in von Neumann architecture, enhancing energy and area efficiency for AI applications. Uncertainty in these chips arises from fuzziness due to analog to digital converters (ADCs) and stochasticity. The uncertainty in neuromorphic computing chips comes from fuzziness caused by ADCs and stochasticity induced by NVM devices. The stochasticity of NVM devices is due to random particle movement, leading to varied conductance and different output currents even with the same voltage. In neuromorphic computing chips, the stochasticity of NVM devices is caused by random particle movement, resulting in varied conductance and different output currents even with the same voltage. This stochasticity can be utilized to improve chip performance. Various types of NVM devices include phase change memories, filamentary migrating oxide devices, and ferroelectric tunnel junction synapses. The stochasticity of NVM devices in neuromorphic computing chips can be utilized to enhance performance. Different types of NVM devices exhibit varying levels of stochasticity due to intrinsic physical mechanisms. A Gaussian distribution is commonly used to model device stochasticity, with the mean representing the conductance value of the stable state. The variance of the distribution is typically related to the mean based on experimental findings. The stochasticity of NVM devices in neuromorphic computing chips can be modeled using a Gaussian distribution, where the mean represents the conductance value of the stable state. The variance of the distribution is usually correlated to the mean. A linear relationship between the standard deviation and the mean is assumed for simplicity. Conductances below a minimum value are cut off in the model. The conductance distribution is positively correlated to the mean in NVM devices. Conductances below a minimum value are cut off, and the stochastic conductance is modeled using a Gaussian distribution. Writing the conductance of each device is essential for neuromorphic computing chips in AI applications. The conductance of each device in a neuromorphic computing chip is crucial for AI applications. Mapping the target conductance based on neural network weights involves scaling weights into the device's working range. Using the difference between two device conductances to express a weight allows for positive or negative values. To optimize energy efficiency, lower conductances are preferred. The mapping algorithm involves determining the absolute value of a weight and maximizing it within the conductance range. The conductance of each device in a neuromorphic computing chip is crucial for AI applications. To optimize energy efficiency, lower conductances are preferred. The mapping algorithm involves determining the absolute value of a weight and maximizing it within the conductance range. However, the conductance cannot be written accurately due to the stochasticity of the device and the fuzziness of the circuit. The conductance of a neuromorphic computing chip is affected by device stochasticity and measurement accuracy. A model using Gaussian distribution is used to describe fuzziness, with a constant \u03b2 representing device uncertainty. Direct programming of a DNN into the chip introduces uncertainty. The uncertainty in neuromorphic computing chips, represented by \u03b2, impacts the performance of DNNs. Uncertainty can decrease classification accuracy but can be mitigated by uncertainty adaptation training schemes (UATS). UATS guides neural networks to handle uncertainty during training by introducing a stochasticity model in the feed forward process. The uncertainty adaptation training scheme (UATS) improves accuracy by introducing stochasticity and fuzziness models during training. Stochasticity model uses random variables in feed forward process, while fuzziness model replaces weights with random variables after k epochs. The uncertainty adaptation training scheme (UATS) introduces stochasticity and fuzziness models during training to improve accuracy. The fuzziness model replaces weights with random variables after k epochs, and the loss function is calculated by the average output of n FF processes with the same input batch. UATS was evaluated on multiple models and datasets. The network was trained with uncertainty, and a new way to calculate the loss function was introduced. The models were evaluated on the MNIST dataset with different MLP and CNN models. Training set had 60,000 images, with 50,000 for training and 10,000 for validation. Test error was calculated on 10,000 images. Different values of G min, G low, and G high were used in the experiments. The LeNet-5 CNN model was used with 60,000 images in the MNIST dataset. 50,000 images were for training, 10,000 for validation, and 10,000 for testing. Different uncertainty levels were applied, and the CNN model showed the best performance with higher uncertainty leading to higher test errors. Without using the UATS, uncertainty increased test errors for both MLP and CNN models. The CNN model (LeNet-5) performed best without uncertainty but was most affected by it. UATS was used to tune weights and retrain models, showing significant improvement in performance. UATS was used to tune weights and retrain models, showing significant improvement in performance. k=5,n=5 and k=10,n=5 were used in fine-tuning and retraining experiments respectively, with 25 and 100 epochs. UATS improved accuracies with the same uncertainty level in both experiments, with retraining results mostly better than fine-tuning. UATS achieved comparable results to the ideal case when uncertainty level was small. Additionally, UATS was validated on CIFAR-10 dataset with a more complex DNN model, ResNet-44, using \u03b1 = 0.1, \u03b2 = 0.1, k = 10. The UATS was validated on CIFAR-10 dataset with a more complex DNN model, ResNet-44, achieving lower error rates than ideal cases with proper hyper-parameters. It acts as a regularization method for easier DNN training, especially effective with neural networks having more layers. The uncertainty in intelligent systems is crucial, and Bayesian networks are useful for building uncertain neural networks, although controlling weight distributions can be challenging for neuromorphic computing chips. The Bayesian network is a useful method for building uncertain neural networks, but controlling weight distributions can be challenging for neuromorphic computing chips. Various distributions, such as Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli, have been explored to model device stochasticity. The Bayesian network is used for uncertain neural networks on neuromorphic computing chips. Different distributions like Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli are used to model device stochasticity. The performance of networks using these distributions with the same mean and variance is similar due to the VMM transforming individual device distributions into random parameters. The computation intensity of UATS can be reduced by sampling weights for inputs or batches instead of every VMM. The computation intensity of UATS can be reduced by sampling weights for inputs or batches instead of every VMM, accelerating simulation speed while achieving similar results."
}