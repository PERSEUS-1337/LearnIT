{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework using spelling error correction, tokenization, and neural models was proposed to standardize non-systematic names to systematic names, achieving 54.04% accuracy on the test dataset. The framework proposed involves converting non-systematic chemical names to systematic names using spelling error correction, tokenization, and neural models. The standardization accuracy on the test dataset reached 54.04%, a significant improvement. Chemical substances are assigned systematic names based on their structures according to IUPAC rules, but they may also have common or trivial names. The International Union of Pure and Applied Chemistry (IUPAC) defines rules for systematic chemical names. However, besides systematic names, chemical substances may also have common names or proprietary names in the pharmaceutical industry to differentiate products. For example, sucrose is commonly known as sugar, while Aspirin is a proprietary name for acetylsalicylic acid. In the chemistry industry, producers create proprietary names for chemical substances to differentiate their products. Chemical information extraction research extracts useful chemical knowledge from text to update databases like PubChem and SciFinder. Chemical databases like PubChem and SciFinder store chemical information, including names, structures, and formulas. Extracting chemical information from papers is ongoing work to update these databases. Systematic names can be converted to SMILES and InCHI representations for generating structural formulas. OPSIN is a system that can automatically convert systematic names to SMILES with high precision. The differences between non-systematic names and systematic names can be considered as errors in natural language processing. These errors include spelling, ordering, common name, and synonym errors. OPSIN is a system that converts systematic names to SMILES with high precision. Non-systematic names have errors such as spelling, ordering, common name, and synonym errors. These errors can occur simultaneously in a single name, making the task challenging. OPSIN is a system that accurately converts systematic names to SMILES. Non-systematic names can have spelling, ordering, common name, and synonym errors. These errors can appear together in a single name, making the task challenging. A framework is proposed to automatically convert non-systematic names to systematic names, involving spelling error correction, BPE tokenization, and a sequence to sequence model. This task is challenging and few works have been done on chemical name standardization. The text discusses the challenges of standardizing chemical names and introduces a framework involving spelling error correction, BPE tokenization, and a sequence to sequence model. Previous work in this area, such as ChemHits, relied heavily on chemical knowledge, while the proposed approach utilizes a sequence to sequence model commonly used in neural machine translation. The text introduces a sequence to sequence model for standardizing chemical names, avoiding reliance on chemical knowledge. The model is trained end-to-end without external data, achieving 54.04% accuracy on test data extracted from Chemical Journals with High Impact Factors. The framework is fully data-driven, achieving 54.04% accuracy on test data from Chemical Journals with High Impact Factors. The corpus contains 384816 data pairs of non-systematic and systematic chemical names, with an overview of Levenshtein distance distribution shown in FIG1. In the experiment, 80%, 19%, and 1% of the data are used as training, test, and development sets respectively to correct spelling errors in chemical substance names. The names are separated into elemental words to build vocabularies for systematic and non-systematic names. To correct spelling errors in chemical substance names, vocabularies for systematic and non-systematic names are built by separating names into elemental words. The systematic names are split to build the systematic elemental vocabulary, while the non-systematic names are used to create an elemental vocabulary of common names or synonyms. These two vocabularies are then combined to form a final elemental vocabulary for efficient correction search using BK-Tree BID0. Illustration of the framework for building elemental vocabularies from systematic and non-systematic names using BK-Tree for efficient spelling error correction. The BK-Tree is used to efficiently correct spelling errors in non-systematic names by comparing Levenshtein distances. It allows for easy insertion of new training data, making it scalable. The process involves breaking down chemical substance names into elemental words, inputting them into the BK-Tree for correction, and then combining them to form the full name. The BK-Tree corrects spelling errors in non-systematic chemical names by comparing Levenshtein distances. It breaks names into elemental words, corrects them, and combines them to form the full name. This process reduces noise in training the sequence to sequence model. The BK-Tree corrects spelling errors in chemical names using Levenshtein distances. It tokenizes names using Byte Pair Encoding (BPE) BID11 for the sequence-to-sequence model. The paper utilizes Byte Pair Encoding (BPE) BID11 for tokenizing chemical names. It initializes a symbol set by splitting names into characters and iteratively merges symbol pairs to create a vocabulary set for tokenization. BPE is chosen for its ability to handle out-of-vocabulary words and separate names into meaningful subwords. The paper uses Byte Pair Encoding (BPE) for tokenizing chemical names. BPE can handle out-of-vocabulary words and separate names into meaningful subwords. After tokenization, split pairs are used to train a sequence to sequence model, which consists of two recurrent neural networks (RNN) - an encoder and a decoder. The sequence to sequence model BID12 is adapted from OpenNMT BID5 with modifications. It consists of an encoder using a multilayer bidirectional LSTM (BiLSTM) to generate a context vector H, and a decoder to produce target sequences. The BiLSTM includes two LSTMs processing sequences forward and backward, with hidden states at each time step. The context vector H is obtained by combining all hidden states at the final time step of the encoder. The sequence to sequence model BID12 utilizes a bidirectional LSTM encoder to generate a context vector H by combining forward and backward hidden states. The decoder calculates the probability of output sequences, with parameters such as threshold values for BK-Tree and merge operations for BPE. The model uses word embeddings and hidden states of dimension 500. In the experiment, different threshold values and merge operation values were tested for the BK-Tree and BPE stages. The sequence to sequence model used 500 dimensions for word embeddings and hidden states, with a vocabulary size determined by basic characters and merge operations. The encoder and decoder both had 2 layers. Spelling error correction was done before training. Parameters were trained jointly using SGD with a cross-entropy loss function. During training, parameters of the sequence to sequence model are trained jointly using stochastic gradient descent (SGD) with a cross-entropy loss function. The loss is computed over a minibatch of size 64 and normalized. Weights are initialized with a random uniform distribution. The model is trained for 15 epochs with a dropout rate of 0.3. A beam size of 5 is set for decoding. Another experiment replaces the sequence to sequence model with a Statistical Machine Translation (SMT) model using the Moses system. In the experiment, the model is trained for 15 epochs with a dropout rate of 0.3 and a beam size of 5 for decoding. Data augmentation is used to handle noisy data, specifically spelling errors, by inserting errors into non-systematic names with a probability of 0.025. The neural model learning technique involves dealing with noisy data, specifically spelling errors. Data augmentation is used by inserting errors into non-systematic names with a probability of 0.025. Four types of error insertion methods are applied equally, and accuracy and BLEU score BID10 are used to measure standardization quality. The experiment results for different models on a test dataset are shown in TAB3, with the combination of spelling error correction, BPE tokenization, and sequence to sequence model achieving the best performance. The framework shows a significant improvement compared to the SMT model and the ChemHits system. The results for different numbers of BPE merge operations are shown in TAB4, with 5000 being the optimal value. The experiment results show that the framework has a significant improvement over the SMT model and ChemHits system. BPE tokenization is useful, with 5000 merge operations being optimal. Spelling error correction is helpful, while data augmentation is not as effective. Overcorrection can reduce standardization quality. Examples in Table 6 demonstrate the capabilities of the sequence to sequence model. Data augmentation is not as effective as spelling error correction in the framework. Overcorrection can reduce standardization quality. Examples in Table 6 show the sequence to sequence model's ability to fix non-alphabet spelling errors and correct ordering and synonym errors in chemical names. The sequence to sequence model can correct ordering and synonym errors in chemical names, as shown in examples in Table 6. An illustration of attentions in an example is provided in FIG2, where non-systematic names are standardized to their corresponding systematic names. The seq2seq model can correct errors in chemical names, such as ordering and synonym errors. Examples in Table 6 show how non-systematic names are standardized to systematic names. In a study, 100 samples of failed attempts were analyzed to identify error types. In a study, 100 samples of failed attempts were analyzed to identify error types in standardizing chemical names. Synonym errors were found to be the most confusing, while spelling errors were handled well by the system. Common errors were challenging due to the lack of rules for unseen common names. Some samples were nearly correct, some were totally incorrect, and the rest were partially correct. The system performed poorly in standardizing chemical names, with 10 samples nearly correct, 7 totally incorrect, and the rest partially correct. The accuracy varied based on name length, performing best for names between 20 and 40 characters. The model also lacked consideration of chemical rules, leading to some names not following them. Our framework achieves the best performances for systematic names of length between 20 and 40 but performs poorly for names longer than 60, which make up 37% of the test dataset. The model does not consider chemical rules, resulting in some generated names not following them. Examples of failed attempts are provided in Table 8. The framework proposed in this work aims to automatically convert non-systematic chemical names to systematic names using spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. It achieves an accuracy of 54.04% on the dataset, outperforming previous rule-based systems. This advancement enables more practical extraction of chemical information. Our framework, utilizing spelling error correction, byte pair encoding tokenization, and a sequence to sequence model, achieves an accuracy of 54.04% on our dataset. This outperforms previous rule-based systems by nine times, enabling practical extraction of chemical information. The framework is end-to-end trained, data-driven, and independent of external chemical knowledge, opening a new research line in chemical information extraction."
}