{
    "title": "rkhlb8lCZ",
    "content": "Wavelet Pooling is introduced as an alternative to traditional neighborhood pooling in Convolutional Neural Networks for image and object classification. It decomposes features into a second level decomposition, reducing feature dimensions and addressing overfitting. Experimental results show it outperforms or performs comparably with other pooling methods like max, mean, mixed, and stochastic pooling. The proposed method addresses overfitting in max pooling, providing a more compact feature reduction approach. Experimental results show it outperforms or matches other pooling methods like max, mean, mixed, and stochastic pooling in benchmark datasets. CNNs are widely used for image and object classification due to their superior accuracy compared to vector-based deep learning techniques. Researchers constantly innovate on CNN components like convolutional and pooling layers to enhance accuracy and efficiency. The convolutional neural network (CNN) algorithm is constantly evolving to improve accuracy and efficiency. Pooling, such as max pooling and average pooling, helps reduce parameters, increase computational efficiency, and prevent overfitting in CNNs. Max pooling, introduced by Cresceptron, is the most popular form of pooling. Pooling in deep learning involves subsampling the results of convolutional layers to gradually reduce spatial dimensions, aiming to decrease parameters, enhance computational efficiency, and prevent overfitting. Max pooling and average pooling are common methods, but they have limitations that hinder optimal network learning. Other approaches like mixed pooling and stochastic pooling use probabilistic methods to address these issues. Despite their differences, all pooling operations utilize a neighborhood approach similar to nearest neighbor interpolation in image processing, which offers speed, simplicity, and efficiency but may introduce artifacts. The proposed wavelet pooling algorithm offers a new approach to subsampling features in deep learning, aiming to minimize artifacts and improve data regularization. It utilizes second-level wavelet decomposition instead of nearest neighbor interpolation, providing a more accurate representation of feature contents. The algorithm is compared to other pooling methods like max, mean, mixed, and stochastic pooling to validate its effectiveness. Our approach utilizes second-level wavelet decomposition for feature subsampling, avoiding nearest neighbor interpolation for a more accurate representation. We compare our pooling method to max, mean, mixed, and stochastic pooling on image classification datasets like MNIST, CIFAR-10, SHVN, and KDEF. The paper is organized into background, proposed methods, experimental results, and summary/conclusion sections. In this study, different pooling methods like max pooling and average pooling are compared for feature map condensation in convolutional layers. The paper discusses the use of second-level wavelet decomposition for accurate feature subsampling in image classification datasets like MNIST, CIFAR-10, SHVN, and KDEF. The organization of the paper includes sections on background, proposed methods, experimental results, and summary/conclusion. Pooling methods like max pooling and average pooling are compared for feature map condensation in convolutional layers. Max pooling selects the maximum value of a region for the condensed feature map, while average pooling calculates the average value. Both methods have their advantages and disadvantages, with max pooling potentially erasing details from an image depending on the data. Researchers have developed probabilistic pooling methods like mixed pooling, which combines max and average pooling by randomly selecting one method during training to address the shortcomings of traditional pooling methods. Researchers have created probabilistic pooling methods like mixed pooling, which combines max and average pooling by randomly selecting one method during training. Mixed pooling can be applied in three different ways: for all features within a layer, mixed between features within a layer, or mixed between regions for different features within a layer. Another method, stochastic pooling, improves upon max pooling by randomly sampling from neighborhood regions based on probability values. The pooling method called stochastic pooling improves upon max pooling by randomly sampling from neighborhood regions based on probability values. The pooled activation is sampled from a multinomial distribution to pick a location within the region, with activations having the highest probabilities having a higher chance of selection. Stochastic pooling is based on probability, not determinism, and avoids being deterministic. The proposed pooling method uses wavelets to reduce feature map dimensions and minimize artifacts from neighborhood reduction. It avoids the shortcomings of max and average pooling while incorporating some advantages of max pooling. The proposed wavelet pooling method reduces feature map dimensions and minimizes artifacts from neighborhood reduction by performing a 2nd order decomposition in the wavelet domain. This approach aims to capture data compression more organically, resulting in fewer jagged edges and artifacts that could affect image classification. The proposed wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT) to obtain detail and approximation coefficients. This process involves applying the FWT twice on images to generate detail subbands (LH, HL, HH) and an approximation subband (LL) at each decomposition level. The proposed wavelet pooling algorithm performs backpropagation by reversing the process of its forward propagation. It involves 1st order wavelet decomposition of image features, followed by upsampling of detail coefficient subbands by a factor of 2 to create a new 1st level. The wavelet pooling algorithm for forward propagation involves 1st order wavelet decomposition of image features, followed by upsampling of detail coefficient subbands to create new levels. Backpropagation reverses this process using Haar wavelet basis for reconstruction. The backpropagation algorithm of wavelet pooling uses Haar wavelet basis for reconstruction. Experiments are conducted on a 64-bit operating system with specific hardware configurations. Different CNN structures are tested with various regularization techniques for pooling results evaluation. The study evaluates different regularization techniques for pooling results on CIFAR-10 and SHVN datasets. The proposed method outperforms all others, with max pooling showing signs of overfitting. The network architecture is based on the MNIST structure with batch normalization. The study compares different pooling methods on CIFAR-10 and SHVN datasets. The proposed method outperforms others, with max pooling showing signs of overfitting. Different pooling methods show varying trajectories in learning and error reduction. Two sets of experiments are run with and without dropout layers and batch normalization to observe their effects. The study compares various pooling methods on CIFAR-10 and SHVN datasets. Two sets of experiments are conducted, one without dropout layers and the other with dropout and batch normalization. The proposed method demonstrates the second highest accuracy, with wavelet pooling resisting overfitting. Mixed and stochastic pooling show consistent learning progression. In experiments comparing pooling methods on SHVN dataset, dropout and TAB1 method show second highest accuracy. Max pooling overfits quickly, while wavelet pooling resists overfitting. Learning rate changes prevent overfitting, with slower learning progression. Mixed and stochastic pooling maintain consistent learning. Two sets of experiments are run, one without dropout and one with dropout, using different numbers of images for training and validation. The SHVN experiments compared different pooling methods, with dropout and TAB1 showing the second highest accuracy. Max pooling overfits quickly, while wavelet pooling resists overfitting. Mixed, stochastic, and average pooling maintain a slow progression of learning. The proposed method follows the path of max pooling but performs slightly better in maintaining stability. Our method, following the path of max pooling, performs slightly better in maintaining stability compared to other pooling methods. The KDEF dataset used for experiments contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at different poses. Some errors in the dataset were fixed before conducting the experiments. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at different poses. Errors in the dataset were fixed by mirroring missing or corrupted images and manually cropping them to match specified dimensions. The data was shuffled, with 3,900 images used for training and 1,000 for testing. Images were resized to 128x128 due to memory and time constraints, and dropout layers were used to regulate the network and prevent overfitting. Our proposed method showed improved stability compared to other pooling methods. The KDEF dataset consists of 4,900 images of 35 people displaying seven basic emotions. The data was shuffled, with 3,900 images for training and 1,000 for testing. Images were resized to 128x128 and dropout layers were used to prevent overfitting. The proposed method showed improved stability compared to other pooling methods, with wavelet pooling resisting overfitting and maintaining consistent learning progression. The computational complexity of wavelet pooling was noted as inefficient, presented as a proof-of-concept. The proposed wavelet pooling method shows improved stability and consistent learning progression compared to other pooling methods. However, its computational efficiency is noted as inefficient, serving as a proof-of-concept for potential improvements in code optimization and mathematical operations. The efficiency of different pooling methods is calculated based on mathematical operations utilized. Max pooling considers worst-case scenarios, average pooling involves additions and division, mixed pooling combines average and max pooling, stochastic pooling involves random selection based on probability, and wavelet pooling calculates operations for each subband at each level. The proposed wavelet pooling method, while showing improved stability and learning progression, is noted for its computational inefficiency, highlighting the need for code optimization and mathematical operation improvements. The efficiency of different pooling methods is compared based on mathematical operations. Average pooling is the most computationally efficient, followed by mixed pooling and then max pooling. Stochastic pooling is the least efficient, using about 3x more operations than average pooling. Wavelet pooling is the least efficient, requiring 54 to 213x more operations than average pooling. Wavelet pooling is the least computationally efficient method, using 54 to 213x more mathematical operations than average pooling. However, with improvements in coding practices, GPUs, and an enhanced FTW algorithm, this method can still be a viable option. There are enhancements to the FTW algorithm that utilize multidimensional wavelets, lifting, parallelization, and other methods to improve efficiency in speed and memory. Through good coding practices, GPUs, and an improved FTW algorithm, wavelet pooling has the potential to outperform traditional methods in CNNs. Our proposed method excels in the MNIST dataset, performs well in the CIFAR-10 and KDEF datasets, and shows promising results in the SHVN dataset. The addition of dropout and batch normalization enhances network regularization, leading to improved performance across various datasets. Our proposed wavelet pooling method performs well in the CIFAR-10 and KDEF datasets, showing comparable results to other pooling methods in the SHVN dataset. The addition of dropout and batch normalization improves network regularization. Results confirm that no single pooling method is superior, with some performing better depending on the dataset and network structure. Future work could explore varying wavelet bases for optimal performance. Future work could involve varying wavelet bases to determine the best performing basis for pooling. Adjusting the upsampling and downsampling factors in decomposition and reconstruction can enhance image feature reduction beyond the 2x2 scale. Retaining discarded subbands for backpropagation may improve accuracy. Improving the computational efficiency of the FTW method is crucial. Analyzing the structural similarity (SSIM) of wavelet pooling compared to other methods could further validate its effectiveness."
}