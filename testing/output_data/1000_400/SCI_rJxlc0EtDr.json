{
    "title": "rJxlc0EtDr",
    "content": "Recent research has focused on developing neural network architectures with external memory, often using the bAbI question and answering dataset for challenging tasks requiring reasoning. A study employed an associative inference task from human neuroscience to assess existing memory-augmented architectures' reasoning capacity. Current architectures struggle with long-distance associations, as shown in tasks involving finding the shortest path between nodes. A new architecture, MEMO, was developed to reason over longer distances by introducing two novel components. MEMO is a novel architecture designed to reason over longer distances by introducing two new components: a separation between memories/facts stored in external memory and the items that make up these facts, and an adaptive retrieval mechanism allowing for variable 'memory hops'. It can solve novel reasoning tasks and all 20 tasks in bAbI, addressing the challenge of connecting facts acquired at different times for making judgments in everyday life. MEMO is capable of solving novel reasoning tasks and all 20 tasks in bAbI by connecting facts acquired at different times for making judgments in everyday life. Inferential reasoning, supported by the hippocampus, involves flexible recombination of single experiences to infer unobserved relationships. Inferential reasoning involves flexible recombination of single experiences to infer unobserved relationships, supported by the hippocampus. The hippocampus stores memories independently through pattern separation to minimize interference and allow for recall of specific events as 'episodic' memories. This separation poses a conflict with the hippocampus's role in generalization. Recent research has shown that the integration of separated experiences occurs during retrieval through a recurrent mechanism, allowing for multiple pattern separated codes to interact and support inference. This insight is used to explore how neuroscience models can enhance inferential reasoning in neural networks, such as the Differential Neural Computer (DNC). Neural networks with external memory like the DNC and EMN have shown remarkable abilities in computational tasks. Recent advancements in attention mechanisms and context utilization have also improved traditional neural networks. However, some tasks like bAbI present challenges with repetitive patterns that neural networks can exploit. To address this, a new task was introduced. The introduction of a new task, Paired Associative Inference (PAI), derived from neuroscientific literature, aims to capture inferential reasoning by forcing neural networks to learn abstractions to solve unseen associations. This task is procedurally generated and designed to challenge neural networks to appreciate distant relationships among elements distributed across multiple facts or memories. Our approach, called MEMO, retains the full set of facts into memory and learns a linear projection paired with a powerful recurrent attention mechanism. This is in contrast to other models that use fixed memory representations based on combining word embeddings with positional encoding transformations. MEMO is a new approach that retains all facts in memory and utilizes a linear projection with a recurrent attention mechanism for flexible weighting. It addresses the issue of prohibitive computation time in neural networks. The components in memory allow for flexible weighting of elements to support inferential reasoning. The problem of prohibitive computation time in neural networks is addressed by adapting the amount of compute time to the complexity of the task, drawing inspiration from a model of human associative memory called REMERGE. In neural networks, the amount of compute time is adapted to the task's complexity using inspiration from REMERGE, a model of human associative memory. The network outputs an action to determine if it should continue computing or if it can answer the task. This approach is based on adaptive computation time techniques. The neural network architecture implements a halting policy inspired by adaptive computation time techniques. The network outputs an action to decide whether to continue computing or answer the task, learning the termination criteria of a fixed point operator. The halting random variable is trained using reinforcement learning to adjust weights based on the optimal number of computation steps. This approach allows for a variable amount of computation, as investigated in previous studies. Our approach uses reinforcement learning to adjust weights based on the optimal number of computation steps, aiming to minimize the expected number of computation steps. This encourages the network to prefer representations and computation that require less computation. Our contributions include a new task emphasizing reasoning and an investigation of memory representation. Our contributions include a new task focusing on reasoning, an investigation of memory representation, and a REINFORCE loss component for learning the optimal number of iterations. Empirical results show effectiveness on tasks like paired associative inference and shortest path finding. The paper introduces End-to-End Memory Networks and focuses on tasks like paired associative inference and shortest path finding. It discusses the architecture and setup for predicting answers based on knowledge inputs and queries. The paper introduces End-to-End Memory Networks for tasks like paired associative inference and shortest path finding. It discusses predicting answers based on knowledge inputs and queries, with a focus on embedding matrices and positional encoding. The paper introduces End-to-End Memory Networks for tasks like paired associative inference and shortest path finding, focusing on embedding matrices and positional encoding. EMN calculates weights over memory elements and produces outputs based on queries. MEMO embeds input differently by deriving a common embedding for each input matrix. MEMO embeds input differently by deriving a common embedding for each input matrix, adapting them to be keys or values without using hand-coded positional embeddings. Multiple heads are used to attend to the memory, allowing flexibility in capturing different parts of the input sentence. MEMO uses multiple heads to attend to the memory, allowing flexibility in capturing different parts of the input sentence. Each head has a different view of the common inputs, enabling the model to learn how to weight each item during a memory lookup. This approach contrasts with hand-coded positional embeddings used in previous models. MEMO utilizes multiple heads for memory attention, enabling flexibility in capturing various aspects of input data. Each head learns to weight items during memory lookup, contrasting with hand-coded positional embeddings in previous models. The attention mechanism in MEMO is adapted for multi-head attention, incorporating DropOut and LayerNorm for improved generalization and learning dynamics. MEMO utilizes multiple heads for memory attention, incorporating DropOut and LayerNorm for improved generalization and learning dynamics. The attention mechanism in MEMO involves matrices for transforming logits and queries, producing the answer a t. It differs from previous models by preserving the query separated from the keys and values, reducing computational complexity. MEMO's attention mechanism differs from Vaswani et al. (2017) by preserving query separation from keys and values, reducing computational complexity. It uses GRUs and MLPs to learn the number of computational steps required for effective answers. To determine the number of computational steps needed to answer a query effectively, information is collected at each step and processed by GRUs and MLPs to create an observation. This observation is then used to define a binary policy and approximate the value function. The input to this network is formed by the Bhattacharyya distance between attention weights at current and previous time steps, along with the number of steps taken so far. The goal is to avoid querying the memory repeatedly if attention remains on the same memory slot for too long. The network is trained using REINFORCE with parameters adjusted using a n-step look ahead values. The objective function is to minimize L Hop, which follows from the fact that \u03c0 is a binary policy. The expectation of a binary random variable is its probability, and the expectation of their sum is the sum of the expectation. The objective function of the network is to minimize L Hop, a term introduced to minimize the expected number of hops in the binary policy. This term encourages the network to prefer representations that minimize required computation. The variance issue with REINFORCE training of discrete random variables is addressed in the case of a binary halting random variable. The network aims to minimize required computation by preferring representations that minimize the expected number of hops in the binary policy. The variance issue with REINFORCE training of discrete random variables is addressed for a binary halting random variable, where the variance is bounded by 1/4. The reward structure is defined by the target answer and prediction from the network, with a maximum number of hops set for the network. The final layer of M LP R was initialized with bias init to increase the chances of producing a probability of 1. A maximum number of hops, N, was set for the network, with no gradient sharing between the hop network and the main MEMO network. Memory-augmented networks have gained interest for abstract reasoning tasks, with the Differential Neural Computer being another influential model in this area. There has been increasing interest in memory-augmented networks for abstract reasoning tasks. The Differential Neural Computer (DNC) operates on inputs sequentially, learning to read and write to memory. An extension with sparsity improved performance on larger-scale tasks. Other memory-augmented architectures have been developed since, such as the Dynamic Memory Network. Several memory-augmented architectures have been developed since the publication of initial models like the DNC, including the Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet. These models enable relational reasoning over memory contents and have shown good performance on various tasks. The Working Memory Network, based on EMNs, incorporates a working memory buffer and RelationNet for relational reasoning over memory contents. New models like ACT and Adaptive Computation Time dynamically adjust computational steps based on task complexity. Adaptive Computation Time (ACT) and Adaptive Early Exit Networks are approaches to adjust computational steps based on task complexity. REINFORCE is used to learn discrete latent variables for conditional computation in neural networks. Conditional computation using REINFORCE has been applied to recurrent neural networks to dynamically adjust the number of computation steps. This technique has also been used to learn how many steps to \"jump\" in sequence, reducing the total number of processed inputs. Additionally, a similar idea has been applied to neural networks augmented with external memory, introducing the concept of using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory. Our method introduces the idea of using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory. Graph Neural Networks consist of an iterative message passing process that propagates node and edge embeddings throughout a graph, implementing similar computation to attention mechanisms. Our method introduces adaptive computation to modulate the number of message passing steps in graph neural networks, eliminating the need for message passing between memories. Our method introduces adaptive computation to modulate the number of message passing steps in graph neural networks, eliminating the need for message passing between memories. The paper introduces a task derived from neuroscience to probe the reasoning capacity of neural networks, capturing the essence of reasoning by appreciating distant relationships among elements distributed across multiple facts. One contribution of this paper is to introduce a task, derived from neuroscience, to probe the reasoning capacity of neural networks by appreciating distant relationships among elements distributed across multiple facts. This task is formalized in a prototypical task known as the paired associative inference (PAI) task, where two images are randomly associated together to test generalization. The paired associative inference (PAI) task involves presenting two images randomly associated together to test generalization. During test time, direct and indirect queries can be asked to assess episodic memory and inference abilities, respectively. The paired associative inference task involves direct and indirect queries to test episodic memory and inference abilities. Direct queries rely on retrieving experienced episodes, while indirect queries require inference across multiple episodes. The network is presented with a cue, image A, and two choices: the match, originally paired with B, or the lure, forming a different triplet A-B-C. The right answer requires linking A and C because they were both paired with B. This task is analogous to recognizing associations, like two people walking with the same little girl. The comparison of MEMO with other memory-augmented architectures is detailed in the appendix. The comparison of MEMO with other memory-augmented architectures, including End to End Memory Networks (EMN), DNC, and Universal Transformer (UT), was detailed in the appendix. Table 1 summarizes the results of our model (MEMO) and the baselines on the hardest inference query for each of the PAI tasks. MEMO achieved the highest accuracy on the smaller set A-B-C, along with DNC, while EMN struggled to reach the same level even with multiple hops. MEMO outperformed other memory-augmented architectures like EMN, DNC, and UT on the hardest inference queries for PAI tasks. It achieved the highest accuracy on the smaller set A-B-C and was the only architecture successful on longer sequences. Further analysis showed that MEMO required fewer hops than DNC to achieve the same accuracy on a length 3 PAI task. MEMO outperformed other memory-augmented architectures like EMN, DNC, and UT on the hardest inference queries for PAI tasks. It required fewer hops than DNC to achieve the same accuracy on a length 3 PAI task. In analyzing how MEMO approached the task, attention weights of an inference query were examined. The network stored associations A-B and B-C in slots 10 and 25, respectively. In the first hop, MEMO retrieved the memory in slot 10 containing the CUE, ID 611, and the associated item, ID 191, forming an A-B association. In the second hop, MEMO activated slot 16 with the memory association B-C, IDs 191 and 840, confirming the correct inference decision. Slot 13, associated with the LURE ID 943, also received some mass. This activation sequence aligns with computational models of the hippocampus. In the second hop, MEMO assigned probability masses to slots supporting inference decisions, aligning with computational models of the hippocampus. Different patterns of memory activation were observed in instances with varying numbers of hops, indicating the algorithm's dependence on network traversal. This may relate to knowledge distillation in neural networks. The algorithm used for inference in MEMO depends on the number of hops the network takes, similar to knowledge distillation in neural networks. Ablation experiments confirmed that specific memory representations and recurrent attention mechanism are crucial for successful inference. This conclusion only applied to inference queries, not direct queries. The specific memory representations and recurrent attention mechanism are essential for successful inference in MEMO. This conclusion applies to inference queries but not direct queries. Additionally, the adaptive computation mechanism in MEMO was found to be more data efficient compared to ACT for this task. The adaptive computation mechanism in MEMO was more data efficient for inference queries compared to ACT. Synthetic reasoning experiments on randomly generated graphs showed the accuracy of models in finding the shortest path between nodes. In synthetic reasoning experiments on randomly generated graphs, MEMO outperformed other models in predicting the shortest path between nodes, showing great scalability and accuracy. MEMO outperformed DNC in predicting nodes in complicated graphs with high connectivity, showing great scalability. Universal Transformer had different performance in predicting first versus second nodes in shortest path, slightly lower than MEMO in the latter case. Test results for best hyper-parameters for MEMO are reported, along with results from best run for EMN, UT, and DNC. The focus then shifts to the bAbI question. In comparing performance, MEMO outperformed DNC in predicting nodes in complex graphs. Universal Transformer showed slightly lower performance than MEMO in tasks requiring direct reasoning. Test results for best hyper-parameters for MEMO, EMN, UT, and DNC are reported. The focus then shifts to the bAbI question answering dataset, where MEMO achieved high accuracy in solving tasks. In the 10k training regime, MEMO achieved high accuracy in solving all tasks on the bAbI dataset, matching the number of tasks solved by other models but with lower error. Ablation experiments showed that memory representations and recurrent attention were critical for state-of-the-art performance. The use of layernorm in the recurrent attention mechanism contributed to a more stable training regime and better performance. In an in-depth investigation of memory representations for inferential reasoning, MEMO, an extension to existing memory architectures, achieved state-of-the-art results on the bAbI dataset. It utilized powerful recurrent attention and layernorm for stable training and better performance. Additionally, MEMO excelled in a new task called paired associative inference. MEMO, an extension to existing memory architectures, achieved state-of-the-art results on inferential reasoning tasks, including the paired associative inference and the bAbI dataset. It demonstrated the ability to solve long sequences and showed flexible weighting of individual elements in memory through a powerful recurrent attention mechanism. Our analysis supported the hypothesis that flexible weighting of individual elements in memory, achieved by combining separated storage of facts with a recurrent attention mechanism, led to solving the 20 tasks of the bAbI dataset. The task was made challenging by starting from the ImageNet dataset and creating three distinct datasets with sequences of varying lengths. Each dataset contained a large number of training, evaluation, and testing images. We created three distinct datasets with sequences of varying lengths (3, 4, and 5 items) using ImageNet. Each dataset contained a large number of training, evaluation, and testing images. The batches were built with memory, query, and target entries, with N sequences selected from the pool. In the batch selected from the pool, N sequences were used with N = 16. Memory content was created with pairwise associations between items in the sequence. Queries consisted of cue, match, and lure images, with direct and indirect types. Direct queries involve cue and match in the same memory slot, while indirect queries require inference. In 'direct' queries, the cue and match are in the same memory slot, while 'indirect' queries require inference across multiple episodes. The network receives concatenated image embedding vectors for the cue, match, and lure, with randomized positions for the match and lure to avoid degenerate solutions. The network receives concatenated image embedding vectors for the cue, match, and lure, with randomized positions to avoid degenerate solutions. The task requires appreciating the correct connection between images and avoiding interference from other items in memory. Each batch is balanced with direct and indirect queries. The task involves generating queries based on memory store, balancing direct and indirect elements in each batch. Longer sequences result in more indirect queries requiring different levels of inference. The task involves generating queries based on memory store, balancing direct and indirect elements in each batch. Longer sequences result in more indirect queries requiring different levels of inference. The inputs for different models vary: EMN and MEMO use memory and query as inputs, DNC embeds stories and query in the same way as MEMO, and UT embeds stories and query like MEMO before using its encoder for output. The task involves generating queries based on memory store, balancing direct and indirect elements in each batch. Different models use memory and query inputs, with UT embedding stories and query like MEMO before using its encoder for output. Graph generation and representation follow specific methods for training networks. The task involves generating queries based on memory store, balancing direct and indirect elements in each batch. Graphs are generated by sampling two-dimensional points from a unit square to train networks. The graph description consists of tuples representing connections between nodes, with queries represented as tuples indicating the path to find. During training, mini-batches of 64 graphs, queries, and target paths are sampled. During training, mini-batches of 64 graphs, queries, and target paths are sampled. Queries are represented as a matrix of size 64 \u00d7 2, targets are of size 64 \u00d7 (L \u2212 1), and graph descriptions are of size 64 \u00d7 M \u00d7 2. Networks were trained for 2e4 epochs with 100 batch updates each. For EMN and MEMO, the graph description is set to be the contents of their memory. In experiments, the upper bound M for the number of nodes in a graph is set to the maximum number of nodes multiplied by the out-degree. Networks are trained for 2e4 epochs with 100 batch updates each. For EMN and MEMO, the graph description is the memory contents, and the query is used as input. The model predicts answers for nodes sequentially, with different numbers of hops for each. EMN uses the ground truth answer of the first node as the query for the second, while MEMO uses the predicted answer for the first node. The model predicts answers for nodes sequentially, with different numbers of hops for each. For EMN, the ground truth answer of the first node is used as the query for the second, while MEMO uses the predicted answer for the first node. The weights for each answer are not shared. The Universal Transformer embeds the query and graph description, concatenates the embeddings, and uses the encoder output as the answer. For the Universal Transformer and DNC models, the query and graph description are embedded and used to generate answers. The weights for each answer are not shared. DNC presents the graph description tuples first, followed by the query tuple, and uses pondering steps to output the shortest path. Training is done using Adam with cross-entropy loss. The models DNC and UT have a 'global view' to answer the second node in the path by reasoning and working backwards from the end node. Training is done using Adam with cross-entropy loss, and evaluation involves sampling a batch of 600 graph descriptions, queries, and targets to calculate mean accuracy over all nodes of the target path. The models DNC and UT have a 'global view' to answer the second node in the path by reasoning and working backwards from the end node. MEMO, on the other hand, has a 'local view' where the answer to the second node depends on the answer about the first node. This makes it unable to perform better than chance if the answer to the first node is incorrect. MEMO has a 'local view' where the answer to the second node depends on the answer about the first node. Comparing MEMO and EMN, an experiment was conducted using the ground truth answer of the first node and the predicted answer for the first node as queries for the second node. When MEMO was given the ground truth for node 1 as a query for node 2, its performance increased significantly compared to using the prediction of the first node. In contrast, EMN performed almost similarly to MEMO when using the same training regime where the prediction is used to query the second node. In experiments with 20 nodes and 5 outbound edges, giving MEMO the ground truth for node 1 as a query for node 2 significantly improved performance compared to using the prediction of the first node. However, when EMN was trained in a similar manner to MEMO, its performance dropped to almost chance level. These results were also observed in a simpler scenario with 20 nodes and 3 outbound edges using the English Question Answer dataset by Weston et al. (2015). The dataset was pre-processed by converting all text to lowercase, ignoring periods and interrogation marks, treating blank spaces as word separation tokens, and considering commas only in answers. The training and test datasets provided are pre-processed by converting all text to lowercase, ignoring periods and interrogation marks, treating blank spaces as word separation tokens, and considering commas only in answers. Each input corresponds to a single answer throughout the dataset, with queries separated from stories. During training, a mini-batch of 128 queries and corresponding stories are sampled from the test dataset. This results in queries being a matrix of 128 \u00d7 11 tokens, and sentences of size 128 \u00d7 320 \u00d7 11. During training, a mini-batch of 128 queries and corresponding stories are sampled from the test dataset. Queries are a matrix of 128 \u00d7 11 tokens, and sentences are of size 128 \u00d7 320 \u00d7 11. Stories and queries are used as inputs in different architectures like EMN, MEMO, DNC, and UT. Stories and queries are presented in sequence to the model for prediction. During training, stories and queries are embedded in the same way for DNC and UT models. The encoder of UT is used with a specific architecture, and Adam optimization is performed for all models. Tasks in bAbI require temporal context, which is addressed in MEMO by adding a time encoding column vector. Networks are trained for 2e4 epochs with 100 batch updates each. During training, stories and queries are embedded in the same way for DNC and UT models. The encoder of UT is used with a specific architecture, and Adam optimization is performed for all models. In MEMO, a time encoding column vector is added to address the temporal context required for tasks in bAbI. Networks are trained for 2e4 epochs with 100 batch updates each. Evaluation involves sampling a batch of 10,000 elements from the dataset and computing the forward pass similar to training. Mean accuracy is computed over examples, along with accuracy per task for the 20 tasks of bAbI. MEMO is trained using cross entropy loss with polynomial learning rate decay. Batch size is set to 64 for PAI and shortest path, and 128 for bAbI tasks. MEMO was trained using cross entropy loss with polynomial learning rate decay. Batch size was set to 64 for PAI and shortest path tasks, and 128 for bAbI. The halting policy network parameters were updated using RMSProp. MEMO has a temporal complexity of O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d). MEMO has a temporal complexity of O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where various parameters are fixed constants. It performs the hopping procedure for every answer, interacting with all memory slots for each hop. In terms of spatial complexity, MEMO maintains constant information of all weights. MEMO has a temporal complexity of O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d) and a spatial complexity of O(I \u00b7 S \u00b7 d). The halting unit h is defined based on the binary policy of MEMO, which differs slightly from the original ACT implementation. In our implementation of MEMO, the halting unit h is defined based on the binary policy \u03c0 t, which is different from the original ACT. This change aims to increase fairness in comparison and enable more powerful representations. The halting probability is then defined to evaluate the feasibility of this mechanism. The halting unit in MEMO is defined based on a binary policy to enable more powerful representations. The halting probability is then evaluated to assess the feasibility of this mechanism. The architecture used is similar to previous works by Graves et al. (2016) and Dehghani et al. (2018), with hyperparameters optimized through a search process. The architecture in MEMO is based on previous works by Graves et al. (2016) and Dehghani et al. (2018), with hyperparameters optimized through a search process. The specific implementation used is 'universal_transformer_small' available at https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py."
}