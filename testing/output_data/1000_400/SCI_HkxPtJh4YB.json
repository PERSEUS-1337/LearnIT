{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for exponential family defined over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, addressing the problem of marginal inference for large permutations. The effectiveness of this method is demonstrated in probabilistic neuron identification in C.elegans. C.elegans Let P be a binary matrix representing a permutation of n elements. Sinkhorn variational marginal inference is introduced to compute the matrix of expectations efficiently. The method approximates \u03c1 as S(L), the Sinkhorn operator applied to L, resulting in a doubly stochastic matrix. Sinkhorn variational marginal inference efficiently computes expectations by approximating \u03c1 as S(L), the Sinkhorn operator applied to L, resulting in a doubly stochastic matrix. The method is sensible and produces the best results for probabilistic inference of neural identity in C.elegans. The relation between marginal inference and the normalizing constant is key in this approach. The Sinkhorn approximation is key in efficiently computing expectations for probabilistic inference. The relation between marginal inference and the normalizing constant is crucial, linking marginal inference of \u03c1 L and computation of the permanent Z L. This is achieved through an optimization problem in a variational inference scheme. The matrix of marginals, \u00b5(L) = \u03c1 L and the dual function A * (\u00b5(L)) coincides with the negative entropy. Marginal inference of \u03c1 L and computation of the permanent Z L are linked by an optimization problem. The quality of the approximated \u03c1 depends on the tightness of the approximation to Z L. The approximation is based on replacing the intractable dual function with a component-wise entropy, resulting in the Sinkhorn permanent, perm S (L). The Sinkhorn permanent approximation, perm S (L), is derived from the intractable dual function A * (\u00b5) using component-wise entropy. Bounds for this approximation are provided, and it has been proposed independently by Powell and Smith (2019). The Bethe variational inference method offers a general rationale for obtaining variational approximations in graphical models. The Bethe variational inference method provides a general rationale for variational approximations in graphical models, approximating the dual function A*(\u00b5) as if the underlying Markov random field had a tree structure. This approximation has been successfully applied to permutations, computing the approximate marginal B(L) through belief propagation and offering better theoretical guarantees than the Sinkhorn approximation. Computational differences exist in the Bethe approximation of the permanent. The Bethe approximation computes the approximate marginal B(L) through belief propagation, with better theoretical guarantees than the Sinkhorn approximation. Computational differences exist between the two methods, with the Bethe approximation showing better performance in practice. In practice, the Bethe approximation produces better permanent approximations compared to the Sinkhorn approximation. However, the Sinkhorn approximation often yields qualitatively better marginals by putting more mass on non-zero entries. Additionally, for moderate n values, the Sinkhorn approximation scales better in terms of computational efficiency. The Sinkhorn approximation is observed to scale better for moderate n values compared to the Bethe approximation. This is demonstrated by the faster iteration times for Sinkhorn, with each iteration taking only 0.0027 seconds for n = 710. The comparison between Bethe and Sinkhorn approximations is illustrated using submatrices from the C.elegans dataset, showing differences in log permanents and mean absolute errors of log marginals. Sampling-based methods are also considered for additional analysis. The study compares the Sinkhorn and Bethe approximations using submatrices from the C.elegans dataset, showing differences in log permanents and mean absolute errors of log marginals. Sampling-based methods are also considered for further analysis, with a focus on marginal inference. Recent advances in neurotechnology have enabled whole brain imaging of the worm C.elegans, a species with a stereotypical nervous system. The challenge now is to identify and assign canonical labels to the volumetric images of worm neurons. A methodology for probabilistic neural identification was applied in the context of NeuroPAL, a multicolor C.elegans transgene designed to facilitate neural identification. Given volumetric images of worm neurons, canonical labels are assigned using a probabilistic neural identification methodology within the context of NeuroPAL, a multicolor C.elegans transgene. The goal is to estimate the matrix of marginal probabilities for identifying observed neurons with canonical identities, providing uncertainty estimates for model predictions. A gaussian model is used for each canonical neuron, with parameters inferred from previously annotated worms. The probabilities of identifying observed neurons with canonical identities are estimated using a gaussian model. The likelihood of observing data is determined by a permutation, inducing a posterior over P. The NeuroPAL system involves a downstream task with a deterministic coloring scheme for neuron identification. In the context of NeuroPAL, a flat prior over P induces a posterior with a deterministic coloring scheme for neuron identification. A downstream task involves manually labeling uncertain neurons to improve identification accuracy. Annotations lead to model updates and faster approximation quality. The uncertainty in neuron identification is resolved as humans annotate neurons, leading to increased accuracy. Different approximation methods are compared, including Sinkhorn, Bethe, MCMC, random and naive baselines, and a 'ground truth' protocol. Results are shown in Fig 3 with details in the Appendix. The uncertainty in neuron identification is resolved by comparing different approximation methods: Sinkhorn, Bethe, MCMC, random and naive baselines, and a 'ground truth' protocol. Results show that Sinkhorn and Bethe approximations outperform other baselines, with Sinkhorn slightly better. MCMC does not provide better results than the naive baseline, indicating lack of convergence. The Sinkhorn and Bethe approximations outperform other baselines in resolving neuron identification uncertainty. MCMC does not show better results than the naive baseline, suggesting lack of convergence. The Sinkhorn approximation may offer faster and more accurate marginal inferences compared to the Bethe approximation. Further analysis is needed to understand the relationship between permanent approximation quality and corresponding marginals. The Sinkhorn and Bethe approximations are effective in resolving neuron identification uncertainty. Further analysis is required to understand the relationship between permanent approximation quality and corresponding marginals. Additionally, the (log) Sinkhorn approximation of the permanent of L can be obtained by evaluating S(L) in the problem it solves. The dataset used consists of ten NeuroPAL worm heads with available human labels and log-likelihood matrices computed with specific methods. The dataset used for analysis includes ten NeuroPAL worm heads with human labels and log-likelihood matrices. The Sinkhorn and Bethe approximations were computed with 200 iterations, ensuring convergence. The MCMC sampler method by Diaconis (2009) was used with 100 chains of length 1000 on an Intel Xeon W-2125 processor. The MCMC sampler method by Diaconis (2009) was used with 100 chains of length 1000, and for each of them considered samples of multiples of 10 starting from iteration 500 on. Results were obtained on a desktop computer with an Intel Xeon W-2125 processor. An efficient log-space implementation of the message passing algorithm described in (Vontobel, 2013, Lemma 29) was used, with the parameter eps introduced for numerical stability. Submatrices of size n were randomly drawn from ten available log likelihood C.elegans matrices. Error bars were omitted due to their small size."
}