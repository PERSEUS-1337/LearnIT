{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is proposed for two layer ReLU networks, offering a tighter generalization bound. The capacity bound correlates with test error behavior with increasing network sizes and partly explains the improvement in generalization with over-parametrization. Additionally, a matching lower bound for the Rademacher complexity is presented, improving over previous capacity lower bounds for neural networks. The curr_chunk discusses the success of deep neural networks in learning various tasks, emphasizing the over-parametrization of networks to improve test performance. Despite being able to fit random labels, these networks achieve smaller generalization error when trained with real labels. This challenges traditional wisdom on model capacity and overfitting. Increasing the model size in neural networks has been found to improve generalization error, even without explicit regularization techniques like weight decay or early stopping. This challenges traditional beliefs about overfitting and model capacity. Complexity measures that depend on the total number of parameters of the network, such as VC bounds, do not capture the improvement in generalization with over-parametrization. Existing works suggest different norm, margin, and measures to explain this phenomenon. The improvement in generalization with over-parametrization is not captured by complexity measures that depend on the total number of parameters of the network. Different norm, margin, and sharpness based measures have been suggested to explain the capacity of neural networks and the observed generalization behavior. Even when the network is large enough to fit the training data completely, the test error continues to decrease for larger networks. Even with a large enough network to fit the training data, the test error decreases for larger networks. Unit capacity and unit impact are important factors in network capacity. Empirically, both average unit capacity and unit impact decrease faster than 1/ \u221a h where h is the number of hidden units. Bartlett et al. (2017) showed a generalization bound based on spectral norm and 1,2 norm. The average unit capacity and unit impact decrease faster than 1/ \u221a h as the number of hidden units increases. Existing complexity measures fail to explain why over-parametrization helps and increase with network size. Dziugaite & Roy (2017) evaluated a generalization bound based on PAC-Bayes, showing that numerical generalization bounds increase with network size. To further study this phenomenon, two layer ReLU networks were chosen to simplify the architecture while preserving the property of interest. The paper proves a tighter generalization bound for two layer ReLU networks. The paper presents a tighter generalization bound for two layer ReLU networks, showing that the capacity bound correlates with test error and decreases with increasing hidden units. The key insight is characterizing complexity at a unit level, with unit level measures shrinking faster than 1/ \u221a h for each hidden unit as network size increases. The generalization bound depends on the Frobenius norm of the top layer and the difference of hidden layer weights with the initialization. The generalization bound for two layer ReLU networks decreases with increasing hidden units, with unit level measures shrinking faster than 1/ \u221a h. The bound depends on the Frobenius norm of the top layer and the difference of hidden layer weights with the initialization, which decreases as network size increases. In the over-parametrized setting, training just the top layer minimizes training error as the randomly initialized hidden layer has all possible features. In the over-parametrized setting, training just the top layer minimizes training error as the randomly initialized hidden layer has all possible features. The large number of hidden units represent all possible features, making the optimization problem involve picking the right features to minimize training loss. Studies have shown that as networks are over-parametrized, optimization algorithms need to do less work in tuning the weights of hidden units to find the right solution. Contributions: The role of over-parametrization in generalization of neural networks is empirically investigated in this paper. Dziugaite & Roy (2017) evaluated a PAC-Bayes measure from the initialization used by algorithms, showing that the Euclidean distance to the initialization is smaller than the Frobenius norm of the parameters. Nagarajan & Kolter (2017) observed the significant role of initialization and proved an initialization dependent generalization bound for linear networks. Liang et al. (2017) suggested a Fisher-Rao metric based complexity measure that correlates with generalization behavior in larger networks. In this paper, the authors empirically investigate the role of over-parametrization in generalization of neural networks on three different datasets. They show that existing complexity measures increase with the number of hidden units but do not explain generalization behavior with over-parametrization. The authors also propose tighter generalization bounds for two-layer ReLU networks and introduce a complexity measure that decreases with the increasing number of hidden units, potentially explaining the effect of over-parametrization on generalization. The authors propose tighter generalization bounds for two-layer ReLU networks and introduce a complexity measure that decreases with the increasing number of hidden units. They provide a lower bound for the Rademacher complexity of these networks, which considerably improves over previous bounds. The authors introduce a lower bound for the Rademacher complexity of two-layer ReLU networks, which surpasses previous bounds. They define the margin operator and ramp loss for c-class classification tasks. The margin operator \u00b5 is defined as the difference between the score of the correct label and the maximum score among other labels. The ramp loss is defined for any distribution D and margin \u03b3 > 0. The expected margin loss of a predictor f (.) is bounded between 0 and 1. The empirical estimate of the expected margin loss is denoted as L \u03b3 (f ).\u03b3 = 0 reduces the loss to classification loss, denoted as L 0 (f ). The function class F is denoted as \u03b3 \u2022 F. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels. It increases with the complexity of the class. By bounding the Rademacher complexity of neural networks, we can obtain a bound on the generalization error. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels. By bounding the Rademacher complexity of neural networks, we can obtain a bound on the generalization error. The choice of function class is crucial to explain the decrease in generalization error with increasing width. Experiments on network layers with increasing hidden units are conducted on the CIFAR-10 dataset. The experiments on network layers with increasing hidden units on the CIFAR-10 dataset show that while spectral and Frobenius norms initially decrease, they eventually increase with h, with the Frobenius norm increasing at a faster rate. The increase in the Frobenius norm of weights in larger networks is attributed to the increase in the Frobenius norm of random initialization. The increase in Frobenius norm of weights in larger networks is due to the increase in random initialization. The distance to initialization per unit decreases with increasing hidden units, leading to a shift in the distribution of angles between learned and initial weights. This per unit distance is referred to as unit capacity, defined as \u03b2 i = u i \u2212 u 0 i 2. The distribution of angles shifts from orthogonal to aligned in large networks, with a decrease in Frobenius norm and distance to initialization in the second layer. The norm of outgoing weights decreases faster than 1/ \u221a h as network size grows. In large neural networks, the Frobenius norm and distance to initialization decrease in the second layer. The norm of outgoing weights decreases faster than 1/ \u221a h as network size grows, indicating a limited role of initialization for this layer. The impact of each classifier on the final decision shrinks with a rate faster than 1/ \u221a h, defined as unit impact. Unit impact is measured by \u03b1 i = v i 2, the magnitude of outgoing weights from unit i. The unit impact, defined as \u03b1 i = v i 2, plays a crucial role in two-layer neural networks. Empirical observations suggest that networks from real data have bounded unit capacity and impact. Studying the generalization behavior of these networks can provide a better understanding. The hypothesis class of neural networks represented by parameters in set W is considered for generalization properties analysis. The generalization behavior of neural networks with bounded unit capacity and impact is studied by analyzing the Rademacher complexity of two-layer ReLU networks. A new technique is used to decompose the complexity and provide a generalization bound for the class of neural networks. The Rademacher complexity of two-layer neural networks is analyzed to provide a generalization bound for the class of neural networks. A new technique decomposes the complexity across hidden units, leading to a tighter bound compared to previous works that focused on layer complexity and Lipschitz property. The generalization bound in Theorem 1 applies to any function in the defined function class. The Rademacher complexity of two-layer neural networks is analyzed to provide a generalization bound. By decomposing complexity across hidden units, a tighter bound is obtained compared to focusing on layer Lipschitzness. The generalization bound in Theorem 1 applies to any function in the defined class, while Theorem 2 provides a generalization bound for any two-layer ReLU network. The generalization bound for any two-layer ReLU network is provided in Theorem 2, showing an improvement over existing bounds. The bound decreases with increasing width for networks learned in practice. An explicit lower bound for Rademacher complexity is also shown, matching the first term in the generalization bound. The additive factor in the bound is small in the regimes of interest, resulting in an overall decrease in capacity with over-parametrization. Further extensions to the generalization bound are discussed in Appendix Section B. The generalization bound for two-layer ReLU networks in Theorem 2 shows an improvement over existing bounds, decreasing with increasing width. The additive factor in the bound is small in relevant regimes, leading to decreased capacity with over-parametrization. Further extensions to the bound are discussed in Appendix Section B, including a finer tradeoff between terms for p norms. The generalization bound for two-layer ReLU networks in Theorem 2 improves existing bounds, decreasing with increasing width. The key complexity term in the bound is U \u2212 U 0 F V F, with differences in V 2 and V F mainly due to the number of classes. Experimental comparison shows training and test errors for networks of varying sizes on CIFAR-10 and SVHN datasets. The study compares the performance of two-layer ReLU networks on CIFAR-10 and SVHN datasets with varying sizes. Networks larger than size 128 show better generalization even without regularization. Unit capacity and unit impact decrease with increasing network size. Larger networks require fewer epochs to achieve a low cross-entropy loss. Experimental results also compare different capacity bounds across increasing network sizes. The study compares the performance of two-layer ReLU networks on CIFAR-10 and SVHN datasets with varying sizes. Larger networks show better generalization without regularization. The number of epochs needed for a low cross-entropy loss decreases with network size. Experimental results compare different capacity bounds across increasing network sizes, with the effective capacity decreasing with network size. The study compares the performance of two-layer ReLU networks on CIFAR-10 and SVHN datasets with varying sizes. Larger networks show better generalization without regularization. Our capacity bound is the only one that decreases with the size even for networks with about 100 million parameters. Our capacity bound decreases with network size, even for networks with about 100 million parameters. It could potentially point to the properties allowing over-parametrized networks to generalize. The complexity measure correlates well with generalization behavior when comparing networks trained on real and random labels. In this section, a lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound. The lower bound is shown on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer. This allows for comparison with existing results and extends the lower bound to the bigger class F W. Theorem 3 defines the parameter set DISPLAYFORM0 and the function class F W. The complete proof is provided in the supplementary Section C.3. The complexity lower bound matches the upper bound of Theorem 1, extending to a bigger class F W with an extra constraint on the hidden layer. The proof is detailed in the supplementary Section C.3, showing tightness of the upper bound even with additional information. The lower bound for spectral norm bounded classes of neural networks with a scalar output and element-wise activation functions shows a gap between the Lipschitz constant of the network and the network's capacity, even when additional information is considered. The lower bound for spectral norm bounded classes of neural networks with a scalar output and element-wise activation functions reveals a capacity gap compared to the Lipschitz constant of the network, especially excluding neural networks with rank-1 matrices as weights. This gap is significant for networks with ReLU activations versus linear networks. The construction can be extended to more layers by setting weight matrices in intermediate layers to the Identity matrix. The lower bound for spectral norm bounded classes of neural networks with a scalar output and element-wise activation functions shows a capacity gap compared to the Lipschitz constant, especially for networks with ReLU activations. Extending the construction to more layers by setting weight matrices in intermediate layers to the Identity matrix results in a stronger lower bound for the function class. This improvement surpasses previous lower bounds by a factor of \u221ah. In this paper, a new capacity bound for neural networks is presented, which decreases with the increasing number of hidden units. The focus is on the role of width in the generalization behavior of two-layer networks, with potential explanations for better performance of larger networks. Future studies will explore the interplay between depth and width in controlling network capacity. The study focuses on the role of width in the generalization behavior of two-layer networks and the capacity of neural networks. Future research will explore the interplay between depth and width in controlling network capacity. The paper also presents a new capacity bound for neural networks that decreases with the increasing number of hidden units. In this paper, the focus is on the generalization behavior of two-layer networks and neural network capacity. The study trained a pre-activation ResNet18 architecture on CIFAR-10 dataset with specific architecture details provided. The paper does not address optimization algorithms' convergence to low complexity networks or the effects of different hyperparameter choices on solution complexity. Future work will explore implicit regularization effects of optimization algorithms for neural networks. In the experiment, a pre-activation ResNet18 architecture was trained on the CIFAR-10 dataset with specific architecture details. The architecture includes a convolution layer, 8 residual blocks, and a linear layer. Different configurations were tested by varying the number of channels in the first convolution layer. Training was done using SGD with specific parameters and stopping criteria. The performance of each architecture was compared using a reference line in the plots. In experiments, fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets with various architectures. Training was done using SGD with specific parameters and data augmentation techniques. Different architectures were tested with increasing hidden units, and performance was compared based on achieving a specific loss threshold. For experiments on CIFAR-10, SVHN, and MNIST datasets, fully connected feedforward networks were trained with 13 architectures ranging from 2^3 to 2^15 hidden units. Training used SGD with specific parameters and stopped when cross-entropy reached 0.01 or after 1000 epochs. Evaluation included calculating exact generalization bounds and adjusting bounds for binary classification. The exact generalization bounds were calculated for different architectures trained on SVHN and MNIST datasets. The over-parametrization phenomenon was observed in the MNIST dataset. The behavior of measures on networks trained on SVHN and MNIST datasets is shown in Figures 6 and 7. The over-parametrization phenomenon in the MNIST dataset is depicted in the left panel of FIG10, while our generalization bound is compared to others in the middle and right panels. Theorem 2 is generalized to p norm, with Lemma 11 introducing a cover for the p ball with entry-wise dominance. Theorem 5 provides a bound for generalization error with probability 1 - \u03b4 over the choice of the training set. The generalization error bound for the p norm is provided in Theorem 5, with Lemma 11 introducing a cover for the p ball with entry-wise dominance. The bound decreases with h for larger values, particularly for p = ln h. Corollary 6 further bounds the generalization error for any function f(x) = V[Ux] +. The generalization error bound for the p norm is provided in Theorem 5, with Lemma 11 introducing a cover for the p ball with entry-wise dominance. Corollary 6 further bounds the generalization error for any function f(x) = V[Ux] +. Lemma 7 relates the norm of a vector to the expected magnitude of its inner product with Rademacher random variables, and Lemma 9 decomposes the Rademacher complexity of networks to that of hidden units. The Rademacher complexity of the class of networks can be decomposed to that of hidden units, as shown in Lemma 9. The complexity is bounded for a training set S and \u03b3 > 0, with a proof provided through induction. The ramp loss plays a key role in the inequality proof. Lemma 10 discusses the Ledoux-Talagrand contraction, involving convex and increasing functions, Lipschitz functions, and Rademacher random variables. It will be utilized in the context of the Rademacher complexity of neural networks. The right-hand side of the inequality is bounded, completing the induction proof. Lemma 10, the Ledoux-Talagrand contraction, involves convex, increasing functions, Lipschitz functions, and Rademacher random variables. It will be used in the proof of Theorem 1. The proof involves applying Lemma 10 with specific parameters and summing the resulting inequality. Additionally, a covering lemma is introduced to prove the generalization bound in Theorem 5 without requiring knowledge of network parameter norms. The covering lemma introduced in Lemma 10 allows for proving the generalization bound in Theorem 5 without the need for knowledge of network parameter norms. Lemma 11 presents a method to cover a p ball with dominating elements entry-wise, with a bound on the cover size. Lemma 13 provides a bound on generalization error for given parameters and training set, with probability 1 - \u03b4. Lemma 13 provides a bound on generalization error for given parameters and training set, with probability 1 - \u03b4. The generalization error is bounded under certain conditions. Lemma 14 provides specific results for the case p = 2, giving a bound on generalization error for a function f(x) = V[Ux] +. The generalization error is bounded under certain conditions with probability 1 - \u03b4. The generalization error for a function f(x) = V[Ux] + is bounded under certain conditions with probability 1 - \u03b4 for any p \u2265 2, as stated in Lemma 15. Lemma 15 provides a generalization bound for any p \u2265 2, with extra constants and logarithmic factors compared to p = 2. The generalization error is bounded for a function f(x) = V[Ux] + under certain conditions with probability 1 - \u03b4. The proof of Theorem 5 follows from Lemma 15, using notation to hide constants and logarithmic factors. The proof of Theorem 3 starts with the case where h = d = 2k, m = n^2k for some k, n \u2208 N. The dataset is divided into 2k groups, each containing n copies of a different element in a standard orthonormal basis. The dataset is divided into 2k groups, each with n copies of different elements in a standard orthonormal basis. Define F(\u03be) as [f1, f2, ..., f2k] where f i = 0 if i(\u03be) < 0. Choose U(\u03be) as Diag(\u03b2) \u00d7 F(\u03be), with U(\u03be) 2 \u2264 max i \u03b2 i."
}