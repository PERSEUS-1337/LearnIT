{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model designed for forecasting data streams from geospatial point-cloud sources. It utilizes a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. The D-Conv operator can be integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM is applied to mobile service traffic and air quality forecasting using point-cloud streams. Results show accurate long-term predictions, outperforming other neural network models. Point-cloud stream forecasting predicts future values from geospatial point-cloud data sources like mobile network antennas and air quality sensors. Unlike traditional spatiotemporal forecasting, it focuses on individual trajectories rather than grid-structural data. Point-cloud stream forecasting operates on irregular sets of points from data sources like mobile network antennas and air quality sensors. Vanilla LSTMs struggle with spatial features, while ConvLSTM and PredRNN++ are limited to grid-structural data, making them unsuitable for scattered point-clouds. The proposed PointCNN model aims to forecast point-cloud data streams directly without pre-processing, unlike ConvLSTM and PredRNN++ which are limited to grid-structured data. The PointCNN model leverages spatial-local correlations of point clouds for forecasting, unlike architectures designed for static data. The proposed CloudLSTM architecture introduces the DConv operator and combines Seq2seq learning and attention for improved forecasting over point cloud-streams. The CloudLSTM architecture introduces the DConv operator for precise forecasting over point-cloud streams, defining a point-cloud as a set of N points with value features and coordinates. Different channels of the point cloud can be obtained at each time step through various measurements. The CloudLSTM architecture introduces the DConv operator for forecasting over point-cloud streams, which consist of sets of features including value features and coordinates. An ideal point-cloud stream forecasting model should have properties like order invariance and information intactness. The ideal point-cloud stream forecasting model should exhibit order invariance, information intactness, interaction among points, robustness to transformations, and adaptability to changing spatial correlations over time. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM model, aiming to be robust to correlation-preserving transformations and adaptable to changing spatial correlations over time. DConv generalizes ordinary convolution on grids by computing weighted summations on point-clouds, inheriting properties of ordinary convolution while handling dynamic correlations among points. The DConv operator generalizes convolution on grids by computing weighted summations on point-clouds, ensuring information intactness. It takes U in channels of a point-cloud S as input and outputs U out channels of a point-cloud with the same number of elements. Points in S are defined as Q K n, including the K nearest points to p n in the Euclidean space. The DConv operator extends convolution to point-clouds by computing weighted summations on point sets. It utilizes subsets of points QKn in the input set Si to calculate values and coordinates for points in the output set Sj. The DConv operator extends convolution to point-clouds by computing weighted summations on point sets QKn in the input set Si to calculate values and coordinates for points in the output set Sj. It utilizes learnable weights shared across different anchor points in the input map, with scalar weights for input and output channels, nearest neighbors, and bias terms. The DConv operator extends convolution to point-clouds by computing weighted summations on point sets to calculate values and coordinates for points in the output set. It utilizes learnable weights shared across anchor points in the input map, with scalar weights for input and output channels, nearest neighbors, and bias terms. The coordinates of raw point-clouds are normalized to improve transformation robustness. The DConv operator extends convolution to point-clouds by computing weighted summations on point sets to calculate values and coordinates for points in the output set. The coordinates of raw point-clouds are normalized to (0, 1) to improve transformation robustness. The K nearest points can vary for each channel at each location, reflecting different types of measurements in the dataset. Spatial correlations vary between measurements due to human mobility, impacting data consumption of apps and air quality indicators. The DConv operator in CloudLSTM allows each channel to find the best neighbor set for spatial correlations in data consumption and air quality indicators affected by human mobility. The operator weights its K nearest neighbors across features to produce values and coordinates in the next layer. CloudLSTM introduces the DConv operator, which weights its K nearest neighbors across features to produce values and coordinates in the next layer. DConv is a symmetric function that captures local dependencies and improves robustness to global transformations. DConv is a symmetric function that captures local dependencies, improves robustness to global transformations, and learns the layout and topology of the cloud-point for the next layer. It enables dynamic positioning tailored to each channel and time step, essential for spatiotemporal forecasting neural models. The DConv operator enables dynamic positioning tailored to each channel and time step in spatiotemporal forecasting neural models. It can be efficiently implemented using simple 2D convolution and builds upon PointCNN and Deformable Convolution on grids. The DConv operator, based on PointCNN and Deformable Convolution, introduces variations for pointcloud structural data. It ensures order invariance without extra complexity or information loss by aligning weights based on distance rankings. The DConv operator maintains order invariance by aligning weights based on distance rankings, ensuring no information loss. It can be seen as a variation of DefCNN for point-clouds, offering transformation modeling flexibility and adaptive receptive fields for convolution. Additionally, DConv can be easily integrated into LSTMs to learn spatial and temporal correlations. DefCNN and DConv offer transformation modeling flexibility and adaptive receptive fields for convolution. DConv can be integrated into LSTMs to learn spatial and temporal correlations over point-clouds. CloudLSTM is formulated with input, forget, and output gates, memory cell, hidden states, and learnable weight and bias tensors. The DConv operator is utilized in CloudLSTM, with a simplified version that operates over the gates. The CloudLSTM cell combines point cloud representations with learnable weight and bias tensors. It integrates a simplified DConv operator for gate computations and is used in Seq2seq learning with soft attention mechanism for forecasting. The Seq2seq CloudLSTM architecture combines CloudLSTM with Seq2seq learning and the soft attention mechanism for forecasting. It includes an encoder and decoder made up of CloudLSTMs, connected via a context vector. Data is processed by CloudCNN layers before generating predictions. The encoder and decoder states are linked using soft attention via a context vector. Point Cloud Convolutional layers process the data before forecasting. A two-stack encoder-decoder architecture is used with 36 channels for each CloudLSTM cell. Different models like CloudRNN and CloudGRU are explored by incorporating DConv into vanilla RNN and Convolutional GRU. The study explores new models like CloudRNN and CloudGRU by incorporating DConv into vanilla RNN and Convolutional GRU. These models, along with CloudLSTM, are compared for forecasting mobile service demands and air quality indicators using measurement datasets from mobile services and air quality monitoring stations. The study utilizes measurement datasets from mobile services and air quality monitoring stations to forecast future demands and indicators in target regions. CloudLSTM is compared with 12 baseline deep learning models using TensorFlow and TensorLayer libraries. The models are trained on a computing cluster with NVIDIA Tesla GPUs and optimized using the Adam optimizer to minimize mean square error. The experimental results are reported after discussing the datasets and baseline models. The study trains various architectures on a computing cluster with NVIDIA Tesla GPUs, optimizing models with the Adam optimizer to minimize mean square error. Experiments focus on spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments, omitting coordinate features in the final output. Different scenarios are explored, including mobile traffic forecasting with large-scale multi-service datasets. In mobile traffic forecasting experiments, large-scale multi-service datasets from two European metropolitan areas were used. The data included traffic volume from 792 and 260 antennas in the cities, aggregated over 5-minute intervals. The antennas were non-uniformly distributed, forming 2D point clouds over space, with 24,482 traffic snapshots collected for 38 different mobile services. Coordinate features were omitted in the final output for these forecasting tasks. The dataset used for mobile traffic forecasting experiments in two European cities includes traffic volume data from non-uniformly distributed antennas aggregated over 5-minute intervals for 38 mobile services. The air quality forecasting performance is evaluated using a public dataset from China, comprising six air quality indicators collected by 437 monitoring stations over a year. The study investigates forecasting performance using a public dataset of six air quality indicators collected by 437 monitoring stations in China. The dataset includes data from two city clusters, with measurements taken hourly over a year. Missing data is filled using linear interpolation, and measurements are transformed into different input channels. Coordinate features are normalized, and baseline models requiring grid-structural input are utilized. The study investigates forecasting performance using a public dataset of six air quality indicators collected by 437 monitoring stations in China. The measurements associated with each mobile service and air quality indicator are transformed into different input channels of the point-cloud S. Coordinate features are normalized to the (0, 1) range. The point-clouds are transformed into grids using the Hungarian algorithm. The ratio of training plus validation, and test sets is 8:2. CloudLSTM is compared with baseline models such as PointCNN, CloudCNN, and PointLSTM for feature extraction from point-clouds. The study compares CloudLSTM with baseline models like PointCNN, CloudCNN, and PointLSTM for feature extraction from point-clouds. Other baseline models such as MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++ are also discussed in detail in the appendix. The study discusses CloudLSTM and its variations, CloudRNN and CloudGRU, along with baseline models like MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++. The accuracy of CloudLSTM is evaluated using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Mobile traffic snapshots are likened to \"urban images,\" and metrics like Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are used to assess forecast fidelity. All neural networks are employed for city-scale mobile traffic prediction. For mobile traffic prediction, various neural networks are used to forecast consumption over 30 minutes with 6 past measurements. The models are then evaluated over a 3-hour horizon. In air quality forecasting, models receive 12 measurements and predict indicators for the following 12 hours, with an extension to 3 days for long-term performance assessment. In air quality forecasting, models are evaluated over a 3-hour horizon with 12 measurements as input. The number of prediction steps is extended to 3 days for RNN-based models. CloudLSTM, CloudRNN, and CloudGRU outperform other architectures in terms of performance metrics. The study compares different architectures for air quality forecasting, with RNN-based models like CloudLSTM, CloudRNN, and CloudGRU showing superior performance over CNN-based models and MLP. CloudLSTM performs better than CloudGRU and CloudRNN, with the forecasting performance being insensitive to the number of neighbors. The attention mechanism improves forecasting performance by capturing better dependencies. In the study, CloudLSTM outperforms CloudGRU and CloudRNN in air quality forecasting. The forecasting performance is not affected by the number of neighbors, suggesting the use of a small K for simplicity. The attention mechanism enhances forecasting by capturing dependencies between input sequences and vectors. The long-term forecasting performance is evaluated up to 36 time steps, showing minimal MAE growth with prediction steps for most models in city 1. In city 1, the MAE does not significantly increase with the prediction step for most models, indicating reliable long-term forecasting. For city 2, low K may impact CloudLSTM's long-term performance. CloudLSTMs outperform ConvLSTM by up to 12.2% and 8.8% in MAE and RMSE for 12-step air quality forecasting. The guideline discusses choosing K for different forecast lengths. CloudLSTMs outperform ConvLSTM by up to 12.2% and 8.8% in MAE and RMSE for 12-step air quality forecasting. CloudCNN is superior to PointCNN for feature extraction. CloudLSTM models are effective for spatiotemporal point-cloud stream data modeling. Performance evaluations for long-term forecasting up to 72 future time steps are conducted on RNN-based models. Performance evaluations for long-term forecasting up to 72 future time steps are conducted on RNN-based models, showing the effectiveness of CloudLSTM models for spatiotemporal point-cloud stream data. The experiments follow strict variable-controlling methodology to study the impact of each factor, revealing that D-Conv significantly improves performance. Comparisons between different models like LSTM, ConvLSTM, PredRNN++, PointLSTM, CloudLSTM, CloudRNN, CloudGRU, and CloudLSTM highlight the contributions of various core operators. The study compares different models like LSTM, ConvLSTM, PredRNN++, PointLSTM, CloudLSTM, CloudRNN, CloudGRU, and CloudLSTM, showing that D-Conv significantly improves performance. CloudLSTM, a dedicated neural model for spatiotemporal forecasting tailored to pointcloud data streams, builds upon the DConv operator for learning spatial features while maintaining permutation invariance. The core operator is ranked higher than the RNN structure and attention mechanism in terms of contribution. CloudLSTM is a neural model for spatiotemporal forecasting using pointcloud data streams. It utilizes the DConv operator for spatial feature learning while maintaining permutation invariance. DConv can be combined with various RNN models and attention mechanisms, and efficiently implemented using standard 2D convolution operators. The input and output tensors of DConv are 3D tensors with specific shapes. The DConv operator is efficiently implemented using a standard 2D convolution operator for spatial feature learning. Input and output tensors are 3D tensors with specific shapes, and the process involves finding nearest neighbors and transforming the input tensor. The algorithm for DConv implementation includes steps like reshaping the output map and applying the sigmoid function. The DConv operator is efficiently implemented using a standard 2D convolution operator for spatial feature learning. The algorithm involves reshaping the output map and applying the sigmoid function to translate DConv into a standard convolution operation. The complexity of DConv is analyzed by separating the operation into finding nearest neighbors and performing weighting computation. The complexity of the DConv operator is analyzed by finding nearest neighbors and performing weighting computation. The overall complexity is similar to a vanilla convolution operator, introducing extra complexity by searching the K. The DConv operator introduces extra complexity by searching for nearest neighbors and performing weighting computation. Normalizing coordinates enables transformation invariance with shifting and scaling, making the model invariant to these transformations. The proposed CloudLSTM is combined with the attention mechanism for improved performance. The proposed CloudLSTM model combines with an attention mechanism for improved performance. It is compared against baseline models like MLP, CNN, and 3D-CNN in mobile traffic forecasting. In this study, the proposed CloudLSTM model is compared against baseline models like MLP, CNN, and 3D-CNN in mobile traffic forecasting. Other models such as DefCNN, LSTM, ConvLSTM, PredRNN++, CloudRNN, and CloudGRU are also discussed in relation to spatiotemporal forecasting and predictive learning. The PredRNN++ is considered the state-of-the-art model for spatiotemporal forecasting on grid-structural data, outperforming other models like ConvLSTM. The CloudRNN and CloudGRU models share a similar architecture with CloudLSTM but do not use the attention mechanism. The detailed configuration and parameter numbers for each model are shown in Table 3. Increasing the number of layers did not improve the performance of ConvLSTM, PredRNN++, and PointLSTM. In this study, different models were considered for spatiotemporal forecasting, with 2 layers used for ConvLSTM, PredRNN++, and PointLSTM. The use of 3x3 filters was found effective, yielding a receptive field of 9, supporting fair comparison. Various configurations of 2-stack Seq2seq CloudLSTM with different channels and K values were explored, along with an attention mechanism. All architectures were optimized using the MSE loss function. In this study, different models were considered for spatiotemporal forecasting, with 2 layers used for ConvLSTM, PredRNN++, and PointLSTM. Various configurations of 2-stack Seq2seq CloudLSTM with different channels and K values were explored, along with an attention mechanism. The models were optimized using the MSE loss function and evaluated using MAE, RMSE, PSNR, and SSIM metrics. The performance of models was evaluated using metrics like PSNR, VAR, and COV. Antenna locations in two cities were anonymized for data collection through deep packet inspection at the packet gateway. The operator and target regions were not disclosed for confidentiality reasons. The measurement data is collected via deep packet inspection at the packet gateway using proprietary traffic classifiers to associate flows to specific services. Anonymized locations of antenna sets in both cities are shown in Figure 5. Data collection was supervised by the national privacy agency to ensure compliance with regulations and privacy protection. The dataset used for the study only contains mobile service traffic information at the antenna level, without personal subscriber data. The dataset used for the study is fully anonymized and does not contain personal subscriber data. The raw data cannot be made public due to a confidentiality agreement. The analysis includes 38 different services, with a power law observed in the demands generated by individual mobile services. The dataset comprises 38 services, with streaming being the dominant type of traffic, accounting for almost half of the total consumption. Other services like web, cloud, social media, and chat also consume significant fractions of mobile traffic. Gaming only contributes 0.5% of the demand. The air quality dataset includes information from 43 cities in China, with 2,891,393 air quality records. The air quality dataset contains 2,891,393 records from 437 monitoring stations in 43 Chinese cities. The stations are divided into two clusters, A with 274 stations and B with 163. Missing data has been filled through linear interpolation. The dataset can be accessed at https://www.microsoft.com/en-us/research/project/urban-air/. The dataset from 43 Chinese cities contains 2,891,393 records from 437 monitoring stations. Missing data was filled through linear interpolation. The performance of Attention CloudLSTMs in forecasting accuracy for mobile services is evaluated, showing higher prediction errors for services with higher traffic volume due to more frequent fluctuations. The evaluation of RNN-based models shows that services with higher traffic volume result in higher prediction errors due to more frequent fluctuations, making the traffic series harder to predict. The MAE for long-term air quality forecasting increases over time for all models. Larger K values in CloudLSTM improve robustness, with slower MAE growth over time. This finding aligns with conclusions from the mobile traffic forecasting task. The CloudLSTM with different K values shows improved robustness, with slower MAE growth over time. Visualization of hidden features provides insights into the knowledge learned by the model. The hidden state in CloudLSTM and Attention CloudLSTM at both stacks, along with input snapshots from City 2, are visualized in scatter subplots. Features are extracted at a higher level in stack 2, showing direct spatial correlations with the output. NO2 forecasting examples in City Cluster A and B are generated by RNN-based models for air quality prediction, offering a performance comparison visually. In City Cluster A and B, RNN-based models generate NO2 forecasting examples for air quality prediction. The performance of different architectures is compared visually, with Attention Cloud-LSTMs showing better long-term visual fidelity in capturing trends in point-cloud streams. The CloudLSTM model uses DConv with Sigmoid functions to improve representability and refine the positions of input points for accurate forecasting, even with outlier points identified using DBSCAN. The CloudLSTM model, utilizing DConv with Sigmoid functions, refines input point positions for accurate forecasting, even with outlier points identified by DBSCAN. CloudCNN, with the DConv operator, outperforms other CNN-based models in forecasting performance. The CloudLSTM model achieves the lowest prediction error compared to other models, including CloudCNN with DConv operator. Experiments show CloudLSTM's robustness to outliers by moving outlier weather stations in a controlled scenario. In a toy dataset, 10 weather stations are randomly selected as outliers and their positions are moved away from the center by different distances on both x and y axes. The positions of the remaining 40 weather stations remain unchanged. The CloudLSTM model performs well in forecasting over both inliers and outliers, outperforming PointLSTM. The CloudLSTM model performs well in forecasting over both inliers and outliers, outperforming PointLSTM. It is robust to outliers, achieving significantly better performance. Additionally, comparisons with simple baselines show CloudLSTM's superior performance on the air quality dataset. The CloudLSTM model outperforms MLPs and LSTMs by considering only the K nearest neighbors' data for prediction. The number of neighbors K affects the model's receptive field, with a small K focusing on local spatial dependencies and a large K looking at larger location spaces. Results show that K does not significantly impact baseline performance, while CloudLSTM excels at extracting local spatial dependencies. The CloudLSTM model outperforms MLPs and LSTMs by considering only the K nearest neighbors' data for prediction. It extracts local spatial dependencies through DConv kernels and merges global spatial dependency via stacks of time steps and layers. The results suggest that the K value does not significantly affect baseline performance. Additionally, seasonal information in mobile traffic series can be utilized to enhance forecasting, but feeding the model with data spanning multiple days is challenging due to the long sequences involved. To improve forecasting performance, seasonal information in mobile traffic series can be utilized by concatenating 30-minute sequences with a sub-sampled 7-day window. This creates an input with a more manageable length of 90 data points. Experiments conducted on a subset of the dataset show enhanced forecasting performance with the inclusion of seasonal information. By concatenating 30-minute sequences with a sub-sampled 7-day window, seasonal information is efficiently captured for forecasting mobile traffic. Experiments on a subset of the dataset demonstrate improved forecasting performance with the inclusion of seasonal information, indicating that the model learns periodic patterns to reduce prediction errors. However, this approach increases input length and model complexity, prompting future work on more efficient fusion of seasonal information with minimal complexity increase."
}