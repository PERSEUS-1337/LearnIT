{
    "title": "Byl5NREFDr",
    "content": "We study model extraction in natural language processing, where an adversary can reconstruct a victim model using only query access. The attacker can successfully extract the model without real training data or meaningful queries, using random word sequences and task-specific heuristics. This exploit is enabled by transfer learning methods in NLP, allowing an attacker to extract a model close in performance to the victim model with a query budget of a few hundred dollars. Machine learning models are valuable intellectual property, often only accessible through web APIs. Attackers can extract models with a few hundred dollars, slightly worse than the victim model. Defense strategies like membership classification and API watermarking can be circumvented by clever adversaries. Machine learning models are valuable intellectual property, often only accessible through web APIs. Malicious users may attempt to steal models by locally reproducing them using collected (input, output) pairs. This can lead to theft of intellectual property, leakage of sensitive information, and the generation of adversarial examples. NLP APIs based on ELMo and BERT are popular due to their contextualized pretrained representations, which enhance performance and reduce sample complexity. These models may be easier to extract due to their shallow task-specific networks. Pretrained representations in NLP models like BERT boost performance and reduce sample complexity. Model extraction is possible even without access to training data, using randomly sampled word sequences and task-specific heuristics. This contrasts with prior work that required access to semantically-coherent data for large-scale attacks. Extraction attacks are feasible with randomly sampled word sequences and simple heuristics, contrasting prior work that needed access to coherent data. Using Wikipedia sentences and paragraphs as queries improves performance. Attacks are cost-effective, with the most expensive one estimated at $500. The attacker samples words to form queries and fine-tunes their BERT model using victim outputs as labels. The attacker conducts extraction attacks by sampling words to form queries and fine-tuning their BERT model using victim outputs as labels. The process is cost-effective, with the most expensive attack estimated at $500. The randomly generated queries are effective in extracting models, despite being nonsensical and uninterpretable. The randomly generated queries are effective in extracting models, despite being nonsensical and uninterpretable. Pretraining on the attacker's side makes model extraction easier. Simple defenses like membership classification and API watermarking work against na\u00efve adversaries but fail against clever ones. This research aims to inspire stronger defenses against model extraction. The study evaluates defenses against model extraction attacks, highlighting their effectiveness against naive adversaries but failure against clever ones. The research aims to inspire stronger defenses in this area and relates to prior efforts in computer vision applications and zero-shot distillation. Model extraction attacks have been studied empirically and theoretically, mostly against image classification APIs. Prior work on NLP systems attempted extraction using pool-based active learning, while this study focuses on nonsensical inputs for BERT-large models in tasks like question answering. The study focuses on extracting information from BERT-large models using nonsensical inputs for tasks like question answering, unlike prior work that used pool-based active learning. The research is related to data-efficient distillation methods and model extraction literature that utilize rubbish inputs to generate high-confidence predictions. The study explores using nonsensical inputs to extract information from BERT-large models for question answering tasks. Prior work has focused on data-efficient distillation methods and model extraction using rubbish inputs to generate high-confidence predictions. Unnatural text inputs have been shown to produce overly confident model predictions and trigger disturbing outputs from text generators. BERT-large is a 24-layer transformer model that converts word sequences into vector representations. The model's parameters are learned through masked language modeling on unlabelled data. Unnatural text inputs have been effective in training models for NLP tasks without real examples. BERT-large is a 24-layer transformer model that contextualizes vector representations of word sequences through masked language modeling. The model's parameters are learned on unlabelled text data, revolutionizing NLP with state-of-the-art performance on various tasks. Fine-tuning with task-specific networks further enhances performance by leveraging the composite function of BERT and the task-specific network. The text discusses the use of fine-tuning methodology with BERT for task-specific networks. It also describes extraction attacks where a malicious user attempts to reconstruct a local copy of a black-box API model without access to training data. The text discusses extraction attacks where an attacker reconstructs a local copy of a black-box API model using a task-specific query generator. The attacker fine-tunes a public release of BERT on nonsensical word sequences to train the model for various NLP tasks. The text discusses different input and output spaces for various NLP tasks, including binary sentiment classification, ternary natural language inference classification, extractive question answering, and boolean question answering. Two query generators, RANDOM and WIKI, are studied for generating input queries. The paragraph discusses query generators RANDOM and WIKI for NLP tasks, but they are found insufficient for tasks requiring complex interactions. Task-specific heuristics are applied to address this issue. The paragraph discusses the limitations of query generators RANDOM and WIKI for NLP tasks requiring complex interactions. Task-specific heuristics are applied to address this issue, such as randomly replacing words in the premise for MNLI and sampling words from the passage for SQuAD/BoolQ questions. Representative example queries are provided in Table 1. The paragraph discusses limitations of query generators for NLP tasks and evaluates extraction procedure with different query budgets using Google Cloud Platform's Natural Language API calculator. Development set accuracy of extracted models at various query budgets is shown in Table 3. The paragraph discusses the accuracy and agreement of extracted models on the original development set, showing high accuracy even with nonsensical inputs. Extracted SQuAD models on WIKI recover 95% of original accuracy despite training on nonsensical questions. The accuracy of extracted models remains high on original development sets, even when trained with nonsensical inputs. However, agreement between models is only slightly better than accuracy, with lower agreement on held-out sets. Extracted WIKI and RANDOM models show poor functional equivalence despite being trained on the same data distribution. The extracted model's functional equivalence is poor compared to the victim model, as shown by the F1 scores. An ablation study on query generation heuristics for SQuAD and MNLI is conducted. Results from experiments with argmax labels only show minimal accuracy drop, suggesting that access to the full probability distribution is not crucial for model extraction. The study presents results from WIKI experiments for SST2, MNLI, and BoolQ with argmax labels, showing minimal accuracy drop. Query efficiency is measured with varying budgets, indicating successful extraction even with small queries. The cost of attacks can be inferred from the results. The study raises questions about the effectiveness of nonsensical input queries in model extraction and the performance without large pretrained language models. In this section, an analysis is performed to understand the effectiveness of nonsensical input queries in model extraction. The study raises questions about the properties of these queries and their impact on model performance. It also explores whether different victim models produce the same answer with nonsensical queries and if some queries are more representative of the original data distribution. Additionally, the study questions if task-specific heuristics make nonsensical queries \"interpretable\" to humans. In this section, the study delves into the properties of nonsensical queries to understand their impact on model performance. Specifically, the RANDOM and WIKI extraction configurations for SQuAD are examined to determine if different victim models agree on answers to nonsensical queries. Five victim SQuAD models are trained with identical hyperparameters, showing high agreement on answers from the SQuAD training set. The study examines the agreement between victim SQuAD models on nonsensical queries from RANDOM and WIKI datasets. While models agree on SQuAD training and development set queries, agreement drops significantly on WIKI and RANDOM queries. High-agreement queries may be more useful for model extraction, as victim models tend to be brittle on nonsensical inputs. High-agreement queries are more useful for model extraction, showing large F1 improvements compared to random and low-agreement subsets. This suggests that agreement between victim models is a good indicator of input-output pair quality for extraction. Future work could explore integrating this observation into active learning objectives for better extraction. The agreement between victim models is a good proxy for input-output pair quality for extraction. Future work could explore if high-agreement nonsensical queries are interpretable to humans. An investigation was conducted to determine if high-agreement nonsensical textual inputs could be interpreted by humans. Annotators matched victim models' answers 23% of the time on the WIKI subset and 22% on RANDOM, using a word overlap heuristic. However, they scored significantly higher (77% exact match) on original SQuAD questions. Most nonsensical question-answer pairs remained mysterious to humans. In interviews, annotators used a word overlap heuristic to select answer spans, but many nonsensical question-answer pairs remained mysterious. The impact of pretraining setup on extraction accuracy was examined, comparing fine-tuning a BERT model to extracting a QA model from scratch. In examining the impact of pretraining setup on extraction accuracy, it was found that starting from BERT-large resulted in higher accuracy compared to BERT-base. Consistency in model choice between attacker and victim also led to better results. Starting from BERT-large gives higher accuracy compared to BERT-base. Consistency in model choice between attacker and victim improves results. Fine-tuning BERT provides attackers an advantage due to the good starting representation of language. QANet achieves high accuracy with original SQuAD inputs and BERT-large labels. The QANet model trained on SQuAD without pretraining has 1.3 million parameters. High accuracy is achieved with original SQuAD inputs and BERT-large labels, but F1 drops significantly with nonsensical queries. Better pretraining allows models to start with a good language representation, simplifying extraction. BERT-based models are vulnerable to extraction, prompting investigation into defense strategies. BERT-based models are vulnerable to model extraction, prompting investigation into defense strategies. Two defenses are explored that are effective against weak adversaries, using membership inference to detect nonsensical inputs or adversarial examples. The defense strategy against weak adversaries involves using membership inference to detect nonsensical inputs or adversarial examples, issuing random outputs when such inputs are detected. Membership inference is treated as a binary classification problem, utilizing datasets labeled as real and fake examples. Input features for training the classifier include logits and final layer representations of the victim model. The text discusses using membership inference to detect fake examples in datasets like MNLI and SQuAD. Classifiers trained on model confidence scores and rare word representations transfer well to balanced development sets. The defense strategy also includes watermarking a fraction of queries to prevent extraction. The defense strategy against extraction includes watermarking a fraction of queries to prevent model memorization of fake examples. This defense anticipates post-hoc detection of extracted models if deployed publicly. The defense strategy involves watermarking a fraction of queries to prevent model memorization of fake examples, anticipating post-hoc detection of extracted models if deployed publicly. Watermarking is evaluated on MNLI and SQuAD tasks, with minimal impact on API performance. Extracted models perform similarly with or without watermarking on the development set. Watermarking is effective in preventing model memorization of fake examples, with high WM Label Acc and low Victim Label Acc. Watermarked models behave differently from non-watermarked models, especially when looking at the watermarked subset of the training data. Training with more epochs exacerbates these differences. However, watermarking can only be used after an attack has occurred and assumes the attacker will deploy an extracted model publicly with black-box query access. Model extraction attacks against NLP APIs serving BERT-based models are effective at extracting good models with low query budgets, even when watermarking is used to prevent model memorization of fake examples. However, the effectiveness of watermarking diminishes when the attacker keeps the extracted model private or takes steps to prevent detection. Model extraction attacks against NLP APIs serving BERT-based models are surprisingly effective, even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, and further research is needed to develop robust defenses against adaptive adversaries. Further research is needed to develop robust defenses against adaptive adversaries in model extraction attacks. Future directions include leveraging nonsensical inputs for model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity. The paper also provides a distribution of agreement between victim SQuAD models on RANDOM and WIKI queries. In this paper, the cost estimate from Google Cloud Platform's Calculator was used to calculate costs for different datasets. The Natural Language APIs allow inputs of up to 1000 characters per query. Costs were extrapolated for tasks not covered by Google Cloud APIs, such as entity analysis and sentiment analysis. The study focused on models with similar computational requirements to BERT-large. In this paper, costs were estimated for various tasks using APIs like entity analysis and sentiment analysis. The study focused on models with computational requirements similar to BERT-large. API providers offer free queries, and attackers could exploit this by setting up multiple accounts for data extraction. Emulating HTTP requests for web scraping could allow for large-scale information extraction at no cost. API costs can vary depending on infrastructure and revenue models. It is important to focus on the low costs needed for data extraction rather than actual estimates. Tasks like machine translation and speech recognition are relatively inexpensive. For example, it costs -$430.56 to extract the Switchboard LDC97S62 dataset. In this section, details are provided on input generation algorithms for datasets like SST2. Two methods are discussed: one using a vocabulary built from wikitext103 and randomly sampling tokens, and the other preserving the top 10000 tokens based on unigram frequency. Input generation algorithms for datasets like SST2 involve building a vocabulary from wikitext103 and randomly sampling tokens. The top 10000 tokens based on unigram frequency are preserved, while others are discarded. Sentences are randomly chosen from wikitext103, and words not in the top-10000 vocabulary are replaced with randomly chosen words from the same vocabulary. The top-10000 wikitext103 vocabulary is used to replace words in datasets like MNLI and SQuAD. Words are randomly sampled from this vocabulary to construct premises and hypotheses. Additionally, paragraphs are generated by sampling tokens from the wikitext103 vocabulary based on unigram probabilities. The vocabulary for constructing paragraphs and questions is based on wikitext103, with unigram probabilities stored for each token. Paragraphs are created by sampling tokens from the vocabulary, and questions are built by randomly sampling paragraph tokens. The questions are then formed by adding a question starter word and a question mark. (SQuAD, WIKI) - Questions are generated similarly to (SQuAD, RANDOM) and (BoolQ, RANDOM), without adding a question mark. In this section, additional query generation heuristics are studied. Table 11 compares extraction datasets for SQuAD 1.1, showing that RANDOM performs better when paragraphs are sampled based on unigram frequency in wikitext103. In this section, additional query generation heuristics are studied. Table 11 compares extraction datasets for SQuAD 1.1, showing that RANDOM works better with paragraphs sampled based on unigram frequency in wikitext103. Starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. A similar ablation study on MNLI in Table 12 reveals that lexical overlap between premise and hypothesis affects model predictions. The general findings from recent work on MNLI show that the model's predictions are influenced by the lexical overlap between the premise and hypothesis. When the overlap is too low or too high, the dataset extraction signal is limited or unbalanced. Annotators were asked to evaluate sets of questions, and using frequent words aids in extraction. The study involved human annotators evaluating question sets with different levels of agreement among victim models. Annotators were English-speaking graduate students unfamiliar with the research goals. The inter-annotator agreement was highest for original SQuAD questions, followed by WIKI questions with high agreement, and then RANDOM questions with high agreement. In an ablation study on input features for the membership classifier, two candidates were considered: 1) the logits of the BERT classifier indicating confidence scores, and 2) the last layer representation containing lexical, syntactic, and semantic information. The ordering of inter-annotator agreement was observed to reflect the closeness to the actual input distribution. The ablation study on input features for the membership classifier compared the effectiveness of using the logits of the BERT classifier versus the last layer representations. Results showed that the last layer representations were more effective in distinguishing between real and fake inputs, but the best results were achieved by using both feature sets. The study measured accuracy on different sets (WIKI, RANDOM, SHUFFLE) and noted that the last layer representations were more effective in classifying points as real or fake."
}