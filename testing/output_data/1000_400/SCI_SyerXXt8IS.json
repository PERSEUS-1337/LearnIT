{
    "title": "SyerXXt8IS",
    "content": "Auto-generate stronger input features for ML methods with limited training data. Biological neural nets (BNNs) excel at fast learning, particularly the insect olfactory network which learns new odors rapidly using competitive inhibition, sparse connectivity, and Hebbian updates. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, outperforming other methods and reducing test set errors by 20% to 55% on MNIST and Omniglot datasets. The \"insect cyborgs\" created by MothNet, combining BNN and ML methods, outperform baseline ML methods on MNIST and Omniglot datasets, reducing test set errors by 20% to 55%. MothNet's feature generator surpasses PCA, PLS, and NNs, showing the potential of BNN-inspired feature generators in ML. The limited-data constraint in ML targets using medical, scientific, or field-collected data hinders high performance and deployment. To address this, an architecture is proposed to automatically generate new class-separating features from existing features, inspired by the rapid learning ability of Biological Neural Networks (BNNs) like the insect olfactory network. This network, consisting of the Antennal Lobe (AL) and Mushroom Body (MB), can learn new odors with just a few exposures, showcasing effective feature generation capabilities. Learning requires effective ways to separate classes, and BNNs like the insect olfactory network, with the Antennal Lobe (AL) and Mushroom Body (MB), can learn new odors with just a few exposures. The MothNet model, based on the AL-MB network, demonstrated rapid learning of vectorized MNIST digits, outperforming standard ML methods with limited training samples per class. Key elements include competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. The MothNet model of the M. sexta moth AL-MB showed rapid learning of vectorized MNIST digits, outperforming standard ML methods with limited training samples per class. It includes competitive inhibition in the AL, sparsity in the MB, and weight updates affecting only MB\u2192Readout connections. The MothNet model of the M. sexta moth AL-MB demonstrates rapid learning of vectorized MNIST digits, surpassing standard ML methods with limited training samples per class. It incorporates competitive inhibition in the AL, sparsity in the MB, and weight updates that only impact MB\u2192Readout connections. The architecture serves as a front-end feature generator for an ML classifier, combining MothNet with a downstream ML module to enhance automatic feature generation. The MothNet model serves as a front-end feature generator for an ML classifier by combining it with a downstream ML module. The trained Mothnet Readouts significantly improved the accuracies of ML methods on a non-spatial dataset, providing class-relevant information encoded by the AL-MB network. The MothNet model improved ML accuracies on non-spatial datasets by providing class-relevant information encoded by the AL-MB network. MothNet-generated features outperformed PCA, PLS, NNs, and transfer learning in enhancing ML accuracy. vMNIST was created by downsampling and vectorizing the MNIST dataset to have 85 pixels-as-features. The insect-derived network (MothNet) significantly improved ML accuracy compared to other methods. vMNIST was created by downsampling and vectorizing the MNIST dataset to 85 pixels-as-features. Full network architecture details and Matlab code for the experiments can be found in references [11] and [12]. MothNet instances were randomly generated from connectivity templates for experiments comparing Cyborg vs baseline ML methods on vMNIST. The MothNet instances were randomly generated from connectivity templates for experiments comparing Cyborg vs baseline ML methods on vMNIST. The experiments involved training ML methods on samples from vMNIST, training MothNet using stochastic differential equation simulations and Hebbian updates, and then retraining ML methods with outputs from the trained MothNet as additional features. Trained ML accuracies of the baselines and cyborgs were compared. Using time-evolved stochastic differential equation simulations and Hebbian updates, ML methods were retrained from scratch with Readout Neuron outputs from MothNet as additional features. These \"insect cyborgs\" were compared to baselines to assess gains in trained ML accuracies. MothNet features were compared to features generated by PCA and PLS on vMNIST experiments. Feature generators used in the study included PCA, PLS, NN pre-trained on vMNIST, and NN with weights initialized by training on Omniglot data. CNNs were not used due to the lack of spatial content in vMNIST. The baseline NN method utilized one hidden layer, as adding a second layer did not improve performance. The study utilized feature generators such as PCA, PLS, and NN pre-trained on vMNIST and Omniglot data. One hidden layer was used in the baseline NN method, as adding a second layer did not enhance performance. MothNet features significantly improved ML accuracy compared to other feature generators like PCA, PLS, and NN. The MothNet architecture effectively captured new class-relevant features, outperforming other feature generators like PCA, PLS, and NN. MothNet features increased accuracy across all ML models, with a relative reduction in test set error ranging from 20% to 55%. NN models saw the greatest benefits, with a 40% to 55% relative reduction in error. The MothNet features improved accuracy across all ML models, with a relative reduction in test set error ranging from 20% to 55%. NN models benefited the most, with a 40% to 55% reduction in error. Gains were significant in almost all cases, even when the baseline accuracy exceeded MothNet's ceiling. The MothNet features significantly improved accuracy across all ML models, with gains observed in cases where N > 3. The cyborg framework was utilized on vMNIST using various feature generators such as PCA, PLS, and NN. MothNet features outperformed other methods, showcasing the effectiveness of the competitive inhibition layer (AL) and high-dimensional sparse layer (MB). The MothNet architecture consists of a competitive inhibition layer (AL) and a high-dimensional sparse layer (MB). Testing the MB alone showed significant accuracy improvements, with cyborgs using a pass-through AL still outperforming baseline ML methods. The AL layer added value by generating strong features, benefiting neural networks the most. The MothNet architecture with pass-through ALs showed gains between 60% and 100% compared to cyborgs with normal ALs, indicating the importance of the high-dimensional trainable layer. Competitive inhibition of the AL layer contributed up to 40% of the total gain, with NNs benefiting the most. A bio-mimetic feature generator based on a simple BNN significantly improved learning abilities on vMNIST and vOmniglot datasets. The MothNet architecture with pass-through ALs showed significant gains compared to cyborgs with normal ALs, with competitive inhibition contributing up to 40% of the total improvement. MothNet's bio-mimetic feature generator enhanced learning abilities on vMNIST and vOmniglot datasets by making class-relevant information more accessible. The sparse connectivity from AL to MB functions as an additive feature, enhancing classification by creating attractor basins for inputs. The sparse connectivity from AL to MB creates attractor basins for inputs, pushing samples towards their respective class attractors. The insect MB functions similarly to sparse autoencoders but with differences such as not seeking to match the identity function and having a greater number of active neurons. The MB requires very few samples to improve classification and lacks recurrent connections found in Reservoir Networks. The Mushroom Body (MB) has more active neurons than the input dimension, requires few samples for structure improvement, lacks recurrent connections, and uses a Hebbian update mechanism distinct from backpropagation. The dissimilarity of optimizers (MothNet vs ML) may increase total encoded information."
}