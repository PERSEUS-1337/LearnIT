{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this bias, a new balanced face image dataset with 108,501 images representing 7 race groups was created. Models trained on this dataset showed improved accuracy across different race and gender groups. The YFCC-100M Flickr dataset was labeled with race, gender, and age groups for evaluations on face attribute datasets. The model trained on this dataset showed higher accuracy on novel datasets and consistent performance across race and gender groups. Comparison of commercial computer vision APIs also reported balanced accuracy across gender, race, and age groups. Numerous large-scale face image datasets have been proposed, fostering research in automated face detection, alignment, and recognition. Various systems have been developed for automated face detection, alignment, recognition, generation, modification, and attribute classification. These systems have been applied in various fields such as security, medicine, education, and social sciences. However, existing public face datasets are biased towards Caucasian faces, with other races being underrepresented. Existing public face datasets are biased towards Caucasian faces, with other races significantly underrepresented. This bias can lead to unfairness in automated systems and raises ethical concerns about the fairness of AI models. Biased data can lead to biased models, raising ethical concerns about fairness in automated systems. Commercial face gender classification systems have been criticized for their asymmetric accuracy across sub-demographics, performing better on male and light faces due to biases in training data. Unwanted biases in image datasets can easily occur, impacting the performance of AI models. The commercial face gender classification systems perform better on male and light faces due to biases in training data. Biases in image datasets can easily occur, especially in datasets collected from popular online media platforms that predominantly feature White people. To address race bias, a novel face dataset with balanced race composition containing 108,501 facial images has been proposed. The proposed novel face dataset contains 108,501 facial images with a balanced race composition of 7 groups. It outperforms existing datasets on unseen data with more nonWhite faces and shows consistent performance across racial groups. The new face attribute dataset includes Latino, Middle Eastern, East Asian, and Southeast Asian faces, improving generalization to unseen data with more nonWhite faces. This dataset enhances the applicability of computer vision methods to various fields by including major racial groups previously missing in existing datasets. The new face attribute dataset includes major racial groups previously missing in existing datasets, improving generalization to unseen data with more nonWhite faces. Face attribute recognition aims to classify human attributes like gender, race, age, emotions, and expressions from facial appearance. Existing datasets are typically dominated by the White race, so inclusion of diverse demographics enlarges the applicability of computer vision methods. Face attribute recognition is used in tasks like face verification and person re-identification. Most datasets for face attribute recognition are dominated by the White race, but efforts are being made to include more diverse demographics. It is crucial for these systems to perform equally well across different gender and race groups to avoid incidents of racial bias, which can damage the reputation of service providers and the research community. Notable incidents include Google Photos misidentifying African American faces and Nikon's cameras prompting messages to Asian users. The computer vision research community has faced incidents of racial bias, such as Google Photos recognizing African American faces as Gorilla and Nikon's cameras prompting messages to Asian users. These incidents often lead to termination of services or features. Commercial service providers have stopped providing race classifiers due to these issues. Face attribute recognition is used for demographic surveys in marketing and social science research to understand human social behaviors and demographic backgrounds. Social scientists use off-the-shelf tools and commercial services to infer demographic attributes from images of people. The use of face attribute recognition for demographic surveys in marketing and social science research has led to the analysis of human social behaviors and demographic backgrounds. Social scientists utilize images of people to infer demographic attributes and analyze behaviors, with a focus on algorithmic fairness and avoiding biases in datasets and models. The cost of unfair classification is significant as it can skew analysis of specific sub-populations, leading to policy implications. AI and machine learning communities are increasingly focusing on algorithmic fairness and biases in datasets and models. Different definitions of fairness exist in the literature, but this paper specifically looks at balanced accuracy, ensuring attribute classification is independent of race and gender. Research in fairness aims to ensure models produce fair outcomes regardless of protected attributes like race or gender. Studies in algorithmic fairness either audit existing bias in datasets or focus on discovering bias in systems. Research in fairness focuses on ensuring fair outcomes regardless of protected attributes like race or gender. Studies in algorithmic fairness aim to discover bias in datasets or systems and improve datasets or algorithms. The paper discussed in the curr_chunk falls into the categories of auditing bias in datasets and improving datasets for gender classification from facial images. Buolamwini & Gebru (2018) highlighted biases in commercial gender classification systems, particularly in accuracy for dark-skinned females, possibly due to skewed image origins. The paper aims to mitigate biases in gender classification from facial images by collecting more diverse face images from non-White race groups, improving generalization performance to novel image datasets. The paper aims to mitigate biases in gender classification by collecting diverse face images from non-White race groups, improving generalization performance to novel image datasets. The dataset includes Southeast Asian and Middle Eastern races, addressing discrimination in major race group representation. The dataset defines 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is based on physical traits while ethnicity is based on cultural similarities. Latino is considered a race based on facial appearance. In this study, 7 race groups were defined based on physical traits, including White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Latino is considered a race, not just an ethnicity, and facial appearance is used for classification. The dataset bias measurement criterion is debated, with some studies using skin color as a proxy for race or ethnicity grouping. The experiments in this paper focused on 7 race classification and debated whether dataset bias should be measured based on skin color or race. Skin color is used as a proxy for racial or ethnicity grouping in some studies, but it has limitations due to variations affected by lighting conditions and being one-dimensional compared to the multidimensional concept of race. The text discusses the limitations of using skin color as a proxy for race classification, highlighting variations in skin color due to lighting conditions and the one-dimensional nature of skin color compared to the multidimensional concept of race. The study emphasizes the importance of using race as a multidimensional concept and annotating physical race based on human judgments. The text discusses the limitations of using skin color as a proxy for race classification, emphasizing the importance of annotating physical race based on human judgments. Face datasets sourced from public figures may introduce biases, such as age and attractiveness disparities. Some datasets collected via web search using keywords may prioritize stereotypical or celebrity faces. The text discusses biases in face datasets sourced from public figures, emphasizing the need to annotate physical race based on human judgments. Some datasets collected via web search using keywords may prioritize stereotypical or celebrity faces. The goal is to minimize selection bias and maximize diversity in the dataset by starting from a large public image dataset and incrementally increasing dataset size. The dataset was incrementally increased by detecting and annotating faces from the Yahoo YFCC100M dataset, ensuring a balanced representation of different races. The dataset excluded U.S. and European countries to prevent dominance by White faces, with a minimum face size set at 50 by 50 pixels. We adjusted the number of images for each country to avoid dominance by the White race. The minimum face size was set to 50 by 50 pixels to ensure recognizability. Only images with specific Creative Commons licenses were used, and race, gender, and age group annotations were done using Amazon Mechanical Turk with three workers per image. Ground-truth values were determined based on worker agreement. The study used images with \"Attribution\" and \"Share Alike\" Creative Commons licenses for annotation of race, gender, and age group using Amazon Mechanical Turk. Three workers were assigned per image, with ground-truth values determined by worker agreement. Annotations were refined using a model trained on initial annotations and manually verified for accuracy. Race composition of datasets was measured, with annotations done for datasets without race information. The study measured the race composition of datasets by annotating race labels for 3,000 random samples. Most face attribute datasets are biased towards the White race. Gender balance in datasets ranges from 40%-60% male ratio. Model performance was compared using the ResNet-34 architecture trained from each dataset. Face detection was done using the ADAM optimization algorithm. The study compared model performance using the ResNet-34 architecture trained on different datasets. Face detection was conducted using the ADAM optimization algorithm and a CNN-based face detector. Three datasets were compared: UTKFace, LFWA+, and CelebA, with UTKFace and LFWA+ having race annotations for comparison. CelebA was only used for gender classification as it lacks race annotations. The study compared model performance using the ResNet-34 architecture trained on different datasets: UTKFace, LFWA+, and CelebA. UTKFace and LFWA+ have race annotations for comparison, while CelebA was used only for gender classification due to the lack of race annotations. FairFace defines 7 race categories but only 4 races were used for comparison. Cross-dataset classifications were performed using models trained from these datasets. The study compared model performance using the ResNet-34 architecture trained on different datasets: UTKFace, LFWA+, and CelebA. Cross-dataset classifications were performed by alternating training and test sets. FairFace, with 7 races, was merged with other datasets for compatibility. Results for race, gender, and age classification were shown in Tables 2 and 3. The model performed best on LFWA+ due to its diversity and generalizability. Generalization performance was tested on three novel datasets collected from different sources. The model performed best on the LFWA+ dataset due to its diversity and generalizability. To test generalization performance, three novel datasets were used, collected from different sources, including geo-tagged Tweets and media photographs. The study collected images from geo-tagged Tweets, media photographs from professional outlets, and a protest dataset for analysis. The study collected 8,000 faces from media photographs and a protest dataset for analysis. The dataset includes diverse race and gender groups engaged in various activities across different countries. The faces were annotated for gender, race, and age. Gender classification accuracy was measured on external validation datasets, and different models were evaluated. The dataset is larger than LFWA+. The study collected 8,000 faces from diverse race and gender groups engaged in various activities across different countries. Faces were annotated for gender, race, and age. Gender classification accuracy was measured on external validation datasets, and different models were evaluated. The FairFace model outperformed all others for race, gender, and age on novel datasets, even with smaller training image sizes. This suggests dataset size is not the only factor for improved accuracy. The FairFace model outperforms other models for race, gender, and age on novel datasets from different sources, even with smaller training image sizes. It also shows more consistent results across different race groups and measures model consistency for fair classification. The FairFace model demonstrates superior performance for race, gender, and age on new datasets from various sources, with consistent results across different race groups. Model consistency is measured by standard deviations of classification accuracy on various sub-populations, with the FairFace model achieving the lowest maximum accuracy disparity compared to other models. The FairFace model achieves the lowest maximum accuracy disparity in gender classification, with less than 1% discrepancy between male and female, and White and non-White groups. Other models show a strong bias towards males, with the LFWA+ model having the biggest gender performance gap at 32%. The study found that most models exhibit a strong bias towards males in gender classification, with lower accuracy for females and non-White groups. The largest gender performance gap was observed in the LFWA+ model at 32%. The unbalanced representation in training data is likely the cause of these biases. Additionally, data diversity was measured using t-SNE visualization of facial embeddings from various datasets. The dataset characteristics were analyzed to measure data diversity. Faces from biased datasets were visualized in 2D space using t-SNE, showing loose separation of race groups. FairFace had well-spread faces, while LFWA+ and UTKFace focused on clusters. The study also found gender bias in models, with lower accuracy for females and non-White groups. The dataset analysis revealed that LFWA+ and UTKFace datasets focus on clusters of faces, with UTKFace showing tightly clustered and similar faces. In contrast, LFWA+ exhibited diverse faces despite mostly white faces. This diversity was attributed to the training of the face embedding model. The UTKFace dataset had tightly clustered and similar faces, while LFWA+ showed diverse faces despite being mostly white. The diversity in LFWA+ was attributed to the training of the face embedding model. Previous studies have shown inconsistent classification accuracies in face analytic models across different demographic groups. Testing with FairFace images revealed variations in gender classification using online APIs. The study measured distances of faces in 3 datasets using L1 distance on face embedding. They tested gender classification with FairFace images using various online APIs. The dataset was diverse in race, age, expressions, head orientation, and photographic conditions, making it a better benchmark for bias measurement. The experiments were conducted on 7,476 random samples from FairFace, excluding children under 20 years old. The study measured distances of faces in 3 datasets using L1 distance on face embedding. Gender classification was tested with FairFace images using various online APIs. 7,476 random samples from FairFace were used, excluding children under 20 years old. The experiments were conducted on August 13th -16th, 2019. Table 6 displays gender classification accuracies of tested APIs on 7,476 faces, with Amazon Rekognition detecting all faces. Detection rates are reported in Table 8 in the Appendix. Two sets of accuracies are provided: one including mis-detections as mis-classifications and one excluding them. A model trained with the dataset is included for comparison. The study analyzed gender classification accuracies of various APIs on 7,476 faces, with Amazon Rekognition detecting all faces. Results showed a bias towards male category and higher error rates for dark-skinned females. Some APIs classified Indians more accurately than Whites despite darker skin tones. The study found that gender classifiers still favor the male category, with dark-skinned females experiencing higher error rates. However, skin color alone is not a sufficient indicator of bias. Face detection can also introduce gender bias, as seen with Microsoft's model. The paper proposes a new face image dataset balanced on race, gender, and age, which outperforms existing datasets in classification performance. This paper introduces a novel face image dataset balanced on race, gender, and age, achieving better classification performance for gender, race, and age compared to existing datasets. The dataset includes non-White faces from sources like Twitter and online newspapers, leading to balanced accuracy across race groups. Derived from the Yahoo YFCC100m dataset, images are available for academic and commercial use in training new models. Our dataset, derived from Yahoo YFCC100m, ensures balanced accuracy across race groups. It can be used for training new models and verifying classifier accuracy. Algorithmic fairness is crucial in AI systems design, especially as they impact decision making in society. The dataset aims to mitigate race and gender bias in computer vision systems. The novel dataset proposed in this paper aims to mitigate race and gender bias in computer vision systems, improving transparency and acceptance in society. The dataset ensures balanced accuracy across race groups and can be used for training new models and verifying classifier accuracy."
}