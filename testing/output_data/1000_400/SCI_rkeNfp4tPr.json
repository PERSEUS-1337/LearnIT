{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is popular for training deep neural networks. The addition of a momentum term biases the update in the direction of the previous change in parameters, improving convergence time. Theoretical justification for the use of stochastic momentum remains an open question. In training deep networks, stochastic momentum improves convergence time by helping SGD escape saddle points faster and find a second order stationary point more quickly. The ideal momentum parameter should be large (close to 1), as suggested by theoretical and empirical findings. Experimental results further support these conclusions. SGD with stochastic momentum is a widely used algorithm in nonconvex optimization and deep learning. It has been adopted in various machine learning applications such as computer vision, speech recognition, natural language processing, and reinforcement learning. The ideal momentum parameter should be close to 1, as supported by theoretical and empirical findings. The advantage of using SGD with stochastic momentum has been widely observed in the literature. SGD with stochastic momentum is widely used in machine learning applications like computer vision, speech recognition, natural language processing, and reinforcement learning. The success of momentum in training deep neural nets has been demonstrated to achieve faster convergence compared to standard SGD. Momentum is considered a necessary tool for designing new optimization algorithms in deep learning. Popular adaptive stochastic gradient methods like Adam and AMSGrad also incorporate the use of momentum. In this paper, a theoretical analysis is provided for stochastic gradient descent (SGD) with momentum. The use of momentum in optimization algorithms, including popular methods like Adam and AMSGrad, has shown empirical improvements in practice. The momentum parameter setting, typically a large value like \u03b2 = 0.9, has been observed to work well. Algorithm 1, which includes momentum, is the default method in software packages like PyTorch and Tensorflow. In this paper, a theoretical analysis is provided for stochastic gradient descent (SGD) with momentum. The analysis identifies conditions that guarantee SGD with stochastic momentum escapes saddle points faster than standard SGD. Stochastic heavy ball momentum maintains a weighted average of stochastic gradients and updates the current iterate in the direction of the momentum. In this paper, the focus is on finding a second-order stationary point for smooth non-convex optimization using SGD with stochastic heavy ball momentum. The goal is to obtain a guarantee for second-order optimality in nonconvex optimization problems. The paper aims to find a second-order stationary point in nonconvex optimization using stochastic heavy ball momentum. The goal is to show the benefit of using momentum to reach an approximate second-order stationary point. The paper focuses on using stochastic heavy ball momentum to reach an approximate second-order stationary point in nonconvex optimization. It introduces a condition ensuring updates correlate with negative curvature directions of the function. The paper introduces the stochastic momentum m t satisfying Correlated Negative Curvature (CNC) to escape saddle points faster in nonconvex optimization. It shows that under certain assumptions and constraints, SGD with momentum can reach a second-order stationary point in T = O((1 \u2212 \u03b2) log(1/(1 \u2212 \u03b2) ) \u221210 ) iterations. The paper demonstrates that a larger momentum parameter \u03b2 can help escape saddle points faster in nonconvex optimization. It shows that under certain properties like Almost Positively Aligned with Gradient (APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature Exploitation (GrACE), SGD with momentum can reach a second-order stationary point in T = O((1 \u2212 \u03b2) log(1/(1 \u2212 \u03b2) ) \u221210 ) iterations. The text discusses the benefits of stochastic momentum in avoiding saddle points during optimization and deep learning. It explains how a larger momentum parameter can help escape saddle points faster, leading to reaching a second-order stationary point in fewer iterations. In an iterative update scheme, parameters can enter a saddle point region where gradient updates may drift slowly. Moving along the direction of the smallest eigenvector guarantees a fast escape from the saddle point. Daneshmand et al. (2018) study non-momentum SGD to avoid prohibitively expensive 2nd-order methods. The CNC property in stochastic momentum ensures updates escape saddle points by requiring the update direction to be strongly non-orthogonal to the direction of large negative curvature. This property is similar to the one studied by Daneshmand et al. (2018) in non-momentum SGD. In this paper, the study focuses on stochastic momentum and the CNC property, which requires the update direction to be non-orthogonal to the negative curvature direction. The analysis shows that updates escape saddle points due to this property, especially when \u03b2 is close to 1. Momentum accelerates the escape process by maintaining a significant correlation with the negative curvature direction. The effectiveness of momentum in improving performance is questioned. The study focuses on the effectiveness of stochastic momentum in accelerating the escape process from saddle points. Momentum can speed up saddle-point escape by a factor of 1 \u2212 \u03b2, but \u03b2 is constrained and cannot be chosen arbitrarily close to 1. Empirical evidence demonstrates the clear benefit of stochastic momentum in solving optimization tasks with significant saddle points. The study demonstrates the clear benefit of stochastic momentum in accelerating the escape process from saddle points in optimization tasks. Two tasks with significant saddle points are constructed, showing the effectiveness of stochastic momentum. In optimization tasks, stochastic momentum accelerates the escape from saddle points. Two tasks with significant saddle points demonstrate the effectiveness of stochastic momentum. Perturbations are given by b i \u223c N (0, diag([0.1, 0.001])), with lower noise in the escape direction. Convergence in function value f (\u00b7) is plotted, with initialization always set as w0 = 0. All algorithms use the same step size \u03b7 = 5 \u00d7 10 \u22125. Convergence in relative distance to the true model w * is also plotted, capturing progress as the global sign of the objective is unrecoverable. SGD and SGD with momentum are initialized at the origin to escape saddle points before convergence. The second objective (4) is related to the phase retrieval problem with real applications. In optimization tasks, stochastic momentum accelerates escape from saddle points. Larger choices of \u03b2 significantly accelerate convergence for both objectives. Empirical findings show trajectories with large momentum escape saddle points more effectively. Phase retrieval problem involves finding unknown w * \u2208 R d with access to few samples y i = (a i w * ) 2. The heavy ball method accelerates convergence in optimization tasks, particularly in finding optimal solutions in phase retrieval. Larger choices of \u03b2 significantly speed up convergence for both objectives, with trajectories with large momentum escaping saddle points more effectively. This empirical finding highlights the effectiveness of stochastic momentum in accelerating the search for optimal solutions. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases. However, in specialized algorithms aiming to reach a second-order stationary point, explicit exploitation of negative curvature can help escape saddle points faster. Our work belongs to the category of specialized algorithms designed to exploit negative curvature and escape saddle points faster. Previous pioneer works in this category include Ge et al. (2015) and Jin et al. (2017). Phase retrieval is nonconvex with the strict saddle property, where every local minimizer is global up to phase and each saddle exhibits negative curvature. Our work assumes Correlated Negative Curvature (CNC) for stochastic momentum instead of gradient, aiming to escape saddle points faster. The algorithm avoids perturbing updates with isotropic noise and compares results with related works in Appendix A. The gradient is assumed to be L-Lipschitz and the Hessian is assumed to be L-smooth. Our work assumes Correlated Negative Curvature (CNC) for stochastic momentum instead of gradient, avoiding isotropic noise perturbations. The gradient is assumed to be L-Lipschitz, and the Hessian is assumed to be \u03c1-Lipschitz. Stochastic momentum analysis relies on three key properties, which we aim to demonstrate empirically. The analysis of stochastic momentum relies on three key properties: Almost Positively Aligned with Gradient (APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature Exploitation (GrACE). These properties are important in natural settings and are demonstrated empirically in standard problems. The analysis of stochastic momentum relies on key properties such as Almost Positively Correlated with Gradient (APCG) and Gradient Alignment or Curvature Exploitation (GrACE). APCG requires a parameter \u03c4 and a PSD matrix M t, while GrACE demands that the momentum term aligns with the gradient to ensure progress in the algorithm. The analysis of stochastic momentum relies on key properties such as Almost Positively Correlated with Gradient (APCG) and Gradient Alignment or Curvature Exploitation (GrACE). APCG requires a parameter \u03c4 and a PSD matrix M t, while GrACE demands that the momentum term aligns with the gradient to ensure progress in the algorithm. The bias relative to the gradient of the deterministic f should not be too large for APAG, which is only needed when the gradient is large to guarantee progress. APCG requires the momentum term to be almost positively correlated with the gradient in the Mahalanobis norm induced by M t, measuring the local curvature of the function with respect to the trajectory of the SGD with momentum dynamic. The analysis focuses on properties such as Almost Positively Correlated with Gradient (APCG) and Gradient Alignment or Curvature Exploitation (GrACE) in stochastic momentum. Empirical results show that these properties hold on natural problems with a reasonable constant. The value is large when the gradient is significant, except during transitions, and almost always nonnegative. The figures demonstrate the APAG and APCG properties of SGD with momentum in experiments, particularly in the phase retrieval problem. The analysis focuses on properties like Almost Positively Correlated with Gradient (APCG) and Gradient Alignment or Curvature Exploitation (GrACE) in stochastic momentum. Results show these properties hold on natural problems with a reasonable constant. The value is large when the gradient is significant, except during transitions, and almost always nonnegative. The figures demonstrate the APAG and APCG properties of SGD with momentum in experiments, particularly in the phase retrieval problem. Additionally, the analysis does not require APCG to hold when the gradient is large or the update is at a second-order stationary point. The analysis focuses on properties like Almost Positively Correlated with Gradient (APCG) and Gradient Alignment or Curvature Exploitation (GrACE) in stochastic momentum. The first term measures alignment between stochastic momentum and gradient, while the second term measures curvature exploitation. A small sum of the two terms allows bounding the function value of the next iterate. Figures show quantities related to APAG and APCG in experiments with SGD momentum, and a quantity regarding GrACE. The analysis focuses on properties like Almost Positively Correlated with Gradient (APCG) and Gradient Alignment or Curvature Exploitation (GrACE) in stochastic momentum. The proof is structured into three cases based on the gradient values, and the algorithm analyzed is Algorithm 2 with a boosted step size. The analysis focuses on properties like Almost Positively Correlated with Gradient (APCG) and Gradient Alignment or Curvature Exploitation (GrACE) in stochastic momentum. The precise algorithm analyzed is Algorithm 2, with a boosted step size, showing progress in different cases and reaching a second-order stationary point. The algorithm with stochastic momentum shows progress in different cases and reaches a second-order stationary point with high probability. The algorithm with stochastic momentum satisfies CNC, APAG, APCG T thred, and GrACE properties, reaching a second-order stationary point in a certain number of iterations with high probability. Higher \u03b2 values help escape saddle points faster. Constraints on \u03b2 are necessary to prevent it from being too close to 1. Higher \u03b2 values enable faster escape from saddle points. Constraints on \u03b2 are necessary to prevent it from being too close to 1. In the high momentum regime, Algorithm 2 is shown to be strictly better than CNC-SGD, indicating that higher momentum can help find a second-order stationary point faster. The text demonstrates escaping saddle points faster with momentum, showing that Algorithm 2 is better than CNC-SGD in the high momentum regime. Empirically, it is found that certain conditions are easily satisfied for a wide range of \u03b2 values. The analysis focuses on escaping saddle points by SGD with momentum, showing that it takes at most T thred iterations to escape a region with small gradient and large negative eigenvalue of the Hessian. The text discusses escaping saddle points faster with momentum in Algorithm 2 compared to CNC-SGD in the high momentum regime. It focuses on showing that it takes at most T thred iterations to escape a region with small gradient and large negative eigenvalue of the Hessian. The analysis uses a proof by contradiction to demonstrate that the function value decreases by at least F thred on expectation when escaping the region. The analysis shows that the function value must decrease by at least F thred in T thred iterations on expectation. Larger momentum helps in escaping saddle points faster. Lemma 1 provides an upper bound of the expected distance, with the proof in Appendix C. Lemma 1 gives an upper bound on the expected distance, while Lemma 2 provides a lower bound based on the recursive dynamics of SGD with momentum. Lemma 3 further analyzes the dominant term in the lower bound. Lemma 3 analyzes the dominant term in the lower bound of the expected distance in the context of the recursive dynamics of SGD with momentum. The proof of Lemma 2 is provided in Appendix D. The lower bound is shown to be monotone increasing with t and the momentum parameter \u03b2. Lemma 5 states that if SGD with momentum has the APCG property, then certain conditions must be met for reaching a second-order stationary point faster. Lemma 5 states that if SGD with momentum has the APCG property, then certain conditions must be met for reaching a second-order stationary point faster. The paper identifies three properties that ensure faster convergence to a second-order stationary point with higher momentum, supporting the use of a large momentum parameter. The greater momentum helps escape strict saddle points faster by enlarging the projection to an escape direction. However, ensuring that SGD with momentum possesses these properties is not clear, and further research is needed to understand the conditions that guarantee these properties. The heavy ball method, proposed by Polyak in 1964, does not provide a speedup over standard gradient descent in most cases, except for convex quadratic objectives. Recent research has explored its application to other optimization problems beyond quadratic functions. In the deterministic setting, the heavy ball method does not offer a convergence speedup compared to standard gradient descent, except for convex quadratic objectives. Recent research has analyzed its application to other optimization problems, showing that the expected gradient norm converges at a rate of O(1/ \u221a t), which is not better than standard SGD. Some variants of stochastic accelerated algorithms have been proposed, but they do not capture the stochastic heavy ball momentum used in practice. The framework proposed by Ghadimi & Lan (2016; 2013) does not include the stochastic heavy ball momentum used in practice. Kidambi et al. (2018) found that SGD with heavy ball momentum may not achieve the best convergence rate for specific problems. Various works focus on reaching a second order stationary point, with specialized algorithms designed to exploit negative curvature for faster escape from saddle points. Various works aim at reaching a second order stationary point, categorized into specialized algorithms and simple GD/SGD variants. Specialized algorithms exploit negative curvature to escape saddle points faster, such as using a preconditioning matrix for updates. Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates, assuming stochastic gradients can help escape saddle points. Iteration complexity results for simple SGD variants are summarized in Table 1. The text discusses the average-SGD algorithm proposed by Fang et al. (2019) and compares it to standard SGD. It also explores the effectiveness of stochastic heavy ball momentum in practice. The analysis framework is based on previous work by Daneshmand et al. (2018). The text discusses the advantage of using stochastic heavy ball momentum in practice, building on previous work by Daneshmand et al. (2018). Lemmas 6, 7, and 8 show that SGD with momentum decreases the function value under certain conditions and bounds the increase of function value for the next iterate. Lemma 6, 7, and 8 demonstrate that stochastic heavy ball momentum decreases the function value and bounds the increase of function value for the next iterate under specific conditions. Lemma 6, 7, and 8 show that stochastic heavy ball momentum reduces the function value and limits its increase for the next iteration. The APAG property is used in the inequalities, and the GrACE property is considered for the update step w t+1 = w t \u2212 \u03b7m t. The update rule w t+1 = w t \u2212 \u03b7m t is analyzed, taking into account the \u03c1-Lipschitzness of the Hessian. The update rule for stochastic heavy ball momentum is analyzed, considering the \u03c1-Lipschitzness of the Hessian. Lemmas 1, 2, and 3 provide bounds on the function value decrease and limit its increase for the next iteration. The update step w t+1 = w t \u2212 \u03b7m t is discussed, utilizing the APAG and GrACE properties. The update rule for stochastic heavy ball momentum is analyzed with consideration of the \u03c1-Lipschitzness of the Hessian. Lemmas 2 and 3 provide bounds on function value decrease and limit its increase for the next iteration, utilizing a quadratic approximation at time t0. The update rule for stochastic heavy ball momentum is analyzed with consideration of the \u03c1-Lipschitzness of the Hessian. Lemmas 2 and 3 provide bounds on function value decrease and limit its increase for the next iteration, utilizing a quadratic approximation at time t0. The proof involves rewriting equations and using definitions to establish the desired results. The update rule for stochastic heavy ball momentum is analyzed with consideration of the \u03c1-Lipschitzness of the Hessian. Lemmas 2 and 3 provide bounds on function value decrease and limit its increase for the next iteration, utilizing a quadratic approximation at time t0. By recursively expanding equations and using definitions, the proof establishes the desired results, including constraints on the parameter \u03b2. The constraints on parameter \u03b2 ensure it is not too close to 1, upper-bounding its value. The dependence on L, \u03c3, and c is artificial and can be adjusted without loss of generality. Lemmas with parameter choices are used to prove Lemma 5. The function is assumed to be 1-smooth with L > 1. The dependence on \u03c3 can be adjusted by increasing the variance of the stochastic gradient. Lemmas with parameter choices are utilized to prove Lemma 5, including upper bounding E t0 [ q q,t\u22121 ] and deriving upper bounds for gradients. The text discusses upper bounds on gradients and Hessian Lipschitz properties in the context of a 1-smooth function with L > 1. Lemmas and parameter choices are used to analyze the gradients and derive upper bounds. The text discusses upper bounds on gradients and Hessian Lipschitz properties in the context of a 1-smooth function with L > 1. Lemmas and parameter choices are used to analyze the gradients and derive upper bounds. The text then provides proofs and lower bounds for certain expressions based on the established conditions and results from previous Lemmas. The text provides proofs and lower bounds for expressions based on established conditions and results from previous Lemmas, focusing on bounding E t0 [2\u03b7 q v,t\u22121 , q m,t\u22121 ] using Lemma 13. The text presents proofs and lower bounds for expressions based on established conditions and results from previous Lemmas, focusing on bounding E t0 [2\u03b7 q v,t\u22121 , q m,t\u22121 ] using Lemma 13. It discusses the properties of a symmetric positive semidefinite matrix B and its relation to the matrix product U. Additionally, it explores lower bounding 2\u03b7E t0 [ q v,t\u22121 , q w,t\u22121 ] with Lemma 14, under the condition that SGD with momentum has the APCG property. The text discusses proving lower bounds and conditions related to the APCG property in SGD with momentum. It focuses on bounding E t0 [ q v,t\u22121 , q w,t\u22121 ] using Lemma 14 and showing contradictions to establish that the function value must decrease. The text discusses proving lower bounds and conditions related to the APCG property in SGD with momentum. It focuses on bounding E t0 [ q v,t\u22121 , q w,t\u22121 ] using Lemma 14 and showing contradictions to establish that the function value must decrease. The strategy involves leveraging negative curvature and showing that the lower bound is larger than the upper bound, leading to a contradiction. The text discusses proving lower bounds and conditions related to the APCG property in SGD with momentum. It focuses on bounding E t0 [ q v,t\u22121 , q w,t\u22121 ] using Lemma 14 and showing contradictions to establish that the function value must decrease. The strategy involves leveraging negative curvature and showing that the lower bound is larger than the upper bound, leading to a contradiction. The proof involves using inequalities and choosing appropriate values for T thred to guarantee certain conditions. The text discusses proving lower bounds and conditions related to the APCG property in SGD with momentum. It focuses on bounding E t0 [ q v,t\u22121 , q w,t\u22121 ] using Lemma 14 and showing contradictions to establish that the function value must decrease. The strategy involves leveraging negative curvature and showing that the lower bound is larger than the upper bound, leading to a contradiction. The proof involves using inequalities and choosing appropriate values for T thred to guarantee certain conditions. Define the event Set T = 2T thred f (w 0 ) \u2212 min w f (w) /(\u03b4\u2206). We return w uniformly randomly from w 0 , w T thred , w 2T thred , . . . , w kT thred , . . . , w KT thred , where K := T /T thred. Then, with probability at least 1 \u2212 \u03b4, we will have chosen a w k where \u03a5 k did not occur. If SGD with momentum has certain properties, it reaches a second order stationary point in T = O((1 \u2212 \u03b2) log( Lcm\u03c3 2 \u03c1c c h (1\u2212\u03b2)\u03b4\u03b3 ) \u221210 ) iterations with high probability 1 \u2212 \u03b4. The proof of Theorem 1 involves replicating Lemma 15 to show that uniformly sampling a w from a specific set leads to a second order stationary point with high probability. By choosing appropriate values for T thred, conditions can be satisfied to guarantee this outcome. With certain properties, SGD with momentum can reach a second order stationary point in a specified number of iterations with high probability. The proof of Theorem 1 involves replicating Lemma 15 to show that uniformly sampling a w from a specific set leads to a second order stationary point with high probability. By choosing appropriate values for T thred, conditions can be satisfied to guarantee this outcome. With certain properties, SGD with momentum can reach a second order stationary point in a specified number of iterations with high probability. In Appendix F, w is randomly chosen from a set of values, and conditions need to be met for the proof to be completed using Lemma 15. The proof of Theorem 1 involves replicating Lemma 15 to show that sampling a w from a specific set leads to a second order stationary point with high probability. By choosing appropriate values for T thred, conditions can be satisfied to guarantee this outcome. SGD with momentum can reach a second order stationary point in a specified number of iterations with high probability. By applying Lemma 15 and selecting parameters from Table 3, Algorithm 2 can find a second order stationary point efficiently. The algorithm outperforms previous methods by utilizing higher momentum for faster convergence."
}