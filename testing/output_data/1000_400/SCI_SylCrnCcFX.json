{
    "title": "SylCrnCcFX",
    "content": "Deep networks aim to understand complex mappings through locally linear behavior. The challenge lies in the instability of derivatives, especially in networks with piecewise linear activation functions. A new learning problem is proposed to promote stable derivatives over larger regions. The algorithm involves identifying stable linear approximation regions and expanding them. This approach is demonstrated on image and sequence datasets using residual and recurrent networks. Our algorithm focuses on stable linear approximation regions and their expansion, with a novel relaxation to scale it to realistic models. It is demonstrated on image and sequence datasets using residual and recurrent networks, emphasizing the importance of derivatives in understanding complex mappings. The derivatives discussed in this paper are with respect to input coordinates rather than parameters. Deep learning models are typically over-parametrized, leading to unstable functions and derivatives. Due to unstable derivatives, first-order approximations lack robustness. Deep learning models are often over-parametrized, resulting in unstable functions and derivatives. Unstable derivatives lead to lack of robustness in first-order approximations. Gradient stability is different from adversarial examples, with stable gradients being important for robustness. Techniques to protect against adversarial examples focus on stable function values rather than stable gradients. In this paper, the focus is on deep networks with piecewise linear activations to ensure gradient stability. The special structure of these networks allows for inferring lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable. The investigation specifically looks at the case of p = 2 due to the lower bound implications. In this paper, deep networks with piecewise linear activations are studied to ensure gradient stability. Lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable are inferred. The focus is on the case of p = 2, leading to a regularization problem formulation. The learning problem is relaxed similar to support vector machines, requiring evaluation of neuron gradients with respect to inputs. A novel perturbation algorithm is proposed for collecting exact gradients in piecewise linear networks. In this study, a novel perturbation algorithm is proposed for collecting exact gradients in piecewise linear networks resembling support vector machines. The algorithm addresses the computational challenge of evaluating neuron gradients with respect to inputs. Empirical examination of the algorithm is conducted on various network architectures with image and time-series datasets. Key contributions include inference algorithms for identifying input regions of neural networks. The study proposes a novel perturbation algorithm for collecting exact gradients in piecewise linear networks. It addresses the computational challenge of evaluating neuron gradients with respect to inputs and examines the algorithm on various network architectures with image and time-series datasets. Key contributions include inference algorithms for identifying input regions of neural networks with piecewise linear activation functions. The paper focuses on neural networks with piecewise linear activation functions, such as ReLU, and proposes a mixed integer linear representation for these networks. Various network architectures like FC, CNN, RNN, and ResNet are considered as plausible candidates. The approach encodes the active linear piece of the activation function for each neuron. The proposed approach involves a mixed integer linear representation for piecewise linear networks with activation patterns. Each neuron's active linear piece is encoded, leading to a linear model when the activation pattern is fixed. The feasible set in the input space ensures stable derivatives, with neighboring regions possibly sharing the same linear coefficients. This set induced by an activation pattern is called a linear region, forming a connected subset of the input space with consistent network derivatives. The feasible set induced by an activation pattern forms a linear region in the input space with stable derivatives. This complete linear region is a connected subset with consistent network derivatives, studied in various contexts like visualizing neurons, reachability of output values, and adversarial attacks or defense. In contrast to quantifying the number of linear regions as a measure of complexity, the focus here is on local linear regions and expanding them via learning. The notion of stability considered differs from adversarial examples, with different methods used for finding exact adversarial examples. The methods for finding exact adversarial examples differ from the notion of stability considered. Adversarial example computation is NP-complete and not scalable, while our algorithm certifies a 2 margin around a point by forwarding O(D) samples in parallel. Our unbiased estimation scales to high-dimensional images like ResNet on ImageNet. The proposed learning algorithm certifies a 2 margin around a point by forwarding O(D) samples in parallel, with unbiased estimation that scales to high-dimensional images like ResNet on ImageNet. It maximizes the 2 margin of linear regions around each data point in an unsupervised manner, akin to transductive/semi-supervised SVM. Our approach maximizes the 2 margin of linear regions around each data point in an unsupervised manner, similar to transductive/semi-supervised SVM. It involves a smooth relaxation of the margin and novel perturbation algorithms for gradient stability, addressing interpretability and transparency of complex models. The gradient is crucial for various explanation methods in deep models. Algorithms are developed for gradient stability in realistic networks, with implications for interpretability of complex models. The focus is on establishing robust derivatives, using FC networks with ReLU activations. The approach includes inference and learning algorithms. In the context of gradient stability in realistic networks, algorithms are developed to establish robust derivatives using FC networks with ReLU activations. The approach includes inference and learning algorithms for neural networks with hidden layers and neurons. In the context of gradient stability in realistic networks, algorithms are developed for robust derivatives using FC networks with ReLU activations. The neural network's output is a linear transformation of the last hidden layer, which can be further processed by a nonlinearity like softmax for classification. The focus is on the piecewise linear property of neural networks represented by f \u03b8 (x), utilizing a generic loss function L(f \u03b8 (x), y) to incorporate nonlinear mechanisms. The activation pattern used in this paper is defined as a set of indicators for neurons that specify functional constraints. Points on the boundary of a linear region can be feasible for multiple activation patterns. An activation pattern is a set of indicators for neurons that specify functional constraints. Points on the boundary of a linear region can be feasible for multiple activation patterns. The feasible set of the activation pattern is equivalent to a convex polyhedron in the input space. The feasible set S(x) is a convex polyhedron defined by linear constraints in the input space R D. The p margin of x, denoted as \u02c6 x,p, is a lower bound subject to its activation pattern. Directional feasibility is checked by exploiting the convexity of S(x) for a directional perturbation. The feasibility of a directional perturbation in a convex polyhedron can be checked by verifying if x + \u00af\u2206x \u2208 S(x). This can be computed by checking if x + \u00af\u2206x satisfies the activation pattern O in S(x). The method can be applied to 1-ball feasibility, where the number of extreme points is linear to dimension D. Given a point x, a feasible set S(x), and a 1-ball B, Proposition 5 can be generalized for an \u221e-ball. However, in high dimension D, the number of extreme points of an \u221e-ball is exponential to D, making it intractable. Instead, the number of extreme points of a 1-ball is only linear to D. Feasibility on a 1-ball is tractable due to convexity of S(x) and can be efficiently certified by binary search. Proposition 6 discusses the minimum 2 distance between x and the union of hyperplanes. Proposition 6 discusses the minimum 2 distance between a point x and the union of hyperplanes, which can be efficiently certified analytically by exploiting the polyhedron structure of S(x). The sizes of linear regions and the number of linear regions in f \u03b8 are related, especially in a bounded input space, but counting them is intractable due to the combinatorial nature of activation patterns. The text discusses certifying the number of complete linear regions in a neural network among data points, proposing a method to efficiently compute this number. It highlights the importance of considering the structure of the data manifold and introduces the concept of complete linear region certificates. The text discusses certifying the number of complete linear regions in a neural network among data points, proposing a method to efficiently compute this number. It focuses on maximizing the 2 margin\u02c6 x,2 through a regularization problem, but faces challenges due to a rigid loss surface. To address this, a hinge-based relaxation to the distance function is proposed. The text discusses a regularization problem to maximize the margin in a neural network. A hinge-based relaxation to the distance function is proposed to address the challenges of a rigid loss surface. The text discusses a regularization problem to maximize the margin in a neural network by deriving a relaxation that solves a smoother problem. This involves relaxing the squared root and reciprocal on the 2 norm, using a hinge loss, and considering a hyper-parameter C. The proposed methods are visualized using a toy 2D binary classification dataset and training a 4-layer fully connected network with different loss functions. The text discusses maximizing the margin in a linear model scenario by using distance regularization and relaxed regularization. A toy 2D binary classification dataset is used to train a 4-layer fully connected network with different loss functions. The distance regularization enlarges linear regions around training points, while the relaxed regularization generalizes the property to the whole space, resulting in a smoother prediction boundary. The text discusses maximizing the margin in a linear model scenario using distance regularization and relaxed regularization. The relaxed regularization creates a smoother prediction boundary by considering a set of neurons with high losses, leading to the final objective of learning Robust Local Linearity (ROLL). The text introduces a generalized loss function for learning Robust Local Linearity (ROLL) by considering a set of neurons with high losses. This approach aims to stabilize training, simplify computation, and induce a strong synergy effect between gradient norms. The special case when \u03b3 = 100 eliminates the nonlinear sorting step, making the training process more efficient. The text discusses a parallel algorithm for learning Robust Local Linearity (ROLL) without back-propagation, utilizing the functional structure of neural networks. By constructing a linear network g \u03b8 based on the same parameters as f \u03b8, the algorithm computes derivatives efficiently by forwarding two samples. The proposed approach constructs a linear network g \u03b8 identical to f \u03b8 in S(x) using fixed linear activation functions. Derivatives of neurons can be computed efficiently by forwarding two samples. The complexity analysis assumes no overhead for parallel computation and batch matrix multiplication. The perturbation algorithm for computing gradients takes 2M operations, while back-propagation takes DISPLAYFORM0. Despite parallelizable computation, computing loss for large networks remains challenging. The perturbation algorithm for computing gradients takes 2M operations, while back-propagation takes DISPLAYFORM0. Despite parallelizable computation, computing loss for large networks remains challenging due to memory constraints. An unbiased estimator of the ROLL loss in Eq. FORMULA17 is proposed when \u00ce(x, \u03b3) = I, with efficient computation of (i,j)\u2208I C max(0, 1 \u2212 |z i j |) in one forward pass. The sum of gradient norms can be efficiently computed using a decoupling method, which is generally storable within GPU memory. The proposed algorithm efficiently computes the sum of gradient norms using a decoupling method, allowing for unbiased approximation with a reduced memory requirement. It can be applied to deep learning models with affine transformations and piecewise linear activation functions, but does not generalize to nonlinearity like maxout/max-pooling. The proposed algorithm efficiently computes the sum of gradient norms using a decoupling method for deep learning models with affine transformations and piecewise linear activation functions. It does not generalize to nonlinearity like maxout/max-pooling. In comparison ('ROLL') with a baseline model ('vanilla'), experiments show performance on a testing set using a single GPU with 12G memory. In this section, we compare our approach ('ROLL') with a baseline model ('vanilla') in various scenarios on a testing set. Experiments are conducted on a single GPU with 12G memory, evaluating accuracy, number of complete linear regions, and margins of linear regions. The experiments use a 55,000/5,000/10,000 split of the MNIST dataset for training/validation/testing on a 4-layer FC model with ReLU activations. Parameter analysis was conducted on the MNIST dataset using a 4-layer FC model with ReLU activations. The models with the largest median\u02c6 x,2 among validation data were reported, with tuned models having specific parameters. The ROLL loss achieved significantly larger margins for most percentiles compared to the vanilla loss, with a tradeoff of 1% accuracy resulting in even larger margins. The tuned models have specific parameters and satisfy the condition for certifying #CLR with tight bounds. ROLL loss achieves significantly larger margins compared to the vanilla loss, with a tradeoff of 1% accuracy resulting in even larger margins. The Spearman's rank correlation between\u02c6 x,1 and\u02c6 x,2 is high for all cases. The lower #CLR in our approach reflects the presence of larger linear regions. Points within the same linear region in the ROLL model with 98% accuracy have the same label. A parameter analysis was done to analyze the ACC and P 50 of\u02c6 x,2 under different parameters. The ROLL model shows distinct linear regions with different labels for points, while a parameter analysis in Figure 2 reveals the impact of C, \u03bb, and \u03b3 on accuracy. Higher values of C and \u03bb lead to decreased accuracy with a larger margin, while higher \u03b3 values indicate less sensitivity to hyper-parameters. Efficiency is validated by measuring running time for gradient descent steps, comparing vanilla loss, full ROLL loss, and approximate ROLL loss. The efficiency of the proposed method is validated by measuring the running time for performing gradient descent steps. A comparison is made between the vanilla loss, full ROLL loss, and approximate ROLL loss. The approximate loss is about 9 times faster than the full loss, with the perturbation algorithm achieving about 12 times empirical speed-up compared to back-propagation. Overall, the computational overhead of the method is minimal. Our approach is only twice slower than the vanilla loss, with the approximate loss being about 9 times faster than the full loss. The perturbation algorithm achieves about 12 times empirical speed-up compared to back-propagation. The computational overhead of our method is minimal, achieved by the perturbation algorithm and the approximate loss. We implement RNNs for speaker identification on a Japanese Vowel dataset with variable sequence length and channels. The network uses the scaled Cayley orthogonal RNN with LeakyReLU activation. Results show our approach outperforms the vanilla loss with larger margins on testing data. Sensitivity analysis reveals a high Spearman's rank correlation. Our approach for speaker identification on a Japanese Vowel dataset outperforms the vanilla loss with larger margins on testing data. Sensitivity analysis shows a high Spearman's rank correlation and stability bounds for derivatives. Experiments on Caltech-256 BID18 dataset with a 18-layer ResNet model are conducted. The ROLL model with 98% accuracy outperforms the vanilla model on the Caltech-256 BID18 dataset. The stability bound of the ROLL regularization is consistently larger. The 18-layer ResNet model is used with downsized images and pre-trained parameters from ImageNet. Evaluation measures are challenging due to high input dimensionality, so a sample-based approach is used to evaluate gradient stability. The implementation details are in Appendix I. Due to high input dimensionality, a sample-based approach is used to evaluate the stability of gradients for the ground-truth label in local regions. The evaluation measures the stability of gradient distortion in terms of expected and maximum distortion within a defined intersection. The stability of gradient distortion is evaluated using labeled data, measuring expected and maximum distortion within an intersection. The adversarial gradient is found by maximizing distortion over a norm ball, requiring optimization due to the involvement of the Hessian. The stability of gradient distortion is evaluated using labeled data, measuring expected and maximum distortion within an intersection. The adversarial gradient is found by maximizing distortion over a norm ball, requiring optimization due to the involvement of the Hessian. The ROLL loss yields more stable gradients than the vanilla loss with marginally superior precisions. The ROLL loss provides stable gradients with slightly better precision compared to the vanilla loss. Only a small number of images change prediction labels in both models. The ROLL loss yields stable shapes and intensities of gradients, while the vanilla loss does not. The paper introduces a new learning problem to create locally transparent neural networks. The paper introduces a new learning problem to create locally transparent neural networks with stable gradients using the ROLL loss. It focuses on constructing piecewise linear networks based on a margin principle similar to SVM, expanding regions with stable derivatives and generalizing the stable gradient property across linear regions. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. It shows that if x satisfies all constraints before layer i > 1 and follows fixed activation indicators, then it can be rewritten for each j \u2208 [N i]. The proof follows by induction, demonstrating directional feasibility and 1-ball feasibility. The proof by induction shows directional feasibility, 1-ball feasibility, and 2-ball certificate for convex sets and linear constraints. Proposition 6 states that the 2-ball Certificate involves finding the minimum 2 distance between a point x and a set of hyperplanes. The proof involves constructing a neural network feasible model with the same loss as the optimal model. The text discusses constructing a neural network model with a linear activation function to compute gradients efficiently. The model, denoted as g \u03b8, has the same weights and biases as the original model f \u03b8. The activation function o i j = max(0, o i j ) is fixed, allowing for linearity in the network. The process involves collecting partial derivatives with respect to an input axis k by feeding zero and unit vectors to g \u03b8. The text discusses efficiently computing gradients in a neural network model with a fixed linear activation function. By feeding zero and unit vectors to the model, partial derivatives with respect to an input axis k can be collected. The process allows for the computation of derivatives for all neurons with just 2 forward passes. The proposed approach efficiently computes gradients in a neural network model with a fixed linear activation function. By using a perturbation algorithm, gradients for all neurons can be computed with 2 forward passes, taking a total of 2M operations. In contrast, back-propagation requires sequential computation for each neuron, taking 2i operations per neuron. The proposed approach efficiently computes gradients in a neural network model with a fixed linear activation function using perturbations. Forward passes with perturbations are used to obtain gradients, taking a total of 2M operations. In contrast, back-propagation requires sequential computation for each neuron, taking 2i operations per neuron. The chain-rule of Jacobian is exploited for dynamic programming to compute all gradients efficiently. The dynamic programming approach efficiently computes gradients in neural networks using the chain rule of Jacobian. It iterates through layers with the Jacobian of previous layers and is efficient for fully connected networks but not for convolutional layers. An introductory guide is provided for deriving methods for maxout/max-pooling nonlinearity. For convolutional layers, deriving inference and learning methods for maxout/max-pooling nonlinearity is feasible but not recommended due to new linear constraints. It is suggested to use convolution with large strides or average-pooling instead. The target network is assumed to have a single nonlinearity mapping N neurons to 1 output. Activation patterns determine which input is selected, leading to a degeneration to a linear model once fixed. The target network with a single nonlinearity maps N neurons to 1 output. Activation patterns determine input selection, leading to a degeneration to a linear model. Feasible activation patterns induce stable derivatives in the input space. The feasible set is a convex polyhedron with linear constraints. The feasible set of activation patterns in a neural network can be represented as a convex polyhedron with linear constraints. In a fully connected model with 4 hidden layers of 100 neurons each, training is done using sigmoid cross entropy loss function and Adam optimizer for 5000 epochs. The model is selected based on training loss, with a fixed parameter C and varying \u03bb for distance regularization. The model consists of 4 fully-connected hidden layers with 100 neurons each. Input dimension is 2, output dimension is 1. Sigmoid cross entropy loss function is used with Adam optimizer for 5000 epochs. The model is selected based on training loss. Regularization parameters are tuned with \u03bb = 1. Data normalization is done with \u00b5 = 0.1307 and \u03c3 = 0.3081. Margin calculations are reported in the table. Exact ROLL loss is computed during training. The FC model consists of 4 hidden layers with 300 neurons each, ReLU activation function, and cross-entropy loss with soft-max. Training involves 20 epochs using stochastic gradient descent with Nesterov momentum. Tuning includes grid search on \u03bb, C, \u03b3 parameters. The scaled margin \u03c3\u02c6 x,p is reported in the table, reflecting the original data space. The curr_chunk discusses the training process using stochastic gradient descent with Nesterov momentum, grid search on \u03bb, C, \u03b3 parameters, and the use of a single layer scoRNN for representation learning. The hidden neurons in scoRNN are set to 512 with LeakyReLU activation functions. The loss function used is cross-entropy. The representation is learned with a single layer scoRNN using LeakyReLU activation functions and 512 hidden neurons. The loss function is cross-entropy with soft-max, and AMSGrad optimizer is used with a learning rate of 0.001 and batch size of 32 sequences. Grid search is done on \u03bb, C, \u03b3 parameters, and models with similar testing accuracy to the baseline model are reported. Training is done on normalized images with a bijective mapping between normalized distance and original space. The pre-trained ResNet-18 model is modified by replacing max-pooling with average-pooling after the first convolutional layer and enlarging the receptive field of the last pooling layer. This modification aims to reduce linear constraints and increase the output dimension to 512. The models with similar testing accuracy to the baseline model are reported after grid search on parameters \u03bb, C, and \u03b3. A bijective mapping between normalized distance and original space is used during training on normalized images. The ResNet-18 model is adjusted by replacing max-pooling with average-pooling and increasing the receptive field of the last pooling layer to 512 dimensions. The model is trained using stochastic gradient descent with Nesterov momentum for 20 epochs, with a batch size of 32. The learning rate is initially set at 0.005 and adjusted to 0.0005 after 10 epochs. The best model based on validation loss is selected after tuning with fixed parameters. The ResNet-18 model is trained using stochastic gradient descent with Nesterov momentum for 20 epochs. The initial learning rate is 0.005, adjusted to 0.0005 after 10 epochs, with a momentum of 0.5 and a batch size of 32. Tuning involves fixing parameters, using a limited number of samples, and adjusting \u03bb and C values. A genetic algorithm (GA) BID33 with 4800 populations P and 30 epochs is implemented for further training. The genetic algorithm (GA) BID33 with 4800 populations P and 30 epochs is used for further training. Initially, 4800 samples are uniformly sampled in the domain for P. In each epoch, samples are evaluated based on their gradient distance from the target x, sorted, and the top 25% kept in the population. The remaining 75% samples are replaced with a random linear combination of pairs from P. Projection is done to ensure feasibility, and the sample in P with the maximum distance is selected. The genetic algorithm (GA) BID33 with 4800 populations P and 30 epochs is used for further training. Initially, 4800 samples are uniformly sampled in the domain for P. In each epoch, samples are evaluated based on their gradient distance from the target x, sorted, and the top 25% kept in the population. The remaining 75% samples are replaced with a random linear combination of pairs from P. Projection is done to ensure feasibility, and the sample in P with the maximum distance is selected. The GA algorithm does not include mutation due to computational reasons. The crossover operator in GA is analogous to a gradient step where the direction is determined by other samples and the step size is determined randomly. Visualizations include the original image, original gradient, adversarial gradient, image of adversarial gradient, and original integrated gradient. The visualization process includes the original image, original gradient, adversarial gradient, image of adversarial gradient, original integrated gradient, and adversarial integrated gradient. The gradients and integrated gradients are visualized by aggregating derivatives, taking absolute values, normalizing, and clipping values. The visualization process involves aggregating derivatives, taking absolute values, normalizing, and clipping values to visualize gradients and integrated gradients. The integrated gradient is visualized as a gray-scaled image, highlighting differences in settings with visually indistinguishable inputs. Examples from the Caltech-256 dataset are visualized based on different percentiles of maximum gradient distortions. The integrated gradient is visualized to highlight differences in settings with visually indistinguishable inputs. Examples from the Caltech-256 dataset are visualized based on different percentiles of maximum gradient distortions. Table 4 shows percentiles computed by interpolation, while Figures 5 and 6 display examples from the Caltech-256 dataset with maximum gradient distortions. The vanilla model has specific distortions for different images, while the ROLL model shows different distortions as well. The ROLL model exhibits maximum 1 gradient distortions of 1367.9 for 'Bear' and 3882.8 for 'Rainbow', while the vanilla model shows distortions of 1547.1 for 'Bear' and 5473.5 for 'Rainbow'."
}