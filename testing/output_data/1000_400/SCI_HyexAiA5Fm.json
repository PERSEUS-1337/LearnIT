{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are neural generative models successful in modeling high-dimensional continuous measures. A scalable method for unbalanced optimal transport (OT) is presented using the generative-adversarial framework. The approach involves learning a transport map and a scaling factor to push a source measure to a target measure in a cost-optimal manner. The formulation is theoretically justified and an algorithm based on stochastic alternating gradient updates is proposed. Numerical experiments show the methodology's application to population modeling. The algorithm proposed by Liero et al. (2018) is based on stochastic alternating gradient updates, similar to GANs, for solving unbalanced optimal transport problems in population modeling. This involves finding a cost-optimal way to transform one measure to another by considering mass variation and transport. The methodology addresses the need to model mass transport and local mass variations to account for evolving features and sub-populations in the target population. The text discusses the importance of modeling mass transport and local mass variations in population modeling using optimal transport methods. Classical optimal transport focuses on pushing a source to a target distribution without allowing for mass variations, while modern approaches use the Kantorovich formulation to find optimal probabilistic couplings between measures. Regularizing the objective with an entropy term can improve efficiency in solving the dual problem using the Sinkhorn algorithm. Stochastic methods based on the dual objective have been proposed for continuous settings. Optimal transport has applications in various fields such as computer graphics and domain adaptation. Optimal transport methods, including the use of entropy regularization, have been proposed for efficient solving of the dual problem in mass transport modeling. Applications of optimal transport include computer graphics, domain adaptation, image translation, natural language translation, and biological data integration. Transport maps can also be learned using generative models like GANs to push a source distribution to a target distribution. Optimal transport methods, including entropy regularization, are used for solving mass transport modeling efficiently. Applications include computer graphics, domain adaptation, image translation, natural language translation, and biological data integration. Generative models like GANs are employed to learn transport maps for pushing a source distribution to a target distribution. Various transport problems in different fields have been addressed using GAN variants with strategies like conditioning and cycle-consistency to ensure correspondence between original and transported samples. Optimal transport methods, including entropy regularization, are used for efficient mass transport modeling. GAN variants with conditioning and cycle-consistency strategies are employed to learn transport maps for pushing a source distribution to a target distribution. Extensions of optimal transport theory to handle unbalanced masses have been proposed, with scaling algorithms developed for approximating solutions to optimal entropy-transport problems. Innovative framework developed for unbalanced optimal transport using GANs for high-dimensional problems in various applications like computer graphics, tumor growth modeling, and computational biology. The framework developed for unbalanced optimal transport directly models mass variation and stochastic transport maps for cost-optimal transportation between source and target measures. It generalizes the unbalanced Monge OT problem and offers scalable methodology for solving the relaxed optimization problem. The goal is to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. This generalizes the unbalanced Monge OT problem and offers scalable methodology for solving the relaxed optimization problem. The methodology is demonstrated in practice using various datasets, including handwritten digits and single-cell RNA-seq data from zebrafish embryogenesis. Additionally, a new scalable method is proposed for solving the optimal-entropy transport problem. The text discusses the application of a new scalable method for solving the optimal-entropy transport problem in population modeling using various datasets, including handwritten digits and single-cell RNA-seq data from zebrafish embryogenesis. The method extends previous work on unbalanced optimal transport and provides an alternative for large or continuous datasets. The text introduces the Borel \u03c3-algebra and spaces of probability measures over X. It discusses optimal transport problems, including the Monge and Kantorovich formulations, addressing the transportation of measures in a cost-optimal manner. The Monge problem involves deterministic transport maps, while the Kantorovich problem is a convex relaxation of the Monge problem. The Monge problem involves deterministic transport maps, while the Kantorovich OT problem is a convex relaxation that formulates OT as a search over probabilistic transport plans. The relaxed problem is a linear program that can be solved efficiently using the Sinkhorn algorithm with entropic regularization. The relaxed Monge problem can be solved efficiently using the Sinkhorn algorithm with entropic regularization. Stochastic algorithms have been proposed for computing transport plans that handle continuous measures, extending classical optimal transport to handle mass variation. Existing numerical methods are based on optimal-entropy transport formulation, which minimizes a cost function to find a measure that relaxes marginal constraints. State-of-the-art discrete algorithms like iterative scaling algorithms are used for computing regularized optimal transport plans. However, practical algorithms for unbalanced optimal transport between continuous measures in high-dimensional spaces are lacking. In the discrete setting, iterative scaling algorithms like BID8 are used for computing regularized optimal transport plans. However, practical algorithms for unbalanced optimal transport between continuous measures in high-dimensional spaces are currently lacking. The proposed algorithm for unbalanced optimal transport directly models mass variation and can be applied to transport between high-dimensional continuous measures. The unbalanced optimal transport problem aims to find a stochastic transport map and scaling factor to move a source measure to a target measure in a cost-optimal way. It involves penalizing the cost of mass transport and variation, with a constraint to ensure exact pushing of the source to the target. The approach considers stochastic maps for a more practical model compared to deterministic ones. The unbalanced optimal transport problem involves finding a stochastic transport map and scaling factor to move a source measure to a target measure in a cost-optimal way. It considers penalizing the cost of mass transport and variation, with a constraint to ensure exact pushing of the source to the target. Stochastic maps are preferred over deterministic ones for practical modeling purposes. The unbalanced optimal transport problem involves finding a stochastic transport map and scaling factor to move a source measure to a target measure in a cost-optimal way. Different models of transformation are optimal depending on the relative costs of mass transport and variation. The unbalanced optimal transport problem involves finding a stochastic transport map and scaling factor to move points from a source measure to a target measure in a cost-optimal way. An unbalanced transport map with a scaling factor can help balance class imbalances by downweighing or upweighing samples in the source distribution. The relaxation of the optimization problem involves using a divergence penalty instead of an equality constraint, leading to a Monge-like version of the optimal-entropy transport problem. This formulation specifies a joint measure \u03b3 in the search space of joint measures, allowing for a broader range of solutions. The optimal-entropy transport problem involves formulating an objective function using \u03b3 instead of (T, \u03be). The search space for \u03b3 is broader than for (T, \u03be), as not all joint measures \u03b3 can be specified by (T, \u03be). The support of \u03b3 is restricted to supp(\u00b5) \u00d7 Y in the asymmetric Monge formulation, where mass transported to Y must come from within the support of \u00b5. Equivalence is established by restricting \u03b3 to supp(\u00b5) \u00d7 Y. The support of \u03b3 is restricted to supp(\u00b5) \u00d7 Y in the optimal-entropy transport problem. Equivalence can be established by restricting \u03b3 to supp(\u00b5) \u00d7 Y, and solutions of the relaxed problem converge to solutions of the original problem for an appropriate choice of divergence penalty. Solutions of the relaxed problem converge to solutions of the original problem with an appropriate divergence penalty. The transport map and scaling factor can be learned using stochastic gradient methods with the relaxation of unbalanced Monge OT. The transport map and scaling factor can be learned using stochastic gradient methods with the relaxation of unbalanced Monge OT, optimizing with alternating stochastic gradient updates after parameterizing T, \u03be, and f with neural networks. This optimization procedure is similar to GAN training and involves an adversarial game between (T, \u03be) and f. The optimization procedure involves an adversarial game between (T, \u03be) and f, where T transports points from X to Y, \u03be determines importance weights, and f measures divergence. Cost functions encourage finding a cost-efficient strategy. Parameters are updated using gradient descent. Examples of divergences with entropy functions can be plugged into the optimization process. The optimization procedure involves an adversarial game between (T, \u03be) and f, where T transports points from X to Y, \u03be determines importance weights, and f measures divergence. Parameters are updated using gradient descent. Examples of divergences with entropy functions can be plugged into the optimization process. The probabilistic Monge-like formulation is similar to the Kantorovich-like entropy-transport problem in theory but results in different numerical methods in practice. Algorithm 1 solves the non-convex formulation and learns a transport map T and scaling factor \u03be parameterized by neural networks for scalable optimization using stochastic gradient descent. Algorithm 1 solves the non-convex formulation by learning a transport map T and scaling factor \u03be using neural networks for scalable optimization. The neural architectures imbue their function classes with a specific structure, enabling effective learning in high-dimensional settings. However, it is not guaranteed to find the global optimum. In contrast, the scaling algorithm of BID8 based on a convex optimization problem is proven to converge but is currently only practical for discrete problems. The new stochastic method proposed in Section A of the Appendix generalizes the approach of BID8 for handling transport between continuous measures and overcomes scalability limitations. However, the output is in the form of the dual solution, which is less interpretable for practical applications compared to Algorithm 1. The new stochastic method in the Appendix generalizes BID8 for transport between continuous measures, overcoming scalability limitations. However, the output in the form of the dual solution is less interpretable for practical applications compared to Algorithm 1. The goal is to learn a scaling factor that \"balances\" measures \u00b5 and \u03bd in causal inference. In Section 4, Algorithm 1 is used to directly learn a transport map and scaling factor. The goal is to balance measures \u00b5 and \u03bd in causal inference by scaling the importance of different members from a control population based on their likelihood to be present in a treated population. This helps eliminate selection biases in treatment effect inference. The method illustrated in this section focuses on unbalanced optimal transport with applications in population modeling, specifically on MNIST data. Algorithm 1 is used for unbalanced optimal transport in population modeling, specifically on MNIST data. It aims to eliminate selection biases in treatment effect inference by scaling the importance of different members from a control population based on their likelihood to be present in a treated population. Algorithm 1 was evaluated on transporting source distribution to target distribution with a high cost of transport. The scaling factor learned reflects class imbalances, modeling growth or decline of different classes in a population. This validates Algorithm 1's ability to reweight based on population drift. The scaling factor learned by Algorithm 1 reflects class imbalances and can model growth or decline of different classes in a population. Unbalanced OT from the MNIST dataset to the USPS dataset is applied to model the evolution of distributions, with transport cost as the Euclidean distance between images. The reweighting process during unbalanced OT is illustrated in FIG1. The evolution of the MNIST distribution to the USPS distribution is modeled using Algorithm 1, with transport cost as the Euclidean distance between images. The unbalanced transport is visualized in FIG1, showing how MNIST digits preserve their likeness during the transport. The scaling factor of the original MNIST images determines their prominence in the USPS dataset according to the unbalanced OT model. The study compared MNIST and USPS datasets using unbalanced OT model. MNIST digits preserved their likeness during transport, with brighter digits having higher scaling factors. The analysis extended to CelebA dataset, modeling the transformation from young to aged faces. The study applied Algorithm 1 on the CelebA dataset to perform unbalanced OT from young to aged faces using a variational autoencoder. The unbalanced transport is visualized in FIG2. The study applied Algorithm 1 on the CelebA dataset to perform unbalanced optimal transport from young to aged faces using a variational autoencoder. The transported faces retain key features of the original faces, with exceptions like gender swaps. Young faces with higher scaling factors were significantly enriched for males. The study applied Algorithm 1 on the CelebA dataset to perform unbalanced optimal transport from young to aged faces using a variational autoencoder. Young faces with higher scaling factors were significantly enriched for males, predicting a growth in the prominence of male faces compared to female faces as the population evolves. The study demonstrated a strong gender imbalance between young and aged populations, with young population being predominantly female and aged population being predominantly male. Using Algorithm 1, the researchers applied unbalanced optimal transport to single-cell gene expression data from two stages of zebrafish embryogenesis, showcasing the relevance of learning the scaling factor. In a study on zebrafish embryogenesis, researchers applied Algorithm 1 to single-cell gene expression data from blastulation and gastrulation stages. By analyzing scaling factors, they identified genes associated with mesoderm development, showcasing the potential for biological discovery. The study on zebrafish embryogenesis used a ranked list of upregulated genes to identify genes linked to mesoderm development. A stochastic method for unbalanced OT based on the regularized dual formulation was presented, showing potential for biological discovery. The regularized dual formulation for an optimization problem involves adding a strongly convex regularization term to the primal objective, encouraging plans with high entropy. The dual of the regularized problem is expressed in terms of expectations, assuming access to samples from \u00b5, \u03bd, and normalized measures. The relationship between the primal optimizer \u03b3 * and dual optimizer (u * , v * ) is given by DISPLAYFORM5. Algorithm 2 describes the optimization process using neural networks u \u03b8 , v \u03c6 and stochastic gradient descent for unbalanced optimal transport. The algorithm generalizes classical OT to unbalanced OT, with the dual solution learned from Algorithm 2 used to reconstruct the primal solution \u03b3 * indicating mass transport between points in X and Y. The marginals of \u03b3 * do not necessarily match \u00b5 and \u03bd, allowing for implicit mass variation in the problem. The primal solution \u03b3 * is a transport map indicating mass transport between points in X and Y. The marginals of \u03b3 * do not necessarily match \u00b5 and \u03bd, allowing for implicit mass variation. A stochastic algorithm is proposed for learning a deterministic mapping from X to Y based on the dual solution. Algorithm 3 updates functions u, v using gradient descent. The equivalence between objectives (6) and (3) is shown by reformulating in terms of \u03b3. Lemma 3.3 formalizes the relation between the formulations. The text discusses the relationship between different formulations in terms of \u03b3 and the use of gradient descent in updating functions u and v in Algorithm 3. It also mentions the disintegration theorem and the existence of measurable functions in a family of probability measures. The disintegration theorem states that there exists a family of measurable functions that relate to probability measures. This leads to a relation involving a change of variables and the Radon-Nikodym derivative, ultimately proving a certain inequality. The Radon-Nikodym theorem establishes a relation between (T, \u03be) satisfying DISPLAYFORM10 and (12). By applying Fubini's Theorem for Markov kernels and change of variables, it is shown that W c1,c2,\u03c8 (\u00b5, \u03bd) \u2265 L \u03c8 (\u00b5, \u03bd), leading to the uniqueness of the joint measure \u03b3 in optimal entropy-transport. If c1, c2, \u03c8 are strictly convex and \u03c8 \u221e = \u221e, and c1 satisfies Corollary 3.6 BID27, then the joint measure \u03b3 specified by any minimizer of L\u03c8(\u00b5, \u03bd) is unique. The minimizers of L\u03c8(\u00b5, \u03bd) also determine the product measure uniquely. Based on Lemma 3.3 and the uniqueness of minimizers in BID27, it is shown that the product measure generated by minimizers of L\u03c8(\u00b5, \u03bd) is unique. For certain cost functions and divergences, L\u03c8 defines a proper metric between positive measures \u00b5 and \u03bd. This corresponds to the Hellinger-Kantorovich or Wasserstein-Fisher-Rao metric. By choosing an appropriate divergence penalty, theoretical analysis and constrained optimization show the uniqueness of the joint measure \u03b3 specified by minimizers of L\u03c8(\u00b5, \u03bd). Based on Lemma 3.3 and uniqueness of minimizers in BID27, the product measure generated by minimizers of L\u03c8(\u00b5, \u03bd) is unique. Theoretical analysis and constrained optimization show that solutions of the relaxed problem converge to solutions of the original problem. The convergence is proven through the pointwise convergence of \u03b6 k \u03c8(s) to the equality constraint \u03b9 = (s). This implies the uniqueness of the joint measure \u03b3 specified by minimizers of L\u03c8(\u00b5, \u03bd). Based on Lemma 3.3 and uniqueness of minimizers in BID27, the product measure generated by minimizers of L\u03c8(\u00b5, \u03bd) is unique. The convergence of solutions from the relaxed problem to the original problem is proven through the pointwise convergence of \u03b6 k \u03c8(s) to the equality constraint \u03b9 = (s). This implies the uniqueness of the joint measure \u03b3 specified by minimizers of L\u03c8(\u00b5, \u03bd). Additionally, the sequence of minimizers \u03b3 k is bounded and equally tight under certain assumptions, leading to the existence of a solution. The sequence of minimizers \u03b3 k is bounded and equally tight under certain assumptions, leading to the existence of a solution. By an extension of Prokhorov's theorem, a subsequence of \u03b3 k weakly converges to some \u03b3, which is a minimizer of the objective function. The convex conjugate form of \u03c8-divergence is presented to rewrite the main objective as a min-max problem. In this section, the convex conjugate form of \u03c8-divergence is used to rewrite the main objective as a min-max problem. Lemma B.2 states that for non-negative finite measures P, Q over T \u2282 R d, a certain equality holds under specific conditions. This result has been used in generative modeling and a rigorous proof can be found in a referenced source. The optimal f over the support of P \u22a5 is equal to \u03c8 \u221e, as shown in a simple proof. This result has been used in generative modeling and has specific conditions outlined in Lemma B.2. The cost functions c1 and c2 must satisfy Proposition B.1 for the problem to be well-posed. In practice, c1 can be a measurement of correspondence between X and Y, while c2 should be a convex function that vanishes at 1 and prevents \u03be from becoming too small or too large. Various entropy functions can be used for \u03c8-divergences, as shown in Table 1. The cost functions c1 and c2 must satisfy Proposition B.1 for the problem to be well-posed. In practice, c1 measures correspondence between X and Y, while c2 should be a convex function that prevents \u03be from becoming too small or too large. Different entropy functions can be used for \u03c8-divergences, as detailed in Table 1. Any \u03c8-divergence can be used to train generative models to match a generated distribution P to a true data distribution Q. Jensen's inequality shows that D \u03c8 (P |Q) is minimized when P = Q for probability measures P, Q. The entropy function \u03c8, D \u03c8 (P |Q) is uniquely minimized when P = Q for probability measures. However, when P, Q are not probability measures, additional constraints on \u03c8 are needed to ensure divergence minimization matches P to Q. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. The divergence is minimized when P = \u221e and Q = 0. Additional constraints on \u03c8 are required when P, Q are not probability measures to ensure divergence minimization matches P to Q. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. The divergence is minimized when P = \u221e and Q = 0. Additional constraints on \u03c8 are needed when P, Q are not probability measures to ensure divergence minimization matches P to Q. Examples of \u03c8 corresponding to common divergences for unbalanced OT are provided in Table 1. Choice of activation layers and neural architectures for parameterizing f using a neural network are discussed. Choice of f should belong to a class of functions that maps from Y to (\u2212\u221e, \u03c8 \u221e ]. This can be enforced by parameterizing f using a neural network with an output activation layer. Examples of activation layers and neural architectures used in experiments are provided."
}