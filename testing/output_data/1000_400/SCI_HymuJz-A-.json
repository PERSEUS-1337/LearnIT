{
    "title": "HymuJz-A-",
    "content": "The limitations of modern machine vision algorithms in learning visual relations are highlighted through controlled experiments. Convolutional neural networks struggle with visual-relation problems when intra-class variability exceeds their capacity, eventually breaking down. Relational networks also face similar limitations, despite their success in solving visual question answering problems. Feedback mechanisms like working memory and attention are proposed as key components for abstract visual reasoning, inspired by the success of biological vision. The limitations of modern machine vision algorithms in learning visual relations are highlighted through controlled experiments. Relational networks struggle with visual-relation problems when intra-class variability exceeds their capacity. Feedback mechanisms like working memory and attention are proposed as key components for abstract visual reasoning, inspired by the success of biological vision. A deep convolutional neural network successfully classifies complex images, surpassing human accuracy on the ImageNet classification challenge. The CNN, trained on millions of photographs, accurately categorizes images into natural object categories, surpassing human accuracy on the ImageNet challenge. However, it struggles to recognize simple visual relations, highlighting the limitations of modern machine vision algorithms. The CNN excels at categorizing images but struggles with recognizing simple visual relations, as shown in the example of two curves in panel (b) of Figure 1. This difficulty has been overlooked despite being known in the field. The difficulty of recognizing visual relations, such as the task shown in panel (b) of Figure 1, has been overlooked in computer vision algorithms, including CNNs and relational networks (RNs). RNs have only been tested on toy datasets, and they also struggle with the same limitations as CNNs for tasks like the one depicted in the image. The limitations of modern computer vision algorithms, like CNNs and RNs, are evident in their failure to recognize visual relations across different species. Despite the ability of animals to perceive visual relationships, existing models struggle with relational reasoning tasks. Previous studies have shown that black-box classifiers also struggle with visual reasoning problems, highlighting the need for a deeper understanding of computational principles in visual reasoning. There has been no systematic exploration of machine learning algorithms on relational reasoning problems. Previous studies have shown that black-box classifiers struggle with visual reasoning tasks, and CNNs fail to solve various visual-relation problems. The study explores the limitations of CNNs and other visual reasoning networks on visual-relation tasks, questioning whether the failure is due to hyperparameters or a systematic issue. Through experiments, it is shown that CNNs struggle with visual-relation tasks, and even specialized networks designed for this purpose do not alleviate the limitations. The study suggests that working memory and attention are key mechanisms in primates' visual reasoning abilities. The study demonstrates that CNNs struggle with visual-relation tasks, even specialized networks like RNs do not help. It suggests that working memory and attention are crucial for visual reasoning in primates. The study introduces a new visual-relation challenge and highlights the need for feedback mechanisms to improve computer vision models. The study introduces a new visual-relation challenge, the SVRT challenge, consisting of twenty-three binary classification problems. It reveals that CNNs solve same-different tasks through rote memorization and suggests rethinking existing visual question answering challenges by drawing inspiration from neuroscience and cognitive science. The SVRT challenge consists of twenty-three binary classification problems involving abstract rules for stimuli. CNNs were trained on different hyper-parameter combinations for each problem. The SVRT challenge involved training CNNs on different hyper-parameter combinations for twenty-three binary classification problems. High-throughput analysis showed that CNNs produced lower accuracies on same-different problems compared to spatial-relation problems. CNNs with varying depths and receptive field sizes were tested on twenty-three binary classification problems. The models were trained using an ADAM optimizer on 2 million examples split into training and test sets. The study found that CNNs performed better on spatial-relation problems compared to same-different problems. The study tested CNNs with different depths and receptive field sizes on twenty-three binary classification problems. The models were trained using an ADAM optimizer on 2 million examples split into training and test sets. CNNs performed better on spatial-relation problems compared to same-different problems. The best networks' accuracy for each problem was sorted and colored based on problem descriptions provided by BID6. The study tested CNNs on twenty-three binary classification problems, sorting them by accuracy and coloring them based on problem descriptions. Same-Different (SD) problems were colored red, while Spatial-Relation (SR) problems were colored blue. CNNs performed better on SR problems than on SD problems, with some SD problems resulting in accuracy not substantially above chance. This suggests that SD tasks pose a particularly difficult challenge to CNNs. The study found that CNNs performed better on Spatial-Relation (SR) problems compared to Same-Different (SD) problems. Some SD problems resulted in accuracy not much better than chance, indicating that SD tasks are particularly challenging for CNNs. Additionally, larger networks yielded significantly higher accuracy on SD problems than smaller ones. This result aligns with previous evidence of a visual-relation dichotomy. The study found that larger networks yielded higher accuracy on SD problems compared to smaller ones. Experiment 1 supports previous findings that feedforward models struggle with visual-relation problems. The SVRT challenge has limitations in its sample of visual relations. The SVRT challenge is useful for evaluating algorithm efficacy on various visual relations, but has limitations in its sample selection. The SVRT challenge evaluates visual relations with different problems that require unique image generation methods, making direct comparisons difficult. For example, Problem 2 requires one large and one small object, conflicting with Problem 1 where two identically-sized items must be positioned without overlap. The SVRT challenge evaluates visual relations with different problems that require unique image generation methods, making direct comparisons difficult. Problem 2 requires one large and one small object, conflicting with Problem 1 where two identically-sized items must be positioned without overlap. Using closed curves in SVRT images hinders quantification of image variability and task difficulty. The SVRT challenge involves evaluating visual relations with unique image generation methods, making direct comparisons challenging. Using closed curves in SVRT images hinders quantification of image variability and task difficulty. To address these issues, a new dataset was constructed with two idealized problems: Spatial Relations (SR) and Same-Different (SD). In SR, images are classified based on the orientation of items arranged horizontally or vertically. The new dataset consists of two idealized problems: Spatial Relations (SR) and Same-Different (SD). In SR, images are classified based on the orientation of items arranged horizontally or vertically. In SD, images are classified based on whether they contain identical items. The image generator creates gray-scale images using square binary bit patterns on a blank background, controlled by item size, image size, and number of items. The image generator produces gray-scale images using square binary bit patterns on a blank background, controlled by item size, image size, and number of items. Item size, image size, and number of items determine image variability at the item and spatial levels. The number of items (k) controls item and spatial variability in image generation. When k \u2265 3, the SD category label is based on the presence of at least 2 identical items, and the SR category label is determined by the average orientation of item displacements. The test for quantifying possible images in a dataset is called Parametric SVRT (PSVRT). Each image is generated by assigning joint class labels for SD and SR. The Parametric SVRT (PSVRT) test quantifies possible images in a dataset by generating images with joint class labels for SD and SR. Images are created by sampling items and placing them in an n \u00d7 n image with background spacing. In the experiment, 1 and k \u2212 1 identical copies of the first item are created, and k unique items are consecutively sampled and placed in an n \u00d7 n image with background spacing. The goal was to assess the difficulty of learning PSVRT problems with varying image variability parameters using a baseline architecture. Each combination of item size, image size, and item number was trained from scratch to measure performance. For each combination of item size, image size, and item number, a baseline architecture was used to measure the difficulty of learning PSVRT problems. The number of training examples required for the architecture to reach 95% accuracy was measured as a training-to-acquisition metric. The network was trained from scratch without a holdout test set, and the effect of varying image parameters on learnability was examined in three sub-experiments. The difficulty of fitting training data in various conditions was estimated by training a baseline CNN from scratch without a holdout test set. Three sub-experiments were conducted by varying image parameters separately. The baseline CNN had four convolution and pool layers, four fully-connected layers, and was trained with 20 million images and a batch size of 50. The best-case result for each experimental condition was reported. The baseline CNN was trained from scratch with 20 million images and a batch size of 50. It had four convolution and pool layers, four fully-connected layers, and used an ADAM optimizer with a base learning rate of 10^-4. The network size was varied to examine its effect on learnability. The fully-connected layers had 256 units with dropout probability of 0.5. An ADAM optimizer with base learning rate \u03b7 = 10^-4 was used. Weights were initialized with the Xavier method. Experiments were repeated with a larger network control. Strong dichotomy in learning curves was observed, with a sudden rise in accuracy from chance-level called the \"learning event\". In experiments, a sudden rise in accuracy from chance-level, known as the \"learning event\", was observed. Training runs that exhibited this event almost always reached 95% accuracy within 20 million images. Final accuracy showed a strong bi-modality - either chance-level or close to 100%. In experiments, a sudden rise in accuracy known as the \"learning event\" occurred, with final accuracy showing a strong bi-modality - either chance-level or close to 100%. The network learned different experimental conditions with varying success rates, with some image parameters causing a significant straining effect. Increasing image size and the number of items in the image had a significant impact on the network's ability to learn a specific problem. The network struggled to learn when the image size was increased, with no learning events occurring at sizes of 150x150 and above. Additionally, having 3 or more items in an image also hindered the network's learning. Even when a relaxation of the same-different rule was applied, the CNN still did not learn for any parameter configuration. Increasing image size and the number of items in the image significantly impacted the network's ability to learn. The CNN did not learn for any parameter configuration, even with a relaxation of the same-different rule. The increase in image variability due to larger image sizes and more items strained the CNN, reflecting the exponential relationship between image size and item number. Increasing image size and item number impact image variability exponentially. Larger image sizes with 2 items result in quadratic-rate increase, while more items lead to exponential-rate increase. Straining effect on CNN is strong regardless of network width, with a constant shift in TTA curve. Item size has no visible effect on CNN. Learnability remains stable across different item sizes. The transition to the problem being essentially impossible was delayed by one step in the image size parameter. Increasing item size had no visible straining effect on CNN. Learnability is preserved and stable over the range of item sizes considered. It is possible to construct feedforward feature detectors that can generalize to coordinated item variability. These results imply that CNNs build a feature set tailored for a particular dataset when learning a PSVRT condition. The study suggests that when CNNs learn a PSVRT condition, they are creating a feature set specific to the dataset rather than learning a general rule. The features learned by CNNs are not invariant rule-detectors but rather templates covering a particular distribution in the image space. The Relational Network (RN) is an architecture designed to detect visual relations and sits on top of a CNN. It learns a map from pairs of high-level CNN feature vectors to answers for relational questions, outperforming baseline models. The Relational Network (RN) is a CNN-based system that processes relational questions using LSTM or binary strings. It outperforms baseline models on visual reasoning tasks, including the \"sort-of-CLEVR\" task with simple 2D items. The RN can answer both relational and non-relational questions in scenes with up to six items of two shapes and six colors. The RN was trained to answer relational and non-relational questions in scenes with two shapes and six colors. The sort-of-CLEVR task has limitations due to low item variability and lack of concept learning. To address this, the model was trained on a two-item sort-of-CLEVR same-different task and PSVRT stimuli to measure performance without these handicaps. The model was trained on a two-item sort-of-CLEVR same-different task and PSVRT stimuli to measure performance without handicaps. Architecture details include a convolutional network with four layers and ReLu activations. The model architecture for visual reasoning included a convolutional network with four layers, using ReLu activations. The relational network component consisted of multiple fully connected layers with dropout, trained with a cross-entropy loss function and ADAM optimizer. The model architecture for visual reasoning included a convolutional network with four layers, ReLu activations, and a relational network with fully connected layers and dropout. The final layer used a softmax function and was trained with cross-entropy loss and ADAM optimizer. We confirmed the model's ability to reproduce results on the sort-of-CLEVR task with twelve different datasets missing color+shape combinations. The study focused on the CNN+RN architecture's performance on the sort-of-CLEVR task with twelve datasets missing color+shape combinations. The model did not generalize well to left-out combinations, but learned faster due to fewer items to generalize across compared to CNNs on PSVRT stimuli. The CNN+RN model did not generalize well to left-out color+shape combinations on the sort-of-CLEVR task. Despite learning faster with fewer items to generalize across, the model's validation accuracy remained at chance levels. The CNN+RN model struggled to generalize to left-out color+shape combinations on the sort-of-CLEVR task, despite training on 20M images and using PSVRT bit patterns. The model behaved like a vanilla CNN, with validation accuracy remaining at chance levels. The CNN+RN model, trained on 20M images with PSVRT bit patterns, behaves like a vanilla CNN. It achieves over 95% accuracy for image sizes of 120 pixels or below, but fails to learn for sizes of 150 and 180 pixels. This cutoff point may be due to the representational capacity of the RN architecture, which struggles with some same-different tasks on PSVRT. The results suggest that visual-relation problems can exceed the capacity of CNNs. Our results show that while CNNs can handle learning templates for individual objects, they struggle with learning templates for arrangements of objects due to the combinatorial explosion in the number of templates needed. This limitation in representing stimuli with a combinatorial structure is overlooked by current computer vision scientists, despite being acknowledged by cognitive scientists. In contrast, biological visual systems excel at detecting relations. The limitations of representing stimuli with a combinatorial structure have been acknowledged by cognitive scientists but overlooked by computer vision scientists. Biological visual systems excel at detecting relations and can learn complicated visual rules with few examples. For instance, humans can generalize complex visual rules from just a few training examples, while CNNs struggle with learning templates for arrangements of objects due to the combinatorial explosion in the number of templates needed. The hardest SVRT problem for CNNs in Experiment 1, problem 20, involves complex shapes and reflection. Despite extensive training, the best network could not perform significantly above chance. Visual reasoning abilities are not exclusive to humans, as birds and primates can also learn same-different relations. Ducklings have shown impressive one-shot learning abilities in visual tasks. Ducklings demonstrated one-shot learning of same-different relations with simple 3D objects, suggesting rapid abstract concept acquisition or innate knowledge. In contrast, CNN+RN in Experiment 3 struggled to transfer this concept to novel objects even after extensive training. The ducklings quickly learned same-different relations with 3D objects, showing rapid abstract concept acquisition. In contrast, the CNN+RN in Experiment 3 struggled to transfer this concept to new objects despite extensive training. There is evidence that visual-relation detection in the brain may rely on feedback signals beyond feedforward processes. Certain visual recognition tasks can be achieved with minimal cortical feedback, primarily through feedforward activity in the visual cortex. The detection of natural object categories can occur with minimal cortical feedback through feedforward activity in the visual cortex. However, object localization in cluttered scenes requires attention due to the spatial relations between objects, which cannot be accurately processed with just a feedforward sweep. The processing of spatial relations between objects in cluttered scenes requires attention and working memory, as shown by converging neuroscience evidence. Working memory plays a role in solving tasks that involve spatial reasoning, highlighting the computational role of attention and working memory in detecting visual relations. Imaging studies have shown the role of working memory in prefrontal and premotor cortices when solving tasks like Raven's progressive matrices. Attention and working memory play a computational role in detecting visual relations by allowing flexible representations to be constructed dynamically, avoiding capacity overload in neural networks. Humans can easily detect spatial relations between objects. Humans excel at detecting visual relations and constructing structured descriptions effortlessly. This ability surpasses that of modern computers, highlighting the importance of exploring attentional and mnemonic mechanisms in computational understanding of visual reasoning."
}