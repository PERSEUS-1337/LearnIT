{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, including humans, despite individual incentives conflicting with the common good. The study focuses on intertemporal social dilemmas (ISDs) and demonstrates that cooperation can be learned through a model-free approach using a modular architecture for deep reinforcement learning agents. The study combines MARL with natural selection to show that individual biases for cooperation can be learned in a model-free way. An innovative modular architecture for deep reinforcement learning agents supports multi-level selection, with results in challenging environments interpreted in the context of cultural and ecological evolution. Nature exhibits cooperation at all scales, despite natural selection favoring selfish interests. Altruism can be favored by selection when cooperating individuals interact with other cooperators. Altruism can be favored by selection when cooperating individuals interact with other cooperators, despite natural selection favoring selfish interests. Cooperation among self-interested agents is an important topic in multi-agent deep reinforcement learning, formalized as an intertemporal social dilemma. Social dilemmas involve a trade-off between collective welfare and individual utility, as predicted by evolutionary theory. In multi-agent deep reinforcement learning, the problem of cooperation among self-interested agents is formalized as an intertemporal social dilemma. Social dilemmas involve a trade-off between collective welfare and individual utility. Evolutionary theory predicts that self-interested agents tend to converge to defecting strategies instead of achieving the collectively optimal outcome. Various solutions have been proposed, including opponent modeling, long-term planning with perfect knowledge, and intrinsic motivation functions from behavioral economics. Evolution can be applied to remove hand-crafted intrinsic motivation in multi-agent deep reinforcement learning, similar to other applications in deep learning. Evolution has been used for optimizing hyperparameters, black-box optimization, evolving neuroarchitectures, regularization, loss functions, behavioral diversity, and reward functions. Evolution has been applied in deep learning for various tasks such as optimizing hyperparameters, black-box optimization, evolving neuroarchitectures, regularization, loss functions, behavioral diversity, and reward functions. In the context of intrinsic motivation in multi-agent deep reinforcement learning, evolution has been used to address specific challenges by distinguishing between optimization processes unfolding over two distinct time-scales. Evolution has been applied in deep learning for tasks like optimizing hyperparameters, neuroarchitectures, and reward functions. In the context of intrinsic motivation in multi-agent deep reinforcement learning, evolution helps address challenges by distinguishing between fast learning and slow evolution processes. The intrinsic motivation is modeled as an additional term in the reward of each agent, implemented as a neural network for evolution. Evolutionary theory predicts that evolving individual intrinsic reward weights across a population does not lead to altruistic behavior. To achieve this goal, evolutionary dynamics must be structured. A \"Greenbeard\" strategy is implemented where agents choose interaction partners based on cooperativeness signals, termed assortative matchmaking. To achieve altruistic behavior, evolutionary dynamics must be structured. A \"Greenbeard\" strategy involves agents choosing partners based on honest signals of cooperativeness, known as assortative matchmaking. However, this approach has limitations and a new modular training scheme called shared reward network evolution is introduced to address them. In response to the limitations of assortative matchmaking, a new modular training scheme called shared reward network evolution is introduced in ISD models for deep reinforcement learning. This scheme involves agents with policy and reward network modules evolving separately, where the policy network is trained using modified rewards specified by the reward network. The fitness for the policy network is based on individual rewards, while the fitness for the reward network is determined by the collective return of the group of co-players. The network and reward modules evolve separately in the shared reward network evolution scheme for ISD models. Policy networks are lower level units of evolution, while reward networks are higher level units. Evolving them separately prevents overfitting and may explain the evolutionary origin of social inductive biases. Parameters like environments, reward network features, and matchmaking are varied and explored. The evolutionary paradigm resolves ISDs without handcrafting and suggests a mechanism for social inductive biases. Parameters like environments, reward network features, and matchmaking are varied and explored in Markov games within a MARL setting, focusing on intertemporal social dilemmas. In intertemporal social dilemmas, individual actions benefit in the short term but harm the group in the long term. Two dilemmas are studied using partially observable Markov games on a 2D grid. In the Cleanup game, agents collect apples with spawning rates linked to aquifer cleanliness, creating a dilemma as waste accumulation reduces apple respawn until none can spawn. In the Harvest game, agents collect rewarding apples with spawn rates depending on nearby apples, leading to a dilemma between quick individual harvesting and rapid depletion of apples. In the Harvest game, agents collect apples with spawn rates depending on nearby apples, leading to a dilemma between quick individual harvesting and rapid depletion. The reward components for agents include total, extrinsic, and intrinsic rewards, influencing their decision-making process. The reward components for players in the Harvest game include extrinsic and intrinsic rewards. Extrinsic reward is obtained from the environment based on actions taken, while intrinsic reward is calculated using a neural network with evolved parameters. The parameters of a 2-layer neural network with 2 hidden nodes are evolved based on fitness. The elements of v correspond to coefficients related to inequity aversion. Each agent has access to the same set of features, with its own feature demarcated specially. Features are a function of received or expected future rewards. In Markov games, rewards received by players may not be aligned in time. In Markov games, rewards for players may not align in time. Social preferences should not be influenced by precise temporal alignment of rewards. Two ways of aggregating rewards are considered. Agent A adjusts policy using off-policy actor-critic method. Architecture includes intrinsic and extrinsic value heads, policy head, and reward evolution. The architecture for aggregating rewards in Markov games includes intrinsic and extrinsic value heads, a policy head, and reward network evolution. Two ways of temporally aggregating rewards are explored: retrospective method based on past rewards and prospective method based on future rewards. The architecture for aggregating rewards in Markov games includes intrinsic and extrinsic value heads, a policy head, and reward network evolution. Two ways of temporally aggregating rewards are explored: retrospective method based on past rewards and prospective method based on future rewards. The training framework involves distributed asynchronous training in multi-agent environments, with a population of 50 agents trained using policies {\u03c0 i } and sampled in groups of 5 for 500 arenas running in parallel. Matchmaking processes were used to sample agents for each arena. The study involved training a population of 50 agents with policies {\u03c0 i } in 500 arenas running in parallel. Agents were sampled using matchmaking processes, and episode trajectories lasted 1000 steps. Weight updates were done using V-Trace, and parameters of the policy network were inherited in a Lamarckian fashion. Agents were allowed to observe their last actions and rewards as input to the LSTM in their neural network. The study involved training a population of 50 agents with policies in 500 arenas running in parallel. Agents observed their last actions and rewards as input to the LSTM in their neural network. Evolution was based on a fitness measure calculated as a moving average of total episode return. Evolution of agents was based on fitness calculated from total episode return. Matches were determined by random or assortative matchmaking, grouping cooperative agents together. The study used cooperative matchmaking to group agents based on recent cooperativeness, ensuring that highly cooperative agents played with each other. Cooperativeness was calculated differently for Cleanup and Harvest scenarios. Cooperative metric-based matchmaking was not used for the multi-level selection model. The study utilized cooperative matchmaking to group agents based on recent cooperativeness, with a focus on high cooperativeness yielding a high ranking. Cooperative metric-based matchmaking was implemented with individual reward networks or no reward networks. Unlike the multi-level selection model, this approach allowed for separate evolution of the reward network within its own population, enabling independent exploration of hyperparameters and generalization to a wide range of scenarios. The study utilized cooperative matchmaking to group agents based on recent cooperativeness, allowing for independent exploration of hyperparameters and generalization to a wide range of scenarios. Separate reward networks were evolved within their own population, enabling exploration of the hyperparameter landscape and generalization to various policies. Fitness was determined based on individual agent return, while reward network parameters were evolved based on total episode return across the group of co-players. The network weights and optimization-related hyperparameters were evolved based on individual agent return, while the reward network parameters were evolved based on total episode return across the group of co-players. This approach focuses on evolving social features for cooperation rather than remapping environmental events, and aims to address the tension in Intrinsic Motivation Systems (ISDs) by evolving a form of communication for social cooperation. Shared reward networks play a critical role in a social setting, providing a biologically principled method for group interaction. In a social setting, shared reward networks are crucial for cooperation, evolving a form of communication for social cooperation. This method mixes group fitness on a long timescale with individual reward on a short timescale, contrasting with hand-crafted aggregation methods. Without using an intrinsic reward network, performance is poor in games like Cleanup and Harvest. Random matchmaking versus assortative matchmaking with reward networks using retrospective social features show significant differences in outcomes. The intrinsic reward network performs poorly on Cleanup and Harvest games, with total episode rewards asymptoting to 0 and 400 apples, respectively. Comparing random and assortative matchmaking with PBT and reward networks using retrospective social features, individual reward network agents show little improvement over PBT on Cleanup and only moderate improvement on Harvest. Assortative matchmaking experiments without a reward network perform the same as the PBT baseline, while individual reward networks lead to high performance, highlighting the importance of conditioning internal rewards on social features and promoting cooperation among agents. The assortative matchmaking experiments showed that individual reward networks led to high performance, indicating the importance of conditioning internal rewards on social features and promoting cooperation among agents. Shared reward network agents performed as well as assortative matchmaking, even with random matchmaking, suggesting that agents did not necessarily need immediate access to honest signals of other agents' cooperativeness to resolve the dilemma. The study found that agents with the same intrinsic reward function, evolved based on collective episode return, could resolve the cooperation dilemma without needing immediate access to honest signals of other agents' cooperativeness. The retrospective variant of reward network evolution outperformed the prospective variant, which relied on agents learning good value estimates before the reward networks became useful. Various social outcome metrics were plotted to better understand agent behavior. The study compared the performance of retrospective and prospective reward network variants in resolving the cooperation dilemma. Sustainability, equality, and tagging metrics were used to analyze agent behavior. The retrospective variant showed higher equality and sustainability compared to the prospective variant. The study compared the performance of retrospective and prospective reward network variants in resolving the cooperation dilemma. Equality is calculated as E(1 \u2212 G(R)), where G(R) is the Gini coefficient over individual returns. Tagging measures the average number of times a player fined another player throughout the episode. The final weights of the retrospective shared reward networks evolved in a way that suggests different social preferences are needed to resolve each game. The final weights of the retrospective shared reward networks evolved differently for each game, suggesting varying social preferences are required. In Cleanup, simpler reward networks sufficed, while Harvest needed a more complex reward function to prevent over-exploitation of resources. Random matchmaking led to arbitrary positive values in the first layer weights. In Harvest, a more complex reward function was needed to prevent over-exploitation of resources. The first layer weights tended to take on arbitrary positive values due to random matchmaking. Natural selection via genetic algorithms did not lead to cooperation, but assortative matchmaking was effective in generating cooperative behavior. Intrinsic rewards from other agents in the environment did not lead to cooperation through genetic algorithms. Assortative matchmaking and shared reward networks were effective in promoting cooperative behavior. Evolution helps solve the intertemporal choice problem and mitigates the social dilemma. Evolutionary mechanisms promote cooperation by improving credit assignment between selfish acts and negative group outcomes, exposing social signals related to selfishness, and enabling mechanisms like competitive altruism and inequity aversion. Laboratory experiments show increased cooperation with communication. The shared reward network evolution model draws inspiration from multi-level selection theory. The shared reward network evolution model is inspired by multi-level selection theory, promoting cooperation through altruism, other-regarding preferences, and inequity aversion. Humans cooperate more when communication is possible. Modularity in evolution can be seen in various forms in nature, such as microorganisms forming multi-cellular structures and prokaryotes incorporating plasmids for cooperation in social dilemmas. Free-living microorganisms form multi-cellular structures for adaptive purposes, while prokaryotes can incorporate plasmids for cooperation. In humans, a reward network represents a shared cultural norm based on accumulated cultural information. Future research could explore alternative evolutionary mechanisms like kin selection and reciprocity in understanding cooperation. Future research could explore alternative evolutionary mechanisms like kin selection and reciprocity in understanding cooperation. Investigating the emergence of cooperation through different social biases and assortative matchmaking models could provide further insights. Additionally, combining an evolutionary approach with multi-agent communication to study cooperative behaviors like cheap talk would be intriguing. Games have specific parameters such as episode duration and playable area size, with agents having limited observation capabilities. The games Cleanup and Harvest have specific parameters like episode duration and playable area size. Agents can only observe through a 15x15 RGB window and have actions like moving, rotating, tagging, and cleaning waste. Training involved joint optimization of network parameters and evolution of hyperparameters/reward network parameters. The Cleanup game penalizes players by deducting 50 reward points for certain actions. Training involved optimizing network parameters and hyperparameters/reward network parameters using SGD and evolution in a PBT setup. Gradient updates were applied for trajectories up to 100 steps with a batch size of 32. Optimization was done with RMSProp, a discount factor of 0.99, and evolving entropy cost using PBT. Learning rates started at 4 \u00d7 10 \u22124 and evolved. PBT uses genetic algorithms to search hyperparameters, resulting in adaptive tuning. The entropy cost and learning rates were evolved using PBT, with a mutation rate of 0.1 and perturbations of \u00b120% for entropy cost and learning rate. A burn-in period of 4 \u00d7 10 6 agent steps was implemented before evolution to assess fitness accurately."
}