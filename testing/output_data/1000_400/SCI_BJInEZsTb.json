{
    "title": "BJInEZsTb",
    "content": "In this paper, a deep autoencoder network is introduced for learning representations of geometric data represented as point clouds. The learned representations outperform state-of-the-art methods in 3D recognition tasks and enable shape editing applications. Different generative models, including GANs and Gaussian mixture models, are studied, with GMMs trained in the latent space of the autoencoders producing samples of the best fidelity and diversity. The paper introduces a deep autoencoder network for learning representations of geometric data as point clouds, outperforming state-of-the-art methods in 3D recognition tasks. Different generative models like GANs and Gaussian mixture models are studied, with GMMs trained in the autoencoder's latent space producing samples with the best fidelity and diversity. The evaluation of generative models focuses on fidelity and diversity measures based on matching point clouds. Various 3D representations of real-life objects are essential for vision, robotics, medicine, and augmented reality applications. Traditional shape representations like 3D meshes and CAD models are complemented by newer encodings like view-based projections and volumetric grids, but these may lack semantics and are not ideal for generative model design. Recent advances in deep learning offer a data-driven approach to designing and editing objects, eliminating the need for complex parametric models. Deep learning tools like autoencoders and Generative Adversarial Networks have revolutionized the process by removing the manual feature crafting and model design. This shift is particularly beneficial in domains with abundant data, where deep learning architectures can generate representations with improved fidelity and diversity. Recent advances in deep learning have made it easier to design and edit objects without the need for manual feature crafting. Deep learning tools like autoencoders and Generative Adversarial Networks have proven successful in learning complex data representations and generating realistic samples. Point clouds, a relatively unexplored 3D modality, offer a compact representation of surface geometry that is easily manipulated. Point clouds are a compact representation of surface geometry that are easily manipulated and commonly used in devices like the Kinect and iPhone for face identification. Deep learning architectures like PointNet have successfully tackled classification and segmentation tasks using point clouds. Deep architectures for 3D point clouds have been used for classification and segmentation tasks. Generative models for point clouds have gained attention, but training GAN-based pipelines is challenging. Evaluating generative models involves assessing fidelity and coverage of the generated data points. The text discusses the challenges of training GAN-based generative pipelines for 3D point clouds and the importance of evaluating generative models based on fidelity and coverage. It introduces a new AE architecture inspired by recent classification architectures to learn compact representations of point clouds with high reconstruction quality. The text introduces a new AE architecture for learning compact representations of point clouds with high reconstruction quality. It also discusses the creation of generative models capable of generating point clouds similar to training data and proposes a workflow for training GANs in a fixed latent representation. The text proposes a workflow for training GANs in a fixed latent representation, showing that latent GANs are easier to train and achieve superior reconstruction. Multi-class GANs perform almost as well as dedicated GANs trained per-object category in the latent space. The text discusses the superior reconstruction achieved by GMMs trained in the latent space of fixed AEs. Multi-class GANs show comparable performance to dedicated GANs when trained in the latent space. Various metrics are evaluated for learning good representations and evaluating generated samples, including proposing fidelity and coverage metrics for generative models. The paper proposes fidelity and coverage metrics for generative models, based on optimal matching between generated point clouds and a test set. It outlines the necessary background, introduces evaluation metrics, models for latent representations, and generation of point clouds. The models are evaluated quantitatively and qualitatively in Section 4, with further results in the appendix. The code for all models is publicly available. Autoencoders are deep architectures that aim to reproduce their input with a narrow bottleneck layer. The Encoder compresses data into a low-dimensional representation, while the Decoder reproduces the input from the encoded version. Generative Adversarial Networks (GANs) are state-of-the-art models that involve a game between a generator and a discriminator to synthesize samples indistinguishable from real data. The code for all models is publicly available. Generative Adversarial Networks (GANs) involve a game between a generator and a discriminator to synthesize samples indistinguishable from real data. The improved Wasserstein GAN has shown improved stability during training. Point clouds present unique challenges when building a network. Point clouds pose unique challenges for network architecture due to their lack of grid-like structure, making them harder to encode than images or voxel grids. Recent work bypasses this issue by avoiding 2D convolutions and addressing the unordered nature of point clouds. Recent classification work on point clouds bypasses the challenges of encoding them by avoiding 2D convolutions and addressing their unordered nature. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature, including the Earth Mover's distance (EMD) which transforms one set to another. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature: Earth Mover's distance (EMD) and Chamfer distance (CD). EMD transforms one set to another through a transportation problem, while CD measures the squared distance between each point and its nearest neighbor in the other set. These metrics are used to evaluate representations and generative models of point clouds. The paper discusses evaluation metrics for representations and generative models of point clouds. It compares point-cloud distributions to ground truth using metrics like Coverage, which measures how well a point-cloud distribution matches a ground truth distribution using either Chamfer distance (CD) or Earth Mover's distance (EMD). The paper evaluates the faithfulness and diversity of a generative model by measuring potential mode-collapse. Metrics like Coverage, COV-CD, COV-EMD, MMD-CD, and MMD-EMD are used to compare point-cloud distributions to ground truth distributions. Coverage measures the fraction of point-clouds in G matched to A, while MMD captures fidelity by finding the minimum distance between point clouds in G and A. The fidelity of a generative model is evaluated by matching point clouds of G to A using Minimum Matching Distance (MMD). MMD-CD and MMD-EMD can be used to measure structural distances. Jensen-Shannon Divergence (JSD) compares distributions in 3D space to assess similarity between point clouds of A and B. In this section, the architectures of representation and generative models for point clouds are described, starting with an autoencoder design. A GAN architecture tailored to point-cloud data is introduced, followed by a more efficient pipeline that learns an AE and then trains a smaller GAN in the latent space. Additionally, a simpler generative model based on Gaussian Mixtures is discussed. The input to the AE network is a point cloud with 2048 points, representing a 3D shape. The encoder architecture follows the principle of 1-D convolutional layers with kernel size 1. The encoder architecture for the autoencoder (AE) network follows the principle of 1-D convolutional layers with kernel size 1 and increasing number of features. It encodes every point independently using 5 1-D conv layers, each followed by a ReLU and a batch-norm layer. The output of the last conv layer is passed to a feature-wise maximum to produce a k-dimensional vector, the basis for the latent space. The decoder transforms the latent vector with 3 fully connected layers to produce a 2048 \u00d7 3 output. In our implementation, we use 5 1-D conv layers with ReLU and batch-norm. The output of the last conv layer is passed to a feature-wise maximum to create a k-dimensional vector for the latent space. The decoder consists of 3 fully connected layers to produce a 2048 \u00d7 3 output. We explore EMD-distance and Chamfer-Distance as structural losses, resulting in two AE models: AE-EMD and AE-CD. We experimented with different bottleneck sizes and trained the AEs with point-clouds of a single object class under the two losses. The study explored different bottleneck sizes for autoencoders (AEs) trained on point clouds of a single object class under two losses. The best generalization error was found with a bottleneck size of k = 128, while achieving minimal reconstruction error on the train split. Additionally, the first version of the generative model operated directly on the raw 2048 \u00d7 3 point set input, presenting a GAN for point clouds with a discriminator architecture similar to the AE. The study explored different bottleneck sizes for autoencoders trained on point clouds of a single object class under two losses. The first version of the generative model operates directly on the raw 2048 \u00d7 3 point set input, presenting a GAN for point clouds with a discriminator architecture similar to the AE. The l-GAN passes data through a pre-trained autoencoder and operates on the 128-dimensional bottleneck variable. The l-GAN operates on the 128-dimensional bottleneck variable of a pre-trained autoencoder for each object class, simplifying the architecture compared to the r-GAN. Shallow designs for the generator and discriminator are found to produce realistic results. Additionally, Gaussian Mixture Models are trained on the latent spaces learned by the autoencoders. The study explores the use of Gaussian Mixture Models (GMMs) in addition to l-GANs, trained on latent spaces learned by autoencoders. GMMs with varying numbers of components are fitted, and can be used as point-cloud generators. The shapes are reconstructed using class-specific autoencoders, with shapes sourced from the ShapeNet repository. Reconstructions of unseen shapes from the test split of input data using class-specific autoencoders trained on shapes from the ShapeNet repository. Evaluation of the latent features computed by the autoencoder is done by applying them as feature extractors on supervised datasets. The latent features computed by the autoencoder are used as feature extractors on supervised datasets to evaluate their performance. The autoencoder was trained on 57,000 models from 55 categories of man-made objects. A bigger bottleneck of 512 was used, and features were extracted from input 3D shapes by feeding their point-cloud to the network. These features were then processed by a linear classification SVM. The results are compared to the previous state of the art, which used a GAN to derive a 7168-long feature. The 512-dimensional bottleneck layer vector extracted from the point-cloud is processed by a linear classification SVM trained on ModelNet BID32. Compared to the previous state of the art BID31, our feature is more intuitive and parsimonious. CD loss performs better when there is increased variation within the collection, possibly due to its ability to understand rough edges and high frequency geometric details. The EMD and CD losses perform equivalently on larger objects with fewer categories than ModelNet40. However, CD produces better results when variation within the collection increases, possibly due to its ability to understand rough edges and high frequency geometric details. The experiment also demonstrates the domain-robustness of learned features. Visual assessment through reconstruction results shows the ability of learned representation to generalize to unseen shapes. Our Autoencoders (AEs) demonstrate the ability to reconstruct unseen shapes and enable shape editing applications such as interpolations, part editing, and analogies. The learned latent representation shows generalization ability to unseen shapes, as evidenced by visual comparisons and quantitative measurements. Our AEs can reconstruct unseen shapes, demonstrated in results and quantitative measurements. Five generative models were trained on chair point-cloud data, including AEs with 128-dimensional bottleneck trained with CD or EMD loss. l-GANs were trained in each AE's latent space, with additional models utilizing Wasserstein objective. In the study, various generative models were trained on chair point-cloud data, including autoencoders with different loss functions. l-GANs were trained in the latent space of the autoencoders, with additional models using the Wasserstein objective. Model selection was based on how well the synthetic results matched the ground-truth distribution. The study trained various generative models on chair point-cloud data, including l-GANs and models using the Wasserstein objective. Model selection was based on how well the synthetic results matched the ground-truth distribution, using JSD or MMD-CD metrics every 100 epochs. The optimal number of Gaussian components for GMM was found to be 32, with full covariance matrices performing better. The study selected various models based on JSD criterion and obtained optimal values for Gaussian components. GMMs performed better with full covariance matrices. MMD-CD criterion also yielded models of similar quality with 40 Gaussians. Evaluation of 5 generators on chair dataset showed an average classification score of 84.7%. Comparisons were made with the volumetric approach of BID31. The study evaluated 5 generators on the chair dataset using models selected based on minimal JSD on the validation-split. Comparison was made with the volumetric approach of BID31. The average classification score for ground-truth point clouds was 84.7%. Quantitative evaluation was conducted by comparing the capacity of the models to generate synthetic samples resembling the train and test splits of the ground truth distribution. The results were reported in Table 2, along with the average classification probability for samples recognized as chairs using the PointNet classifier BID17. The study evaluated 5 generators on the chair dataset by generating synthetic point clouds and comparing them against the train and test set distributions. Results were reported in Table 2, including the average classification probability for samples recognized as chairs using the PointNet classifier BID17. A similar experiment was conducted on the test-split dataset, with results averaged over 3 repetitions and reported in TAB10. Training a simple Gaussian mixture model was also explored. The study evaluated 5 generators on the chair dataset by generating synthetic point clouds and comparing them against the train and test set distributions. Results were reported in Table 2, including the average classification probability for samples recognized as chairs using the PointNet classifier BID17. Additionally, training a simple Gaussian mixture model in the latent space of the EMD-based AE yielded the best results in terms of fidelity and coverage. The achieved fidelity and coverage were very close to the reconstruction baseline, with comparable MMD-EMD values to the ground truth training data. The AE-EMD achieved an MMD-EMD of 0.05 with respect to the ground truth training data, comparable to the GMMs' MMD-EMD value of 0.06 on the test data. The models show generalization ability with comparable performance on training vs. testing splits. Synthetic datasets for testing and validation are three times bigger than the ground truth dataset. The number of synthetic point clouds generated for the train split experiment is equal to the size of the train dataset. For the test split experiment and model selection comparisons, synthetic datasets three times bigger than the ground truth dataset are used to reduce sampling bias. This is necessary for measuring MMD or Coverage statistics. The MMD-CD distance to the test set for r-GANs appears small, but qualitative inspection shows otherwise. Chamfer distance may not distinguish pathological cases effectively. The r-GANs have a small MMD-CD distance to the test set, but qualitative inspection reveals otherwise. The inadequacy of the chamfer distance to distinguish pathological cases is highlighted with examples in Fig. 3. Nearest neighbor comparisons between synthetic and ground truth point clouds are shown, indicating limitations in distinguishing results using CD and EMD metrics. The r-GAN results show a small MMD-CD distance to the test set, but qualitative inspection reveals lower quality compared to l-GAN. The chamfer distance is inadequate in distinguishing pathological cases, as it fails to capture the visual differences in point cloud generation. The CD metric is \"blind\" to partial matches between shapes, leading to limitations in distinguishing results using CD and EMD metrics. The CD metric is \"blind\" to partial matches between shapes, leading to limitations in distinguishing results using CD and EMD metrics. The JSD distance between ground truth and synthetic datasets generated by various models during training is plotted in FIG2. During training, the r-GAN struggles to provide good coverage of the test set, as shown by JSD distance, EMD-based MMD, and Coverage metrics. The l-GAN (AE-CD) performs better in terms of fidelity with fewer visual quality issues. The l-GAN (AE-CD) performs better in terms of fidelity with fewer epochs, but coverage remains low due to unnatural topologies promoted by the CD distance. Switching to an EMD-based AE (l-GAN, AE-EMD) results in a dramatic improvement in coverage and fidelity, although both l-GANs still suffer from mode collapse during training. Switching to a latent WGAN largely eliminates mode collapse in GANs on point-cloud data, improving coverage and fidelity. Comparison to voxel-based methods shows promising results in terms of JSD on the training set of the chair category. The authors propose GANs on point-cloud data, comparing their models to a voxel-grid based approach BID31 in terms of JSD on the training set of the chair category. They convert voxel grid output into a point-set with 2048 points and report better diversity and realistic results with their r-GAN compared to BID31. The l-GANs outperform BID31 in terms of diversity and classification, with shorter training time per epoch. The r-GAN also shows better diversity and realistic results compared to BID31. The l-GAN produces high-quality synthetic point clouds with less noise compared to the r-GAN, showcasing the advantage of using a good structural loss on the pre-trained AE. The l-GAN produces crisper synthetic point clouds compared to the r-GAN, demonstrating the advantage of using a good structural loss on the pre-trained AE. The synthetic point clouds were generated by samples produced with l-GAN and a 32-component GMM, both trained on the latent space of an AE using the EMD loss. Extensions to multiple classes were also explored, with an AE-EMD trained on a mixed set containing point clouds from 5 categories. The study involved training and testing datasets for an autoencoder (AE) using models from different classes. The multi-class AE with a bottleneck size of 128 was trained for 1000 epochs and compared against class-specific AEs. Additionally, l-WGANs were trained for 2K epochs and evaluated for fidelity/coverage. Results showed that l-WGANs based on the multi-class AE performed similarly to class-specific ones. The study evaluated l-WGANs trained for 2K epochs using MMD-CD measurements. Results showed that l-WGANs based on multi-class AE performed similarly to class-specific ones in terms of fidelity and visual quality. Limitations were observed in decoding chairs with rare geometries and missing high-frequency details. The study evaluated l-WGANs trained for 2K epochs using MMD-CD measurements. Results showed that l-WGANs based on multi-class AE performed similarly to class-specific ones in terms of fidelity and visual quality. Limitations were observed in decoding chairs with rare geometries and missing high-frequency details. Some failure cases of the models were shown, including issues with faithfully decoding chairs with rare geometries and missing high-frequency geometric details. The r-GAN also struggled to create realistic-looking shapes for some shape classes, particularly with cars. Designing more robust raw-GANs for point clouds is suggested for future work. The study focused on designing more robust raw-GANs for point clouds, with a particular emphasis on improving the generation of cars. Training Gaussian mixture models in the latent space of an autoencoder, related to VAEs, was also discussed. The authors found that fixing the AE before training generative models yielded good results in 3D point-cloud representation learning and generation. The study presented novel architectures for 3D point-cloud representation learning and generation, showing good generalization to unseen data. The best-performing generative model was a GMM trained in the fixed latent space of an AE, suggesting that simple classic tools should not be dismissed. Further investigation on the power of simple latent GMMs compared to adversarially trained models is of significant interest. The best-performing generative model in the study was a GMM trained in the fixed latent space of an AE. The AE used for the experiments had specific encoder and decoder configurations, with batch normalization and online data augmentation applied. Training was done with CD loss and EMD for 1000 and 1100 epochs, respectively. Other AEs had different encoder configurations with a bottleneck size denoted as k. The AE was trained with batch normalization and online data augmentation, using CD loss and EMD for 1000 and 1100 epochs respectively. Different AE setups did not show significant advantages over the \"vanilla\" architecture. Adding drop-out layers resulted in worse reconstructions, while using batch-norm on the encoder only sped up training and improved generalization error. The architecture of the discriminator in the r-GAN model consists of 1D-convolutions with specific filter sizes and leaky-ReLU activation, followed by max-pooling. The generator includes FC-ReLU layers with varying numbers of neurons. Training was done with Adam optimizer, using a spherical Gaussian noise vector. The generator in the r-GAN model consists of 5 FC-ReLU layers with varying numbers of neurons, while the discriminator has 2 FC-ReLU layers. Training was done with Adam optimizer using a spherical Gaussian noise vector. The l-Wasserstein-GAN used a gradient penalty regularizer and trained the critic for 5 iterations per one iteration of the generator. The same training parameters and noise distribution were used for the classification experiments with a linear SVM. The l-Wasserstein-GAN model used a gradient penalty regularizer with \u03bb = 10 and trained the critic for 5 iterations per generator iteration. For classification experiments, a linear SVM with l2 norm penalty and balanced class weights was used. Optimization parameters can be found in Table 5. Training parameters of SVMs used in each dataset with each structural loss of the AE are detailed in Table 5. The reconstruction quality of CD and EMD-based AEs is compared using JSD. The AEs show the ability to generalize across different shapes, trained across all 55 object classes. The AE-EMD trained across all 55 object classes showcases its ability to encode features for different shapes. Shape editing applications involve editing parts in point clouds using vector arithmetic on the AE latent space. Structural properties of object categories can be modified using shape annotations as guidance. The shape of convertibles, chairs with armrests, and mugs without handles can be modified using shape annotations from Yi et al. Structural differences between object sub-categories can be modeled using latent representations. Objects can be transformed by changing their latent representations, as shown in Figure 8. Interpolation between different shapes is possible in the latent space representation, as depicted in Figure 9. Interpolating shapes in the latent space representation allows for morphing between structurally and topologically different shapes. By linearly interpolating between latent representations of two shapes, intermediate variants can be obtained, enabling morphing between shapes of significantly different appearance. The latent representation allows for morphing between shapes by producing intermediate variants and enabling morphing between shapes of different classes. Shape analogies can be found through linear manipulations and nearest-neighbor searching in the latent space. In this section, the authors demonstrate finding shape analogies through linear manipulations and nearest-neighbor searching in the latent space using a combination of point-cloud generators and voxel-based autoencoders. They utilized a full-GMM model with 32 centers for generation, experimenting with grid resolutions of 32^3 and 64^3. The authors utilized an AE that works with occupancy grids of 3D shapes and a full-GMM model with 32 centers for generation. They compared their results with Wu et al.'s voxel-based GANs and found that their latent AE-based GMM models outperformed the \"raw\" GAN architecture by a significant margin. For more details and quantitative results, refer to Table 7. The authors compared their latent AE-based GMM models with Wu et al.'s voxel-based GANs, showing significant performance improvement. The use of latent representations in voxel generation proved advantageous. Additionally, the fidelity of results was not significantly affected by resolution differences. The authors found that their point-cloud-based models outperformed voxel-based models in fidelity to ground truth, as measured by MMD. Voxel-based latent-space models showed a bigger coverage boost compared to MMD, likely due to how the coverage metric is computed. The coverage boost of voxel-based latent-space models compared to MMD is likely due to how the coverage metric is computed, matching all generated shapes against the ground truth regardless of quality. Voxel-based models often produce shapes with missing components, leading to poor quality matchings that artificially increase coverage. Inspection confirmed that voxel-based output covered mostly very poor quality partial shapes. The voxel-based method shows a heavier \"tail\" indicating poor quality matchings. Qualitative inspection confirmed that the covering mostly came from very poor quality partial shapes. The voxel-based models use GMMs with full covariances and different dimensional latent codes. The mesh conversion is done using the marching cubes algorithm with an iso-surface value of 0.5. The results with the point-cloud based GMM are also shown. Our volumetric models utilize GMMs with full covariances and varying dimensional latent codes. The mesh conversion is achieved through the marching cubes algorithm with an iso-surface value of 0.5. The voxel-based AEs are fully-convolutional with specific layer parameters listed. The voxel-based autoencoders are trained for 100 epochs with specific parameters. The reconstruction quality is compared to a state-of-the-art method using a 0.5 occupancy threshold. Our dense voxel-based autoencoder is compared to a state-of-the-art method using a 0.5 occupancy threshold. The reconstruction quality is measured by the intersection-over-union between input and synthesized voxel grids. The GMM-generator is evaluated against a model that memorizes training data for the chair class. The coverage/fidelity obtained by our generative models is slightly lower than the memorized sets. Our generative models, evaluated against a model that memorizes training data for the chair class, show slightly lower coverage/fidelity. Learning the structure of the underlying space enables compact representation of data and generation of novel shapes. Despite some mode collapse, our generative results demonstrate good coverage/fidelity with respect to the test set. The learned representation focuses on the structure of the underlying space, allowing for compact data representation and generation of novel shapes. Despite some mode collapse, the generative models show good fidelity. Additional comparisons with BID32 for major ShapeNet classes are provided in Tables 10, 11, and 12. In Table 10, JSD-based comparisons are provided for two models, while TAB12 shows MMD/Coverage comparisons on the test split. Generalization error of various GAN models is analyzed using JSD and MMD-CD metrics in FIG0. GMM model selection is also discussed in Figure 17, with GMMs trained on the latent space learned by an AE with EMD and a bottleneck of 128. The JSD and MMD-CD metrics are used to estimate the closeness of synthetic results to training and test distributions in various GANs. GMM model selection is discussed, showing that models with full covariance achieve smaller JSD than those with diagonal covariance. Additionally, 30 or more clusters are sufficient to minimize JSD. The 32 centers of the GMM fitted to latent codes are also shown, along with evaluation of five generators on a test-split of chair data based on minimal MMD-CD selection. The text discusses the evaluation of GAN models using JSD and MMD-CD metrics, highlighting the importance of full covariance models and a sufficient number of clusters. It also presents the results of five generators on a test-split of chair data based on minimal MMD-CD selection. The text discusses evaluating GAN models using JSD and MMD-CD metrics, emphasizing the significance of full covariance models and an adequate number of clusters. It also shows the results of five generators on a chair data test-split based on minimal MMD-CD selection. GMM-40-F represents a GMM with 40 Gaussian components with full covariances. Synthetic point-clouds are sampled for each model, three times the size of the ground truth test dataset, to measure how well they match the ground truth in terms of MMD-CD. This complements a previous evaluation measure shown in FIG2."
}