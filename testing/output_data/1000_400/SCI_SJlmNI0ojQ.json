{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models have gained popularity for their ease of training, scalability to large datasets, and integration with downstream tasks like spoken language understanding. This paper describes methods to create contextual acoustic word embeddings from a supervised sequence-to-sequence model, showing competitive performance on standard sentence evaluation tasks and spoken language understanding. Our embeddings, created from a supervised sequence-to-sequence model, perform well on sentence evaluation tasks and spoken language understanding. They match the performance of text-based embeddings in a spoken language understanding task, showcasing their effectiveness in processing variable length data like words or sentences. The research community has seen the rise of popular methods like word2vec, GLoVE, CoVe, and ELMo for natural language processing tasks. In speech recognition, similar advancements have been made in creating fixed-size representations for short-term sequences. In the natural language processing and speech recognition communities, popular methods like word2vec, GLoVE, CoVe, and ELMo are used for various tasks. Prior work in learning word representations from variable length acoustic frames involved aligning speech and text or chunking input speech into fixed-length segments. Our work focuses on constructing individual acoustic word embeddings grounded in utterance-level acoustics, unlike previous techniques that ignore specific audio context. We present methods for obtaining these embeddings from an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word speech recognition. Our work focuses on constructing individual acoustic word embeddings from an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word speech recognition. This allows us to automatically segment and classify input speech into individual words without the need for pre-defined boundaries. Our contextual acoustic word embeddings are shown to be useful in non-transcription downstream tasks. In this paper, the authors demonstrate the usability of attention for aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE). They show that CAWE is competitive with text-based word2vec embeddings and can be useful in non-transcription downstream tasks. Additionally, they highlight the utility of CAWE in a speech-based downstream task of Spoken Language Understanding. The study demonstrates the effectiveness of Contextual Acoustic Word Embeddings (CAWE) in speech recognition tasks, showing competitive performance with text-based word2vec embeddings. CAWE can be utilized for transfer learning in speech-based downstream tasks, similar to VGG in vision or CoVe in natural language understanding. The models for Acoustic-to-Word (A2W) modeling have mainly used CTC and S2S models, requiring large amounts of training data, but recent progress shows the possibility of training with smaller datasets. Recent progress in the field of Acoustic-to-Word (A2W) modeling has shown the potential to train models with smaller amounts of data and restricted vocabularies. Solutions for generating out-of-vocabulary words involve using smaller units like characters or sub-words. A study presents S2S models for large vocabulary A2W recognition with a 300-hour dataset and a vocabulary of 30,000 words. Subsequent work has improved the training of these models for large vocabulary tasks. The models for large vocabulary A2W recognition have evolved from pure-word models. Previous works have improved training for this task, with one model showing the ability to learn word boundaries without supervision. Various methods have been explored to learn acoustic word embeddings, with some using unsupervised learning and others using a supervised Convolutional Neural Network approach. Methods like BID4, BID5, BID7, BID6, BID25, and BID8 focus on learning acoustic word embeddings. Most methods use unsupervised learning, except for BID6, which uses a supervised Convolutional Neural Network with short speech frames as input. BID4 proposes an unsupervised method using a fixed context of words in the past and future. Learning text-based word embeddings is also a well-established area of research. BID4 proposes an unsupervised method to learn speech embeddings using a fixed context of words in the past and future. Learning text-based word embeddings is a rich area of research with established techniques. Our work ties A2W speech recognition model with learning contextual word embeddings from speech, similar to the Listen, Attend and Spell model. The trained machine translation model re-uses its encoder in other tasks, such as A2W speech recognition. The model consists of an encoder network, a decoder network, and an attention model. The encoder is a pyramidal BLSTM network that maps input acoustic features to higher-level features. The decoder, an LSTM network, generates targets using an attention mechanism. The decoder network in the Long Short Term Memory (BLSTM) network learns to model the output distribution over the next target conditioned on previous predictions. It uses an attention mechanism to generate targets from the input. The model follows the same setup as word-based models but learns 300 dimensional acoustic feature vectors instead of 320 dimensional ones. Our model utilizes an attention mechanism to calculate attention for the current time step, resulting in a peaky distribution. Acoustic word embeddings are obtained from an end-to-end trained speech recognition system, using hidden representations from the encoder and attention weights from the decoder. The method is similar to CoVe for text embeddings but faces challenges in aligning input speech with output words. Our method constructs contextual acoustic word embeddings using an attention mechanism to assign importance to frames in speech recognition. This approach automatically segments continuous speech into words and obtains word embeddings based on this segmentation. The process involves segmenting continuous speech into words and obtaining word embeddings using an attention mechanism. Attention weights on acoustic frames reflect their importance in classifying a word, allowing for the construction of word representations based on the hidden representations of these frames. The attention mechanism assigns importance weights to acoustic frames for word classification. The model maps words to frames with the highest attention weights, then uses attention to create acoustic word embeddings in three different ways. The attention mechanism assigns importance weights to acoustic frames for word classification, mapping words to frames with the highest attention weights. Three ways of obtaining acoustic word embeddings for a word are described: unweighted Average, attention weighted Average, and maximum attention. These techniques are referred to as Contextual Acoustic Word Embeddings (CAWE) due to the use of attention scores. The study introduces Contextual Acoustic Word Embeddings (CAWE) using attention scores to obtain hidden representations of acoustic frames with the highest attention score for a given word. Two datasets are used, Switchboard corpus and How2 BID27 dataset, with A2W achieving a word error rate of 22.2% on Switchboard. The second dataset is a 300-hour subset of the How2 BID27 dataset of instructional videos, containing 13,662 videos with 3.5 million words. The A2W achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome set. The embeddings are evaluated on 16 benchmark sentence evaluation tasks covering various aspects of text analysis. The curr_chunk discusses sentence evaluation tasks covering various aspects of text analysis, including Semantic Textual Similarity (STS), classification tasks like Movie Review (MR) and sentiment analysis (SST), entailment and semantic relatedness using the SICK dataset, and paraphrase detection. The tasks measure correlation between embedding-based similarity and human scores, with scores ranging from [-1, 1]. Training details mention the use of logistic regression for classification in downstream evaluations. The curr_chunk discusses the high correlation between different tasks measured by test classification accuracies using the SentEval toolkit BID26. Logistic regression is used for classification in downstream evaluations, simplifying the model complexity. CAWE-M outperforms U-AVG and CAWE-W on Switchboard and How2 datasets for STS tasks, with varying performance on classification tasks. From the results in TAB1, CAWE-M outperforms U-AVG and CAWE-W on Switchboard and How2 datasets for STS tasks by 34% and 13%, and 33.9% and 12% respectively. CAWE-W usually performs worse than CAWE-M due to noisy estimation of word embeddings. U-AVG performs worse than CAWE-W on STS and SICK-R tasks. The text discusses the comparison between CAWE-M, U-AVG, and CAWE-W on various tasks. It mentions that U-AVG performs worse than CAWE-W on STS and SICK-R tasks due to its construction process. The datasets used for downstream tasks are described, along with training details comparing embeddings from a speech recognition model and word2vec model. The A2W speech recognition model is criticized for not capturing the entire vocabulary. The text discusses the performance of CAWE compared to word2vec CBOW, highlighting that CAWE outperforms word2vec on 10 out of 16 tasks when acoustic embeddings are concatenated with text embeddings. Despite the A2W speech recognition model's limited vocabulary coverage, CAWE shows competitive results. The study compares CAWE with word2vec CBOW, showing that CAWE outperforms word2vec on 10 out of 16 tasks when acoustic embeddings are combined with text embeddings. The gains are more significant in spontaneous conversational speech datasets like Switchboard compared to planned instructional speech datasets like How2. Additionally, CAWE is evaluated on the ATIS dataset for Spoken Language Understanding. The study evaluates CAWE on the ATIS dataset for Spoken Language Understanding, using a simple RNN-based model architecture trained for 10 epochs with RMSProp. Our RNN-based model architecture includes an embedding layer, a single layer RNN variant (Simple RNN, GRU), a dense layer, and softmax. The model is trained for 10 epochs with RMSProp and tested 3 times with different seed values. Results show that speech-based word embeddings can perform as well as text-based embeddings on the ATIS dataset. Comparing CAWE-M, CAWE-W, and CBOW embeddings, we demonstrate the effectiveness of speech-based embeddings in this downstream task. In a speech-based downstream task, contextual acoustic word embeddings were found to perform comparably to text-based word embeddings. The study compared CAWE-M, CAWE-W, and CBOW embeddings, showing the utility of speech-based embeddings. The method involved learning contextual acoustic word embeddings from a sequence-to-sequence acoustic-to-word speech recognition model. Attention played a crucial role in constructing these embeddings, which were competitive with word2vec (CBOW) text embeddings. Two variants of contextual acoustic word embeddings outperformed the simple unweighted average method by up to 34% on semantic textual similarity tasks. These embeddings also matched the performance of text-based embeddings in spoken language understanding. Contextual acoustic word embeddings were found to be highly competitive with word2vec (CBOW) text embeddings, outperforming the simple unweighted average method by up to 34% on semantic textual similarity tasks. These embeddings also matched the performance of text-based embeddings in spoken language understanding, indicating their potential as pre-trained models for speech-based downstream tasks. Future work will focus on scaling the model to larger corpora and vocabularies, and comparing with non-contextual acoustic word embedding methods."
}