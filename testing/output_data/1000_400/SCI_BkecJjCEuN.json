{
    "title": "BkecJjCEuN",
    "content": "The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask and self-supervised learning. Training an end-to-end audio feature extractor based on WaveNet, the study demonstrates improved performance in supervised classification tasks by up to 6% through simultaneous training with self-supervised tasks. Incorporating data augmentation further enhances the multitask setting. In scenarios with limited labeled training data, one can improve supervised classification tasks by up to 6% through simultaneous training with self-supervised tasks. Incorporating data augmentation leads to further performance gains in deep neural networks for modeling auditory data. In scenarios with limited labeled training data, incorporating self-supervised audio-related tasks during model training can significantly improve performance. This approach aims to generalize better by utilizing auxiliary self-supervised auditory tasks alongside supervised tasks, demonstrating successful identification of appropriate tasks and their joint training to enhance model performance. The use of WaveNet as a feature extractor is also highlighted in this research. In this paper, the successful identification of self-supervised audio tasks and their joint training with supervised tasks using WaveNet as a feature extractor is demonstrated to significantly improve performance on audio-related tasks such as audio tagging, speaker identification, and speech command recognition. Leveraging unlabeled data and data augmentation techniques further enhance model performance. The framework explores leveraging unlabeled data to improve performance on supervised classification tasks like audio tagging, speaker identification, and speech command recognition. It also shows that self-supervised tasks can be used for pre-training and transfer learning to enhance performance. The text suggests that a single model can learn multiple tasks if they are related with some underlying structure. Models trained for many tasks may uncover this structure, improving single-task performance with less data. Multitask learning aims to create a general-purpose representation. The challenge lies in obtaining cleanly labeled datasets for breakthroughs in deep learning. Self-supervised learning is a promising solution to label scarcity, leveraging unlabeled data for tasks like image completion, colorization, and motion segmentation. While self-supervision has been successful in the visual domain, little previous work has explored its potential in the audio domain. An end-to-end audio processing network was implemented to find a common embedding of the acoustic waveform within a network modeled after the WaveNet architecture. The text discusses the implementation of an end-to-end audio processing network based on the WaveNet architecture. The network utilizes self-supervision and consists of a trunk network with 3 blocks of 6 dilation stacks each. Each dilation stack includes a gate and filter module with 64 convolutional units per module. The trunk and head networks are trained jointly for various experiments involving supervised and self-supervised tasks. The WaveNet trunk consists of 3 blocks of 6 dilation stacks, each with a gate and filter module. The network has an effective receptive field length of 190 samples. It was tested on audio tagging, speaker identification, and speech command recognition tasks, trained with labeled and unlabeled data. Details of the tasks are provided in the appendix. The audio tagging task is part of the WaveNet trunk setup, trained on the FSDKaggle2018 dataset. Each audio segment is cropped to 2 seconds and padded with zeros if needed. The output is averaged across time to produce a single vector for the entire sequence, which then feeds into a fully-connected layer with 512 units. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Individual clips are sourced from interviews with celebrities in various settings, with one interview held out for a test set. Each audio segment is cropped to 2 seconds before being fed into the network for training. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Data is sourced from interviews with celebrities, with one interview held out for testing. Audio segments are cropped to 2 seconds and normalized before being fed into the network. The head architecture includes a global average pooling layer, 2-layer perceptron, batch normalization, ReLU nonlinearity, and softmax layer with cross-entropy loss. The speech command recognition task is trained on the Speech Commands dataset with 65,000 utterances. The speech command recognition head architecture features a global average pooling layer, 2-layer perceptron with 1024 units per layer, batch normalization, ReLU nonlinearity, and a softmax layer with cross-entropy loss. The dataset consists of 65,000 utterances of 30 short words in WAVE format files. There are 12 categories, with 10 words classified and the rest as unknown or silence. The head includes three 1D convolutions with batch normalization, dropout, ReLU nonlinearity, and final softmax layer for evaluation. The speech command recognition head consists of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The convolution layers have widths of 100, 50, and 25 with strides of 16, 8, and 4. The output is passed to a final softmax layer and evaluated using cross-entropy loss. Self-supervised tasks like next-step prediction, noise reduction, and upsampling were implemented and trained on both main task data and unlabeled data from the Librispeech dataset. The auxiliary tasks in the multitask framework were trained on unlabeled data from the Librispeech dataset. They share a common head architecture with convolutional layers and a regression-type loss function. The focus was on using waveform inputs for audio processing instead of spectrograms to allow for a broader range of tasks. State-of-the-art baseline models for audio processing tasks focus on waveform inputs rather than high-level feature representations like spectrograms. Models trained on raw waveforms offer more flexibility but may vary in network architectures, limiting information gained from self-supervised tasks. Emphasizing multitask learning can improve performance compared to single task baselines trained on raw audio. Multitask learning can improve performance compared to single task baselines trained on raw audio. Joint training with three self-supervised tasks proved beneficial for supervised tasks. Multitask training for audio tagging improved MAP@3 score and top-1 classification rate. Incorporating larger versions of Librispeech into training regimen was investigated for effects. Incorporating larger versions of Librispeech into training regimen showed improved performance on supervised tasks. Multitask training for audio tagging task resulted in increased MAP@3 score and top-1 classification rate. Additional unlabeled data led to further enhancements in performance metrics, with up to a .056 increase in MAP@3 with 500 hours of extra data. Similar trends were observed when swapping tasks, such as speech command classification and speaker identification, with increasing amounts of unlabeled data. Multitask learning improved performance on supervised tasks like speech command classification and speaker identification. Additional unlabeled data enhanced performance metrics, with speech command classification reaching 93.78% and speaker identification at 75.22%. Multitask learning showed improvements without needing extra labeled data, compared to data augmentation techniques. Multitask learning improved performance on supervised tasks without additional labeled data. Comparing with data augmentation techniques, pitch-shift augmentation showed a MAP@3 increase of .066, noise augmentation had a smaller increase of .024. Training with both pitch-shift augmentation and self-supervised tasks yielded the highest increase of .089 in MAP@3. Transfer learning was considered in the context of self-supervised tasks trained on unlabeled data to improve label efficiency. Pre-training self-supervised tasks on unlabeled data followed by fine-tuning with a small amount of labeled data showed promising results in performance improvement. Transfer learning was explored by pre-training self-supervised tasks on unlabeled data and then fine-tuning with a small amount of labeled data, showing improved performance. The study favored transfer learning over training all tasks simultaneously, indicating that jointly training supervised tasks with self-supervised tasks using a WaveNet-based model on raw audio waveforms can lead to performance gains. The present work focuses on training audio tasks on limited labeled data by jointly training supervised tasks with self-supervised tasks using a WaveNet-based model. Improved performance scales with the quantity of unlabeled data and can supplement existing data augmentation schemes. The approach shows promise for a broad range of supervised audio tasks and raises questions about the number of auxiliary tasks a model can benefit from. Our approach to training audio tasks on limited labeled data involves jointly training supervised tasks with self-supervised tasks using a WaveNet-based model. The methodology and results suggest interesting directions for further development, such as exploring the limit on the number of auxiliary tasks a model can benefit from and extracting the representation formed by multitasking models. This exploration could enable handling a broader range of auditory tasks that require higher temporal resolutions. Our model, based on the WaveNet architecture, utilizes causal dilated convolutions to process high temporal resolution raw audio signals efficiently. This approach allows for faster training compared to RNNs and enables the handling of a broader range of auditory tasks. WaveNet models use causal dilated convolutions to process sequential inputs in parallel, making them faster to train than RNNs. The architecture consists of task-specific neural networks built on top of a task-agnostic trunk, with blocks of dilated causal convolutions. Our WaveNet trunk consists of N blocks with dilated causal convolution layers, each block labeled using b = 1, \u00b7 \u00b7 \u00b7 , N. Each layer in the trunk involves a \"residual atom\" computation producing hidden state vector h and layer output x. The first layer applies causal convolutions to raw audio waveforms X. The WaveNet trunk consists of N blocks with dilated causal convolution layers. Each block has an effective receptive field of 1 + b(2S-1), resulting in a total receptive field of \u03c4 = 1+N(2S-1). After a hyperparameter search, N = 3 blocks with S = 6 layers each were chosen, giving a total receptive field of \u03c4 = 190, equivalent to about 12 milliseconds of audio sampled at 16kHz. Each task-specific head in the experiment is a neural network that shares a trunk with other tasks. The trunk has a total receptive field of \u03c4 = 190, equivalent to 12 milliseconds of audio sampled at 16kHz. Each head processes input independently, with tasks having their own objective functions and optimizers. Supervised tasks are designated as primary, while self-supervised tasks are auxiliary. The primary task in the experiments is \"audio tagging\" for supervised classification. The experiment involves task-specific neural network heads sharing a trunk with a total receptive field of 12 milliseconds of audio. Each head has its own objective function and optimizer, with supervised tasks designated as primary and self-supervised tasks as auxiliary. The primary task is \"audio tagging\", while auxiliary tasks include \"next-step prediction\", \"noise reduction\", and \"upsampling\". The head architectures are designed to be simple to force the shared trunk to learn a representation suitable for multiple audio tasks. The head architectures are designed to be simple, using few layers to solve tasks and force the shared trunk to learn a representation suitable for multiple audio tasks. The next-step prediction task involves predicting the next value in a sequence of audio frames, allowing for large training datasets from unlabeled audio data. The prediction head consists of a 2-layer stack of 1x1 convolutional layers with ReLU nonlinearities, taking in \u03c4 frames of data from the trunk and producing an output representing the model's prediction. The next-step prediction head consists of a 2-layer stack of 1x1 convolutional layers with ReLU nonlinearities, taking in \u03c4 frames of data from the trunk and producing an output representing the model's prediction for the next frame of audio in the sequence. The head treats this as a regression problem, using mean squared error as the loss function. The original WaveNet implementation treated next-step prediction as a classification problem. The noise reduction task involves training the model to predict clean audio samples from noisy ones by treating noise as an additive random process on top of the true signal. The model is trained to predict the clean sample given a window of noisy samples. The model for noise reduction in audio involves predicting clean samples from noisy ones by treating noise as an additive random process on top of the true signal. The model is trained to minimize a smoothed L1 loss between the clean and noisy versions of the waveform inputs. The noise reduction head is structured similarly to the next-step head and aims to minimize a smoothed L1 loss between clean and noisy waveform inputs. This loss is computed for each frame and aggregated over frames to obtain the total loss. The smooth L1 loss is preferred for stability in denoising tasks over mean squared error. Additionally, an unsupervised upsampling task can be created by downsampling the audio source, with the downsampled signal as input and the original source as the target. Upsampling is akin to the \"super-resolution\" task in computer vision. The upsampling task involves downsampling the original audio to 4 kHz and repeating every time-point 4 times to mimic a 16 kHz sample rate. The network infers high frequency information lost during the transform, using a structure similar to next-step prediction and noise reduction tasks. A smooth L1 loss function is used to compare the estimated upsampled audio with the original. The upsampling task involves using a structure similar to next-step prediction and noise reduction tasks. A smooth L1 loss function is used to compare the estimated upsampled audio with the original. Raw audio waveform inputs are taken from the FSDKaggle2018 and Librispeech datasets, cropped to two seconds, downsampled to 16 kHz, and normalized. For the noise-reduction task, noisy inputs are obtained by adding noise sampled from ChiME3 datasets at a randomly chosen SNR. For noise reduction task, noisy inputs are obtained by adding noise sampled from ChiME3 datasets at a randomly chosen SNR. Inputs are scaled to lie in the interval [-1, 1]. Hyperparameter search was done over the number of blocks in the trunk, layers per block, layers and units of the main task head, and learning rate. Performance and training characteristics of the network were evaluated. The study involved hyperparameter search for the noise reduction task, exploring different values for the number of blocks, layers per block, and learning rate. The network's performance was found to be largely unaffected by specific architecture specifications. Additionally, the depth and width of each auxiliary task head were optimized by pairing tasks with the main task to achieve the best performance. The model was trained on all tasks simultaneously. The hyperparameters for the model were chosen based on performance on both the main task and auxiliary tasks. Training involved a forward pass for each task, computing the loss function, and calculating gradients using a weighted sum of losses. The \"Adam\" optimizer was used with specific parameters, and the learning rate was decayed every 5 epochs. A batch size of 48 was used consistently. In our experiments, we used the \"Adam\" optimizer with specific parameters and a batch size of 48. Noise reduction and upsampling tasks required separate forward propagation. All important model parameters can be found in TAB3."
}