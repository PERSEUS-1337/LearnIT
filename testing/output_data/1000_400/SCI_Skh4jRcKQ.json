{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified in this paper by analyzing the minimization of training loss in the negative direction of the coarse gradient. In this paper, the theoretical justification of the concept of STE is provided by analyzing the learning process of a two-linear-layer network with binarized ReLU activation and Gaussian input data. The STE-modified chain rule introduces a coarse gradient, which, when properly chosen, correlates positively with the population gradient and leads to a descent direction for minimizing the population loss. A poor choice of STE can result in instability near certain local minima, as demonstrated in CIFAR-10 experiments. Deep neural networks have achieved success in various machine learning applications. However, deploying DNNs typically requires significant memory and computational resources. To address this, recent efforts have focused on training coarsely quantized DNNs to achieve memory savings and energy efficiency during inference. The associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Poor choice of STE can lead to instability near certain local minima, as shown in CIFAR-10 experiments. Recent efforts have focused on training coarsely quantized DNNs to achieve memory savings and energy efficiency during inference. Weight quantization of DNNs is a challenging optimization problem that aims to minimize a nonconvex empirical risk function subject to discrete set-constraints. Various studies have been conducted on weight quantization in DNNs to maintain performance while reducing memory storage and computational requirements. Weight quantization of DNNs is a challenging optimization problem that aims to minimize a nonconvex empirical risk function subject to discrete set-constraints. The gradient in training activation quantized DNNs is almost everywhere zero, requiring a non-trivial search direction using the straight-through estimator (STE) to modify the chain rule. The most effective way to address the issue of zero gradients in training activation quantized DNNs is to use the straight-through estimator (STE) to modify the chain rule. This approach, proposed by Bengio et al. (2013), involves replacing the zero derivative with a surrogate in the backward pass. Additionally, Friesen & Domingos (2017) introduced the target propagation algorithm for learning binary activated networks via convex combinatorial optimization. The concept of STE can be traced back to the perceptron algorithm by Rosenblatt in the 1950s. The straight-through estimator (STE) is a method for training binary activated networks via convex combinatorial optimization. It originated from the perceptron algorithm by Rosenblatt in the 1950s and has been extended to train multi-layer networks with binary activations. Bengio et al. (2013) proposed a variant of STE using the derivative of the sigmoid function. The straight-through estimator (STE) is a method for training binary activated networks via convex combinatorial optimization. It has been extended to train multi-layer networks with binary activations, using variants such as the derivative of the sigmoid function proposed by Bengio et al. (2013). Hubara et al. (2016) substituted the derivative of the signum activation function with 1 {|x|\u22641} in the backward pass, known as the saturated STE. Later, the idea of STE was applied to training DNN with general quantized ReLU activations by various researchers. Despite empirical success, there is limited theoretical understanding of STE. Despite the empirical success of the straight-through estimator (STE) in training DNN with stair-case activations, there is limited theoretical understanding of it. Various researchers have employed STE in training DNN with general quantized ReLU activations, but there are scenarios where certain layers are not desirable for back-propagation, as discussed by recent studies. In the context of training DNN with general quantized ReLU activations, the straight-through estimator (STE) is used in the backward pass. Recent studies have highlighted scenarios where certain layers are not ideal for back-propagation. Researchers have proposed alternative methods like implicit weighted nonlocal Laplacian layers and backward pass differentiable approximations to improve generalization accuracy and break adversarial defenses. The \"gradient\" of the loss function with respect to weight variables is referred to as coarse gradient when using the STE-modified chain rule. The paper discusses the use of the straight-through estimator (STE) in training quantized ReLU nets and explores the optimization perspective and theoretical aspects of STE. It questions the choice of STE, its uniqueness, and its effectiveness in minimizing training loss. The paper explores the optimization and theoretical aspects of the straight-through estimator (STE) in training quantized ReLU nets. It considers different STEs for learning a two-linear-layer network with binary activation and Gaussian data, proving that proper choices of STE lead to descent training algorithms. Specifically, the negative expected coarse gradients based on STEs of vanilla and clipped ReLUs are descent directions for minimizing population loss. The paper examines the optimization of the straight-through estimator (STE) in training quantized ReLU nets. It proves that proper choices of STE result in descent training algorithms, with negative expected coarse gradients from vanilla and clipped ReLUs being descent directions for minimizing population loss. Empirical results show that clipped ReLU STE performs best on deeper networks like VGG-11 and ResNet-20. The empirical performances of three STEs on MNIST and CIFAR-10 classifications with general quantized ReLU are analyzed. Clipped ReLU STE is found to be the best for deeper networks like VGG-11 and ResNet-20. Training with identity or ReLU STE can lead to instability and lower generalization accuracy, indicating poor STEs generate coarse gradients. Convergence guarantees for the identity STE are discussed in relation to the perceptron and Convertron algorithms. The identity STE is not suitable for networks with two trainable layers, as it leads to instability and lower generalization accuracy. Convertron algorithm makes weaker assumptions than the identity STE, but its results do not apply to deeper networks. The monotonicity of quantized activation functions plays a role in coarse gradient descent, with clipped ReLU STE being the best choice for deeper networks. The identity STE is not suitable for networks with two trainable layers, as it leads to instability and lower generalization accuracy. Convertron algorithm makes weaker assumptions than the identity STE, but its results do not apply to deeper networks. The monotonicity of quantized activation functions plays a role in coarse gradient descent, with clipped ReLU STE being the best choice for deeper networks. Analyses of different STEs are extended to other scenarios, with a focus on the role of quantized activation functions in gradient descent. In section 3, the empirical performances of different STEs in 2-bit and 4-bit activation quantization are compared, highlighting the instability phenomena associated with poor STEs in CIFAR experiments. Technical proofs and some figures are deferred to the appendix. Notations include Euclidean norm, spectral norm, identity matrix, inner product, and Hadamard product. The model considered outputs predictions for input Z using trainable weights w and v. The model uses trainable weights w and v to make predictions for input Z, with a convolutional layer and a classifier. The label is generated using a squared sample loss and a binary activation function. The model uses trainable weights w and v for predictions on input Z, with a convolutional layer and a classifier. The label is generated using a squared sample loss and a binary activation function \u03c3(x) = 1 {x>0}. The entries of Z are i.i.d. sampled from a Gaussian distribution. The learning task is framed as a population loss minimization problem. The gradient of the objective function is not readily available. The learning task is framed as a population loss minimization problem with trainable weights w and v. The gradient of the objective function is not directly accessible, so the idea of Straight-Through Estimator (STE) is introduced to replace the zero component with a non-trivial function \u00b5 for back-propagation. The Straight-Through Estimator (STE) replaces the zero component with a non-trivial function \u00b5 for back-propagation in training. Using STE to train a two-linear-layer CNN with binary activation leads to coarse gradient descent. Preliminaries about the population loss function landscape are discussed, including defining the angle between weights w and w*. The population loss function landscape is analyzed, with the angle between weights w and w* defined. The partial gradients of the population loss function with respect to v and w are discussed, along with the conditions for local minimizers in the model. The partial gradients of the population loss function with respect to v and w are discussed, along with conditions for local minimizers in the model. Stationary points and non-differentiable points are identified as potential minimizers, with a focus on saddle points and spurious local minimizers. The population gradient is proven to be Lipschitz continuous on bounded domains. The model has saddle points and spurious local minimizers. The population gradient is Lipschitz continuous on bounded domains. The focus is on the complex case with both saddle points and spurious local minimizers. The behavior of the coarse gradient descent algorithm is analyzed using different derivatives. The coarse gradient descent algorithm behavior is analyzed in the complex case with saddle points and spurious local minimizers. Using derivatives of vanilla or clipped ReLUs leads to convergence to a critical point, while the identity function does not. The objective sequence decreases monotonically, converging to a saddle point or local minimizer of the population loss minimization under certain conditions. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples. With a small learning rate, the objective sequence decreases monotonically towards a saddle point or local minimizer. As sample size increases, empirical loss gains smoothness and monotonicity, explaining the effectiveness of proper stochastic gradient descent with large datasets. The empirical loss descends along the negative coarse gradient direction with increasing sample size, gaining smoothness and monotonicity. This explains the effectiveness of stochastic gradient descent with large datasets. The same results hold even if the Gaussian assumption on input data is weakened. The mathematical analysis for the main results shows that the empirical loss decreases with larger sample sizes, following the negative coarse gradient direction. This supports the effectiveness of stochastic gradient descent with large datasets, even without a Gaussian assumption on input data. Lemma 5 states that the coarse gradient has a non-negative correlation with the population gradient, forming a descent direction for minimizing the loss. If certain conditions are met, there exists a constant A_relu ensuring descent behavior. The significance of this estimate guarantees the descent property of Algorithm 1. Lemma 6 highlights the significance of estimate (12) in ensuring the descent property of Algorithm 1. When Algorithm 1 converges using ReLU STE, it only converges to a critical point of the population loss function. For the STE using clipped ReLU, similar results to Lemmas 5 and 6 apply. Lemma 7 discusses the correlation between coarse and true gradients in clipped ReLU STE, emphasizing their behavior at critical points. The text discusses the behavior of gradients at critical points in clipped ReLU STE. Lemma 8 states that the gradients vanish simultaneously at saddle points when Algorithm 1 converges. However, Lemma 9 shows that the coarse gradient derived from the identity STE does not vanish at local minima, potentially preventing Algorithm 1 from converging there. Lemma 9 highlights that the coarse gradient from the identity STE does not vanish at local minima, hindering Algorithm 1 from converging. If 1/mv* = 0, the coarse gradient descent will not converge near spurious minimizers with \u03b8(w, w*) = \u03c0. Lemma 9 suggests that coarse gradient descent may not converge near spurious minimizers with \u03b8(w, w*) = \u03c0 if 1/mv* = 0. The positive correlation implied by Lemma 10 may lead iterates towards a local minimizer initially, but as they approach it, the training loss may increase due to instability. The comparison of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations shows differences in empirical performance. In this section, the performances of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations are compared. The clipped ReLU is expected to be the best performer as it closely approximates the original quantized ReLU. The instability issue of using an improper STE in training algorithms is also discussed. The resolution \u03b1 for quantized ReLU must be carefully chosen to maintain accuracy, and a modified batch normalization layer is used for this purpose. In experiments, weights are kept float and resolution \u03b1 for quantized ReLU is crucial for accuracy. A modified batch normalization layer is used without scale and shift to approximate a unit Gaussian distribution. The best \u03b1 is pre-computed using Lloyd's algorithm applied to simulated data. This \u03b1 is fixed during training. Batch normalization is added before each activation layer in LeNet-5. The quantization approach used is HWGQ with uniform quantization. The study focuses on using a modified batch normalization layer and quantization approach for training neural networks like LeNet-5, VGG-11, and ResNet-20. The resolution \u03b1 for quantized ReLU is determined beforehand and fixed during training. Stochastic gradient descent with momentum is used as the optimizer, and training epochs vary for different networks. Experimental results are presented in Table 1, showing training losses and validation accuracies. The study compares different quantization approaches for training neural networks like LeNet-5, VGG-11, and ResNet-20 on CIFAR-10. Results show that the derivative of clipped ReLU performs the best, followed by vanilla ReLU and then the identity function. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU shows comparable performance on shallow networks like LeNet-5. The instability issue with the identity function is demonstrated on ResNet-20 with 4-bit activations. The study compares different quantization approaches for training neural networks on CIFAR-10. Results show that clipped ReLU performs the best, followed by vanilla ReLU and then the identity function. The instability issue with the identity function is demonstrated on ResNet-20 with 4-bit activations. The coarse gradient descent algorithms using vanilla and clipped ReLUs converge to minima with higher accuracies compared to the identity function. The study compares different quantization approaches for training neural networks on CIFAR-10. Results show that clipped ReLU performs the best, followed by vanilla ReLU and then the identity function. Training with the identity STE leads to a worse minimum due to the coarse gradient not vanishing at good minima. Similarly, the poor performance of ReLU STE on 2-bit activated ResNet-20 is also due to instability at good minima. The training using the identity STE leads to a worse minimum as the coarse gradient does not vanish at good minima. Similarly, the poor performance of ReLU STE on 2-bit activated ResNet-20 is also due to instability at good minima. The study provides theoretical justification for the concept of STE and considers three STEs for learning a two-linear-layer CNN. The coarse gradient descent using the identity STE is repelled, with a learning rate set to 10^-5 until epoch 20. Theoretical justification is provided for STE, considering derivatives of identity, vanilla ReLU, and clipped ReLU for learning a two-linear-layer CNN with binary activation. Explicit formulas for expected coarse gradients are derived, showing that negative expected coarse gradients based on ReLUs are descent directions for minimizing population loss, while identity STE generates incompatible coarse gradients. Instability issues were confirmed in CIFAR experiments. Future work aims to understand coarse gradient descent for large-scale optimization problems. In CIFAR experiments, improper choices of the identity STE led to instability issues. Future work aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. When using the ReLU STE with a 10^-5 learning rate on ResNet-20, the coarse gradient descent was unstable, resulting in increasing classification and training errors. In CIFAR experiments, using ReLU STE with a 10^-5 learning rate on ResNet-20 led to instability issues, causing classification and training errors to increase. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors. The text discusses the polar representation of Gaussian random variables and angles between vectors. Lemmas 12, 13, and 14 provide mathematical proofs related to Gaussian random vectors and angles. Proof of Lemma 13 involves inequalities related to sin and cos functions, while Lemma 14 discusses the Cauchy-Schwarz inequality and projections onto complement spaces. Lemma 1 states that if w = 0, the population loss f(v, w) is determined. Lemma 2 shows the partial gradients of f(v, w) with respect to v and w when w = 0 and the angle between w and w* is between 0 and \u03c0. The proof involves calculations and inequalities related to the angles and gradients. The text discusses the optimization process for determining saddle points in a mathematical model. It involves analyzing the objective function, Hessian matrix, and perturbed objective value to check for local optimality. The calculations and inequalities related to angles and gradients are used to prove the results. The Hessian matrix is checked for indefiniteness to identify saddle points. Perturbed objective values are analyzed to determine local optimality, with calculations involving angles and gradients to prove results. The text discusses proving claims using perturbed objective values, angles, and gradients. Lemmas are used to establish Lipschitz constants and expected gradients with respect to variables v and w. The Hessian matrix is utilized to identify saddle points for local optimality. The text discusses proving claims using perturbed objective values, angles, and gradients. Lemmas are used to establish Lipschitz constants and expected gradients with respect to variables v and w. The Hessian matrix is utilized to identify saddle points for local optimality. Let \u00b5(x) = max{x, 0}. Proof of Lemma 4 shows the expected coarse gradient w.r.t. w. Lemma 5 states conditions for inner product between expected coarse and true gradients w.r.t. w. Lemma 5 establishes conditions for the inner product between expected coarse and true gradients with respect to variable w. The proof involves utilizing Lemmas 2 and 4, leading to the identification of saddle points satisfying certain criteria. Additionally, Lemma 6 and Lemma 7 provide further insights into the relationships between variables v and w in the context of the perturbed objective values and angles. Lemma 7 discusses the inner product between expected coarse and true gradients with respect to variable w, under certain conditions. It involves computing E Z g crelu (v, w; Z) and establishing relationships between variables v and w. Proof of Lemma 7 involves computing E Z g crelu (v, w; Z) and establishing relationships between variables v and w. The proof utilizes various formulas and the Cauchy-Schwarz inequality to show that certain functions are uniformly bounded. Lemma 9 states that the expected coarse partial gradient with respect to w is derived using specific identities and formulas. Lemma 10 discusses the inner product between expected coarse and true gradients when w is non-zero and within a certain range of \u03b8."
}