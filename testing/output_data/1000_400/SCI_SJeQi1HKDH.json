{
    "title": "SJeQi1HKDH",
    "content": "In this work, the social influence is integrated into reinforcement learning to enable agents to learn from both the environment and peers. The Interior Policy Differentiation (IPD) algorithm encourages agents to develop unique policies while solving tasks, resulting in improved performance and diverse behaviors. This approach is inspired by cognition and animal studies. The Interior Policy Differentiation (IPD) algorithm, inspired by cognition and animal studies, aims to improve performance and promote diverse behaviors in reinforcement learning. Previous works have focused on encouraging behavioral diversity in RL through interactive environments and diverse skill development. In reinforcement learning, promoting behavioral diversity is crucial for species evolution. Two approaches include designing rich environments for agents to learn different skills and motivating agents to explore beyond maximizing rewards. Previous works have focused on these approaches, but final agent performance is not guaranteed. This work addresses the topic of increasing behavioral diversity in RL. In reinforcement learning, promoting behavioral diversity is essential for species evolution. One approach is to design rich environments for agents to learn various skills, while another is to motivate agents to explore beyond reward maximization. This work focuses on increasing behavioral diversity in RL by introducing the concept of social influence, where agents differentiate their actions to be distinct from others. The concept of social influence in reinforcement learning is introduced to promote behavioral diversity. Agents are motivated to differentiate their actions from others, leading to a constrained optimization problem. A policy distance metric is defined to compare agent similarity, and an optimization constraint is developed for immediate feedback in the learning process. Interior Policy Differentiation (IPD) is proposed as a novel method in this work. In this work, a novel method called Interior Policy Differentiation (IPD) is proposed as a solution for a constrained policy optimization problem. The method introduces an additional constraint to encourage agents to perform well in tasks while taking different actions from other agents. The approach is benchmarked on locomotion tasks, demonstrating the ability to learn diverse and effective policies. The Variational Information Maximizing Exploration (VIME) method and curiosity-driven methods propose intrinsic rewards to encourage exploration in contemporary RL algorithms. In VIME, intrinsic rewards based on information gains are added to RL algorithms to promote exploration. Various methods like Random Network Distillation and Competitive Experience Replay define intrinsic rewards differently to tackle sparse reward problems. The Task-Novelty Bisector (TNB) learning method aims to optimize the trade-off between external rewards and intrinsic rewards by updating the policy in the direction of the angular bisector of the two. The TNB learning method optimizes the trade-off between external and intrinsic rewards by updating the policy in the direction of the angular bisector of the two gradients. However, the joint optimization foundation is not solid, requiring additional computation expenses for creating an extra intrinsic reward function and evaluating novelty. DPPO method by Heess et al. enables agents to learn complex locomotion skills in rich environments. The DPPO method introduced by Heess et al. enables agents to learn complex locomotion skills in diverse environments, showcasing impressive and effective results in navigating terrains and obstacles. Different RL algorithms may converge to varying policies for the same task, with policy gradient-based algorithms tending to reach similar local optima. The research explores how rich environments can influence locomotion behaviors in RL. Different algorithms may lead to diverse policies, with policy gradient methods converging to similar optima. The focus is on learning various policies with a single algorithm to avoid local optima. Kurutach et al. maintain model uncertainty using deep neural networks to promote behavioral diversity in RL. In order to avoid local optima, a learning algorithm is used to maintain model uncertainty through an ensemble of deep neural networks. A metric is defined to measure policy differences, denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}. The metric should satisfy identity, symmetry, and triangle inequality properties in a metric space (M, d). A metric space is defined as an ordered pair (M, d) where d is a metric on M, satisfying identity, symmetry, and triangle inequality properties. The Total Variance Divergence is used to measure policy distance, extending to continuous spaces. The factor 1/2 in the distance calculation is omitted for conciseness. The goal is to motivate reinforcement learning with social uniqueness. The Total Variance Divergence is used to measure policy distance in a metric space. It can be extended to continuous state and action spaces. The factor 1/2 in the distance calculation is omitted for conciseness. The goal is to maximize the uniqueness of a new policy by sampling from the domain of the probability distribution. In continuous state spaces, sampling from the probability distribution \u03c1(s) can be challenging due to the need for efficient sample collection. To address this, we propose approximating \u03c1(s|s \u223c \u03b8) to improve sample efficiency, under the condition that the domain of possible states is similar across different policies. This approximation allows for the use of \u03c1(s|s \u223c \u03b8) as the choice of probability distribution, maintaining the properties outlined in Definition 1. The text discusses approximating the probability distribution \u03c1(s|s \u223c \u03b8) to improve sample efficiency in continuous state spaces, under the condition that the domain of possible states is similar across different policies. By adding noise on \u03b8 and ensuring sufficient exploration, the objective function of policy differentiation can be optimized. The text discusses maximizing the expectation of cumulative rewards in traditional RL, using unbiased single trajectory estimation for \u03c1 \u03b8 (s), and developing an efficient learning algorithm. In traditional RL, the objective is to maximize the expectation of cumulative rewards by developing an efficient learning algorithm. Previous approaches consider a weighted sum of rewards from the primal task and intrinsic rewards, such as uniqueness rewards, with a sensitivity to the weight parameter \u03b1. In traditional RL, the objective is to maximize the expectation of cumulative rewards by considering a weighted sum of rewards from the primal task and intrinsic rewards, like uniqueness rewards, with sensitivity to the weight parameter \u03b1. The selection of \u03b1 and the formulation of intrinsic reward r int can significantly impact results. A trade-off exists in choosing \u03b1, as a large \u03b1 may undermine intrinsic reward contribution while a small \u03b1 could ignore the importance of the reward, leading to agent failure in solving the primal task. To address these issues, inspiration is drawn from the passive motivation of social uniqueness, treating it more as a constraint than an additional target. To address the trade-off in selecting the weight parameter \u03b1 in traditional RL, the approach shifts towards treating social uniqueness as a constraint rather than an additional target. This transformation converts the multi-objective optimization problem into a constrained optimization problem, introducing a threshold for minimal permitted uniqueness. The penalty method replaces the constrained optimization problem with a penalty term and coefficient, with the challenge lying in the selection of \u03b1. The selection of r 0 will be discussed further in Appendix D. Eq.(6) is an optimization penalty method replacing the constrained problem in Eq.(7) with penalty term r int and coefficient 1\u2212\u03b1 \u03b1 > 0. Zhang et al. (2019) addressed the challenge using Task Novel Bisector (TNB) in Feasible Direction Methods (FDMs). Our approach proposes solving Eq.(7) using Interior Point Methods (IPMs). Vanilla IPMs reformulate the problem with an additional barrier term in the objective. In this work, we propose solving the constrained optimization problem Eq.(7) using Interior Point Methods (IPMs). Directly applying IPMs can be computationally challenging and numerically unstable, especially when \u03b1 is small. However, in our proposed RL paradigm, where an agent's behavior is influenced by its peers, a more natural approach can be used by bounding the collected transitions in the feasible region. In the proposed RL paradigm, the learning process is based on sampled transitions, and the feasible region is bounded by terminating new agents that step outside it. This ensures that all collected samples during training are inside the feasible region, leading to a new policy with sufficient uniqueness without the need to consider the trade-off between intrinsic and extrinsic rewards. During training, valid samples are collected inside the feasible region to ensure uniqueness in the new policy. This eliminates the need to balance intrinsic and extrinsic rewards, making the learning process more robust. The approach, named Interior Policy Differentiation (IPD) method, is demonstrated on the MuJoCo environment in OpenAI Gym. The Interior Policy Differentiation (IPD) method is applied to the MuJoCo environment in OpenAI Gym, specifically testing on three locomotion environments. Experiments show that policies can differ by selecting different random seeds before training. The method aims to improve behavior diversity by generating uniqueness from stochasticity in training processes. In this work, the Interior Policy Differentiation (IPD) method is applied to the MuJoCo environment in OpenAI Gym to improve behavior diversity by generating uniqueness from stochasticity in training processes. The proposed method is demonstrated based on PPO (Schulman et al., 2017) and compared with TNB and weighted sum reward (WSR) approaches. The uniqueness metric is utilized directly in learning new policies without reshaping. The proposed method, Interior Policy Differentiation (IPD), aims to enhance behavior diversity in the MuJoCo environment by generating uniqueness through stochasticity in training processes. The uniqueness metric is used directly in learning new policies without reshaping. Experimental results comparing IPD with TNB and WSR approaches show promising qualitative outcomes. The proposed method, Interior Policy Differentiation (IPD), aims to enhance behavior diversity in the MuJoCo environment by generating uniqueness through stochasticity in training processes. The 1st policy is trained by ordinary PPO without social influence, while subsequent policies differ from the previous ones. Fig. 2 visualizes the motion of agents, showing acceleration and motion patterns clearly. Experimental results in Fig. 3 demonstrate the relationship between uniqueness and performance of policies. The proposed method, Interior Policy Differentiation (IPD), aims to enhance behavior diversity in the MuJoCo environment by generating uniqueness through stochasticity in training processes. The visualization in Fig. 3 shows experimental results in terms of uniqueness and performance of policies. Our method outperforms others in Hopper and HalfCheetah, while in Walker2d, both WSR and our method improve uniqueness but cannot surpass PPO in performance. Detailed comparisons on task-related rewards are presented in Table 1, with performance and reward gaining curves shown in Fig. 5 and Fig. 6. Fig. 7 in Appendix C provides more detailed results on uniqueness. The comparison of task-related rewards is presented in Table 1, with performance and reward gaining curves shown in Fig. 5 and Fig. 6. Fig. 7 in Appendix C provides more detailed results on uniqueness and success rate metrics for different approaches. Our method consistently outperforms the baseline in training, ensuring reliable performance improvements in the Hopper and HalfCheetah environments. Our method shows noticeable performance improvements in the Hopper and HalfCheetah environments by preventing policies from falling into the same local minimum and encouraging exploration of different action patterns. This enhancement of traditional RL schemes allows for policies to learn from mistakes and improve performance. Our method enhances traditional RL schemes by encouraging exploration of different action patterns to improve performance in environments like HalfCheetah. The environment of HalfCheetah lacks explicit termination signals, leading to initial random actions and high control costs. Interacting with peers allows for termination signals to guide learning. During the learning process, an agent in our method will first learn to terminate itself quickly to avoid high control costs, then adapt its behavior for higher rewards. Social influence can lead to an implicit curriculum, with later policies needing to avoid previous solutions. In this work, an efficient approach is developed to motivate RL to learn diverse strategies inspired by social influence. The performance decrease under different scales of social influence is analyzed, showing a more significant impact in the Hopper environment due to its limited action space. The approach involves defining the distance between policies and introducing a method to encourage the discovery of diverse policies. The Interior Policy Differentiation (IPD) method is introduced to encourage RL to learn diverse strategies inspired by social influence. It addresses the limited action space in the Hopper environment and defines policy uniqueness. Experimental results show IPD can help agents avoid local minimum and act as implicit curriculum learning. The Interior Policy Differentiation (IPD) method encourages RL to learn diverse strategies and avoid local minimum. Experimental results show IPD can be seen as implicit curriculum learning. The uniqueness of policies is optimized in Hopper, Walker2d, and HalfCheetah environments. TNB and WSR can exceed IPD in optimizing uniqueness but may decrease task performance. Hyper-parameter tuning and reward shaping are necessary to balance trade-offs. Calculation of DTV involves the deterministic part of policies. In order to tackle the trade-off problem in RL, hyper-parameter tuning and reward shaping are essential. The calculation of DTV involves using the deterministic part of policies. Actor models in PPO use MLP with 2 hidden layers, with varying numbers of hidden units based on task requirements. In the ablation study, the choice of unit number in the second layer is detailed in Table 2, Table 3, and Fig. 8. Hidden units of 10, 64, and 256 are used for different tasks based on success rate, performance, and computation expense considerations. Training timesteps are fixed for each task, and threshold selection controls policy uniqueness in the proposed method. In the proposed method, the threshold value controls policy uniqueness, affecting agent behavior. A larger threshold leads to more distinct actions but may result in poorer performance. Constraints are based on cumulative uniqueness rather than individual actions. Performance under different thresholds is analyzed in Fig. 9 and Table 2. In the proposed method, constraints are based on cumulative uniqueness rather than individual actions. The constraints can be applied after the first t timesteps for similar starting sequences. WSR, TNB, and IPD methods correspond to different approaches in constrained optimization problems. The proposed method uses cumulative uniqueness as constraints and applies them after the first t timesteps. WSR, TNB, and IPD methods are different approaches in constrained optimization problems. The optimization of policy is based on batches of trajectory samples and implemented with stochastic gradient descent. The Penalty Method considers constraints by putting them into a penalty term and solving the unconstrained problem iteratively. The Penalty Method addresses constraints by incorporating them into a penalty term and solving the unconstrained problem iteratively. WSR approximates the solution by choosing a fixed weight term \u03b1, which heavily influences the final outcome. The Feasible Direction Method (FDM) finds a direction that satisfies the constraints, while the TNB method selects a direction based on the bisector of gradients. The Feasible Direction Method (FDM) considers constraints by finding a direction that satisfies them. The TNB method selects a direction based on the bisector of gradients, with a fixed learning stride. Introducing a barrier term can also influence the optimization process. The TNB method introduces a barrier term to influence the optimization process. The barrier term with a small \u03b1 factor will have minimal impact on the objective but will cause it to increase rapidly as \u03b8 approaches the barrier. To address computational challenges, bounding collected transitions within the feasible region using previous trained policies can be a more natural approach. The TNB method introduces a barrier term with a small \u03b1 factor to influence optimization. To address computational challenges, bounding collected transitions within the feasible region using previous trained policies is a more natural approach. This method ensures that all valid samples collected during training are inside the feasible region, leading to a new policy with sufficient uniqueness and eliminating the need to consider trade-offs between intrinsic and extrinsic rewards. The TNB method introduces a barrier term with a small \u03b1 factor to influence optimization by bounding collected transitions within the feasible region using previous trained policies. This ensures that valid samples are inside the feasible region, leading to a new policy with sufficient uniqueness. The learning process becomes more robust, eliminating the need to consider trade-offs between intrinsic and extrinsic rewards. The pseudo code of IPD based on PPO is shown in Algorithm.1 with additions highlighted in blue."
}