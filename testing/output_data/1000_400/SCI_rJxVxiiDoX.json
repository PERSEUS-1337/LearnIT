{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. By combining quantization-aware training and weight matrix factorization, we can reduce model size and computation for small-footprint keyword spotting while maintaining performance. In this work, quantization-aware training is used to build a small-footprint low-power keyword spotting system. The approach involves training the wake word model using quantization-aware training as a final stage, enabling optimization against quantization errors. The use of 8 bit and 4 bit quantization in keyword spotting systems is common in the industry. In this work, quantization-aware training is utilized to optimize weights against quantization errors in a small-footprint low-power keyword spotting system. The approach involves training the wake word model using 8 bit and 4 bit quantization successfully. Dynamic quantization is used for DNN weight matrices, incorporating accuracy loss via quantization-aware training. In this study, shifts and scales for quantizing DNN weight matrices are calculated independently column-wise, similar to \"bucketing\" or \"per-channel\" quantization. The inputs are quantized row-wise on the fly, with accuracy loss incorporated via quantization-aware training. The keyword 'Alexa' is used for experiments, utilizing a 500 hrs far-field corpus for training and a 100 hrs dataset for evaluation. Models are evaluated using end-to-end Detection Error Tradeoff (DET) curves and GPU-based distributed DNN training. Training is organized into 3 stages, starting with pre-training a small ASR DNN with 3 hidden layers of 128 units. The training process involves pre-training a small ASR DNN with 3 hidden layers of 128 units using full ASR phone-targets. The models' performance is evaluated using Error Tradeoff curves and DET area under curve (AUC). Quantization-aware training improves AUC for quantized models, with DET curves showing differences in performance."
}