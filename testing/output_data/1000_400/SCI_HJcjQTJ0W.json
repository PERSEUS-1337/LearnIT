{
    "title": "HJcjQTJ0W",
    "content": "To enable cloud-based DNN training while protecting data privacy, the proposed method leverages intermediate representations of data by splitting DNNs and deploying them separately on local platforms and the cloud. The local NN generates feature representations from pre-trained NNs, avoiding local training to protect data privacy. The cloud NN is then trained based on these intermediate representations for the target learning task. This approach aims to balance privacy concerns with classification accuracy. The local NN generates feature representations from pre-trained NNs to protect data privacy. The cloud NN is trained based on these representations for the target learning task. PrivyNet optimizes accuracy under privacy loss constraints for local NN topology. PrivyNet demonstrates efficiency with CIFAR-10 dataset under privacy constraints. Cloud-based training schemes protect user data privacy through pre-processing schemes. Transformed representations are uploaded for learning tasks to mitigate privacy risks and policy violations. To protect user data privacy in cloud-based training schemes, different data pre-processing schemes are proposed to generate transformed representations locally. These representations must meet utility and privacy requirements, ensuring accurate learning tasks while constraining private information leakage. The transformation scheme should be flexible for various platforms and data types. The privacy and utility trade-off is a key consideration in related works. The transformation scheme for protecting user data privacy in cloud-based training schemes should be flexible for different platforms and data types. Various methods have been proposed to explore the trade-off between privacy and utility, including syntactic anonymization techniques like k-anonymity, l-diversity, and t-closeness. In recent years, various transformations have been proposed to explore the trade-off between privacy and utility. Syntactic anonymization methods like k-anonymity, l-diversity, and t-closeness aim to protect sensitive attributes in a static database. However, applying syntactic anonymization to high-dimensional continuous data is challenging. Differential privacy offers a formal privacy guarantee by adding noise, but it does not limit total information leakage from released representations. Existing works typically require local implementation to achieve differential privacy. To achieve differential privacy, existing works often involve local platforms in the backward propagation process, making deployment on lightweight platforms difficult. Non-invertible linear and non-linear transformations are proposed for data anonymization, with linear transformations relying on covariance or linear discriminant analysis for filtering training data. However, linear transformations may not provide sufficient privacy protection as original data can be reconstructed from released representations. Recently, nonlinear transformations based on minimax principles have been proposed. The proposed PrivyNet framework aims to provide better privacy protection than existing linear transformations by using nonlinear transformations based on minimax filter or Siamese networks. These methods can only be applied during the inference stage due to the required iterative training scheme between cloud and local platforms. The topology of the local neural network controls the trade-off between privacy and utility. PrivyNet is a DNN training framework that divides the model into local and cloud parts for privacy and utility trade-off control. The local NN generates intermediate representations while the cloud NN is trained for the learning task. Privacy protection is achieved through non-linear transformations like convolution and pooling. PrivyNet is a DNN training framework that divides the model into local and cloud parts for privacy and utility trade-off control. The local NN uses non-linear transformations like convolution and pooling to generate intermediate representations, while the cloud NN is trained for the learning task based on these representations. The local NN is derived from pre-trained NNs to protect privacy and achieve good utility by controlling the specific features to release. PrivyNet is a novel framework that splits DNN models for cloud-based training with fine-grained privacy control. It characterizes privacy loss and utility using CNN as the local NN, identifying key factors for the trade-off. A hierarchical strategy optimizes the local NN's topology for utility while considering computation, storage, and privacy constraints. PrivyNet is validated using CNN-based image classification, showing efficiency and effectiveness. A hierarchical strategy is proposed to optimize the topology of the local NN for utility while considering constraints on computation, storage, and privacy loss. PrivyNet is validated using CNN-based image classification, demonstrating efficiency and effectiveness in leveraging pre-trained NN for intermediate representation generation. The overall characterization flow involves generating feature representations using a pre-trained NN, training an image classification network (ICN) based on these features, and training an image reconstruction network (IRN) to reconstruct original images. Utility is measured by task accuracy and privacy by the distance between reconstructed and original images. The adversarial model assumes knowledge of original images and features but not the transformation (FEN). This aligns with the proposed hierarchical strategy in PrivyNet for optimizing NN topology while considering computation, storage, and privacy constraints. The IRN is trained assuming knowledge of original images and feature representations, while the transformation FEN is unknown. The transformation t is parameterized by the number of FEN layers and filters selected for each layer, generating output representations for each image. The transformation t induced by the FEN is parameterized by the number of layers and filters, generating output representations for images. Utility is evaluated by learning a classifier h* with minimized empirical risk for the target task, where accuracy is defined as the utility achieved by h*. The classifier h* is learned to minimize empirical risk for the target task, with utility defined as the accuracy achieved. Privacy is evaluated by a reconstruction model g* that minimizes the distance between reconstructed and original images, measured by peak signal-to-noise ratio (PSNR). The privacy loss of transformed representations is measured by the peak signal-to-noise ratio (PSNR) compared to original images. Larger PSNR indicates higher privacy loss. The impact of FEN topology on privacy and utility is characterized, with FEN derived from VGG16 pre-trained on Imagenet dataset. CNN is used for image classification task and reconstruction task. Architecture details of VGG16, ICN, and IRN are provided in the appendix. The FEN topology, determined by layers, output depth, and channel selection, is evaluated for its impact on utility and privacy in the PrivyNet framework. Changes in FEN topology generate different representations for training ICN and IRN. Utility and privacy are plotted in FIG2, showing their interplay. In the PrivyNet framework, the impact of the number of FEN layers and output depth on utility and privacy is evaluated. Changes in FEN topology result in different representations for training ICN and IRN. Privacy loss is observed with smaller PSNR of reconstructed images, while accuracy shows varying behaviors based on FEN layers and output depth. The impact of the number of FEN layers and output depth on utility and privacy in the PrivyNet framework is evaluated. Changes in FEN topology result in different representations for training ICN and IRN. Privacy loss is observed with smaller PSNR of reconstructed images, while accuracy varies based on FEN layers and output depth. Trade-off between accuracy and PSNR is shown in FIG2 (c), highlighting the relationship between privacy loss and utility. In the PrivyNet framework, the impact of FEN layers and output depth on utility and privacy is examined. Different FEN topologies lead to similar utility with high privacy loss and better utility with more layers and channel depth. Channel selection also affects privacy and utility, as shown in Figure 4 (a) and Table 4 (c). The utility and privacy loss for representations generated by each channel in the PrivyNet framework are characterized. Results show a large discrepancy in utility and privacy between the best and worst channels, with the best channel achieving 4X utility and 6 dB less privacy loss compared to the worst channel. The impact of output channel selection, FEN layers, and output depth is compared to analyze their effects on utility and privacy. The impact of output channel selection, FEN layers, and output depth on utility and privacy in the PrivyNet framework is analyzed using 6 VGG16 layers to generate the FEN. Results show that utility and privacy depend more on the number of FEN layers and output channel depth compared to output channel selection. In the PrivyNet framework, the impact of FEN layers and output channel depth on privacy and utility is analyzed. The pre-trained CNN can be used to explore the trade-off between privacy and utility by controlling the FEN topology. Larger dependence is observed on the number of FEN layers and output channel depth for both privacy and utility. The framework proposes PrivyNet to optimize utility while maintaining privacy constraints. In the next section, the framework PrivyNet is proposed to optimize utility under privacy constraints by determining the FEN topology. The impact of FEN layers and output channel depth on privacy and utility is highlighted, with a focus on balancing trade-offs. The design of the FEN is crucial for local computation and storage constraints, especially on lightweight platforms like mobile devices. The design of the FEN topology in the PrivyNet framework optimizes utility under privacy constraints by considering local computation and storage limitations, especially on lightweight platforms like mobile devices. Privacy characterization is done using cloud-based services, NN performance profiling is conducted locally, and channel pruning is supervised to balance privacy, computation, and storage constraints. The FEN topology in the PrivyNet framework optimizes utility under privacy constraints by considering local computation and storage limitations. Channel pruning is supervised to balance privacy, computation, and storage constraints, with a focus on the number of layers and output depth of the FEN. The assumption of availability of original images is crucial for worst-case evaluation of privacy loss. The assumption of availability of original images is crucial for worst-case evaluation of privacy loss in the adversarial model. The attackers may inject images into a database to obtain corresponding representations generated by the FEN, while the transformation induced by the FEN is assumed to be unknown to them to limit privacy loss. The FEN anonymity protection is crucial to prevent attackers from reconstructing images. The pre-characterization stage involves profiling pre-trained NNs for performance and storage on different platforms. This is necessary to evaluate and limit privacy loss in released representations. The performance and storage profiling of pre-trained NNs on local platforms is crucial for determining the topology of the FEN applied. Privacy characterization is done using cloud-based services, with the reconstruction network trained on publicly available data. Verification is done by comparing PSNR for FEN with different topologies using datasets like CIFAR-10 and CIFAR-100. The reconstruction network is trained on publicly available data of the same dimension and distribution. Privacy characterization is done for datasets like CIFAR-10 and CIFAR-100, comparing PSNR for FEN with different topologies. Less than 1000 samples are needed for an accurate characterization with data augmentation. The detailed PSNR values can be less accurate, reducing the training sample requirement. Determining the topology for the FEN is the next step. In PrivyNet, the topology for the FEN is determined based on the impact of FEN layers and output channel depth on privacy and accuracy. The relation between privacy, local computation, and storage on a mobile class CPU is shown in Figure 8, guiding the determination of FEN topology considering constraints on computation, storage, and privacy loss. Based on pre-characterization results and constraints on computation, storage, and privacy loss, the FEN topology is determined. Figure 8 illustrates the relationship between privacy, local computation, and storage on a mobile class CPU. The strategy involves selecting the deepest layer for high privacy requirements and a shallow FEN for low privacy requirements to achieve the desired privacy level. The FEN topology is determined based on privacy requirements and constraints on computation and storage. For low privacy requirements, a shallow FEN is selected to minimize local computation and storage consumption. Different FENs with varying layers and output depths can achieve the same privacy level. After determining the FEN's topology based on privacy requirements and computational constraints, a shadow FEN with m = 1 and D = 4 is chosen to minimize local computation and storage. For a low privacy loss requirement of 17 dB, FENs with different layers can achieve the privacy requirement, but selecting m = 6 and D = 4 provides better utility. Output channel selection is crucial due to significant variations in utility and privacy for different channels, layers, and output depths. Directly selecting output channels from the whole set may result in poor utility and privacy outcomes. Output:\nOutput channel selection is crucial due to significant variations in utility and privacy for different channels, layers, and output depths. Directly selecting output channels from the whole set may result in poor utility and privacy outcomes. The correlation between utility and privacy loss for a single channel is negligible, indicating the necessity of channel pruning. In channel pruning, the correlation between utility and privacy loss is negligible, as shown in Figure 10. This allows for optimizing utility while suppressing privacy loss. Fisher's LDA is used to identify channels with the worst utility during the pruning process. Fisher's LDA is utilized to identify ineffective channels during the pruning process by analyzing the distance of representations for images within the same class and among different classes. The criterion is evaluated based on the covariance matrix, proving to be a good method for channel selection. Fisher's LDA is used to identify ineffective channels by analyzing the distance of representations within and among classes. It evaluates the criterion based on the covariance matrix, proving effective for channel selection. The effectiveness of Fisher's LDA in identifying ineffective channels for pruning is verified in the experimental setup. By evaluating Fisher's discriminability for each channel, the ones with the worst utility are pruned to improve accuracy in the learning task. The experimental setup verifies the effectiveness of the LDA-based supervised channel pruning algorithm by leveraging Fisher's discriminability to identify and prune ineffective channels. Results show a 33.5% reduction in the probability of selecting a bad channel compared to random pruning methods. The LDA-based pruning method effectively prunes ineffective channels, resulting in a 33.5% reduction in the probability of selecting a bad channel compared to random pruning methods. The computation complexity scales with the number of samples required for the pruning process. The experimental results show that the computation complexity of the LDA-based pruning process scales with the number of samples. The effectiveness of supervised channel pruning is demonstrated by setting specific parameters and comparing different pruning methods. The experimental results demonstrate the effectiveness of channel pruning based on privacy and utility in three different settings. Pruning the channels with the worst utility and largest privacy loss results in improved utility and privacy compared to random selection without pruning. After channel pruning based on privacy and utility in three settings, the LDA-based pruning strategy showed 1.1% better accuracy and 1.25 dB smaller PSNR compared to random selection without pruning. The method also achieved similar accuracy but with slightly less privacy loss compared to the characterization-based pruning strategy. This verifies the effectiveness of the supervised pruning strategy. The supervised pruning strategy, using LDA-based pruning, showed improved accuracy and smaller privacy loss compared to random selection without pruning or pure characterization-based pruning. The adversarial model adopted in the paper assumes the transformation induced by the FEN is unknown to attackers, enhancing privacy protection. The paper discusses the adversarial model for protecting the anonymity of the Feature Extraction Network (FEN) derived from pre-trained NNs. Strategies include building a pool of pre-trained NNs like VGG16 and applying channel selection procedures for better privacy protection. The protection of the FEN involves building a pool of pre-trained NNs like VGG16 and applying channel selection procedures to enhance privacy. This makes it harder for attackers to guess how the FEN is derived and prevents them from knowing the channels forming the FEN. The privacy and utility trade-off is empirically verified for the intermediate channel selection process. The privacy and utility trade-off is empirically verified by gradually reducing the channel depth of the convolution layers in the pre-trained NN. Despite reducing the channel depth, privacy and utility remain largely unaffected, with a significant reduction in runtime observed. The privacy and utility trade-off is empirically verified by gradually reducing the channel depth of convolution layers in the pre-trained NN. Despite the reduction in channel depth, privacy and utility remain largely unaffected, with a significant reduction in runtime observed. The anonymity of the FEN is well protected through channel selection for intermediate layers, making it difficult for attackers to determine the number of layers and channels. PrivyNet is a flexible framework designed for cloud-based training with fine-grained privacy protection. PrivyNet is a flexible framework for cloud-based training with fine-grained privacy protection. It allows for the release of informative features instead of original data, making it easier for hospitals to train useful models for disease diagnosis and treatment while protecting patient privacy. PrivyNet enables hospitals to release informative features for disease diagnosis while protecting patient privacy. It also allows mobile platforms to collect data for health insights without compromising privacy. The framework is simple, platform-aware, and flexible for fine-grained privacy control. PrivyNet enables mobile platforms to upload collected data to the cloud while protecting private information. It is simple, platform-aware, and flexible for privacy control. The CIFAR-10 dataset has 60000 color images in 10 classes, while CIFAR-100 has images in 100 classes. The FEN is derived from VGG16 pre-trained on ImageNet dataset. The CIFAR-100 dataset contains 6000 images per class, split into 50000 training images and 10000 test images of 32x32 size. VGG16 BID20 is used for privacy and accuracy analysis, with CNN for image classification (h) and a generative NN based on ResNet blocks for image reconstruction (g). The architecture details are provided in the appendices. For image reconstruction, a state-of-the-art generative NN architecture based on ResNet blocks is used. The network consists of 8 ResNet blocks per cluster and is trained using a gradient descent optimizer with a learning rate of 0.003 and a mini-batch size of 128 for 100 epochs. For image classification, a VGG16 model is utilized with an initial learning rate of 0.05, a mini-batch size of 128, and a total of 250 epochs with learning rate adjustments at 100 and 200 epochs. For image reconstruction, the learning rate is set to 0.003 with a mini-batch size of 128 over 100 epochs. For image classification, the initial learning rate is 0.05 with a mini-batch size of 128, training for 250 epochs with learning rate adjustments. Data augmentation includes normalization, brightness, and contrast modifications. The IRN topology is determined before characterization to ensure image recovery accuracy. The number of ResNet block clusters affects the image recovery capability of IRN. Experiments are conducted on FENs with different topologies to assess image quality changes. The image recovery capability of IRN is determined by the number of ResNet block clusters. In experiments, 2 ResNet block clusters with 8 blocks each were chosen. Performance profiling of VGG16 on different platforms was conducted to understand performance and storage requirements. In experiments, the image recovery capability of IRN is determined by the number of ResNet block clusters. Performance profiling of VGG16 on various platforms reveals that as the number of VGG16 layers increases, local computation and storage requirements also increase rapidly. Computation mainly comes from convolution layers, while storage is dominated by fully connected layers, especially with larger input image sizes. Different platforms may face varying bottlenecks in computation and storage configurations. The computation in convolution layers and storage in fully connected layers are the main contributors to the overall workload. As input image size increases, fully connected layers require more storage. Different platforms may have varying bottlenecks in computation and storage. The complexity of extra computations is determined by the number of samples and output dimensions. The complexity of extra computations in the second part is determined by the number of samples N LDA and output dimensions W \u00d7 H. The computation complexity for S b is O(KW 2 H 2) and for S w is O(N LDA W 2 H 2). Computing W \u22121 and the largest eigenvalue of W \u22121 B, both W \u00d7 H in size, has a complexity of O(W 3 H 3). Overall, the complexity is O((K + N LDA)W 2 H 2 + W 3 H 3), with N LDA being a key factor. Small N LDA leads to minimal computation overhead."
}