{
    "title": "HkxStoC5F7",
    "content": "The paper introduces a new framework called ML-PIP for data efficient and versatile learning. It extends existing probabilistic interpretations of meta-learning and introduces \\Versa{}, which uses a flexible amortization network for few-shot learning. This network outputs a distribution over task-specific parameters in a single forward pass, eliminating the need for optimization at test time. \\Versa{} is evaluated on benchmark datasets, achieving state-of-the-art results for arbitrary numbers of shots and classes at train and test time. The approach is showcased through a challenging few-shot ShapeNet view reconstruction task, highlighting its adaptability to new datasets at test time. Despite recent advances in few-shot learning, there is a need for general purpose methods for flexible, data-efficient learning. A unifying view is required to understand and enhance existing frameworks in this field. In this paper, a framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) is developed to improve data efficiency in learning. The framework extends existing interpretations of meta-learning to cover various methods, including gradient-based and metric-based approaches. The ML-PIP framework extends interpretations of meta-learning to cover various methods like gradient-based and metric-based approaches. It leverages shared statistical structure between tasks, shares information on learning and inference, and enables fast learning through amortization. A new method called VERSA is proposed within this framework. The VERSA method is introduced within the ML-PIP framework, offering a faster test-time performance by substituting optimization procedures with forward passes through inference networks. It utilizes a flexible amortization network to handle few-shot learning datasets and outputs a distribution over task-specific parameters in a single forward pass. VERSA employs a flexible amortization network for second derivatives during training, handling arbitrary numbers of shots and classes. The framework includes a multi-task probabilistic model and a method for meta-learning probabilistic inference, aiming to maximize predictive performance and leverage shared statistical structure between tasks. The framework presented consists of a multi-task probabilistic model and a method for meta-learning probabilistic inference. It aims to maximize predictive performance on supervised learning tasks and leverage shared statistical structure between tasks. The model employs shared parameters and task-specific parameters for T tasks, with the goal of meta-learning fast and accurate approximations to the posterior predictive distribution for unseen tasks. The framework involves a multi-task probabilistic model for meta-learning fast and accurate approximations to the posterior predictive distribution for unseen tasks. It utilizes shared and task-specific parameters for T tasks, with point estimates for shared parameters and distributional estimates for task-specific parameters. The probabilistic solution for few-shot learning includes forming the posterior distribution over task-specific parameters and computing. The framework involves a multi-task probabilistic model for meta-learning fast and accurate approximations to the posterior predictive distribution for unseen tasks. Distributional estimates are used for task-specific parameters in the model. The probabilistic solution for few-shot learning comprises forming the posterior distribution over task-specific parameters and computing the posterior predictive. The emphasis is on quick approximation at test time. The approximate posterior predictive distribution is described in detail. The framework involves a multi-task probabilistic model for meta-learning fast and accurate approximations to the posterior predictive distribution for unseen tasks. The approximate posterior predictive distribution is approximated by an amortized distribution q \u03c6 (\u1ef9|D), learned through a feed-forward inference network with parameters \u03c6. This enables fast predictions at test time, constructed by amortizing the approximate posterior q \u03c6 (\u03c8|D) and forming the predictive distribution over the test output \u1ef9 (t). Additional approximation steps, such as Monte Carlo sampling, may be required. The framework involves meta-learning a multi-task probabilistic model for fast and accurate approximations of the posterior predictive distribution for unseen tasks. The approximate posterior predictive distribution is learned through an amortized distribution q \u03c6 (\u1ef9|D), enabling quick predictions at test time by constructing the predictive distribution over the test output \u1ef9 (t). The quality of the approximate posterior predictive is measured by the KL-divergence between the true and approximate distributions, with the goal of minimizing this divergence averaged over tasks during training. Meta-learning the approximate posterior predictive distribution involves minimizing the KL-divergence between the true and approximate distributions. Training aims to find parameters that best approximate the posterior predictive distribution in an average KL sense, supporting accurate prediction through amortized inference. The framework is grounded in Bayesian decision theory and involves selecting tasks randomly for training. The amortized procedure meta-learns approximate inference to recover the true posterior p(\u03c8|D) under identifiability conditions. Training involves selecting tasks randomly, sampling training data, forming posterior predictive distributions, and optimizing log-density. The procedure scores approximate inference by simulating Bayesian held-out log-likelihood evaluation. The training procedure focuses on optimizing the posterior predictive distribution by minimizing KL divergence. The objective is to maximize predictive performance through end-to-end stochastic training. The shared parameters are optimized to achieve this goal. The training procedure involves end-to-end stochastic training to optimize shared parameters for maximizing predictive performance. It includes individual feature extraction, instance pooling, regression onto weights, and episodic train/test splits using Monte Carlo samples for optimization. The training procedure involves stochastic training to optimize shared parameters for predictive performance. The approach developed is Meta-Learning Probabilistic Inference for Prediction (ML-PIP), which uses episodic train/test splits and Monte Carlo samples for optimization. The learning objective does not require an explicit prior distribution over parameters, learning it implicitly instead. The framework unifies existing approaches and supports versatile learning. The Meta-Learning Probabilistic Inference for Prediction (ML-PIP) framework supports versatile learning by enabling rapid and flexible inferences through deep neural networks. Rapid inference involves simple computations like feed-forward passes, while flexibility allows for various tasks without retraining. Design choices are discussed to balance rapid inference with flexibility. The Meta-Learning Probabilistic Inference for Prediction (ML-PIP) framework enables rapid and flexible inferences using deep neural networks. It allows for quick computations without retraining, but typically limited to a single task. Design choices are discussed to maintain flexibility, including processing sets of variable size with permutation-invariant instance-pooling operations. For few-shot image classification, the parameterization of the probabilistic model is inspired by previous work. The instance-pooling operation in the ML-PIP framework allows for processing sets of variable size for rapid and flexible inferences using deep neural networks. For few-shot image classification, a parameterization of the probabilistic model inspired by previous work is utilized, involving a shared feature extractor neural network feeding into task-specific linear classifiers. The proposed method involves amortizing individual weight vectors associated with feature extraction instances for few-shot image classification. This allows for flexible inference and avoids the need to specify the number of few-shot classes ahead of time. The amortization network maps image/angle examples to stochastic inputs for classification. The proposed method involves amortizing weight vectors for few-shot image classification by pooling regression onto stochastic inputs and mapping them onto new images through a generator. An amortization network maps image/angle examples to stochastic inputs, reducing the number of learned parameters. End-to-end training is employed, backpropagating to the generator through the inference network. The classification matrix is constructed by performing feed-forward passes through the inference. In our implementation, end-to-end training is used by backpropagating to \u03b8 through the inference network. The classification matrix \u03c8 DISPLAYFORM4 is constructed by performing C feed-forward passes through the inference network. The assumption of context independent inference is approximated and justified theoretically and empirically in Appendix B. The context independent approximation addresses the limitations of a naive amortization by reducing the number of parameters needed for inference. In the context of few-shot learning, the use of Density Ratio Estimation is demonstrated to show that full approximate posterior distributions closely resemble their context independent counterparts. This approach reduces the number of parameters needed for inference, allowing for meta-training with varying numbers of classes and flexibility in the number of classes at test-time. The application of this method is exemplified in Few-Shot Image Reconstruction for complex output spaces. VERS for Few-Shot Image Reconstruction involves a challenging task with a complex output space. It focuses on view reconstruction, where a small set of observed views is used to infer how an object looks from any desired angle. The generative model utilizes a latent vector and angle representation to produce images at specified orientations. The generator network's parameters are considered global, while the latent inputs are task-specific. The generator network uses a latent vector and angle representation to create images at specified orientations. Parameters of the generator are global, while latent inputs are task-specific. An amortization network processes image representations, view orientations, and instance-pools them to produce a distribution over vectors. The amortization network processes image representations, view orientations, and instance-pools them to produce a distribution over vectors, unifying various meta-learning approaches. The connections between different meta-learning approaches are explored, focusing on point estimates for task-specific parameters. Gradient-based meta-learning involves optimizing task-specific parameters through gradient ascent. This approach is related to semi-amortized inference and Model-agnostic meta-learning, providing a perspective on semi-amortized ML-PIP. VERSATILE (VERSA) is a distributional approach to meta-learning that eliminates the need for back-propagation through gradient updates during training and gradient computation at test time. It offers a perspective on amortization and recovers test-train splits without relying on episodic meta-train/meta-test splits. VERSATILE (VERSA) is a distributional approach to meta-learning that eliminates the need for back-propagation through gradient updates during training and gradient computation at test time. It offers a perspective on amortization and recovers test-train splits without relying on episodic meta-train/meta-test splits. VERSA simplifies inference by treating both local and global parameters, enabling the use of multiple gradient steps in an RNN to compute \u03c8 * which recovers BID44. This method, in comparison to others, is distributional over \u03c8 and does not require back-propagation through gradient updates during training or gradient computation at test time. The shared parameters in network \u03c8 (t) are the lower layer weights. Amortized point estimates for these parameters are constructed by averaging top-layer activations for each class. This leads to a predictive distribution that recovers prototypical networks using a Euclidean distance function. In contrast, VERSA uses a more flexible amortization function for distributional purposes. Amortized MAP inference is used for predicting weights of classes from activations of a pre-trained network to support online learning and transfer tasks. This demonstrates the usage of hyper-networks for amortizing learning about weights. BID43 proposed a method for predicting weights of classes from activations of a pre-trained network to support online learning and transfer tasks. This utilizes hyper-networks to amortize learning about weights and can be recovered by the ML-PIP framework. VERSA goes beyond point estimates, employing end-to-end training and supporting full multi-task learning. Amortization network computes \u03c8 * (D, \u03b8) as part of the model. VERSATILE (VERSA) goes beyond point estimates, utilizing end-to-end training and enabling full multi-task learning. The amortization network computes \u03c8 * (D, \u03b8) as part of the model specification, establishing a strong connection to neural processes. Comparing to Variational Inference (VI), the ML-PIP training procedure for \u03c6 and \u03b8 is equivalent to training a conditional model via maximum likelihood estimation. VERSA significantly improves over standard VI in few-shot classification and compares to recent VI/meta-learning hybrids. Evaluation on various few-shot learning tasks includes toy experiments on amortized posterior inference and results on Omniglot and miniImageNet datasets. In Section 5.2, VERSA's few-shot classification results on Omniglot and miniImageNet datasets show high accuracy across varying shot and way settings. Additionally, in Section 5.3, VERSA's performance on a one-shot view reconstruction task with ShapeNet objects is examined. An experiment is conducted to investigate the approximate inference achieved by the training procedure, generating data from a Gaussian distribution with varying means across tasks. In an experiment to investigate inference, data is generated from a Gaussian distribution with varying means across tasks. Tasks are generated with different numbers of train and test observations, and an inference network is used for amortization. The model is trained with Adam using mini-batches of tasks, and posterior distributions are inferred for unseen test sets. The true posterior is Gaussian and task-dependent. The experiment involves generating data from a Gaussian distribution with varying means for different tasks. An inference network is used for amortization, and posterior distributions are inferred for unseen test sets. The true posterior is Gaussian and task-dependent. VERSA is evaluated on few-shot classification tasks using Omniglot and miniImageNet datasets. VERSATILE (VERSA) is evaluated on few-shot classification tasks using Omniglot and miniImageNet datasets. The experimental protocol follows previous work, with training conducted episodically using k c examples per class. The approximate posterior closely resembles the true posterior in both five and ten shot scenarios. The study evaluates VERSATILE (VERSA) on few-shot classification tasks using Omniglot and miniImageNet datasets. Training is conducted episodically with k c examples per class, and the approximate posterior closely resembles the true posterior given the observed data. Details of data preparation and network architectures are provided in the appendix. The performance of VERSA and competitive approaches in few-shot classification is detailed in TAB1, excluding approaches using pre-training or residual networks to assess the learning algorithm's quality separately from the discriminative model's power. VERSATILE (VERSA) achieves state-of-the-art results on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks using convolution-based network architecture and end-to-end training. Results are competitive on other benchmarks as well. VERSATILE (VERSA) achieves state-of-the-art results on various benchmarks, including 5-way -5-shot miniImageNet and 20-way -1 shot Omniglot. It adapts only the weights of the top-level classifier, outperforming other methods that adapt all learned parameters for new tasks. Further experimental details are provided in Appendix C. VERSA achieves state-of-the-art results on benchmarks by adapting only the weights of the top-level classifier. Comparison to standard and amortized VI shows VERSA improves substantially over amortized VI. Non-amortized VI improves performance but is slower in forming the posterior. Using non-amortized VI improves performance substantially, but does not reach the level of VERSA. Forming the posterior is significantly slower as it requires many forward/backward passes through the network. VERSA allows for varying the number of classes and shots between training and testing, showing high accuracy even when conditions are changed. VERSATILE (VERSA) model demonstrates high accuracy and flexibility by retaining accuracy of approximately 94% when trained for a 20-Way, 5-Shot condition and tested on 100-way conditions. It shows robustness to variations in shots and requires minimal time for evaluation compared to MAML, with a more than 5\u00d7 speed advantage and 4.26% better accuracy. The VERSA model outperforms MAML in speed and accuracy, with a 5\u00d7 speed advantage and 4.26% better accuracy. The dataset used consists of 37,108 objects from 12 object categories, with 36 views generated for each object. VERSA is evaluated against a conditional variational autoencoder (C-VAE) with similar architectures. For training, validation, and testing, 70%, 10%, and 20% of objects are randomly shuffled. Each object has 36 views of 32 \u00d7 32 pixels generated at 10-degree intervals. VERSA is compared to a C-VAE with similar architectures, trained episodically on 12 object classes. VERSA produces sharper images with more detail compared to C-VAE, capturing correct object orientation. VERSATILE (VERSA) and C-VAE were compared in generating views of unseen objects from a single shot. VERSA produced sharper images with more detail compared to C-VAE, capturing correct object orientation. Quantitative comparison results showed VERSA's superiority over C-VAE, with improved performance as the number of shots increased. Table 2 compares VERSA with varying shot numbers to C-VAE, showing VERSA's superiority. Mean squared error and SSIM metrics demonstrate the improvement with more shots. ML-PIP is introduced as a meta-learning framework, leading to the development of VERSA, a few-shot learning algorithm with state-of-the-art performance. VERSATM is a few-shot learning algorithm that outperforms C-VAE, avoiding gradient-based optimization at test time. Prototypical Networks show improved performance when trained on higher \"way\" than tested. The new inference framework in Section 2 is based on Bayesian decision theory. The experimental protocol of Prototypical Networks is consistent with other methods. When trained on 20-way classification and tested on 5-way, the model achieves 68.20 \u00b1 0.66%. The new inference framework in Section 2 is based on Bayesian decision theory, providing a recipe for making predictions for unknown test variables by combining information from training data and a loss function. The text discusses the cost of predicting values in Bayesian decision theory (BDT) and introduces a stochastic variational objective for meta-learning probabilistic inference. It extends BDT to return a full predictive distribution over unknown test variables, quantified through a distributional loss function. The text introduces a method grounded in Bayesian inference and decision theory, extending it to return a full predictive distribution over unknown test variables. It quantifies the quality of the predictive distribution through a distributional loss function and utilizes amortized variational training to optimize expected loss over tasks. Amortized variational training involves forming quick predictions at test time by learning parameters that minimize average expected loss over tasks. Shared variational parameters \u03c6 are optimized to predict outcomes directly from any training dataset. The optimal parameters are found by minimizing expected distributional loss across tasks, without needing to compute the true predictive distribution. This approach emphasizes meta-learning and episodic minibatch training over tasks and data. The procedure involves stochastic approximation by sampling tasks and partitioning data into training and test sets, without computing the true predictive distribution. The log-loss function is used, optimizing the closest predictive distribution to the true distribution. This approach emphasizes meta-learning and episodic minibatch training. The optimal q \u03c6 is the closest member of Q to the true predictive p(\u1ef9|D) in a KL sense, resembling the wake-sleep algorithm. Alternative scoring rules and task-specific losses are left for future exploration. The approximate predictive distribution is specified by replacing the true posterior with an approximation. The justification for this approximation is viewed through density ratio estimation. In this section, theoretical and empirical justifications are provided for the context-independent approximation of the predictive distribution. The optimal softmax classifier is expressed in terms of conditional densities, constructing estimators independently for each class. This mirrors the optimal form using VERSA. The optimal classifier constructs estimators for conditional densities independently for each class, similar to training a naive Bayes classifier. VERSA mirrors this optimal form using a context-independent assumption. An experiment is detailed to evaluate the validity of this assumption by randomly generating tasks from a dataset. The experiment aims to test the assumption of context-independent inference by randomly generating tasks from a dataset and performing free-form variational inference on the weights for each task. The goal is to see if the distribution of weights for a specific class remains similar regardless of other classes in the task. In the experiment, 5-way classification in the MNIST dataset is examined by randomly sampling fifty tasks. The model is trained episodically and achieves 99% accuracy on test examples. The optimized weights cluster by class in 2-dimensional space, with some overlap and occasional outliers. The optimized weights cluster by class in 2-dimensional space, with some overlap and occasional outliers. For tasks containing both class '1' and '2', class '2' weights are located away from their cluster, suggesting independence of class-weights from the task. If the model lacks capacity to assign class weights properly, one weight may be moved to an 'empty' region. The model's capacity to assign class weights independently of the task is crucial. A VI-based objective is derived for the probabilistic model, with options for amortized and non-amortized VI. The objective function remains consistent regardless of the VI approach chosen. An evidence lower bound (ELBO) is expressed for a single task, and a stochastic estimator is derived for optimization. In few-shot classification experiments, a stochastic estimator is derived to optimize the objective function, which differs from the VI-based approach. The model's ability to assign class weights independently of the task is emphasized. The Omniglot dataset is used for these experiments. In few-shot classification experiments, the Omniglot BID32 dataset is used, consisting of 1623 handwritten characters from 50 alphabets. Images are resized to 28x28 pixels and augmented with rotations. Training, validation, and test sets are split randomly, resulting in 4400 training, 400 validation, and 1292 test classes. Training proceeds episodically for C-way, k c -shot classification. Character classes are augmented with rotations of 90 degrees. The training, validation, and test sets consist of a random split of 1100, 100, and 423 characters, resulting in 4400 training, 400 validation, and 1292 test classes. Training for C-way, k c -shot classification is done episodically, with each training iteration consisting of a batch of tasks. During training, C classes are selected randomly, with k c character instances used as training inputs and 15 character instances used as test inputs. The validation set is used to monitor learning progress and select the best model, while the final evaluation is done on 600 randomly selected tasks from the test set. The validation set is used to monitor learning progress and select the best model, while the final evaluation is done on 600 randomly selected tasks from the test set. Models are trained using the Adam optimizer with a constant learning rate and specific iterations for different models. The miniImageNet dataset consists of 60,000 color images divided into 100 classes, each with 600 instances. The 5-shot and 20-way 1-shot models are trained for specific iterations using a Gaussian form for q and \u03c8 samples. The miniImageNet dataset contains 60,000 color images divided into 100 classes. Training is episodic with Adam optimizer and specific learning rates for different models. The neural network architectures for the feature extractor \u03b8, amortization network \u03c6, and linear classifier \u03c8 are detailed. The amortization network provides Gaussian parameters for the weight distributions of \u03c8. The local-reparameterization trick is used for sampling from weight distributions, and the feature extraction network \u03b8 is shared with the amortization network to reduce learned parameters. The linear classifier \u03c8's weight distributions are sampled using the local-reparameterization trick, sampling from the implied logits distribution. The feature extraction network \u03b8 is shared with the amortization network to reduce learned parameters. The network architecture for miniImageNet few-shot learning includes convolutional layers with dropout and pooling. The feature extraction network for miniImageNet few-shot learning includes convolutional layers with dropout and pooling, with Batch Normalization and dropout used throughout. The network architecture consists of multiple conv2d layers with different sizes and strides, leading to a final output size of 256. The feature extraction network for miniImageNet few-shot learning involves convolutional layers with dropout and pooling, resulting in a final output size of 256. The ShapeNetCore v2 BID5 database is used for experimentation, with 12 object categories selected for training. For experiments, 12 largest object categories are used from a dataset of 51,300 unique objects. The dataset is shuffled and split into 70% for training, 10% for validation, and 20% for testing. Each object has 36 views, converted to gray-scale and resized to 32x32 pixels. The model is trained episodically on random tasks with one view per object. The model is trained episodically on random tasks with one view per object. The rendered images are converted to gray-scale and reduced to 32x32 pixels. The system is evaluated by generating views and computing metrics over the test set using an amortization network. Training is done using the Adam optimizer with a constant learning rate of 0.0001 for 500,000 iterations. The network architectures for the encoder, amortization, and generator networks are described in Tables E.2 to E.4. Training is done using the Adam BID25 optimizer with a learning rate of 0.0001 for 500,000 iterations. The ShapeNet Encoder Network has specific layers and output sizes."
}