{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. To address this, a unified framework called EdgeGANRob has been proposed, which focuses on extracting shape/structure features to improve CNN robustness. Additionally, a robust edge detection approach called Robust Canny has been introduced to reduce sensitivity to adversarial perturbations. EdgeGANRob is compared to a simplified version called EdgeNetRob for further insights. EdgeGANRob, a framework focusing on shape/structure features to enhance CNN robustness, is compared to EdgeNetRob. A robust edge detection method, Robust Canny, is proposed to reduce sensitivity to adversarial perturbations. EdgeNetRob boosts model robustness but lowers clean model accuracy, while EdgeGANRob improves clean model accuracy without sacrificing robustness. Extensive experiments demonstrate EdgeGANRob's resilience across various learning tasks and settings. Convolutional neural networks (CNNs) have shown state-of-the-art performance in various tasks but are vulnerable to adversarial examples and data poisoning attacks. Recent studies suggest that CNNs focus on surface statistical regularities rather than high-level abstractions, hindering generalization. Recent studies explore the vulnerability of CNNs to adversarial examples and data poisoning attacks, attributing it to the focus on surface statistical regularities rather than high-level abstractions. Improving the general robustness of DNNs under distribution shifting remains a challenge, with suggestions to train classifiers on \"robust features\" that are insensitive to small perturbations. Recent studies attribute the vulnerability of CNNs to adversarial examples to their focus on non-robust but highly-predictive features. Training classifiers on \"robust features\" that contain necessary information for recognition and are insensitive to perturbations is suggested. Human recognition relies on global object shapes, while CNNs are biased towards local patterns, leading to vulnerability to adversarial examples and distribution shifting. Recent studies suggest that CNNs are vulnerable to adversarial examples due to their focus on non-robust features. Human recognition relies on global object shapes, while CNNs are biased towards local patterns, making them susceptible to attacks. Researchers propose improving CNN robustness by emphasizing global shape structure. The paper proposes using edges as a shape representation to improve CNN robustness to adversarial attacks and distribution shifting. The approach, EdgeGANRob, leverages structural information in images to enhance CNN performance. This paper introduces EdgeGANRob, a new approach to enhance CNN robustness by leveraging structural information in images. A simplified version, EdgeNetRob, detects edges and trains the classifier based on shape information, improving CNNs' robustness. Robust Canny is proposed to improve EdgeNetRob's robustness by enhancing edge detection algorithms. This combined method outperforms adversarial retraining defense. EdgeNetRob enhances CNN robustness but reduces clean accuracy due to missing texture/color information, leading to the development of EdgeGANRob. The unified framework EdgeGANRob improves CNNs' robustness by extracting edge/structure information and refilling texture/colors using GANs. It outperforms adversarial retraining defense and remains robust against adaptive evasion attacks. More visualization results can be found on the website: https://sites.google.com/view/edgenetrob. The paper proposes the EdgeGANRob framework to enhance CNNs' robustness by extracting edge/structure information and refilling texture with GANs. It introduces a robust edge detection approach, Robust Canny, to reduce sensitivity to adversarial perturbations. Evaluation on EdgeNetRob and EdgeGANRob shows effectiveness in tasks like adversarial attacks and distribution shifting. The inpainting GAN in EdgeGANRob is evaluated for effectiveness in learning tasks on robust edge features. Thorough evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Defense methods against adversarial examples are discussed, highlighting the importance of evaluating against customized white-box attacks. Defense methods against adversarial attacks are not robust and should be evaluated against customized white-box attacks. Distribution shifting is common in real-world applications, with CNNs prone to learning superficial statistical cues. Wang et al. (2019a) proposed a method to enhance CNN robustness by penalizing local representations' predictive power and mitigating the fitting of superficial statistical cues. Recent research has focused on enhancing CNN robustness by penalizing local representations' predictive power and mitigating the fitting of superficial statistical cues. Benchmark datasets have been proposed for evaluating model robustness under common perturbations, while methods to detect and protect models from backdoor attacks have also been developed. Recent research has focused on enhancing CNN robustness by detecting and protecting models from poisoning attacks and backdoor attacks. Methods include injecting patterns into training data, using robust statistics to detect poisoned data, and neuron pruning to protect models. Studies have shown that CNNs rely more on textures than global shape structure for image recognition, while humans rely more on shape structure. Adversarially robust models tend to capture global structure of objects, and there are non-robust features in natural images that are highly predictive. In contrast to human reliance on shape structure, CNNs rely more on textures for image recognition. Adversarially robust models prioritize capturing global object structure. Previous research identified non-robust features in natural images that are predictive but not interpretable by humans. A new classification pipeline, EdgeGANRob, proposes using edge features for robustness. This method extracts edge/structure features from images and reconstructs them using a generative adversarial network. In this work, the EdgeGANRob classification pipeline utilizes robust edge features extracted from images. The method involves reconstructing images using a GAN after extracting edge/structure features. The EdgeNetRob backbone procedure consists of two stages: extracting edge maps and training a classifier on them. The pipeline aims to enhance robustness in three evaluated settings. EdgeNetRob consists of two stages: extracting edge maps using an edge detection method and training a standard image classifier on these maps. The goal is to make CNN decisions based solely on edges, reducing sensitivity to local textures. Despite its simplicity, EdgeNetRob degrades CNN performance on clean test data due to missing texture/color information. This motivates further development. EdgeNetRob transforms original images into edge maps, making it less sensitive to local textures. However, it degrades CNN performance on clean test data due to missing texture/color information. This led to the development of EdgeGANRob, which fills edge maps with texture/colors to improve clean accuracy. The robustness of this classification system depends on the edge detector used, motivating the proposal of a robust edge detection algorithm named Robust Canny. The robustness of edge detection algorithms is crucial for accurate recognition tasks. Traditional methods like Canny are inherently robust but can become noisy with adversarial perturbations. To address this, a new algorithm called Robust Canny is proposed to improve robustness by truncating noisy pixels. The proposed Robust Canny algorithm aims to enhance the robustness of edge detection in the presence of adversarial perturbations. It includes stages such as noise reduction, gradient computation using the Sobel operator, noise masking to reduce noise, and non-maximum suppression for edge thinning. The Robust Canny algorithm enhances edge detection by applying the Sobel operator for gradient computation, noise masking to reduce noise, and non-maximum suppression for edge thinning. It includes steps like double thresholding and edge tracking by hysteresis to detect strong and weak edge pixels. The Robust Canny algorithm modifies the vanilla Canny algorithm by adding a noise masking stage after computing image gradients. This includes setting gradient magnitudes lower than a threshold to zero and using a truncation operation to reduce adversarial noise. Parameters like the standard deviation of the gaussian filter and thresholds also impact the algorithm's robustness level. The Robust Canny algorithm enhances the vanilla Canny algorithm by reducing adversarial noise through a truncation operation and noise masking. Parameters like standard deviation and thresholds impact robustness. Careful selection of parameters is crucial for a robust edge detector. Training a Generative Adversarial Network (GAN) in EdgeGANRob is described in detail in the experiment section. To achieve a robust edge detector, careful parameter selection is essential. Training a Generative Adversarial Network (GAN) in EdgeGANRob involves two stages: first, training a conditional GAN using adversarial and feature matching losses, and second, aiming for high accuracy in generated RGB images. The method involves training a conditional GAN with adversarial and feature matching losses in the first stage, followed by fine-tuning the GAN and classifier together to achieve high accuracy in generated RGB images. This approach aims to improve robustness under adversarial attacks, distribution shifting, and backdoor attacks. The method involves training a conditional GAN with adversarial and feature matching losses to generate more realistic images. The approach aims to improve robustness under adversarial attacks, distribution shifting, and backdoor attacks. EdgeGANRob focuses on shape structure to enhance model generalization ability and resist adversarial perturbations. EdgeGANRob focuses on shape structure to improve model generalization and resist distribution shifting. Extracting edges acts as a data sanitization step against backdoor attacks, making them ineffective. The robustness of the method is evaluated, showing promising results even without inpainting GAN. The proposed method, EdgeNetRob, is evaluated for its robustness against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. It is also compared with EdgeGANRob, showing unique advantages in certain settings and as a robust recognition method. The study evaluates the robustness of EdgeNetRob against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. The method is compared with EdgeGANRob and shows unique advantages in certain settings as a robust recognition method. The study evaluates the robustness of EdgeNetRob against adversarial attacks on Fashion MNIST and CelebA datasets. Evaluation includes \u221e adversarial perturbation constraints and standard perturbation budgets. Robustness to white-box attacks is measured using the BPDA attack. The study evaluates the robustness of EdgeNetRob against adversarial attacks on Fashion MNIST and CelebA datasets using different perturbation budgets. The robustness to white-box attacks is measured using the BPDA attack, comparing the performance of three edge detection methods. The study compares the robustness of different edge detection methods against adversarial attacks on Fashion MNIST and CelebA datasets. Results show that using edges generated by RCF is not robust, with a significant accuracy drop under strong adaptive attacks. Adversarial training is effective in achieving strong robustness to white-box attacks. EdgeNetRob and EdgeGANRob show a small drop in clean accuracy compared to the baseline model. The study compares the robustness of different edge detection methods against adversarial attacks on Fashion MNIST and CelebA datasets. EdgeNetRob and EdgeGANRob show a small drop in clean accuracy compared to the baseline model, but achieve higher clean accuracy than adversarial training with = 8. EdgeGANRob has higher clean accuracy than EdgeNetRob on CelebA dataset, validating the necessity of adding GANs on more complicated datasets. Both EdgeNetRob and EdgeGANRob remain robust under strong adaptive attacks. The study demonstrates the robustness of EdgeNetRob and EdgeGANRob against adversarial attacks on Fashion MNIST and CelebA datasets. EdgeGANRob outperforms EdgeNetRob on CelebA, highlighting the importance of GANs on complex datasets. Both models maintain robustness under strong adaptive attacks and show promising results in terms of generalization ability. The study compares EdgeNetRob and EdgeGANRob models against adversarial attacks on perturbed Fashion MNIST and CelebA datasets with various patterns. Results show significant accuracy improvements for EdgeNetRob and EdgeGANRob on negative color, radial kernel, and random kernel patterns compared to the state-of-the-art method PAR. Greyscale images maintain high accuracy similar to baselines. The study demonstrates that EdgeNetRob and EdgeGANRob models enhance accuracy on different patterns compared to PAR. Edge features aid CNN generalization during distribution shifts and defend against backdoor attacks by embedding invisible watermarks in images. Fashion MNIST and CelebA datasets are used for testing, showing qualitative results in Figure 4 and Appendix D. In Tran et al. (2018), invisible watermark patterns are embedded into images for Fashion MNIST and CelebA datasets. Different attack and target pairs are chosen for each dataset, with varying poisoning ratios. A comparison with the Spectral Signature method is made, showing high poisoning accuracy on both datasets. Test accuracy on standard and poisoned data is presented in Table 4 and Table 5. Our embedding pattern successfully attacks the vanilla Net with high poisoning accuracy on CelebA and Fashion MNIST datasets. Spectral Signature does not always perform well with invisible watermark patterns, while EdgeNetRob and EdgeGANRob consistently have low poisoning accuracy. EdgeGANRob achieves better clean accuracy compared to EdgeNetRob, highlighting the benefits of inserting an inpainting GAN. The qualitative results of the backdoor images after edge detection algorithm show that the invisible watermark pattern can be removed. EdgeGANRob outperforms EdgeNetRob in clean accuracy, demonstrating the benefits of using an inpainting GAN. The method combines a robust edge feature extractor with a generative adversarial network to improve model robustness against adversarial attacks and distribution shifting. It also enhances robustness against backdoor attacks by utilizing shape information. The study demonstrates the effectiveness of using shape information to enhance model robustness against adversarial attacks and distribution shifting. Data pre-processing involves resizing CelebA images to 128 \u00d7 128 and normalizing data. Different models are used for Fashion-MNIST and CelebA datasets, trained using stochastic gradient descent with momentum. Various attack methods like PGD and CW are evaluated for robustness testing. The study evaluates the effectiveness of shape information in enhancing model robustness against adversarial attacks. Different models are used for Fashion-MNIST and CelebA datasets, trained using stochastic gradient descent with momentum. Various attack methods like PGD and CW are tested for robustness. Hyper-parameters for Robust Canny evaluation are reported. The study evaluates model robustness against adversarial attacks using Robust Canny with specific hyper-parameters for Fashion MNIST and CelebA datasets. In a white-box attack scenario, backpropagating gradients through non-differentiable transformations can be exploited by attackers using the BPDA technique to construct adversarial examples. The attacker can use the Backward Pass Differentiable Approximation (BPDA) technique to replace non-differentiable transformations and create adversarial examples. A differentiable approximation of the Robust Canny algorithm is sought to strengthen the attack. The transformation is divided into two stages: C1 (steps 1-3) and C2 (steps 4-6), with C2 involving a non-differentiable operation. The transformation is divided into two stages: C1 (steps 1-3) and C2 (steps 4-6). C2 involves a non-differentiable operation where the output is a masked version of the input. To obtain a differentiable approximation of R-Canny for BPDA, the mask is assumed to be constant. Backpropagation gradients are only done through C1, not M. Test accuracy changes are shown under radial and random mask transformations in Figure A. In Figure A, test accuracy changes are displayed under radial and random mask transformations with varying parameters. Additional visualization results for CelebA under distribution shifting are shown in Figure B and ??, while Figure D illustrates qualitative results of EdgeGANRob and EdgeNetRob for backdoor attacks on Fashion MNIST. EdgeNetRob can slightly remove poisoning patterns, and the generated images do not share similar patterns."
}