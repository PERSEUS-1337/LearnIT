{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"conditionally invariant\" features that do not change across domains and \"orthogonal\" features that can change across domains. It is important to use the \"conditionally invariant\" features to guard against future adversarial domain shifts. The domain itself is assumed to be a latent variable. The text discusses the importance of using \"conditionally invariant\" features for image classification to guard against future domain shifts. The domain itself is considered a latent variable, making it challenging to directly observe distributional changes across different domains. In data augmentation, we can generate multiple images from an original image using an ID variable to refer to the relevant original image. This method only requires a small fraction of images to have an ID variable. In data augmentation, images can be generated using an ID variable to refer to the original image. A causal framework is provided by adding the ID variable to the model, treating the domain as a latent variable. Samples with the same class and identifier are treated as counterfactuals under different style interventions. Regularizing the network with a graph Laplacian improves performance in settings with changing domains. This approach links to interpretability, fairness, and transfer learning in deep neural networks. Regularizing the network with a graph Laplacian improves performance in settings with changing domains, such as image quality, brightness, color changes, and more complex variations. This can impact interpretability, fairness, and transfer learning in deep neural networks. Domain shifts can degrade predictive performance due to dependencies that vanish in test distributions, caused by changing conditions like color, background, or location changes during deployment in production. Domain shifts in machine learning systems can be caused by changing conditions like color, background, or location changes during deployment in production. An example is the \"Russian tank legend\" where sampling biases in training data led to high accuracy in distinguishing between Russian and American tanks, but would have failed in practice due to differences in image quality. Hidden confounding factors like image quality can create indirect associations, highlighting the need for large sample sizes in deep learning. The system learned to discriminate between images of different qualities but would have failed in practice due to hidden confounding factors like image quality. Large sample sizes are needed in deep learning to average out the effects of confounding factors and achieve invariance to known factors. Adversarial examples, imperceptibly perturbed inputs misclassified by ML models, highlight the divergence between human and artificial cognition. In deep learning, large sample sizes are necessary to achieve invariance to known factors like translation, point of view, and rotation through data augmentation. Adversarial examples, which are misclassified inputs by ML models, demonstrate the difference between human and artificial cognition. The goal is to mimic human ability to learn invariances from a few instances of the same object and align DNN features with human cognition. Fairness and discrimination considerations also play a role in controlling input data characteristics in learned representations. Counterfactual regularization (CORE) is proposed to address biases in datasets used for training ML algorithms, aiming to prevent replication of biases in estimated models. For example, Google's photo app tagged non-white people as \"gorillas\" due to biased training data. CORE aims to mitigate such issues. CORE, a proposed solution to address biases in ML training datasets, aims to prevent replication of biases in estimated models. It focuses on controlling latent features extracted by an estimator, categorizing them into 'conditionally invariant' (core) and 'orthogonal' (style) features to ensure stability and coherence in classification. This approach makes the estimator robust against adversarial domain shifts. CORE aims to create an estimator that is invariant to style features, ensuring stability and coherence in classification. It leverages knowledge about grouping in datasets to reduce the need for data augmentation and improve predictive performance in small sample size settings. CORE exploits knowledge about grouping in datasets to reduce the need for data augmentation and improve predictive performance in small sample size settings. The manuscript discusses motivating examples, related work, and introduces counterfactual regularization for logistic regression. Performance evaluation is done on the CelebA dataset for classifying whether a person wears glasses, using grouping information to ensure consistent predictions for images of the same person. CORE utilizes grouping information in datasets to enhance predictive performance in small sample size scenarios. The CelebA dataset contains face images of celebrities, and the task involves classifying if a person wears glasses. By considering grouping information, the classification is constrained to provide the same prediction for all images of the same person with the same class label. Additional instances of the same person are referred to as counterfactual observations. The training set includes 10 identities with approximately 30 images each, resulting in a total sample size of 321. Examples from the dataset are shown in Figure 1. Exploiting grouping information in datasets reduces test error significantly. In a subsampled CelebA dataset, using grouping-by-ID with the identity of the person reduces test error by 32%. In an augmented MNIST dataset, grouping-by-ID with the original image reduces test error on rotated digits by 50%. Overall, exploiting the group structure decreases the average test error from 24.76% to 16.89%, approximately a 32% improvement. Using the same network architecture without grouping information but with a standard ridge penalty, the test error is reduced by 32% compared to pooling all samples. Exploiting the group structure decreases the average test error from 24.76% to 16.89%, approximately a 32% improvement. CORE can also make data augmentation more efficient by generating additional samples through modifications like rotation, translation, or flipping of images. Data augmentation involves creating additional samples by modifying original inputs, such as rotating, translating, or flipping images. This process generates invariance of the estimator with respect to style features. Using grouping information for CORE enforces invariance more strongly compared to normal data augmentation. Rotation is sampled uniformly at random from [35, 70] for a total sample size of 10100. Using CORE for data augmentation with rotation on MNIST, the average test error on rotated examples is reduced from 32.86% to 16.33%. This approach differs from other works like BID14 and DANN, as it requires grouped observations instead of relying on unlabeled data. The goal is to learn a representation that contains no discriminative information about the input's origin. The approach proposed in BID13, inspired by BID5, focuses on learning a representation without origin information through adversarial training. In contrast to BID14 and DANN, which use unlabeled data, our approach assumes different realizations of the same object under interventions. BID14 identifies conditionally independent features by adjusting variable transformations to minimize squared MMD. The data generating process assumed in BID14 is similar to our model, where they identify conditionally independent features by adjusting variable transformations to minimize squared MMD distance between distributions in different domains. The fundamental difference is that BID14's domain identifier is observable, while ours is latent. Causal modeling in transfer learning aims to guard against domain shifts, with the advantage that predictions will be more reliable. In contrast to observable domain identifiers, our approach utilizes a latent identifier variable ID to penalize the classifier for using latent features. Causal modeling in transfer learning aims to ensure valid predictions even under large interventions on predictor variables. However, transferring these results to adversarial domain changes in image classification faces challenges due to the anti-causal nature of the classification task and the need to guard against such shifts. The challenges in transferring causal modeling results to adversarial domain changes in image classification include the anti-causal nature of the task and the need to guard against style feature shifts. Various approaches leveraging causal motivations for deep learning have been proposed, but they do not address the specific setting of anti-causal prediction and non-ancestral interventions on style variables. Various approaches have been proposed that leverage causal motivations for deep learning or use deep learning for causal inference, but they do not consider anti-causal prediction and non-ancestral interventions on style variables. The Neural Causation Coefficient (NCC) is used to estimate the probability of X causing Y and distinguish between features of objects and features of the objects' contexts. CGANs are also utilized to find causal relations between image features. The NCC is used to find causal relations between image features, distinguishing between object features and context features. CGANs are fitted in different directions to determine causal relationships, and generative neural networks are used for cause-effect inference. Regularizers combine penalties with estimated probabilities of features being causal, obtained through causality detection networks or scores like the NCC. Connections between GANs and causal generative models are drawn using a group theoretic framework. Incorporating causal reasoning, various models like causal implicit generative models and deep latent variable models are proposed to estimate individual treatment effects and address fairness considerations in machine learning. The generator structure in these models must align with the known causal graph structure. BID29 proposes using deep latent variable models and proxy variables to estimate individual treatment effects, while BID21 utilizes causal reasoning to address fairness considerations in machine learning by deriving causal nondiscrimination criteria. The resulting algorithms aim to avoid proxy discrimination by requiring classifiers to be constant as a function of the proxy variables in the causal graph, similar to disentangling factors of variation in generative modeling. Estimating disentangled factors of variation in generative modeling has gained interest. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce dependence on specified object transforms in the latent representation. They define location as an orthogonal style feature and aim to learn a representation without it. This approach does not predefine which features are orthogonal, it could include location, image quality, posture, brightness, background, and contextual information. In Matsuo et al. (2017), the goal is to learn a latent representation without feature X \u22a5, which could be location, image quality, posture, brightness, background, or contextual information. They address a confounding situation where style features differ based on class, using grouped observations in a variational autoencoder framework to separate style and content. In a variational autoencoder framework, Bouchacourt et al. (2017) aim to separate style and content by exploiting grouped observations, assuming samples within a group share a common but unknown value for one factor of variation while the style can differ. They address a classification task directly without estimating latent factors explicitly in a generative framework, comparing adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. The text discusses the standard notation for classification and the development of a causal graph to compare adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. It explains the target of interest, predictor variables, prediction function, loss function, and training data samples. The goal is to minimize the expected loss in the context of regression or classification tasks. The text discusses parameter estimation through penalized empirical risk minimization in regression and classification tasks. It introduces the concept of a penalty function, such as a ridge penalty, to optimize the model parameters. The structural model includes latent variables and the addition of an identity variable. The structural model includes latent variables and an identity variable (ID) that can change based on class Y. The prediction is anti-causal, with predictors X being non-ancestral to Y. Causal effects from Y on X are mediated by core features X ci and orthogonal style features X \u22a5. The class label Y is causal for the image X, with causal effects mediated by core features X ci and style features X \u22a5. Interventions are possible on style features but not on core features. The style features X \u22a5 and Y are confounded by the latent domain D, while the core features X ci are conditionally independent of D given Y. The distribution P (X ci |Y ) remains constant across domains, while P (X \u22a5 |Y ) can vary. Style features X \u22a5 and Y are confounded by latent domain D. Core features X ci are conditionally independent of D given Y. Style interventions influence both X \u22a5 and X, with f \u03b8 (X(\u2206 = \u03b4)) predicting outcomes under style intervention \u2206 = \u03b4. In potential outcome notation, X \u22a5 (\u2206 = \u03b4) represents style under intervention \u2206 = \u03b4, X(Y, ID, \u2206 = \u03b4) represents the image for class Y and identity ID under style intervention \u2206. The prediction under style intervention \u2206 = \u03b4 is denoted as f \u03b8 (X(\u2206 = \u03b4)). The causal graph is used to explain domain adaptation, transfer learning, and guarding against adversarial examples. The intervention \u2206 is typically assumed to be within an -ball in q-norm around the origin. The text discusses the goals of domain adaptation, transfer learning, and guarding against adversarial examples using a causal graph. It focuses on minimizing adversarial loss by intervening on style features to estimate conditional distribution. The goal is to classify images to prevent misclassification due to imperceptible changes in input dimensions. Adversarial domain shifts involve strong interventions on style features not explicitly known. The text discusses minimizing adversarial loss by intervening on style features to estimate conditional distribution. Adversarial domain shifts involve strong interventions on style features not explicitly known. The text discusses adversarial interventions on style features to protect against distribution shifts in test data. It highlights the challenge of causal inference in observing counterfactuals. The text discusses the challenge of observing counterfactuals in causal inference, emphasizing the impossibility of observing both treatment outcomes simultaneously. It also introduces the concept of counterfactuals in the context of keeping class label and ID constant while allowing the style intervention to change. In causal inference, observing counterfactuals is challenging as one can only observe the outcome under treatment or no treatment but not both. Counterfactuals involve keeping class label and ID constant while allowing the style intervention to change, similar to a treatment in a medical example. This allows for seeing the same object under different conditions in image analysis. For example, in the context of a person wearing glasses, the style intervention includes variables like background, posture, and viewing angle. In image analysis, the style intervention (\u2206) allows for observing the same object (Y, ID) under different conditions, such as background, posture, and viewing angle. The focus is not on the treatment effect of \u2206 but on ruling out parts of the feature space for classification. Any change in classification under different style interventions is penalized, while keeping class and identity constant. The feature space for classification is analyzed, focusing on penalizing changes in classification under different style interventions while keeping class and identity constant. The standard approach is to pool observations and use a ridge penalty in the pooled estimator. The pooled estimator in classification uses a ridge penalty and treats all examples identically. It works well in terms of adversarial loss and relies on conditions related to the information extracted from images. The pooled estimator in classification relies on conditions related to the information extracted from images to work well in terms of the adversarial loss. It ensures that certain relations between Y, Xci, and X\u22a5 are not deterministic, and that specific edges are absent to minimize the adversarial loss. The pooled estimator in classification relies on conditions related to the information extracted from images to minimize the adversarial loss. It ensures that specific edges are absent to achieve this goal, with the challenge of inferring the invariant space from data to optimize the predictor. The invariant space I is approximated using empirical risk minimization, where the unknown parameters are inferred from data. The empirically invariant space In is defined with a regularization constant \u03c4 to allow for variations in predictions. The regularization constant \u03c4 allows for variations in predictions for class labels across counterfactuals of image i. The true invariant space I is a subset of the empirically invariant subspace In. The graph Laplacian regularization penalizes the sum of variances \u03c32. The regularization parameter \u03bb replaces the constraint \u03c4 in the graph Laplacian regularization, forming a graph based on the identifier variable ID. This approach outperforms other regularization methods in guarding against adversarial attacks. The regularization parameter \u03bb is used to form a graph based on the identifier variable ID, outperforming other methods in guarding against adversarial attacks. The outcome is not sensitive to the penalty value \u03bb, and defining the graph in terms of ID is crucial. The adversarial loss is analyzed for the pooled and CORE estimator in logistic regression, showing that the pooled estimator has infinite adversarial loss under certain assumptions. In logistic regression, the CORE estimator converges to optimal adversarial loss as n \u2192 \u221e, handling confounded training data and changing style features. Experiments include classifying elephants and horses based on color, gender, wearing glasses, and brightness. The CORE estimator in logistic regression handles confounded data and changing style features. Experiments involve classifying elephants and horses based on color, gender, wearing glasses, and brightness. Additional experimental results and implementation details are provided in subsequent sections. In logistic regression, the tuning parameter \u03bb is not very sensitive to performance. Stickmen images are used to differentiate between adults and children based on height. Age and movement are related through a hidden common cause in the dataset. The data generating process is illustrated in a figure. The dependence between age and movement in the dataset arises from a hidden common cause. The training dataset shows that large movements are associated with children and small movements with adults. However, when this dependence is removed in test sets 2 and 3, movements are no longer indicative of age. Test set 3 has heavier movements compared to test set 2. In test sets 2 and 3, movements are intervened on to remove the dependence between age and movement. Test set 3 has heavier movements compared to test set 2. Misclassification rates for CORE and the pooled estimator are shown in FIG0, with CORE outperforming the pooled estimator with as few as 50 counterfactual observations. This suggests that the pooled estimator uses movement as a predictor for age, while CORE does not due to counterfactual regularization. Including more counterfactual examples would not improve the performance of the pooled estimator. The pooled estimator's performance on test sets 2 and 3 is poor, with test errors exceeding 40%. The representation of the pooled estimator relies on movement as an age predictor, unlike CORE which avoids this bias through counterfactual regularization. Including more counterfactual examples would not enhance the pooled estimator's performance due to the same bias issue. The CelebA dataset is used to classify images based on eyeglass wear, with image quality varying based on this attribute. In the CelebA dataset, the problem of classifying whether a person is wearing eyeglasses is considered. The image quality is affected by this attribute, mimicking confounding similar to the Russian tank legend. Counterfactual observations are created by sampling image quality from a Gaussian distribution. Different methods for constructing counterfactual observations are discussed. Counterfactual observations for glasses in the CelebA dataset are created by sampling image quality from a Gaussian distribution. The misclassification rates for CORE and the pooled estimator are compared on different test sets, where the pooled estimator performs better on test set 1. Test sets vary in the quality intervention, with some reversing the class of image quality or decreasing the quality of all images. The pooled estimator outperforms CORE on test set 1 by utilizing predictive information from image quality. However, the pooled estimator struggles on test sets 2-4 where image quality affects its performance, unlike CORE which is less affected by changing image quality distributions. Experimental details and results for different quality interventions are provided in \u00a7C.5. In contrast to the pooled estimator, CORE's predictive performance is not significantly impacted by changes in image quality distributions. Experimental details for quality interventions are shown in FIG0.12. The study aims to assess if CORE can exclude \"color\" from its learned representation by including counterfactuals of different colors using the AwA2 dataset. In the study, the CORE model's ability to exclude \"color\" from its learned representation is tested by adding counterfactual examples of different colors to images of horses and elephants from the AwA2 dataset. Counterfactuals are grayscale images added for elephants, with a total sample size of 1850. Results comparing CORE and the pooled estimator's misclassification rates on different test sets are shown in figures. Test set 1 contains original colored images, while test set 2 has grayscale images of horses and elephants. The study tested the CORE model's ability to exclude color from its representation by adding counterfactual examples to images of horses and elephants. Test sets with different color modifications were used, showing that the pooled estimator struggled with changing color distributions, while CORE's performance was hardly affected. The pooled estimator struggles with changing color distributions, while the CORE model's performance is hardly affected. The CORE model demands invariance of the prediction for instances of the same elephant and can learn color invariance with a few added grayscale images. The CORE estimator ensures color invariance by learning with grayscale images, satisfying fairness by not including color in its representation. It distinguishes core and style features in images, using counterfactual regularization to achieve robustness against style interventions. The CORE estimator achieves robustness by distinguishing core and style features in images, ensuring color invariance and classification performance despite sampling biases. It exploits instances of the same object in training data to achieve invariance with respect to style features like image quality, fashion type, and body posture. The CORE estimator achieves robustness by distinguishing core and style features in images, ensuring color invariance and classification performance despite sampling biases. It works with known style features to achieve classification performance with fewer instances and automatically penalizes features that vary strongly between instances of the same object. Using larger models like Inception or ResNet architectures could further enhance the regularization approach. The text discusses the potential benefits of using larger models like Inception or ResNet architectures for training models end-to-end. It also suggests exploring the use of video data to leverage temporal information for grouping and counterfactual regularization, which could potentially help debias word embeddings. An interesting future direction could involve using video data to leverage temporal information for grouping and counterfactual regularization, potentially aiding in debiasing word embeddings. The text also discusses linear structural equations for image and style features, with interventions acting additively on style features and style features influencing images linearly through a matrix. The core features are conditionally invariant. The text discusses interventions acting additively on style features and their linear influence on images through a matrix. Core features are conditionally invariant, with latent variables and logistic regression used for prediction. The text discusses logistic regression for predicting Y from image data X, with training data used to estimate \u03b8 and logistic loss for training and testing. It also explores expected losses on test data under different interventions on style variables. The formulation of Theorem 1 relies on specific assumptions regarding the distribution of the interventions. The loss under adversarial style interventions is discussed, with specific assumptions required for the formulation of Theorem 1. These assumptions include conditions on the distribution of interventions and the number of counterfactual examples in the samples. The sampling process involves collecting independent samples from a distribution that satisfies certain constraints. The pooled estimator has infinite adversarial loss under Assumption 1, and an equivalent result can be derived for misclassification loss. The pooled estimator has infinite adversarial loss under Assumption 1, and an equivalent result can be derived for misclassification loss. Theorem 1 states that with probability 1, the estimator has infinite adversarial loss. The proof involves showing that W t\u03b8pool = 0 with probability 1, leading to the conclusion that \u03b8 pool = \u03b8 * under certain constraints. The proof involves showing that W t\u03b8pool = 0 with probability 1, leading to the conclusion that \u03b8 pool = \u03b8 * under certain constraints. This implies that taking the directional derivative of the training loss with respect to any \u03b4 \u2208 R p in the column space of W should vanish at the solution\u03b8 * . The derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8 * due to the effect of interventions only on the column space of W. This implies that the estimator \u03b8 * remains identical under true training data and counterfactual training data. The model assumptions imply that interventions only affect the column space of W, leading to the estimator \u03b8* being the same for both true and counterfactual training data. The eigenvalues of WtW are positive, and the interventions \u2206i,j are drawn from a continuous distribution. The probability of the left side of the equation not being 0 is 1, completing the proof by contradiction. With probability 1, \u03b8core = \u03b8*. The interventions \u2206i,j are drawn from a continuous distribution, leading to the left side of the equation having a continuous distribution. The probability of it not being 0 is 1, completing the proof by contradiction. With probability 1, \u03b8core = \u03b8*. With probability 1, the estimator remains unchanged when using data without interventions as training data. The population-optimal vector can be written as a linear combination of the counterfactual data. Comparing different formulas, we can show uniform convergence of the loss under sampling assumptions. This completes the proof. The CelebA dataset is used to classify gender based on images, creating confounding by including mostly men wearing glasses. Counterfactuals are used to address this, with \"CF setting 2\" involving using images of the same person without glasses for males and with glasses for females. The study uses the CelebA dataset to classify gender based on images, creating confounding by including mostly men wearing glasses. Counterfactuals are employed, with \"CF setting 2\" using images of the same person without glasses for males and with glasses for females. Test sets with different gender-glasses associations are used to assess the impact of training a four-layer CNN versus using Inception V3 features. In this study, the focus is on training a four-layer CNN versus using Inception V3 features for classifying whether a person in an image is wearing glasses. Results show similar trends in performance as the complexity of the task increases. The pooled estimator performs worse on test set 2 as the number of counterfactual examples grows, indicating a larger exploitation of the data. The study works with the CelebA dataset and explores a confounded setting for classification. The study focuses on training a four-layer CNN versus using Inception V3 features to classify if a person in an image wears glasses. The pooled estimator performs worse on test set 2 as the number of counterfactual examples increases, indicating a larger exploitation of the data. The CelebA dataset is used to analyze a confounded setting for classification, where the hidden common cause of Y and X \u22a5 is brightness. The study compares the performance of a pooled estimator and CORE on different test sets. Test set 1 mirrors the training set, test set 2 reverses the brightness intervention, test set 3 keeps images unchanged, and test set 4 increases brightness. The pooled estimator outperforms CORE on test set 1 by leveraging brightness information, but struggles on test sets 2 and 4 due to its learned representation. The pooled estimator outperforms CORE on test set 1 by leveraging brightness information but struggles on test sets 2 and 4 due to its learned representation. The predictive performance of CORE is hardly affected by changing brightness distributions. Results for different parameters can be found in FIG0.5. The pooled estimator outperforms CORE on test set 1 by leveraging brightness information but struggles on test sets 2 and 4. Results for different parameters can be found in FIG0.5. Counterfactual settings 1, 2, and 3 were evaluated, with setting 1 showing the best results in isolating brightness variations. Counterfactual settings 1, 2, and 3 were evaluated to isolate brightness variations. Setting 1 worked best, controlling only brightness. Performance of CORE on the AwA2 dataset was not very sensitive to the penalty \u03bb. Further results for the experiment introduced in \u00a72.1 were shown, varying the number of identities included. The tuning parameter \u03c4 or penalty \u03bb in Eq. (4) does not significantly affect the performance of CORE on the AwA2 dataset. Varying the number of identities in the training data set shows improved predictive performance, especially with small sample sizes. CORE helps mitigate potential confounders due to small sample sizes, leading to better performance as the number of identities and sample sizes increase. The performance of CORE improves with increasing sample sizes and number of identities in the training data. It helps mitigate confounders due to small sample sizes, leading to better predictive performance. Test results show that CORE outperforms the pooled estimator, especially with small sample sizes. The experiment in \u00a72.2 varied the number of augmented training examples from 100 to 5000 for n = 10000 and c \u2208 {100, 200, 500, 1000} for n = 1000. The rotations were sampled uniformly at random from [35, 70]. Misclassification rates were lower for CORE on test set 1, making data augmentation more efficient. Results for different numbers of counterfactual examples were shown in FIG0 10b, with CORE estimator showing similar results for c \u2208 {50, 500, 2000}. In further results for the experiment introduced in \u00a75.1, FIG0 10b shows the performance of the CORE estimator for different numbers of counterfactual examples. The misclassification rate for c = 20 has a large variance, while for c \u2208 {50, 500, 2000}, the CORE estimator shows consistent results. The pooled estimator struggles with predictive performance on test sets 2 and 3. Experiments were conducted for counterfactual settings 1-3 and for c = 5000, with FIG0 .11 displaying examples from the training and test sets, and FIG0 .12 showing the corresponding misclassification rates. Counterfactual setting 1 performs the best, with minor differences between settings 2 and 3. Experiments were conducted for counterfactual settings 1-3 and c = 5000, showing that setting 1 had the best performance. There were small differences between settings 2 and 3. A large performance gap was observed between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator, possibly due to image quality not being predictive enough. The models were implemented in TensorFlow BID0, with the same network architecture used for CORE and the pooled estimator. In experiments using TensorFlow BID0, models were trained with different settings and evaluated for performance. The latter command rotates image colors cyclically, and in test set 3, images are changed to grayscale. The models use the same network architecture and training procedure, with variations in the loss function. Adam optimizer BID22 is used, and models are trained five times to assess variance. Training data is shuffled in each epoch, with mini batches containing counterfactual observations. Mini batch size is set to 120 in all experiments. In each epoch of training, the data is shuffled to ensure mini batches contain counterfactual observations. Mini batch size is set to 120 in all experiments, making optimization more challenging for small c."
}