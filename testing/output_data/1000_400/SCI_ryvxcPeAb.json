{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, the adversarial perturbation is decomposed into model-specific and data-dependent components, with the latter contributing mainly to transferability. Adversarial examples are crafted using noise reduced gradient (NRG) to approximate the data-dependent component, enhancing transferability significantly across various ImageNet classification models. Low-capacity models outperform high-capacity models in attack capability with comparable test performance. These findings offer a principled approach to constructing successful adversarial examples and insights for designing defense strategies against black-box attacks in the era of neural networks. Adversarial examples can be crafted to fool neural network models, with low-capacity models outperforming high-capacity ones in attack capability. This raises questions on understanding and defending against such attacks, with potential implications for designing effective defense strategies in the era of neural networks. Adversarial examples can manipulate inputs to deceive models, leading to incorrect outputs. These manipulated samples can transfer across different models, posing a threat to system security. The vulnerability to adversarial attacks is a significant concern in the realm of deep neural networks. The vulnerability to adversarial attacks in deep neural networks has been studied extensively. Different methods such as FGSM, DeepFool, and ensemble-based approaches have been proposed to generate adversarial examples for black-box systems. Transferability of adversarial examples and the impact of high dimensionality have also been analyzed. Several methods have been proposed to generate adversarial examples for black-box systems, including DeepFool and ensemble-based approaches. Transferability of adversarial examples and defense mechanisms such as defensive distillation and adversarial training have also been explored. Additionally, image transformation techniques have been used to mitigate the impact of adversarial perturbations. Some works focus on detecting adversarial attacks rather than making classifiers more robust. In this work, the authors explain the transferability of adversarial examples and enhance black-box attacks by decomposing adversarial perturbations into model-specific and data-dependent components. They observe that the model-specific component is noisy and influenced by the model architecture and random initialization. The transferability of adversarial examples is explained by decomposing perturbations into model-specific and data-dependent components. The data-dependent part contributes to transferability across models, leading to the proposal of the noise-reduced gradient (NRG) method for constructing adversarial examples. Benchmark results on the ImageNet validation set support this approach. The proposed noise-reduced gradient (NRG) method enhances black-box attacks on ImageNet validation set by utilizing the data-dependent component of the gradient. Model-specific factors like capacity and accuracy influence the success rate of attacks, with higher accuracy and lower capacity models showing greater capability to attack unseen models. This phenomenon is attributed to transferability and provides insights for attacking unseen models. The success rate of black-box attacks depends on model-specific factors like capacity and accuracy. Models with higher accuracy and lower capacity are better at attacking unseen models. This can be explained by transferability and provides guidance for attacking unseen models. The model function f : R d \u2192 R K is obtained by minimizing empirical risk over the training set. High dimensionality makes the model vulnerable to adversarial perturbations, where a small imperceptible perturbation \u03b7 exists for each input x. The high dimensionality of the model makes it vulnerable to adversarial perturbations, where small imperceptible perturbations exist for each input. Adversarial examples are studied mainly in the context of deep neural networks, but also exist in other models like support vector machines and decision trees. Non-targeted attacks aim to misclassify the input, while targeted attacks aim to produce a specific wrong label. In non-targeted attacks, the adversary aims to misclassify the input without control over the output. In a black-box attack, the adversary lacks knowledge of the target model but can craft adversarial examples on a local model to fool the target model. This contrasts with white-box attacks targeting the source model itself. The adversary can create adversarial examples on a local model trained on the same dataset as the target model, deploying them to deceive the target model in a black-box attack. Crafting adversarial perturbation involves optimizing a loss function J to measure prediction discrepancies, with a constraint for image data. BID2 introduced a loss function manipulating output logit directly, commonly used in works. The distortion measurement is crucial in evaluating adversarial examples. The perturbation magnitude for image data is constrained to [0, 255] with the number of pixels. The commonly used loss function is cross entropy, but BID2 introduced a loss function that manipulates output logit directly. Distortion measurement is challenging to quantify, with \u221e and 2 norms commonly used. Ensemble-based approaches, like using a large ensemble of source models, can improve adversarial example strength. Averaging predicted probabilities of each model is a common ensemble strategy. Using a large ensemble of source models can improve adversarial example strength. A common ensemble strategy is averaging predicted probabilities of each model. The normalized-gradient based optimizer is mainly used to solve optimization problems. The Fast Gradient Based Method attempts to solve problems by performing one-step iterations. The method is empirically shown to be fast and effective for both \u221e and q-attacks. The normalized-gradient based optimizer includes the Fast Gradient Based Method and Iterative Gradient Method. The Fast Gradient Based Method performs one-step iterations with a normalized gradient vector, shown to be fast and effective for both \u221e and q-attacks. The Iterative Gradient Method performs normalized-gradient ascent for k steps, using a projection operator and step size. The Iterative Gradient Method performs normalized-gradient ascent for k steps, using a projection operator and step size. Transferability of adversarial examples between models is crucial for black-box attacks and defenses, with research suggesting it comes from similarity in decision boundaries and a contiguous subspace. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. Research suggests that similarity in decision boundaries and a contiguous subspace enable transferability. Models A and B, having high performance on the same dataset, must have learned a similar function on the data manifold. However, their behavior off the data manifold can differ due to architectural differences and random initializations. This suggests decomposing the perturbation into on and off data manifold factors. The behavior of models A and B on and off the data manifold can differ due to architectural differences and random initializations. Decomposing the perturbation into data-dependent and model-specific components, we hypothesize that the transferability between models mainly comes from the data-dependent information shared on the data manifold. Model-specific components contribute less to transferability due to different behaviors off the data manifold. The transferability between models A and B is mainly influenced by the data-dependent information shared on the data manifold. The model-specific components contribute less to transferability due to their different behaviors off the data manifold. The adversarial perturbation crafted from model A can mislead both models, with the data-dependent component attacking model B easily. The model-specific component contributes little to the transfer from A to B. The perturbation is split into a data-dependent component \u2207f A and a model-specific one \u2207f A \u22a5. \u2207f A aligns with inter-class deviation, making it effective for attacking model B. The model-specific \u2207f A \u22a5 contributes little to the transfer between models A and B. The NRG method aims to enhance success rates of black-box attacks by reducing model-specific noise. The NRG method proposes reducing model-specific noise to increase success rates of black-box attacks. By applying local average to remove noisy information, NRG captures more data-dependent details than the ordinary gradient. This method is effective in visualizing NRG for various sample sizes. The Noise-Reduced Gradient (NRG) method aims to remove model-specific noise to improve black-box attack success rates. By using a local average to reduce noise, NRG captures more data-dependent information than the ordinary gradient. This approach is visualized for different sample sizes, showing that larger samples lead to smoother and more data-dependent gradients. The proposed Noise-Reduced Iterative Sign Gradient Method (nr-IGSM) drives the optimizer towards more data-dependent solutions by using NRG instead of the noisy model-specific gradient. The Noise-Reduced Gradient (NRG) method, specifically the Noise-Reduced Iterative Sign Gradient Method (nr-IGSM), drives the optimizer towards data-dependent solutions by using NRG instead of the model-specific gradient. This approach aims to enhance transferability in black-box attacks, as demonstrated with classification models trained on the ImageNet dataset. The study focuses on using Noise-Reduced Gradient (NRG) method, specifically nr-IGSM, to enhance transferability in black-box attacks with classification models trained on ImageNet dataset. The experiments involve using ImageNet ILSVRC2012 validation set with pre-trained models like resnet and vgg provided by PyTorch. The study constructs adversarial perturbations targeting models unable to classify images correctly. Various pre-trained models from PyTorch are used, including resnet and vgg models. Adversarial examples are evaluated for white-box attack performance and targeted attacks with specified labels. In this study, adversarial examples are generated targeting models like resnet and vgg. The Top-1 success rate is used to evaluate targeted attacks with specified labels. The cross entropy loss function and two distortion measures are considered. Both FGSM and IGSM optimizers are examined for effectiveness. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with commonly-used methods like FGSM and IGSM optimizers. Results show that nr-FGSM performs better than original FGSM consistently and dramatically for both blackbox and white-box attacks, indicating the enhanced transferability of noise-reduced gradient. The noise-reduced gradient technique, when combined with FGSM and IGSM optimizers, shows significant improvements in performance for both blackbox and white-box attacks. Nr-FGSM outperforms original FGSM consistently, indicating enhanced transferability. Nr-IGSM also generates adversarial examples that transfer more easily than IGSM, suggesting that noise-reduced gradient guides the optimizer towards more data-dependent solutions. Table 2 shows that adversarial examples generated by nr-IGSM transfer more easily than those by IGSM, indicating the noise-reduced gradient guides the optimizer towards data-dependent solutions. Large models like resnet152 are more robust to adversarial transfer than small models. IGSM generally generates stronger adversarial examples than FGSM, except for attacks against alexnet, contradicting previous claims. The model-specific component contributes to transfer across models with similar architectures. IGSM generally generates stronger adversarial examples than FGSM, except for attacks against alexnet. The higher confidence adversarial examples have in the source model, the more likely they will transfer to the target model. Inappropriate hyperparameters may lead to underfitting. Alexnet is significantly different from source models in architecture and test accuracy. The inappropriate choice of hyperparameters, such as \u03b1 = 1 and k = min(\u03b5 + 4, 1.24\u03b5) in BID6, leads to underfitting. The alexnet model differs significantly from source models in architecture and test accuracy, causing IGSM to overfit more than FGSM. Trusting the objective in Eq. (2) completely may result in overfitting to source model-specific information and poor transferability. The noise reduced gradient technique removes model-specific information from gradients, leading to a more data-dependent solution with better cross-model generalization capability. Our noise reduced gradient technique regularizes the optimizer by removing model-specific information, leading to better cross-model generalization capability. NRG method is applied to ensemble-based approaches with a reduced evaluation set of 1,000 images. Both non-targeted and targeted attacks are tested, showing improvements in Top-5 success rates for IGSM attacks. Results for FGSM and nr-FGSM attacks can be found in Appendix C. The Top-1 success rates of IGSM attacks are nearly saturated, so the corresponding Top-5 rates are reported in Table 3 (a) to show the improvements more clearly. Targeted attacks are harder to generate than non-targeted examples, and single-model approaches are ineffective for this. Targeted adversarial examples are sensitive to the step size used in optimization procedures, with a large step size necessary for strong examples. More detailed analysis can be found in Appendix A. In targeted attacks, a large step size is crucial for generating strong adversarial examples. The NRG methods outperform normal methods by a significant margin in both targeted and non-targeted scenarios, as shown in Table 3. The Top-5 success rates of ensemble-based approaches are reported, indicating attack performances against the target model. In Table 3, NRG methods significantly outperform normal methods in both targeted and non-targeted attacks. Sensitivity of hyper parameters m, \u03c3 is explored for black-box attacks using nr-FGSM approach. Results for four attacks are shown in FIG6. In this section, the sensitivity of hyper parameters m and \u03c3 is explored for black-box attacks using the nr-FGSM approach. Larger m leads to a higher fooling rate due to better estimation of the data-dependent gradient direction. An optimal value of \u03c3 is found for best performance, with overly large or small values being detrimental. The optimal \u03c3 varies for different source models, with values around 15 for resnet18 and 20 for densenet161 in this experiment. The optimal \u03c3 varies for different source models, with values around 15 for resnet18 and 20 for densenet161. The robustness of adversarial perturbations to image transformations is explored, which is important for real-world applications. Destruction rate is used to quantify the influence of transformations on adversarial examples surviving in the physical world. The destruction rate is used to quantify the impact of transformations on adversarial examples surviving in the physical world. Densenet121 and resnet34 models are compared using rotation, Gaussian noise, Gaussian blur, and JPEG compression transformations. Results show that adversarial examples generated by NRG methods are more robust than vanilla methods. Decision boundaries of different models are studied to understand the performance of NRG-based methods. The study compares the robustness of adversarial examples generated by NRG methods against vanilla methods using various transformations. Decision boundaries of different models are analyzed to understand the superior performance of NRG-based methods. The study compares the robustness of adversarial examples generated by NRG methods against vanilla methods using various transformations. Each point in a 2-D plane corresponds to an image perturbed along different directions. The direction of sign \u2207f is as sensitive as sign (\u2207f \u22a5 ) for the source model resnet34, but other target models are more sensitive along sign \u2207f. Removing \u2207f \u22a5 penalizes the optimizer along the model-specific direction, avoiding overfitting solutions. The study compares the robustness of adversarial examples generated by NRG methods against vanilla methods using various transformations. The direction of sign \u2207f is as sensitive as sign (\u2207f \u22a5 ) for the source model resnet34, but other target models are more sensitive along sign \u2207f. Removing \u2207f \u22a5 penalizes the optimizer along the model-specific direction, avoiding overfitting solutions. Additionally, the minimal distance u to produce adversarial transfer varies for different models, with larger distances for complex models like resnet152 compared to smaller models like resnet50. Adversarial examples crafted from alexnet generalize worst across models. The comparison between different models like resnet152 and resnet50 shows why big models are more robust. Adversarial examples from alexnet perform poorly across models, while attacks from densenet121 transfer well to different models. This suggests that different models have varying performances in attacking the same target model. The study aims to understand this phenomenon to guide the selection of a better local model for generating adversarial examples. Target models vgg19 bn and resnet152 are chosen for further analysis. Different models exhibit varying performances in attacking the same target model. By selecting vgg19 bn and resnet152 as target models, it was found that models with smaller test error and lower capacity have stronger attack capabilities. The results are summarized in FIG11, showing that models with powerful attack capability concentrate in the bottom left corner. The models with powerful attack capability concentrate in the bottom left corner, while those with large test error and number of parameters have low fooling rates. Smaller test error and lower capacity indicate stronger attack capability. This phenomenon can be explained by transferability, where a less complex model facilitates the data-dependent factor to dominate, leading to strong adversarial examples. In this study, it was found that a less complex model allows the data-dependent factor to dominate, resulting in strong adversarial examples. Adversarial perturbations were decomposed into model-specific and data-dependent components, with the latter contributing more to the transferability of adversarial examples. Noise-reduced gradient (NRG) based methods were proposed to craft adversarial examples effectively. Based on the understanding that data-dependent components contribute more to the transferability of adversarial examples, noise-reduced gradient (NRG) based methods were proposed for crafting more effective adversarial examples. Models with lower capacity and higher test accuracy are better equipped for black-box attacks. Future plans include combining NRG-based methods with adversarial training to defend against black-box attacks, as data-dependent factors are low-dimensional and hypothesized to be defensible, unlike white-box attacks originating from high-dimensional spaces. Combining NRG-based methods with adversarial training to defend against black-box attacks. The transferability component is data-dependent and low-dimensional, making black-box attacks defensible. White-box attacks originate from high-dimensional spaces, making defense more challenging. Future research includes learning stable features for transfer learning with NRG strategy. Evaluating the influence of hyperparameters on the quality of adversarial examples generated using IGSM. The influence of hyperparameters on the quality of adversarial examples generated using IGSM is explored. Success rates are evaluated on 1,000 images with resnet152 and vgg16 bn as target models. The optimal step size is crucial, with both too large and too small sizes harming attack performance. Interestingly, a small step size with a large number of iterations can perform worse than a small number of iterations. In an experiment on the MNIST dataset, the influence of model redundancy on attack capability was further investigated. Different models with varying depths were considered, with the success rates of cross-model attacks being evaluated. The results showed that a large step size can prevent overfitting and encourage exploration of model-independent areas, leading to better performance. In an additional experiment on the MNIST dataset, models of different depths were tested for their attack capability. Results showed that low-capacity models had stronger attack capabilities compared to large-capacity models. The success rates of FGSM and IGSM attacks against resnet152 were also evaluated, with a chosen distortion of \u03b5 = 15."
}