{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The student can be trained independently of the teachers and shows superior performance in various supervised and reinforcement learning tasks compared to fine-tuning and other knowledge exchange methods. Research communities have developed various deep net architectures for different tasks, with some trained from scratch and others fine-tuned using structurally similar networks. In reinforcement learning, different approaches like progressive neural nets, PathNet, 'Growing a Brain', and Actor-mimic have been explored to improve model performance. Various deep net architectures have been developed for different tasks, including progressive neural nets, PathNet, 'Growing a Brain', Actor-mimic, and Knowledge distillation. However, these techniques have limitations such as computational complexity and parameter constraints. Knowledge flow addresses limitations of existing techniques by moving knowledge from multiple teachers to train a student model. This ensures independence of the student model regardless of the number of teachers used, with a constant size of the resulting student net. Knowledge flow addresses limitations by transferring knowledge from multiple teachers to train a student model, ensuring independence regardless of teacher size. The approach is flexible for various tasks and does not restrict deep net sizes. It is applicable to tasks from reinforcement learning to fully-supervised training and is evaluated on different tasks. The text discusses evaluating knowledge flow on tasks ranging from reinforcement learning to fully-supervised learning. It compares different approaches using the same discount factor and defines the expected future reward. The goal is to find a policy that maximizes future rewards, following the A3C formulation. The policy and value function are modeled using deep nets with specific parameters. In this paper, the asynchronous advantage actor-critic (A3C) formulation is followed. The policy mapping and value function are modeled using deep nets with specific parameters. The optimization of policy parameters involves a loss function based on negative log-likelihood and a negative entropy regularizer. The scalar \u03b2 encourages exploration by favoring a uniform probability distribution. The paper follows the A3C formulation, using deep nets to model policy mapping and value function. Optimization involves a loss function based on negative log-likelihood and entropy regularization. The scalar \u03b2 encourages exploration with a uniform distribution. The proposed knowledge flow framework addresses optimization shortcomings by transferring knowledge between deep nets. The paper introduces a knowledge flow framework to transfer knowledge from multiple pre-trained deep nets (teachers) to a deep net under training (student) to improve optimization. Initially, the student heavily relies on one teacher, but as training progresses, the student becomes independent. This process is illustrated using example deep nets in a figure. The knowledge flow framework transfers knowledge from multiple pre-trained deep nets (teachers) to a deep net under training (student) to improve optimization. Initially, the student relies on one teacher, but as training progresses, the student becomes independent. The student net's parameters are trained while taking advantage of fixed teacher parameters obtained from pre-trained models on different source tasks. Teacher representations are added to the student net by transforming and scaling intermediate representations. The knowledge flow framework transfers knowledge from pre-trained deep nets to a student net by adding transformed and scaled teacher representations. Teacher input is adjusted using a trainable matrix and normalized weights to guide the student net's learning process. The goal is for the student model to perform well on its own without relying on teachers. During training, the student model initially relies heavily on the teacher's knowledge for better performance but gradually becomes more independent. By the final stages, the student must master the task on its own without relying on teachers, as shown in FIG0. Two additional loss functions are introduced to encourage this transfer of knowledge. During training, the student model transitions from relying on teachers to becoming more independent. Two additional loss functions are introduced to encourage this transfer of knowledge, ensuring that the student masters the task on its own. The student model gradually becomes more independent during training, with the teacher's influence decreasing. Additional loss functions are introduced to facilitate this transition, ensuring the student can master the task independently. The student model gradually becomes more independent during training, with the teacher's influence decreasing. Parameters \u03b8 and \u03b8 old are used to control the strength of the teacher's influence. The method adjusts \u03bb 1 and \u03bb 2 to decrease the weight of teacher layers and prevent negative transfer. Despite differences in objectives, students can benefit from low-level knowledge transfer from teachers. The proposed method quickly decreases the weight for teacher layers to reduce their influence on the student model. Low-level knowledge transfer from teachers is observed in experiments. To decide which representation to trust at each layer, normalized weights are introduced. The proposed method reduces the influence of teacher layers on the student model by introducing normalized weights for each layer. The combined intermediate representation of the student model is obtained using a specific formula. The number of introduced matrices in the framework is limited. In practice, not every student layer is linked to every teacher network layer. In our framework, we limit the number of introduced matrices Q and recommend linking one teacher layer to one or two student layers for optimal results. Additional trainable parameters Q and w are introduced but are not part of the resulting student network. In the final stage of training, the student becomes independent and no longer relies on additional parameters like Q and w introduced in the framework. The influence of teachers is gradually decreased during training to encourage the student to rely less on them. During training, the student becomes independent and no longer relies on teachers' influence, as the weights for the student's layers increase. This encourages the student to become more independent gradually. However, a fast decrease in the teacher's influence can degrade performance as it takes time to find good transformations. During the final stage of training, the student becomes independent of the teachers' influence, with a gradual increase in the student's weights. A fast decrease in the teacher's influence can harm performance, as it takes time to find optimal transformations. To prevent rapid changes in the student's output distribution, a Kullback-Leibler regularizer is used. During the final stage of training, a Kullback-Leibler regularizer is used to prevent rapid changes in the student's output distribution. In supervised learning, \u03b8 represents current parameters and \u03b8 old represents previous ones. In reinforcement learning, knowledge flow is evaluated using Atari games with raw images as input. The agent predicts actions based on rewards and input images, choosing an action every four frames. The fully forward architecture of A3C is used for all teacher and student models. The agent learns to predict actions based on rewards and input images from the environment using a fully forward architecture of A3C. It has three hidden layers with specific filter sizes and strides. The model outputs a probability distribution over actions and an estimated value function. The hyper-parameter settings are similar to previous work, except for the learning rate. The third layer in the A3C architecture consists of a fully connected layer with 256 hidden units. It outputs a softmax distribution for actions and a scalar value function. The learning rate is set to 10^-4 and decreases gradually. The selection of \u03bb1 and \u03bb2 follows a method from previous work, with experiments repeated 25 times with different random seeds. In our framework, \u03bb1 and \u03bb2 are selected by randomly sampling from specific ranges and gradually increasing \u03bb1 during training. Experiments are repeated 25 times with different random seeds. Evaluation metrics follow a specific procedure, and results are compared with state-of-the-art transfer reinforcement learning frameworks. Results from experiments evaluating our transfer reinforcement learning framework show that compared to PathNet and PNN, our framework achieves higher scores in multiple experiments with fewer parameters. Knowledge transfer from teachers to the student model is effective, as demonstrated by the results. In a two-teacher framework, our student model with 0.7M parameters outperforms PNN with 16M parameters in five out of seven experiments. Increasing the number of teachers from one to two significantly improves the student's performance across all experiments. Training curves in FIG1 show consistent high performance. Different environment/teacher settings not used by PathNet and PNN are evaluated in TAB2, demonstrating the effectiveness of knowledge transfer. Our approach generally performs well, and we experiment with different environment/teacher settings to evaluate knowledge flow. Results in TAB2 show that knowledge flow with an expert teacher outperforms the baseline in all experiments, indicating successful knowledge transfer. Our A3C implementation outperforms the scores reported by BID17. Knowledge flow with expert teachers shows better performance than the baseline, indicating successful knowledge transfer. Additionally, knowledge flow with non-expert teachers also outperforms fine-tuning due to the ability to learn from multiple teachers and avoid negative impacts from insufficiently pretrained teachers. In knowledge flow, students can benefit from learning from multiple teachers, avoiding negative impacts from insufficiently pretrained teachers. Training curves are shown in FIG5, with more in the Appendix (Fig. 6). The student model benefits from intermediate representations of teachers, even with different input/output spaces and objectives. For example, in FIG5, learning from Chopper Command and Space Invaders improves scores significantly compared to fine-tuning from a single teacher. The student model benefits from learning from multiple teachers with different objectives, as shown in FIG5. It achieves scores ten times larger than learning without a teacher and fine-tuning from a teacher. Evaluation metrics include top-1 error rate on various image classification benchmarks like CIFAR-10 and CIFAR-100. The student model is evaluated based on the top-1 error rate on the test set of each dataset, with results averaged from three runs using different random seeds. CIFAR-10 and CIFAR-100 datasets consist of colored images of size 32 \u00d7 32, with 10 and 100 classes respectively. Training and test sets contain 50,000 and 10,000 images. Experiments are conducted with standard data augmentation using Densenet (depth 100, growth rate 24) as a baseline. Teachers are trained on CIFAR-10, CIFAR-100, and SVHN before training the student model with different teacher combinations. Results are compared to fine-tuning and the baseline model. The study uses Densenet (depth 100, growth rate 24) as a baseline and trains teachers on CIFAR-10, CIFAR-100, and SVHN datasets. The student model is then trained with different teacher combinations. Results show that fine-tuning from CIFAR-100 improves performance, while fine-tuning from SVHN performs worse. Knowledge flow from both good and inadequate teachers improves results by 13% over the baseline. Similar results are observed on the CIFAR-100 dataset. Knowledge flow improves by 13% over the baseline when presented with both good and inadequate teachers, demonstrating the ability to leverage a good teacher's knowledge while avoiding misleading influence. Results are similar on the CIFAR-100 dataset, and additional details on knowledge flow properties are provided in the appendix. Different techniques for knowledge transfer have been considered, with a focus on leveraging multiple pre-trained teacher nets. In contrast to previous approaches like PathNet and Progressive Net, our method utilizes multiple pre-trained teacher nets to ensure independence of the student during training. Distral combines distillation and transfer learning for joint training of multiple tasks, sharing a distilled policy for common behavior. Distral is a neologism combining 'distill & transfer learning' and considers joint training of multiple tasks with a shared policy to encourage consistency between different tasks. In contrast to multi-task learning frameworks like Distral, knowledge flow focuses on a single task and leverages information from multiple teachers to help a student learn a new task better. In knowledge flow, information from multiple teachers is used to help a student learn a new task better. This approach allows training a deep net from any number of teachers, showing improvements in reinforcement and supervised learning compared to training from scratch or fine-tuning. Future plans include determining when to use which teacher and how to optimize their use. The study developed a knowledge flow approach for training deep nets using multiple teachers, showing improvements in reinforcement and supervised learning. Future plans include optimizing teacher usage and actively swapping teachers during student training. Experiments were conducted on MNIST, MNIST with missing digit '3', CIFAR-100, and ImageNet using teacher and student models with varying parameters. The study conducted experiments on MNIST, MNIST with missing digit '3', CIFAR-100, and ImageNet using different teacher and student models. The student model in their framework outperformed Knowledge Distillation (KD) by benefiting from both the output layer and intermediate layer representations of the teacher. The study used an 18-layer ResNet as the student model and compared its performance to Knowledge Distillation (KD). The 'EMNIST Letters' dataset consists of 26 balanced classes of handwritten letters, while the 'EMNIST Digits' dataset consists of 10 balanced classes of handwritten digits. The training and test sets for each dataset contain a specific number of images. The MNIST model from Chen (2017) was used as a baseline, teacher, and student model in this case. Teachers were trained on different variations of the EMNIST dataset. The 'EMNIST Digits' dataset contains 28x28 pixel images of handwritten digits with 10 balanced classes. The training and test sets have 240,000 and 40,000 images respectively. Teachers were trained on various EMNIST datasets, and the student model's performance was compared to baseline and state-of-the-art results. Results show better performance with expert, semi-expert, and non-expert teachers. The STL-10 dataset consists of colored images sized 96x96. In our experiment, student learning with expert, semi-expert, and non-expert teachers showed better performance compared to baseline and fine-tuning. The STL-10 dataset consists of colored images of size 96x96 pixels with 10 balanced classes. The training set has 5,000 labeled images and 100,000 unlabeled images, while the test set has 8,000 images. Teachers were trained on CIFAR-10 and CIFAR-100, and results were compared to baseline and fine-tuning in Table 6. In our experiment, teachers trained on CIFAR-10 and CIFAR-100 were compared to baseline and fine-tuning using the STL-10 dataset. Results in Table 6 show that pretraining on CIFAR-10 and CIFAR-100 reduces test errors by over 10%. Our approach further decreases test error by 3% compared to fine-tuning, using less labeled data. In our framework, pretraining on CIFAR-10 and CIFAR-100 with weights reduces test errors by more than 10%. Compared to fine-tuning, our approach further decreases test error by 3%. The results are obtained using fewer data and may not be directly comparable to other approaches. In a multi-task reinforcement learning framework, Distral is trained for 120M steps on Atari games with three tasks. Results show that Distral is suboptimal due to aiming to learn a multi-task agent. Our framework can decrease a teacher's influence and reduce negative transfer. Our framework aims to decrease a teacher's influence and reduce negative transfer in multi-task reinforcement learning. Averaged normalized weight (p w) for teachers and the student in the C10 experiment is plotted, showing that the C100 teacher has a higher p w value than the SVHN teacher. The student benefits from the knowledge of teachers, as verified by an ablation study. The plot in FIG8 shows that the C100 teacher has higher normalized weights than the SVHN teacher throughout training. An ablation study confirms that learning with knowledgeable teachers leads to better performance than learning with untrained teachers, as seen in the higher average rewards achieved. Learning with knowledgeable teachers ('Ours with seaquest and riverraid teacher') achieves higher rewards compared to learning with untrained teachers in different environments and teacher-student settings. The KL term helps maintain the student's output distribution when teachers' influence decreases. Ablation study shows that without the KL term, rewards drop drastically, highlighting the importance of the KL term in teacher-student learning dynamics. In an ablation study, setting the KL coefficient to zero results in a drastic drop in rewards when a teacher's influence decreases. However, with the KL term, performance remains stable, achieving higher rewards compared to learning without the KL term. Additionally, using a different architecture for the teacher model, such as BID16 with 3 convolutional layers, further enhances the student's learning process. In additional experiments, the teacher model is based on BID16 with 3 convolutional layers, while the student model is based on BID17 with 2 convolutional layers. Both models have fully connected layers for actions and values, with specific links between their convolutional layers. In experiments, teachers with different architectures achieve similar performance to those with the same architecture. The target task is KungFu Master, with teachers being experts in Seaquest and Riverraid. Results are shown in FIG0. Learning with teachers of different architectures can achieve similar performance as learning with teachers of the same architecture. In an experiment with KungFu Master as the target task and teachers from Seaquest and Riverraid, the average reward was 37520 for different architectures and 35012 for the same architecture. Knowledge flow can lead to higher rewards even when teacher and student architectures differ. Additionally, an average network can be used for the parameters \u03b8 old, with an experiment showing that using an exponential running average of model weights for \u03b8 old can affect performance. Using an average network to compute \u03b8 old with an exponential running average of model weights results in similar performance as using a single model. In a specific example with Boxing as the target task and a Riverraid expert as the teacher, the average reward achieved with the average network is 96.2, slightly higher than the 96.0 achieved with a single network. Additional results on using an average network are shown in other figures. Using an average network to compute \u03b8 old with an exponential running average of model weights achieves similar performance as using a single model. Various techniques for knowledge transfer have been explored, such as fine-tuning, progressive neural nets, PathNet, 'Growing a Brain', actor-mimic, and learning without forgetting. PathNet allows multiple agents to train the same deep net while reusing parameters to avoid catastrophic forgetting. In contrast to PathNet and Progressive Net, our method introduces scaling with normalized weights to ensure independence of the student during training, addressing a limitation in existing approaches. The discussed method leverages transfer and avoids catastrophic forgetting by introducing lateral connections with scaling using normalized weights. Distral combines distill and transfer learning, focusing on joint training of multiple tasks with a shared policy to encourage consistency. Knowledge flow, on the other hand, addresses a single task, emphasizing the transfer of information. In contrast to Distral, which focuses on multi-task learning, knowledge flow addresses a single task by leveraging information from multiple teachers to help a student learn a new task. Knowledge distillation BID9 distills information from a larger deep net into a smaller one, while our technique allows knowledge transfer between different domains. Actor-mimic BID20 enables an agent to learn how to address multiple tasks simultaneously. Our technique enables knowledge transfer between different domains, allowing a student to learn new tasks by leveraging expert teachers' guidance. Unlike Distral, which focuses on multi-task learning, our approach combines feature regression and cross entropy loss to encourage the student to produce similar actions and representations. Additionally, Learning without forgetting BID13 allows adding new tasks to a deep net without compromising its original capabilities. Our proposed technique leverages a teacher's representation at the beginning of training to transfer knowledge more explicitly. By using only data from the new task, old capabilities are retained by recording the old network's output on the new data. This approach differs from previous techniques and allows for the addition of new tasks without forgetting the original capabilities."
}