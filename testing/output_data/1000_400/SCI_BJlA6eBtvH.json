{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a challenge for neural networks in dynamic data environments. A Differentiable Hebbian Consolidation model addresses this by combining rapid learning with fixed parameters, allowing retention of learned representations. Task-specific synaptic consolidation methods can be integrated to maintain important weights for each target task. Our proposed model combines rapid learning with fixed parameters to address catastrophic forgetting in neural networks. It integrates task-specific synaptic consolidation methods and outperforms comparable baselines on various benchmarks, reducing forgetting without requiring additional hyperparameters. This approach aims to mimic the adaptability and learning capabilities of human intelligence in dynamic environments. Our proposed model addresses class imbalance and concept drift without needing extra hyperparameters, outperforming baselines by reducing forgetting. Human intelligence's ability to adapt in dynamic environments is challenging to embed in artificial intelligence. Machine learning models face non-stationarity during deployment, leading to performance degradation when trained with new data after learning is complete, known as catastrophic forgetting. During deployment, models face non-stationarity as data distributions change over time, leading to catastrophic forgetting in deep neural networks. Continual learning aims to adapt to new tasks without losing performance on previous ones, crucial for scalable and efficient models over long timescales. Continual learning involves adapting to consecutive tasks without forgetting previous ones, crucial for scalable models over long timescales. In real-world applications, the stability-plasticity dilemma arises due to concept drift, imbalanced class distributions, and incomplete initial data availability. This poses a challenge for both artificial and biological neural networks. Continual learning presents a challenge for machine learning systems due to imbalanced class distributions, concept drift, and incomplete data availability. The stability-plasticity dilemma requires a balance between integrating new knowledge (plasticity) and preserving existing knowledge (stability). Biological neural networks rely on synaptic plasticity for learning and memory, with theories inspired by mammalian neocortex consolidation. Synaptic plasticity is crucial for learning and memory in biological neural networks. Two theories explain continual learning in humans: synaptic consolidation preserves important synaptic parameters, while the complementary learning system theory stores high-level structural information in different brain areas alongside episodic memories. Recent work on differentiable plasticity has shown that neural networks can be trained end-to-end through backpropagation and stochastic gradient descent to optimize both \"slow weights\" for long-term memory and \"fast weights\" for plasticity in synaptic connections. Recent work on differentiable plasticity has shown that neural networks can be trained to optimize both \"slow weights\" for long-term memory and \"fast weights\" for plasticity in synaptic connections. The fast weights act as short-term memory, allowing reactivation of long-term memory traces. Networks with learned plasticity have been shown to outperform those with uniform plasticity. Approaches have been proposed to address catastrophic forgetting by dynamically adjusting the plasticity of each synapse. Differentiable Hebbian Consolidation 1 model extends work on differentiable plasticity to task-incremental continual learning. It adapts quickly to changing environments and consolidates previous knowledge by adjusting synapse plasticity. The model modifies the traditional softmax layer and augments slow weights in the final fully-connected layer. Our Differentiable Hebbian Consolidation 1 model adapts quickly to changing environments and consolidates previous knowledge by adjusting synapse plasticity. It modifies the softmax layer and augments slow weights in the final fully-connected layer with plastic weights using Differentiable Hebbian Plasticity. The model combines task-specific synaptic consolidation approaches to overcome catastrophic forgetting. Our model combines task-specific synaptic consolidation methods to adapt rapidly to new data while leveraging compressed episodic memories in the softmax layer to mitigate catastrophic forgetting. Tested on various benchmark problems, including Permuted MNIST and Vision Datasets Mixture, our approach outperforms networks with uniform plasticity, demonstrating the effectiveness of task-specific synaptic consolidation in plastic networks. Incorporating task-specific synaptic consolidation methods in plastic networks improves performance on benchmark problems like Permuted MNIST and Vision Datasets Mixture. This approach outperforms networks with uniform plasticity, highlighting the importance of non-uniform plasticity in neural networks. The modification of synaptic strength through weight plasticity, based on Hebb's rule, enhances connections between neurons. Hebbian learning theory suggests that synaptic strength increases after learning, with decreased plasticity to retain knowledge. Recent studies have shown the use of fast weights in neural networks for one-shot and few-shot learning tasks. Fast weights are incorporated into models to improve performance on tasks like Permuted MNIST and Vision Datasets Mixture. Recent studies in meta-learning have demonstrated the use of fast weights in neural networks for one-shot and few-shot learning tasks. Munkhdalai & Trischler (2018) proposed a model with fast weights in FC layers before the softmax, implemented with non-trainable Hebbian learning. Rae et al. (2018) introduced a Hebbian Softmax layer for improved learning of rare classes. Miconi et al. (2018) suggested differentiable plasticity, optimizing synaptic plasticity alongside fixed weights. Miconi et al. (2018) proposed differentiable plasticity using SGD to optimize synaptic plasticity in addition to fixed weights, mainly demonstrated on RNNs for pattern memorization and maze exploration tasks. Our work augments FC layer slow weights with fast weights using DHP, updating only the softmax output layer parameters. Our work addresses the challenge of catastrophic forgetting by incorporating fast weights in the FC layer using DHP. We update only the softmax output layer parameters to achieve fast learning and knowledge preservation over time. Strategies include task-specific synaptic consolidation and a dual memory system to protect and retain previously learned knowledge. - The CLS Theory proposes a dual memory system where the neocortex gradually learns structured representations while the hippocampus rapidly memorizes new instances. Task-specific synaptic consolidation helps overcome catastrophic forgetting by estimating the importance of each synapse. Regularization strategies in continual learning literature focus on retaining memories by adjusting synaptic strengths. Regularization strategies in continual learning literature estimate the importance of each parameter or synapse to prevent forgetting of previously learned tasks. A regularizer is added to the loss function when learning new tasks to adjust plasticity and retain important parameters. The main difference lies in the method used to compute parameter importance. In continual learning, regularization strategies aim to estimate parameter importance to prevent forgetting of previous tasks. Different methods, such as Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), compute the importance of parameters to adjust plasticity and retain key information. EWC uses an offline approach with Fisher information matrix, while SI offers an online method to improve scalability and computational efficiency. Our work draws inspiration from CLS theory, a computational framework for representing memories with a dual memory system. Various online methods like Synaptic Intelligence (SI) and Memory Aware Synapses (MAS) have been proposed to compute parameter importance in continual learning tasks. Our work draws inspiration from CLS theory, a computational framework for representing memories with a dual memory system. Approaches based on CLS principles involve pseudo-rehearsal, exact or episodic replay, and generative replay methods. iCaRL performs rehearsal and regularization using an external memory to store exemplar patterns from old task data. Our work focuses on neuroplasticity techniques inspired by CLS theory to mitigate catastrophic forgetting during continual learning. Previous research has shown how synaptic connections can have fixed weights for long-term knowledge storage and fast-changing weights for temporary associative memory, similar to CLS theory. This approach aims to address catastrophic forgetting by incorporating slow and fast weights. The approach involves slow and fast weights in synaptic connections, inspired by CLS theory to prevent catastrophic forgetting in continual learning. Recent research has explored various methods such as replacing soft attention mechanisms with fast weights in RNNs, Hebbian Softmax layer, augmenting slow weights with fast weights matrix, differentiable plasticity, and neuromodulated differentiable plasticity. These methods were not compared against neuroplasticity-inspired CLS methods designed for meta-learning problems. The methods discussed include plasticity, neuromodulated differentiable plasticity, and the Hebbian Softmax layer. These methods were designed for rapid learning on simple tasks or meta-learning over a distribution of tasks. The Hebbian Softmax layer adjusts its parameters based on an engineered scheduling scheme to achieve fast binding for rarer classes. The Hebbian Softmax layer adjusts parameters for fast binding of rarer classes, switching between Hebbian and SGD updates based on task complexity. The goal is to metalearn a local learning rule for fast weights using fixed weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights and Hebbian weights. The model focuses on metalearning a local learning rule for fast weights in the softmax layer using fixed weights and an SGD optimizer. It involves slow weights \u03b8, Hebbian plastic component with \u03b1 and Hebb, accumulating mean hidden activations for each target label in a mini-batch. The goal is to adjust parameters for fast binding of rarer classes and switch between Hebbian and SGD updates based on task complexity. The Hebbian traces accumulate hidden activations for each target label in a mini-batch. Post-synaptic activations are computed using pre-synaptic activations, leading to unnormalized log probabilities. The softmax function is then applied to obtain predicted probabilities. The \u03b7 parameter dynamically adjusts the learning rate for plastic connections and prevents instability in the Hebbian traces. Network parameters are optimized by gradient descent during sequential training on different tasks. The \u03b7 parameter acts as a decay term for Hebbian traces to prevent instability, and it dynamically adjusts the learning rate for plastic connections. Network parameters are optimized by gradient descent during sequential training on different tasks. In our model, the weight connection has fixed weights, equivalent to setting plasticity coefficients \u03b1 = 0. In our model, Hebbian traces are updated in batches during training to learn tasks continually. The hidden activations are accumulated into the softmax output layer weights for better initial representations and long-term retention. During test time, the most recent Hebb traces are used for predictions. The model utilizes Hebb traces for predictions and optimizes hidden activations directly into softmax output layer weights for improved initial representations and long-term retention. Fast learning is facilitated by a plastic weight component, enhancing test accuracy. The plastic component decays between tasks to prevent interference, while selective consolidation into a stable component protects old memories, enabling the model to learn and remember effectively. This approach simplifies memory management compared to external memory systems. The plastic component decays between tasks to prevent interference, while selective consolidation into a stable component protects old memories, enabling effective learning and memory retention. The DHP Softmax method simplifies memory management and allows for easy scalability with increasing tasks. The plastic component in the final hidden layer quickly stores memory traces for recent experiences without interference. Hidden activations of the same class are averaged into one vector, forming a compressed episodic memory. This method improves learning of rare classes and speeds up binding of class labels to deep representations. The plastic component in the final hidden layer quickly stores memory traces for recent experiences without interference. Hidden activations of the same class are averaged into one vector, forming a compressed episodic memory. This method improves learning of rare classes and speeds up binding of class labels to deep representations. Hebbian Synaptic Consolidation is implemented to update the network parameters in an online manner, enhancing learning without introducing additional hyperparameters. Incorporating various consolidation methods like EWC, Online EWC, SI, and MAS, the network's loss is regularized and synaptic importance parameters are updated online. The quadratic loss for Hebbian Synaptic Consolidation is derived, focusing on the weights between pre- and post-synaptic activities. Task-specific consolidation approaches are adapted without computing synaptic importance on the plastic component, only regularizing the slow weights. During training the first task, the synaptic importance parameter is set to 0 for all methods except SI, which estimates it. Incorporating various consolidation methods like EWC, Online EWC, SI, and MAS, the network's loss is regularized and synaptic importance parameters are updated online. Task-specific consolidation approaches are adapted without computing synaptic importance on the plastic component, only regularizing the slow weights. The plastic component of the softmax layer in the model helps alleviate catastrophic forgetting by optimizing the connections' plasticity. Comparisons are made with vanilla neural networks using Online EWC, SI, and MAS, showing increased DNN capacity with the addition of plastic weights. Our approach utilizes gradient descent to optimize the plasticity of connections in neural networks, comparing it to other methods like Online EWC, SI, and MAS. To increase DNN capacity, we add plastic weights and slow weights to the softmax output layer. Testing on various benchmarks, including Permuted MNIST and Split MNIST, shows improved performance in sequential task learning. Our model was tested on various benchmarks such as Permuted MNIST and Split MNIST to evaluate memory retention and flexibility. The evaluation was based on classification accuracy on all tasks learned so far, with a focus on the first and most recent tasks. Forgetting was measured using the backward transfer metric, which indicates the impact of learning new tasks on previous ones. The backward transfer metric, BWT, measures the influence of learning new tasks on previous tasks. BWT < 0 indicates catastrophic forgetting, while BWT > 0 shows improvement in previous task performance. Neural networks were trained with Online EWC, SI, and MAS consolidation methods on all tasks sequentially. Hyperparameters were consistent with and without DHP Softmax, and MNIST pixels were permuted differently for each task. Details can be found in Appendix A. In the benchmark, the hyperparameters for consolidation methods (EWC, SI, MAS) remain unchanged with or without DHP Softmax. The plastic components are not regularized, and MNIST pixels are permuted differently for each task. The input distribution changes between tasks, indicating concept drift. A multi-layered perceptron (MLP) network with specific parameters is used for the benchmarks. The performance comparison is conducted. The study used a multi-layered perceptron network with specific parameters for benchmarks. The network with DHP Softmax showed improvement in alleviating catastrophic forgetting compared to the baseline network. Performance was compared with and without DHP Softmax using task-specific consolidation methods. The study compared the performance of a network with DHP Softmax in alleviating catastrophic forgetting to a baseline network. Results showed that DHP Softmax with consolidation maintained higher test accuracy during sequential training of tasks. An ablation study further examined the network's structural parameters and Hebb traces for interpretability. The behavior of the proposed model during training on 10 tasks in the Permuted MNIST benchmark is analyzed. The synaptic connections become more plastic initially to acquire new information quickly, then decay to prevent interference between learned representations. The Hebb trace grows without runaway positive feedback, maintaining a memory of recent activity, while plasticity coefficients grow within each task. The Frobenius Norm of the Hebb trace suggests that Hebb grows without runaway positive feedback when learning new tasks, maintaining a memory of recent activity. The plasticity coefficients grow within each task, indicating the network leverages the structure in the plastic component. Gradient descent and backpropagation are used for meta-learning to tune the structural parameters. The Imbalanced Permuted MNIST problem introduces an imbalanced distribution for each task, motivated by the challenges of class imbalance and concept drift in predictive performance. The Imbalanced Permuted MNIST problem involves tasks with imbalanced class distributions, aiming to address challenges of class imbalance and concept drift in predictive performance. DHP Softmax achieves 80.85% accuracy after learning 10 tasks sequentially, showing a 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered effectively. In a sequential manner, DHP Softmax with MAS achieves a 0.04 decrease in BWT, resulting in an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered effectively, showing significant improvement over standard neural network baseline. The MNIST dataset is split into 5 binary classification tasks with disjoint output spaces. The MNIST dataset is split into 5 binary classification tasks with disjoint output spaces. An MLP network with two hidden layers is used, achieving 98.23% accuracy with DHP Softmax. Combining DHP Softmax with task-specific consolidation decreases BWT and improves test accuracy across tasks, especially the most recent one, T5. Performance after learning T 5 tasks shows that DHP Softmax achieves 98.23% accuracy, a 7.80% improvement compared to a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreases BWT and improves average test accuracy across all tasks, especially T 5. Continual learning is performed on a sequence of 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 using a CNN architecture. MNIST, notMNIST, and FashionMNIST datasets are zero-padded to match the resolution of SVHN and CIFAR-10 images. The MNIST, notMNIST, and FashionMNIST datasets are zero-padded to match the resolution of SVHN and CIFAR-10 images. A CNN architecture similar to previous studies is used, with an initial \u03b7 parameter value of 0.0001. Training is done with mini-batches of size 32 and plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. Results show that DHP Softmax combined with MAS improves average test accuracy by 2.14% over MAS alone. SI with DHP Softmax outperforms other methods with an average test performance of 81.75% after learning all five tasks. Adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. DHP Softmax combined with MAS improves average test accuracy by 2.14% over MAS alone, while SI with DHP Softmax achieves an average test performance of 81.75% after learning all five tasks. The addition of compressed episodic memory in the softmax layer through DHP, along with task-specific updates on synaptic parameters, helps alleviate catastrophic forgetting in continual learning environments. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax automatically adjusts the plastic connections, allowing for better adaptation to new information while protecting old knowledge. This approach shows significant improvement across all benchmarks compared to a standard neural network. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax adjusts plastic connections to balance between protecting old knowledge and acquiring new information quickly. DHP Softmax outperforms traditional softmax with slow changing weights across benchmarks without introducing extra hyperparameters. The model's flexibility allows for Hebbian Synaptic Consolidation in addition to DHP Softmax. The model's flexibility allows for Hebbian Synaptic Consolidation in addition to DHP Softmax. Combining DHP Softmax with SI improves performance on Split MNIST and 5-Vision Datasets Mixture. Combining DHP Softmax and MAS leads to superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The combination of DHP Softmax and MAS consistently outperforms other methods on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The local variant of MAS computes synaptic importance parameters based on Hebb's rule, prioritizing highly correlated connections. The model shows lower negative BWT and higher test accuracy with DHP, indicating Hebbian plasticity aids in continual learning and memory retention, reducing catastrophic forgetting in sequential datasets. Our model with Hebbian plasticity consistently outperforms other methods on various benchmarks, showing lower negative BWT and higher test accuracy. This indicates that Hebbian plasticity enables neural networks to learn continually, remember distant memories, and reduce catastrophic forgetting in dynamic environments. Continual synaptic plasticity plays a key role in learning from limited labeled data and adapting at long timescales. Our work aims to explore gradient descent optimized Hebbian consolidation for learning and memory in DNNs to enable continual learning on a sequence of tasks. The curr_chunk discusses new investigations into gradient descent optimized Hebbian consolidation for learning and memory in DNNs to enable continual learning through training on sequential tasks with associated data. Each task has its own task-specific loss to prevent catastrophic forgetting. The curr_chunk discusses training on sequential tasks with associated data, each with its own task-specific loss to prevent forgetting. The model learns an approximated mapping to the true function and maps new inputs to target outputs for all tasks learned. Different classes are used in each task, as shown in benchmarks like SplitMNIST and Vision Datasets Mixture. Experiments were conducted on Nvidia Titan V or RTX 2080 Ti, training on tasks with mini-batches of size 64 using plain SGD with a learning rate of 0.01 for at least 10 epochs. Early stopping is performed once the training is complete. The experiments involved training on sequential tasks with different classes using Nvidia Titan V or RTX 2080 Ti. The network was trained on tasks with mini-batches of size 64 using plain SGD with a learning rate of 0.01 for at least 10 epochs. Early stopping was implemented based on validation error improvement. Hyperparameters were set for task-specific consolidation methods in Permuted MNIST experiments. For the Permuted MNIST experiments, hyperparameters were set for task-specific consolidation methods. The regularization hyperparameter \u03bb was adjusted for Online EWC, SI, and MAS methods. Grid search was conducted to find the best hyperparameter combination for each method. In a grid search for synaptic consolidation methods, hyperparameters were optimized for Online EWC, SI, and MAS. Imbalanced Permuted MNIST tasks involved removing training samples based on random probabilities from each class in the original MNIST dataset. The distribution of classes for each task is shown in Table 2. The distribution of classes in imbalanced datasets for tasks T n=1:10 is shown in Table 2, where samples are removed based on random probabilities from each class. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. In SI, the damping parameter \u03be was set to 0.1. Hyperparameter combinations were found through grid search using a task sequence determined by a single. For the Split MNIST experiments, the regularization hyperparameters were \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. Grid search was used to find the best hyperparameter combinations for each method. In the Split MNIST experiments, hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. Grid search was conducted to optimize these parameters for each method. The Vision Datasets Mixture benchmark consists of a sequence of 5 tasks with different image classification datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The notMNIST dataset includes font glyphs for letters 'A' to 'J', with 60,000 training and 10,000 testing grayscale images. FashionMNIST has 10 clothing categories with 60,000 training and 10,000 testing grayscale images. The dataset includes font glyphs for letters 'A' to 'J', with 60,000 training and 10,000 testing grayscale images. FashionMNIST has 10 clothing categories with 60,000 training and 10,000 testing grayscale images. SVHN consists of digits '0' to '9' from Google Street View images with 73,257 training and 26,032 testing color images. CIFAR-10 has 50,000 training and 10,000 testing color images from 10 categories. The CNN architecture includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling operations. The CNN architecture for the dataset consists of 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling operations. A multi-headed approach was used due to different class definitions between datasets, with a trainable \u03b7 value for each connection in the final output layer. In the benchmark problem, a trainable \u03b7 value is assigned to each connection in the final output layer, improving stability and convergence. Separate \u03b7 parameters for each connection allow for modulation of plasticity rates. Using a single \u03b7 value across all connections led to optimization instability. Hyperparameters for the 5-Vision Datasets Mixture experiments include regularization values for different task-specific consolidation methods. In the 5-Vision Datasets Mixture experiments, different regularization hyperparameters were used for task-specific consolidation methods: \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. A random search was conducted to find the best hyperparameter combinations for each method. Sensitivity analysis was performed on the Hebb decay term \u03b7 to observe its impact on average test performance in continual learning. In the sensitivity analysis, different values of \u03bb were tested for Online EWC, SI, and MAS methods. The impact of the Hebb decay term \u03b7 on test performance in continual learning was analyzed for various tasks, showing that lower values of \u03b7 were more effective in reducing catastrophic forgetting. The sensitivity analysis on the impact of the Hebb decay term \u03b7 on test performance in continual learning showed that lower values of \u03b7 were more effective in reducing catastrophic forgetting. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to the final model, initializing weights and learning rates for plastic connections. The DHP Softmax model in PyTorch adds compressed episodic memory to the final output layer of a neural network through plastic connections. It outperforms Finetune in test accuracies on CIFAR-10 and CIFAR-100 datasets. The network was trained on CIFAR-10 and 5 additional tasks from CIFAR-100 dataset. DHP Softmax outperforms Finetune in test accuracies in class-incremental learning setup. Test accuracies of Finetune, training from scratch, and SI were taken from a previous study."
}