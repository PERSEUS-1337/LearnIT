{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. NPs efficiently fit observed data with linear complexity and can learn a wide family of conditional distributions. However, they suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data they condition on. To address this issue, attention is incorporated into NPs, allowing each input location to attend to relevant context points for prediction, resulting in improved accuracy, faster training, and an expanded range of functions that can be learned. Incorporating attention into Neural Processes (NPs) improves prediction accuracy, speeds up training, and expands the range of functions that can be modeled. Regression tasks involve modeling the distribution of output given input, either through a deterministic function or by computing a distribution over functions using training data. This alternative approach allows for making predictions on test inputs using draws from the distribution. In regression tasks, models like Neural Processes (NPs) can compute a distribution over functions to make predictions on test inputs. Gaussian Processes (GPs) are also popular non-parametric models for this approach in Bayesian machine learning. NPs offer an efficient method for modeling a distribution over regression functions with linear prediction complexity. Neural Processes (NPs) provide an efficient way to model a distribution over regression functions, with prediction complexity linear in the context set size. They can predict the distribution of an arbitrary target output based on a set of context input-output pairs of any size, allowing them to model data generated from a stochastic process. However, NPs and Gaussian Processes (GPs) have different training regimes, with NPs trained on samples from multiple realizations of a stochastic process and GPs usually trained on observations from one realization. Comparing the two directly is not usually feasible. Neural Processes (NPs) are trained on multiple realizations of a stochastic process, while Gaussian Processes (GPs) are trained on observations from a single realization. NPs tend to underfit the context set, leading to inaccurate predictions. The encoder aggregates the context set to a fixed-length latent summary, and the decoder maps the latent and target input. The encoder in Neural Processes aggregates context information to a fixed-length latent summary, leading to underfitting issues. This bottleneck is caused by the mean-aggregation step, which gives equal weight to all context points, making it hard for the decoder to learn relevant information. Increasing dimensionality does not fully solve this problem. Inspiration is drawn from Gaussian Processes to address this issue. The decoder in Neural Processes struggles to learn relevant information due to equal weighting of context points. Drawing inspiration from Gaussian Processes, a mechanism using differentiable attention is implemented in NPs to attend to relevant contexts for a given target, preserving permutation invariance. The Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by using differentiable attention to attend to relevant contexts for a given target, preserving permutation invariance. ANPs show enhanced expressiveness, better reconstruction of contexts, and faster training compared to NPs. They are evaluated on 1D function regression and 2D image regression, demonstrating their ability to model a wider range of functions. The Neural Process (NP) is a regression model that maps input x to output y. It defines conditional distributions based on observed contexts and targets, with permutation invariance. The model uses a deterministic function to aggregate context pairs into a finite representation. The Neural Process (NP) model defines conditional distributions for arbitrary contexts and targets, with permutation invariance. It uses a deterministic function to aggregate context pairs into a finite representation. The likelihood is modelled by a Gaussian factorised across the targets, with mean and variance determined by passing inputs through a MLP. The model also includes a global latent variable to account for uncertainty in predictions. The Neural Process (NP) model incorporates a global latent variable to address uncertainty in predictions. This latent variable is modelled by a factorised Gaussian parametrised by specific properties, complementing the deterministic path of the model. The global latent allows for modeling different realisations of the data generating stochastic process. The Neural Process (NP) model includes a global latent variable to model different realisations of the data generating stochastic process. The encoder and decoder parameters are learned by maximizing the ELBO for a subset of contexts and targets using the reparametrisation trick. The Neural Process (NP) model learns encoder and decoder parameters by maximizing ELBO for a subset of contexts and targets using the reparametrisation trick. The NP reconstructs targets with a KL term to ensure the summary of contexts is close to the summary of targets. The model is scalable and can learn a wide range of conditional distributions. Neural Processes (NPs) learn a wide range of conditional distributions by randomly selecting contexts and targets at each training iteration. NPs offer scalability, flexibility, and permutation invariance but lack consistency in contexts. Neural Processes (NPs) learn conditional distributions by randomly selecting contexts and targets, offering scalability and flexibility. However, they lack consistency in contexts. Maximum-likelihood learning minimizes the KL between data-generating process and NP distributions. Attention mechanism computes weights for key-value pairs based on a query, allowing the query to attend to the pairs. Neural Processes (NPs) use attention mechanism to compute weights for key-value pairs based on a query, allowing the query to attend to the pairs. The permutation invariance property of attention is key in its application to NPs. The idea of using a differentiable addressing mechanism has been successfully applied in various areas of Deep Learning. The attention mechanism has been successfully applied in Deep Learning, particularly in handwriting generation, neural machine translation, natural language processing, and image modeling. Different forms of attention, such as locality-based and dot-product attention, are used to compute weights for key-value pairs based on queries. The attention mechanism in Deep Learning has been applied in various fields like handwriting generation, neural machine translation, natural language processing, and image modeling. Different forms of attention, such as locality-based and dot-product attention, compute weights for key-value pairs based on queries. Stationary kernels like the Laplace kernel and dot-product attention are used to weight keys according to their distance from the query. Multihead attention extends this concept by linearly transforming keys, values, and queries for each head, allowing the query to attend to different keys for each head. The multihead architecture in (Vaswani et al., 2017) extends the concept of attention by linearly transforming keys, values, and queries for each head. This allows the query to attend to different keys for each head, resulting in smoother query-values compared to dot-product attention. Self-attention is applied to context points to compute representations of each (x, y) pair, which are then used in cross-attention to predict the target output. The self-attention mechanism models interactions between context points before the mean-aggregation step. The self-attention mechanism models interactions between context points to compute representations of each (x, y) pair. This helps obtain richer representations that encode relations between the context points. Higher order interactions are modeled by stacking self-attention. In the deterministic path, mean-aggregation is replaced by cross-attention, where each target query attends to the context. In the model, higher order interactions are captured by stacking self-attention. The deterministic path uses cross-attention for query-specific representations, allowing each query to focus on relevant context points. The latent path preserves global structure by inducing dependencies between target predictions through correlations in the marginal distribution. The latent path in the model preserves global structure by inducing dependencies between target predictions through correlations in the marginal distribution, while the deterministic path models fine-grained local structure. The decoder remains the same, with a query-specific representation replacing the shared context representation. ANP is trained using the same loss as the original NP, with added expressivity. The attention mechanism in the ANP preserves permutation invariance in contexts, leading to increased accuracy at the cost of higher computational complexity. Despite the increased complexity, training time for ANPs remains comparable to NPs, with ANPs learning significantly faster. The (A)NP learns a stochastic process and should be trained on multiple functions. Training time for ANPs remains comparable to NPs, with ANPs learning significantly faster. Training involves drawing a batch of realisations from the data generating process and optimizing the loss. The (A)NP is trained on data from a stochastic process, with random points selected as targets and contexts to optimize loss. The decoder architecture remains consistent, with 8 heads for multihead. The model is tested on 1D function regression using synthetic GP data, exploring fixed and randomly varying hyperparameters. The number of contexts and targets are randomly chosen at each iteration. The study explores two settings for training the (A)NP model on 1D data with synthetic GP data. The number of contexts and targets are randomly chosen at each iteration, and the model uses cross-attention in the deterministic path. The ANP shows faster error reduction and lower values at convergence compared to NP. The study compares the performance of ANP and NP models on 1D data with synthetic GP data. ANP shows faster error reduction and lower values at convergence compared to NP, especially with dot product and multihead attention mechanisms. The computation times of Laplace and dot-product ANP are similar to NP, while multihead ANP takes around twice the time. The size of the bottleneck in the deterministic and latent paths of NP affects underfitting behavior. The study compares the performance of ANP and NP models on 1D data with synthetic GP data. ANP shows faster error reduction and lower values at convergence compared to NP, especially with dot product and multihead attention mechanisms. Raising the bottleneck size in NPs helps achieve better reconstructions, but there is a limit to improvement. Using ANPs has significant benefits over simply increasing the bottleneck size in NPs. Visualizations in FIG2 show the learned conditional distribution for a qualitative comparison of attention mechanisms. The study compares the performance of ANP and NP models on 1D data with synthetic GP data. ANP shows faster error reduction and lower values at convergence compared to NP, especially with dot product and multihead attention mechanisms. Using ANPs has significant benefits over simply increasing the bottleneck size in NPs. Visualizations in FIG2 show the learned conditional distribution for a qualitative comparison of attention mechanisms, highlighting the differences in predictive means between Laplace and dot-product attention. The study compares the performance of ANP and NP models on 1D data with synthetic GP data. ANP shows faster error reduction and lower values at convergence compared to NP, especially with dot product and multihead attention mechanisms. Dot-product attention outperforms Laplace attention in predictive accuracy, but displays non-smooth predictions with higher uncertainty. Multihead attention helps smooth out interpolations and improves context reconstruction and target prediction. The study compares the performance of ANP and NP models on 1D data with synthetic GP data. ANP shows faster error reduction and lower values at convergence compared to NP, especially with dot product and multihead attention mechanisms. The multiple heads in multihead attention help smooth out interpolations and improve context reconstruction and target prediction, while preserving increased predictive uncertainty away from the contexts. The ANP is more expressive than the NP and can learn a wider range of functions. Using the trained (A)NPs, a toy Bayesian Optimization problem is tackled to find the minimum of test functions drawn from a GP prior. The ANP outperforms the NP in performance, showing faster error reduction and lower values at convergence. It can learn a wider range of functions and is able to sample entire functions, demonstrating accurate context reconstructions. Image data can be seen as generated from a stochastic process, with pixel intensity prediction as a regression problem. Each image represents one realization of the process. The ANP outperforms the NP in performance, showing faster error reduction and lower values at convergence. It can learn a wider range of functions and is able to sample entire functions, demonstrating accurate context reconstructions. Image data can be interpreted as generated from a stochastic process, with pixel intensity prediction as a regression problem. Each image represents one realization of the process. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. The use of self-attentional layers in the encoder is explored, stacking them as in BID21. Results of three different models are shown on both datasets: NP, Multihead ANP, and Stacked Multihead ANP. Predictions of the full image with varying numbers of random context pixels are generated. The NP gives reasonable predictions with diversity for fewer contexts, but inaccurate reconstructions of the whole image compared to Stacked Multihead ANP. The use of attention in the Stacked Multihead ANP model results in accurate reconstructions of images, with crisper inpaintings and diverse predictions for different samples of z. This diversity in faces and digits demonstrates z's ability to model the global structure of the image. The NP model, on the other hand, provides reasonable predictions but lacks accuracy in reconstructing the entire image. The Stacked Multihead ANP model shows improved context reconstruction error and NLL for target points compared to the NP model. There are noticeable gains in crispness and global coherence with multihead crossattention and stacked self-attention. The model generalizes well even with limited training data. The Multihead ANP model demonstrates improved context reconstruction error and NLL for target points compared to the NP model. Each head of the attention focuses on different regions, with varying roles such as focusing on the target pixel, nearby pixels, or larger regions. The Multihead ANP model shows improved context reconstruction error and NLL for target points compared to the NP model. Each head of the attention has different roles, with varying focus on the target pixel, nearby pixels, or larger regions. The model can map images from one resolution to another by predicting pixel intensities in a continuous space. The Multihead ANP model can map images from one resolution to another by predicting pixel intensities in a continuous space. Using a finer grid as the target, the model can accurately map a given resolution to a higher resolution, unlike NPs which may produce inaccurate reconstructions. The Multihead ANP model can accurately map images from one resolution to another, unlike NPs which may have inaccurate reconstructions. The ANP can map low resolutions to realistic 32x32 outputs and even higher resolutions like 256x256. The model shows diversity in mappings for different resolution contexts. The Multihead ANP model can accurately map images from low to high resolutions, showing diversity in mappings for different resolution contexts. It learns to create realistic high-resolution images with sharper edges and internal representations of features like filling in missing details in images. The ANP model can accurately map images from low to high resolutions, creating realistic high-resolution images with sharper edges and filling in missing details. It is not a replacement for state-of-the-art algorithms but highlights its flexibility in modeling conditional distributions. The ANP model demonstrates the ability to map images from low to high resolutions, creating realistic high-resolution images with sharper edges and filling in missing details. It is not intended to replace state-of-the-art algorithms but showcases flexibility in modeling conditional distributions. The use of attention in NPs is related to Deep Kernel Learning and Gaussian Processes, which provide a measure of similarity between points in the same domain. The use of attention in Neural Processes (NPs) is related to Deep Kernel Learning and Gaussian Processes (GPs), which measure similarity between points in the same domain. Learning in NPs involves maximizing marginal likelihood, while GPs require updating kernel hyperparameters. Direct comparison between the two methods is challenging due to different training regimes. Gaussian Processes (GPs) have predictive uncertainties dependent on kernel choice, while Neural Processes (NPs) learn uncertainties from data directly. GPs offer consistent stochastic processes with exact closed-form expressions for predictions, a feature lacking in current NPs formulations. Variational Implicit Processes (VIP) are related to NPs, using a similar decoder setup with a finite dimensional z. Meta-Learning (A)NPs focus on few-shot learning, where input-output pairs from a new function at test time are used to reason about the function's predictive distribution. Various works in few-shot classification utilize attention mechanisms for locating relevant information. Few-shot classification works utilize attention mechanisms to locate relevant information in input-output pairs from a new function at test time. Attention has also been used in tasks such as Meta-RL for continuous control and visual navigation. Few-shot density estimation using attention has been extensively explored in various works, with some models incorporating local latents on top of a global latent. The focus for ANPs is on the less-explored regression setting. The curr_chunk discusses various models such as Neural Statistician and Variational Homoencoder with permutation invariant encoders. It also mentions regression settings for ANPs and multitask learning in the GP literature. Generative Query Networks are highlighted for spatial prediction tasks. Multitask learning in the GP literature has been addressed by various works. Generative Query Networks are models for spatial prediction tasks, where x represents viewpoints and y represents frames of a scene. ANPs have been proposed to augment NPs with attention, improving prediction accuracy, training speed, and the range of functions that can be modeled. Future work for ANPs includes exploring different model architectures. ANPs augment NPs with attention to improve prediction accuracy, training speed, and model functionality. Future work includes exploring different model architectures, such as incorporating cross-attention and global latent paths. ANPs could be trained on text data to fill in blanks stochastically, and have connections with the Image Transformer (ImT) BID21 for image applications. ANPs could be trained on text data to fill in blanks stochastically and have connections with the Image Transformer (ImT) BID21 for image applications. By replacing the MLP in the decoder of the ANP with self-attention across target pixels, a model resembling an ImT defined on arbitrary orderings of pixels can be created. This approach extends the expressiveness of ANPs, but the ordering and grouping of targets will become important due to the interdependence of predictions. In contrast to the original ImT, ANPs are designed with self-attention in the decoder to extend their expressiveness. The targets' ordering and grouping become crucial as they affect each other's predictions. The architectural details of NP and Multihead ANP models for regression experiments are shown in Figure 8. The models use relu non-linearities in MLPs except for the final layer, which has no non-linearity. The latent path outputs parameterize q(z|s C) and the decoder outputs parameterize p(y i |z, x C, y C, x i). The 1D and 2D regression experiments involve MLPs with relu non-linearities, latent path outputs \u00b5 z , \u03c9 z \u2208 R d, and decoder outputs \u00b5 y , \u03c9 y. The experiments use multihead cross-attention and self-attention modules without dropout to limit stochasticity to the latent z. The self-attention module outputs representations based on input representations. The Image Transformer BID21 utilizes multihead crossattention without dropout to limit stochasticity to the latent z. Self-attention module outputs representations based on input representations, allowing for stacking of 2 layers in Stacked Multihead ANP. Different kernel hyperparameters are used for fixed and random cases, with a batch size of 16. For the experiments, different kernel hyperparameters were used for fixed and random cases, with a batch size of 16. The Adam Optimiser BID14 with a fixed learning rate of 5e-5 was used, along with one sample of q(z|s C) to estimate the loss. The predictions of the Multihead ANP model were compared to an oracle GP in Figure 9, showing that the Multihead ANP model was closer to the oracle GP. The Adam Optimiser BID14 with a fixed learning rate of 5e-5 was used for training the models. The Multihead ANP model showed closer predictions to the oracle GP compared to the NP model, but still underestimated the predictive variance. Variational inference used in learning the ANP may lead to underestimates of predictive variance, requiring further investigation. The conditional distributions for fixed kernel hyperparameters exhibited non-smooth behavior, similar to the random kernel hyperparameter case. The conditional distributions for fixed kernel hyperparameters exhibit non-smooth behavior, similar to the random kernel hyperparameter case. This behavior arises when dot-product attention collapses to a local minimum, leading to good reconstructions but poor interpolations between context points. KL term in NP loss differs between training on fixed and random kernel hyperparameter GP data. The KL term in NP loss varies between training on fixed and random kernel hyperparameter GP data. In the fixed hyperparameter case, the model quickly deems the deterministic path sufficient for accurate predictions. However, in the random hyperparameter case, there is added variation in the data, leading to non-zero KL and the use of latents to model uncertainty in the stochastic process given context points. In the random hyperparameter case, there is added variation in the data, leading to non-zero KL and the use of latents to model uncertainty in the stochastic process given context points. (A)NPs can be used for Bayesian optimization by considering all previous function evaluations as context points, obtaining an informed surrogate of the target function. Thompson sampling is used to draw a function from the surrogate and act accordingly. The use of NPs with multihead attention in Bayesian optimization shows consistently smaller simple regret compared to other NPs, approaching the performance of an oracle GP. The cumulative regret decreases most rapidly for multihead NPs, indicating effective utilization of previous function evaluations for predicting the function minimum. The cumulative regret decreases rapidly for NPs with multihead attention in Bayesian optimization, showing effective utilization of previous function evaluations for predicting the function minimum. Random pixels of images are used as targets and contexts during training, with rescaled x and y values. MNIST and CelebA datasets are used with a batch size of 16 and specific learning rates. The x and y values are rescaled for training with random pixels as targets and contexts. A batch size of 16 is used for MNIST and CelebA datasets, with specific learning rates. The stacked self-attention architecture is similar to the Image Transformer BID21, without Dropout or positional embeddings. Little tuning has been done regarding architectural hyperparameters. One sample of q(z|s C) is used for a MC estimate of the loss. The NP model does not use Dropout or positional embeddings. Little tuning has been done regarding architectural hyperparameters. One sample of q(z|s C) is used for a MC estimate of the loss. The NP with attention reduces uncertainty significantly as the number of contexts increases. Stacked Multihead ANP improves results over Multihead ANP, providing sharper images with better global coherence. The NP model with attention reduces uncertainty as context increases. Stacked Multihead ANP improves results significantly, giving sharper images with better global coherence. Different heads play roles even when the target is disjoint from the context. All heads become useful for target prediction in such cases. Visualizations show pixels attended by each head in multihead attention. In the NP model with attention, different heads play roles even when the target is disjoint from the context. Visualizations show pixels attended by each head in multihead attention."
}