{
    "title": "SJg9z6VFDr",
    "content": "Recently, various neural networks have been proposed for irregularly structured data such as graphs and manifolds. All existing graph networks have discrete depth, but a new model called graph ordinary differential equation (GODE) has been introduced. GODE extends the idea of continuous-depth models to graph data by parameterizing the derivative of hidden node states with a graph neural network. Two efficient training methods for GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. Direct backprop has been shown to outperform the adjoint method in experiments. Additionally, a family of bijective blocks is introduced to enable efficient memory consumption. The GODE model introduces direct back-propagation through the ODE solver, outperforming the adjoint method. Bijective blocks enable efficient memory consumption. GODE can be easily adapted to different graph neural networks, improving accuracy in various tasks. Graph neural networks offer a continuous model with memory efficiency and accurate gradient estimation for various tasks. While CNNs excel in tasks like image classification and segmentation, they are limited to grid-based data like images and text. Graph structures are essential for modeling irregular data like social networks and protein interactions. Traditional methods like random walk have been used in early works. The graph data structure represents objects as nodes and relations as edges, widely used for modeling irregularly structured data like social networks, protein interaction networks, and citation graphs. Traditional methods like random walk and graph embedding have been used, but graph neural networks (GNN) have shown superior performance by generalizing convolution operations to capture local information on graphs. Spectral and non-spectral methods are used for performing convolution on graphs. Graph neural networks (GNN) generalize convolution operations to capture local information on graphs. Two types of methods for convolution on graphs are spectral and non-spectral. Spectral methods compute the graph Laplacian for filtering, while non-spectral methods directly perform convolution in the graph domain. GraphSAGE learns a convolution kernel in an inductive manner. All existing GNN models have discrete layers. The GraphSAGE model learns a convolution kernel in an inductive manner. Existing GNN models have discrete layers, making it difficult to model continuous diffusion processes in graphs. The neural ordinary differential equation (NODE) extends to graphs as graph ordinary differential equations (GODE), where message propagation is modeled as an ODE. In this work, the authors propose graph ordinary differential equations (GODE) to model message propagation on graphs as an ODE. They address the issue of gradient estimation errors during training of NODEs and introduce a memory-efficient framework for accurate gradient estimation. The authors propose a memory-efficient framework for accurate gradient estimation in free-form NODEs, improving performance on image classification tasks. The framework also achieves constant memory usage when applied to restricted-form invertible blocks. The authors introduce a framework for free-form NODEs to estimate gradients accurately, improving performance on image classification tasks. The framework is memory-efficient and achieves constant memory usage with restricted-form invertible blocks. Additionally, they generalize ODE to graph data with GODE models, demonstrating improved performance on various datasets. Innovations in neural ordinary differential equations (NODE) have been proposed by various researchers, including Lu (2017) and Haber & Ruthotto (2017). NODE treats neural networks as continuous ODEs and has been used in generative models. Studies have focused on training methods and improving expressive capacity. However, the issue of inaccurate gradient estimation remains unaddressed, leading to inferior performance in benchmark tasks compared to state-of-the-art models. Spectral and non-spectral methods in graph neural networks (GNNs) are discussed. Spectral GNNs operate in the Fourier domain of a graph, requiring information of the entire graph for graph Laplacian determination. Non-spectral GNNs focus on message aggregation around neighbor nodes, leading to localized computations. Bruna et al. (2013) introduced graph convolution in the Fourier domain based on the graph Laplacian. Spectral methods in graph neural networks involve determining the graph Laplacian using the entire graph, while non-spectral methods focus on localized message aggregation around neighbor nodes. Various approaches have been proposed to address the computation burden and improve efficiency, such as incorporating graph estimation procedures and using Chebyshev expansion for filter approximation. Defferrard et al. (2016) and Kipf & Welling (2016) introduced efficient methods for graph convolution using Chebyshev expansion and localized first-order approximation, respectively. Other approaches like MoNet, GraphSAGE, graph attention networks, and GIN have also been proposed for fast localized inference and learning different weights for node neighbors. Invertible blocks are neural network blocks that have a bijective mapping, allowing accurate reconstruction of input from outputs. They are used in normalizing flow models to calculate the log-density of data distribution. Jacobsen et al. (2018) utilized bijective blocks to construct invertible structures. Invertible blocks are utilized in normalizing flow models to calculate data distribution log-density. Bijective blocks are used by Jacobsen et al. (2018) to create invertible structures. Invertible blocks are used in normalizing flow models to calculate data distribution log-density. Discrete-layer models with residual connections can be represented as neural ordinary differential equations (NODE). The difference equation transforms into a continuous case with shared weights and a derivative parameterized by a network. The forward pass of the model with discrete layers involves differentiable functions and hidden states representation. The forward pass of a NODE involves integrating states with a shared function across all layers. Various ODE solvers can be used for integration, and the adjoint method is commonly applied in this context. The transformation of states in a NODE is modeled as the solution to the NODE, with an output layer applied. Integration in the forward pass can be done with various ODE solvers. The adjoint method, used in optimal process control, is applied with model parameters denoted as \u03b8. The adjoint is defined and compared to back-propagation methods on NODE. Comparison of two methods for back-propagation on NODE. The ODE solver is discretized at points during the forward pass, with hidden states solved in forward-time and reverse-time. Direct back-propagation saves evaluation time points during the forward pass and accurately reconstructs the hidden state for gradient evaluation. In direct back-propagation, evaluation time points are saved during the forward pass and the computation graph is rebuilt during the backward pass. This allows for accurate reconstruction of the hidden state and evaluation of the gradient. Gradient descent is then performed to optimize \u03b8 and minimize the loss function. Reverse-time integration is used, which can be solved with any ODE solver. The forward pass solves Eq. 2 forward in time, while the backward pass solves Eq. 2 and 6 reverse in time. Initial conditions are determined from Eq. 5 at time T. In direct back-propagation, evaluation time points are saved during the forward pass. Reverse-time integration is used to solve Eq. 2 and 6 in the backward pass. The reverse-time ODE solver may cause inaccuracies in gradient calculations due to mismatch between hidden states solved forward-time and reverse-time. This error impacts the gradient dL d\u03b8. The reverse-time ODE solver may introduce inaccuracies in gradient calculations due to a mismatch between hidden states solved forward-time and reverse-time. This error affects the gradient dL d\u03b8, indicating instability in either the forward-time or reverse-time ODE. Proposition 1 states that if the Jacobian of the original system has eigenvalues with non-zero real parts, either the forward-time or reverse-time ODE is unstable. High |Re(\u03bb)| values make the ODE sensitive to numerical errors. This instability impacts the accuracy of solutions and computed gradients. To address this, direct back-propagation through the ODE solver is proposed for accurate hidden states. To address the instability of ODE solutions due to high eigenvalues, direct back-propagation through the ODE solver is proposed for accurate hidden states. This involves back-propagating through the ODE solver using discretization for numerical integration, ensuring accurate hidden states at evaluated time points. In forward-time, z(t i ) = h(t i ) guarantees accurate direct back-propagation regardless of Eq. 2 stability. Adjoint for discrete forward-time ODE solution is defined, with Eq. 7 as a numerical discretization of Eq. 6. Eq. 6 is derived from an optimization perspective. Algorithm 1 outlines accurate gradient estimation in ODE solver for free-form functions. During forward-time, the solver integrates numerically with adaptive step sizes based on error estimation. The integrated value and evaluation time points are outputted during the forward pass. The algorithm ensures accurate gradient estimation for ODE solvers with free-form functions. During the forward pass, the solver performs numerical integration with adaptive step sizes and outputs the integrated value and evaluation time points. The backward pass rebuilds the computation graph without adaptive searching, performing reverse-time integration. The algorithm supports free-form continuous dynamics and has no constraints on the form of the function f. Memory consumption analysis considers the number of forward evaluation steps and adaptively searches for an optimal solution. During backward pass, the solver performs reverse-time integration numerically. Our algorithm is a generic method with no constraints on the form of the function f. Memory consumption analysis shows our method consumes less memory compared to a naive solver. If a step-wise checkpoint method is used, memory consumption can be further reduced. The solver can handle free-form functions and invertible blocks. The memory-efficient solver with invertible blocks reduces memory consumption to O(Nf) by splitting input x into two parts and using bijective blocks for forward and inverse operations. The memory-efficient solver with invertible blocks reduces memory consumption by splitting input x into two parts and using bijective blocks for forward and inverse operations. Bijective blocks are defined by differentiable neural networks and bijective functions, allowing for accurate reconstruction of x from y without the need to store activations. Theorem 1 states that if a function \u03c8(\u03b1, \u03b2) is bijective with respect to \u03b1 when \u03b2 is given, then the block defined by Eq. 8 is also bijective. This allows for the application of different \u03c8 functions for various tasks, leading to memory-efficient operations without the need to store activations. The text also introduces graph neural networks with discrete and continuous layers, extending to graph ordinary differential equations (GODE). The representation of a graph with nodes and edges is illustrated in Fig. 2, with current GNNs typically following a message passing scheme. Graph neural networks (GNNs) are represented with nodes and edges, where each node is assigned a unique color. GNNs operate in a message passing scheme, with nodes exchanging information through message passing and aggregation stages. The message passing stage involves neighbor nodes sending information to a specific node, while the aggregation stage combines all received messages at the node. This process is facilitated by differentiable functions parameterized by neural networks. A Graph Neural Network (GNN) operates in a 3-stage model for a specific node u: message passing, where neighbor nodes send information to u through a function \u03c6; message aggregation, where u combines messages from neighbors using a permutation invariant function \u03b6; and update, where u's state is updated based on its original state and aggregated messages. This discrete-time GNN can be converted to a continuous-time GNN using a graph ordinary differential equation (GODE), which captures highly non-linear dynamics. The discrete-time GNN can be converted to a continuous-time GNN using a graph ordinary differential equation (GODE), which captures highly non-linear functions and potential stability issues related to over-smoothing phenomena. Graph convolution is a special case of Laplacian smoothing, demonstrating the relationship between input and output in a graph-conv layer. Graph convolution is a special case of Laplacian smoothing, where the relationship between input and output in a graph-conv layer is demonstrated. The continuous smoothing process involves a positive scaling constant and eigenvalues of the symmetrically normalized Laplacian. The ODE is asymptotically stable, suggesting that all nodes will become very similar with a large enough integration time T in experiments. In experiments, the ODE is asymptotically stable with non-zero eigenvalues of the normalized Laplacian, leading to nodes becoming similar with large integration time T. The method was evaluated on image and graph classification tasks using CNN-NODE and various benchmark datasets. The method was evaluated on various benchmark datasets for graph classification tasks without pre-processing. For image classification tasks, a ResNet18 was modified into its corresponding NODE model with a sequence of conv-bn-relu layers. In image classification tasks, a ResNet18 model was modified into a NODE model with conv-bn-relu layers. GODE can be applied to various graph neural networks by replacing functions and structures. GODE can be easily generalized to existing structures like GCN, GAT, ChebNet, and GIN for graph neural networks. Different depths of layers were trained and the best results were reported for each model structure. Hyper-parameters such as channel number were kept consistent across models for fair comparison. The study compared different models for graph classification tasks, setting consistent hyper-parameters like channel numbers and number of hops. Various GNN structures were tested with different hidden layer depths, showing that direct back-propagation yielded higher accuracy. Additionally, a ResNet18 was modified into NODE18 for CNN-NODE classification tasks. The study experimented with different hidden layer depths and compared adjoint method and direct back-propagation on the same network. Direct back-propagation showed higher accuracy for both CNN-NODE and graph networks. The training method reduced error rates for NODE18 on CIFAR10 and CIFAR100 compared to the adjoint method. Our training method outperforms the adjoint method on image classification tasks, reducing error rates for NODE18 on CIFAR10 and CIFAR100. Additionally, NODE18 with the same parameters as ResNet18 surpasses deeper networks like ResNet101 on both datasets. The method also shows superior performance on benchmark graph datasets. Different ODE solvers of varying orders were implemented for robustness, with HeunEuler, RK23, and RK45 evaluated 1, 2, and 4 times respectively per step forward in time. Our method supports adaptive ODE solvers of different orders, such as HeunEuler, RK23, and RK45. Using different solvers during inference is equivalent to changing model depth without re-training the network. Our method is robust to different orders of ODE solvers and supports NODE and GODE models with free-form functions. Bijective blocks defined as Eq. 8 can be easily generalized, with F and G as general neural networks and \u03c8(\u03b1, \u03b2) as any differentiable bijective mapping. The study demonstrates the effectiveness of bijective blocks in generalized neural networks for different tasks. Results show that GODE models outperform discrete-layer models significantly, indicating the importance of continuous-time models. Additionally, lower memory costs are validated, with details provided in the appendix. Results show that GODE models outperform discrete-layer models significantly on node classification tasks, emphasizing the importance of continuous-time models. Different \u03c8 functions behaved similarly, indicating the model's significance over the coupling function. Lower memory costs were also validated. Graph classification tasks with various structures were experimented on, with GODE models performing better in most cases. Integration time influence was tested for NODE and GODE models during inference. The study compared GODE models with discrete-layer counterparts using paired t-test, showing GODE models performed better. Integration time was tested during inference, with short times leading to lack of information and long times causing over-smoothing issues. GODE enables continuous diffusion process modeling on graphs and a memory-efficient back-propagation method for NODEs. The paper also relates GNN over-smoothing to ODE asymptotic stability. Our paper introduces GODE for modeling continuous diffusion on graphs and proposes a memory-efficient back-propagation method for NODEs. We validate its superior performance on image classification and graph data tasks, and relate GNN over-smoothing to ODE asymptotic stability. Experiments are conducted on various datasets including citation networks, social networks, and bioinformatics datasets. Our paper introduces GODE for modeling continuous diffusion on graphs and proposes a memory-efficient back-propagation method for NODEs. Experiments are conducted on various datasets including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained with important modifications, including a parameter state checkpoint method for accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch. The proposed parameter state checkpoint method allows bijective blocks to be called multiple times for accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch, emphasizing memory efficiency by deleting unnecessary variables and computation graphs. Training a GODE model with bijective blocks demonstrates reduced memory consumption. The bijective block in the GODE model is memory efficient, as demonstrated by comparing memory consumption with a conventional method. Results show a significant reduction in memory usage when using the proposed method, especially as the depth of ODE blocks increases. The memory-efficient bijective blocks in the GODE model significantly reduce memory consumption compared to conventional methods, especially as the depth of ODE blocks increases. The bijective block only slightly increases memory usage due to caching states of F and G, but this step requires minimal memory compared to input data. The memory-efficient bijective blocks in the GODE model reduce memory consumption by caching states of F and G, with minimal memory usage compared to input data. The stability of ODEs in forward-time and reverse-time requires eigenvalues of J to have non-positive real parts. The eigenvalue of J f must have a non-positive real part for stability in both forward-time and reverse-time ODEs. The bijective block mapping is proven to be injective and surjective, ensuring a bijective mapping. The forward mapping is proven to be bijective by demonstrating injective and surjective properties. This is achieved by showing that the mapping is both injective and surjective, ensuring a bijective mapping. The text discusses the bijective nature of the forward mapping in a neural-ODE model, demonstrating injective and surjective properties. It includes a computation graph with forward pass denoted by black arrows and gradient back-propagation by red arrows. The gradient of parameters is derived from an optimization perspective, extending from continuous to discrete cases. Notations and problem setup are also explained. The text discusses the optimization perspective of parameters in a neural-ODE model, extending from continuous to discrete cases. Notations and problem setup are explained, with the forward pass and loss function defined. The training process is formulated as an optimization problem. The loss function is defined as J(\u0177, y) = J(z(T ), y). The training process is formulated as an optimization problem using the Lagrangian Multiplier Method. Karush-Kuhn-Tucker (KKT) conditions are necessary for an optimal solution. The derivative w.r.t. \u03bb is derived at the optimal point. The Karush-Kuhn-Tucker (KKT) conditions are necessary for an optimal solution. Starting from the KKT condition, we derive results by considering derivatives with respect to \u03bb. The conditions for Leibniz integral rule are checked, leading to the equation dz(t)/dt - f(z(t), t, \u03b8) = 0. In discrete cases, ODE conditions are replaced with finite sums, resulting in a corresponding discrete version of the analysis. In discrete cases, the ODE condition becomes a finite sum. The discrete version of the analysis corresponds to Eq. 10 and 11."
}