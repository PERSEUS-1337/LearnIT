{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function shows strong convexity and smoothness near the ground truth, allowing gradient descent to converge linearly to a critical point close to the ground truth. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks. The first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks has been established. This method converges linearly to a critical point close to the ground truth without needing new samples at each iteration. The theoretical underpinnings of deep neural networks' success remain largely mysterious, despite their practical success in domains like computer vision and artificial intelligence. Efforts have been made to understand the theoretical underpinnings behind the success of deep neural networks in practical domains like computer vision and artificial intelligence. One important research area focuses on model recovery, aiming to recover the underlying model parameter W from training samples generated from a neural network model with the ground truth parameter W. This is crucial for the network to generalize well. Efforts have been made to understand the theoretical underpinnings behind the success of deep neural networks in practical domains like computer vision and artificial intelligence. Previous studies have focused on recovering the underlying model parameter W from training samples generated from a neural network model with the ground truth parameter W. This is crucial for the network to generalize well. Studies have looked at regression and classification problems in different settings. Previous studies have focused on recovering neural network parameters using gradient descent for regression and classification problems. Statistical guarantees for model recovery using squared loss have been provided in various settings. In both regression and classification settings, previous studies focused on recovering neural network parameters using gradient descent with squared loss. Guarantees for model recovery were provided, including positive definite Hessian in the local neighborhood of the ground truth and uniform geometry for linear convergence without needing to resample per iteration. The study aims to develop a strong statistical guarantee for the loss function in eq. (2) for one-hidden-layer neural networks using the cross entropy loss function, which is more practical than the squared loss for classification problems. This is the first performance guarantee for such recovery, with contributions including establishing uniform geometry for linear convergence without per-iteration resampling. This study provides the first performance guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function. For multi-neuron classification with sigmoid activations, the empirical risk function based on the cross entropy loss is uniformly strongly convex in a local neighborhood of the ground truth. Gradient descent converges linearly to a critical point when initialized in this neighborhood. The cross entropy loss in eq. (2) is uniformly strongly convex in a local neighborhood of the ground truth W. Gradient descent converges linearly to a critical point W n with a sample complexity of O(dK 5 log 2 d). The recovery of W is up to certain statistical accuracy, converging to W at a rate of O(dK 9/2 log n/n) in the Frobenius norm. The convergence guarantee does not require a fresh set of samples at each iteration. To obtain -accuracy, it requires a computational complexity of O(ndK 2 log(1/ )). The tensor method proposed in BID38 provably provides these results. The tensor method proposed in BID38 provides an initialization near the ground truth W, with convergence to W at a rate of O(dK 9/2 log n/n) in the Frobenius norm. The convergence guarantee does not require new samples at each iteration, and the computational complexity for achieving accuracy is O(ndK 2 log(1/ )). The proof introduces new techniques to analyze the cross-entropy loss function, leveraging statistical information on geometric curvatures. Our proof introduces new techniques to analyze the challenging cross-entropy loss function for shallow neural networks, leveraging statistical information on geometric curvatures. The technique also provides performance guarantees for classification using the squared loss. The focus is on theoretical and algorithmic aspects of learning nonconvex optimization in signal processing problems. The parameter recovery viewpoint is crucial for non-convex learning in signal processing problems. It removes worst-case instances, allowing focus on average-case performance with benign geometric properties. Studies on one-hidden-layer network models are categorized into landscape analysis and model recovery. If the network size is large enough compared to the data input, there are no spurious local minima. In the landscape analysis of one-hidden-layer network models, it is known that with a large network size compared to the data input, there are no spurious local minima in the optimization landscape. However, for cases with multiple neurons, there can be spurious bad local minima even at the population level. Studies have provided characterizations for the local Hessian in regression problems with various activation functions. In the landscape of the population squared loss surface with ReLU activations, spurious bad local minima exist even at the population level. Studies have characterized the local Hessian for regression problems with various activation functions. In the model recovery problem, gradient descent converges linearly with a single neuron and ReLU activation under Gaussian input, as long as the sample complexity is O(d) for the regression problem. When the activation function has bounded derivatives, there are no critical points other than the unique global minimum, and gradient descent converges linearly. The ReLU activation function with zero initialization has sample complexity O(d) for regression. BID21 showed bounded derivatives lead to unique global minimums for classification with linear convergence. The study analyzes cross entropy loss and multi-neuron model recovery, a novel approach compared to previous work on neural networks. Our study analyzes the cross entropy loss function in a unique form and focuses on the model recovery classification problem under the multi-neuron case. Previous research has looked at one-hidden-layer or two-layer neural networks with different structures, but our approach is different. The paper is organized into sections describing the problem formulation, main results on local geometry and convergence of gradient descent, and the initialization method. The paper introduces the Porcupine Neural Network and discusses various sections including problem formulation, local geometry, convergence of gradient descent, initialization method, numerical examples, and conclusions. Notable notations and definitions are also provided throughout the paper. The paper introduces the Porcupine Neural Network and discusses problem formulation, local geometry, convergence of gradient descent, initialization method, and numerical examples. Notable notations and definitions are provided, including spectral norms, gradient, Hessian, singular values, and constants. The generative model for training data and the gradient descent algorithm for learning network weights are described. The paper introduces the Porcupine Neural Network, discussing problem formulation, local geometry, convergence of gradient descent, initialization method, and numerical examples. The generative model for training data and the gradient descent algorithm for learning network weights are described, focusing on the classification setting with a one-hidden layer neural network model. The goal is to estimate W by minimizing the empirical risk function, which is the cross entropy loss. The paper introduces the Porcupine Neural Network, focusing on the classification setting with a one-hidden layer neural network model. The goal is to estimate W by minimizing the empirical risk function using the gradient descent algorithm with a well-designed initialization scheme. The algorithm is summarized in Algorithm 1, utilizing a step size \u03b7 for updates. The gradient descent algorithm is implemented with a well-designed initialization scheme, detailed in Section 4. The update rule is given as DISPLAYFORM0 with \u03b7 as the step size. The algorithm, summarized in Algorithm 1, uses the same set of training samples throughout execution, unlike other methods like BID38 which resamples at every iteration. An important quantity regarding \u03c6(z) is introduced to capture geometric properties of the loss function. Before stating the main results, an important quantity regarding \u03c6(z) capturing geometric properties of the loss function is introduced. The local strong convexity of f n (W) in a neighborhood of the ground truth W is characterized, with details on the Euclidean ball B(W, r) and the condition number \u03ba = \u03c31/\u03c3K. The local strong convexity of the empirical risk function f n (W) in a neighborhood of the ground truth W is guaranteed to have a positive definite Hessian with high probability for a classification model with sigmoid activation function. Theorem 1 ensures this property for all column permutations of W, with a condition on W F and a constant C, under certain conditions. Theorem 1 guarantees that the Hessian of the empirical cross-entropy loss function is positive definite in a neighborhood of the ground truth W for a classification model with sigmoid activation, under certain conditions. The sample complexity for the classification problem with quantized labels is order-wise near-optimal in the dimension parameter up to polynomial factors of K and log d. The strong convexity of the empirical risk function ensures the existence of at most one critical point in the local neighborhood of W, which is the unique local minimizer if it exists. The strong convexity of the empirical risk function ensures the existence of at most one critical point in the local neighborhood of W. Theorem 2 states that for the classification model with sigmoid activation function, there exists a unique critical point Wn close to the ground truth W, and gradient descent converges linearly to Wn under certain conditions. Theorem 2 guarantees the existence of a unique critical point Wn close to the ground truth W, with gradient descent converging linearly to Wn at a rate of O(K 9/4 d log n/n). The computational complexity for achieving -accuracy is O(ndK 2 log (1/ )), with initialization using the tensor method proposed. The tensor method proposed in BID38 is used for initialization, ensuring the recovery of W consistently as n approaches infinity. Gradient descent converges linearly to Wn at a linear rate when initialized in the basin of attraction. Achieving -accuracy requires a computational complexity of O(ndK 2 log (1/ )), with the tensor method defining product \u2297 and introducing vectors P2 and P3. The tensor method proposed in BID38 defines product \u2297 and vectors P2 and P3. Algorithm 2 outlines the initialization process, involving estimating column directions of W and reducing a third-order tensor for decomposition. The tensor method proposed in BID38 involves estimating column directions of W and reducing a third-order tensor for decomposition. The algorithm then decomposes P2 to approximate the subspace spanned by {w1, w2, ..., wK} and applies non-orthogonal tensor decomposition on R3 to output the estimate siVwi, where si \u2208 {1, -1}. The classification problem makes technical assumptions about the activation function and the curvature around the ground. The activation function \u03c6(z) must satisfy certain conditions, including non-zero values for at least one of M3 and M4. Homogeneous assumption is not required, instead, a condition on the curvature around the ground truth is assumed. The performance guarantee for the initialization algorithm is presented under these assumptions. The performance guarantee for the initialization algorithm is presented under certain assumptions. There exists a positive constant \u03b4 such that the function is strictly monotone over an interval, and the derivative is lower bounded by a constant. The output of the algorithm satisfies a certain condition with high probability. The proof involves accurate estimation of the direction and norm of the output. More details can be found in the supplementary materials. The proof of the initialization algorithm's performance guarantee involves accurate estimation of the direction and norm of the output. Gradient descent is implemented to verify the strong convexity of the empirical risk function around W. Multiple random initializations converge to the same critical point Wn with the same training samples. Variance of the output is calculated to quantify the standard deviation of the estimator Wn. The variance of the output is calculated to quantify the standard deviation of the estimator Wn under different initializations with the same set of training samples. The successful rate of gradient descent is shown to converge to the same local minima with high probability as long as the sample complexity is large enough. The successful rate of gradient descent converges to the same local minima with high probability as long as the sample complexity is large enough. Statistical accuracy of the local minimizer is shown for gradient descent when initialized close to the ground truth. Average estimation error decreases with increasing sample size, matching theoretical predictions. Performance of gradient descent algorithm is further compared. The average estimation error decreases gracefully with increasing sample size, matching theoretical predictions. Gradient descent algorithm performance is compared, showing that cross entropy loss achieves lower error rates than squared loss in a classification problem. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. In this paper, the model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem is studied. The sample complexity for guaranteeing local strong convexity near the ground truth is characterized, ensuring linear convergence of gradient descent when initialized properly. Future work includes extending the analysis to different activation functions and network structures. The paper discusses model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. It characterizes the sample complexity for ensuring local strong convexity near the ground truth and linear convergence of gradient descent if initialized properly. Future work involves extending the analysis to different activation functions and network structures. The proof of Theorem 1 involves showing smoothness and convexity properties of the Hessian of the population loss function. The paper discusses model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. It characterizes the sample complexity for ensuring local strong convexity near the ground truth and linear convergence of gradient descent if initialized properly. Future work involves extending the analysis to different activation functions and network structures. The proof of Theorem 1 involves showing smoothness and convexity properties of the Hessian of the population loss function. Additionally, the Hessian of the empirical loss function is shown to be close to its popular counterpart in a neighborhood of W, establishing local strong convexity and smoothness. Lemma 1 states that for sigmoid activations, if W F \u2264 1, then a certain condition holds for a constant C when W \u2212 W F \u2264 0.7. Lemma 2 establishes local strong convexity and smoothness of the population loss function for sigmoid activations within a neighborhood around the ground truth. Lemma 3 shows that under certain conditions, the Hessian of the empirical loss function is close to the Hessian of the population loss function. The Hessian of the empirical loss function is shown to be close to the Hessian of the population loss function for sigmoid activations. With certain conditions met, Theorem 1 establishes strong convexity in a specific region, ensuring only one critical point exists. The proof of Theorem 2 demonstrates the concentration of the gradient around the true gradient within this region. The sample size n is set to satisfy certain conditions in a specific region, ensuring the existence of at most one critical point. The proof of Theorem 2 shows the concentration of the gradient around the true gradient in this region, leading to the convergence of gradient descent to a critical point W n. Lemma 4 establishes that the gradient \u2207f n (W ) concentrates around \u2207f (W ) for sigmoid activation function. With Lemma 3 and Lemma 4, (Mei et al., 2016, Theorem 2) guarantees the existence of a unique critical point W n in a specific region. W n is shown to be close to W by the intermediate value theorem and Cauchy-Schwarz inequality. The text discusses the existence of a critical point W n in a specific region, showing its proximity to W through mathematical inequalities and theorems. It also establishes the local linear convergence of gradient descent with specific conditions on the learning rate. The proof demonstrates the linear convergence of gradient descent to the local minimizer W n, with two parts: (a) accurate estimation of the direction of W, and (b) proof based on a mild condition in Assumption 2. Part (b) of the proof is based on a mild condition in Assumption 2. A tensor operation is defined for matrices A, B, and C. The main idea of the proof is to bound the estimation error of P 2 and R 3 using Bernstein inequality. The proof for size n \u2265 dpoly (K, \u03ba, t, log d) holds with high probability for DISPLAYFORM1. For the classification problem, Bernstein inequality is used to bound the estimation error of P 2 and R 3. The label y i in the classification model is naturally bounded, while the output y i in the regression model needs to be upper bounded. Refer to BID38 for detailed proof. Different proof provided for estimating w i for i = 1, . . . , K. The label y i in the classification model is naturally bounded, while the output y i in the regression model needs to be upper bounded. A different proof is provided for estimating w i for i = 1, . . . , K, which does not require homogeneous conditions on the activation function. The text discusses estimating w i in a regression model without requiring homogeneous conditions on the activation function. It involves solving an optimization problem and substituting estimated values to obtain an estimate of w i. The sign of \u03b2 i can correctly estimate s i, and there exists a constant \u03b4 > 0 for the inverse function. The text discusses estimating w i in a regression model without requiring homogeneous conditions on the activation function. It involves solving an optimization problem and substituting estimated values to obtain an estimate of w i. The sign of \u03b2 i can correctly estimate s i, and there exists a constant \u03b4 > 0 for the inverse function. An estimate a i of w i can be obtained via an equation, and useful definitions and results are introduced for the proofs. The text introduces definitions of sub-gaussian and sub-exponential norms for random variables, along with calculations of the gradient and Hessian of a regression model. It discusses estimating w i without homogeneous conditions on the activation function and the use of estimated values to obtain an estimate of w i. X, denoted as X \u03c81, is a sub-gaussian random variable with calculations of the gradient and Hessian provided. The goal is to upper bound E T 2 j,l,k. The text discusses calculating \u2206 j,l and obtaining an upper bound for E T 2 j,l,k using various inequalities and a sigmoid activation function. The goal is to find a large enough constant C for the given formula. The text discusses obtaining upper and lower bounds for the Hessian of the population risk at ground truth, using various inequalities and a sigmoid activation function. The goal is to find a large enough constant C for the given formula. The text discusses obtaining upper and lower bounds for the Hessian of the population risk at ground truth using various inequalities and a sigmoid activation function. Lemma 1 is applied to obtain a uniform bound in the neighborhood of W, with a focus on finding a large enough constant C for the given formula. Lemma 1 provides a uniform bound in the neighborhood of W, focusing on finding a large enough constant C for the given formula. The analysis adapts the approach in BID21, using the -covering number N of the Euclidean ball B(W, r) and a -cover set W = {W1, ..., WN}. The proof involves bounding the terms P(A_t), P(B_t), and P(C_t) separately. Lemma 7 provides an upper bound for G_i \u03c81, with a constant C, in the context of bounding the terms P(B_t) in the proof."
}