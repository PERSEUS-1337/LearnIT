{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. Extensive empirical investigations on the NSynth dataset show that GANs outperform WaveNet baselines in generating audio faster and more efficiently. Neural audio synthesis is a challenging task that requires modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine-scale details but have slow sampling speeds. Efforts to speed up generation introduce overhead, such as training secondary networks or customizing kernels. Generative Adversarial Networks (GANs) have been successful in generating high resolution images efficiently. They use transposed convolutions conditioned on a latent vector for global latent control. GANs have the potential to transform audio as well, unlocking domain transformations similar to those seen in images. Recent success in generating high resolution images with GANs has led to the exploration of adapting these architectures for audio waveform generation. However, current attempts have not achieved the same level of perceptual fidelity as image counterparts. Frame-based techniques, such as transposed convolutions or STFTs, are used for audio waveform estimation, focusing on locally-coherent waves with a local periodicity. Frame-based techniques like transposed convolutions or STFTs are used for audio waveform estimation, focusing on locally-coherent waves with a local periodicity. The alignment between the audio periodicity and output stride precesses in time, making it challenging for transposed convolutional filters to cover all necessary frequencies and phase alignments. STFTs can unwrap phase over the 2\u03c0 boundary to calculate instantaneous radial frequency, showing the constant relationship between audio frequency and frame frequency. STFT unwraps phase over 2\u03c0 boundary to calculate radial frequency, maintaining phase coherence. GAN research progresses from focused datasets like CelebA to broader domains like NSynth dataset. The CelebA dataset is limited to centered and cropped faces, providing a common reference for improving texture and features. NSynth dataset focuses on individual notes from musical instruments, aligned and cropped to emphasize fine-scale details like timbre and fidelity. Both datasets enable conditional generation and have influenced research in GAN models. The NSynth dataset focuses on individual musical notes, emphasizing timbre and fidelity. Various approaches have been explored for generation, including autoregressive WaveNet autoencoders, VAEs with perceptual priors, and adversarial regularization. Adversarial training and noncausal convolutional generation are introduced to improve representations for audio waveforms like speech and music. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms. Unlike images, audio waveforms are highly periodic and require maintaining regularity over short to intermediate timescales. Convolutional filters trained on audio data commonly form frequency selective filter banks spanning the range of human hearing. Maintaining the regularity of periodic signals over short to intermediate timescales is crucial due to discontinuities and irregularities in waveforms. Phase precession occurs when the stride of frames does not align with a waveform's periodicity, presenting a challenge for synthesis networks to learn frequency and phase combinations. This phenomenon is similar to the short-time Fourier transform and occurs when filterbanks overlap. Phase precession is observed in situations where filterbanks overlap, similar to the short-time Fourier transform. A new approach to generating coherent waveforms is presented, inspired by the phase vocoder BID7. Unwrapping the phase causes it to grow linearly, with the derivative remaining constant and equal to the angular difference between frame stride and signal periodicity, known as the instantaneous angular frequency. The unwrapped phase grows linearly with a constant derivative equal to the angular difference between frame stride and signal periodicity, known as the instantaneous angular frequency. This paper investigates architecture and representation in synthesizing coherent audio with GANs, finding that generating log-magnitude spectrograms and phases directly can produce more coherent waveforms. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Key findings include generating log-magnitude spectrograms and phases directly with GANs for more coherent waveforms, estimating IF spectra for even more coherence, and the importance of preventing harmonics from overlapping by adjusting STFT frame size and switching to mel frequency scale. GANs can outperform WaveNet in the NSynth dataset evaluations. In the study, GANs on the NSynth dataset outperform WaveNet in generating coherent audio. Size and switching to mel frequency scale improve performance by creating more separation between harmonic frequencies. Global conditioning allows GANs to generate smooth timbre interpolation and consistent timbral identity across pitch. The NSynth dataset contains 300,000 musical notes from 1,000 instruments with labels for pitch, velocity, instrument, and acoustic qualities. Each sample is four seconds long and sampled at 16kHz. The NSynth dataset contains 300,000 musical notes from 1,000 instruments with labels for pitch, velocity, instrument, and acoustic qualities. Each sample is four seconds long, sampled at 16kHz, and has 64,000 dimensions. The study focused on acoustic instruments and fundamental pitches ranging from MIDI 24-84, resulting in 70,379 examples from strings, brass, woodwinds, and mallets. A new 80/20 test/train split was created from shuffled data for better performance in audio quality evaluations. We adapted progressive training methods to generate audio spectra from a random vector using transposed convolutions. The model samples a random vector z and generates output data x = G(z) fed into a discriminator network for estimation. The model samples a random vector from a spherical Gaussian and uses transposed convolutions to upsample and generate output data. A discriminator network estimates the divergence measure between real and generated distributions. Gradient penalty and pixel normalization are used for Lipschitz continuity. Progressive training shows slightly better convergence time and sample diversity. The method involves conditioning on additional information. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. An auxiliary classification BID24 loss is added to the discriminator to predict the pitch label. Spectral representations are computed using STFT magnitudes and phase angles with specific parameters. To utilize pitch information, an auxiliary classification BID24 loss is added to the discriminator. STFT magnitudes and phase angles are computed with specific parameters, resulting in spectral representations. The Nyquist frequency is trimmed, and padding is done to create an \"image\" of size (256, 512, 2) with magnitude and phase in two channels. Magnitudes are scaled to be between -1 and 1, and the phase angle is also scaled accordingly. Variants include \"phase\" models and \"instantaneous frequency\" (\"IF\") models obtained by unwrapping the phase angle and taking the finite difference. Performance is noted to be sensitive to these variations. The phase angle and magnitudes of spectral images are scaled between -1 and 1 for the generator network. Variants include \"phase\" models and \"instantaneous frequency\" (\"IF\") models, with high frequency resolution and \"IF-Mel\" variants for lower frequencies. Converting back to linear STFTs involves an approximate inverse linear transformation. To enhance frequency separation, variants with high frequency resolution and \"IF-Mel\" variants for lower frequencies are utilized. WaveGAN is adapted for waveform generation with GANs, achieving similar performance to WaveNet without progressive training. WaveNet is currently the state of the art in generative modeling of audio. Strong WaveNet baselines are created by adapting the architecture to accept pitch conditioning signals. The study focuses on creating strong WaveNet baselines by adapting the architecture to accept pitch conditioning signals. They train variants using different output distributions and evaluate the models using diverse metrics, including human evaluation for audio quality. The study evaluates models using diverse metrics, including human evaluation for audio quality. Participants compare audio samples on a Likert scale to determine better quality. The study evaluated models using various metrics, including human evaluation for audio quality. Participants compared audio samples on a Likert scale to determine better quality. Amazon Mechanical Turk was used for a comparison test on examples from all models in TAB0. 3600 ratings were collected, with each model involved in 800 comparisons. Key metrics included Number of Statistically-Different Bins (NDB) and Inception Score. The training examples are clustered into k = 50 Voronoi cells by k-means in log-spectrogram space, and the generated examples are also mapped into the same space. NDB is reported as the number of cells where the number of training examples is statistically significantly different from the number of generated examples. Inception Score (IS) is a metric for evaluating GANs by measuring the mean KL divergence between imageconditional output class probabilities and the marginal distribution. The Inception Score (IS) measures the KL divergence between image-conditional output class probabilities and the marginal distribution. It penalizes models with examples that are not easily classified into a single class or belong to only a few classes. The metric is now using features from a pitch classifier trained on spectrograms. Additionally, Pitch Accuracy (PA) and Pitch Entropy (PE) are measured separately to account for models producing distinct pitches. Fr\u00e9chet Inception Distance (FID) evaluates GANs based on the distance between multivariate Gaussians. The text discusses the evaluation of GANs using metrics like Fr\u00e9chet Inception Distance (FID) and Inception Score, with a focus on pitch classifier features. Results show a decrease in audio quality as output representations move from IF-Mel to Waveform, with IF-Mel being the highest quality model. Human evaluation confirms this trend. The study evaluates GANs using pitch classifier features instead of Inception features. Results show a decrease in audio quality as output representations move from IF-Mel to Waveform. Human evaluation confirms this trend, with IF-Mel being judged slightly inferior to real data. NDB score follows the same trend as human evaluation, with high frequency resolution improving scores. WaveNet baseline occasionally breaks down, receiving the worst NDB score. The NDB score follows the trend of human evaluation, with high frequency resolution improving scores. WaveNet baseline receives the worst NDB score due to lack of diversity in sample distributions. FID scores also show lower scores for IF models with high frequency resolution. Phase models have high FID scores even at high frequency resolution. The FID scores for different models show that Mel scaling has less impact on FID compared to listener study. Phase models have high FID scores, indicating poor sample quality. Classifier metrics like IS, Pitch Accuracy, and Pitch Entropy perform well for most models due to explicit conditioning. High-resolution models generate examples classified similarly to real data, but sample quality may be affected by distribution discrepancies. Discriminative information about sample quality is limited due to mode collapse and other issues. The high-resolution models generate examples classified similarly to real data, but discriminative information about sample quality is limited due to distribution discrepancies and mode collapse. The metrics provide a rough measure of which models are less reliably generating classifiable pitches, with low frequency models and baselines showing lower reliability. Visualizing qualitative audio concepts is recommended, along with listening to accompanying audio examples for a better understanding. The WaveGAN and PhaseGAN models show phase irregularities in waveform visualization, while the IFGAN model is more coherent with small variations. Rainbowgrams display consistent colors for each harmonic in real data and IF models, but PhaseGAN has speckles due to phase discontinuities. The IFGAN model is more coherent with small variations compared to the PhaseGAN and WaveGAN models. Rainbowgrams show consistent colors for each harmonic in real data and IF models, indicating clear phase coherence. PhaseGAN exhibits phase discontinuities, while WaveGAN is irregular in waveform visualization. The PhaseGAN has phase discontinuities, while the WaveGAN is irregular. Rainbowgrams depict frequency magnitude as brightness and IF as color, showing clear phase coherence in real data and IFGAN. GANs allow conditioning on the same latent vector for the entire sequence, unlike WaveNet which is limited to short subsequences. WaveNet autoencoders learn local latent codes on a millisecond scale but have a structure that must be modeled. WaveNet autoencoders, like those in BID9, learn local latent codes for generation on a millisecond scale but have limitations. Pretrained WaveNet autoencoders are compared with IF-Mel GAN in Figure 4, showing differences in interpolating waveforms and latent codes. While WaveNet improves mixing in timbre space, it struggles with complex priors on latents, leading to oscillations and failures. In contrast, IF-Mel GAN has a spherical gaussian prior. WaveNet autoencoders struggle with complex priors on latents, resulting in oscillations and failures during waveform interpolation. In contrast, IF-Mel GAN utilizes a spherical gaussian prior for more successful interpolation. The WaveNet autoencoder struggles with complex priors on latents, leading to unrealistic interpolations. In contrast, the IF-Mel GAN uses a global conditioning approach for smooth interpolation, producing high-fidelity audio examples. The IF-Mel GAN uses global conditioning for smooth interpolation, creating high-fidelity audio examples. Timbre morphs smoothly between instruments while pitches follow a composed piece, maintaining timbral identity across octaves. The GAN maintains consistent timbral identity across pitches, allowing for parallel processing of training and generation on modern GPU hardware. This results in significantly reduced latency for audio sample synthesis. The GAN allows for parallel processing of training and generation on modern GPU hardware, reducing latency for audio sample synthesis. This enables realtime neural network audio synthesis on devices, expanding the range of expressive sounds that can be explored. The work on realtime neural network audio synthesis allows for a broader range of expressive sounds to be explored. Compared to speech synthesis, audio generation for music is relatively under-explored, with autoregressive models showing promise in synthesizing musical instrument sounds. In comparison to speech, audio generation for music is relatively under-explored. Previous work has focused on autoregressive models for synthesizing musical instrument sounds, but these models have slow generation times. GANs have been applied to audio generation with promising results, but have not yet achieved the audio fidelity of autoregressive models. Recent advances in GAN literature, such as modifications to loss functions and progressive training, have shown improved training stability and generation quality. Our work builds on recent advances in GAN literature, proposing a modification to the loss function for improved training stability and generation quality. Progressive training is introduced to enhance generation quality within a limited training time. The NSynth dataset, known as the \"CelebA of audio,\" was used to interpolate between musical instrument timbres. BID23 incorporated an adversarial domain confusion loss for timbre transformations across various audio sources. BID5 achieved significant sampling speedups by training a frame-based regression model. BID5 used WaveNet autoencoders to achieve timbre transformations between audio sources with significant sampling speedups. They trained a frame-based regression model for mapping pitch and instrument labels to raw waveforms, yielding good frequency estimation but lacking phase coherency. Their GAN-based approach on the NSynth dataset surpassed WaveNet fidelity. The architecture for audio generation with GANs on the NSynth dataset surpasses WaveNet fidelity, but requires a large amount of channels. Further work is needed to validate and expand to other types of natural sound, opening up possibilities for domain transfer and other applications of adversarial losses to audio. The work on audio generation with GANs surpasses WaveNet fidelity but requires validation and expansion to other natural sound types. Possible applications of adversarial losses to audio are explored, with issues of mode collapse and diversity addressed in further research. The models were trained with the ADAM optimizer and tested with different learning rates and weights for the auxiliary classifier loss. A learning rate of 8e-4 and classifier loss of 10 performed the best. The networks used box upscaling/downscaling and pixel normalization. The discriminator included the standard deviation of minibatch activations as a scalar channel. Tanh output nonlinearity was used for the generator, and real data was normalized before passing to the discriminator. The discriminator appends the standard deviation of minibatch activations as a scalar channel. Real data is normalized before passing to the discriminator. GAN variants are trained for 4.5 days on a single V100 GPU with a batch size of 8. Progressive models train on 1.6M examples per stage, with 7 stages in total. Training continues until the 4.5 days are completed. The GAN variant is trained for 4.5 days on a single V100 GPU with a batch size of 8. Progressive models train on 1.6M examples per stage, with 7 stages in total. The WaveNet baseline uses a Tensorflow implementation with a decoder composed of 30 layers of dilated convolution. The conditioning stack operates on a one-hot pitch conditioning signal distributed in time. The WaveNet model consists of 3 stacks of 10 layers each, with increasing dilation in each stack. The audio encoder stack is replaced with a conditioning stack operating on a one-hot pitch signal. The conditioning stack consists of 5 layers of dilated convolution and 3 layers of regular convolution. The model uses mulaw encoding for the 8-bit version and a quantized mixture for the 16-bit version. Training took 2 days with 32 GPUs and synchronous SGD."
}