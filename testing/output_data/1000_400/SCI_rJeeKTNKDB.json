{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive, adding new substructures while resolving their attachment to the molecule. Our model outperforms previous baselines in multiple molecular optimization tasks. Our model significantly outperforms previous state-of-the-art baselines in molecular optimization tasks by translating input molecular graphs into better forms with improved chemical properties. This task is challenging due to the vast space of potential candidates and complex dependencies involved in graph generation. The task of molecular graph generation is complex due to the vast space of candidates and intricate dependencies. Previous work utilized a junction tree encoder-decoder approach but had limitations in separate encoding of trees and graphs, and strictly successive decoding steps. Our proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation addresses limitations of previous approaches by interleaving the prediction of substructure components with their attachments in an auto-regressive manner. This allows for more consistent substructure attachments across different nodes in the junction tree. The proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation addresses limitations by predicting substructure components and their attachments in an auto-regressive manner. This enables modeling strong dependencies between successive attachments and substructure choices, with the encoder representing molecules at different resolutions to match the decoding process. The encoder-decoder model predicts substructure components and attachments in an auto-regressive manner, capturing essential information at different resolutions. The decoding process is efficient, decomposing generation steps to avoid combinatorial explosion. Conditional translation allows for handling different criteria combinations at test time. The method efficiently decomposes generation steps to avoid combinatorial explosion and handles conditional translation criteria. By interleaving tree and graph decoding steps, the model can prevent the generation of invalid junction trees. An autoregressive decoder is proposed to address inconsistent local substructure attachments during training. The new model is evaluated on various molecular optimization tasks, surpassing previous state-of-the-art graph generation methods. Our new autoregressive decoder improves molecule generation by interleaving substructure prediction with attachments. It outperforms previous methods, achieving 3.3% and 8.1% improvement on QED and DRD2 optimization tasks. The model runs 6.3 times faster during decoding and shows advantages in hierarchical decoding and multi-resolution encoding. Our new autoregressive decoder improves molecule generation by interleaving substructure prediction with attachments, yielding significant improvements on QED and DRD2 optimization tasks. The model runs 6.3 times faster during decoding and demonstrates advantages in hierarchical decoding and multi-resolution encoding. Previous methods for generating molecular graphs have utilized various approaches, such as generating molecules based on SMILES strings. Various methods have been developed for generating molecular graphs, including those that generate molecules based on SMILES strings and others that output adjacency matrices and node labels at once. Some models decode molecules sequentially node by node, while others adopt node-by-node approaches in the context of reinforcement learning. Additionally, a hypergraph grammar based method has been developed for molecule generation. Jin et al. (2018) have also worked on generating molecules based on substructures using a two-stage procedure. Our work is closely related to Jin et al. (2018) who generated molecules based on substructures using a two-stage procedure. They used a node-by-node approach and a junction tree to capture coarse relative arrangements, but had drawbacks of introducing local independence assumptions and applying steps stage-wise during decoding. In contrast, our method jointly predicts substructures and their attachments with an autoregressive decoder. The second step introduced local independence assumptions and the decoder is not autoregressive. These steps are applied stage-wise during decoding, first realizing the junction tree and then reconciling attachments without feedback. Our method jointly predicts substructures and their attachments with an autoregressive decoder, representing molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees. Our method represents molecules as hierarchical graphs, spanning from atom-level graphs to substructure-level trees. It is related to graph encoders for molecules but differs by utilizing a hierarchical approach. The encoder represents each molecule across three levels: atom layer, attachment layer, and substructure layer. Our approach involves constructing multiple layers of graph hierarchy for molecule representation. Different from existing methods focusing on graph representation as a single vector, we encode molecules into multiple sets of vectors at various resolutions for graph generation tasks. These vectors are dynamically aggregated by decoder attention modules in each step of graph generation. Our approach involves graph generation for molecule representation by encoding molecules into multiple sets of vectors at different resolutions. The decoder attention modules dynamically aggregate these vectors in each generation step to create a new molecule with improved chemical properties. The attachment prediction process involves predicting attaching points in the new substructure and their corresponding attaching points in the current graph. This hierarchical generation process is supported by our encoder-decoder with neural attention. Our model encodes molecules into a hierarchical graph with substructure, attachment, and atom layers. The encoder represents molecules at multiple resolutions to provide necessary information for each decoding step in the generation process. The model encodes molecules into a hierarchical graph with substructure, attachment, and atom layers. It represents nodes as substructure vectors, attachment vectors, and atom vectors for decoding predictions. The decoder utilizes a multi-layer neural network and bilinear attention for generating substructures from molecules. The paper discusses the construction of a substructure tree T to characterize how substructures are connected in a molecule G. Substructures S1 to Sn are defined as subgraphs of the molecule induced by atoms and bonds. The vocabulary of substructures S is constructed from the training set with high coverage on test sets. The paper discusses constructing a substructure tree T to characterize connections in molecule G. Substructures are defined as subgraphs induced by atoms and bonds, with high coverage on test sets. The graph decoder generates molecule G by expanding its substructure tree in a depth-first order, making predictions for new substructures and their attachments. The paper discusses constructing a substructure tree T to characterize connections in molecule G. The graph decoder generates molecule G by expanding its substructure tree in a depth-first order, making predictions for new substructures and their attachments. The model predicts new substructures and their attachments based on the encoding of input X. It first predicts whether there will be a new substructure attached to the current node S k, then decides to create a new substructure S t and predicts its type, followed by predicting how this new substructure should be attached to the graph. The model predicts new substructures and their attachments based on the encoding of input X. It first predicts whether a new substructure will be attached to the current node S k, creates a new substructure S t, predicts its type, and then determines how this new substructure should be attached to the graph. The model predicts new substructures and their attachments based on the encoding of input X. It first predicts attaching atoms {v j} to S k from a vocabulary A(S t), then finds corresponding atoms {u j} in S k. The predictions form an autoregressive factorization of the distribution over the next substructure and its attachment. The model predicts new substructures and their attachments based on the encoding of input X. The probability of a candidate attachment M is computed using atom representations. The predictions follow an autoregressive factorization, with each step depending on the previous one. Teacher forcing is applied during training, with a depth-first traversal over the ground truth substructure tree. Attachment enumeration is tractable due to small substructure sizes in experiments. The encoder represents a molecule X with a hierarchical graph H X. The encoder represents a molecule X with a hierarchical graph H X, where the generation order is determined by a depth-first traversal over the ground truth substructure tree. The attachment enumeration is tractable due to small substructure sizes, with an average attachment vocabulary size of less than 5 and fewer than 20 candidate attachments. The hierarchical graph of molecule X consists of an atom layer and an attachment layer derived from the substructure tree. The hierarchical graph of molecule X includes an atom layer with labels for atom type and charge, an attachment layer derived from the substructure tree, and a substructure layer. The attachment layer provides information for attachment prediction, with nodes representing attachment configurations of substructures. The substructure layer is similar to the attachment layer. The hierarchical graph of molecule X consists of an atom layer with atom type and charge labels, an attachment layer derived from the substructure tree, and a substructure layer. The attachment layer shows attachment configurations of substructures for prediction, while the substructure layer provides essential information for substructure prediction in the decoding process. Edges are introduced to connect atoms and substructures between layers for information propagation, forming the hierarchical graph H X encoded by a hierarchical message passing network (MPN). The hierarchical graph of molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The MPN architecture from Jin et al. (2019) is used for encoding, with the encoder containing three MPNs for each layer. The MPN encoding process is denoted as MPN \u03c8 (\u00b7) with parameter \u03c8. The atom layer MPN encodes the atom layer of H X by propagating message vectors between atoms for T iterations and outputs the atom representation h v for each atom v. The attachment layer MPN processes the input features of each node A i. The atom layer of molecule X is encoded using a message passing network (MPN) with three layers: atom, attachment, and substructure. The MPN propagates message vectors between atoms for T iterations to output atom representations. The attachment layer processes input features of each node, while the substructure layer computes substructure representations through message passing. The hierarchical encoder uses message passing to compute substructure representations in multiple layers. The output vectors represent a molecule at different resolutions and are used in the decoder attention during decoding. The hierarchical encoder outputs vectors representing a molecule at different resolutions, used in the decoder attention. The decoder uses the same architecture to encode the hierarchical graph during decoding, generating diverse outputs for molecular pairs in the training set. A variational translation model is extended to generate molecule Y given inputs c X and z. The model generates diverse outputs for molecular pairs by extending a variational translation model to include an additional input z, sampled from a Gaussian prior during testing. The latent vector z indicates the intended mode of translation and is computed using variational inference. The model encodes structural changes from molecule X to Y and samples z using a reparameterization trick for decoding. The model encodes structural changes from molecule X to Y and samples z using a reparameterization trick for decoding. During testing, users cannot change the behavior of a trained model, which may limit multi-property optimization. Conditional translation allows desired criteria to be fed as input for the translation process. During testing, users cannot change the behavior of a trained model, limiting multi-property optimization. To address this, conditional translation allows desired criteria to be inputted for the translation process, enabling control over the outcome based on specified criteria. The translation model allows users to specify criteria for molecular optimization, following an experimental design by Jin et al. (2019). A novel conditional optimization task is constructed where desired criteria are inputted to control the outcome. Molecular similarity between input and output molecules must meet a threshold to prevent arbitrary translations. The model is evaluated on single-property optimization tasks with four different datasets. The model is trained for single-property optimization tasks on four different datasets. For the LogP Optimization task, the model aims to improve the solubility and synthetic accessibility of a compound by translating input X into output Y with logP(Y) > logP(X). The molecular similarity between X and Y must be above a threshold of 0.4. The model aims to improve compound solubility and synthetic accessibility by translating input X into output Y with logP(Y) > logP(X). Two similarity thresholds are experimented with, and the model is trained under conditional translation setup. Evaluation metrics include translation accuracy and diversity. The model is trained to translate input molecules into output molecules with improved properties. Evaluation metrics include translation accuracy, diversity, and success rate based on similarity and property constraints. The average property improvement is reported for logP optimization tasks, while diversity is measured using Tanimoto distance between successfully translated compounds. HierG2G is compared against baselines like GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. Seq2Seq generates molecules using SMILES strings, JTNN generates molecules structure by structure, and CG-VAE decodes molecules atom by atom. The comparison includes Seq2Seq, JTNN, and CG-VAE for molecule generation. AtomG2G is developed as a baseline for atom-based translation, predicting atom and bond types in each generation step. The encoder of AtomG2G encodes only the atom-layer graph, while the decoder captures edge dependencies. AtomG2G predicts atom and bond types in each generation step, achieving state-of-the-art results on translation tasks with improved accuracy and output diversity compared to JTNN. The autoregressive decoder allows for more expressive mappings, leading to better performance. Our model outperforms JTNN and AtomG2G on translation tasks, showing higher accuracy and output diversity. The autoregressive decoder allows for more expressive mappings, leading to faster decoding times. Comparing with other methods like Seq2Seq, our model consistently achieves better results in both accuracy and diversity. Our model outperforms other translation methods like Seq2Seq, JTNN, and AtomG2G in accuracy and output diversity. Training on specific criteria yields varying success rates, with our model showing transfer of knowledge from different pairs. Ablation studies on QED and DRD2 tasks demonstrate the benefits of our hierarchical decoder over atom-based decoding. The study shows that the hierarchical decoder in our model benefits translation accuracy over atom-based decoding. Experimenting with different architecture choices, we found that reducing hierarchies in the encoder and decoder led to a drop in translation accuracy. The DRD2 task particularly benefits from structure-based decoding due to the importance of specific functional groups in biological target binding. The study found that the DRD2 task benefits more from structure-based decoding due to the importance of specific functional groups in biological target binding. Reducing hierarchies in the encoder and decoder led to a drop in translation accuracy. Removing substructure layers and the attachment layer significantly degraded performance on both datasets. Replacing the LSTM MPN with the original GRU MPN resulted in a decrease in translation performance, but our method still outperformed JTNN. In this paper, a hierarchical graph-to-graph translation model was developed to generate molecular graphs using chemical substructures as building blocks. The model, which is fully autoregressive, learns coherent multi-resolution representations and outperforms previous models in various settings. The LSTM MPN architecture was used for HierG2G and AtomG2G baseline, despite a slight decrease in translation performance when compared to the original GRU MPN used in JTNN. Our model, fully autoregressive, outperforms previous models in various settings. The message passing network MPN \u03c8 over graph H is defined as Algorithm 3 LSTM MPN with T message passing iterations. Our attention layer is a bilinear attention function. Figure 7 illustrates the AtomG2G decoding process. AtomG2G is an atom-based translation method that directly compares to HierG2G. The decoding process involves predicting new atoms and bond types in a molecular graph representation. Training set sizes and substructure vocabulary are listed for each dataset. The training set sizes and substructure vocabulary for each dataset are listed in Table 3. Multi-property optimization was created by combining the QED and DRD2 optimization task training sets. The test set includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters for HierG2G and AtomG2G models are specified. For HierG2G and AtomG2G models, hyperparameters were set with hidden layer dimensions, embedding layer dimensions, latent code dimensions, KL regularization weight, and number of message passing iterations. CG-VAE models were used for molecule generation and property prediction tasks. Three CG-VAE models were trained for logP, QED, and DRD2 optimization tasks. At test time, compounds were translated following a specific procedure. At test time, compounds are translated using a CG-VAE model to maximize predicted property scores. Keeping the KL regularization weight low is crucial for meaningful results, as a higher weight leads to dissimilar molecule generation. Ablation studies were conducted to further analyze the results. In ablation studies, experiments were conducted to modify the decoder and reduce the number of hierarchies in the encoder and decoder MPN. The results showed the importance of keeping the KL regularization weight low for generating meaningful molecules. In experiments to modify the decoder and reduce hierarchies in the encoder and decoder MPN, the hidden and embedding layer dimensions were set to 300 to match the original model size. Two-layer and one-layer models were tested, with adjustments made for topological and substructure predictions."
}