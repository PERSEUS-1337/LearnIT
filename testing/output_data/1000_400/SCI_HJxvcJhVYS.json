{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A proposed method uses Bayesian optimization to approximate inference by applying Stein variational gradient descent on Gaussian process model estimates. Initial results show promise for likelihood-free inference in reinforcement learning environments. The method uses Bayesian optimization and Stein variational gradient descent on Gaussian process model estimates to learn approximations to the posterior distribution for likelihood-free inference in reinforcement learning environments. It addresses the problem of estimating parameters of a physical system when the likelihood function is not available, resorting to methods like approximate Bayesian computation and conditional density estimation. Recent methods have been developed to improve the efficiency of simulations in robotics and reinforcement learning applications. These methods involve constructing conditional density estimators from joint data or sequentially learning approximations to the likelihood function and running Markov chain Monte Carlo. Gutmann and Corander (2016) have introduced an active learning approach using Bayesian optimization. This paper explores combining variational inference methods with Bayesian optimization to improve simulation efficiency in robotics and reinforcement learning. A Thompson sampling strategy is used to refine variational approximations to a black-box posterior, with parameters proposed using Stein variational gradient descent. The approach combines variational inference methods with Bayesian optimization to refine variational approximations to a black-box posterior. Parameters are proposed using Stein variational gradient descent over samples from a Gaussian process. The method optimally subsamples variational approximations for batch evaluations of simulator models. The goal is to estimate a distribution that approximates a posterior distribution over simulator parameters given observations from a target system. The approach involves deriving an optimal distribution q to approximate a posterior distribution over simulator parameters without access to a likelihood function. A Bayesian optimization algorithm is used to minimize the discrepancy between q and the target distribution, utilizing a GP model and Thompson sampling acquisition function. The approach involves deriving an optimal distribution q to approximate a posterior distribution over simulator parameters without access to a likelihood function. A Bayesian optimization algorithm is used to minimize the discrepancy between q and the target distribution, utilizing a GP model and Thompson sampling acquisition function. The algorithm uses a black-box approach to solve Equation 1 without requiring gradients of the target distribution p. Stein variational gradient descent (SVGD) is applied directly to learn q, bypassing the need for modeling the map from q's parameters to the corresponding KSD. The approach involves using Stein variational gradient descent (SVGD) to learn the optimal distribution q directly, bypassing the need for gradients of the target distribution p. A Gaussian Process (GP) is used to model the synthetic likelihood function, providing an approximation for the simulations-observations discrepancy \u2206 \u03b8. Background details on the Kernelized Stein Discrepancy (KSD) are provided in the appendix. The simulations-observations discrepancy \u2206 \u03b8 is approximated using a Gaussian Process (GP) for smooth kernels, enabling the application of SVGD in the Bayesian optimization (BO) loop. Candidate distributions q n are selected based on Thompson sampling from the GP posterior, accounting for uncertainty in the model. Thompson sampling has been successful in BO problems for selecting point candidates \u03b8 \u2208 \u0398. Thompson sampling is applied to Bayesian optimization problems by selecting point candidates \u03b8 \u2208 \u0398. For models with finite feature maps like sparse spectrum Gaussian processes (SSGPs), sampling weights w n from a multivariate Gaussian constitutes a sample from the posterior. The acquisition function is defined based on an approximation to the target posterior using SVGD, representing the variational distribution as a set of particles. The acquisition function in Bayesian optimization is defined using SVGD, representing the variational distribution as a set of particles. These particles are optimized through perturbations guided by gradients of logp n for diversification and convergence to local maxima. Gradients of sample functions are available for SSGP models with differentiable mean functions. The text discusses using SVGD to optimize particles in Bayesian optimization, with gradients of logp n guiding diversification and convergence to local maxima. Gradients of sample functions are available for SSGP models with differentiable mean functions. Using a large number of particles improves Algorithm 1 in Bayesian optimization by maximizing the acquisition function via SVGD. Kernel herding is used to select a subset of query parameters to run the simulator efficiently. The error in empirical estimates is minimized by constructing a set of samples that minimizes the maximum mean discrepancy between the kernel embedding of the distribution and its subsampled version. Kernel herding constructs a set of samples to minimize error on empirical estimates under a given distribution. The procedure involves selecting informative samples based on the GP posterior kernel, which provides an embedding for the distribution. The GP posterior kernel encodes information to select informative samples for the model. It provides an embedding for the distribution based on previously observed locations in the GP data. The DBO algorithm is summarized in Algorithm 1 and evaluated against MDNs on synthetic data scenarios using OpenAI Gym's cart-pole environment. The proposed method is evaluated against mixture density networks (MDNs) on synthetic data scenarios in OpenAI Gym's cart-pole environment. A uniform prior with specific bounds for the environment is used, and results show that the method outperforms the MDN approach in recovering the target system's posterior. Further details can be found in Appendix B. The paper presents a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations to the posterior compared to MDN. Results indicate that distributional Bayesian optimization is more sample-efficient for inferring parameters in reinforcement learning environments. Future work includes scalability and theoretical advancements. The results demonstrate the potential of distributional Bayesian optimization for reinforcement learning applications, showing it to be more sample-efficient than other likelihood-free inference methods. Future work includes scalability and theoretical analysis. The method allows for representing any function sampled from the SSGP posterior, with fast incremental updates to reduce time complexity. The posterior over g is determined by a decomposition to update the GP posterior with fast incremental updates, reducing time complexity to O(M^2)."
}