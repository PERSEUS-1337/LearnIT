{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets of the phrase's meaning and outperform strong baselines on benchmark datasets. A neural model predicts cluster centers from input text during test time, providing interpretable semantic representations and outperforming baselines on various NLP tasks. Many NLP models learn from raw text co-occurrence statistics without supervision, using embeddings like word2vec, GloVe, skip-thoughts, ELMo, and BERT. These models typically use a single embedding for a sentence or phrase. Word embeddings like word2vec, GloVe, skip-thoughts, ELMo, and BERT are commonly used in NLP models. These models usually use a single embedding for a sentence or phrase, which can limit their ability to capture multiple senses or topics. To address this limitation, word sense induction methods and multi-mode word embeddings represent target words as multiple points in a semantic space by clustering words that appear together. Word sense induction methods and multi-mode word embeddings represent target words as multiple points in a semantic space by clustering words that appear together, addressing the limitation of single embeddings in NLP models. Real property is used as an example to illustrate this multi-mode representation, where different senses are discovered by clustering neighboring words. This approach contrasts with topic modeling like LDA, as it requires solving a distinct clustering problem for each target word. The previous approaches in word sense induction and multi-mode word embeddings discover senses by clustering neighboring words, contrasting with topic modeling like LDA. Extending these representations to phrases or sentences faces efficiency challenges due to the large number of unique sequences and parameters required for clustering. The previous approaches in word sense induction and multi-mode word embeddings discover senses by clustering neighboring words. Extending these representations to phrases or sentences faces efficiency challenges due to the large number of unique sequences and parameters required for clustering. The number of parameters for clustering-based approaches is significant, making it time and space-consuming. Our compositional model aims to predict the embeddings of cluster centers from the sequence of words in the target phrase to reconstruct the co-occurring distribution effectively. The compositional model in this work aims to predict cluster centers' embeddings from the target phrase's word sequence to effectively reconstruct the co-occurring distribution. The approach uses a neural encoder and decoder to compress redundant parameters in local clustering problems. The approach in this work uses a neural encoder and decoder to compress redundant parameters in local clustering problems by learning a mapping between target sequences and cluster centers. The proposed model uses a neural network to generate cluster centers in any order by using a sparse coefficient matrix. This allows for joint training of the model and captures compositional meanings of words better than traditional methods. The model can also measure asymmetric relations like hypernymy without supervision. The proposed model captures compositional meanings of words better than traditional methods and can measure asymmetric relations like hypernymy without supervision. It outperforms single-mode alternatives in sentence representation, as shown in an extractive summarization experiment. The training setup, objective function, and architecture of the prediction mode are formalized in Sections 2.1, 2.2, and 2.3, respectively. The model represents each sentence as multiple codebook embeddings. The model represents each sentence as multiple codebook embeddings predicted by the sequence to embeddings model. The loss function encourages the model to generate codebook embeddings that can reconstruct co-occurring words but not negatively sampled words to avoid predicting common topics. The sequence of words in the corpus is expressed as I t = w xt ...w yt <eos>, where x t and y t are the start and end positions of the target sequence. The model represents sentences as codebook embeddings to reconstruct co-occurring words but not negatively sampled words. The sequence of words in the corpus is expressed as I t = w xt ...w yt <eos>, with neighboring words related to the target sequence. Training signals differ for sentences and phrases, requiring separate models for each representation. The model uses fixed window size for sentence representation, with different training signals for sentences and phrases. To predict co-occurring words, it clusters words that could possibly occur beside a sequence, focusing on semantics rather than syntax. This approach allows learning from similar sequences even if the co-occurring words are not observed in the corpus. The model focuses on predicting co-occurring words by clustering them based on semantics rather than syntax. It considers word order information in the input sequence but ignores the order of co-occurring words. The distribution of co-occurring words is modeled in a pre-trained word embedding space using a matrix arrangement. The model utilizes a pre-trained word embedding space to predict co-occurring words by clustering them based on semantics. It arranges the embeddings into matrices and predicts cluster centers for the input sequence. The number of clusters is fixed to simplify the prediction model design, and the reconstruction loss of k-means clustering in the word embedding space is calculated. In this work, the model simplifies the prediction model design by fixing the number of clusters. The reconstruction loss of k-means clustering in the word embedding space is discussed, along with the use of Non-negative sparse coding to generate diverse cluster centers. The model adopts Non-negative sparse coding to encourage diversity in cluster centers, as opposed to k-means clustering which collapses to fewer modes. The NNSC loss is smoother and easier to optimize for neural networks, leading to better reconstruction of embeddings. The coefficient value M k,j is constrained to be \u2264 1 to prevent learning centers with small magnitudes. The proposed loss in NNSC efficiently minimizes L2 distance in a pre-trained embedding space, avoiding computationally expensive permutations between predictions and ground truth words. The coefficient value M k,j is constrained to \u2264 1 to prevent learning centers with small magnitudes, ensuring stability in optimization. The proposed loss in NNSC efficiently minimizes L2 distance in a pre-trained embedding space by estimating M Ot using convex optimization. The neural network architecture is similar to Word2Vec, encoding compositional meaning and decoding multiple embeddings. Our method is a generalization of Word2Vec that encodes compositional meaning and decodes multiple embeddings. The neural network architecture is similar to a transformation-based seq2seq model. The encoder transforms input sequences into contextualized embeddings to map sentences with similar word distribution closer together. Unlike typical seq2seq models, our decoder predicts a sequence of embeddings instead of words, allowing us to predict all codebook embeddings in a single forward pass efficiently. Our decoder predicts a sequence of embeddings instead of words, allowing for efficient prediction of all codebook embeddings in a single forward pass. Different linear layers are used to capture different aspects of the embeddings, and removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation. The decoder predicts embeddings instead of words, using different linear layers to capture aspects. Removing attention on contextualized word embeddings increases validation loss for sentence representation. The framework is flexible, allowing for different architectures and input features. The framework is flexible, allowing for different architectures and input features. The cluster centers predicted by the model summarize the target sequence well, with more codebook embeddings capturing semantic facets. Codebook embeddings can improve performances in unsupervised semantic tasks, indirectly measuring the quality of generated topics. The codebook embeddings can enhance unsupervised semantic tasks by improving performance and indirectly measuring topic quality. Different versions of pre-trained GloVe embeddings are used for sentence and phrase representation. The model is trained on Wikipedia 2016, with stop words removed from co-occurring words. Noun phrases are considered in experiments, with boundaries extracted using regular expression rules on POS tags. Sentence boundaries and POS tags are detected using spaCy. No additional resources like PPDB are required for the models. Our models, trained on Wikipedia 2016 with stop words removed, do not require additional resources like PPDB. Noun phrases are considered in experiments, with boundaries extracted using regular expression rules on POS tags. Sentence boundaries and POS tags are detected using spaCy. The models are compared with baselines using raw text and sentence/phrase boundaries. The transformers have a GloVe embedding size of 300 dimensions and are trained on a single GPU within a week. The models trained on Wikipedia 2016 with stop words removed use a GloVe embedding size of 300 dimensions and are trained on a single GPU within a week. Comparing with BERT, our models underfit the data due to their smaller size and lack of syntax information preservation. BERT base model is effective for supervised tasks, trained with more parameters and resources. It uses a word piece model to handle out-of-vocabulary issues. Unsupervised performances are based on cosine similarity. Semeval 2013 and Turney 2012 are standard benchmarks for phrase similarity evaluation. BiRD and WikiSRS provide ground truth phrase similarities. Semeval 2013 distinguishes similar from dissimilar phrases, while Turney aims to identify query bigrams. The task involves distinguishing similar phrase pairs from dissimilar ones. Turney aims to identify the most similar unigram to a query bigram. BiRD and WikiSRS measure phrase relatedness and similarity. Our model evaluates phrase similarity using transformer encoder embeddings and cosine similarity, labeled as Ours Emb. Our model evaluates phrase similarity by averaging contextualized word embeddings from a transformer encoder and computing cosine similarity between them. We also calculate a symmetric distance SC by comparing normalized codebook embeddings of phrases. When retrieving similar phrases, negative distance represents similarity. Comparison with 5 baselines like GloVe Avg and Word2Vec Avg is done, which compute cosine similarity between averaged word embeddings. Our model outperforms 5 baselines in phrase similarity evaluation by averaging contextualized word embeddings and computing cosine similarity. The symmetric distance SC is calculated by comparing normalized codebook embeddings. Our strong performances in Turney dataset validate the effectiveness of our encoder. The performances of our models significantly outperform baselines in 4 datasets, especially in Turney dataset, indicating the effectiveness of non-linearly composing word embeddings. Our results support the idea that multi-mode embeddings may not always improve performance in word similarity benchmarks. The performance of our models, particularly Ours (K=1), is slightly better than Ours (K=10), supporting the idea that multi-mode embeddings may not always enhance word similarity benchmarks. Despite this, Ours (K=10) still outperforms baselines, indicating that the number of clusters does not significantly impact similarity performance. The STS benchmark is a widely used sentence similarity task that evaluates semantic similarity scores between sentence pairs. The STS benchmark evaluates semantic similarity scores between sentence pairs using various models like BERT, GloVe, word mover's distance, and cosine similarity. Additionally, weighting words in sentences based on probability has been proposed by Arora et al. In addition to BERT CLS, BERT Avg, and GloVe Avg, the method is compared with word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). Arora et al. propose weighting words in sentences based on probability, with \u03b1 set to 10^-4 in the STS benchmark. The post-processing method from Arora et al. is used to remove the first principal component in GloVe SIF. Performance is also reported before removing principal components, known as GloVe Prob_avg. The method GloVe SIF estimates word importance using training distribution. It suggests considering word embeddings in addition to sentence embeddings for measuring sentence similarity. Multi-facet embeddings allow for estimating word importance in predicting co-occurring words. Cosine similarity is used to compute word importance in the sentence. The multi-facet embeddings enable the estimation of word importance by predicting co-occurring words. Cosine similarity is utilized to compute the importance of a word in a sentence, which is then multiplied with original weighting vectors to generate results for different methods. Our SC method outperforms WMD in matching topics between sets. The proposed method uses importance weighting to enhance GloVe embeddings, outperforming other methods in topic matching. The benefits of multi-mode representation are demonstrated, especially in STSB Low scenarios. The proposed attention weighting significantly boosts performance, particularly when not relying on the training distribution's generalization assumption. The proposed method uses importance weighting to enhance GloVe embeddings, outperforming other methods in topic matching. It demonstrates the benefits of multi-mode representation, especially in STSB Low scenarios. The attention weighting significantly boosts performance, particularly when not relying on the training distribution's generalization assumption. A variant of the method using a bi-LSTM as the encoder and a LSTM as the decoder performs worse than the transformer alternative. The variant outperforms ST Cos, supporting the approach of ignoring the order of co-occurring words in the NNSC loss. The model is applied to HypeNet for unsupervised hypernymy detection. The proposed method outperforms other methods in topic matching by using importance weighting to enhance GloVe embeddings. It demonstrates the benefits of multi-mode representation, especially in STSB Low scenarios. The attention weighting significantly boosts performance, particularly when not relying on the training distribution's generalization assumption. A variant of the method using a bi-LSTM as the encoder and a LSTM as the decoder performs worse than the transformer alternative, but still outperforms ST Cos. The model is applied to HypeNet for unsupervised hypernymy detection, where the predicted codebook embeddings of a hyponym often reconstruct the embeddings of its hypernym better than the other way around. The asymmetric scoring function defined in the document compares the AUC of detecting hypernyms with other relations and the accuracy of detecting the hypernym direction. The methods proposed outperform baselines by providing asymmetric similarity measurement. The summary A with normalized embeddings best reconstructs the distribution of normalized word embeddings in the document. The importance of a word is determined by \u03b1 \u03b1+p(w), and the summary A is assumed to consist of various aspects representing all topics/concepts in the document. The extractive summarization method aims to discover a summary A with normalized embeddings that reconstruct the distribution of word embeddings in the document. The summary consists of T sentences, and sentences are greedily selected to optimize the process. Multiple codebook embeddings are generated to represent each sentence, allowing for a comprehensive representation of different aspects. Our model generates multiple codebook embeddings to represent each sentence in the document, capturing different aspects. Comparing with alternative methods like average word embeddings and using all words in sentences as aspects, our approach with fixed codebook embeddings avoids issues. Testing baselines include selecting random sentences and first n. The method W Emb uses word embeddings for sentence aspects, while our approach with fixed codebook embeddings avoids issues. Testing includes selecting random sentences and first n. Results are compared using ROUGE F1 scores. The methods in the study focus on evaluating unsupervised sentence embeddings by comparing different approaches without assuming the first few sentences are a good summary. Results show that predicting more aspects leads to better performance, with a cluster number of K=100 yielding the best results. This highlights the importance of larger cluster numbers in achieving optimal performance. Our method allows for setting a large cluster number K to improve performance in unsupervised sentence embeddings. Topic modeling has been widely studied and applied for its interpretability and flexibility in incorporating different input features. Neural networks have also been shown to discover coherent topics efficiently. Sparse coding on word embedding space is used to model multiple aspects of a word efficiently. Parameterizing word embeddings with neural networks helps test hypotheses and save storage space. Words are represented as single or multiple regions in Gaussian embeddings to capture asymmetric relations. One challenge is designing a neural decoder for sets rather than sequences, requiring a matching step between elements and computing distance loss. Chamfer distance is a popular loss used in auto-encoder models for point clouds, with more advanced matching loss options available. The studies focus on measuring symmetric distances between ground truth and predicted sets, with set decoders reconstructing sets using fewer bases. Various methods for achieving permutation invariants loss in neural networks include removing predicted elements from the ground truth set, beam search, and predicting permutations using CNNs or transformers. Our goal is to efficiently predict clustering centers that reconstruct observed instances, overcoming computational challenges in learning multi-mode representations for long sequences. Using a neural encoder to model target sequence meaning and a neural decoder to predict codebook embeddings. In this work, a neural encoder is used to model the meaning of target sequences, while a neural decoder predicts codebook embeddings as representations of sentences or phrases. A non-negative sparse coefficient matrix is employed during training to match predicted embeddings to observed words. The proposed models can predict interpretable clustering centers and outperform BERT, skip-thoughts, and GloVe in unsupervised benchmarks. The proposed models can predict interpretable clustering centers for sequences, outperforming BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings perform best for sequences with multiple aspects, while single-facet embeddings are effective for sequences with one aspect. Future work aims to generate multi-facet embeddings for both phrases and sentences, evaluating them as pre-trained embeddings for supervised or semi-supervised settings. In the future, the goal is to train a single model that can generate multi-facet embeddings for phrases and sentences, and evaluate it as a pre-trained embedding approach for supervised or semi-supervised settings. Additionally, the method will be applied to other unsupervised learning tasks that rely on co-occurrence statistics. The model is kept simple to converge training loss quickly, without fine-tuning hyper-parameters, using a smaller model compared to BERT with similar architecture details and hyper-parameters. The sparsity penalty weight is set at 0.4, with a maximal sentence size of 50. The model uses a smaller architecture compared to BERT, with similar hyper-parameters. The sparsity penalty weight is set at 0.4, with a maximal sentence size of 50. The number of dimensions in transformers is set to be 300, with specific settings for sentence and phrase representation. The number of transformer layers on the decoder side is 5 for sentence representation and 2 for phrase representation, with dropout on attention set at 0.1 and 0.5 respectively. The window size is 5, and hyperparameters are determined by validation loss. The number of codebook embeddings K is chosen based on training data performance. The number of codebook embeddings K is determined by training data performance, with larger K potentially leading to longer training times. The skip-thoughts hidden embedding size is set to 600, and the model has fewer parameters than BERT base, requiring less computational resources for training. The skip-thoughts model uses a hidden embedding size of 600 and was retrained on Wikipedia 2016 for 2 weeks. Comparing with BERT Large, BERT large performs better in similarity tasks but worse in hypernym detection. Despite BERT's performance gains, the method presented is superior in most cases, especially in phrase similarity tasks. Increasing the model size of BERT may be a future direction, but the current method remains more effective. BERT performs better in similarity tasks compared to hypernym detection tasks. Increasing the model size of BERT may be a future direction, but the current method remains more effective, especially in phrase similarity tasks. The bad performances of W Emb methods may be due to the tendency of selecting shorter sentences. In Section 3.4, a comparison is made between different unsupervised summarization methods when they choose the same number of sentences. The performance of W Emb (*) methods may suffer due to selecting shorter sentences. A plot of R-1 performance versus sentence length shows that Ours (K=100) outperforms W Emb (GloVe) and Sent Emb (GloVe) when summaries are of similar length. Additionally, W Emb (*) generally outperforms Sent Emb (*) when comparing summaries of similar length, although this comparison may not be entirely fair. In comparison to W Emb (GloVe) and Sent Emb (GloVe), Ours (K=100) performs better when summaries are of similar length. W Emb (*) tends to outperform Sent Emb (*) in this scenario, although the comparison may not be entirely fair. Ours (K=100) is the best choice for summaries less than 50 words, while W Emb (BERT) is more effective for longer summaries. The study compares different methods for summarization. Ours (K=100) is best for summaries under 50 words, while W Emb (BERT) is more effective for longer summaries. Combining methods like BERT with Ours may improve performance. The study compares different methods for summarization, with our method (K=100) being best for summaries under 50 words. Word embeddings from 10 randomly selected sentences in the validation set are visualized in a GloVe embedding space."
}