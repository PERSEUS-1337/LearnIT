{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need the ability to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the importance of providing online explanations during execution to reduce mental workload is emphasized. The challenge lies in the interdependence of different parts of an explanation. A general formulation for online explanation generation is presented, with three implementations satisfying various online properties. The method is based on a model reconciliation setting from prior work and is evaluated with human subjects in a planning competition domain and in simulation with different problems. As intelligent robots become more prevalent, human-AI interaction is crucial. Explanations from AI agents help maintain trust and shared awareness. Prior work on explanation generation often overlooks the recipient's perspective. To address this, explanations should consider discrepancies between human and AI models. Explanations from AI agents are essential for maintaining trust and shared awareness in human-AI interactions. To address discrepancies between human and AI models, explanations should be generated from the recipient's perspective, considering model differences and reconciling them to align with the human's expectations. The human generates expectations of the robot using a model. When the plans differ, the robot should provide an explanation to reconcile the models. Model reconciliation is the decision-making process in the presence of model differences. An issue is the human's mental workload in understanding explanations, especially complex ones. In this work, the focus is on providing online explanations that intertwine with plan execution to reduce mental workload. Generating online explanations spreads out information smoothly to avoid cognitive dissonance, especially for complex explanations. The online explanation generation process spreads out information smoothly to avoid cognitive dissonance. It illustrates this concept through a scenario between two friends planning a study session. Mark plans to break the review session into two 60-minute parts, have lunch in between, and go for a walk after. Emma prefers to keep the review in one session and have lunch afterwards. Mark doesn't reveal his plan to Emma but suggests going to lunch after studying for 60 minutes, as he needs energy. He also refrains from mentioning the walk he needs, to avoid Emma suggesting he takes a walk alone while she continues studying. Mark explains to Emma that he needs energy to continue, making lunch the best option for both. He gradually reveals his reasoning to maintain his plan, ensuring it is acceptable and understandable to Emma. The key is to explain minimally and only when necessary, spreading out the information throughout the plan execution to reduce mental workload. In this paper, a new method for explanation generation called online explanation is developed, intertwining explanation with plan execution. This approach considers the mental workload of the receiver by breaking explanations into multiple parts communicated at different times during plan execution. Three different approaches for online explanation generation are implemented, each focusing on different \"online\" properties. The new method of explanation, known as online explanation, breaks explanations into multiple parts communicated at different times during plan execution to reduce the mental workload of the recipient. Three approaches for online explanation generation are implemented, focusing on different properties such as matching the plan prefix and making the next action understandable to the human teammate. These approaches are evaluated with human subjects and in simulation. The new method of explanation, called online explanation, breaks explanations into parts communicated at different times during plan execution to reduce mental workload. AI agents need to be self-explanatory in their behaviors to operate as a teammate, and explainable AI is crucial for human-AI collaboration. Explainable AI is essential for human-AI collaboration as it helps improve human trust in AI agents and maintain shared situation awareness. The effectiveness of explainable agency is based on accurately modeling human perception of the AI agent, enabling the agent to generate legible motions. The effectiveness of an explainable AI agent is assessed based on its ability to model human perception accurately. This includes modeling other agents' perception of itself, allowing the agent to infer expectations. The agent can generate legible motions, explicable plans, or assistive actions by considering cost and explicability. The agent can also signal its intention before execution to improve human understanding and explain its behavior through generated explanations. The model can be used by an AI agent to signal its intention before execution and explain its behavior through generated explanations. Research has focused on generating the \"right\" explanations based on the recipient's perception model, but the mental workload required for understanding an explanation is often ignored. In prior work, the focus was on generating explanations based on the recipient's perception model. However, the mental workload required for understanding an explanation is often overlooked. The current work argues for online explanation generation, especially for complex explanations, intertwining it with plan execution. This approach aims to provide a minimal amount of information to explain part of the plan effectively. The current work focuses on online explanation generation, intertwining it with plan execution to provide a minimal amount of information to explain part of the plan effectively. The problem is closely associated with planning problems, defined as a tuple (F, A, I, G) using PDDL, where F is the set of predicates, A is the set of actions, I is the initial state, and G is the goal state. The problem is defined as a tuple (F, A, I, G) using PDDL, where F is the set of predicates, A is the set of actions, I is the initial state, and G is the goal state. Actions have preconditions, add and delete effects. The robot's plan to be explained is required to be optimal according to M R, considering rational agents and human's model M H. Model reconciliation occurs when the robot's behavior matches the human's expectation. The model reconciliation setting involves bringing two models, M H and M R, close enough by updating M H so that the robot's plan becomes fully explainable in the human's model. A mapping function in BID6 converts a planning problem into a set of features that specify the problem in the feature space. The explanation generation problem involves reconciling two models, M H and M R, by updating M H so that the robot's plan becomes fully explainable in the human's model. A mapping function in BID6 converts a planning problem into a set of features that specify the problem in the feature space. Explanation generation involves reconciling two models, M H and M R, by updating M H to make the cost difference between human's expected plan and robot's plan smaller. A complete explanation is one that satisfies cost(\u03c0 * DISPLAYFORM3) and contains the minimum number of unit feature changes. Online explanation generation addresses the mental workload requirement for understanding explanations. Online explanation generation introduces a method to provide minimal information during plan execution to explain the part of the plan that is of interest and not explainable. It involves generating a set of sub-explanations at specific steps in the plan to match the human's expectation with the robot's actions. Online explanation generation involves creating sub-explanations during plan execution to match human expectations with robot actions. Three approaches are discussed: OEG with Plan Prefix matching, OEG with Next Action matching, and OEG with any prefix matching. The planning process must consider how to generate sub-explanations for an online explanation. The planning process for online explanation generation involves generating sub-explanations to match human expectations with robot actions. The challenge lies in ensuring that model changes do not render a mismatch in previously reconciled plan prefixes. The planning process for online explanation generation involves generating sub-explanations to match human expectations with robot actions. The challenge is to ensure that model changes do not disrupt previously reconciled plan prefixes. To address this, a search process from M R to M H is conducted to find the largest set of model changes that would not alter plan prefixes after further sub-explanations. This process is illustrated in FIG1 and involves recursively creating OEG-PP sets of subexplanations. The search process illustrated in FIG1 involves generating OEG-PP sets of subexplanations to match human expectations with robot actions. The process recursively creates sub-explanations by starting from the robot model and stopping when plan prefixes match with the updated human model, similar to MME BID6. Our approach in matching plan prefixes with the updated human model differs from previous methods. While our process is more similar to MME BID6, we run the matching process multiple times, which allows us to outperform MCE and MME in terms of computation. The dotted line represents the maximum state space model modification in the robot model to reconcile the two models up to the current plan execution. Our approach beats MCE and MME in computation by considering only a small set of changes. The dotted line represents the maximum state space model modification in the robot model to reconcile the two models up to the current plan execution. The largest set of model changes to M R is found, ensuring compatibility for future steps. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. To search for e k, a recursive model reconciliation procedure is used on the model space, starting with finding the difference between M DISPLAYFORM1 and M R. This process modifies M R with respect to M H to find the largest set of model changes that can satisfy constraints introduced in Eq. The algorithm for model space OEG, presented in Algorithm 1, finds e k given E k\u22121 through a recursive model reconciliation procedure. It starts by finding the difference between M DISPLAYFORM1 and M R, modifying M R with respect to M H to satisfy constraints. The goal is to ensure the robot and human plans match, relaxing the plan prefix condition to reconcile between M R and M H. The goal of explanation generation is to ensure that the robot and human plans match, relaxing the plan prefix condition. The robot only needs to reconcile between M R and M H at the very next action in the plan, regardless of earlier actions. This approach considers the human's limited cognitive memory span and focuses on explaining differences between the most recent human plan and the robot's plan. The agent in DISPLAYFORM4 focuses on explaining the immediate next action that differs between the most recent human plan and the robot's plan, without comparing the entire plan prefix. The search process starts from M H \\M H for computational efficiency, maintaining the match between plan prefixes. The OEG search process focuses on explaining the immediate next action that differs between human and robot plans, starting from M R. The algorithm combines search from M H and M R for better performance, reconciling plans through model space search. The OEG-PP approach focuses on reconciling human and robot plans by combining search from M H and M R. It relaxes the assumption of a single right plan and aims to match human optimal plans with the robot's plan prefix efficiently. Instead of generating all human optimal plans, a compilation approach is implemented to reduce computational cost. The OEG-AP goal is to match human optimal plans with the robot's plan prefix efficiently. A compilation approach is used to ensure that a plan prefix in the robot's plan is also a prefix in the human's model. If the cost of the human's optimal plan in the new domain model matches the cost before compilation, an optimal plan in the human's model that matches the prefix exists. Otherwise, an explanation is needed. The key is to ensure that a plan prefix is always satisfied in the compiled model by adding predicates to actions. Recursive model reconciliation is used to search for differences between models. The process involves adding predicates to actions to satisfy a plan prefix, using recursive model reconciliation to search for differences between models. The agent checks for a human optimal plan that matches the robot's plan up to the next action, continuing until an optimal human plan aligns with the robot's plan. The process involves identifying new sub-explanations through model space search until an optimal human plan matches the robot's plan. Evaluation was done with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach across different domains. The study evaluated an approach for online explanation generation in comparison to the Minimally Complete Explanation (MCE) BID6 approach. It was tested on ten problems in rover and barman domains. The differences between M H and M R were assessed by randomly removing preconditions. The hypothesis was that online explanation generation would reduce mental workload and improve task performance, which was confirmed in a human subject study on a modified rover domain. The study evaluated online explanation generation in a rover domain where the robot explores Mars, takes samples, and communicates results. The rover must calibrate its camera and have space to store samples. It can only store one sample at a time and must drop the current sample to take another. The robot also serves drinks in a barman domain. The rover in the rover domain can only store one sample at a time and must drop the current sample before taking another. In the barman domain, the robot serves drinks using drink dispensers, glasses, and a shaker. Simulation results compare different approaches for explanations in both domains. The simulation results compare minimally complete explanations (MCE) with OEG-PP, OEG-NA, and OEG-AP approaches for problems in the rover and barman domains. OEG focuses on generating minimal information at each time step, leading to more shared features in total for OEG-PP and OEG-NA compared to MCE. The comparison between OEG-PP, OEG-NA, and OEG-AP approaches in the rover domain shows that OEG-PP and OEG-NA share more features in total than MCE. The reason for this lies in the dependence between features and planner behavior. OEG-AP considers all optimal plans, showing an advantage over MCE and OEG-PP. However, there is still a distance between the robot's and human's plans in terms of plan action distance. OEG-NA only considers the immediate next action, contributing to this distance. In comparing OEG-NA and OEG-AP approaches with MCE and OEG-PP, there is a remaining distance between the robot's plan and the human's plan in terms of plan action distance. OEG-NA only considers the immediate next action, while OEG-AP considers all optimal human plans, leading to potential differences between the robot's and human's plans. The plan distance gradually decreases during execution in OEG approaches, potentially reducing the human's mental workload. In our implementation, model updates are sorted based on feature size, with backtracking performed if consistency check fails. This search process capitalizes on the fact that later information may not affect previous explanations. A human study compared three online explanation generation approaches for effectiveness. The study compared three online explanation generation approaches for effectiveness in a human study using Amazon Mechanical Turk with 3D simulation. Explanations were provided in plain English language, and rover actions were depicted using GIFs. Each subject had a 30-minute limit to finish the task. The experiment involved using Amazon Mechanical Turk with 3D simulation to test MCE-R. Subjects acted as rover commanders on Mars, observing the rover's actions and determining their validity with explanations provided by OEG approaches or MCEs. Each subject could only perform the task once to avoid influence between runs. In an experiment using Amazon Mechanical Turk, subjects act as rover commanders on Mars, observing the rover's actions and determining their validity with explanations provided by OEG approaches or MCEs. To create cognitive demand, spatial puzzles were added as a secondary task. Hidden information introduced differences between M H and M R in model reconciliation. The experiment involved subjects acting as rover commanders on Mars, with hidden information causing differences between M H and M R in model reconciliation. The robot shared all information at the beginning in the MCE setting, while in MCE-R, information was randomly broken down. The robot used different approaches for online explanation generation in the OEG setting, intertwining explanation communication with plan execution. In the experiment, subjects acted as rover commanders on Mars with hidden information causing differences between MCE and MCE-R settings. The robot used various approaches for online explanation generation in the OEG setting, intertwining explanation communication with plan execution. The subjects were asked to determine the sense of the robot's actions and evaluate the efficiency of different explanation approaches using the NASA Task Load Index (TLX) questionnaire. The study evaluated the efficiency of different explanation approaches using the NASA Task Load Index (TLX) questionnaire, a subjective workload assessment tool. NASA TLX measures mental workload through variables like mental demand, physical demand, temporal demand, performance, effort, and frustration. The experiment did not include questions related to physical demand. The study assessed mental workload using the NASA Task Load Index questionnaire, measuring aspects like mental demand, physical demand, temporal demand, performance, effort, and frustration. The experiment excluded questions related to physical demand and recruited 150 subjects on MTurk, with valid responses from 94 participants. The study examined human subjects' understanding of the robot's plan based on different explanations. A total of 94 valid responses were collected from subjects aged 18 to 70, with 29.8% being female. The distance between the human's expected plan and the robot's plan was calculated to measure understanding, with lower values indicating closer alignment. Averaged results were calculated across five different settings. The study analyzed human subjects' understanding of the robot's plan through different explanations. Results showed that OEG approaches reduced human mental workload better than MCE approaches, as evidenced by improved performance in NASA TLX measures. OEG approaches also increased temporal demand during plan execution. FIG6 displays objective performance measures and subjective results from the human study across 5 TLX categories. The OEG approaches showed better performance in NASA TLX measures, with lower questionable actions and higher accuracy compared to MCEs. OEG-AP had the least questionable actions and highest accuracy. Results also indicated a statistical significance in mental load based on subjective measures. The OEG approaches, particularly OEG-AP, showed better performance with lower questionable actions and higher accuracy compared to MCEs. Results indicated a statistical significance in mental load based on subjective measures, with OEG approaches having a lower overall time taken to accomplish tasks. In this paper, a novel approach for explanation generation is introduced to reduce mental workload during human-robot interaction. The approach breaks down complex explanations into smaller parts and conveys them in an online fashion. Three different approaches are provided, focusing on providing correct and easily understandable explanations. The average time taken for task completion for different categories is compared, with no statistically significant difference observed. The key idea is to break down complex explanations into smaller parts and convey them online, intertwined with plan execution. Three approaches were provided, each focusing on a different aspect of explanation generation. Evaluation using simulation and human subjects showed improved task performance and reduced mental workload, a step towards achieving explainable AI."
}