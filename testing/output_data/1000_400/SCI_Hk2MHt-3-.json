{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters while improving performance. This arrangement also introduces an additional form of regularization. The branches are tightly coupled by averaging their log-probabilities, promoting the learning of better representations. This branched architecture, termed \"coupled ensembles,\" is applicable to various neural network architectures. The branched architecture of \"coupled ensembles\" tightly couples branches by averaging log-probabilities, leading to better representations in neural networks. This approach, applicable to various architectures, achieved lower error rates on CIFAR-10, CIFAR-100, and SVHN tasks compared to independently trained branches. With a parameter budget of 25M, error rates were 2.92%, 15.68%, and 1.50% respectively. The design of early convolutional architectures involved choices of hyper-parameters such as filter size and number of filters. VGGNet introduced a template with fixed filter size of 3x3 and N feature maps, down-sampling by maxpooling or strided convolutions, and doubling feature maps after each down-sampling. State-of-the-art models like ResNet and DenseNet extended this template with skip-connections between non-contiguous layers. This work adds another element to the template. The proposed template extends the design of early convolutional architectures by introducing \"coupled ensembling,\" where the network is decomposed into branches similar to complete CNNs. This approach achieves comparable performance to state-of-the-art models with fewer parameters. The study shows that splitting parameters among branches is more effective than having them in a single branch, as seen in current networks. Different ways to combine activations of parallel branches are also compared. In this paper, the authors introduce the concept of coupled ensembles to improve convolutional networks' performance on CIFAR and SVHN datasets with a reduced parameter count. They show that splitting parameters among branches is more effective than having them in a single branch. By combining activations of parallel branches using an arithmetic mean of individual logprobabilities, they achieve significant performance improvements. Further ensembling of coupled ensembles leads to additional enhancements. The paper is organized into sections discussing related work, introducing coupled ensembles, evaluating the proposed approach, and comparing it with the state of the art. The paper introduces coupled ensembles to enhance convolutional networks' performance on CIFAR and SVHN datasets by reducing parameter count. The proposed network architecture is similar to Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network but differs in training a single model with branches and a fixed parameter budget. Further ensembling of coupled ensembles leads to additional improvements. The Multi-Column Deep Neural Network (MCDNN) BID3 is a type of ensemble network with element blocks. The coupled ensemble networks differ in training a single model with branches and a fixed parameter budget, combining branch activations, and using the same input for all branches. Multi-branch architectures have been successful in various vision applications. The Multi-Column Deep Neural Network (MCDNN) is an ensemble network with element blocks that differ in training a single model with branches, combining branch activations, and using the same input for all branches. Recently, modifications using \"grouped convolutions\" have been proposed for these architectures to enhance spatial and depth-wise feature extraction. The Multi-Column Deep Neural Network (MCDNN) utilizes element blocks with modifications like \"grouped convolutions\" for enhanced feature extraction. In contrast, a generic modification at the global model level proposes a template with replicated \"element blocks\" forming the final model. Shake-Shake regularization BID5 improves performance but requires more epochs and is batch size dependent. Our method proposes a generic rearrangement of architecture parameters, avoiding local modifications like in BID26. It leads to efficient parameter usage and does not introduce additional choices. Additionally, ensembling neural networks is a reliable technique for improving model performance. Our method proposes a generic rearrangement of architecture parameters for efficient parameter usage without introducing additional choices. Ensembling neural networks is a reliable technique for improving model performance by combining outputs from multiple trainings of the same architecture. Our model architecture consists of parallel branches trained jointly, not as an ensemble of independent networks. The proposed model architecture consists of parallel branches trained jointly, similar to ResNet and ResNeXt, leading to improved performance. Ensembling can still be applied for fusion of independently trained models, resulting in significant performance gains. Snapshot ensembles were used in BID10 and BID16 for training checkpoints instead of fully converged models. The proposed model architecture includes parallel branches for increased performance. Ensembling can be applied to independently trained models for performance improvement. Snapshot ensembles used in BID10 and BID16 involve training checkpoints instead of fully converged models. The approach aims to maintain model size while enhancing performance or achieving the same performance with a smaller model size. Our approach involves using parallel branches in the model architecture to improve performance. Each branch utilizes an element block like DenseNet-BC or ResNet with pre-activation. The branches are combined using a fuse layer, where we experiment with different operations. In our approach, we use DenseNet-BC and ResNet with pre-activation as element blocks in parallel branches of the model architecture. The fuse layer combines these branches by averaging their individual log probabilities over target classes. This setup is applied to a classification task where samples are associated with one class from a finite set. The neural network models output a score vector of the same dimension as the target classes, typically followed by a fully connected (FC) layer and SoftMax (SM) layer for probability generation. Neural network models for image classification utilize a fully connected (FC) layer followed by a SoftMax (SM) layer to produce a probability distribution over target classes. The differences among architectures lie in the setup before the FC layer, but all result in an \"element block\" that takes an image as input and outputs a vector of N values. The differences in neural network models for image classification lie in the setup before the last FC layer. Fusion in ensemble models involves computing individual predictions separately for each model instance and averaging them, which is equivalent to a \"super-network\" with parallel branches. Supernetworks are not commonly implemented due to memory constraints, but the AVG layer operation can be implemented separately. The model consists of parallel branches that produce score vectors for target categories. These vectors are fused through a \"fuse layer\" during training to create a single prediction. Three options are explored for combining score vectors during training and inference: Activation (FC) average, where the output of FC layers is averaged. The model utilizes parallel branches to generate score vectors for target categories, which are fused through a \"fuse layer\" during training to produce a single prediction. Three methods are explored for combining score vectors during training and inference: Activation (FC) average, Probability (LSM) average, and Log Likelihood (LL) average. This approach leads to improved performance with a lower parameter count in all experiments. The FC layer activations involve averaging log probabilities to combine branch score vectors, leading to improved performance with fewer parameters. The parameter vector W of the composite branched model is the concatenation of parameter vectors W e of the element blocks. The architecture is evaluated on CIFAR-10, CIFAR-100, and SVHN datasets. The proposed architecture is evaluated on CIFAR-10, CIFAR-100, and SVHN datasets, consisting of various training and test images categorized into different classes. Hyperparameters are set according to the original descriptions, with input images normalized and standard data augmentation used during training. During training on CIFAR datasets, standard data augmentation is used, including random horizontal flips and random crops. For SVHN, no data augmentation is applied, but a dropout ratio of 0.2 is used for DenseNet. Testing is conducted after normalizing the input in the same manner as during training. Error rates are presented as percentages averaged over the last 10 epochs. DenseNet-BC's PyTorch implementation is utilized, and execution times are measured using a single NVIDIA 1080Ti GPU with optimal micro-batch 2. The experiments in sections 4.3 and 4.4 were conducted on the CIFAR-100 dataset using DenseNet-BC with specific parameters. The proposed branched architecture is compared to an ensemble of independent models, showing error rates for both cases. The proposed branched architecture is compared to an ensemble of independent models on the CIFAR-100 dataset using DenseNet-BC. Results show that the jointly trained branched configuration has a lower test error compared to averaging predictions from independent models. Additionally, the error from the multi-branch model is considerably lower than a single branch model with a similar number of parameters. The proposed branched architecture outperforms an ensemble of independent models on the CIFAR-100 dataset using DenseNet-BC. Results indicate that the multi-branch model has significantly lower error compared to a single branch model with similar parameters. The efficiency of arranging parameters into parallel branches is highlighted, showing improved performance as the number of branches increase. In Section 4.5, the performance of a proposed branched model with different \"fuse layer\" combinations is compared. Experiments evaluate training and prediction fusion after different layers in a branched model with e = 4. Results are presented in Table 1, showing the top-1 error rate on the CIFAR-100 test set for various fusion combinations versus a single branch model. Table 1 compares the performance of a branched model with different \"fuse layer\" combinations, evaluating training and prediction fusion after different layers. The top-1 error rate on the CIFAR-100 test set is shown for various fusion combinations versus a single branch model. The performance of \"fuse layer\" choices during inference is compared in Table 1, showing models with different training methods. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with significantly more parameters. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with significantly more parameters. Coupled ensembles with LSM fusion result in lower error rates for \"element blocks\" trained jointly, indicating better representations and complementary feature learning. Averaging log probabilities helps update all branches consistently, providing a stronger gradient signal during training. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with more parameters. Averaging log probabilities helps update all branches consistently, providing a stronger gradient signal during training. Ensemble combinations except Avg. FC training outperform single branch networks. Using 4 branches with a parameter budget of 3.2M reduces error rate to 17.61. Training with Avg. FC shows individual branches do not perform well, but Avg. FC training with Avg. SM prediction works better. The best single branch model has an error rate of 20.01, but using 4 branches reduces it to 17.61. Training with Avg. FC shows that individual branches do not perform well, but Avg. FC training with Avg. SM prediction works better. The Avg. FC prediction often performs significantly better than Avg. SM prediction due to the spread of information at the FC layer. All experiments use Avg. LSM for the training \"fuse layer\" in branched models. In this section, the optimal number of branches e for a given model parameter budget is investigated using DenseNet-BC on CIFAR-100. The results suggest that the number of instances e depends on the network architecture, parameter budget, and dataset. Further experiments with larger models are presented in table 3. The optimal number of instances e in DenseNet-BC may vary depending on network architecture, parameter budget, and dataset. Larger models were tested, and results are shown in table 3. Model configurations were selected just below the target parameters for fair comparison. DenseNet-BC parameter counts are quantified based on depth L and growth rate k, which is critical for moderate size models like the 800K one targeted here. Some models have slightly more parameters for possible interpolation and accurate comparisons. In DenseNet-BC models, the optimal number of branches is e = 3, L = 70, k = 9 for the 800K parameter model. Using 2 to 4 branches improves performance significantly over the single branch case, with a decrease in error rate from 22.87 to 21.10. However, using 6 or 8 branches performs worse due to thin element blocks. The DenseNet-BC model performance is improved with 2 to 4 branches compared to a single branch, with an error rate decrease from 22.87 to 21.10. However, using 6 or 8 branches performs worse due to thin element blocks. The model's performance is robust to variations in parameters L, k, and e. The increased performance comes at the cost of longer training and prediction times. The performance gain of DenseNet-BC model with 2 to 4 branches is notable, but using 6 or 8 branches leads to decreased performance due to thin element blocks. The model's robustness to parameter variations in L, k, and e is highlighted, with increased performance at the expense of longer training and prediction times. Coupled ensembles were evaluated against existing models, including DenseNet-BC and ResNet BID8 with pre-activation as element blocks, showcasing their performance in comparison to state-of-the-art models. The current state of the art models and performance of coupled ensembles are shown in TAB3. Results in the table are based on single model predictions, with ensembles coupled as a single global model. Coupled ensembles with ResNet pre-act as element block and e = 2, 4 outperform single branch models. For DenseNet-BC, 6 different network sizes were considered, ranging from 0.8M to 25.6M parameters. BID11 presents results for extreme cases, with depth L and growth rate k values chosen accordingly. Our experiments with DenseNet-BC architecture involved 6 different network sizes, ranging from 0.8M to 25.6M parameters. We found that the trade-off between depth L and growth rate k is not critical for a given parameter budget. Additionally, we experimented with single-branch and multi-branch versions of the model, with varying numbers of branches. For DenseNet-BC architecture, experiments were conducted with 6 network sizes and single-branch/multi-branch models. Error rates were compared with BID11, showing better performance with coupled ensemble models. Larger models had error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. The coupled ensemble approach with larger DenseNet-BC models outperforms other state-of-the-art implementations, with error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. The approach is limited by GPU memory size and training time, with models not exceeding 25M parameters. The coupled ensemble approach is limited by GPU memory size and training time, with models not exceeding 25M parameters. The classical ensembling approach based on independent trainings was used to improve performance further. SGDR with snapshots BID16 showed significant improvement from 1 to 3 models but not much from 3 to 16 models. Multiple times the same training is costly for large models, so ensembling was done instead. The coupled ensemble approach, limited by GPU memory and training time, shows significant improvement with SGDR snapshots BID16 from 1 to 3 models but not much from 3 to 16 models. Ensembling four large coupled models resulted in a significant gain by fusing two models, with little improvement from further fusion. These ensembles outperform all state-of-the-art implementations. The error rates between single and multi-branch models are highlighted, with a branched model with 13M parameters having an error rate of 16.24 for CIFAR-100. The proposed approach involves replacing a single deep convolutional network with \"element blocks\" that resemble standalone CNN models. These blocks are coupled via a \"fuse layer\" where their intermediate score vectors are averaged during training. The ensembles of coupled ensemble networks outperform all state-of-the-art implementations, with a branched model with 13M parameters achieving an error rate of 16.24 for CIFAR-100. The proposed approach involves replacing a single deep convolutional network with \"element blocks\" that resemble standalone CNN models. These blocks are coupled via a \"fuse layer\" where their intermediate score vectors are averaged during training, leading to a significant performance improvement over a single branch configuration. This improvement comes at the cost of a small increase in training and prediction times. The approach also results in better individual \"element block\" performance compared to when they are trained independently. The proposed approach involves using \"element blocks\" in place of a single deep convolutional network, leading to improved performance. However, this comes with increased training and prediction times due to sequential processing of branches. To address this, data parallelism can be extended to the branches or spread across multiple GPUs. The proposed approach involves using \"element blocks\" instead of a single deep convolutional network for improved performance. Data parallelism can be extended to branches or spread across multiple GPUs to reduce training and prediction times. Preliminary experiments on ImageNet show that coupled ensembles have lower errors with the same parameter budget compared to single branch models. Future experiments will expand on these findings. The proposed approach involves using \"element blocks\" instead of a single deep convolutional network for improved performance. Figure 3 illustrates the structure of test and train networks, showing how an averaging layer can be placed in different locations. The e model instances do not need to share the same architecture, and element blocks are reused from other groups for efficiency and meaningful comparisons. Each e branch is defined by a parameter vector W e with the same parameters. The global network in the LL layer uses \"element blocks\" from other groups for efficiency and meaningful comparisons. Each branch is defined by a parameter vector W e, and the global network is defined by a concatenation of all W e vectors. Training and prediction are done in coupled or separate modes using dedicated scripts to split the parameter vector. The same global parameter vector W is used for all train and test versions, allowing for different training conditions. The global network in the LL layer utilizes \"element blocks\" from other groups for efficiency and meaningful comparisons. Training and prediction can be done in coupled or separate modes using dedicated scripts to split the parameter vector. The overall network architecture is determined by global hyper-parameters specifying train versus test mode, number of branches, and placement of the AVG layer. Larger models may face challenges in training. The global network architecture in the LL layer uses \"element blocks\" from different groups for efficiency and comparisons. For larger models, data batches are split into micro-batches to handle training challenges. Gradient accumulation and averaging over micro-batches are used to approximate processing data as a single batch, with BatchNorm layer adjustments for normalization. To handle training challenges in larger models, data batches are split into micro-batches. Gradient accumulation and averaging over micro-batches approximate processing data as a single batch, with adjustments in the BatchNorm layer for normalization. The micro-batch \"trick\" helps adjust memory needs for optimal throughput. In the single branch case, memory needs depend on network depth and batch size. Using micro-batches adjusts memory usage without affecting performance. Multi-branch models require more memory only if branch width is reduced. Hyper-parameter search showed that maintaining branch width was optimal. The multi-branch version does not require more memory if the branches' width is kept constant. Memory is only needed if the branches' width is reduced. Hyper-parameter search experiments showed that reducing both width and depth was the best option. Training with 25M parameters was done within 11GB memory using different micro-batch sizes for single-branch and multi-branch versions. Splitting the network over two GPU boards did not significantly increase speed or improve performance. Splitting the network over two GPU boards does not significantly increase speed or improve performance. Comparing FC average and logsoftmax, using two branches provides a significant gain over a single-branch architecture of comparable size. The performance remains stable against variations in depth and growth rate. Using only two branches provides a significant gain over a single-branch architecture of comparable size. The performance remains stable against variations in depth and growth rate. Experimentation on a validation set suggests that the combination of (L = 82, k = 8, e = 3) should be the best, with a slight advantage seen in the combination of (L = 70, k = 9, e = 3). Comparisons with model architectures recovered using meta learning techniques are also discussed in the context of parameter usage and performance. The (L = 82, k = 8, e = 3) combination is predicted to be the best on the test set, with a slight advantage seen in the (L = 70, k = 9, e = 3) combination. Comparisons with model architectures recovered using meta learning techniques are discussed in terms of parameter usage and performance. Various sources of variation in performance measures are identified, including framework used for implementation, random seed for network initialization, and CuDNN non-determinism during training. Sources of variation in performance measures include the underlying framework used for implementation (Torch7 and PyTorch), random seed for network initialization, CuDNN non-determinism during training, fluctuations in batch normalization, and the choice of model instance from training epochs. Sources of variation in performance measures include fluctuations in batch normalization during training. The choice of model instance from training epochs can also impact evaluation measures due to different random initializations leading to different local minima. Despite efforts to control for variations in batch normalization and epoch sampling, the dispersion of evaluation measures can still be influenced by random initialization. While properly designed neural networks should yield similar performance across local minima, differences in measures below their dispersions can complicate method comparisons. Statistical significance tests may not be reliable in this context, as even models with the same seed can exhibit significant differences. Experiments show that differences in measures below their dispersions can complicate method comparisons, making statistical significance tests unreliable. Even models with the same seed can exhibit significant differences, especially in the case of DenseNet-BC with L = 100, k = 12 on CIFAR 100. The dispersion estimation was done for a moderate scale model due to limitations in conducting a large number of trials for larger models. The results of experiments on DenseNet-BC with L = 100, k = 12 on CIFAR 100 are presented in table 8. Different combinations were tried using Torch7 and PyTorch with the same seed or different seeds. Performance measures included error rates at the last epoch, average error rates of the last 10 epochs, and error rates of the model with the lowest error rate. The results show minimum, median, maximum, and mean values with standard deviation over 10 measures from 10 identical runs. The study compared Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results showed no significant difference between the two implementations or using the same seed versus different seeds. Reproducing exact results was not possible due to observed dispersion. The study compared Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results showed no significant difference between the two implementations or using the same seed versus different seeds. Reproducing exact results was not possible due to observed dispersion. There was no significant difference between means over 10 measures computed on the single last epoch and the last 10 epochs. The standard deviation of measures computed on 10 runs was consistently smaller when computed on the last 10 epochs, reducing fluctuations. The study compared Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results showed no significant difference between the two implementations or using the same seed versus different seeds. Reproducing exact results was not possible due to observed dispersion. The mean of the measures computed on the 10 runs is significantly lower when the measure is taken at the best epoch than when they are computed either on the single last epoch or on the last 10 epochs. A method for ensuring the best reproducibility and fairest comparisons is proposed. The study compared Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results showed no significant difference between the two implementations or using the same seed versus different seeds. Reproducing exact results was not possible due to observed dispersion. A method for ensuring the best reproducibility and fairest comparisons is proposed, addressing issues with error rate selection during training. In experiments comparing Torch7 and PyTorch implementations of DenseNet-BC on CIFAR 100, it was found that using the error rate at the last 10 epochs for performance estimation is preferred for single experiments. The standard deviation is smaller for the last 10 epochs compared to the last iteration. The number of epochs used for error rate calculation does not significantly impact performance. For CIFAR experiments, the average error rate of the last 10 epochs was used for robustness. In CIFAR experiments, the average error rate of the last 10 epochs is used for robustness and conservatism. For SVHN experiments, the last 4 iterations are used. The principle of using the average error rate from the last epochs for more robust results is likely to be general. Comparisons between single-branch and multi-branch architectures have shown a clear advantage for multi-branch networks under a constant parameter budget. In this study, comparisons between single-branch and multi-branch architectures have shown a clear advantage for multi-branch networks under a constant parameter budget. However, the training time of multi-branch networks is currently longer than single-branch networks. Ways to reduce training time include reducing iterations, parameter count, or increasing width while reducing depth. The study compares single-branch and multi-branch architectures, showing the advantage of multi-branch networks with a constant parameter budget. Ways to reduce training time include decreasing iterations, parameter count, or adjusting width and depth. Results for different options are presented for CIFAR 10 and 100 datasets, with statistics on 5 runs for each configuration. The multi-branch baseline has a longer training time compared to the single-branch baseline. The study compares single-branch and multi-branch architectures, highlighting the advantage of multi-branch networks with a constant parameter budget. Different options are presented for reducing training time, including decreasing iterations, parameter count, or adjusting width and depth. Results for various configurations on CIFAR 10 and 100 datasets show that all options perform better than the single-branch baseline, with some options slightly underperforming compared to the full multi-branch baseline. In this section, the performance of single branch models and coupled ensembles is compared in a low training data scenario. Different configurations of DenseNet-BC models are evaluated on STL-10 and CIFAR-100 datasets, showing that even with reduced parameters and training time, they outperform the single-branch baseline. In a comparison of single branch models and coupled ensembles in a low training data scenario, results show that coupled ensembles outperform single branch models for a fixed parameter budget. Experiments were conducted on STL-10 and CIFAR-100 datasets, with data augmentation involving random flips and crops. Preliminary experiments on ILSVRC2012 were also done, using images of size 256x256 due to constraints. The study compared single-branch models with multi-branch coupled ensembles using DenseNet models on ILSVRC2012 dataset. Due to constraints, experiments were done on 256x256 images with limited data augmentation. Results in table 11 show that the coupled ensemble approach with two branches significantly outperformed the single-branch model. The study compared single-branch models with multi-branch coupled ensembles using DenseNet models on ILSVRC2012 dataset. Experiments with full-sized images and increased data augmentation are ongoing, with updated results to be included in the manuscript after the deadline. Table 11 displays current results, showing that the coupled ensemble approach with two branches yields a significant improvement over the baseline, even with a constant training time budget."
}