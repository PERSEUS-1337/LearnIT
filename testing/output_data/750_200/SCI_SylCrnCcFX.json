{
    "title": "SylCrnCcFX",
    "content": "Deep networks aim to understand complex mappings through locally linear behavior. Derivatives are used for sensitivity analysis and prediction explanation, but they are inherently unstable. A new learning problem is proposed to promote stable derivatives in deep networks with piecewise linear activation functions. The algorithm identifies regions where linear approximation is stable. Our algorithm focuses on networks with piecewise linear activation functions, using an inference step to identify stable linear approximation regions and an optimization step to expand them. A novel relaxation is proposed to scale the algorithm to realistic models, demonstrated on residual and recurrent networks with image and sequence datasets. Local linearization and derivatives play crucial roles in understanding complex mappings and model predictions. The curr_chunk discusses the importance of derivatives in deep learning models, particularly in sensitivity analysis and model explanations. It highlights the challenges of unstable derivatives in functions parameterized by deep learning models. The focus is on derivatives with respect to input coordinates rather than parameters. State-of-the-art deep learning models are referenced. The instability of derivatives in deep learning models, particularly due to over-parametrization, affects both function values and derivatives themselves. This instability leads to a lack of robustness in first-order approximations used for explanations. Gradient stability is emphasized as different from adversarial examples, with the former needing to remain approximately invariant within a local region. Stable gradient, whether large or small, should remain approximately invariant within a local region. Adversarial examples are small perturbations that change predicted output. Robust estimation techniques focus on stable function values rather than stable gradients but can indirectly impact gradient stability. In this paper, the focus is on deep networks with piecewise linear activations to ensure gradient stability. The special structure of these networks allows for inferring lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable. The special case of p = 2 is investigated due to its analytical solution, enabling the formulation of a regularization technique. In this study, deep networks with piecewise linear activations are focused on to ensure gradient stability. The special case of p = 2 is examined for its analytical solution, allowing for the formulation of a regularization problem. The learning problem is relaxed similar to support vector machines, and a novel perturbation algorithm is proposed for collecting exact gradients in piecewise linear networks. The study focuses on deep networks with piecewise linear activations to ensure gradient stability. A novel perturbation algorithm is proposed for collecting exact gradients in these networks, even when GPU memory constraints arise. Inference algorithms are developed for identifying input features in fully-connected, residual, and recurrent networks on image and time-series datasets. The study focuses on deep networks with piecewise linear activations to ensure gradient stability. It introduces inference algorithms for identifying input regions in neural networks with piecewise linear activation functions. Novel learning and perturbation algorithms are proposed to expand regions of provably stable derivatives and scale computation to high dimensional data. Empirical evaluation is conducted on various types of networks, with a focus on ReLU BID15 and its variants. Neural networks with piecewise linear activation functions, like ReLU and its variants, are the focus of this paper. These networks, including FC, CNN, RNN, and ResNet, are inherently piecewise linear due to their activation functions. The proposed approach involves a mixed integer linear representation of piecewise linear networks. The paper focuses on piecewise linear networks, specifically using a mixed integer linear representation of activation patterns. The activation pattern encodes the active linear piece for each neuron, leading to a degeneration into a linear model. The feasible set in the input space ensures stable derivatives, even in cases where neighboring regions have the same linear coefficients. The activation pattern in piecewise linear networks encodes the active linear piece for each neuron, leading to a degeneration into a linear model. This induces a feasible set in the input space ensuring stable derivatives, even when neighboring regions have the same linear coefficients. Activation patterns have been studied in various contexts such as visualizing neurons, reachability of specific output values, and adversarial attacks. In contrast to quantifying the number of linear regions as a measure of complexity, the focus is on local linear regions and expanding them via learning. The notion of stability considered differs from adversarial examples, with different methods employed. The focus is on local linear regions and expanding them via learning, with a notion of stability different from adversarial examples. Methods for finding exact adversarial examples are NP-complete and do not scale, while layer-wise relaxations of ReLU activations are more scalable but yield bounds instead of exact solutions. Defense methods are still intractable on ImageNet scale images, but the inference algorithm certifies the exact 2 margin around a point subject to its activation pattern. The proposed inference algorithm certifies the exact 2 margin around a point subject to its activation pattern by forwarding O(D) samples in parallel, scaling to ResNet on 299 \u00d7 299 \u00d7 3 dimensional images. The learning algorithm is based on the inference problem with 2 margins, reminiscent of SVM objective but differs in purpose. The approach maximizes the 2 margin of linear regions around each data point in an unsupervised manner, similar to transductive/semi-supervised SVM. The idea of margin is extended to nonlinear classifiers, while a smooth relaxation is developed for adversarial training. In contrast to previous methods, this approach focuses on developing a smooth relaxation of the margin and novel perturbation algorithms for gradient stability in complex models. The gradient is a key component for various explanation methods for deep models, such as gradient saliency map and its variants, which provide a gradient-based attribution of predictions to inputs for visualization. The curr_chunk discusses the development of gradient-based attribution methods for visualization, focusing on establishing robust derivatives in neural networks with ReLU activations. The approach aims to address the instability of gradient-based explanations and presents inference and learning algorithms for complex models. The curr_chunk introduces notation for a neural network with hidden layers and neurons, discussing the computation of activated neurons using transformation matrices and biases in an FC architecture with ReLU activations. The curr_chunk discusses the computation of ReLU activations and the linear transformation of the last hidden layer in a neural network. It also mentions using a nonlinearity like softmax for classification and a generic loss function to handle the nonlinear mechanism. The curr_chunk focuses on the piecewise linear property of neural networks and uses a generic loss function to handle the nonlinear mechanism. It introduces the activation pattern BID20 and defines an activation pattern as a set of indicators for neurons that specify functional constraints. The activation pattern BID20 is defined as a set of indicators for neurons that specify functional constraints. Each linear region of f \u03b8 is characterized as a convex polyhedron with linear constraints in the input space R D. The activation pattern BID20 defines constraints for neurons in a convex polyhedron with linear constraints in the input space R D. The p margin of x subject to its activation pattern is a lower bound with respect to a feasible activation pattern, ensuring a derivative specification. The convexity of S(x) is used to check the feasibility of a directional perturbation. The convexity of S(x) is utilized to verify the feasibility of a directional perturbation. Propositions 4 and 5 provide conditions for linear and 1-ball feasibility, respectively. Proposition 5 generalizes feasibility for an \u221e -ball, but in high dimensions, it becomes intractable due to the exponential number of extreme points. Instead, 1-ball feasibility is more manageable with a linear number of extreme points. Binary searches can be used to find certificates for directional perturbations and 1-balls, making certification efficient. The feasibility of x,1 is tractable due to convexity of S(x) and its certification is efficient by a binary search; x,2 can be certified analytically using the polyhedron structure of S(x). Proposition 6 states that x,2 is the minimum 2 distance between x and the union of hyperplanes, computed efficiently using forward passes. The number of linear regions in a function can be efficiently computed using forward passes. Certifying the number of complete linear regions among data points is proposed to capture the structure of the data manifold. This certification method is more effective than counting linear regions on the whole space. Certifying the number of complete linear regions among data points is proposed to capture the structure of the data manifold efficiently. The number of complete linear regions of f \u03b8 among D x is upperbounded by the number of different activation patterns and lower-bounded by the number of different Jacobians. The focus is on maximizing the 2 margin\u02c6 x,2 in this section. In this section, methods are discussed to maximize the 2 margin\u02c6 x,2 by formulating a regularization problem. A hinge-based relaxation approach is used to alleviate optimization challenges. If the condition in Lemma 8 is not met, an upper bound is still valid. The regularization problem is formulated to maximize the 2 margin by relaxing constraints and using a hinge-based approach. If Lemma 8 condition is not met, an upper bound is still valid due to a smaller feasible set. The relaxed problem involves a maximum aggregation of TSVM losses with a hyper-parameter C. The relaxed regularization problem aims to maximize the 2 margin by aggregating TSVM losses with a hyper-parameter C. BID9 also computes the p margin to maximize the margin in a linear model scenario. Visualization is done on a 2D binary classification dataset using different loss functions. In a toy 2D binary classification dataset, a 4-layer fully connected network is trained with binary cross-entropy loss, distance regularization, and relaxed regularization. The distance regularization enlarges linear regions around training points, while the relaxed regularization generalizes the property to the whole space, resulting in a smoother prediction boundary with a special central region allowing gradients to change directions. Visualization of piecewise linear regions and prediction heatmaps is shown in FIG3. The relaxed regularization in a 4-layer fully connected network enlarges linear regions around training points, resulting in a smoother prediction boundary with a special central region allowing gradients to change directions smoothly. The generalized loss objective for learning RObust Local Linearity (ROLL) involves a set of neurons with top \u03b3 percent relaxed loss. The generalized loss objective for learning RObust Local Linearity (ROLL) involves a set of neurons with top \u03b3 percent relaxed loss denoted as \u00ce(x, \u03b3). When \u03b3 = 100, the nonlinear sorting step disappears, leading to a simple additive structure that stabilizes training, is easy to parallelize, and allows for an approximate learning algorithm. Setting \u03b3 = 100 can also induce a strong synergy effect between gradient norms in different layers. The paragraph discusses a parallel algorithm developed to address heavy computation demands in the ROLL loss objective for learning RObust Local Linearity. By exploiting the functional structure of the network, the algorithm avoids back-propagation and constructs a linear network g \u03b8 based on the same parameters as f \u03b8. The paragraph discusses constructing a linear network g \u03b8 identical to f \u03b8 in S(x) using fixed linear activation functions. The derivatives of all neurons to an input axis can be computed by forwarding two samples. The complexity of the approach is analyzed assuming no overhead for parallel computation. The perturbation algorithm for computing gradients of neurons in a batch of inputs takes 2M operations, while back-propagation takes DISPLAYFORM0. Despite the parallelizable computation of \u2207 x z i j, it is challenging to compute loss for large networks in high dimensions. An unbiased estimator of the ROLL loss in Eq. FORMULA17 is proposed when \u00ce(x, \u03b3) = I. An unbiased estimator of the ROLL loss in Eq. FORMULA17 is proposed for efficient computation using GPU memory, by sampling input axes to approximate the gradient norms. The proposed algorithms provide an efficient way to compute partial derivatives with respect to D axes in deep learning models with affine transformations and piecewise linear activation functions. They do not immediately generalize to nonlinearity of maxout/max-pooling, but suggest using average-pooling or convolution with large strides instead. In the Appendix E, it is suggested to use average-pooling or convolution with large strides instead of maxpooling for efficiency. The comparison between 'ROLL' and 'vanilla' models is done on various scenarios, with evaluation measures including accuracy, number of complete linear regions, and margins of linear regions. Experiments were conducted on a single GPU with 12G memory. Parameter analysis was conducted on the MNIST dataset using a 4-layer FC model with ReLU activations. The evaluation measures included accuracy, number of complete linear regions, and margins of linear regions. Experiments were performed on a 12G memory GPU, with a 55,000/5,000/10,000 split for training/validation/testing. The models with the largest median among validation data were reported. The implementation details of a 4-layer FC model with ReLU activations are provided in Appendix G. Two models with the largest median among validation data are reported, achieving similar accuracy to the baseline model. Tuned models have specific parameters, and the ROLL loss shows significantly larger margins compared to the vanilla loss. By sacrificing 1% accuracy, even larger margins can be achieved. The Spearman's rank correlation between\u02c6 x,1 and\u02c6 x,2 among testing data is at least 0.98 for all cases. Our approach shows lower #CLR than the baseline model, indicating larger linear regions. Points within the same linear region in the ROLL model with ACC= 98% share the same label, while visually similar digits may be in the same region in another ROLL model. Parameter analysis in Figure 2 explores ACC and P 50 of\u02c6 x,2 under different C, \u03bb, and \u03b3 with fixed hyper-parameters. In Figure 2, a parameter analysis is conducted on ACC and P 50 of\u02c6 x,2 under varying C, \u03bb, and \u03b3 values. Higher C and \u03bb result in decreased accuracy with an increased margin. Higher \u03b3 values indicate less sensitivity to hyper-parameters C and \u03bb. The efficiency of the proposed method is validated by measuring the running time for mini-batch gradient descent steps. Different loss functions are compared for performance. The approximate ROLL loss is compared to the full loss in terms of accuracy and margins, showing comparable results. The perturbation algorithm is about 12 times faster than back-propagation, with minimal computational overhead. Our perturbation algorithm achieves about 12 times empirical speed-up compared to back-propagation, with minimal computational overhead. Training RNNs for speaker identification on a Japanese Vowel dataset with variable sequence length and multiple channels. The Vowel dataset from the UCI machine learning repository BID11 has variable sequence length, 12 channels, and 9 classes. The network is implemented using the state-of-the-art scaled Cayley orthogonal RNN (scoRNN) with LeakyReLU activation to prevent gradient issues. Results are reported in TAB3, showing larger margins on testing data compared to previous approaches. The results in TAB3 show that our approach, with 1% lower ACC, produces a model with significantly larger margins on testing data compared to the vanilla loss. The Spearman's rank correlation between\u02c6 x,1 and\u02c6 x,2 is 0.98. Sensitivity analysis on derivatives identifies stability bounds at each timestamp and channel. Visualization in FIG4 compares the vanilla and our ROLL model with 98% ACC. The ROLL regularization model shows consistently larger stability bounds compared to the vanilla model. Experiments were conducted on Caltech-256 dataset using a 18-layer ResNet with ROLL loss and 120 random samples per channel. Testing set consisted of 15 samples per class, with the remaining data used for training. The implementation details involve randomly selecting samples for validation and testing sets. Due to high input dimensionality, a sample-based approach is used to evaluate gradient stability. This approach aims to reveal stability across different linear regions for the ground-truth label. The gradient stability of prediction models is evaluated using labeled data, focusing on expected distortion and maximum distortion within specific regions. The gradient distortion is defined and visualized using examples from Caltech-256 dataset. Visualization of examples in Caltech-256 dataset showing P50 and P75 of maximum gradient distortions on ROLL model. Adversarial gradient maximized over \u221e-norm ball with radius 8/256. Genetic algorithm BID33 used for optimization due to gradient complexity. Implementation details in Appendix J. The genetic algorithm BID33 is used for black-box optimization in the study. 8000 samples are used to approximate the expected 1 distortion, with a \u221e -ball radius set to 8/256. Results in Table 4 show that the ROLL loss yields more stable gradients and slightly better precisions compared to the vanilla loss. Only 40 and 42 out of 1024 examined examples show a change in prediction labels for gradient-distorted images in the ROLL and vanilla model. The ROLL loss produces more stable gradients and slightly better precisions compared to the vanilla loss. Only 40 and 42 out of 1024 examples change prediction labels for gradient-distorted images in the ROLL and vanilla model, respectively. The ROLL loss aims to create locally transparent neural networks with faithful derivatives. The central focus is on constructing locally transparent neural networks with stable derivatives, using a margin principle similar to SVM. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. The feasible set of activation patterns is equivalent to satisfying linear constraints. The feasible set of activation patterns is equivalent to satisfying linear constraints. If x satisfies constraints before layer i > 1 and previous layers follow fixed activation indicators, each can be rewritten. The proof follows by induction, showing directional feasibility for a point x, feasible set S(x), and unit vector \u2206x. The feasible set of activation patterns is equivalent to satisfying linear constraints. If x satisfies constraints before layer i > 1 and previous layers follow fixed activation indicators, each can be rewritten. The proof follows by induction, showing directional feasibility for a point x, feasible set S(x), and unit vector \u2206x. If x + \u00af \u2206x \u2208 S(x), then f \u03b8 is linear in {x + \u2206x : 0 \u2264 \u2264 \u00af}. The proof involves constructing a neural network feasible in Eq. (5) with the same loss as the optimal model in Eq. (4). The minimum 2 distance between x and the hyperplanes is the maximizing distance that satisfies certain conditions. This is also optimal for Eq. (4). The proof involves constructing a neural network feasible in Eq. (5) with the same loss as the optimal model in Eq. (4). The network g \u03b8 is constructed with the same weights and biases as f \u03b8 but with a linear activation function. Each layer in g \u03b8 is represented as: DISPLAYFORM4. The activation function \u00f4 is fixed for x, applying linearity to\u1e91. To compute derivatives with respect to input axis k, feed zero vector 0 to g \u03b8 for\u1e91 i j (0) and unit vector e k for\u1e91 i j (e k ). Derivatives of neurons with respect to x k can be computed as\u015d, allowing for computation of all neuron derivatives with 2 forward passes. The proposed approach allows for the computation of neuron derivatives with 2 forward passes, scaling by computing all gradients of z. The complexity analysis assumes no overhead for parallel computation and a unit operation for batch matrix multiplication. Our perturbation algorithm uses two forward passes to obtain activation patterns and gradients, totaling 2M operations. In contrast, back-propagation requires sequential computation for each neuron, taking Mi=1 2iN i operations in total. The chain-rule of Jacobian can be exploited for dynamic programming. The chain-rule of Jacobian can be used for dynamic programming to compute gradients efficiently. The Jacobian for each layer can be represented by previous layers' Jacobians and weights. This approach is more efficient than sequential back-propagation. The dynamic programming approach efficiently computes gradients using the chain-rule of Jacobian. It is effective for fully connected networks but inefficient for convolutional layers due to the expensive linear transformation representation. An introductory guide is provided for deriving methods for maxout/max-pooling nonlinearity, highlighting the feasibility but not recommending its use due to new linear constraints induced by max-pooling neurons. The text discusses the inefficiency of using max-pooling nonlinearity in a piecewise linear network and suggests using convolution with large strides or average-pooling instead. It also mentions that once an activation pattern is fixed, the network reverts to a linear model. The text explains that when an activation pattern is fixed, the network becomes a linear model again as the nonlinearity in max-pooling disappears. This activation pattern creates a feasible set in the input space where derivatives remain stable. The feasible set can be represented as a convex polyhedron with linear constraints. The text discusses how FORMULA0 is equivalent to DISPLAYFORM3 with linear constraints, creating a convex polyhedron feasible set. The FC model has 4 hidden layers with 100 neurons each, input dimension D is 2, output dimension L is 1, and loss function is sigmoid cross entropy. The model is trained for 5000 epochs with Adam optimizer, selecting based on training loss. C is fixed at 5, and \u03bb is increased from 10^-2 to 10^2. The model is trained for 5000 epochs with Adam optimizer, selecting based on training loss. C is fixed at 5, and \u03bb is increased from 10^-2 to 10^2. The tuned \u03bb in both cases are 1. Data is normalized with \u00b5 = 0.1307 and \u03c3 = 0.3081. The margin\u02c6 x,p is computed in the normalized data, and the scaled margin \u03c3\u02c6 x,p is reported in the table. The FC model consists of 4 hidden layers with 300 neurons each, using ReLU activation function. Training involves 20 epochs with stochastic gradient descent and Nesterov momentum. Parameters are tuned through grid search on \u03bb, C, \u03b3. The FC model has 4 hidden layers with 300 neurons each and uses ReLU activation. Training includes 20 epochs with stochastic gradient descent and Nesterov momentum. Parameters are tuned through grid search on \u03bb, C, \u03b3, with specific ranges for each. The data is not normalized, and the representation is learned with a single layer scoRNN. The representation is learned with a single layer scoRNN using LeakyReLU activation functions. The hidden neurons dimension is set to 512, with a cross-entropy loss function and AMSGrad optimizer. Tuning involves grid search on \u03bb, C, \u03b3, with a learning rate of 0.001 and batch size of 32 sequences. The models achieve similar testing accuracy compared to the baseline model. The models are trained on normalized images with a bijective mapping between normalized distance and original space distance. Pre-trained ResNet-18 is downloaded and its architecture is revised. Max-pooling after the first convolutional layer is replaced with average-pooling. We revise the model architecture by replacing max-pooling with average-pooling after the first convolutional layer and enlarging the receptive field of the last pooling layer to output 512 dimensions. The model is trained with stochastic gradient descent with Nesterov momentum for 20 epochs. The model is trained with stochastic gradient descent with Nesterov momentum for 20 epochs, using a 3-dimensional image dataset with higher dimensions. The initial learning rate is 0.005, adjusted to 0.0005 after 10 epochs, with a momentum of 0.5 and a batch size of 32. Tuning involves fixing C = 8, using 18 samples for learning, and adjusting \u03bb until the model's validation accuracy is significantly worse than the vanilla model. After tuning \u03bb to find the highest plausible value of 0.001 and fixing C at 8, a model is trained with 360 random samples for approximate learning. A genetic algorithm (GA) BID33 with 4800 populations P and 30 epochs is implemented, where samples are evaluated based on their gradient distance from the target x. In a genetic algorithm (GA) with 4800 populations P and 30 epochs, samples are sorted based on their gradient distance from the target x. The top 25% samples are kept, while the remaining 75% are replaced with a random linear combination. Updated samples are projected to ensure feasibility, and the one achieving the maximum distance is returned. Mutation was not implemented due to computational reasons. In a genetic algorithm (GA) with 4800 populations P and 30 epochs, samples are sorted based on their gradient distance from the target x. The top 25% samples are kept, while the remaining 75% are replaced with a random linear combination. Updated samples are projected to ensure feasibility, and the one achieving the maximum distance is returned. Mutation was not implemented due to computational reasons. Finally, the sample in P that achieves the maximum 1 distance is returned. The crossover operator in GA is analogous to a gradient step where the direction is determined by other samples and the step size is determined randomly. Visualizations include the original image, original gradient, adversarial gradient, image of adv. gradient, original int. gradient, and adversarial int. gradient. The integrated gradient attribution BID28 is visualized on the original image and the 'image of adv. gradient'. The process involves aggregating derivatives, taking absolute values, normalizing, and clipping values above 1. No optimization is done to find the image with maximum distorted integrated gradient. The integrated gradient attribution involves aggregating derivatives, taking absolute values, normalizing, and clipping values above 1 to visualize as a gray-scaled image. The original integrated gradient paper visualizes the element-wise product between the grayscaled integrated gradient and the original image, but for highlighting differences, only the integrated gradient is visualized. The examples in the Caltech-256 dataset show visually indistinguishable images with different gradient distortions. The visualization in Figure 5 and 6 displays the P 25, P 50, P 75, and P 100 percentile of maximum gradient distortions on the ROLL model. The values slightly differ from Table 4 due to interpolation methods. The figures in Figure 5 and 6 show examples from the Caltech-256 dataset with varying gradient distortions. Figure 5 displays P 25 and P 50 percentiles, while Figure 6 shows P 75 and P 100 percentiles of maximum gradient distortions on the ROLL model. The maximum gradient distortions for the vanilla model are 893.3 for 'Projector' in Figure 5g, 1547.1 for 'Bear' in Figure 6g, and 5473.5 for 'Rainbow' in Figure 6q. The ROLL model shows maximum gradient distortions of 1367.9 for 'Bear' and 3882.8 for 'Rainbow', while the vanilla model has distortions of 1547.1 for 'Bear' and 5473.5 for 'Rainbow'."
}