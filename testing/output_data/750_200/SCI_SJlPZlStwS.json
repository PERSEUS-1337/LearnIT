{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. To address this, a unified framework called EdgeGANRob has been proposed, which focuses on extracting shape/structure features from images to improve CNN robustness. This framework uses a generative adversarial network (GAN) to reconstruct images with texture information. EdgeGANRob utilizes a GAN to reconstruct images with texture information, improving CNN robustness. A robust edge detection approach called Robust Canny is proposed to reduce sensitivity to adversarial perturbation. Comparing EdgeGANRob with EdgeNetRob, it is found that EdgeNetRob boosts model robustness but at the cost of clean model accuracy, while EdgeGANRob improves clean model accuracy without losing robustness. EdgeGANRob enhances clean model accuracy without compromising robustness, unlike EdgeNetRob which sacrifices accuracy for robustness. Extensive experiments demonstrate EdgeGANRob's resilience across various learning tasks and settings. Convolutional neural networks (CNNs) have achieved state-of-the-art performance in many learning tasks but are vulnerable to adversarial examples and data poisoning attacks. Recent studies show that CNNs tend to have reduced generalization accuracy in the presence of such attacks. Recent studies have shown that CNNs are vulnerable to attacks where training data is manipulated to reduce the model's generalization accuracy. CNNs tend to learn surface statistical regularities instead of high-level abstractions, leading to a lack of generalization to superficial pattern transformations. Improving the general robustness of DNNs under these settings remains a challenge. Researchers are exploring the underlying causes of CNN vulnerability to enhance their robustness. Recent studies explore the vulnerability of CNNs and suggest training classifiers on \"robust features\" to improve robustness. Human recognition relies on global object shapes, while CNNs are biased towards local patterns. Texture-shape cue conflicts have been created to study this bias. CNNs are biased towards local features, leading to vulnerability to adversarial examples and backdoor attacks. Previous research shows that human recognition relies on global object shapes, while CNNs focus on textures. Texture-shape cue conflicts highlight this bias. The previous research highlights the bias of CNNs towards local features, while human recognition relies on global object shapes. To improve CNN robustness, the focus should shift towards global shape structures, specifically edges. The paper proposes using edges as a shape representation to improve CNN robustness. A new approach called EdgeGANRob leverages structural information in images to enhance CNNs' resilience to adversarial attacks and distribution shifting. The unified framework EdgeGANRob improves CNN robustness by leveraging structural information in images through EdgeNetRob, which extracts shape information by detecting edges. This approach eliminates texture bias and enhances resilience to attacks, although challenges remain with vulnerable edge detection algorithms. Robust Canny algorithm enhances EdgeNetRob's robustness against attacks, outperforming adversarial retraining. EdgeNetRob improves CNN robustness but reduces clean accuracy due to missing texture/color information, leading to the development of EdgeGANRob with a generative model to refill the gaps. The development of EdgeGANRob aims to improve CNNs' robustness by refilling texture/color information based on edge images before classification. The unified framework explicitly extracts edge/structure information and reconstructs original images using GANs, enhancing robustness against multiple tasks simultaneously. More visualization results can be found on the anonymous website: https://sites.google.com/view/edgenetrob. The development of EdgeGANRob enhances CNNs' robustness by extracting edge/structure information from input images and reconstructing them using GANs. A robust edge detection approach, Robust Canny, is proposed to reduce sensitivity to adversarial attacks. Additionally, the effectiveness of the inpainting GAN in EdgeGANRob is demonstrated through evaluation on EdgeNetRob. EdgeNetRob improves its simplified backbone procedure by learning tasks directly on robust edge features. Thorough evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Defense methods against adversarial examples often lack robustness against adaptive attacks, with gradient obfuscation being a common pitfall. State-of-the-art defense methods rely on adversarial training. State-of-the-art defense methods rely on adversarial training and should be evaluated against customized white-box attacks. Carlini et al. (2019) suggested evaluating defense methods against strong adaptive attacks. Distribution shifting, more common in real-world applications, can affect CNNs' learning of statistical cues. Wang et al. (2019a) proposed a method to robustify CNNs by penalizing predictive power. Wang et al. (2019a) proposes a method to robustify CNNs by penalizing the predictive power of local representations and mitigating the tendency to fit superficial statistical cues. Hendrycks and Dietterich (2019) introduce benchmark datasets for evaluating model robustness under common perturbations, while backdoor attacks involve injecting patterns into training data to manipulate model predictions. Backdoor attacks involve injecting patterns into training data to manipulate model predictions. Tran et al. (2018) proposed a procedure to detect poisoned training data using robust statistics. Neuron pruning is suggested as an approach to protect models from backdoor attacks. Recent research has shown a connection between recognition robustness and robust visual features, with CNNs relying more on textures than global shape structure for image recognition. Recent research has shown a connection between recognition robustness and robust visual features in image recognition. CNNs rely more on textures than global shape structure, while humans rely more on shape structure than detailed texture. Adversarially robust models tend to capture global structure of objects. There are non-robust features in natural images that are highly predictive but not interpretable by humans. CNNs can obtain robustness by learning from images containing only robust features, such as edges. In this work, a new classification pipeline called EdgeGANRob is proposed, focusing on using edge features as robust features for image recognition. The method involves extracting edge/structure features, reconstructing images with a GAN, and then classifying the generated images. The process includes a simplified backbone procedure named EdgeNetRob, Robust Canny, and inpainting GAN. EdgeNetRob is a simplified backbone of EdgeGANRob, consisting of two stages: extracting edge maps using an edge detection method and training an image classifier on these maps. The pipeline aims to solve a problem involving the data distribution and loss function. EdgeNetRob forces CNN decisions based on edges, reducing sensitivity to textures. Despite simplicity, it degrades CNN performance on clean data. This led to the development of EdgeGANRob, which refills texture/colors of edge images using a generative model. EdgeGANRob embeds a generative model to refill texture/colors of edge images, improving clean accuracy. The robustness of classification systems relying on edges is crucial, motivating the proposal of a robust edge detection algorithm called Robust Canny. Most neural network-based edge detectors are non-robust, highlighting the need for a more resilient approach. The proposal of a robust edge detection algorithm named Robust Canny addresses the non-robust nature of neural network-based edge detectors. Traditional methods like Canny are inherently robust but can become noisy when facing adversarial perturbations. The improvement suggested involves truncating noisy pixels in the intermediate output of Canny. The proposal introduces Robust Canny, an enhanced version of the Canny edge detector to address noise from adversarial perturbations. The 6 stages of Robust Canny include noise reduction, gradient computation using the Sobel operator, noise masking with thresholding, and non-maximum suppression. The Robust Canny edge detector includes noise reduction, gradient computation, noise masking with thresholding, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The Robust Canny edge detector includes noise reduction, gradient computation, noise masking with thresholding, non-maximum suppression, double thresholding, and edge tracking by hysteresis. Edge pixels are detected by finding strong pixels or weak pixels connected to other strong pixels. A noise masking stage is added after computing image gradients to reduce perturbation noise. Gradient magnitudes lower than a threshold \u03b1 are set to zero to mitigate noise, and a truncation operation helps reduce adversarial noise on the gradient map without affecting the final edge maps. The parameters of the Canny edge detector, such as the standard deviation of the Gaussian filter \u03c3 and thresholds \u03b8 l , \u03b8 h, impact its robustness. Larger \u03c3 and higher thresholds result in better robustness but may lead to blurrier images and loss of useful information in the output edges. Careful selection of parameters is crucial for obtaining a robust edge detector. To obtain a robust edge detector, careful parameter selection is crucial. Training a Generative Adversarial Network (GAN) in EdgeGANRob involves following the common setup of pix2pix and using an objective function with L adv and L F M terms. More details on parameter choices are provided in the experiment section. In the second stage, the trained GAN is fine-tuned along with the classifier to achieve high accuracy over generated RGB images by minimizing the classification loss. The function min where L cls represents the classification loss of generated images by inpainting GAN. The method improves robustness under adversarial attack, distribution shifting, and backdoor attack by focusing on generating realistic images first. EdgeGANRob enhances robustness by using edges that are resistant to small adversarial perturbations. EdgeGANRob focuses on leveraging edge features to improve model generalization ability, making it less sensitive to distribution changes during testing. This approach enhances robustness under adversarial attacks, distribution shifting, and backdoor attacks by focusing on shape structures that are resistant to small adversarial perturbations. The proposed method EdgeNetRob aims to remove malicious patterns from training data to prevent backdoor attacks. It is shown to have unique advantages in certain settings and is considered a robust recognition method. The inpainting GAN is evaluated for its unique advantages in certain settings and as a robust recognition method. The experiments include testing robustness against adversarial attacks, performance over distribution shifting, and resistance to backdoor attacks on Fashion MNIST and CelebA datasets. The evaluation on CelebA focuses on gender classification due to the limitations of MNIST and CIFAR-10 datasets. On CelebA, gender classification is evaluated using the same network architecture for both the method and vanilla classifier. The method is tested with \u221e adversarial perturbation constraints within the input range [0, 1]. MNIST and CIFAR-10 datasets were not chosen due to their limitations in providing informative benchmarks for the study. Our methods are evaluated using standard perturbation budgets on Fashion MNIST and CelebA datasets. The evaluation includes robustness testing against white-box attacks using the BPDA attack. The study evaluates the robustness of edge detection methods against white-box attacks using the BPDA attack. Three methods are compared: RCF, traditional Canny, and Robust Canny, with results reported for Fashion MNIST in Table 1. The study compares the robustness of edge detection methods against white-box attacks using the BPDA attack. Results for Fashion MNIST show that using edges generated by RCF is not robust. Adversarial training is highlighted as an effective defense method, achieving strong robustness to attacks. Adversarial training is effective in defending against white-box attacks, with EdgeNetRob and EdgeGANRob showing higher clean accuracy compared to the vanilla baseline model. EdgeGANRob outperforms EdgeNetRob on the CelebA dataset, emphasizing the importance of using GANs on complex datasets. In terms of adversarial robustness, EdgeNetRob performs well under strong adaptive attacks. In terms of adversarial robustness, EdgeNetRob and EdgeGANRob show robustness comparable to adversarial training baselines, with EdgeNetRob being time-efficient due to not using adversarial training. Generalization ability is tested under distribution shifting using perturbed Fashion MNIST and CelebA datasets. In experiments testing models under perturbed Fashion MNIST and CelebA datasets with various transformations, including greyscale, negative color, random kernel, and radial kernel. Comparison with state-of-the-art method PAR shows visualization results in Appendix D and overall results in Table 3. The study introduces EdgeNetRob and EdgeGANRob, which improve accuracy on different patterns and grayscale images. These methods leverage edge features to enhance CNN generalization and defend against backdoor attacks by embedding invisible watermark patterns. Our method can defend against backdoor attacks by embedding invisible watermark patterns. Qualitative results are shown for Fashion MNIST and CelebA datasets. Different attack and target pairs are chosen for each dataset, with varying poisoning ratios. Our method is compared with a baseline method proposed in previous research. Our method successfully defends against backdoor attacks by embedding invisible watermark patterns. Results on Fashion MNIST and CelebA datasets show high poisoning accuracy compared to the baseline method. The Spectral Signature method struggles with invisible watermark patterns, while EdgeNetRob and EdgeGANRob consistently maintain low poisoning accuracy. Edge detection algorithms can remove the effect of invisible watermark patterns, with EdgeGANRob achieving better clean accuracy than EdgeNetRob. A new method based on robust edge features improves general model robustness by combining a robust edge feature extractor with an inpainting GAN. Our method combines a robust edge feature extractor with a generative adversarial network to improve model robustness against adversarial attacks and distribution shifting. It also enhances robustness against backdoor attacks by utilizing shape information. Data pre-processing involves resizing CelebA images to 128 \u00d7 128 using bicubic interpolation. For data pre-processing, CelebA images are resized to 128 \u00d7 128 using bicubic interpolation and normalized to the range of [\u22121, 1]. Different models are used for Fashion-MNIST and CelebA datasets, trained with stochastic gradient descent. Adversarial attacks such as Projected Gradient Descent (PGD) and Carlini & Wagner \u221e attack (CW) are evaluated with varying steps and distances. For the CW attack, 10 and 40 steps PGD attacks are evaluated separately. Step sizes are set for different distances. 1,000 images are randomly sampled for CW attack evaluation. Robust Canny is used for assessing adversarial robustness with specific hyper-parameters for Fashion MNIST and CelebA datasets. The Robust Canny algorithm is used to assess adversarial robustness with specific hyper-parameters for Fashion MNIST and CelebA datasets. In a white-box attack scenario, gradients need to be backpropagated through non-differentiable transformations for constructing adversarial samples. The Backward Pass Differentiable Approximation (BPDA) technique can be used by attackers to replace non-differentiable transformations with differentiable approximations. In a white-box attack scenario, the Robust Canny algorithm is used to assess adversarial robustness with specific hyper-parameters for Fashion MNIST and CelebA datasets. Attackers can use the Backward Pass Differentiable Approximation (BPDA) technique to replace non-differentiable transformations with differentiable approximations, such as in constructing adversarial examples. To strengthen the attack, a differentiable approximation of the Robust Canny algorithm is found by breaking the transformation into two stages: C1 (steps 1-3) and C2 (steps 4-6). Note that C2 involves a non-differentiable operation where the output is a masked version of the input. The Robust Canny algorithm involves two stages: C1 (steps 1-3) and C2 (steps 4-6), with C2 being a non-differentiable operation where the output is a masked version of the input. To make R-Canny differentiable for BPDA, a constant mask assumption is made, only backpropagating gradients through C1 and not through the mask. Test accuracy changes are shown under radial and random mask transformations in Figure A. In Figure A, test accuracy changes are displayed under radial and random mask transformations with varying parameters. Additional visualization results for CelebA under distribution shifting are shown in Figure B and ??, while qualitative results of EdgeGANRob and EdgeNetRob for backdoor attacks on Fashion MNIST are presented in Figure D. EdgeNetRob can slightly remove poisoning patterns, with each generated image having distinct patterns."
}