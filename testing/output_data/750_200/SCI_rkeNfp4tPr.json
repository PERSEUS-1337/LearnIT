{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is commonly used in nonconvex optimization, especially for training deep neural networks. The addition of a momentum term biases parameter updates in the direction of the previous change, aiming to reduce convergence time. However, proving the effectiveness of momentum adjustments in stochastic and non-convex settings has been challenging. Empirically, stochastic momentum has shown benefits in training deep networks. Stochastic momentum improves deep network training by helping SGD escape saddle points faster and find second order stationary points more quickly. Theoretical justification for its use has been a significant open question. Stochastic momentum accelerates SGD to escape saddle points and find second order stationary points faster. The ideal momentum parameter should be close to 1, as supported by theoretical and empirical findings. Experimental results further confirm the effectiveness of SGD with stochastic momentum in nonconvex optimization and deep learning. Modern techniques in computer vision, speech recognition, natural language processing, and reinforcement learning use SGD with stochastic momentum for training machine learning models. The advantage of SGD with stochastic momentum has been widely observed, helping achieve faster convergence compared to standard SGD. Sutskever et al. (2013) found that using stochastic momentum in training deep neural nets leads to faster convergence compared to standard SGD. Momentum is now a key component in designing optimization algorithms for deep learning, including popular methods like Adam and AMSGrad. Despite its widespread use, the empirical improvements and guidelines for setting the momentum parameter remain unclear. In this paper, a theoretical analysis is provided for stochastic gradient descent (SGD) with momentum. The momentum parameter remains a key component in optimization algorithms for deep learning, with large values like \u03b2 = 0.9 often working well in practice. Algorithm 1, the default momentum method in popular software packages, involves updating the iterate based on the stochastic gradient and momentum parameters. SGD with stochastic momentum accelerates escape from saddle points compared to standard SGD. Stochastic heavy ball momentum maintains a weighted average of gradients and updates the iterate in the direction of momentum. This amplifies escape from saddle points. In this paper, the focus is on finding a second-order stationary point for smooth non-convex optimization using SGD with stochastic heavy ball momentum. The goal is to obtain a point where the gradient is small and the Hessian matrix is close to the identity matrix. The paper aims to find a second-order stationary point in nonconvex optimization by utilizing stochastic heavy ball momentum with SGD. The focus is on achieving an approximate second-order stationary point with Lipschitzness assumptions in gradients and Hessian. The paper aims to find a second-order stationary point in nonconvex optimization using stochastic heavy ball momentum with SGD. It focuses on achieving an approximate second-order stationary point with Lipschitzness assumptions in gradients and Hessian, following related works like Allen-Zhu & Li (2018) and Daneshmand et al. (2018). The stochastic momentum helps in escaping saddle points faster by amplifying the escape signal \u03b3. Under the Correlated Negative Curvature (CNC) assumption, if SGD with momentum satisfies properties like Almost Positively Aligned with Gradient (APAG) and Almost Positively Correlated with Gradient (APCG), it can find second-order stationary points in nonconvex optimization. The properties of momentum, such as Almost Positively Aligned with Gradient (APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature Exploitation (GrACE), can help escape saddle points faster in optimization and deep learning. A larger momentum parameter \u03b2 leads to faster escape from saddle points, with theoretical results showing that T = O((1 \u2212 \u03b2) log(1/(1 \u2212 \u03b2) ) \u221210 ) iterations are needed to reach a second-order stationary point. Alternatively, T = O((1 \u2212 \u03b2) log(1/(1 \u2212 \u03b2) ) \u22125 ) iterations can achieve an ( , \u221a ) second order stationary point. The benefits of stochastic momentum in optimization and deep learning include faster escape from saddle points, as explained by the properties of momentum such as APAG, APCG, and GrACE. The use of SGD with momentum enables faster training by avoiding saddle points in the loss landscape. The benefit of stochastic momentum is its ability to help parameters escape saddle points in optimization. Moving in the direction of the smallest eigenvector of the Hessian matrix guarantees a fast escape from these regions. Moving along the direction of the smallest eigenvector of the Hessian matrix ensures a fast escape from saddle points in optimization, but this 2nd-order method is costly. Gradient methods are typically preferred, with the assumption that stochastic gradients are strongly non-orthogonal to the direction of large negative curvature, driving updates away from saddle points. The study focuses on stochastic momentum, where the update direction must be strongly non-orthogonal to the direction of large negative curvature to escape saddle points. The analysis shows that updates escape saddle points due to this property, especially with momentum updates when \u03b2 is close to 1. The study discusses how momentum updates, particularly when \u03b2 is close to 1, can accelerate the escape process from saddle points by amplifying the effect in successive iterations. This is illustrated by the correlation between m t0 and the negative curvature direction v t0. Momentum can help speed up the escape process and improve overall performance. The study demonstrates that momentum can accelerate saddle-point escape by a factor of 1 \u2212 \u03b2. Empirical evidence shows the benefits of stochastic momentum in solving optimization tasks with significant saddle points. The study shows that momentum can speed up escaping saddle points by a factor of 1 \u2212 \u03b2. Problem (3) involves a non-convex optimization challenge with a saddle point represented by matrix H := diag([1, \u22120.1]) and stochastic gaussian perturbations. Initialization is w0 = 0 with step size \u03b7 = 5 \u00d7 10 \u22125. In the experiment, all algorithms are initialized at the same point and use the same step size. SGD and SGD with momentum start at the origin to escape saddle points before convergence. The second objective in the phase retrieval problem involves real values. In the experiment, algorithms start at the origin to escape saddle points before convergence. The second objective in the phase retrieval problem involves real values and aims to find an unknown vector with limited samples. Convergence is accelerated by larger choices of \u03b2 for both objectives. The heavy ball method accelerates convergence by using larger choices of \u03b2, allowing trajectories to escape saddle points more quickly. This empirical finding shows the speedup of stochastic momentum in phase retrieval. The heavy ball method, proposed by Polyak (1964), does not provide a convergence speedup over standard gradient descent in most cases. However, it can accelerate convergence in convex quadratic objectives. Specialized algorithms and simple GD/SGD variants are used to reach a second-order stationary point. Our work belongs to the category of specialized algorithms designed to exploit negative curvature and escape saddle points faster. Previous pioneer works in this category include Ge et al. (2015) and Jin et al. (2017), who showed that adding isotropic noise in each iteration helps gradient descent escape saddle points and reach a second-order stationary point. Phase retrieval is known to be nonconvex with the strict saddle property. Phase retrieval is nonconvex with the strict saddle property, where every local minimizer is global up to phase and each saddle exhibits negative curvature. Daneshmand et al. (2018) assume the Correlated Negative Curvature (CNC) for stochastic gradient to escape saddle points faster. Our work assumes CNC for stochastic momentum instead of stochastic gradient, ensuring faster escape from saddle points. We assume L-Lipschitz gradient and \u03c1-Lipschitz Hessian, with bounded noise and momentum. Comparisons with related works are provided in Appendix A. Our analysis of stochastic momentum relies on three properties, including Almost Positively Aligned with Gradient (APAG). We aim to demonstrate these properties empirically in standard problems of interest. SGD with stochastic momentum satisfies Almost Positively Aligned with Gradient (APAG) if the momentum term is not significantly misaligned with the gradient. The APAG property requires that the momentum term is not significantly misaligned with the gradient, ensuring progress in the algorithm when the gradient is large. APCG is a property related to momentum in optimization algorithms, where the momentum term should be positively correlated with the gradient. It measures the local curvature of the function with respect to the trajectory of the algorithm. Empirical evidence supports this property on natural problems. APCG is crucial in saddle regions with significant iterations and when the gradient is large. The experiments show that SGD with momentum exhibits APAG and APCG properties in saddle regions with significant iterations and large gradients. The value is mostly nonnegative, except during transitions, indicating positive correlation between momentum and gradient. SGD with momentum shows APAG and APCG properties in experiments, with expected values potentially being nonnegative for the phase retrieval problem. Analysis does not require APCG to hold in certain conditions. GrACE measures alignment between stochastic momentum and gradient, as well as curvature exploitation. The alignment between stochastic momentum and gradient, as well as curvature exploitation, are measured by GrACE. A small sum of terms allows bounding the function value of the next iterate. Experimental results show APAG and APCG properties with SGD momentum in solving specific problems. The analysis includes quantities related to APAG and APCG, gradient norm in solving problems using SGD with momentum, and a quantity regarding GrACE. The proof is structured into three cases, leading to a second-order stationary region. The algorithm analyzed is Algorithm 2, with a boosted step size. The analysis of Algorithm 2 in a second-order stationary region involves adjusting step size parameters, momentum, and period parameters. The algorithm shows progress in different cases, with a focus on boosting the step size. The algorithm in Algorithm 2 adjusts step size parameters, momentum, and period parameters to show progress in different cases, focusing on boosting the step size. The analysis proves that a second-order stationary point is reached with high probability, borrowing tools from previous studies but introducing novel momentum analysis. The stochastic momentum in SGD with momentum (Algorithm 2) satisfies CNC, APAG, APCG T thred, and GrACE properties, leading to reaching a second-order stationary point in ) iterations with high probability. Higher \u03b2 enables faster escape from saddle points. Higher \u03b2 enables faster escape from saddle points in stochastic momentum SGD. Constraints on \u03b2 are needed to prevent it from being too close to 1. The dependency on 1 - \u03b2 in T thred makes it smaller than previous results, demonstrating faster escape from saddle points with momentum. The dependency on 1 - \u03b2 in the result shows faster escape from saddle points with momentum. Algorithm 2 is proven to be better than CNC-SGD in the high momentum regime, indicating that higher momentum helps find a second order stationary point faster. Empirical findings suggest that the conditions for escaping saddle points are easily met in the phase retrieval problem. In the phase retrieval problem, conditions for escaping saddle points are easily met, with faster escape shown with higher momentum. The process of escaping saddle points by SGD with momentum is analyzed, showing that it takes at most T thred iterations to escape a region with a small gradient and large negative eigenvalue of the Hessian. The technique used involves proving by contradiction, assuming that the function value does not decrease by at least F thred in T thred iterations. Upper and lower bounds are established for the expected distance, showing that the function value must decrease by at least F thred in T thred iterations on expectation. The function value must decrease by at least F thred in T thred iterations on expectation, with larger momentum helping in escaping saddle points faster. Lemma 1 provides an upper bound of the expected distance, while the lower bound is obtained from the recursive dynamics. The lower bound of E[t0[wt0+Tthred\u2212wt02]] is obtained through the recursive dynamics of SGD with momentum. Lemma 2 defines a quadratic approximation for wt0+t\u2212wt0, using specific quantities qv,t\u22121, qm,t\u22121, qq,t\u22121, qw,t\u22121, q\u03be,t\u22121. Lemma 3 shows the dominant term in the lower bound critical for ensuring its validity. Lemma 3 demonstrates the critical dominant term in the lower bound, ensuring its validity in the context of SGD with momentum dynamics. The lower bound in (8) is shown to be monotone increasing with t and the momentum parameter \u03b2, growing exponentially in t. To prove the contradiction, it must be larger than the upper bound. Lemma 5 states that if SGD with momentum has the APCG property, then certain conditions are met. In this paper, three properties are identified that guarantee SGD with momentum reaches a second-order stationary point faster with a higher momentum. A greater momentum helps in escaping strict saddle points faster by enlarging the projection to an escape direction. However, ensuring that SGD with momentum possesses these properties is not clear, and further research is needed to identify conditions for guaranteeing these properties. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases. However, it may offer an accelerated rate in specific scenarios like convex quadratic objectives. This sheds light on the success of SGD with momentum in non-convex optimization and deep learning. In recent years, efforts have been made to analyze the heavy ball method for optimization problems beyond quadratic functions. A unified analysis of stochastic heavy ball momentum and Nesterov's momentum for non-convex objectives shows a convergence rate of O(1/ \u221a t), which is not superior to standard SGD. Variants of stochastic accelerated algorithms have also been proposed. The heavy ball method for optimization problems has been analyzed beyond quadratic functions, showing a convergence rate of O(1/ \u221a t) which is not better than standard SGD. Some variants of stochastic accelerated algorithms have been proposed, but they do not capture the stochastic heavy ball momentum used in practice. Kidambi et al. (2018) found that for specific problems, SGD with heavy ball momentum fails to achieve the best convergence rate compared to other algorithms. Some algorithms aim at reaching a second order stationary point by exploiting negative curvature explicitly. Specialized algorithms use a preconditioning matrix to update the gradient, helping to escape saddle points faster than standard SGD under certain conditions. The algorithm updates weights to escape saddle points faster than standard SGD. Average-SGD uses a suffix averaging scheme for updates. Iteration complexity results are compared with other works. SGD with heavy ball momentum is popular due to its effectiveness. The text discusses the effectiveness of the popular algorithm SGD with heavy ball momentum compared to other approaches. It focuses on the advantage of using stochastic heavy ball momentum and how it can improve performance in practice. The analysis framework is based on previous work, and the authors believe that modifying assumptions and algorithms can further demonstrate the benefits of stochastic momentum. Lemma 6, 7, and 8 discuss the advantages of stochastic momentum in improving performance compared to other algorithms. Lemma 7 shows that SGD with momentum decreases function value by a constant under certain conditions, while Lemma 8 bounds the increase of function value in the next iterate. Lemma 6 discusses the GrACE property in SGD with momentum. Lemma 7 provides a condition for the step size in SGD with momentum. Lemma 8 explores the advantages of the GrACE property in SGD with momentum. Lemma 8 states that if SGD with momentum has the GrACE property, then the update step can be represented as w t+1 = w t \u2212 \u03b7m t. The proof involves considering the update rule and the Lipschitzness of the Hessian. The proof involves bounding the expectation of a term using the Lipschitzness of the Hessian and the triangle inequality. The proof involves bounding the expectation of a term using the Lipschitzness of the Hessian and the triangle inequality. Denote t 0 any time such that (t 0 mod T thred ) = 0. Define a quadratic approximation at w t0, Q(w) := f(w t0) + w - w t0, \u2207f(w t0) + 1/2 (w - w t0) H(w - w t0). Then, w t0+t - w t0 = qv,t-1 + \u03b7qm,t-1 + \u03b7qq,t-1 + \u03b7qw,t-1 + \u03b7q\u03be,t-1. The proof involves bounding the expectation of a term using the Lipschitzness of the Hessian and the triangle inequality. Define a quadratic approximation at w t0, Q(w) := f(w t0) + w - w t0, \u2207f(w t0) + 1/2 (w - w t0) H(w - w t0). Recursively expanding the equation leads to completing the proof in Lemma 3. Lemma 5 states that if SGD with momentum has the APCG property, then certain inequalities hold. The proof involves various conditions and notations from previous Lemmas and Theorems. The constraints on parameter \u03b2 ensure it is not too close to 1, bounding its value. These constraints are used in proofs but are mostly artifacts of the analysis, not highly relevant dependencies. To prove Lemma 5, a series of lemmas with parameter choices from Table 3 are needed. Upper bounding E t0 [ q q,t\u22121 ] is done using Lemma 9 and conditions from Lemma 1 and Lemma 2. The proof involves triangle inequality and the assumption of L-Lipschitz. The text discusses upper bounds derived using the assumption of L-Lipschitz gradient and triangle inequality. It also analyzes \u03a0 t\u22121 j=s+1 G j 2 using specific notation. The text provides upper bounds by analyzing \u03a0 t\u22121 j=s+1 G j 2 with specific notation, using assumptions of L-Lipschitz gradient and triangle inequality. It discusses the choice of parameters like \u03bb and \u03b7 to bound terms in the equations. The text discusses the lower bounding of certain terms by utilizing specific coefficients and the tower rule, under the conditions outlined in Lemma 1 and Lemma 2. The text presents Lemmas 12 and 13, which lower bound certain terms using coefficients and the tower rule under the conditions of Lemma 1 and Lemma 2. Lemma 12 involves bounding E t0 [2\u03b7 q v,t\u22121 , q \u03be,t\u22121 ], while Lemma 13 deals with bounding E t0 [2\u03b7 q v,t\u22121 , q m,t\u22121 ] by defining a symmetric positive semidefinite matrix B. The text discusses the properties of symmetric positive semidefinite matrices and their relation to the matrix product U. It also presents a proof involving the APCG property and the use of Lemmas 1 and 2. The strategy for the proof involves a contradiction assumption. The proof involves showing that the function value must decrease by at least F thred in T thred iterations on expectation, using a contradiction assumption. By leveraging negative curvature, upper and lower bounds are established for the expected distance, leading to the conclusion that the function value decreases as required. The proof involves showing that the function value must decrease by at least F thred in T thred iterations on expectation, using a contradiction assumption. By leveraging negative curvature, upper and lower bounds are established for the expected distance, leading to the conclusion that the function value decreases as required. The contradiction is obtained by showing that \u03b7E t0 [ q v,T thred \u22121 , q m,T thred \u22121 ] \u2265 0 and \u03b7E t0 [ q v,T thred \u22121 , q \u03be,T thred \u22121 ] = 0, with the constraint of \u03b7. The proof involves showing that the function value must decrease by at least F thred in T thred iterations on expectation, using a contradiction assumption. By leveraging negative curvature, upper and lower bounds are established for the expected distance, leading to the conclusion that the function value decreases as required. The contradiction is obtained by showing that \u03b7E t0 [ q v,T thred \u22121 , q m,T thred \u22121 ] \u2265 0 and \u03b7E t0 [ q v,T thred \u22121 , q \u03be,T thred \u22121 ] = 0, with the constraint of \u03b7. By choosing T thred large enough, we can guarantee the required inequality holds. With high probability, if SGD with momentum satisfies certain properties, it will reach a second-order stationary point in a certain number of iterations. The proof involves showing that the function value decreases as required in a certain number of iterations. The proof involves showing that uniformly sampling a w from a specific set guarantees a second-order stationary point with high probability. The process involves selecting w from a sequence of values and ensuring that a certain condition does not occur. The proof involves selecting a w from a sequence of values to guarantee a second-order stationary point with high probability. The conditions for this selection involve bounding the difference in function values based on gradient norms and curvatures. The proof involves selecting a w from a sequence of values to guarantee a second-order stationary point with high probability. The conditions for this selection involve bounding the difference in function values based on gradient norms and curvatures. In the other condition, the expected increase of function value due to a large step size is limited when w is a second-order stationary point. By satisfying these conditions, Lemma 15 can be applied to finish the proof of the theorem. The proof involves selecting a w to guarantee a second-order stationary point. By satisfying conditions related to gradient norms and curvatures, Lemma 15 can be applied to complete the proof. The parameters value on Table 3 are used to set T in Algorithm 2, which outperforms previous results by not depending on the variance of stochastic gradient. Algorithm 2 outperforms previous results by not depending on the variance of stochastic gradient, showing that a higher momentum can help find a second-order stationary point faster."
}