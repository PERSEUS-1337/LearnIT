{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology creates positive definite kernels from dissimilarity measures on structured inputs like time series or discrete structures. It utilizes Random Features to build a kernel from a random feature map specified by the distance measure, using a finite number of random objects for each instance. The D2KE methodology proposes using a finite number of random objects to create a random feature embedding for each instance. It shows better generalizability than universal Nearest-Neighbor estimates and extends Random Features methods to complex structured inputs. Our proposed framework for classification experiments compares favorably to existing distance-based learning methods in terms of testing accuracy and computational time across various domains such as time series, strings, and histograms for texts and images. It is often easier to specify a dissimilarity function between instances than to construct a feature representation, especially with structured inputs like real-valued time series, strings, histograms, and graphs. Standard machine learning methods are designed for vector representations, but there has been less focus on distance-based methods for complex structured inputs like strings, histograms, and graphs. Various dissimilarity measures exist for structured inputs, such as Dynamic Time Warping for time series and Edit Distance for strings. Nearest-Neighbor Estimation (NNE) is a common distance-based method for predicting outcomes using an average of nearest neighbors in the input space. However, it can be unreliable with high variance when neighbors are far apart, especially in cases of large intrinsic dimensions. Research is ongoing to address this issue. Research has focused on developing global distance-based machine learning methods to address issues with Nearest-Neighbor Estimation when neighbors are far apart. This includes using similarity functions and kernel methods like Support Vector Machines or kernel ridge regression. Many similarity measures do not provide a positive-definite kernel, leading to non-convex optimization problems in machine learning methods like Support Vector Machines. Efforts have been made to estimate a positive-definite Gram matrix that approximates the similarity matrix. This can be achieved by clipping, flipping, or shifting eigenvalues of the similarity matrix, or by explicitly learning a positive-definite approximation. Efforts have been made to estimate a positive-definite Gram matrix that approximates the similarity matrix by clipping, flipping, or shifting eigenvalues, or by explicitly learning a positive-definite approximation. However, modifications to the similarity matrix often lead to information loss and the enforced positive-definite property may only hold on the training data, causing inconsistency between testing and training samples. Another approach involves selecting a subset of training samples as a representative set and using distances or similarities to structured inputs as the feature function. Proper scaling is crucial for this method. The proposed framework D2KE constructs PD kernels from a dissimilarity measure on structured inputs, offering a more general and effective approach compared to using a representative set for feature functions. D2KE constructs PD kernels from a dissimilarity measure on structured inputs, creating novel kernels for a given distance measure. The kernels ensure Lipschitz-continuity in the corresponding RKHS and provide a tractable estimator with improved generalization properties. Our framework constructs novel kernels in a Reproducing Kernel Hilbert Space (RKHS) based on a given distance measure. It provides a tractable estimator with better generalization properties and produces a feature embedding for each instance. In classification experiments across various domains, our framework outperforms existing distance-based learning methods in terms of accuracy and computational efficiency, especially with large datasets or structured inputs. Our main contributions include proposing a methodology for constructing PD kernels via Random Features from a given distance measure for structured inputs. We also generalize Random Features methods to complex structured inputs of variable sizes for the first time. This is the first time a generic RF method has been used to accelerate kernel machines on structured inputs like time-series, strings, and histograms. Existing approaches for Distance-Based Kernel Learning have limitations such as strict conditions on the distance function or constructing empirical PD Gram matrices that may not generalize well to test samples. The limitations of existing approaches for Distance-Based Kernel Learning include strict conditions on the distance function and the need to construct empirical PD Gram matrices that may not generalize well to test samples. BID22 and BID42 provide conditions for obtaining a PD kernel through simple transformations of the distance measure, but these conditions are not met by commonly used dissimilarity measures like Dynamic Time Warping, Hausdorff distance, and Earth Mover's distance. Approaches for Distance-Based Kernel Learning include finding a Euclidean embedding or a theoretical foundation for an SVM solver in Krein spaces. Specific methods focus on building a PD kernel for structured inputs like text and time-series by modifying distance functions into kernels. Randomized feature maps have been used to approximate non-linear kernel machines, leading to a significant reduction in training and testing times for kernel-based learning algorithms. Various explicit nonlinear random feature maps have been developed for different types of kernels to address the diagonal-dominance problem in kernel Gram matrices. Random Fourier Features (RFF) method approximates Gaussian Kernel function by multiplying input with a Gaussian random matrix, leading to reduced training and testing times for kernel-based learning algorithms. Various explicit nonlinear random feature maps have been constructed for different types of kernels, including Gaussian, Laplacian, intersection, additive, dot product, and semigroup kernels. These methods have been extensively studied both theoretically and empirically. D2KE accelerates Random Fourier Features (RFF) by computing the RF with a structured distance metric for inputs of different sizes, unlike traditional RF methods that only consider vector representations. D2KE accelerates Random Fourier Features (RFF) by using a structured distance metric for inputs of varying sizes, unlike traditional RF methods that focus on vector representations. D2KE also constructs a new PD kernel through a random feature map, making it computationally feasible via RF. The differences between D2KE and existing RF methods are outlined in Table 1. A recent work developed a kernel and algorithm for computing embeddings of single-variable real-valued time-series, but it cannot be applied to discrete structured inputs like strings, histograms, and graphs. In contrast, a unified framework for various structured inputs beyond the limitations of the previous method is provided, along with a theoretical analysis regarding KNN and other distance-based kernel methods for estimating a target function from samples. The function f maps structured input objects x to output observations y. The dissimilarity measure d is used between input objects instead of a feature representation of x. The size of structured inputs x may vary widely. The dissimilarity matrix in BID4 can be found for size-structured inputs x i, which can vary widely. An ideal feature representation for learning should be compact and result in a simple target function. The dissimilarity measure d(x1, x2) for learning a target function should satisfy certain properties, where small dissimilarity implies a small difference in the function. The dissimilarity measure for learning a target function should have small Lipschitz-continuity to ensure a small difference in the function, while also maintaining a small expected distance among samples. The covering number N(\u03b4; X, d) measures the size of the space implied by a dissimilarity measure d on a compact input domain X. This quantity affects the estimation error of a Nearest-Neighbor Estimator in structured input spaces. The effective dimension of k-nearest-neighbor estimation error in structured input spaces X with distance measure d is defined by the covering number N(\u03b4; X, d). An example is provided for measuring the effective dimension in the space of Multisets. The Hausdorff Distance is used to measure the distance between elements in a set. The (modified) Hausdorff Distance measures the distance between elements in a set. The covering number N(\u03b4; V, \u2206) is used to cover sets under a ground distance. Effective dimension helps bound the estimation error of the k-Nearest-Neighbor estimate of f (x). The k-Nearest-Neighbor estimate of the target function f decreases slowly with n when p X,d is reasonably large. The estimation error needs the number of samples to scale for it to be bounded. In Assumption 3, the estimation error of k-NN decreases slowly with n when p X,d is large. To address this, an estimator based on a RKHS derived from the distance measure is developed with better sample complexity for higher effective dimension problems. A method called D2KE is introduced to convert a distance measure into a positive definite kernel for structured input domains. D2KE is a method that constructs positive-definite kernels from a given distance measure for structured input domains. The kernels are parameterized by a distribution over random structured objects and a parameter \u03b3. The kernel can be interpreted using a soft minimum function. The kernel in Equation (4) can be interpreted as a soft version of the distance substitution kernel. When \u03b3 \u2192 \u221e, the value is determined by min \u03c9 \u2208\u2126 d(x, \u03c9) + d(\u03c9, y), which equals d(x, y) if X \u2286 \u2126. The kernel in Equation (4) is a soft version of the distance substitution kernel. When X \u2286 \u2126, the kernel is always positive definite by construction. Random Feature Approximation can be used to approximate the kernel in Equation (4) when it cannot be evaluated analytically. The kernel in Equation (4) can be approximated using Random Features when it cannot be evaluated analytically. This allows for the use of the kernel in large-scale settings with a large number of samples, where standard kernel methods are not efficient enough. The RF approximation enables the direct learning of a target function as a linear function of the RF feature map by minimizing a domain-specific empirical risk. The RF approximation allows for direct learning of a target function as a linear function of the RF feature map by minimizing domain-specific empirical risk. This approach is orthogonal to a recent work that selects random features in a supervised setting. The overall RF-based empirical risk minimization for D2KE kernels is outlined in Algorithm 1. The structured distance measure in Algorithm 1 computes random feature embeddings using an exponent function parameterized by \u03b3, contrasting with traditional RF methods. A detailed analysis of the estimator will be provided in Section 5, comparing its statistical performance to K-nearest-neighbor. The relationship to the Representative-Set Method is discussed. The structured distance measure in Algorithm 1 computes random feature embeddings using an exponent function parameterized by \u03b3, contrasting with traditional RF methods. It compares its statistical performance to K-nearest-neighbor and discusses the relationship to the Representative-Set Method. The approach gives a kernel Equation (4) depending on data distribution, with a Random-Feature approximation obtained by creating an R-dimensional feature embedding. The form DISPLAYFORM2 in Algorithm 1 is a 1/\u221aR-scaled version of the embedding function in the representative-set method. By interpreting Equation (8) as a random-feature approximation to the kernel in Equation (4), a better generalization error bound is obtained even as R approaches infinity. This contrasts with the analysis in the Representative-Set Method, where a small representative set size is needed for reasonable generalization performance. The choice of p(\u03c9) is crucial in this context. In BID7, a small representative set size (O(n)) is crucial for good generalization performance. The choice of p(\u03c9) in the kernel is important, with \"close to uniform\" distributions outperforming data distribution choices. For example, in time-series domain with DTW dissimilarity, a distribution of random time series with Gaussian elements performs better than the Representative-Set method. In string classification and Hausdorff distance, random distributions outperform Representative-Set Method. Two potential reasons for this performance improvement are conjectured. The chosen distribution p(\u03c9) with elements drawn uniformly from a unit sphere outperforms RSM in string classification and Hausdorff distance. The better performance is attributed to the ability to generate unlimited random features and provide a better approximation to the exact kernel. This is in contrast to RSM, which requires limited held-out samples from the data. In this section, the proposed framework is analyzed from the perspective of error decomposition in RKHS. H represents the RKHS corresponding to the kernel, with DISPLAYFORM0 as the population risk minimizer subject to the RKHS norm constraint f H \u2264 C, and DISPLAYFORM1 as the corresponding. The RKHS corresponding to the kernel in Equation (4) is denoted as H. DISPLAYFORM0 is the population risk minimizer subject to the RKHS norm constraint f H \u2264 C, while DISPLAYFORM1 is the corresponding empirical risk minimizer. The estimated function from the random feature approximation is denoted as f R. The population and empirical risks are denoted as L( f ) and L( f ) respectively. The risk decomposition is given by DISPLAYFORM2, with a discussion on the function approximation error in a smaller function space compared to Lipschitz-continuous functions. The RKHS implied by the kernel in Equation FORMULA12 is a smaller function space than Lipschitz-continuous functions. Any function in the RKHS is Lipschitz-continuous w.r.t. the given distance. Additional smoothness is imposed via the RKHS norm constraint and kernel parameter. The goal is for the best function within this class to approximate the true function well in terms of the approximation error. The RKHS aims to approximate the true function well with low approximation error. The estimation error is bounded by a tuning parameter \u03bb, with a preference for smaller values relative to sample size n. The eigenvalues of the kernel play a role in determining \u03bb, with a kernel-independent bound suggesting \u03bb = 1/ \u221a n for error control. The estimation error in RKHS estimation is controlled by a tuning parameter \u03bb, with a preference for smaller values relative to sample size n. The error has a better dependency on n (i.e. n \u22121/2 ) compared to the k-nearest-neighbor method, especially for higher effective dimension. Tighter bounds on D \u03bb and better rates w.r.t. n may be possible with further analysis, but it is challenging due to the lack of an analytic form of the kernel. The difficulty of estimating D \u03bb for the kernel in Equation FORMULA12 is due to the lack of an analytic form. The error from RF approximation can be bounded by the estimation error in Equation FORMULA3. Analyzing the approximation error of the kernel DISPLAYFORM3 shows uniform convergence with effective dimension p X,d. To ensure |\u2206 R (x 1 , x 2 )| \u2264 with probability 1 \u2212 \u03b4, effective dimension p X,d is needed. Proposition 2 provides an approximation error based on kernel evaluation. The Representer theorem leads to the optimal solution for empirical risk minimization. Corollary 1 states that to guarantee L(f R ) \u2212 L(f n ) \u2264 , M must be Lipschitz-continuous. To ensure |\u2206 R (x 1 , x 2 )| \u2264 with probability 1 \u2212 \u03b4, effective dimension p X,d is needed. Corollary 1 states that to guarantee L(f R ) \u2212 L(f n ) \u2264 , M must be Lipschitz-continuous. The proposed framework can achieve -suboptimal performance with a number of Random Features proportional to the effective dimension O(p X,d / 2). The proposed framework can achieve -suboptimal performance by combining error terms. It shows that the target function lies close to the population risk minimizer in the RKHS spanned by the D2KE kernel. The method is evaluated in various domains like time-series, strings, texts, and images. The proposed method achieves -suboptimal performance by combining error terms and shows that the target function lies close to the population risk minimizer in the RKHS spanned by the D2KE kernel. It is evaluated in various domains such as time-series, strings, texts, and images. The method involves dissimilarity measures like Dynamic Time Warping (DTW) for time-series, Edit Distance for strings, Earth Mover's distance for measuring semantic distance between Bags of Words, and (Modified) Hausdorff distance for representing documents. The Mover's distance BID40 is used to measure semantic distance between Bags of Words, while the (Modified) Hausdorff distance BID24 BID15 is used for measuring semantic closeness of Bags of Visual Words. The distance measures are computationally demanding, with quadratic complexity, and C-MEX programs were adapted or implemented for efficiency. Four datasets were selected for experiments in each domain, with varying lengths of multivariate time-series data. For time-series data, 4 datasets were selected with multivariate time-series of varying lengths. Three datasets are from the UCI Machine Learning repository BID18, and one is generated from IQ samples from a wireless communication system. String data consists of 4 datasets with alphabet sizes between 4 and 8, and text data overlaps with BID26 datasets. For text data, datasets were chosen partially overlapped with those in BID26, with document lengths varying from 9.9 to 117. Image data datasets were derived from Kaggle, with SIFT feature vectors sizes ranging from 1 to 914. Each dataset was split into 70/30 train and test subsets. D2KE was compared against 5 state-of-the-art baselines, including KNN and DSK_RBF. Properties of the datasets are summarized in TAB5 in Appendix B. In Appendix B, D2KE is compared against 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, KSVM, and RSM. In Appendix B, D2KE is compared against 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, KSVM, and RSM. The baselines have quadratic complexity in both the number of data samples and the length of the sequences, while D2KE has linear complexity in both. Parameters are optimized through 10-fold cross validation for each method. For our new method D2KE, we optimize parameters through 10-fold cross validation and use random samples to achieve performance close to an exact kernel. The best number of samples falls within the range of R = [4, 4096]. We employ linear SVM for embedding-based methods and LIBSVM BID5 for precomputed dissimilarity kernels. D2KE consistently outperforms or matches other state-of-the-art baselines. D2KE outperforms baseline methods in classification accuracy and computation time, surpassing KNN and other kernels like DSK_RBF and DSK_ND. The method shows superior performance by inducing a representation from a positive definite kernel. Our method, operating on indefinite similarity matrix, outperforms RSM and D2KE in practical construction of feature matrix. Random objects sampled by D2KE perform significantly better, as discussed in section 4. More detailed experimental results for each domain are in Appendix C. The proposed framework derives a positive-definite kernel and feature embedding from dissimilarity measures for structured input domains. It encompasses existing approaches and suggests creating embeddings based on distance to random objects. Extending this to a deep architecture is a promising direction for future research. The curr_chunk discusses creating distance-based embeddings within a deep architecture for structured inputs. It involves bounding the magnitude of Hoefding's inequality for input pairs in X. The curr_chunk discusses bounding the magnitude of Hoefding's inequality for input pairs in X using distance-based embeddings within a deep architecture. It involves applying union bound over a covering E of X with respect to d(., .) and choosing parameters to yield the desired result. The curr_chunk discusses the optimality of parameters for the objective using an approximate kernel, followed by the general setup of methods including parameter search and cross-validation. The Matlab implementation is used for running experiments, and random selection is employed to obtain data samples for a new method called D2KE. The curr_chunk discusses using random selection to obtain data samples for the new method D2KE, with the best number of samples falling within the range of 4 to 4096. Linear SVM is used for embedding-based methods, while precomputed dissimilarity kernels are implemented using LIBSVM. All datasets are sourced from popular public websites for Machine Learning and Data Science research. The datasets used for the study were collected from various public websites for Machine Learning and Data Science research. Computation was done on a DELL system with Intel Xeon processors and SUSE Linux operating system, using multithreading to accelerate the computation of different dissimilarity kernels. The study utilized a DELL system with Intel Xeon processors and SUSE Linux operating system, employing multithreading for faster computation. Distance computations were accelerated using 12 threads, with a focus on the DTW distance measure for time-series data. Gaussian distribution with parameters \u03c3 and random time series lengths were optimized. D2KE showed superior classification accuracy compared to other baselines, with reduced computation time. D2KE outperforms other baselines in classification accuracy for multivariate time-series with less computation time. It achieves 26.62% higher performance than KNN on IQ_radio due to KNN's sensitivity to data noise. Our method outperforms DSK_RBF, DSK_ND, and KSVM on data sets like Auslan, showing that a representation induced from a truly p.d. kernel utilizes data better than indefinite kernels. RSM is similar in construction of the feature matrix, but D2KE's random time series sampling performs significantly better due to noise issues in RSM's method. Our method samples a short random sequence to denoise and find patterns in the data, unlike RSM which suffers from noise and redundant information. The number of possible random sequences drawn from the distribution is unlimited, making the feature space more abundant. RSM incurs significant computational cost for long time-series due to its quadratic complexity in length. Our method uses Levenshtein distance to measure similarity between strings by generating random strings from the data alphabet. We optimize parameters for \u03b3 and random string length. Results show D2KE outperforms other distance-based methods. Results show that D2KE consistently outperforms other distance-based baselines, including DSK_RBF with Levenshtein distance. D2KE offers a clear advantage over baseline methods, especially on relatively large datasets. On large datasets, D2KE outperforms other baselines with better performance and less computation. It achieves higher accuracy with significantly less runtime compared to DSK_RBF, DSK_ND, and KSVM due to lower computational costs for kernel matrix construction and eigendecomposition. For text data, earth mover's distance is used as the distance measure between documents. For text data, earth mover's distance is used as the distance measure between documents. Bag of Words is computed for each document and represented as a histogram of word vectors. Random documents are generated with word vectors sampled from the unit sphere. Parameters for \u03b3 and length of random documents are optimized. D2KE outperforms other baselines on all datasets, showing the effectiveness of SVM over KNN on text data. The random documents of short length used in D2KE fit well for document classification tasks related to \"topic\" learning. For image data, the modified Hausdorff distance (MHD) is used as the distance measure between images, showing excellent performance in the literature. SIFT-descriptors with dimension 128 are generated using the OpenCV library, and MHD is then used to compute the distance between sets of SIFT descriptors. Random images are generated for the task. The OpenCV library is used to generate SIFT-descriptors with dimension 128, followed by using MHD to compute the distance between sets of descriptors. Random images are created for each SIFT-descriptor. The best parameters for \u03b3 and the length of random SIFT-descriptor sequence are searched for. Results show that D2KE outperforms other baselines in most cases. The DSK_RBF algorithm performs best on dataset decor, possibly due to the ineffectiveness of SIFT features. However, the quadratic complexity of DSK_RBF, DSK_ND, and KSVM makes scaling to large data challenging. Despite this, D2KE outperforms KNN and RSM, indicating its potential as a strong alternative across applications."
}