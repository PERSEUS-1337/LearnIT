{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution in the spectral domain. Efficient audio synthesis using GANs can generate high-fidelity audio faster than WaveNet baselines, as shown through empirical investigations on the NSynth dataset. Neural audio synthesis aims to efficiently produce high-fidelity audio with global structure, requiring modeling of temporal scales over a wide range. Autoregressive models like WaveNet have made significant advancements but suffer from slow sampling speeds due to generating waveforms one sample at a time. Generative Adversarial Networks (GANs) have been successful in generating high-resolution images, in contrast to autoregressive models like WaveNet which have slow sampling speeds due to inefficient ancestral sampling. Research has focused on speeding up generation methods, but they come with significant overhead such as training secondary networks or writing customized kernels. GANs operate at a fine timescale and can model global latent structures efficiently. Generative Adversarial Networks (GANs) have been successful in generating high-resolution images efficiently. While GANs have shown potential for audio transformations, adapting image GAN architectures for waveform generation has not achieved the same level of fidelity. Frame-based techniques for audio waveform generation, such as transposed convolutions or STFTs, struggle to match the perceptual fidelity of image-based GAN architectures. The alignment between audio periodicity and output stride causes issues in generating locally-coherent waves. Transposed convolutional filters face challenges in covering all necessary frequencies and phase alignments to preserve coherence. Instantaneous radial frequency can be derived from unwrapped phase in an STFT, showing the constant relationship between audio and frame frequency. Spectra for a trumpet note from the NSynth dataset are illustrated. GAN researchers have made significant progress in image modeling. Researchers have made rapid progress in image modeling by evaluating models on focused datasets with limited degrees of freedom. The NSynth dataset was introduced with similar motivation for audio. The NSynth dataset consists of individual notes from musical instruments across various pitches, timbres, and volumes. The data is aligned and cropped to focus on fine-scale details like timbre and fidelity. Each note is accompanied by attribute labels for conditional generation. The dataset introduced autoregressive WaveNet autoencoders and bottleneck spectrogram autoencoders. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms, such as speech and music. Unlike images, audio waveforms are highly periodic. Effective representations for noncausal convolutional generation in audio waveforms, like speech and music, are crucial for maintaining the regularity of periodic signals over short to intermediate timescales. Convolutional filters trained on this data form frequency selective filter banks spanning the range of human hearing, and the alignment of frames with waveform periodicity is essential for human perception. Phase precession is crucial for synthesis networks to learn frequency and phase combinations to produce coherent waveforms. This phenomenon is similar to short-time Fourier transform (STFT) and occurs when filterbanks overlap in convolutional networks. Phase precession is important for synthesis networks to learn frequency and phase combinations for coherent waveform generation. It is similar to short-time Fourier transform (STFT) and occurs when filterbanks overlap in convolutional networks. Additionally, a method inspired by the phase vocoder BID7 involves unwrapping the phase of a pure tone to create a linearly growing precessing phase, with the derivative of the unwrapped phase remaining constant and equal to the angular difference between frame stride and signal periodicity. Phase precession is crucial for synthesis networks to learn frequency and phase combinations for coherent waveform generation. It is akin to the short-time Fourier transform (STFT) and occurs when filterbanks overlap in convolutional networks. The derivative of the unwrapped phase remains constant and is equal to the angular difference between the frame stride and signal periodicity, known as the instantaneous angular frequency. The unwrapped phase smoothly diverges, forming solid bands in the instantaneous frequency (IF) spectra where harmonic frequencies are present. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Key findings include generating log-magnitude spectrograms and phases directly with GANs for more coherent waveforms, estimating IF spectra for even more coherent audio, and the importance of preventing harmonics from overlapping by increasing STFT frame size and switching to mel frequency scale. In the study, GANs on the NSynth dataset outperform WaveNet in generating music examples faster. Global conditioning on latent and pitch vectors helps GANs produce smooth timbre interpolation and consistent timbral identity across pitch. The NSynth dataset contains 300,000 musical notes from 1,000 instruments with diverse timbres and pitches. Each sample is four seconds long and sampled at 16kHz. Training was done on acoustic instruments and fundamental pitches ranging from MIDI 24-84 for human evaluations on audio quality. For human evaluations on audio quality, training was restricted to acoustic instruments and fundamental pitches from MIDI 24-84. This resulted in 70,379 examples from strings, brass, woodwinds, and mallets. A new 80/20 test/train split was created from shuffled data to avoid the original split by instrument type. Inspired by image generation successes, progressive training methods were adapted to generate audio spectra. Hyperparameter configurations and learning rates were explored during the search process. The study adapted progressive training methods to generate audio spectra by sampling a random vector and using transposed convolutions. Gradient penalty was used to promote Lipschitz continuity. More details can be found in the original paper and the appendix. The architecture involves using a gradient penalty for Lipschitz continuity and pixel normalization. Both progressive and nonprogressive training variants were tested, with slightly better results seen in progressive training. The method includes conditioning on a one-hot representation of musical pitch in the latent vector. Our method involves conditioning on a one-hot representation of musical pitch in the latent vector to achieve independent control of pitch and timbre. An auxiliary classification loss is added to the discriminator to predict the pitch label. STFT magnitudes and phase angles are computed using TensorFlow's implementation with specific parameters. We use an STFT with specific parameters to create an \"image\" of size (256, 512, 2) representing magnitude and phase. Magnitudes are scaled to -1 and 1, while phase angles are also scaled. Optionally, we unwrap the phase angle and calculate the finite difference to create \"instantaneous frequency\" models. The text chunk discusses the process of unwrapping phase angles and taking the finite difference to create \"instantaneous frequency\" models. It also mentions the importance of sufficient frequency resolution at lower frequencies and increasing STFT frame size and stride for high frequency resolution. Additionally, it talks about transforming magnitudes and frequencies to a mel frequency scale without compression for better separation of lower frequencies. The text chunk introduces \"IF-Mel\" variants by converting frequencies to a mel scale without compression. It compares against WaveGAN for waveform generation and mentions training GANs on the NSynth dataset. WaveNet is the state of the art in generative audio modeling, with strong baselines created by adapting the architecture to accept pitch conditioning signals. WaveNet baselines were improved by adapting the architecture to accept one-hot pitch conditioning signals. Variants were trained using 8-bit mu law and 16-bit mixture of logistics for output distributions, with the 8-bit model proving more stable and outperforming the 16-bit model. Evaluation of generative models is challenging due to the subjective nature of perceptually-realistic audio generation goals. Multiple metrics were used to evaluate model performance. To evaluate model performance, diverse metrics are used, including human evaluation for audio quality. Human perception is sensitive to phase irregularities, which can disrupt a listener's experience. Amazon Mechanical Turk was used for a comparison test on all models, including a hold-out dataset. The study used Amazon Mechanical Turk to compare models in TAB0 for audio quality. Participants rated pairs of 4s examples on a Likert scale. 3600 ratings were collected, with each model involved in 800 comparisons. The diversity of generated examples is measured using a metric proposed by BID27, where training examples are clustered into 50 Voronoi cells in log-spectrogram space. The Inception Score (IS) proposed by BID28 is used to evaluate GANs by calculating the mean KL divergence between generated examples and a pretrained Inception classifier. The Inception Score (IS) is a standard metric in GAN literature that evaluates the diversity of generated examples by measuring the mean KL divergence between imageconditional output class probabilities and the marginal distribution. It penalizes models that produce examples that are not easily classified into a single class or belong to only a few possible classes. The metric has been modified to use features from a pitch classifier trained on spectrograms of an acoustic dataset, introducing Pitch Accuracy (PA) and Pitch Entropy (PE) as additional evaluation criteria. The Fr\u00e9chet Inception Distance (FID) is a metric proposed for evaluating GANs based on the distance between multivariate Gaussians fit to features extracted from a pretrained Inception classifier. It correlates with perceptual quality and diversity on synthetic distributions. The study compares different models and representations using a metric similar to Inception Score. Human evaluation shows a clear trend of decreasing quality as output representations move from IF-Mel to Waveform. IF-Mel is judged slightly inferior to real data, while WaveNet produces high-fidelity sounds but may occasionally break down. The highest quality model, IF-Mel, is slightly inferior to real data. WaveNet produces high-fidelity sounds but may occasionally break down. NDB score follows the same trend as human evaluation, with high frequency resolution improving the score. WaveNet receives the worst NDB score despite assigning high likelihood to training data. The TAB0 autoregressive sampling has a lack of diversity, leading to oscillation for each pitch conditioning. FID scores show IF models with high frequency resolution perform better, while phase models have poor sample quality even at high resolution. Mel scaling has less impact on FID compared to listener study. Phase models have high FID due to poor sample quality. Models perform well on classifier metrics like IS, Pitch Accuracy, and Pitch Entropy. High-resolution models generate examples with similar accuracy to real data, but discriminative information about sample quality is limited due to distribution mismatch. The metrics provide a rough measure of model performance, with low frequency models and baselines showing less reliability in generating classifiable pitches. Visualizing qualitative audio concepts is recommended, along with listening to accompanying audio examples for a better understanding. At https://goo.gl/magenta/ gansynth-examples, examples show waveform differences between WaveGAN, PhaseGAN, and IFGAN models. Real data is highly periodic, while WaveGAN and PhaseGAN have phase irregularities. IFGAN is more coherent with small variations. Rainbowgrams show consistent colors for IF models, while PhaseGAN has speckles. The Rainbowgrams visualize waveform differences between real data, IF models, PhaseGAN, and WaveGAN. Real data and IF models show coherent waveforms with strong colors for each harmonic, while PhaseGAN has speckles due to phase discontinuities and WaveGAN is irregular. Phase coherence is depicted in Rainbowgrams BID9, showing log magnitude of frequencies as brightness and IF as color. The Rainbowgrams BID9 visualization depicts phase coherence of harmonics in real data and IFGAN, with consistent colors. PhaseGAN shows discontinuities as speckled noise, while WaveGAN appears largely incoherent. GANs allow conditioning on the same latent vector for the entire sequence, unlike autoregressive models like WaveNet. WaveNet autoencoders in BID9 learn local latent codes for generation on a millisecond scale but have limited scope. Interpolating between examples in the raw waveform and the global code of an IF-Mel GAN shows perceptual mixing of sounds. WaveNet improves on this by mixing in the space of timbre. WaveNet improves upon the perceptual mixing of sounds by mixing in the space of timbre. The GAN model has a spherical gaussian prior for global interpolation. The WaveNet autoencoder lacks a compact prior over time series, resulting in less realistic in-between sounds. In contrast, the IF-Mel GAN with global conditioning produces high-fidelity audio examples during interpolation. The IF-Mel GAN with global conditioning produces high-fidelity audio examples during interpolation, ensuring smooth perceptual changes and alignment with the prior. Timbre morphs smoothly between instruments while pitches follow a composed piece like Bach's Suite No. 1 in G major. The timbre of sounds morph smoothly between instruments in the IF-Mel GAN with global conditioning, maintaining consistency across pitches. The generated audio examples demonstrate the unique instrument identity created by varying pitch conditioning over five octaves. The Bach prelude rendered with a single latent vector shows a consistent harmonic structure. The IF-Mel GAN with global conditioning smoothly morphs between instruments, maintaining consistency across pitches. Training and generation can be done in parallel, reducing latency significantly compared to WaveNet baseline. The IF-Mel GAN drastically reduces latency for audio synthesis, making it 53,880 times faster than WaveNet. This opens up the possibility for real-time neural network audio synthesis on devices, allowing users to explore a wider range of expressive sounds. Previous applications of WaveNet autoencoders relied on prerendering all possible sounds due to long synthesis latency. The curr_chunk discusses the focus on deep generative models for audio, particularly in speech synthesis, using recurrent and/or autoregressive models for variable length inputs and outputs. Comparatively, music audio generation is less explored. Adapting GANs for variable-length conditioning or recurrent generators is a future research direction. In comparison to speech, music audio generation is relatively under-explored. van den Oord et al. propose autoregressive models for synthesizing musical instrument sounds, but suffer from slow generation. Our work introduces a modification to GANs' loss function for improved training stability and architectural robustness, building on recent advances in GAN literature. Improved training stability and architectural robustness are achieved through progressive training in BID16. They also suggest architectural tricks to enhance quality. The NSynth dataset, known as the \"CelebA of audio,\" was used in previous works like BID20 and BID9. BID23 incorporated an adversarial domain confusion loss for timbre transformations in audio sources. BID23 improved timbre transformations in audio sources by incorporating an adversarial domain confusion loss. BID5 achieved significant sampling speedups by training a frame-based regression model for raw waveform mapping from pitch and instrument labels. Their architecture, however, requires a large amount of channels, slowing down sample generation and training. The study focused on high-quality audio generation with GANs on the NSynth dataset, surpassing WaveNet baseline fidelity while generating samples much faster. Further research is needed to validate and expand to different types of natural sound. This work also suggests potential for domain transfer. The study focused on high-quality audio generation with GANs on the NSynth dataset, surpassing WaveNet baseline fidelity while generating samples much faster. Further research is needed to validate and expand to different types of natural sound. This work also suggests potential for domain transfer. Additional work is required to address issues of mode collapse and diversity in GANs for audio, potentially by combining adversarial losses with encoders or regression losses. A pitch classifier could be added to the discriminator for measuring diversity across generated examples. The study focused on high-quality audio generation with GANs on the NSynth dataset, surpassing WaveNet baseline fidelity while generating samples much faster. Models were trained with the ADAM optimizer and different learning rates and weights were tested, with a learning rate of 8e-4 and classifier loss of 10 performing the best. Both networks used box upscaling/downscaling and pixel normalization in the generators. The discriminator in the GAN model appends the standard deviation of minibatch activations as a scalar channel. Real data is normalized before passing to the discriminator, using a Tanh output nonlinearity for the generator. Training each GAN variant takes 4.5 days. The GAN variants are trained for 4.5 days on a single V100 GPU, with a batch size of 8. Progressive models train on 1.6M examples per stage (7 stages), with 800k examples during alpha blending and 800k after blending. The WaveNet baseline also uses a Tensorflow implementation with a decoder consisting of 30 layers of dilated convolution, each with 512 channels. The WaveNet baseline model trains on \u223c11M examples with a decoder composed of 30 layers of dilated convolution, each with 512 channels. The audio encoder stack is replaced with a conditioning stack operating on a one-hot pitch conditioning signal distributed in time. The conditioning stack in the WaveNet model consists of 5 layers of dilated convolution, increasing to 25, followed by 3 layers of regular convolution, all with 512 channels. A 1x1 convolution is applied to each layer of the decoder, and the output is added to each layer. The model uses mulaw encoding for the 8-bit version and a quantized mixture of 10 logistics for the 16-bit version. WaveNets converged in 150k iterations over 2 days using 32 V100 GPUs trained with synchronous SGD."
}