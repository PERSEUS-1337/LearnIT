{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework is proposed in this paper to automatically standardize non-systematic names to systematic names using spelling error correction, byte pair encoding tokenization, and neural networks. Our framework utilizes spelling error correction, byte pair encoding tokenization, and neural sequence to sequence model to automatically standardize non-systematic chemical names to systematic names. The standardization accuracy on the test dataset is 54.04%, a significant improvement from previous results. There are over 100 million named chemical substances worldwide, each requiring unique systematic names based on their structures, defined by the International Union of. The International Union of Pure and Applied Chemistry (IUPAC) defines rules for systematic names of chemical substances. However, besides systematic names, chemicals also have common or trivial names for simplicity. For example, sucrose is commonly known as sugar. In chemistry, chemicals have systematic names defined by IUPAC rules, but also have common names for simplicity. For example, sucrose is known as sugar. In the pharmaceutical industry, proprietary names are often created to distinguish products, like Aspirin for 2-Acetoxybenzoic acid. Chemical substances can have multiple names, including systematic, common, and proprietary names like Aspirin for 2-Acetoxybenzoic acid. Chemical information extraction extracts useful chemical knowledge from text to update databases like PubChem and SciFinder. Chemical information extraction is an ongoing process to update databases like PubChem and SciFinder by converting systematic names to SMILES and InCHI representations. Systems like Open Parser for Systematic IUPAC Nomenclature can automatically convert systematic names to SMILES with high precision. Some online systems, like OPSIN BID7, are highly accurate in converting systematic names to SMILES strings. Non-systematic names can have errors such as spelling, ordering, and common name errors. Errors in non-systematic chemical names include spelling errors, ordering errors, common name errors, and synonym errors. Synonym errors, where words in nonsystematic names differ from systematic names but share the same root, are the most common. For example, 2-(Acetyloxy)benzoic Acid has synonyms Acetylsalicylic Acid and Acetysal, all sharing the root \"Acety\". The framework proposed aims to automatically convert non-systematic chemical names to systematic names by correcting spelling errors, tokenizing names using Byte Pair Encoding (BPE), and utilizing a sequence-to-sequence model. The task is challenging due to the presence of multiple error types in a single name, such as ordering and synonym errors. The text discusses the challenges of chemical name standardization and mentions the limited work done in this area. The BID2 system is highlighted as the only work deserving citation, but it is noted to have limitations due to its reliance on chemical knowledge. Our framework adopts a sequence to sequence model for chemical name standardization, inspired by machine translation techniques. This approach addresses common name errors, synonym errors, and ordering errors in chemical names. The framework for chemical name standardization addresses vocabularies, word order errors, and achieves 54.04% accuracy on test data. The corpus used contains non-systematic and systematic names of chemical substances extracted from Chemical Journals with High Impact Factors. The corpus used for chemical name standardization includes non-systematic and systematic names of chemical substances. There are 384816 data pairs in the corpus, with an overview of the Levenshtein distance between non-systematic and systematic names shown in FIG1. The experiment divides data into training, test, and development sets, with a focus on correcting spelling errors in chemical substance names. To correct spelling errors in chemical substance names, two vocabularies are set up from the dataset: one for systematic elemental words and one for non-systematic elemental words. Systematic names are split to build the systematic vocabulary, while non-systematic names are used to create an elemental vocabulary. The text discusses the process of building a final elemental vocabulary by combining systematic and non-systematic names. The non-systematic names are used to create a vocabulary of common names or synonyms, which are then combined with the systematic vocabulary. The elemental vocabulary is structured using a BK-Tree for efficient correction searches. The text explains the use of a BK-Tree to efficiently correct spelling errors in a final elemental vocabulary. BK-Tree is a tree structure where a root node is selected, and subtrees are built based on Levenshtein distance between vocabulary items. This allows for rapid retrieval of vocabulary items with the smallest Levenshtein distance from a given word. The BK-Tree is used to correct spelling errors in a final elemental vocabulary by finding vocabulary items with the smallest Levenshtein distance from a given word. It is scalable and allows for easy insertion of new training data. The process involves separating a chemical substance name into elemental words, inputting them into the BK-Tree, correcting errors, and combining the words to get the full name. The BK-Tree corrects spelling errors in elemental vocabulary by finding words with the smallest Levenshtein distance. It involves breaking down chemical names into elemental words, inputting them into the tree, correcting errors, and combining them to form the full name. This process aids in training sequence to sequence models by reducing noise in elemental words. The Levenshtein distance between nodes in the same subtree is consistent. Chemical names are tokenized using Byte Pair Encoding (BPE) BID11 for sequence-to-sequence model application. The symbol set is initialized with single characters before iterative counting. Byte Pair Encoding (BPE) is used for tokenization by initializing a symbol set with single characters and iteratively counting symbol pairs. The most frequent pair is replaced with a new symbol, resulting in a final symbol set size equal to the initial character size plus the number of merge operations. BPE helps with out-of-vocabulary problems and generates vocabularies at the character level. Reasons for choosing BPE include its ability to handle out-of-vocabulary issues by generating vocabularies at the character level and to tokenize chemical names into meaningful subwords. BPE is applied to chemical names, and the tokenization results can be used to train a sequence to sequence model, such as OpenNMT with modifications. The sequence to sequence model BID12 is commonly used in machine translation. It consists of an encoder that generates a context vector H from source sequences and a decoder that uses this vector to generate target sequences. The encoder utilizes a multilayer bidirectional LSTM (BiLSTM) for processing sequences forward and backward. The multilayer bidirectional LSTM (BiLSTM) BID3 processes sequences forward and backward, generating a context vector H at the final time step of the encoder. In the decoder, it calculates the probability of an output sequence \u0177. The spelling error correction stage only requires adjusting the threshold of the BK-Tree parameter. In the experiment, various threshold values and merge operation numbers were tested for the BK-Tree and BPE stages. The sequence to sequence model used 500 dimensions for word embeddings and hidden states, with a vocabulary size determined by basic characters and BPE merge operations. Both encoder and decoder had 2 layers. The sequence to sequence model used 2 layers in both encoder and decoder. Spelling error correction was done for non-systematic names in the training data. Parameters were trained jointly using stochastic gradient descent. Loss function was cross-entropy, computed over minibatches of size 64 and normalized. Weights were initialized with a random uniform distribution. Initial learning rate was 1.0 with decay applied every epoch after epoch 8. The experiment involved training a model with a random uniform distribution for weights, a learning rate of 1.0 with decay applied every epoch after epoch 8, a dropout rate of 0.3, and training for 15 epochs. The beam size for decoding was set to 5. Another experiment was conducted using a Statistical Machine Translation (SMT) model with specific parameters such as limiting training sequence length to 80, using a 3-grams language model, and tokenization with BPE and 5000 merge operations. In training, sequences are limited to 80 with a 3-grams language model using KenLM BID4. Tokenization is done with BPE and 5000 merge operations. Data augmentation is used to handle noisy data, specifically spelling errors. Error insertion types include random character insertion, deletion, exchange, and replacement. In the experiment, four types of data augmentation methods are used: random character insertion, deletion, exchange, and replacement. These methods are applied with equal probability. The standardization quality is measured using accuracy and BLEU score BID10. Accuracy is calculated based on the successful standardization of non-systematic names. The experiment results on test dataset show that the combination of spelling error correction, BPE tokenization, and sequence to sequence model achieves the best performance. The framework has a significant improvement compared to the SMT model and the ChemHits system. The results for different numbers of BPE merge operations indicate that 5000 is the optimal value. The results show that 5000 is the best value for BPE merge operations. Spelling error correction and data augmentation are beneficial for the framework, with spelling error correction outperforming data augmentation. Overcorrection may occur with large Levenshtein distance thresholds, reducing standardization quality. Successful standardization examples are shown in Table 6. The sequence to sequence model can successfully standardize non-systematic names by fixing spelling errors and correcting order and synonym errors. Examples in Table 6 demonstrate these capabilities, such as replacing parentheses in names and correcting the order of chemical compounds. The sequence to sequence model successfully standardizes non-systematic chemical names by correcting errors in order and synonyms. Examples include correcting the order of ethane,1,2-dichloro to 1,2-dichloroethane and standardizing P-anise alcohol. Additionally, a visualization of attentions for an example is provided, showing the correction of adenine,9-methyl-(7ci,8ci) to 9-methyl-9H-purin-6-amine. The seq2seq model successfully standardizes non-systematic chemical names by correcting errors in order and synonyms. Examples include correcting the order of ethane,1,2-dichloro to 1,2-dichloroethane and standardizing P-anise alcohol. The model can find the relation between adenine and 9H-purin-6-amine, correcting the name to 9-methyl-9H-purin-6-amine. The system analyzes failed standardization attempts, manually labeling error types. Synonym errors are the most common, while spelling errors are handled well. Common errors pose a challenge due to the lack of rules between common and systematic names. The system struggles to standardize unseen common names to systematic names, with 10 samples nearly correct, 7 totally incorrect, and the rest partially correct. About half of non-systematic names remain unstandardized. The accuracy for systematic names of different lengths is illustrated in Figure 6. Our framework achieves the best performance for systematic names of length between 20 and 40 but performs poorly for names longer than 60. Some names generated by our model do not follow chemical rules, and some subwords generated during tokenization are not explainable. Examples of failed attempts are shown in Table 8. The framework achieves the best performance for systematic names of length between 20 and 40 but performs poorly for names longer than 60. Some names generated do not follow chemical rules, and subwords generated during tokenization by BPE are not explicable. Examples of failed attempts are shown in Table 8. In this work, a framework is proposed to automatically convert non-systematic chemical names to systematic names using spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. The framework achieves an accuracy of 54.04% on the dataset, outperforming previous rule-based systems. It is trained end-to-end, data-driven, and independent of external chemical knowledge, opening up new possibilities for chemical information extraction."
}