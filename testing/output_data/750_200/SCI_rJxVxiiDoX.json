{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits. Additionally, combining quantization-aware training with weight matrix factorization significantly reduces model size and computation for small-footprint keyword spotting while maintaining performance. By tuning a threshold, context information is incorporated by stacking frames in the input for small-footprint keyword spotting. Quantization-aware training is used to mitigate performance degradation caused by quantization in keyword models deployed on devices. This method considers quantized weights in full precision representation to inject quantization error into training. Quantization-aware training is utilized to optimize weights against quantization errors in building a small-footprint low-power keyword spotting system. The method enables successful training of 8 bit and 4 bit quantized KWS models. The paper introduces the use of dynamic quantization approach for shifts and scales. The paper introduces quantization-aware training for optimizing weights in a keyword spotting system. Dynamic quantization is used for shifts and scales, with inputs quantized row-wise during training. The keyword 'Alexa' is chosen for experiments using a 500 hrs far-field corpus for training and a 100 hrs dataset for evaluation. For experiments, a 500 hrs far-field corpus and a 100 hrs dataset are used. 70 models are evaluated using DET curves for miss rate vs. FAR and DET AUC. Training involves a 3-stage process with GPU-based distributed DNN training. Initial stage pre-trains a small ASR DNN with 3 hidden layers, followed by exponential learning rate decay. The performance of pre-trained models using full ASR phone-targets is evaluated through quantization-aware training. Results show improvement in AUC and DET curves for quantized models compared to full-precision models."
}