{
    "title": "rygDeZqap7",
    "content": "Natural language understanding research has shifted towards complex Machine Learning and Deep Learning algorithms, which often outperform simpler models. To address the challenge of limited labeled data availability, a methodology for extending training datasets and training data-hungry models using weak supervision is proposed. This approach is applied to biomedical relation extraction, a task crucial for drug discovery but costly to create datasets for. In two small-scale experiments, our method enhances LSTM network performance comparable to hand-labeled data. We discuss optimal weak supervision settings for extracting structured information from increasing biomedical papers. Automating Information Extraction from unstructured text can greatly impact various tasks in fields like drug design and adverse drug effect detection. Efforts have been made to automate this process due to the labor-intensive nature of manual annotation by domain experts. This work focuses on automating semantic triple extraction from biomedical abstracts, specifically on two different relations. The work focuses on automating semantic triple extraction from biomedical abstracts for two important relations: Regulations (CPR) and Chemically Induced Diseases (CID). This automation will aid in drug design, safety, and discovery by enabling researchers to filter or select chemical substances with specific properties faster. The main focus of the work is on automating the extraction of semantic triples related to Regulations (CPR) and Chemically Induced Diseases (CID) from biomedical abstracts. This involves identifying entities of interest in unstructured text and building a classifier for relation extraction. Increasing the learning algorithm's capacity is necessary for this complex task, but it should be accompanied by a larger training dataset. The methodology proposed involves training multiple base learners on a small labeled dataset to predict labels for a larger unlabeled dataset, followed by using a denoiser to derive weak labels for the unlabeled set. This approach combines ideas from semi-supervised and ensemble learning to address the labor-intensive process of annotating training datasets for complex tasks like relation extraction. The methodology involves training base learners on a small labeled dataset to predict labels for a larger unlabeled dataset, using a denoiser to derive weak labels for the unlabeled set, and training a strong meta-learner using weak supervision. Key contributions include proposing a detailed methodology for relation extraction, demonstrating its effectiveness in a controlled experiment, and investigating denoising methods' impact on system performance. The curr_chunk discusses the release of code for a small-scale experiment on denoising methods' effects on system behavior. It also covers literature on information and relation extraction from biomedical text, and semi-supervised learning methods. The curr_chunk discusses different approaches to information extraction, including fully-supervised, semi-supervised, and unsupervised methods like Open Information Extraction BID4. Semi-supervised methods, such as bootstrapping algorithms like DIRPE BID6, leverage both labeled and unlabeled data for extracting patterns. Other examples of semi-supervised algorithms mentioned are Snowball BID0, KnowItAll BID11, and TextRunner BID4. The curr_chunk discusses distant supervision as a method to generate weak labels for relation extraction problems using Knowledge Bases (KB) instead of manual annotation. The curr_chunk discusses the use of Knowledge Bases (KB) to generate weak labels for relation extraction without the need for manual annotation. This approach has shown benefits on large-scale datasets in the biomedical field, complementing distant supervision methods. The curr_chunk discusses the use of weak classifiers in biomedical relation extraction, focusing on Chemically-induced Diseases and relations between Chemicals and Proteins. The best performing teams utilized ensembles of Support Vector Machines, LSTM, CNN, and SVMs. The curr_chunk focuses on identifying relations between Chemicals and Proteins on a sentence level. The best performing team used an ensemble of LSTM, CNN, and SVMs, while the second highest score was achieved with a Support-Vector Machines algorithm. Overfitting problems were observed with approaches using only Deep Neural Networks, highlighting the importance of training data scarcity in this domain. The work aims to combine techniques to improve generalization, especially when using Deep Neural Networks. Our work aims to combine ensemble methods with semi-supervised learning to improve generalization in Machine Learning models. Ensemble learning reduces high variance by combining multiple learners, while semi-supervised learning utilizes unlabeled data for better generalization. This approach has not been investigated for this task before. Ensemble methods can enhance semi-supervised learning by providing multiple views, improving performance with less data. The combination of ensembles with semi-supervised learning has not been thoroughly studied, despite potential benefits. Expanding the dataset with unlabeled data can increase diversity between learners. Co-training, a system where two learning algorithms utilize unlabeled data, has shown success even without complete independence. Recent research incorporates expert-defined lexicons and co-training to reduce noise and enhance signal in distant supervision. Using lexicons and an auxiliary natural language processing system, a system was developed to create noisy annotations and incorporate co-training to reduce noise and augment signal in distant supervision. The system learned to accurately annotate data samples in functional genomics without using manually labeled data, outperforming state-of-the-art supervised methods. Tri-training extends co-training to three learners, while Co-forest involves even more learners in the decision-making process. Co-forest BID18 extends the learning process to more learners, where an ensemble system decides whether to include an unlabeled example in re-training. Unlike previous methods, the base learners in the ensemble system are not used for final predictions but for generating weak labels. This approach allows for the use of all unlabeled data, instead of just a few high-confidence annotated examples for re-training. Weak supervision and the data programming paradigm focus on training models using labels of questionable quality, allowing for the use of all unlabeled data. This approach contrasts with previous methods that only included a few high-confidence annotated examples for re-training. This work complements research on learning language representations and tuning them for specific machine learning tasks using a small labeled set. Weak supervision and data programming involve training models with low-quality labels, utilizing all unlabeled data. Weak supervision sources are defined and encoded into Labeling Functions, which can provide labels or abstain from voting for each data point. These sources may include textual patterns, crowd-workers, or distant supervision resources. Weak supervision sources, such as textual patterns or crowd-workers, can provide labels or abstain from voting for unlabeled data points. The Labeling Functions (LFs) are applied to derive a vote matrix \u039b, and data programming uses a Generative Model (GM) for denoising and deriving weak labels close to the true labels. Data programming utilizes a probabilistic graphical Generative Model (GM) with trainable parameters to estimate correlations of Labeling Functions. It maximizes the marginal log-likelihood of observed votes occurring under the GM without access to ground truth, generating probabilistic weak labels for training a noise-aware discriminative model. Based on weak supervision and data programming, a methodology for semi-supervised learning is proposed to leverage multiple learners. The approach involves using predicted label distributions as weak labels for training a noise-aware discriminative model. The model is trained using generated labels and minimizes a noise-aware loss function with respect to the weak labels. To capitalize on the benefits of multiple learners, it is proposed to augment a gold-labeled training set with additional lower quality data. Machine learning models of lower complexity are used as weak supervision sources, allowing for easy adaptation of existing pipelines to similar tasks. This approach aims to scale the dataset size when the available labeled training set is insufficient for training a complex model. The major advantage of adapting an already implemented pipeline with little effort is the use of a labeled training set D B of size m for task T. An unlabeled dataset D U of size M m from the same distribution as D B is also required, along with a validation set D V for hyperparameter tuning and a test set D T for evaluation. K base learners are trained on solving T to maximize individual performance and capture different data views. To create multiple learners for solving task T, 162 base learners are generated by varying hyperparameters and design choices in the relation extraction pipeline. One important design choice is sentence pruning, where irrelevant words within a sentence are removed to focus on the entities of interest. In the relation extraction pipeline, various approaches can be used to extract key information, such as constructing dependency-based parse trees and including words within the Shortest Path connecting entities of interest. Additionally, sequential features like tri-grams can be utilized, and the corpus can be converted to numerical representations for analysis. In our approach, we use up to tri-grams for text representation and employ different machine learning algorithms like Logistic Regression, Support Vector Machines, Random Forest Classifiers, LSTMs, and CNNs. Note that some feature engineering steps were not applicable when using LSTMs & CNNs. After using various machine learning algorithms like Logistic Regression, Support Vector Machines, Random Forest Classifiers, LSTMs, and CNNs, it was found that some feature engineering steps were not applicable for the last two models. To reduce computational cost, only a subset of base learners is selected, aiming to maximize their individual performance and diversity. A simple method is used to discard classifiers with lower performance than a set threshold while maximizing diversity. To maximize performance and diversity, a simple method is used to discard classifiers below a performance threshold, set above random guess baseline but low enough to include less accurate classifiers. A similarity-based clustering method is then employed to select the most diverse classifiers based on predictions from base learners. Using predictions from base learners, a similarity matrix is constructed to identify representative learners through K-means clustering. The number of clusters is determined using silhouette score coefficient. The selected base learners predict labels for new data, generating a binary prediction matrix based on distilled knowledge. We predict labels for new data using selected base learners, creating a binary prediction matrix based on distilled knowledge. A denoiser is then used to reduce the vote matrix into weak labels. Different denoisers are considered, including a Majority Vote denoiser and an Average Vote denoiser. Finally, hyperparameters are selected using a validation dataset, and a discriminative model is used in the last step. In practice, training the meta-learner with weak supervision trades label quality for quantity, benefiting when the meta-learner's performance is limited by the training set size. High-capacity models like Deep Neural Networks are used as meta-learners to learn their own features and improve accuracy by leveraging a larger, albeit noisy, training dataset. In experiments using Snorkel, the BioCreative CHEMPROT and CDR datasets are utilized for relation extraction with weak supervision. The methodology requires three gold-labeled datasets and a held-out test set, with the original test sets used as the held-out test set. The methodology for relation extraction using Snorkel involves utilizing three gold-labeled datasets and a held-out test set. The original test sets are used as the held-out test set, while the training and development sets are merged and shuffled to create datasets for training, validation, and unlabelled data. This setup ensures unbiased document selection during the process. To ensure unbiased document selection, it is important to meet two key requirements: (a) all datasets must be drawn from the same distribution to avoid bias, and (b) all documents must undergo the same pre-processing steps. This controlled approach allows for comparison of the meta-learner's performance trained with weak supervision. The text pre-processing pipeline for the algorithm involves steps such as sentence splitting, tokenization, and dependency parsing using SpaCy (v1.0). Named Entity Tags are manually annotated in both datasets for Candidate Extraction. Comparing the meta-learner's performance with weak supervision to optimal performance is possible due to this controlled approach. The algorithm's text pre-processing pipeline involves steps like sentence splitting, tokenization, and dependency parsing using SpaCy. Named Entity Tags are manually annotated in both datasets for Candidate Extraction. Snorkel is used for candidate extraction and mapping to ground-truth labels, with a focus on relationship classification rather than memorization of interacting pairs. Entities of interest are replaced with 'ENTITY1' and 'ENTITY2' for prediction purposes. In experiments, 'ENTITY1' and 'ENTITY2' are used to predict interacting pairs, with other entities labeled as 'CHEMICAL', 'GENE', or 'DISEASE'. A bi-directional LSTM network is employed with random word embeddings and under-sampling for class balance. Different hyperparameters like dropout values and training epochs are explored. In experiments, 'ENTITY1' and 'ENTITY2' are used to predict interacting pairs, with other entities labeled as 'CHEMICAL', 'GENE', or 'DISEASE'. A bi-directional LSTM network is employed with random word embeddings and under-sampling for class balance. Different hyperparameters like dropout values and training epochs are explored to enhance biomedical relation extraction using Machine Learning classifiers as weak supervision sources. Research questions focus on the optimal setting for weak supervision in this task. Related literature suggests that adding weakly labeled data can improve the performance of the meta-learner in the optimal setting for using weak supervision on this task. As the amount of weakly labeled data increases, the performance of the meta-learner is expected to improve quasi-linearly compared to scenarios with ground-truth labels. The weak supervision sources should have accuracy better than random guess, overlap, and disagree with each other enough for their accuracy to be estimated. Machine Learning classifiers have not been used as weak supervision sources in this setting, raising questions about the diversity and size of base learners trained on the same dataset. The study evaluates the impact of weak supervision on a meta-learner's performance by comparing different training modes. The experiments involve full-supervision on D B, weak-supervision on D U, and a combination of weak-supervision on D U with full-supervision on D B. The goal is to determine if weak-supervision can achieve results comparable to full-supervision when using all ground-truth labels. The study evaluates weak supervision's impact on a meta-learner's performance by comparing different training modes. The number of base learners is crucial, with the optimal number balancing performance and diversity. Increasing base learners while benchmarking weak labels and the meta-learner's performance is essential. The denoising component determines the quality of weak labels for the final learner's training. The denoising component is crucial for determining the quality of weak labels used to train the final learner. Different denoising methods are assessed, producing binary or marginal weak labels with varying distributions. An error analysis is conducted to understand how these labels impact training and the meta-learner's performance. The study investigates the impact of using supervised machine learning classifiers as weak classifiers and the optimal settings for applying weak supervision. Different denoising methods are tested to produce weak labels for training the meta-learner. The performance of base learners, weak labels, and the meta-learner is evaluated based on silhouette scores. Training the meta-learner with weak labels from denoisers, along with ground-truth labels, consistently outperforms training with only gold labels. This demonstrates the effectiveness of augmenting training data using weak supervision. Using weak supervision to augment training data with ground-truth labels (D B + D U) can achieve performance comparable to full supervision. In some cases, weak supervision may even yield slightly better results, although these differences are not statistically significant due to high variance in meta-learners' performance. Notably, when weak supervision outperforms, the under-sampled training set size of the final learner is larger. The under-sampled training set size of the final learner in weak supervision was larger when weak supervision outperformed. Majority Vote often outperforms the meta-learner, but this does not diminish the significance of the results. LSTM model cannot outperform Majority Voting with a small training dataset, even with gold quality labels. Learning curves of the meta-learner starting from groundtruth labels are visualized. The meta-learner's learning curves from groundtruth labels show an upward trend, indicating statistically significant results. However, the F1 score on the training set is consistently higher than the test score, suggesting overfitting. Additional training data is needed to improve the meta-learner's performance. The meta-learner's performance shows high variance (overfitting) due to the model capacity not being fulfilled. Additional training data is needed to improve performance. The small dataset size limits drawing definite answers, but an analysis based on experimental results will be discussed. The F1 score of weak Majority Vote labels for 5 learners is the lowest, while the Generative model weak marginals show no significant pattern in the F1 score. The meta-learner's performance varies due to model capacity not being met. More training data is needed for improvement. The F1 score for weak Majority Vote labels is lowest with 5 learners. Generative model weak marginals show inconsistent F1 scores. Performance of meta-learner improves with Average Marginals and Generative Model marginals. The metalearner's performance is best with Average Marginals, and Generative Model marginals also enhance its performance. However, GM marginals depend on hyperparameters chosen based on F1 score validation. Weak labels can be binary or marginal, with marginal labels improving the meta-learner's performance compared to binary labels. The denoisers produce weak labels, either binary or marginal, with marginal labels improving the meta-learner's performance compared to binary labels. Generative Model marginals tend to follow a U-shaped distribution, unlike average marginals. Error analysis on the validation set shows misclassifications using a classification boundary of 0.5. The error analysis on the validation set using a classification boundary of 0.5 shows that Average Vote labels are of higher quality compared to weak labels. The F1 score is unsuitable for evaluating marginal weak labels, as most misclassified labels are closer to 0.5. Training the LSTM with marginal labels shows changes in training loss and validation scores over epochs. When training the LSTM with marginal weak labels, the training error remains high initially, especially with Average weak marginals. However, after a few epochs, the LSTM starts predicting binary training labels accurately. Using marginal labels can be seen as a regression problem, where the model is penalized for failing to predict the exact number output by the denoiser. In practice, the model is penalized for failing to predict the exact number output by the denoiser. The distributions of predicted logits become more spread as the training marginals distributions become more uniform. Efforts are discussed to apply the methodology on the CPR task, expanding datasets and training base learners. The performance of the meta-learner decreases when weakly labeled data is added, indicating issues with data quality. Class imbalance is observed in the outgoing citations dataset compared to the original. t-SNE algorithm is used to validate this, showing differences in dataset distributions. Visualizations confirm the disparity between candidate samples from the original set versus both sets. The new dataset is unsuitable for the use case as most candidates lie in specific regions of the 2D space. Weak supervision can enhance the performance of complex models like deep neural networks by utilizing unlabeled data and multiple base learners. The proposed methodology is feasible for the task at hand. The proposed methodology utilizes deep neural networks and multiple base learners to leverage unlabeled data effectively. It shifts the focus from hand-labeling examples to feature engineering and constructing diverse learners, allowing for better generalization and performance on unlabeled data from the same domain. The methodology shifts human effort from hand-labeling examples to feature engineering and constructing diverse learners, enabling scalability in training datasets while improving performance. It can be reused on similar tasks with different datasets, eliminating the need for repeated hand-labeling in supervised learning. In supervised learning, hand-labeling large datasets is typical. To improve metalearner performance, exploring larger unlabelled datasets is crucial. This would help draw stronger conclusions on research questions and determine performance thresholds with increased dataset size. The importance of exploring larger unlabeled datasets to improve metalearner performance and determine performance thresholds is highlighted. Challenges in collecting appropriate unlabeled datasets and the need for a more suitable metric than the F1 score for evaluating weak labels are discussed. The need for a more suitable metric than the F1 score for evaluating weak labels is emphasized. This metric would enable direct conclusions from weak labels without the need for additional steps. Further investigations could involve experimenting with the meta-learner and defining a better selection method for Base Learners. Further research could explore improving the selection method for Base Learners, potentially by having them abstain from voting on uncertain examples. This approach could provide a modeling advantage for the Generative Model compared to unweighted methods like Majority Voting."
}