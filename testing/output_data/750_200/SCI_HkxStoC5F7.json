{
    "title": "HkxStoC5F7",
    "content": "The paper introduces a new framework called ML-PIP for data efficient and versatile learning. It extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. \\Versa{} is a framework that uses an amortization network to handle few-shot learning datasets, generating task-specific parameters in a single forward pass. It eliminates the need for optimization at test time by amortizing the cost of inference. \\Versa{} is a framework for few-shot learning that achieves state-of-the-art results on benchmark datasets, handling arbitrary numbers of shots and classes at train and test time. It demonstrates its power through a challenging few-shot ShapeNet view reconstruction task, showcasing rapid adaptation to new datasets at test time. In this paper, a framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) is developed to improve data-efficient learning in few-shot tasks. The framework focuses on amortizing posterior predictive distributions to enhance flexibility and efficiency in learning. The framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) enhances data-efficient learning in few-shot tasks by leveraging shared statistical structure between tasks and sharing information on how to learn. The text discusses hierarchical probabilistic models for multi-task and transfer learning, sharing information between tasks for inference using meta-learning, and enabling fast learning through amortization. A new method called VERSA is proposed to substitute optimization procedures with forward passes through inference networks, resulting in faster test-time performance. VERSATM substitutes optimization procedures at test time with forward passes through inference networks, resulting in faster test-time performance and relieving the need for second derivatives during training. It employs a flexible amortization network for few-shot learning datasets, outputting a distribution over task-specific parameters in a single forward pass. VERSA can handle arbitrary numbers of shots and classes for classification at train and test time. Evaluation on standard benchmarks shows new state-of-the-art results, even in settings where test conditions differ from training. In Section 5, VERSA is evaluated on standard benchmarks, different test conditions, and a one-shot view reconstruction task. The framework includes a multi-task probabilistic model and a method for meta-learning probabilistic inference, guided by discriminative models and shared statistical structure. The standard multi-task directed graphical model in FIG1 utilizes shared parameters \u03b8 and task-specific parameters {\u03c8 for T tasks, aiming to leverage shared statistical structure between tasks. The goal is to meta-learn fast and accurate approximations to the posterior predictive distribution p( for unseen tasks, providing a framework for meta-learning approximate inference. The framework for meta-learning approximate inference involves using point estimates for shared parameters \u03b8 and distributional estimates for task-specific parameters {\u03c8. The probabilistic solution for few-shot learning includes forming the posterior distribution over task-specific parameters and computing it. The solution for few-shot learning involves forming the posterior distribution over task-specific parameters and computing the posterior predictive. The emphasis is on quick approximation at test time, with details on the optimization problem and implementation efficiency provided. The framework approximates the posterior predictive distribution by an amortized distribution q \u03c6 (\u1ef9|D), enabling fast predictions at test time. This involves learning an inference network with parameters \u03c6 that takes training data D (t) and test input x to return the predictive distribution over the test output \u1ef9 (t). Additional approximation steps like Monte Carlo sampling may be needed. The approximate posterior predictive distribution is formed using a factorized Gaussian distribution for q \u03c6 (\u03c8|D (t) ). The training method involves meta-learning the approximate posterior predictive distribution. The quality is measured by the KL-divergence between the true and approximate posterior predictive distribution. The goal of learning is to minimize the KL-divergence between the true and approximate posterior predictive distribution. Training returns parameters that best approximate the posterior predictive distribution in an average KL sense, supporting accurate prediction through meta-learning approximate inference. The amortized procedure meta-learns approximate inference for accurate prediction by selecting tasks randomly, sampling training data, forming posterior predictive distributions, and optimizing log-density. This unbiased estimation process is grounded in Bayesian decision theory. The training procedure involves averaging results for an unbiased estimate of the objective, scoring approximate inference by simulating Bayesian log-likelihood evaluation. The procedure focuses on the posterior predictive distribution and minimizes KL divergence. The training differs from standard variational inference and involves end-to-end stochastic training. The training procedure focuses on the posterior predictive distribution and minimizes KL divergence. It involves end-to-end stochastic training, optimizing an objective over shared parameters to maximize predictive performance. The approach for Meta-Learning involves individual feature extraction, instance pooling, regression onto weights, and episodic train/test splits at meta-train time. The integral over \u03c8 is approximated using Monte Carlo samples, and optimization is enabled by the local reparametrization trick. The learning objective implicitly learns the prior distribution over parameters through q \u03c6 (\u03c8|D, \u03b8). The approach for Meta-Learning involves feature extraction, instance pooling, regression onto weights, and episodic train/test splits. The learning objective implicitly learns the prior distribution over parameters through q \u03c6 (\u03c8|D, \u03b8). This formulation unifies existing approaches and supports versatile learning for rapid and flexible inferences. In Meta-Learning, rapid and flexible inference is achieved through a deep neural network that supports various tasks without retraining. The system allows for quick inference with simple computation while maintaining flexibility for different tasks. The design choices enable flexibility by using sets as inputs for the amortization network. Inference is limited to a single specific task. Design choices enable flexibility by using sets as inputs for the amortization network. The network can process any number of training observations. For few-shot image classification, the probabilistic model parameterization is inspired by early work and recent extensions to deep learning. The probabilistic model for few-shot image classification is inspired by early work and recent extensions to deep learning. It involves a feature extractor neural network shared across tasks, task-specific linear classifiers, and an approximate posterior distribution over weight matrices. However, specifying the number of few-shot classes ahead of time limits inference and makes it challenging to metalearn systems that output large matrices directly. The proposed approach aims to address the limitations of specifying the number of few-shot classes ahead of time by amortizing weight vectors associated with individual feature extraction instances, allowing for context-independent specification. This involves pooling regression onto stochastic inputs and mapping them onto new images through a generator network. The proposed approach involves amortizing weight vectors for individual feature extraction instances, pooling regression onto stochastic inputs, and mapping them onto new images through a generator network. This is achieved by concatenating a test angle with the input and using an amortization network to map image/angle examples to stochastic inputs. The amortization network operates directly on extracted features to reduce the number of learned parameters. The classification matrix is constructed by performing feed-forward passes through the inference network. The assumption of context independent inference is validated theoretically and empirically. The full approximate posterior distributions are shown to be close to their context independent counterparts. The context independent approximation in BID36 BID49 addresses limitations of naive amortization by reducing the number of parameters needed for inference, allowing meta-training with different numbers of classes, and accommodating varying class numbers at test-time. This approach is applied to few-shot image reconstruction tasks. Few-shot image reconstruction involves inferring object appearance from different angles based on limited views. A generative model uses a latent vector and angle representation to generate output images. The decoder of a VAE uses a latent vector and angle representation to generate images at specified orientations. Global parameters are used for the generator network, while task-specific parameters are the latent inputs. A Gaussian likelihood in pixel space is used for the generator outputs, with a sigmoid activation to ensure output means are between zero and one. An amortization network parameterized by \u03c6 processes image representations of an object. The output means are between zero and one, using a sigmoid activation after the final layer. An amortization network parameterized by \u03c6 processes image representations of an object and concatenates them with view orientations before instance-pooling. q \u03c6 (\u03c8|D, \u03b8) produces a distribution over vectors \u03c8 (t). This process unifies various meta-learning approaches as approximate inference in hierarchical models. ML-PIP unifies various meta-learning approaches by connecting gradient and metric-based variants, amortized MAP inference, and conditional modeling. It relies on point estimates for task-specific parameters and compares previous approaches to VERSA. Semi-amortized inference involves taking a step of gradient ascent in training loss with shared inference parameters. This perspective complements Model-agnostic meta-learning, providing a viewpoint as semi-amortized ML-PIP. The one-step gradient parameter update in MAML is justified through MAP inference and the prior p(\u03c8|\u03b8), with episodic meta-train/meta-test splits not falling under this perspective. VERSATILE (VERSA) is a distributional method that simplifies inference by avoiding back-propagation through gradient updates during training and computing gradients at test time. It treats both local and global parameters, enabling the recovery of test-train splits without the need for multiple gradient steps or RNN computations. Metric-Based Few-Shot Learning involves task-specific parameters as top layer softmax weights and biases, with shared parameters as lower layer weights. Amortized point estimates are constructed by averaging top-layer activations for each class, leading to a predictive distribution that recovers prototypical networks using Euclidean distance. Amortized MAP inference involves predicting weights of classes from activations of a pre-trained network to support online learning and transfer tasks. This method utilizes hyper-networks to amortize learning about weights. Amortized MAP inference involves predicting weights of classes from activations of a pre-trained network to support online learning and transfer tasks. VERSA goes beyond point estimates and supports full multi-task learning by sharing information between tasks. The amortization network computes \u03c8 * (D, \u03b8) in cases where task-specific parameters are used. The amortization network computes \u03c8 * (D, \u03b8) for task-specific parameters, supporting multi-task learning. ML-PIP training is equivalent to training a conditional model via maximum likelihood estimation, connecting to neural processes. Comparing to Variational Inference, amortized VI optimizes free-energy w.r.t. \u03c6 and \u03b8. In the multi-task discriminative model, amortized VI BID3 optimizes free-energy with \u03c8 using Monte Carlo approximation. It differs from ML-PIP by not using meta train/test splits and includes KL for regularization. VERSA improves few-shot classification and is compared to VI/meta-learning hybrids. It is evaluated on various few-shot learning tasks, starting with toy experiments to study the properties of amortized posterior inference. In Section 5.2, VERSA is evaluated on few-shot classification tasks using Omniglot and miniImageNet datasets, showcasing its ability to maintain high accuracy with varying shot and way at test time. Additionally, in Section 5.3, VERSA's performance on a one-shot view reconstruction task with ShapeNet objects is examined. An experiment is conducted to investigate the approximate inference during training by generating data from a Gaussian distribution with varying mean across tasks. In an experiment evaluating our training procedure, data is generated from a Gaussian distribution with varying means across tasks. T = 250 tasks are created with N \u2208 {5, 10} train observations and M = 15 test observations. An inference network q \u03c6 (\u03c8|D) is introduced for amortizing inference. The learnable parameters \u03c6 = {w \u00b5 , b \u00b5 , w \u03c3 , b \u03c3 } are trained using an objective function and Adam BID25. The model is trained with mini-batches of tasks and posterior inference is performed on a separate set of tasks. The study involves generating mini-batches of tasks from a dataset and inferring the posterior distribution using amortization parameters. The true posterior over \u03c8 is Gaussian and can be computed analytically. The evaluation demonstrates accurate recovery of posterior distributions despite minimizing predictive KL divergence. VERSA is evaluated on few-shot classification tasks, showing improvements compared to previous methods. VERS follows the implementation in Sections 2 and 3, and the approximate inference scheme in Eq. (5) for few-shot classification tasks on Omniglot and miniImageNet datasets. The experimental protocol established by previous work is followed, showing that the approximate posterior closely resembles the true posterior in both five and ten shot cases. The experiment involves training approximate posteriors for unseen test sets in five and ten shot cases on miniImagenet using equivalent architectures. Training is done episodically with k c examples per task, and results are compared with competitive approaches in few-shot classification performance. VERSATILE achieves new state-of-the-art results (67.37% - up 1.38% over the previous best) on 5-way - 5-shot classification, excluding approaches using pre-training and/or residual networks. The comparison for Omniglot is affected by unspecified training, validation, and test splits in previous methods. VERSATILE achieves new state-of-the-art results on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks using a convolution-based network architecture and end-to-end training. Results are competitive on other benchmarks as well. Results on the Omniglot 20 way -5-shot benchmark show competitive performance with other approaches, with VERSA achieving state-of-the-art results by adapting only the weights of the top-level classifier. Comparison to standard and amortized VI methods reveals substantial improvement with VERSA over amortized VI, despite using the same amortization network. Further experimental details are provided in the appendix. Using non-amortized VI improves performance substantially, but does not reach the level of VERSA. Forming the posterior is significantly slower as it requires many forward/backward passes through the network, similar to MAML. VERSATILE: VERSA allows for varying the number of classes and shots between training and testing, showing high accuracy across different conditions. At test-time, VERSA can handle 100 way conditions with 94% accuracy. It shows flexibility and robustness, requiring fewer forward passes compared to MAML. VERSA outperforms MAML in accuracy by 4.26% and is more than 5 times faster on evaluation tasks. VERSATILE (VERSA) outperforms MAML in accuracy by 4.26% and is more than 5 times faster on evaluation tasks, completing them in 53.5 seconds on a NVIDIA Tesla P100-PCIE-16GB GPU. The dataset used for experiments consists of 37,108 objects from 12 object categories in ShapeNetCore v2 BID5. Each object has 36 views of size 32 \u00d7 32 pixels spaced evenly every 10 degrees in azimuth. VERSA is evaluated by comparing it to a conditional variational autoencoder (C-VAE) with view angles as labels and identical architectures. Training is done episodically for VERSA and in batch-mode for C-VAE on all 12 object classes. Views of unseen objects from the test set are generated and compared to ground truth views. Both VERSA and C-VAE capture the correct details. FIG10 displays unseen objects generated by VERSA and C-VAE from a single shot, compared to ground truth views. VERSA produces sharper, more detailed images with accurate object orientation. Despite occlusion, VERSA can impute missing information. Quantitative comparisons in Table 2 show VERSA's superiority over C-VAE. Table 2 presents comparison results between VERSA and C-VAE, showing VERSA's superiority with increasing shots. Mean squared error (MSE) and structural similarity index (SSIM) are measured for view reconstruction. ML-PIP is introduced as a meta-learning framework, leading to the development of VERSA. ML-PIP is a probabilistic framework for meta-learning that unifies various methods and proposes alternative approaches. VERSA, a few-shot learning algorithm, avoids gradient-based optimization at test time by amortizing posterior inference of task-specific parameters. It demonstrated state-of-the-art performance on few-shot learning tasks and a challenging 1-shot view reconstruction task. Prototypical Networks perform better when trained on higher \"way\" than that of testing, achieving 68.20 \u00b1 0.66% accuracy when trained on 20-way classification and tested on 5-way. The new inference framework in Section 2 is based on Bayesian decision theory, providing a recipe for making predictions for unknown test variables by combining information from observed training data and a loss function. The text discusses Bayesian decision theory (BDT) for making predictions on unknown test variables by combining information from training data and a loss function. It presents a derivation of a stochastic variational objective for meta-learning probabilistic inference. The text presents a derivation of a stochastic variational objective for meta-learning probabilistic inference, grounded in Bayesian decision theory. It generalizes BDT to return a full predictive distribution over unknown test variables, quantified through a distributional loss function. The optimal predictive distribution is found by optimizing the expected distributional loss. The text discusses amortized variational training, where the optimal predictive distribution is found by minimizing expected distributional loss across tasks using shared variational parameters. This allows for quick predictions at test time and direct prediction of unknown variables based on training data. The text discusses amortized variational training, where optimal variational parameters are found by minimizing expected distributional loss across tasks. This approach allows for quick predictions at test time without computing the true predictive distribution, emphasizing the meta-learning aspect of the procedure. The text discusses the use of log-loss in amortized variational training, emphasizing the meta-learning aspect of the procedure. The optimal q \u03c6 is the closest member of Q to the true predictive p(\u1ef9|D) in a KL sense, similar to the wake-sleep algorithm's sleep phase. The text discusses the use of log-loss in amortized variational training, emphasizing the meta-learning aspect of the procedure. It explores alternative proper scoring rules and task-specific losses for future work. The approximate predictive distribution is specified by replacing the true posterior with an approximation motivated by the optimal predictive distribution. The theoretical and empirical justifications for the context-independent approximation are detailed, with a focus on density ratio estimation. The text provides a principled justification for an approximation using density ratio estimation. It shows that the optimal softmax classifier can be expressed in terms of conditional densities, constructing estimators independently for each class. The text discusses the construction of estimators independently for each class using conditional densities, similar to training a naive Bayes classifier. It introduces an experiment to evaluate the context-independent inference assumption in the design. The experiment aims to test the validity of the context-independent inference assumption by conducting free-form variational inference on weights for tasks generated from a dataset. The goal is to see if the distribution of weights for a specific class remains similar regardless of other classes in the task. In an experiment testing context-independent inference, weights for tasks in the MNIST dataset are examined to ensure similarity of a specific class regardless of other classes. The model achieves 99% accuracy on test examples, and t-SNE plot shows clustering of weights in 2 dimensions. After achieving 99% accuracy on test examples in the MNIST dataset, weights for each class in each task are examined. A t-SNE plot reveals clustering of weights in 2 dimensions, with some overlap between classes. For tasks containing both class '1' and '2', class '2' weights are located away from their cluster, suggesting independence of class-weights from the task. The model lacks capacity to assign class weights properly, leading to 'moving' weights to different regions of space. A VI-based objective is derived for the probabilistic model, with \"amortized\" VI parameterized by a neural network and \"non-amortized\" VI optimized independently for each new task. The model uses amortized and non-amortized VI for parameter optimization. The objective function remains the same for both options. An ELBO is used for a single task, with a stochastic estimator derived for optimization. The objective function in Eq. (4) differs from Eq. (C.2) in two key ways. The few-shot classification experiments involve using the Omniglot BID32 dataset with 1623 handwritten characters from 50 alphabets. Images are resized to 28 \u00d7 28 before training. The objective function in Eq. (4) differs from Eq. (C.2) in two important ways. The Omniglot BID32 dataset consists of 1623 handwritten characters from 50 alphabets. Images are resized to 28 \u00d7 28 pixels and character classes are augmented with rotations of 90 degrees. The training, validation, and test sets are split randomly with 1100, 100, and 423 characters respectively. This results in 4400 training, 400 validation, and 1292 test classes, each with 20 character instances. Training for C-way, k c -shot classification is done episodically with batches of tasks selecting C classes randomly from the training set. During training, tasks consist of selecting C classes at random from the training set. K c character instances are used as training inputs and 15 character instances as test inputs. The validation set is used to monitor learning progress and select the best model, while the final evaluation is done on 600 randomly selected tasks from the test set using the Adam BID25 optimizer with a constant learning rate of 0.0001. The Adam BID25 optimizer with a learning rate of 0.0001 is used to train models with k c character instances for both training and testing. Different models are trained for varying numbers of iterations, with the miniImageNet dataset consisting of 100 classes with 600 color images each. The miniImageNet dataset consists of 60,000 color images divided into 100 classes with 600 instances each. Images are 84 \u00d7 84 pixels. Training is done episodically using the Adam optimizer with a Gaussian form for q. Different training setups are used for 5-way -5-shot and 5-way -1-shot models. The 1-shot model is trained with 8 tasks per batch for 50,000 iterations using a constant learning rate of 0.00025. The neural network architectures for the feature extractor \u03b8, amortization network \u03c6, and linear classifier \u03c8 are detailed. The output of the amortization network provides Gaussian parameters for the weight distributions of the linear classifier \u03c8. The local-reparameterization trick is employed when sampling from the weight distributions to reduce the number of learned parameters. Features are shared to reduce parameters. The feature extraction network \u03b8 is shared to reduce parameters in the amortization network \u03c8. The network architecture includes convolutional layers with pooling and dropout. The feature extraction network \u03b8 for miniImageNet few-shot learning consists of multiple convolutional layers with pooling, dropout, and batch normalization. The network architecture includes 4 sets of 3x3 conv2d layers with stride 1 and SAME padding, followed by dropout and pooling layers with stride 2. The output sizes range from 84x84x1 to 5x5x64. The curr_chunk discusses the experimentation details using ShapeNetCore v2 BID5 database for view reconstruction training procedure and network architectures. It mentions the use of 12 object categories and a Linear Classifier (\u03c8) for output size layers and input features. The experimentation involved using 12 of the largest object categories from ShapeNetCore v2 BID5 database. A Linear Classifier (\u03c8) was used for output size layers and input features. The dataset consisted of 37,108 objects, with 70% used for training, 10% for validation, and 20% for testing. Each object had 36, 128x128 pixel image views converted to gray-scale and reduced to 32x32 pixels for training in an episodic manner. The study involved training a model episodically on 36 views of objects from the training set. A modified amortization network was used to generate 36 views for evaluation, with quantitative metrics computed over the test set. The study trained a model episodically on 36 object views using a modified amortization network. Network architectures for encoder, amortization, and generator were described. Training used Adam BID25 optimizer with a learning rate of 0.0001 and 24 tasks per batch for 500,000 iterations.\u03c6 network had an output size of 32 \u00d7 32 \u00d7 1 with specific layers for image processing. The model was trained episodically on 36 object views using a modified amortization network with specific network architectures. The \u03c6 network had an output size of 32 \u00d7 32 \u00d7 1 with layers for image processing."
}