{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are popular for their ease of training, scalability, and lack of a lexicon requirement. Contextual acoustic word embeddings can be constructed directly from these models using attention distribution, showing competitive performance on standard sentence evaluation tasks. Our embeddings, derived from an acoustic-to-word speech recognition model using attention distribution, demonstrate competitive performance on standard sentence evaluation tasks. They also match the performance of text-based embeddings in spoken language understanding tasks. Learning fixed-size representations for variable length data is a key focus in current research in natural language processing. Learning fixed-size representations for variable length data like words or sentences is a key focus in current research in natural language processing. Methods such as word2vec, GLoVE, CoVe, and ELMo have gained popularity for their utility in natural language processing tasks. In speech recognition, the challenge lies in processing short-term audio features instead of words or characters, with variability in speakers, acoustics, and microphones adding to the complexity. The challenge in speech recognition is the variability in speakers, acoustics, and microphones. Previous work focused on learning word representations from variable length acoustic frames by aligning speech and text or segmenting input speech into fixed-length segments. However, these techniques do not capture contextual dependencies in speech. Our work focuses on constructing individual acoustic word embeddings from utterance-level acoustics, unlike previous methods that ignore contextual dependencies in speech. We present various methods for obtaining these embeddings using an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word speech recognition. This model automatically segments and classifies input speech into individual words, eliminating the need for pre-defined word boundaries. Our A2W model automatically segments and classifies input speech into individual words, eliminating the need for pre-defined word boundaries. It learns acoustic word embeddings in the context of their containing sentence and shows usability in spoken language understanding tasks. The model uses attention for aligning words to acoustic frames without forced alignment. The paper demonstrates the usability of attention for aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE). It shows that CAWE is competitive with text-based word2vec embeddings and can be used for transfer learning in speech-based tasks. Pretrained speech models can be used for transfer learning in Spoken Language Understanding, similar to VGG in vision or CoVe in natural language understanding. A2W modeling is pursued using CTC and S2S models. Prior work highlights the need for large training data and vocabularies, but progress shows potential with smaller data and restricted vocabularies. Training speech models with smaller data sets and restricted vocabularies has shown potential, with solutions for generating out-of-vocabulary words by using smaller units like characters or sub-words. Recent work has focused on S2S models for large vocabulary A2W recognition, with advancements in training methods for these models. In the 300 hour Switchboard corpus, BID24 BID12 improve training for large vocabulary tasks. BID12 shows the A2W model can learn word boundaries without supervision. BID4 BID5 BID7 BID25 BID8 explore acoustic word embeddings using unsupervised methods, while BID6 uses supervised learning. BID6 is the only method using supervised learning for speech recognition, with short speech frames as input. BID4 proposes an unsupervised method for learning speech embeddings using a fixed context of words in the past and future. The curr_chunk discusses a method for learning speech embeddings using a fixed context of words, highlighting drawbacks such as the need for forced alignment. It also mentions research on text-based word embeddings and contextualized word embeddings. The work ties a speech recognition model with learning contextual word embeddings from speech. The curr_chunk discusses the model's encoder used in downstream tasks, specifically focusing on the A2W speech recognition model and learning contextual word embeddings from speech. The encoder is a pyramidal multi-layer bi-directional LSTM network, while the decoder network models the output distribution conditioned on the target sequence. The decoder network in the model is an LSTM network that learns to model the output distribution over the next target based on previous predictions. It uses an attention mechanism to generate targets from the input. The attention mechanism enforces monotonicity in alignments by applying a convolution across time. The model uses a convolved attention feature to calculate attention for the current time step, leading to a peaky distribution. Acoustic word embeddings are obtained from an end-to-end trained speech recognition system, using hidden representations from the encoder and attention weights from the decoder. The model utilizes a convolved attention feature to calculate attention for the current time step, resulting in a peaky distribution. Acoustic word embeddings are generated from the encoder's hidden representations and the decoder's attention weights. The method for constructing \"contextual\" acoustic word embeddings is similar to CoVe BID2, with a focus on aligning input speech with output words using a location-aware attention mechanism in an A2W model. The A2W model utilizes location-aware attention to segment speech into words and generate word embeddings based on the importance of acoustic frames. This process constructs contextual acoustic word embeddings by weighing hidden representations. The A2W model uses location-aware attention to segment speech into words and generate word embeddings based on the importance of acoustic frames. This process constructs contextual acoustic word embeddings by weighing hidden representations of acoustic frames. The A2W model utilizes location-aware attention to segment speech into words and generate word embeddings based on the importance of acoustic frames. It constructs contextual acoustic word embeddings by weighing hidden representations of acoustic frames. The model obtains mappings of words to acoustic frames and describes three different ways of using attention to obtain acoustic word embeddings. The Contextual Acoustic Word Embeddings (CAWE) are generated using attention weights to combine hidden representations of acoustic frames for a specific word. Three techniques are used: unweighted combination, attention weighted average, and maximum attention. These techniques provide contextual embeddings by considering attention scores across all acoustic frames for a word. The Contextual Acoustic Word Embeddings (CAWE) are generated using attention scores over all acoustic frames for a given word. Two datasets are used: the Switchboard corpus with telephonic conversations and the How2 dataset with instructional videos. The How2 dataset contains outdoor recordings with distant microphones, unlike the indoor telephony speech of Switchboard. The Contextual Acoustic Word Embeddings (CAWE) are evaluated on various tasks using datasets from Switchboard and How2. The A2W achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome set. The embeddings are used in 16 benchmark sentence evaluation tasks covering Semantic Textual Similarity, classification, sentiment analysis, question type, and more. The CAWE are evaluated on tasks using datasets from Switchboard and How2, achieving word error rates. The A2W embeddings are used in benchmark sentence evaluation tasks including Semantic Textual Similarity, classification, sentiment analysis, and question type. The downstream evaluations involve classification tasks using logistic regression. The SentEval toolkit BID26 is used to evaluate classification accuracies. Logistic regression is used for classification in downstream evaluations, using the concatenation of CAWE and CBOW as features without adding tunable embedding parameters. CAWE-M outperforms U-AVG and CAWE-W by significant margins on Switchboard and How2 datasets in terms of average performance on STS tasks. In results from TAB1, CAWE-M outperforms U-AVG and CAWE-W by 34% and 13% on Switchboard and How2 datasets for STS tasks. CAWE-W usually performs worse than CAWE-M due to noisy estimation of word embeddings. U-AVG performs worse than CAWE-W on STS and SICK-R tasks. The attention score is calculated over acoustic frames for a word. U-AVG performs worse than CAWE-W on STS and SICK-R tasks due to its noisier construction process. Datasets for downstream tasks are the same as described in Section 5.1. Training details involve comparing embeddings from the speech recognition model's training set and text-based word embeddings from a CBOW word2vec model trained on all transcripts. The A2W speech recognition model has a limited vocabulary, recognizing only a small number of words. Despite this limitation, the performance of CAWE is competitive with word2vec CBOW. The CAWE-M embeddings extracted from Switchboard training outperform word2vec embeddings on 10 out of 16 tasks, showing that the acoustic embeddings combined with text embeddings improve performance. The gains are more significant in Switchboard compared to How2 due to the nature of the speech in each dataset. The CAWE-M embeddings from Switchboard training outperform word2vec embeddings on 10 out of 16 tasks, showing improved performance by combining acoustic and text embeddings. Evaluation on the ATIS dataset for Spoken Language Understanding (SLU) is also conducted, which is similar in domain to Switchboard, making it a useful test bed for evaluating CAWE on a speech-based downstream task. The model architecture for the task is similar to a simple RNN-based model investigated in BID29, consisting of an embedding layer, a single layer RNN variant (Simple RNN, GRU), a dense layer, and softmax. The model is trained for 10 epochs with RMSProp (learning rate 0.001) and tested 3 times with different seed values for average performance evaluation. The study compared text-based word embeddings with speech-based word embeddings on the ATIS dataset. Results showed that speech-based embeddings performed equally well, highlighting their utility. Different embeddings were tested and fine-tuned for the task. The study compared text-based word embeddings with speech-based word embeddings on the ATIS dataset, showing that speech-based embeddings performed equally well. Different embeddings were tested and fine-tuned for the task, including CAWE-M, CAWE-W, and CBOW embeddings. A method to learn contextual acoustic word embeddings from a sequence-to-sequence acoustic-to-word speech recognition model was presented, with attention playing a key role. The acoustic embeddings were found to be competitive with word2vec (CBOW) text embeddings and outperformed the simple unweighted average method by up to 34% on semantic textual similarity tasks. The embeddings also matched the performance of text-based embeddings in spoken language understanding. The study compared text-based word embeddings with speech-based word embeddings on the ATIS dataset, showing that speech-based embeddings performed equally well. Contextual audio embeddings matched the performance of text-based embeddings in spoken language understanding and improved downstream tasks by up to 34% on semantic textual similarity tasks. The model can be used as a pre-trained model for other speech-based tasks, despite the complexity of noisy audio input. Future work will focus on scaling the model to larger corpora and vocabularies, and comparing with non-contextual acoustic word embedding methods. This work, supported by the Center for Machine Learning and Health at Carnegie Mellon University and Facebook, aims to scale the model to larger vocabularies and compare with non-contextual acoustic word embedding methods."
}