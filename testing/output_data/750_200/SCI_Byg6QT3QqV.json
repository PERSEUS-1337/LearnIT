{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the focus is on generating online explanations during execution to reduce human mental workload. The challenge lies in the interdependence of different parts of an explanation, requiring a careful approach to online explanation generation. Three implementations with varying online properties are presented based on a model reconciliation setting. Evaluation involves human subjects in a standard planning scenario. Our explanation generation method is based on a model reconciliation setting and is evaluated with human subjects in a planning competition domain and in simulation with ten different problems across two domains. As intelligent robots become more prevalent, human-AI interaction is crucial. Explanations from AI agents help maintain trust and shared awareness. Prior work on explanation generation often overlooks the recipient's need to understand. A good explanation should be generated. The agent should consider discrepancies between human and its own model when generating explanations. Explanation becomes a request for human to adjust model reconciliation setting. M R represents robot's model, M H represents human's model of expectation. The robot should explain model differences to reconcile human expectations. This process is termed model reconciliation. In the presence of model differences, model reconciliation is necessary. One issue is the lack of consideration for the mental workload of humans in understanding explanations. Explanations, especially complex ones, should be provided online to reduce mental workload. Online explanations reduce mental workload by spreading out information smoothly, ensuring parts are not dependent on each other. This process intertwines communication with plan execution, illustrated through a scenario between friends Mark and Emma. Mark and Emma have different study session preferences. Mark likes to break the session into two parts with lunch in between, while Emma prefers one continuous session. If Mark had explained his plan to Emma beforehand, she would have suggested ordering takeout for lunch before the session. Mark prefers to split study sessions with lunch in between, while Emma likes continuous sessions. If Mark had explained his plan to Emma, she would have suggested ordering takeout before the session. Mark goes to the library with Emma, studies for 60 minutes, then suggests lunch for energy. He doesn't mention needing a walk until after lunch to avoid Emma suggesting he take a walk alone. This highlights the importance of communication in planning. Mark prefers splitting study sessions with lunch in between, while Emma prefers continuous sessions. Mark gradually explains his plan to Emma during execution, considering their different values. The key is to explain minimally and only when necessary, reducing mental workload. The paper introduces a new method called online explanation, which integrates explanation with plan execution to reduce mental workload. Three approaches for online explanation generation were implemented. The paper introduces a new method called online explanation, integrating explanation with plan execution to reduce mental workload. Three approaches for online explanation generation were implemented, focusing on different \"online\" properties such as matching the plan prefix, making the next action understandable, and matching the prefix of the robot's plan with any possible optimal human plan. A model search method ensures that earlier information communicated does not affect later parts of the explanation, creating a desirable experience for the recipient. The paper introduces a new method called online explanation, integrating explanation with plan execution to reduce mental workload. Our approaches are evaluated with human subjects and in simulation. AI agents are limited in their ability to operate as a teammate, requiring transparency to other team members. Explainable AI is crucial for human-AI collaboration, as it helps improve human trust in AI agents and maintains shared situation awareness by explaining the decision-making process. The effectiveness of explainable agency is measured by its accuracy in modeling human perception of the AI agent. The agency's effectiveness in explainable AI is assessed based on its ability to accurately model human perception of the AI agent, including the perception of other agents. This allows the AI agent to generate understandable motions, plans, and assistive actions. Cost optimality is often replaced with a metric that considers both cost and explicability. Additionally, the AI agent can signal its intention before execution to improve collaboration with humans. The model can be used for intention signaling and generating explanations to improve human understanding of AI behavior. Research focuses on generating the right explanations based on the recipient's perception model. The research focuses on generating the \"right\" explanations based on the recipient's perception model, but it ignores the mental workload required for understanding. The ordering of information in an explanation can influence perception, and sometimes explanations need to be made in an online fashion, especially for complex explanations. In this work, the focus is on online explanation generation for complex explanations intertwined with plan execution, based on the model reconciliation setting. The goal is to provide a minimal amount of information to explain the current part of the plan, closely associated with planning problems. Our problem is closely associated with planning problems, defined as a tuple (F, A, I, G) using PDDL. F is the set of predicates specifying the world state, A is the set of actions changing the state. Actions have preconditions, add and delete effects. I, G are initial and goal states. DISPLAYFORM0 and \u03c0 * I,G represent the robot's plan. Cost(\u03c0 * I,G , M R ) is the plan cost using M R, and cost * M R (I, G) is the optimal plan cost. The robot's plan, \u03c0 * I,G, must be optimal according to model reconciliation (M R), considering rational agents. Model reconciliation involves the human's model (M H) to match the expected behavior. Reconciliation occurs when the robot's plan matches the human's expectations. Explanation generation in a model reconciliation setting involves updating the human's model (M H) to align with the robot's plan (\u03c0 * I,G) for optimal explainability. A mapping function in BID6 converts planning problems into a feature space to facilitate this reconciliation process. Explanation generation involves converting planning problems into a feature space using a mapping function in BID6. An explanation is a set of unit feature changes that reconcile two models by reducing the cost difference between human and robot plans. Explanation generation in BID6 involves creating complete explanations that minimize unit feature changes between human and robot plans. The previous approach considers both models but overlooks the human's mental workload in understanding the explanation. Online explanation generation addresses the issue of mental workload in understanding explanations by providing minimal information during plan execution. It involves creating sub-explanations at specific steps in the plan to explain the part of interest that is not easily explainable. Online explanation generation involves creating sub-explanations at specific steps in the plan to explain the part of interest that is not easily explainable. The robot can split an explanation into multiple parts, made in an online fashion as the plan is executed. Three different approaches of online explanation generation are provided, focusing on different aspects intertwined with plan execution. Online explanation generation involves creating sub-explanations at specific steps in the plan to explain the part of interest that is not easily explainable. The process converts the problem of explanation generation to the problem of model search in the space of possible models, considering how model changes affect human expectations. The challenge lies in the interdependence of model changes, which may impact future explanations. The challenge in online explanation generation is ensuring that model changes do not affect future explanations. This is addressed by searching for the largest set of model changes that will not alter plan prefixes after further sub-explanations. The search process illustrated in FIG1 involves finding an OEG-PP, which is a set of subexplanations that do not change the plan prefix. This process is performed recursively for each sub-explanation, ensuring that model changes do not affect future explanations. Our approach involves searching for a sub-explanation starting from the robot model, stopping when the plan prefixes match. This process is similar to MME but requires running multiple times compared to only once in MME, making it more computationally expensive. Our approach involves searching for a sub-explanation starting from the robot model, stopping when the plan prefixes match. This process is more computationally expensive than MME as it requires running multiple times. The dotted line represents the border of the maximum state space model modification in the robot model, reconciling the two models up to the current plan execution. Maximum updates to the robot model correspond to minimum updates to the human model. The updates to the robot model aim to minimize changes to the human model. By finding the largest set of model changes that match the plan prefixes, we ensure compatibility for future steps. This recursive process ensures that optimal plans can be generated following the search process. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. By starting off with finding the difference between two models M DISPLAYFORM1 and M R, and modifying M R with respect to M H, we aim to find the largest set of model changes that can satisfy constraints. This process continues until the human's plan matches with that of the robot. The algorithm aims to modify M R with respect to M H to find the largest set of model changes that satisfy constraints. The goal is to ensure the robot and human plans have the same prefix during plan execution, relaxing the condition to reconcile between M R and M H for the next action. The algorithm modifies M R to align with M H for the next action in the plan, considering the human's limited cognitive memory span. It focuses on reconciling differences between the most recent human plan and the robot's plan. The algorithm reconciles differences between the most recent human plan and the robot's plan by performing a search from M H \\M H, explaining only the immediate next action that does not match. The OEG algorithm explains the immediate next action that does not match between human and robot plans, combining search from M H and M R for better performance. The robot is assumed to have only the right plan in the OEG-PP approach. The OEG algorithm aims to reconcile differences between human and robot plans by searching for optimal plans. The robot is assumed to have the correct plan in the OEG-PP approach, but this assumption is relaxed to allow for a set of optimal plans. The goal of OEG is to satisfy the human optimal plan generated from the original human model. The OEG aims to reconcile differences between human and robot plans by searching for optimal plans. To address this, a compilation approach is implemented to ensure that the robot's plan prefix matches with the human's model. The OEG aims to reconcile differences between human and robot plans by searching for optimal plans. A compilation approach is implemented to ensure that the robot's plan prefix matches with the human's model, by adding predicates to actions in the compiled model. The compilation approach reconciles differences between human and robot plans by adding predicates to actions in the model space. The agent checks for a human optimal plan that matches the robot's plan up to the next action using this approach. The agent checks for a human optimal plan that matches the robot's plan up to the next action using a compilation approach. This process continues until an optimal human plan exists that matches the robot's plan. The approach was evaluated for online explanation generation with human subjects and in simulation, comparing results with Minimally Complete Explanation (MCE). The study evaluated an approach for online explanation generation with human subjects and in simulation, comparing results with Minimally Complete Explanation (MCE) BID6 approach. The goal was to see the differences in information needed and computation time between online explanation and MCE. The approach was tested on ten different problems in the rover and barman domains. Differences between M H and M R were made by randomly removing preconditions from model features. The human subject study aimed to confirm the benefits of online explanation. The study evaluated online explanation generation with human subjects in a rover domain. The differences between M H and M R were made by randomly removing preconditions from model features. The goal was to confirm the benefits of online explanation generation, hypothesizing that it would reduce mental workload and improve task performance. The rover must calibrate its camera before taking images and communicate results to the base station. It can only store one sample at a time and must drop the current sample to take another. In a different domain, the robot serves drinks using dispensers, glasses, and a shaker. The robot in the barman domain serves drinks using drink dispensers, glasses, and a shaker. Constraints include grabbing one object with an empty hand, grabbing one object with one hand, and ensuring a glass is empty and clean before filling it with a drink. Simulation results compare different explanation approaches for rover and barman domain problems. In the barman domain, there are 5 problems being analyzed. The explanation approaches for rover and barman domain problems are compared based on the number of model features shared at each time step. OEG focuses on minimal information per time step, resulting in more total features shared in OEG-PP and OEG-NA compared to MCE. The comparison between OEG-PP, OEG-NA, MCE, and OEG-AP in the barman domain shows that OEG-PP and OEG-NA share more total information due to feature dependence and planner behavior. OEG-AP considers all optimal plans, highlighting the difference in plan action distance between robot and human plans. In the comparison between OEG-PP, OEG-NA, MCE, and OEG-AP in the barman domain, there is a remaining distance between the robot's plan and the human's plan in terms of plan action distance. OEG-NA only considers the immediate next action, while OEG-AP considers all optimal human plans but does not guarantee a match with the robot's plan. The plan distance gradually decreases between the two plans in OEG approaches as execution and explanation are intertwined. In OEG approaches, the plan distance between the robot's plan and the human's plan gradually decreases during execution, leading to a smoother adjustment for the robot. Model updates are sorted based on feature size, with backtracking performed if needed. The search process in online explanation generation takes advantage of information not affecting previous sub-explanations. A human study compared three approaches for explanation generation, including breaking information into multiple pieces. The experiment was conducted using Amazon Mechanical Turk with 3D simulation. The experiment involved using Amazon Mechanical Turk with 3D simulation to test a new approach called MCE-R, where human subjects act as rover commanders in a Mars mission scenario. Each subject had a 30-minute time limit to complete the task with explanations given in plain English and rover actions shown through GIF images. In an experiment, human subjects act as rover commanders on Mars, assessing the rover's actions with explanations provided. Additional spatial puzzles are included to increase cognitive demand. Certain information is deliberately omitted to observe the subjects' decision-making process. In an experiment, human subjects act as rover commanders on Mars, assessing the rover's actions with explanations provided. Additional spatial puzzles are included to increase cognitive demand. Certain information is deliberately omitted to observe the subjects' decision-making process, leading to scenarios where hidden information affects their actions and explanations. In an experiment, human subjects act as rover commanders on Mars, assessing the rover's actions with explanations provided. The robot shares information at the beginning of the task in one setting, while in another setting, information is communicated at different steps. Different approaches of online explanation generation are used in each setting, intertwining explanation with plan execution. Subjects are asked to determine if the robot's actions make sense. The subjects in the experiment evaluate the robot's actions with explanations provided at different steps. The explanations are generated based on BID6 and online approaches. NASA Task Load Index is used to assess the efficiency of the explanation approaches. NASA TLX is a tool used to evaluate human-machine interface systems by measuring mental workload through different variables. It calculates an overall mental workload score based on sub-scales like mental demand, temporal demand, performance, effort, and frustration. Physical demand questions were excluded in the experiment. The study recruited 150 human subjects on MTurk for an academic survey evaluating mental workload using NASA TLX. Criteria included a HIT acceptance rate >98%, resulting in 94 valid responses. Subjects were aged 18-70, with 29.8% female participants. The study involved 150 human subjects aged 18-70, with 29.8% female participants. The research focused on how well subjects understood the robot's plan through different explanations, comparing distances across five settings. The distance metric calculated the ratio of questionable actions to total actions in a plan, with lower values indicating better understanding. Averaged results were calculated for each setting. The study compared different explanation approaches to understand the robot's plan. Overall, OEG approaches were found to reduce human mental workload better than MCE approaches, as shown by subjective NASA TLX measures. OEG approaches also created more temporal demand during plan execution. FIG6 presents objective performance measures and subjective results. The OEG approaches in the experiment showed lower questionable actions and higher accuracy in identifying correct actions compared to MCEs, indicating more trust towards robots. OEG-AP had the least questionable actions and highest accuracy. The study also presented p-values for mental load based on the results. The study compared OEG approaches with MCEs, showing OEG-AP had the least questionable actions and highest accuracy. There was a significant difference in mental workload between OEG approaches and MCEs. Time analysis revealed OEG-NA had the shortest task completion time. In this paper, a novel approach for explanation generation was introduced to reduce mental workload during human-robot interaction. The approach involves breaking down complex explanations into smaller parts and conveying them in an online fashion while intertwined with plan execution. The overall time taken for task completion varied among different categories, with OEG-NA having the shortest time. There were no significant differences in accuracy between the approaches. In this study, a new approach for explanation generation was introduced to reduce mental workload during human-robot interaction. Three different approaches were provided, each focusing on one aspect of explanation generation weaved in plan execution. Evaluation using simulation and human subjects showed that the approaches improved task performance while reducing mental workload."
}