{
    "title": "HJcjQTJ0W",
    "content": "Massive data on user local platforms cannot support deep neural network (DNN) training due to resource constraints. Cloud-based training poses privacy risks from excessive data collection. To enable cloud-based DNN training while protecting data privacy, we propose using intermediate representations by splitting DNNs between local platforms and the cloud. Local NN generates feature representations from pre-trained NNs to avoid local training and protect data privacy. Cloud NN is deployed separately. The local neural network generates feature representations using pre-trained NNs to avoid local training and protect data privacy. The cloud NN is then trained based on these representations for the target learning task. The idea of DNN splitting is validated by analyzing the relationship between privacy loss and classification accuracy on the local NN topology for image classification tasks. PrivyNet is proposed to optimize the local NN topology for the target learning task while considering privacy loss, local computation, and storage constraints. PrivyNet is proposed to optimize local neural network topology for target learning tasks while considering privacy loss, local computation, and storage constraints. It demonstrates efficiency and effectiveness with the CIFAR-10 dataset, addressing the challenge of computationally intensive training processes on local platforms. Cloud-based services offer an alternative for deep model training but raise privacy concerns due to excessive user data collection. Cloud-based services offer an alternative for deep model training but raise privacy concerns due to excessive user data collection. To protect user data privacy, different data pre-processing schemes are proposed, where transformed representations are generated locally and uploaded for learning tasks, meeting requirements of utility and privacy. The utility and privacy requirements for data pre-processing schemes involve ensuring accurate task completion and limiting private information leakage. The transformation scheme must be adaptable for various platforms and data types. Privacy and utility trade-off is a key focus in privacy research, with different measures proposed based on rate-distortion theory. Privacy and utility trade-off is a key focus in privacy research, with different measures proposed based on rate-distortion theory. Various transformations have been suggested to explore this trade-off, including syntactic anonymization methods like k-anonymity, l-diversity, and t-closeness for protecting sensitive attributes in static databases. However, applying syntactic anonymization to high-dimensional continuous data is challenging. Anonymizing quasiidentifiers and protecting sensitive attributes in static databases is challenging for high-dimensional continuous data. Differential privacy offers a formal privacy guarantee by adding noise, but it does not limit total information leakage from released representations. Existing works typically involve local platforms to achieve differential privacy. Existing works aim to achieve differential privacy by involving local platforms in the backward propagation process, making deployment on lightweight platforms difficult. Non-invertible linear and non-linear transformations are proposed for data anonymization, with linear transformations relying on covariance or linear discriminant analysis for filtering training data. However, linear transformations may not provide sufficient privacy protection as the original data can be reconstructed from released representations. Recently, nonlinear transformations have been suggested as an alternative approach. The proposed PrivyNet framework utilizes nonlinear transformations for better privacy protection, but it requires an iterative training scheme between cloud and local platforms. This framework controls the trade-off between privacy and utility by the topology of the local neural network. PrivyNet is a DNN training framework that divides a model into local and cloud parts for privacy and utility trade-off control. The local NN extracts features while the cloud NN is trained for the learning task. This approach ensures privacy protection through transformations on the local platform. The privacy protection in PrivyNet is achieved through non-linear transformations by the local NN, derived from pre-trained NNs to preserve useful features while ensuring privacy. PrivyNet is a framework that splits DNN models for cloud-based training with fine-grained privacy control. It characterizes privacy loss and utility using CNN as the local NN, identifying key factors for the trade-off. A hierarchical strategy is proposed to optimize the utility by determining the topology of the local NN. Three key factors determining the privacy and utility trade-off are compared. A hierarchical strategy is proposed to optimize the utility of the local NN considering constraints on computation, storage, and privacy loss. PrivyNet is validated using CNN-based image classification, demonstrating efficiency and effectiveness in leveraging pre-trained NN for intermediate representation generation. The overall characterization flow for CNN-based image classification involves generating feature representations using a pre-trained NN, training an image classification network (ICN) based on these features, and training an image reconstruction network (IRN) to reconstruct original images. Utility is measured by target learning task accuracy, and privacy is measured by the distance between reconstructed and original images. Training the IRN assumes access to both original and reconstructed images. The IRN is trained with access to original images and feature representations, while the transformation FEN is unknown. The adversarial model will be detailed in later sections. The collection of training instances is represented by DISPLAYFORM0 DISPLAYFORM1, with each image having D channels of dimensions W \u00d7 H. The label indicator vector y_i \u2208 {0, 1} K denotes correct labels for images. The transformation t : R W \u00d7 H \u00d7 D \u2192 R W \u00d7 H \u00d7 D is induced by the IRN. The transformation t induced by the FEN is parameterized by the number of layers and filters selected. The output feature representations have depth D and dimension W \u00d7 H. The utility is evaluated by learning a classifier h* from the transformed representations. The utility of transformed representations is evaluated by learning a classifier h* that minimizes empirical risk for the target learning task. The accuracy achieved by h* is used to measure utility, with better accuracy indicating better utility. Privacy is evaluated by learning a reconstruction model g* that minimizes the distance between the reconstructed representations. The privacy of transformed representations is evaluated by learning a reconstruction model that minimizes the distance between the reconstructed images and the original images. The privacy loss is measured using the peak signal-to-noise ratio (PSNR) of the reconstructed images compared to the original images, where a larger PSNR implies a larger privacy loss. The impact of different PSNR values is shown through reconstructed images in Appendix F. The FEN topology's impact on privacy and utility of transformed representations is characterized using VGG16 BID20 pre-trained on Imagenet dataset BID17. The FEN is derived from VGG16, with CNN for image classification task and g for reconstruction task. The topology is determined by number of layers, depth of output channels, and subset of factors. Architecture details are provided in Appendix B. The FEN topology's impact on privacy and utility is evaluated by changing the number of layers and output depth. ICN and IRN are trained based on the generated representations to assess utility and privacy, as shown in FIG2. Both utility and privacy are affected by the changes in the FEN topology. The impact of the FEN topology on privacy and utility is evaluated by changing the number of layers and output depth. Privacy loss is observed with smaller PSNR of reconstructed images, indicating less privacy loss with reduced output depth or increased FEN layers. Accuracy shows minimal degradation with reduced output depth and remains stable with increased FEN layers. The trade-off between accuracy and PSNR in the PrivyNet framework is shown in FIG2 (c). Two key observations are highlighted: 1) FEN with different topologies have similar utility when privacy loss is high, and 2) FEN with more layers provide better utility when privacy loss is low. The number of FEN layers and output channel depth impact privacy and utility. Channel selection affects output representations. Utility and privacy loss for each channel in a 4-layer FEN are compared. Results are shown in Figure 4 (a). The utility and privacy loss for representations generated by each channel in a 4-layer FEN are compared in Figure 4 (a) and detailed statistics are in Table 4 (c). When m = 4, the best channel achieves 4X utility compared to the worst channel, with a privacy loss difference of 6 dB. Similar discrepancies are observed with 6 VGG16 layers in Figure 4 (b). The impact of output channel selection is compared with the impact of the FEN layers. The impact of output channel selection is compared with the number of FEN layers and output depth. Privacy and utility show larger dependence on the number of FEN layers and output depth compared to output channel selection. The comparison in FIG4 shows the impact of FEN layers and output channel depth on privacy and utility. Pre-trained CNN can be used to control the trade-off between privacy and utility by adjusting FEN topology. The number of FEN layers, output channel depth, and output channel selection influence the privacy and accuracy trade-off. The number of FEN layers, output channel depth, and output channel selection significantly impact the trade-off between privacy and utility. A framework called PrivyNet is proposed to optimize utility while considering privacy constraints, local computation capability, and storage. The FEN topology can be determined using PrivyNet to control the trade-off effectively. Additionally, the idea of deriving the FEN from a pre-trained NN has been validated in Section 2. The FEN topology, derived from a pre-trained NN, impacts privacy, utility, computation, and storage on local platforms. For lightweight platforms like mobile devices, constraints on computation and storage are crucial. To optimize utility under privacy, computation, and storage constraints, the PrivyNet framework is utilized. The PrivyNet framework focuses on privacy, local computation, and storage. It involves privacy characterization using cloud-based services and performance profiling of neural networks on local platforms. The FEN topology is determined based on constraints and a supervised channel pruning step is conducted to optimize privacy and learning tasks. The FEN topology is determined based on constraints and a supervised channel pruning step is conducted to optimize privacy and learning tasks by selecting output channels randomly. The assumption of availability of original images is crucial for worst-case evaluation of privacy loss induced by releasing feature representations. In our adversarial model, we assume the attackers may inject images into a database to obtain corresponding representations generated by the FEN. It is crucial to protect the anonymity of the FEN, as sophisticated image reconstruction mechanisms could be used if the FEN is known to the attackers. The FEN anonymity protection is crucial to prevent attackers from accessing the architecture and weights of pre-trained NNs. The pre-characterization stage involves performance and storage profiling on local platforms and cloud-based privacy characterization for the pre-trained NNs. Different platforms have varying computation capability and storage configurations, which directly impact the FEN topology. The FEN anonymity protection is essential to prevent attackers from accessing pre-trained NNs. Privacy characterization is done by leveraging cloud-based services, and the reconstruction network is trained on publicly available data. PSNR comparisons are made for different datasets like CIFAR-10 and CIFAR-100 to verify the feasibility of the characterization. Privacy characterization for FEN is conducted on datasets like CIFAR-10 and CIFAR-100, comparing PSNR with different topologies. Experiments show less than 1000 samples are needed for accurate characterization with data augmentation. Detailed PSNR values can be less accurate since it is not the target learning task, reducing the training sample requirement. In PrivyNet, the topology for the FEN is determined based on the number of layers and output channel depth, considering constraints on local computation, storage, and privacy loss. The relation between privacy, local computation, and storage is shown in Figure 8 (a) and 8(b) on a mobile device. In PrivyNet, the FEN topology is determined based on constraints on local computation, storage, and privacy loss. The strategy involves selecting the deepest layer for the FEN when high privacy is required, and adjusting output depth based on privacy constraints. When privacy requirements are low, FENs with different characteristics are considered. When privacy requirements are low, a shallow FEN with minimal local computation and storage consumption is selected based on privacy constraints. For example, a shadow FEN with m = 1 and D = 4 can achieve the required privacy level with a large allowed PSNR of 28 dB. This approach helps to minimize local resources while maintaining privacy. After selecting a shadow FEN with m = 1 and D = 4 to meet low privacy requirements, the number of layers and output depth are determined. Choosing m = 6 and D = 4 improves utility. Output channel selection is crucial, as shown in Figure 4, where discrepancies in utility and privacy are observed for a single channel. Changing layers or increasing channels also impacts privacy and utility. In Figure 4, discrepancies in utility and privacy are observed for a single channel. Similar differences are seen when changing layers or increasing output channel depth in Figure 9. Directly selecting output channels from the whole set can lead to large variances in utility and privacy, necessitating channel pruning. Additionally, there is a negligible correlation between utility and privacy loss for a single channel, as shown in Figure 10. In Figure 4, discrepancies in utility and privacy are observed for a single channel, while Figure 10 demonstrates a negligible correlation between the two. This observation is crucial for optimizing utility and suppressing privacy loss during channel pruning. In channel pruning, both utility and privacy are considered. Privacy loss for each channel is determined from offline pre-characterization to prune channels with the highest privacy loss. Fisher's LDA is used to identify channels with poor utility by analyzing the distance of representations within and between classes using the covariance matrix. In Fisher's LDA scheme, the distance between representations of the same class should be small, while the distance among different classes should be large. It is a good criterion for identifying ineffective channels by leveraging the covariance matrix. The average representations for each class are denoted as z_k, and Fisher's criterion is evaluated for each channel based on the number of data points in each class. In Fisher's LDA scheme, the average representations for each class are denoted as z_k. Fisher's linear discriminability for the j-th output channel can be computed using between-class and within-class variances. The maximum value is achieved when p is the eigenvector corresponding to the largest eigenvalue of S w S b. The Fisher's discriminability is used to identify ineffective channels for pruning in the LDA-based supervised channel pruning algorithm, aiming to improve accuracy for the learning task. The experimental setup involves using the first 6 VGG16 layers to form the FEN and pruning the 32 output channels with the worst utility. The LDA-based supervised channel pruning algorithm uses Fisher's discriminability to identify ineffective channels in the first 6 VGG16 layers. Pruning 32 output channels with the worst utility results in a 33.5% reduction in the probability of selecting a bad channel compared to random pruning methods. Similar results are seen when pruning 64 channels with the worst utility. The LDA-based pruning algorithm reduces ineffective channels in the first 6 VGG16 layers. Pruning 32 or 64 channels with the worst utility shows similar results. The computation complexity scales with the number of samples required for pruning. The LDA-based pruning process in the study shows that the computation complexity scales with the number of samples. The effectiveness of supervised channel pruning is demonstrated by setting the layer of FEN to 6 and output depth to 8. Three settings are compared for privacy and utility: random selection, channel pruning based on privacy and utility characterization, and channel pruning based on privacy characterization and LDA. The study compares three settings for channel pruning: random selection, pruning based on privacy and utility characterization, and pruning based on privacy characterization and LDA. Results show that random selection without pruning has higher average PSNR compared to the other two settings. The study compares three settings for channel pruning: random selection, pruning based on privacy and utility characterization, and pruning based on privacy characterization and LDA. Results show that LDA-based pruning achieves better accuracy and smaller PSNR compared to random selection and pruning based on characterization results. Our supervised pruning strategy achieves similar accuracy to pruning based on characterization results with slightly less privacy loss. The effectiveness of our method is verified through utility and privacy comparison in three settings. The adversarial model adopted in the paper is discussed in detail. In this section, the paper discusses the LDA-based pruning method and the adversarial model adopted. The transformation induced by the FEN is assumed to be unknown to attackers to prevent powerful attacks and enhance privacy protection. Strategies are provided to protect the anonymity of the FEN, including building a pool of pre-trained NNs for FEN derivation. The effectiveness of the method is verified through utility and privacy comparison in different settings. In the protection of the FEN, methods include building a pool of pre-trained NNs like VGG16, VGG19, ResNet, and Inception. Channel selection is also applied to make it harder for attackers to guess the FEN derivation channels. In the protection of the FEN, methods involve using pre-trained NNs like VGG16, VGG19, ResNet, and Inception. Channel selection is applied to increase the difficulty for attackers to identify the FEN derivation channels. Privacy and utility are empirically verified by gradually reducing the channel depth of the first convolution layer from 64 to 16 in the first 6 layers of VGG16. After verifying privacy and utility by gradually reducing channel depth in VGG16, further reductions in channel depth for each layer show similar results with significant runtime reduction. Channel selection for intermediate layers enhances protection against attackers. PrivyNet is a flexible framework designed to protect the anonymity of the FEN while enabling cloud-based training and providing privacy protection. It helps solve resource constraints and policy limitations, making it useful for applications like modern hospitals. PrivyNet addresses resource constraints, lack of knowledge/experience, and policy limitations for applications like modern hospitals. It allows hospitals to release informative features from patient data for disease diagnosis without compromising privacy. It is also useful for pervasive mobile platforms with high data collection capabilities. PrivyNet is a simple, platform-aware solution that enables fine-grained privacy control for different end-users in various situations, such as modern hospitals and mobile platforms with high data collection capabilities. It allows for the secure uploading of collected data to the cloud while protecting the individual's private information. PrivyNet is a flexible solution for privacy control, applicable to different end-users. It uses CIFAR-10 and CIFAR-100 datasets, with 60000 images in 10 classes for CIFAR-10 and 100 classes for CIFAR-100. The FEN is derived from VGG16 pre-trained on ImageNet dataset. The FEN is derived from VGG16 pre-trained on ImageNet dataset, using CNN for image classification and a state-of-the-art generative NN architecture for image reconstruction tasks. The NN architecture based on ResNet blocks BID9 is used for image recovery tasks like super resolution and denoising autoencoder. The image IRN is constructed following BID12. Each ResNet block cluster consists of 8 ResNet blocks. Gradient descent optimizer is used in training with a learning rate of 0.003 and a mini-batch size of 128 for image reconstruction. For image classification, the initial learning rate is 0.05 with a mini-batch size of 128, trained for 100 epochs. For image reconstruction, the IRN architecture uses ResNet block clusters with 8 blocks each. The learning rate is set to 0.003 with a mini-batch size of 128, trained for 100 epochs. Data augmentation includes normalization, brightness, and contrast adjustments. Topology of the IRN is determined before characterization for accurate privacy evaluation. The image recovery capability of IRN is determined by the number of ResNet block clusters, with 2 clusters of 8 blocks each chosen in experiments. The PSNR of reconstructed images saturates with increasing block clusters, as shown in Figure 19. Performance and storage characterization is conducted to understand the implications of the PSNR value. The PSNR value implications are understood through performance and storage characterization of pre-trained NNs on local platforms. Profiling of VGG16 on different CPUs shows increased computation and storage requirements with more layers. Convolution layers contribute most to computation, while fully connected layers impact storage significantly. The increase in computation and storage is rapid, with convolution layers contributing most to computation and fully connected layers impacting storage significantly. Different platforms may face varying bottlenecks, and runtime differences highlight the need for a flexible framework considering local capabilities. The size of input images affects the storage demand of fully connected layers. The complexity of the computation in the second part is determined by the number of samples N LDA and the dimension of the output representations W \u00d7 H. The complexity is O((K + N LDA)W 2 H 2 + W 3 H 3), with N LDA being a key factor in determining the extra computation. The computation complexity is O((K + N LDA)W 2 H 2 + W 3 H 3), with N LDA being a key factor in determining the extra computation induced by the learning process. Usually, a small N LDA is sufficient for good pruning results, resulting in a small overall computation overhead."
}