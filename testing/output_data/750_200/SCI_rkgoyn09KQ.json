{
    "title": "rkgoyn09KQ",
    "content": "In this work, a neural autoregressive topic model is combined with a LSTM based language model to address challenges in probabilistic topic modelling. The approach aims to incorporate language structure by considering word order in local collocation patterns, improving the estimation of word probabilities in a given context. The ctx-DocNADE model combines a neural autoregressive topic model with a LSTM language model to learn word representations and thematic structures in documents. It addresses challenges in probabilistic topic modeling by considering word order and local collocation patterns. This approach improves the estimation of word probabilities in a given context, especially in settings with limited context or smaller training corpora. Incorporating external knowledge into neural autoregressive topic models via a language modelling approach improves word-topic mapping on smaller or short-text corpora. The proposed extension, ctx-DocNADEe, addresses challenges in settings with limited context or data sparsity. The novel neural autoregressive topic model variants with neural language models and embeddings priors outperform state-of-the-art generative topic models in generalization, interpretability, and applicability over long-text. Probabilistic topic models like LDA, RSM, and DocNADE variants are commonly used to extract topics from text collections and predict word probabilities in documents. These models learn latent document representations for NLP tasks but do not consider word order, representing context as a bag of words. Probabilistic topic models like LDA, RSM, and DocNADE variants are commonly used for topic analysis in text collections. Traditional models ignore word order and semantic information, representing context as a bag of words. To address this limitation, there is a need to extend these models to incorporate word order and language structure. In text analysis, traditional topic models like LDA and RSM only consider \"bag-of-words\" approach, ignoring word order and semantics. To improve this, models need to incorporate language structure and word order for better topic generation. Topic models like LDA and RSM ignore syntax and semantics, including proper nouns and functional words. LSTM-LMs can capture language concepts layer-wise but struggle with capturing semantics at a document level. Recent studies have integrated latent topic and neural language models to improve language models with global dependencies. However, existing models struggle to capture long-term dependencies and language concepts. DocNADE variants focus on learning word occurrences across documents. BID15 captures word order in short contexts but struggles with long-term dependencies and language concepts. In contrast, DocNADE variants (BID12, BID8) learn word occurrences across documents with a coarse granularity, ignoring language structure. BID17 shows that recurrent neural networks reduce perplexity in language modeling. The introduction of language structure into neural autoregressive topic models via LSTM-LM accounts for word ordering and semantics. The introduction of language structure into neural autoregressive topic models via LSTM-LM improves word ordering and semantics, leading to accurate word prediction in the proposed ctx-DocNADE model. The ctx-DocNADE model combines joint word and latent topic learning to capture complementary semantics. It is effective for long texts but faces challenges with short texts due to limited context. Distributional word representations like word embeddings have shown to capture semantic and syntactic relatedness in words, even in settings with short texts and few documents. Traditional topic models with \"BoW\" assumption may struggle in such scenarios. In short text analysis, traditional topic models struggle to infer relatedness between word pairs like (falls, drops) due to lack of word-overlap. However, distributed word embeddings can capture semantic relationships, as shown in FIG0. Previous work has used web search results and word similarity from thesauri to enhance information in short texts, integrating word embeddings into models like LDA and DMM. Recently, various models have integrated word embeddings into topic learning, such as BID5, BID20, and BID8. However, these models often overlook language structure like word ordering and syntax. In contrast, DocNADE and its extensions have shown superior performance compared to LDA and RSM models in terms of perplexity and information retrieval. Additionally, distributed compositional priors have been incorporated into DocNADE using pre-trained word embeddings via LSTM-LM to enhance topic and textual representations on smaller corpora. In DocNADE, pre-trained word embeddings are used with LSTM-LM to enhance topic and textual representations in a neural autoregressive framework called ctx-DocNADEe. This approach combines complementary learning and external knowledge to model short and long text documents more effectively. Our approach, ctx-DocNADEe, improves textual representations with better generalizability, interpretability, and applicability. It outperforms state-of-the-art generative topic models on 7 long-text and 8 short-text datasets, showing gains in topic coherence, precision at retrieval fraction 0.02, and F1 for text. Our proposed modeling approach, textTOvec, improves textual representations with gains in topic coherence, precision at retrieval fraction 0.02, and F1 for text classification. The code for textTOvec is available at https://github.com/pgcool/textTOvec. Generative models like Restricted Boltzmann Machine (RBM) BID9 and its variants are used to estimate the probability distribution of multidimensional data. Generative models like Restricted Boltzmann Machine (RBM) BID9 and its variants are used to estimate the probability distribution of multidimensional data. RBM and its variants model binary data, while RSM and its variants are used for word count modeling. NADE BID13 decomposes the joint distribution of binary observations into autoregressive conditional distributions, making gradients of data negative log-likelihood tractable. DocNADE BID12 models collections of documents as bags of words, learning word representations reflecting document topics only. It computes conditional distributions using a feed-forward network, making gradients of data negative log-likelihood tractable. DocNADE BID15 DISPLAYFORM0 computes conditional distributions for word observations using a feed-forward neural network. The log-likelihood of any document is determined by the past word observations, which are orderless due to BoWs. The log-likelihood of a document is calculated based on past word observations using a neural network. Two extensions of the model are proposed: ctx-DocNADE introduces language structure with LSTM-LM, and ctx-DocNADEe incorporates external knowledge with pre-trained word embeddings. The proposed extensions of the DocNADE model are ctx-DocNADE, which introduces language structure with LSTM-LM, and ctx-DocNADEe, which incorporates external knowledge with pre-trained word embeddings. These extensions aim to model short and long texts by considering word ordering, syntactical and semantic structures, long and short term dependencies, and external knowledge. The ctx-DocNADE model introduces language structure with LSTM-LM, while ctx-DocNADEe incorporates external knowledge with pre-trained word embeddings. These extensions aim to model short and long texts by considering word ordering, syntactical and semantic structures, long and short term dependencies, and external knowledge. In ctx-DocNADE, the conditional probability of a word is a function of two hidden vectors: LSTM-based components. The weight matrix W encodes topic information and word vectors for each word in the document. The embedding layer in the LSTM-LM component represents column vectors and extends DocNADE by considering word ordering and language concepts. The unified network shares the weight matrix W to incorporate global and local influences, enhancing the model's ability to capture complementary semantics. The second version of ctx-DocNADE extends the model with distributional priors, initializing the LSTM component's embedding layer with a pre-trained embedding matrix E and weight matrix W. Algorithm 1 and Table 1 compare log p(v) for a document v in Doc-NADE, ctx-DocNADE, and ctx-DocNADEe settings. In DocNADE, tied weights in matrix W allow for reuse of linear activation a in every hidden layer, reducing computational complexity to O(HD). In ctx-DocNADE or ctx-DocNADEe, the computational complexity is reduced to O(HD + N) by reusing linear activation a in every hidden layer. LSTM is used to extract hidden vectors h LM i for target words in the document, allowing for textTOvec representation extraction. The model can be extended to a deep, multiple hidden layer architecture by adding new hidden layers. The textTOvec representation can be extended to a deep architecture by adding new hidden layers for improved performance. The first hidden layer is computed similarly to DocNADE variants, while subsequent layers are computed based on the total number of hidden layers. The conditional probability is computed using the last layer for state-of-the-art comparison. The hidden vectors h DN i,1 and h LM i,1 correspond to equations 1 and 2. The conditional probability p(v i = w|v <i ) is computed using the last layer n. Our modeling approaches improve topic models by incorporating language concepts from LSTM-LM. We evaluate topic models on 8 short-text and 7 long-text datasets using four quantitative measures. The text discusses evaluating topic models using four quantitative measures: generalization (perplexity), topic coherence, text retrieval, and categorization. Data statistics are shown in TAB1, with 20NS representing 20NewsGroups and R21578 representing Reuters21578. The performance of proposed models ctx-DocNADE and ctx-DocNADEe is compared with baselines using word representation glove BID22. The performance of proposed models ctx-DocNADE and ctx-DocNADEe is compared with baselines using various word and document representation techniques, including glove, doc2vec, LDA based BoW TMs, neural BoW TMs, and jointly trained topic and language models. Experimental setup involves training DocNADE on a reduced vocabulary. The study compares the performance of ctx-DocNADE and ctx-DocNADEe models with baselines using different word and document representation techniques, including glove, doc2vec, LDA based BoW TMs, neural BoW TMs, and jointly trained topic and language models. Experimental setup involves training DocNADE on a reduced vocabulary, but also explores training it on full text/vocabulary for fair comparison. The models were evaluated over 200 topics using glove embeddings of 200 dimensions. The study compares the performance of different models using glove embeddings of 200 dimensions over 200 topics. Pre-training with \u03bb set to 0 is done for ctx-DocNADEs to enhance complementary learning. Experimental setup details and hyperparameters can be found in the appendices, including ablation over \u03bb on the validation set. BID14 is used for short-text datasets evaluation in the sparse data setting. All models are initialized with the same pre-trained word embeddings (glove). To compare the performance of different models using glove embeddings of 200 dimensions over 200 topics, the study sets \u03bb to 0 for pre-training ctx-DocNADEs. Experimental setup details and hyperparameters are provided in the appendices, including ablation over \u03bb on the validation set. BID14 is used for evaluating short-text datasets in the sparse data setting. All models are initialized with the same pre-trained word embeddings (glove). The generative performance of topic models is evaluated by estimating log-probabilities for test documents and computing average held-out perplexity per word. In ctx-DocNADE versions, log p(v t) is computed using LDN(v) without considering the mixture coefficient \u03bb=0. The optimal \u03bb is determined based on the validation set, with \u03bb=0.01 achieving lower perplexity than the baseline DocNADE for short and long texts. Topic coherence is assessed using BID4 BID19 BID7, with context features identified for each topic word using a coherence measure proposed by BID25. The coherence of topics in ctx-DocNADE is assessed using a measure proposed by BID25. Higher scores indicate more coherent topics. Gensim module is used to estimate coherence for 200 topics, with ctx-DocNADE showing higher coherence compared to DocNADE. Embeddings in ctx-DocNADEe further improve topic coherence. The introduction of embeddings in ctx-DocNADEe boosts topic coherence, resulting in a 4.6% gain on average over 11 datasets. The proposed models outperform baseline methods like glove-DMM and glove-LDA. Qualitatively, embeddings lead to more coherent topics, as shown in Table 8 with an example from the 20NSshort text dataset. Additionally, the proposed models are compared to other approaches like TDLM, Topic-RNN, and TCNLM. Our proposed models improve topic models for textual representations by incorporating language concepts and external knowledge via neural language models. We compare our models to other approaches like TDLM, Topic-RNN, and TCNLM, focusing on enhancing topic models rather than just language models. Incorporating language concepts and external knowledge via neural language models, our models (ctx-DocNADE and ctx-DocNADEe) are compared to TCNLM in terms of topic coherence on the BNC dataset. The sliding window size and mixture weight of the LM component are key hyper-parameters in the topic modeling process. The top 5 words of seven learnt topics from our models and TCNLM are compared for topic coherence on the BNC dataset. Incorporating word embeddings results in more coherent topics than the baseline model. The BNC dataset is not a collection of short-text or few documents, making our model suitable for sparse data settings. DocNADE does not show improvements in topic coherence over ctx-DocNADE, as observed in a comparison of top 5 words of seven topics from TCNML and our models on the BNC dataset. The BNC dataset is unlabeled, limiting the comparison of model performance to topic coherence only. In comparing model performance, we focus on topic coherence. A document retrieval task is conducted using short-text and long-text documents with label information. Test documents are treated as queries to retrieve closest documents in the training set based on cosine similarity measure. Retrieval precision is computed for different fractions by averaging the number of retrieved training documents with the same label as the query. The study compares model performance based on topic coherence through a document retrieval task using short-text and long-text documents. Retrieval precision scores for different fractions are averaged over multiple labels for each query. The introduction of pre-trained embeddings and language/contextual information improves performance noticeably for short texts. The study explores the impact of embeddings and contextual information on information retrieval tasks, particularly for short texts. It is found that using DocNADE(FV) or glove(FV) without pre-processing improves IR precision. The ctx-DocNADEe model shows a 7.1% gain in IR precision on average across datasets. Additionally, the ctx-DeepDNEe model performs well on TREC6 and Subjectivity datasets. The ctx-DeepDNEe model demonstrates competitive performance on TREC6 and Subjectivity datasets, outperforming DocNADE(RV) with a 6.5% gain in precision. It also surpasses TDLM and ProdLDA 6 by noticeable margins. Text categorization using textTovec representations shows promising results. In the experimental setup, textTOvec of 200 dimensions is extracted for each document. Logistic regression classifier with L2 regularization is used for text categorization. ctx-DocNADEe and ctx-DeepDNEe utilize glove embeddings and outperform topic model baselines. ctx-DocNADEe shows a 4.8% and 3.6% gain in F1 compared to DocNADE(RV) on short and long texts, respectively. In comparison to DocNADE(RV), ctx-DocNADEe shows a 4.8% and 3.6% gain in F1 for short and long texts, respectively. Overall, ctx-DocNADEe outperforms NTM and SCHOLAR in classification accuracy on the 20NS dataset. The meaningful semantics captured via topic extraction are further analyzed. The DocNADE neural topic model baseline is strong, with meaningful semantics captured through topic extraction. The ctx-DocNADEe model extracts more coherent topics due to embedding priors. Analysis of word embeddings and text representations in topic models is done using DocNADE and ctxDoocNADEe models. Top 3 texts are retrieved for each query. In topic models, text analysis is done using DocNADE and ctxDoocNADEe models. The top 3 texts are retrieved for each query, with ctx-DocNADEe model showing more coherent topics. The curr_chunk discusses the quality of representations learned at different fractions of the training set from TMNtitle, with topics related to emerging economies, nuclear plans, and Japan's recovery. The quality of representations learned at different fractions of the training set from TMNtitle data is illustrated. The experimental setup for IR and classification tasks is the same as in section 3.3. The gains in tasks are significant for smaller fractions of the datasets, with ctx-DocNADE and ctx-DocNADEe models showing improvements over DocNADE. For example, ctxDocNADEe reports a precision of 0.580 at 20% fraction compared to 0.444 for DocNADE. The study demonstrates improvements in topic models with word embeddings, particularly in sparse data settings. The ctxDocNADEe model outperforms DocNADE in precision and F1 scores at different fractions of the training set. The study combines neural autoregressive topic models with neural language models to improve word probability estimation in context. By integrating language concepts into the topic model, a latent representation of the entire document is learned while capturing local collocation patterns. External word embeddings further enhance this learning process. Experimental results demonstrate the effectiveness of this approach. The study combines neural autoregressive topic models with neural language models to improve word probability estimation in context. By integrating language concepts into the topic model, a latent representation of the entire document is learned while capturing local collocation patterns. External word embeddings further enhance this learning process. Experimental results show that the proposed modeling approaches consistently outperform state-of-the-art generative topic models on various metrics across 15 datasets. The Contractor must provide proficient English-speaking instructors with clear communication skills for equipment maintenance. Experienced staff must be available 24/7 for on-call maintenance of the Signalling System. Standard applies to all cables, including single and multi-core, LAN, and FO cables. The standard applies to all cables, including single and multi-core, LAN, and FO cables. The Contractor must install asset labels on all equipment and coordinate with the Engineer for label format and content. Interlocking stations must be capable of reversing the service. The Engineer must approve labels on equipment for operations and interlocking. Stations should be able to switch to \"Auto-Turnaround Operation\" independently of the ATS system. Multiple platforms at stations can be selected for service reversal. TAB10 shows perplexity scores for different \u03bb in Generalization task. Class labels are not used during training, only to check document retrieval. In the Generalization task, ablation over validation set labels is not used during training. Class labels are only utilized to verify if retrieved documents match the query document's class label. Document retrieval is performed using the same train/development/test split of documents across datasets. Doc2Vec models were trained using gensim for 12 datasets with specific hyperparameters. In the Generalization task, models were trained with distributed bag of words for 1000 iterations using a window size of 5 and a vector size of 500. A regularized logistic regression classifier was trained on inferred document vectors to predict class labels. Multilabel datasets used a one-vs-all approach. Models were trained with a liblinear solver using L2 regularization. Accuracy and macro-averaged F1 score were computed on the test set. LFTM was used to train glove-DMM and glove-LDA models for 200 iterations. F1 scores were computed on the test set using LFTM to train glove-DMM and glove-LDA models. Models were trained for 200 iterations with 2000 initial iterations using 200 topics. Hyperparameters beta and lambda were set differently for short and long texts. Classification task setup was similar to doc2vec, using relative topic proportions as input. Topic coherence (NPMI) was evaluated with 20 topics, showing values for DocNADE (.18) and SCHOLAR (.35). The experimental results suggest that DocNADE outperforms SCHOLAR in generating representations for tasks like information retrieval and classification. SCHOLAR BID3 generates more coherent topics but performs worse in perplexity and text classification compared to DocNADE. Additionally, SCHOLAR without metadata is similar to ProdLDA, which has shown to be inferior in IR tasks compared to the proposed models. The experimental results indicate that DocNADE outperforms SCHOLAR in generating representations for tasks like information retrieval and classification. However, SCHOLAR excels in interpretability compared to DocNADE. This investigation paves the way for future research in this area."
}