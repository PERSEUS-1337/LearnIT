{
    "title": "rygFWAEFwS",
    "content": "Stochastic Weight Averaging in Parallel (SWAP) is a method proposed to accelerate DNN training by using large mini-batches and averaging weights of multiple models computed independently and in parallel. This approach results in models that generalize well and are produced in a shorter time compared to traditional methods like SGD. The effectiveness of SWAP is demonstrated on CIFAR10, CIFAR100, and ImageNet datasets. In DNN training, using large mini-batches with SGD accelerates the process by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss per iteration. This strategy is effective in a distributed setting where multiple nodes can compute gradient estimates simultaneously. In a distributed setting, multiple nodes can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates, with one synchronization event per iteration. Training with larger mini-batches requires fewer updates, thus fewer synchronization events, yielding good overall scaling behavior. However, there is a maximum batch size after which the resulting model tends to have worse generalization performance. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging the weights of a set of models sampled from the final stages of a training run. This allows for better generalization performance even with larger batch sizes, which tend to have worse generalization performance beyond a certain threshold. Stochastic Weight Averaging in Parallel (SWAP) is a strategy to accelerate model training by generating multiple independent SGD sequences and averaging models from each, resulting in similar generalization performance. This method allows for achieving comparable generalization performance with small-batch training starting from a model trained with large-batches. SWAP is a strategy to accelerate DNN training by utilizing available compute resources efficiently. It achieves generalization performance comparable to small-batch models in a similar time frame as large-batch training runs. SWAP accelerates DNN training by reducing training times for efficient models, outperforming state-of-the-art on CIFAR10. The impact of batch size on generalization performance remains unclear, with theories suggesting larger batches may lead to sharper minima. Using larger mini-batches can lead to getting stuck in sharper global minima, which are sensitive to variations in data. Studies have shown that flat minimizers can be transformed into sharp ones without changing model behavior, and vice versa when weight-decay is not used. The impact of batch size on generalization performance is still uncertain. In (Li et al., 2018) and (McCandlish et al., 2018), authors discuss the impact of batch size on model performance. Larger batch sizes may not significantly improve signal to noise ratio, and increasing batch size can lead to fewer model updates. In (Hoffer et al., 2017), using a larger batch size for a fixed number of epochs results in fewer model updates. This impacts the distance weights travel from their initialization, affecting generalization performance. Training with large batches for longer times improves generalization performance but takes more time than small batches. The batch size also influences optimization. The batch size affects generalization performance and optimization process. In the over-parameterized setting, a critical batch size exists where smaller batches are equivalent to larger batches in terms of convergence rate. Adaptive batch size methods have been proposed but are often dataset-specific. Batch sizes play a crucial role in optimization processes, with various methods proposed in literature. Local SGD and Post-local SGD are distributed optimization algorithms that optimize gradient precision and communication costs effectively. Post-local SGD refines large-batch training output with local-SGD, improving generalization and achieving speedups. SWAP averages models after multiple epochs, unlike Post-local SGD which lets models diverge for a few iterations. Stochastic weight averaging (SWA) is a method where models are sampled from later stages of an SGD training run and their weights are averaged to improve generalization properties. This strategy has been effective in various domains such as deep reinforcement learning and semisupervised learning. In this work, SWA is adapted to accelerate DNN training. The algorithm involves three phases: all workers train a single model with large mini-batch updates and higher learning rate, followed by independent refinement of model copies to produce different weights. In the second phase, workers refine their model copies independently with smaller batch sizes, lower learning rates, and different data randomizations. No synchronization is needed between workers. The final phase involves averaging model weights and computing new batch-normalization statistics for the final output. Phase 1 stops before reaching zero training loss or 100% accuracy to prevent optimization from getting stuck. During phase 2, workers refine their models independently with smaller batch sizes and different data randomizations. Each worker produces a unique model after training. The optimal stopping accuracy is a hyper-parameter that requires tuning. After training with different data randomizations, each worker produces a unique model. In the small-batch phase, models diverge due to stochasticity, resulting in varying testing accuracies. The averaged model outperforms individual models consistently. In the small-batch phase of SWAP, workers produce unique models that diverge due to stochasticity, resulting in varying testing accuracies. The averaged model consistently outperforms individual models, as shown by the test accuracy plot. In the SWAP algorithm, a plane is visualized to show the error of the test network during different phases. Orthogonal vectors are used to plot the loss value of the model at different locations. The training and testing errors for the CIFAR10 dataset are shown in Figure 2. In the SWAP algorithm, the training and testing errors for the CIFAR10 dataset are visualized in Figure 2. The output of phase one ('LB') and one worker of phase two ('SGD') are on the outer edges of the convex basin, while the final model ('SWAP') is closer to the center. The model traversed to a different side of the basin during phase 2. When visualizing the test loss landscape, the 'LB' and 'SGD' points fall in regions of higher error due to variations in the basin's topology. However, the 'SWAP' point is less affected as it is closer to the center. The worker points in Figure 3 are at different sides of the training error basin, while 'SWAP' remains closer to the center despite changes in topology. In Figure 3b, worker points lie in regions of higher testing errors compared to 'SWAP', which remains close to the center. Authors suggest that SGD iterates behave like an Ornstein Uhlenbeck process, reaching a high-dimensional Gaussian distribution with a constant learning rate. The authors argue that SGD iterates behave like a high-dimensional Gaussian distribution with a constant learning rate, centered at the local minimum. The distribution's covariance grows with the learning rate, is inversely proportional to the batch size, and depends on the Hessian of the mean loss and covariance of the gradient. They suggest that the mass of the distribution is concentrated near the 'shell' of the ellipsoid, making it unlikely for SGD to access the interior. Sampling weights from an SGD run with enough time steps between them will choose effectively. The authors argue that SGD iterates behave like a high-dimensional Gaussian distribution with a constant learning rate, centered at the local minimum. The distribution's covariance grows with the learning rate, is inversely proportional to the batch size, and depends on the Hessian of the mean loss and covariance of the gradient. They suggest that the mass of the distribution is concentrated near the 'shell' of the ellipsoid, making it unlikely for SGD to access the interior. Sampling weights from an SGD run will choose weights spread out on the surface of the ellipsoid, with their average closer to the center. Independent samples can be generated from the same stationary distribution by starting all runs in the same basin of attraction. The advantage of SWA and SWAP over SGD is demonstrated by the decreasing cosine similarity between the gradient descent direction and the direction towards the output of SWAP as training progresses. This suggests that towards the end of training, the process moves mostly orthogonally to the basin, slowing progress. Averaging samples from different sides of the basin can improve this. In this section, SWAP is evaluated for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. Best hyper-parameters were found using grid searches. Training was done using mini-batch SGD with Nesterov momentum and weight decay. Data augmentation was done using cutout, and a custom ResNet 9 model was used for training. For image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets, SWAP was evaluated using mini-batch SGD with Nesterov momentum (0.9) and weight decay of 5\u00d710 \u22124. Data augmentation was done using cutout, and a custom ResNet 9 model was used for training. Experiments were conducted on one machine with 8 NVIDIA Tesla V100 GPUs using Horovod for computation distribution. Statistics were collected over 10 runs with specific settings for each phase of the experiment. The experiment involved training models with different batch sizes and configurations. The training accuracy reached 98% after 108 epochs. Different setups were tested, including using 8 workers with one GPU each and 512 samples per batch for 30 epochs. Comparisons were made between models trained with small-batch only, large-batch only, and SWAP, showing the best test accuracies and training times. Test Table 2 compares test accuracies and training times for models trained with small-batches, large-batches, and SWAP. Significant improvement in test accuracies was observed after averaging the models, with small-batches achieving higher accuracy but taking longer to train. SWAP terminates in time comparable to large-batch runs and achieves competitive results. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch can achieve state-of-the-art training speeds for CIFAR10, reaching 94% test accuracy in 27 seconds. This outperforms the current front-runner in the DAWNBench competition, which takes 37 seconds with 4 Tesla V100 GPUs. SWAP accelerates training of a fast-to-train ImageNet model with published results. Using SWAP with 16 Tesla V100 GPUs, large-batch experiments double the batch size and learning rates for 22 epochs in phase 1. Phase 2 uses the same accuracy in 27 seconds with a batch size of 256 samples for one epoch. Doubling batch size and learning rates on 16 Tesla V100 GPUs for SWAP phase 1 and phase 2 with reduced training times. Results in Table 3 show improved generalization performance with no additional tuning. The statistics were collected over 3 runs, achieving accelerations with no tuning other than adjusting learning rates proportionally to batch size increases. Comparing SWAP with SWA, the sequential weight averaging algorithm. In comparing SWAP with SWA, both algorithms were tested on the CIFAR100 dataset with the same number of models and epochs per sample. SWA averages models with 10 epochs in-between, while SWAP uses 8 independent workers for 10 epochs each. The goal was to see if SWA could match the test accuracy of small-batch training on a large-batch run. After testing SWAP and SWA on the CIFAR100 dataset with the same number of models and epochs per sample, the study aimed to determine if SWA could recover the test accuracy of small-batch training on a large-batch training run. The large-batch training run achieved lower training accuracy, but SWA was unable to improve it. Additionally, the effect of executing SWA using small batches after a large-batch training run was evaluated. The study evaluated the effect of small-batch SWA after a large-batch training run. SWA reached test accuracy of a small-batch run but took three times longer than SWAP. The SWA cyclic learning rate schedule started from the best model found by small-batch training. Small-batch SWA starts the cyclic learning rate schedule from the best model found by small-batch training. The peak learning rate is selected using grid-search. SWA achieves better accuracy than SWAP with 6.8x more training time. SWAP achieves a speed-up over SWA by relaxing constraints and increasing the phase two schedule. This results in a test accuracy of 79.11% in 241 seconds, which is 3.5x less time compared to SWA. SWAP is an algorithm that uses a variant of Stochastic Weight Averaging to improve model generalization performance with large mini-batches. The algorithm SWAP uses a variant of Stochastic Weight Averaging to enhance model generalization performance with large mini-batches. It quickly computes an approximate solution with large mini-batches and refines it by averaging weights from models trained with small-batches, resulting in a final model with good generalization performance trained in a shorter time. Using large-batches in training does not hinder models from achieving good generalization performance. Refining the output with models sampled sequentially or in parallel results in models performing as well as those trained with small-batches. This was confirmed in image classification datasets like CIFAR10, CIFAR100, and ImageNet. Visualizations show that averaged weights are closer to the center of a training loss basin compared to models from stochastic gradient descent. The basin that large mini-batch runs converge to appears to be the same. The large-batch training method does not hinder models from achieving good generalization performance. A transition point between large-batch and small-batch training needs to be chosen, which can be done through grid search. Future work will focus on a principled method to select this transition point. In future work, the focus will be on exploring the behavior of SWAP with other optimization schemes such as LARS, mixed-precision training, post-local SGD, or NovoGrad. The design of SWAP allows for substitution of these schemes in the large-batch stage. Parameters used in experiments were obtained through independent grid searches. In experiments, parameters for SWAP were obtained through grid searches. Momentum and weight decay constants were set for CIFAR experiments. Tables list remaining hyperparameters, with a stopping accuracy of 100% indicating maximum epochs used."
}