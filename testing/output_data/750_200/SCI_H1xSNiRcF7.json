{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models, enabling learning from uncertain supervision and inferring soft-inclusions among concepts. Building on the Box Lattice model, which uses high-dimensional hyperrectangles to model soft-inclusions, this approach maintains the geometric inductive bias of hierarchical embedding models. In this work, a novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. This approach improves optimization robustness in the disjoint case while preserving desirable properties. Our approach offers a surrogate to the original lattice measure, enhancing optimization robustness in the disjoint case while maintaining key properties. It shows improved performance in WordNet hypernymy prediction, Flickr caption entailment, and MovieLens market basket dataset, especially with sparse data. Embedding methods are crucial in machine learning, converting semantic problems into geometric ones. Embedding methods are essential in machine learning, converting semantic problems into geometric ones. Word2Vec, a neural word embedding method, sparked a renaissance in embeddings. Recent interest lies in structured or geometric representations, associating objects with complex geometric structures like density functions. In contrast to traditional methods that associate objects with simple points in a vector space, recent interest lies in structured or geometric representations using density functions, convex cones, or hyperrectangles. These geometric objects better capture asymmetry, entailment, ordering, and transitive relations, providing a strong inductive bias for tasks. The focus is on the probabilistic Box Lattice model for its empirical performance in modeling transitive relations and probabilistic interpretation. The Box Lattice model of BID22 is favored for its strong empirical performance in modeling transitive relations and complex joint probability distributions. Box embeddings replace vector lattice ordering with overlapping boxes, but their \"hard edges\" and disjoint nature pose challenges. The Box Lattice model of BID22 uses overlapping boxes instead of vector lattice ordering, but the \"hard edges\" and disjoint nature of the boxes present challenges for gradient-based optimization, especially with sparse data. The Box Lattice model of BID22 uses overlapping boxes instead of vector lattice ordering, but the \"hard edges\" and disjoint nature of the boxes present challenges for gradient-based optimization. In contrast, a new model is proposed based on smoothing the hard edges of the boxes into density functions using Gaussian convolution. This approach demonstrates superior performance in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset, outperforming existing state-of-the-art results. The Box Lattice model of BID22 is extended in this work to improve performance in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. The approach shows significant enhancements in the pseudosparse regime and outperforms existing state-of-the-art results. Relevant related work includes order embeddings of BID20 and the box lattice model of BID22, which is extended in this study. The probabilistic extension of the box lattice model by BID9 and the deterministic box embedding model by BID22 were discussed. Another hyperrectangle-based generalization of order embeddings, known as box embeddings, was also introduced. Different interpretations exist between the models, with the former assigning conditional probabilities based on overlap degrees, while the latter follows a deterministic approach similar to order embeddings. Additionally, methods utilizing hyperbolic space for learning hierarchical embeddings have been proposed recently. Methods based on embedding points in hyperbolic space have been proposed for learning hierarchical embeddings. These models optimize an energy function and are biased towards learning tree structures. However, they may not be as suitable for learning non-treelike DAGs. Smoothing the energy landscape of the model using Gaussian convolution is a common approach in optimization methods. Our approach involves smoothing the energy landscape of the model using Gaussian convolution, which is increasingly used in machine learning models. We focus on embedding orderings and transitive relations, a subset of knowledge graph embedding, with a probabilistic approach to learn an embedding model mapping concepts to subsets of event space. This inductive bias is especially suited for transitive relations and fuzzy concepts. Our approach involves learning an embedding model that maps concepts to subsets of event space, with a focus on transitive relations and fuzzy concepts. We review methods for representing ontologies as geometric objects, including vector and box lattices. The non-strict partially ordered set (poset) is defined by reflexivity, antisymmetry, and transitivity. A poset is a set with a binary relation that generalizes the concept of a totally ordered set. Posets are useful for representing acyclic directed graph data with transitive relations. A lattice is a poset where any subset of elements has a unique least upper bound and greatest lower bound. Bounded lattices have additional elements to denote the least upper bound and greatest lower bound of the entire set. In a bounded lattice, the set P contains two additional elements, (top) and \u22a5 (bottom), denoting the least upper bound and greatest lower bound. Lattices have binary operations \u2228 (join) and \u2227 (meet) for least upper bound and greatest lower bound. Bounded lattices must satisfy specific properties. The extended real numbers and sets partially ordered by inclusion form bounded lattices. In bounded lattices, the set P contains two additional elements, (top) and \u22a5 (bottom), denoting the least upper bound and greatest lower bound. Lattices have binary operations \u2228 (join) and \u2227 (meet) for least upper bound and greatest lower bound. Bounded lattices must satisfy specific properties. The extended real numbers and sets partially ordered by inclusion form bounded lattices. The \u2227 and \u2228 operations can be swapped to give a valid lattice, called the dual lattice. A semilattice has only a meet or join, but not both. In the rest of the paper, \u2227 and \u2228 also denote min and max of real numbers to clarify the intuition behind the model. A vector lattice, also known as a Riesz space or Hilbert lattice, is a vector space with a lattice structure. The Order Embeddings of BID20 embed partial orders as vectors using the reverse product order and restrict the vectors to be positive. The Order Embeddings of BID20 represent partial orders as vectors using the reverse product order, with vectors constrained to be positive. Objects become more specific as they move away from the origin in the lattice representation. FIG0 illustrates a two-dimensional example of this concept. Vilnis et al. introduced a box lattice where each concept in a knowledge graph is associated with minimum and maximum vectors defining an axis-aligned hyperrectangle. FORMULA2 introduced a box lattice in a knowledge graph, where each concept is associated with two vectors representing an axis-aligned hyperrectangle. The lattice structure includes least upper bounds and greatest lower bounds, with set inclusion defining a natural partial order. The lattice meet is the largest box contained within both x and y. The lattice meet is the largest box contained entirely within both x and y, or bottom (the empty set) where no intersection exists, and the lattice join is the smallest box containing both x and y. Marginal probabilities of events are given by the volume of boxes, their complements, and intersections under a suitable probability measure. The probability p(x) is given by n i (x M,i \u2212 x m,i ) when event x has an associated box with interval boundaries (x m , x M ). Use of the uniform measure requires the boxes to be constrained to the unit hypercube, so that p(x) \u2264 1. When using gradient-based optimization to learn box embeddings, a problem arises when two concepts are incorrectly labeled as disjoint, resulting in zero gradient signal flow due to the intersection being zero with zero derivative. This issue stems from the meet semilattice structure and the probability measure used for box embeddings. When two concepts are mistakenly labeled as disjoint by the model, no gradient signal can flow due to zero intersection, leading to recovery issues in sparse lattice embeddings. The authors propose a surrogate function to optimize sparse lattice embeddings, aiming to avoid recovery issues caused by disjoint boxes. They suggest developing alternate measures to improve optimization and model quality, using a more principled framework. The authors propose a relaxation of standard box embeddings to address gradient sparsity issues, using a smoothing kernel to optimize sparse lattice embeddings. They aim to maintain desirable properties while enabling better optimization and preserving geometric intuition. The authors propose using kernel smoothing, specifically convolution with a normalized Gaussian kernel, to optimize sparse lattice embeddings and address gradient sparsity issues in standard box embeddings. This approach replaces indicator functions with functions of infinite support, allowing for better optimization and preserving geometric intuition. The authors suggest using kernel smoothing, specifically convolution with a normalized Gaussian kernel, to optimize sparse lattice embeddings and address gradient sparsity issues. This approach replaces indicator functions with functions of infinite support for better optimization and geometric intuition. The integral solution to the smoothed indicators is given by a closed form solution using an antiderivative of the standard normal CDF. The integral solution to the smoothed indicators is given by a closed form solution using an antiderivative of the standard normal CDF. The solution involves the softplus function and a logistic sigmoid approximation, leading to the original equation in the zero-temperature limit. The formula DISPLAYFORM7 is recovered with equality in the last line due to intervals (a, b) and (c, d). This is expected from convolution with a zero-bandwidth kernel, such as a Dirac delta function. However, multiplication of Gaussian-smoothed indicators does not give a valid meet operation on a function lattice, violating the idempotency requirement. For practical considerations, modifying equation 3 can result in a function p that satisfies p(x \u2227 x) = p(x) while maintaining the smooth optimization properties of the Gaussian model. This modification addresses the issue of violating the idempotency requirement and allows for training on conditional probabilities. In the zero-temperature limit, equations 3 and 7 are equivalent, with equation 7 being idempotent for intervals x = y = (a, b) = (c, d). This holds true for both the hinge and softplus functions. In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for intervals x = y = (a, b) = (c, d). This leads to defining probabilities using a normalized version of equation 7. Softplus function upper-bounds the hinge function, requiring normalization in experiments. In experiments, a function is used to output values greater than 1, requiring normalization. Two approaches are used: unconstrained learning for small entity numbers and projection onto the unit hypercube for large data sets. The final probability is calculated as a product over dimensions. The function used in experiments outputs values greater than 1, requiring normalization. The final probability is calculated as a product over dimensions, with the softplus overlap showing better behavior for highly disjoint events. The softplus overlap function shows better behavior for highly disjoint boxes compared to the Gaussian model, preserving the meet property without vanishing gradients. Experiments on the WordNet hypernym prediction task evaluate these improvements in practice, with the hierarchy containing 837,888 edges. The WordNet hypernym hierarchy has 837,888 edges after transitive closure. Positive examples are randomly chosen from these edges, while negative examples involve swapping terms. Experimental details are in Appendix D.1. The smoothed box model performs almost as well as the original box lattice in terms of test accuracy. The model requires less hyper-parameter tuning but could perform better on tasks with higher sparsity. In further experiments with different numbers of positive and negative examples from the WordNet mammal subset, the smoothed box model is compared to the box lattice and order embeddings as a baseline. The training data consists of 1,176 positive examples, with the dev/test set being the transitive closure of the training data. The transitive reduction of the mammal WordNet subset is compared to the transitive closure in the dev/test set. The training data has 1,176 positive examples, while dev/test sets have 209 positive examples. Negative examples are generated randomly. Models like OE baseline, Box, and Smoothed Box perform well with balanced data. Smoothed Box outperforms OE and Box on imbalanced data, which is crucial for real-world entailment graph learning. Our smoothed box model outperforms OE and Box on imbalanced data, which is crucial for real-world entailment graph learning. Experiments were conducted on the Flickr entailment dataset, a large-scale dataset with 45 million image caption pairs. The F1 scores of the models were compared on the WordNet mammal subset, showing the superiority of the smoothed box model. In order to compare results accurately, the same dataset from BID22 was used with boxes constrained to the unit cube. The experimental setup was similar to BID22, with the addition of applying the softplus function before calculating box volume. Results show a slight performance improvement, especially on unseen captions, compared to the original model. The method was also applied to a market-basket task. We applied our method to a market-basket task using the MovieLens dataset, predicting users' movie preferences. We collected pairs of user-movie ratings above 4 points and pruned to movies with over 100 ratings, resulting in 8545 movies in our dataset. We calculated conditional probabilities and compared with various baselines, showing a performance gain, especially on unseen captions. In the study, 8545 movies were included in the dataset for analyzing conditional probabilities. Various baselines were compared, including low-rank matrix factorization and hierarchical embedding methods. Separate embeddings were used for target and conditioned movies, with additional parameters added for the \"imply\" relation in the complex bilinear model. Evaluation was done using KL divergence, Pearson correlation, and Spearman correlation on the test set. Experimental details can be found in Appendix D.4. Our smoothed box embedding method outperforms the original box lattice and all other baselines in Spearman correlation, a key metric for recommendation tasks. Additional study on the model's robustness to initialization conditions is presented in Appendix C. The approach to smoothing energy and optimization landscape of probabilistic box embeddings was presented, with a focus on decreased hyper-parameters for easier training. The model surpassed state-of-the-art results on various datasets, especially effective with sparse data and robust to poor initialization. Research on learning challenges posed by geometrically-inspired embedding models continues, with the need for further exploration in this area. The research on geometrically-inspired embedding models presents challenges that require further exploration. The study focuses on smoothing energy and optimization landscapes of probabilistic box embeddings, aiming to simplify training with reduced hyper-parameters. The model outperformed existing methods on various datasets, particularly excelling with sparse data and being resilient to poor initialization. The task of evaluating Gaussian overlap formula for lattice elements x and y with smoothed indicators f and g is discussed, emphasizing the normalization of the Gaussian kernel to maintain the total integral equal to 1. The Gaussian kernel is normalized to have total integral equal to 1 to maintain the overall areas of the boxes. The antiderivative of \u03c6 is the normal CDF, which can be seen as the difference \u03a6(x; a, \u03c3 2 ) \u2212 \u03a6(x; b, \u03c3 2 ). However, this does not easily allow for the evaluation of the integral of interest, which is the product of two such functions. To evaluate equation 8, the identity BID8 BID21 is recalled. Fubini's theorem is applied, resulting in the equation DISPLAYFORM4 and with \u03c3 = \u03c4 \u22121, DISPLAYFORM5 is obtained. The MovieLens dataset, although not truly sparse, has a large proportion of small probabilities. The MovieLens dataset, with a large proportion of small probabilities, is suitable for optimization by the smoothed model. Additional experiments test the model's robustness to initialization, specifically focusing on the overlap of intervals within boxes. Control over initialization is possible, but ensuring disjoint boxes is a desired feature for the model's robustness. Parametrizing the initial distribution of boxes with a minimum coordinate and a positive width, the study adjusts the width parameter to determine the model's robustness to disjoint boxes. Results show that the smoothed model does not seem to suffer from disjoint boxes during learning on the MovieLens dataset. The study found that the smoothed model performs well even with disjoint initialization on the MovieLens dataset. The strength of the smoothed box model lies in its ability to optimize smoothly in the disjoint regime. Methodology and hyperparameter selection details are provided for each experiment, with code available for reproduction on GitHub. The model is evaluated every epoch on the development set for a large fixed number of epochs, and the best development model is used to score the test set. Baseline models are trained using the parameters of BID22, with the smoothed model using hyperparameters determined on the development set. Negative examples are generated randomly based on the ratio for each batch of positive examples. A parameter sweep is done for all models to choose the best result. In this section, negative examples are randomly generated based on the ratio for each batch of positive examples. A parameter sweep is conducted for all models to select the best result. The experimental setup utilizes the same architecture as BID22 and BID9, employing a single-layer LSTM to read captions and produce box embeddings. The model is trained for a fixed number of epochs and tested on development data at each epoch, with hyperparameters determined on the development set. The best development model is used to report the test set score. The model is evaluated every 50 steps on the development set, and optimization is stopped if the best development set score fails to improve after 200 steps. The best development model is used to score the test set."
}