{
    "title": "BkecJjCEuN",
    "content": "The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask and self-supervised learning. An end-to-end audio feature extractor based on WaveNet is trained, along with easily implemented self-supervised learning tasks on unlabeled audio data. This approach shows promise in scenarios with limited labeled training data. In scenarios with limited labeled training data, self-supervised learning tasks can significantly improve supervised classification tasks by up to 6%. Data augmentation in multitask settings further enhances performance of deep neural networks for modeling and classifying auditory data. In scenarios with limited labeled training data, self-supervised learning tasks can improve supervised classification tasks. The creation of large-scale corpora of audio training data is costly and time-consuming. Unsupervised learning is a promising research direction due to the abundance of unlabeled data sources. The goal is to develop a technique that incorporates auxiliary self-supervised auditory tasks to improve model generalization. Incorporating self-supervised auditory tasks during model training can improve model generalization. The study successfully identifies relevant audio-related tasks and demonstrates their joint training with supervised tasks to enhance performance. WaveNet is utilized as a feature extractor for rich audio representations from raw waveform data. The hypothesis is that WaveNet-based models can adapt to subtle audio features by learning multi-scale hierarchical representations. WaveNet-based models can adapt to subtle variations in audio tasks by learning multi-scale hierarchical representations from raw waveform data. This framework is applied to supervised classification tasks like audio tagging, speaker identification, and speech command recognition. Leveraging unlabeled data and self-supervised tasks can improve performance and be used for pre-training and transfer learning. Common data augmentation techniques and self-supervised tasks can enhance performance through transfer learning. Models trained for multiple tasks may uncover underlying structures for better single-task performance. Models trained for multiple tasks may synergize to uncover underlying structures, enabling better single-task performance with less data. Shared representations allow pooling data from different datasets, but the problem of scarce labeled data persists. Self-supervised learning is a promising solution to label scarcity. Self-supervised learning is a promising solution to label scarcity in deep learning. It has shown promising results in the visual domain, but little previous work has explored its potential in the audio domain. An end-to-end audio processing network was implemented to find a common embedding of the acoustic waveform within a \"trunk\" network modeled after the WaveNet. The study explores self-supervision in the audio domain using an end-to-end audio processing network modeled after WaveNet. The network consists of a trunk network and task-specific head networks, trained jointly for each experiment. The WaveNet trunk includes 3 blocks of 6 dilation stacks with 64 convolutional units per module. The WaveNet trunk consists of 3 blocks of 6 dilation stacks with 64 convolutional units per module, resulting in an effective receptive field length of 190 samples. It was tested on audio tagging, speaker identification, and speech command recognition tasks using labeled and unlabeled data. The audio tagging task is trained on the FSDKaggle2018 dataset collected through Freesound, containing 11,073 files. Each audio segment is cropped to 2 seconds before being fed to the network. The speaker identification task involves cropping audio segments to 2 seconds, padding with zeros if necessary, and using WaveNet to produce embeddings. The output is averaged across time, fed into a fully-connected layer with 512 units and ReLU nonlinearity, and a softmax output layer. Training minimizes cross entropy between softmax outputs and one-hot encoded labels using the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Clips are sourced from celebrity interviews. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Clips are sourced from interviews with celebrities, cropped to 2 seconds, normalized, and pre-emphasis filter applied. The network architecture includes a global average pooling layer and a 2-layer perceptron with 1024 units per layer. The speech command recognition task involves normalizing clips, applying a pre-emphasis filter, and using a head architecture with global average pooling, 2-layer perceptron, batch normalization, ReLU nonlinearity, softmax layer, and cross-entropy loss. The task is trained on the Speech Commands dataset with 65,000 utterances of 30 short words in 12 categories. The speech command recognition head consists of three 1D convolutions. The speech command recognition head consists of three 1D convolutions with specific widths and strides. Self-supervised tasks like next-step prediction, noise reduction, and upsampling were implemented for auxiliary tasks. The self-supervised auxiliary tasks, trained on both main task data and unlabeled data from the Librispeech dataset, share a common head architecture with convolutional layers and a regression-type loss function. The goal was to create a generic multitask framework for audio applications. The study aimed to develop a generic multitask framework for audio applications using waveform inputs instead of high-level feature representations like spectrograms. While convolutional architectures trained on spectral/cepstral representations can offer better classification performance, they limit the range of audio processing tasks. This results in varying network architectures for different tasks, restricting the amount of information that can be processed. The study focuses on developing a multitask framework for audio applications using waveform inputs instead of spectral representations. It emphasizes the importance of models that make fewer assumptions about input representations to understand the interaction between learning dynamics of different tasks. Multitask learning is highlighted for its performance improvements compared to single task training on raw audio. Closing the performance gap between models trained on spectral representations and waveforms is a future research direction. Joint training with three self-supervised tasks improved performance for supervised tasks in audio applications, including audio tagging. Multitask training enhanced MAP@3 score and top-1 classification rate without increasing training data, by incorporating additional unsupervised tasks. Gradually incorporating larger versions of Librispeech into training regimen showed benefits of self-supervision. Incorporating larger versions of Librispeech into training regimen improved performance metrics with self-supervision. Additional 500 hours of unlabeled data led to a MAP@3 increase of up to .056. Similar trends were observed in speech command classification and speaker identification tasks with increasing amounts of unlabeled data. Multitask learning improved speech command classification and speaker identification performance without additional labeled data. Speaker identification on VoxCeleb dataset saw a top-5 classification increase to 75.22% from 73.81%. Comparing multitask learning with data augmentation showed significant performance improvements. Data augmentation techniques, specifically pitch shifting and additive noise, were compared to multitask learning for improving label efficiency in audio tagging. Pitch-shift augmentation resulted in a MAP@3 increase of .066, similar to the benefits of multitask learning. Noise augmentation showed a smaller increase of .024. Interestingly, augmenting with noisy data produced performance gains comparable to training with a self-supervised noise-reduction task. Training with both pitch-shift augmentation and additional techniques was also explored. Transfer learning in computer vision has expanded to include knowledge transfer from self-supervised tasks trained on unlabeled data to supervised tasks. This approach inspired a reconsideration of multitask learning from a transfer learning perspective, where self-supervised tasks are jointly pre-trained. This led to improved label efficiency, with a MAP@3 increase of .089, suggesting that different methods for enhancing label efficiency are complementary. In transfer learning experiments, self-supervised tasks were pre-trained on unlabeled data before fine-tuning with a smaller amount of labeled data for a supervised task. Results favored transfer learning over training all tasks simultaneously, showing performance gains with limited labeled data. The present work suggests that training a supervised audio task with multiple self-supervised tasks using a WaveNet-based model on raw audio waveforms can lead to performance gains. The improved performance scales with the amount of unlabeled data and can supplement existing data augmentation methods. This approach shows promise for a wide range of supervised audio tasks. Our approach to training a supervised audio task with multiple self-supervised tasks using a WaveNet-based model on raw audio waveforms shows promise for a broad range of audio tasks. The methodology and results suggest potential for further development, including exploring the limits of auxiliary tasks and the representation of audio in multitasking models. Our approach to training a supervised audio task with multiple self-supervised tasks using a WaveNet-based model on raw audio waveforms shows promise for a broad range of auditory tasks. The chosen auxiliary tasks require higher temporal resolutions, which is why we opted for the WaveNet architecture known for processing high temporal resolution raw audio signals. This model is ideal for handling a variety of auditory tasks effectively. WaveNet models are autoregressive networks designed for processing high temporal resolution raw audio signals. They use causal dilated convolutions to process inputs in parallel, making them faster to train than RNNs. Task-specific neural networks are built on top of the WaveNet trunk architecture, which consists of stacked dilated and causal convolutions. The WaveNet trunk architecture consists of stacked dilated and causal convolutions, with outputs fed into task-specific heads. Each block in the WaveNet trunk contains dilated causal convolution layers with increasing dilation factors. Each layer in the trunk consists of a \"residual atom\" involving \"Filter\" and \"Gate\" computations. The WaveNet trunk architecture includes stacked dilated and causal convolutions in each block. Each layer in the trunk has a \"residual atom\" with \"Filter\" and \"Gate\" computations, producing hidden state vector h and layer output x. The first layer applies causal convolutions to raw audio waveforms to generate an output. The WaveNet trunk architecture applies causal convolutions to raw audio waveforms sampled at 16 kHz. The trunk consists of 3 blocks with 6 layers each, resulting in a total receptive field of 190, equivalent to 12 milliseconds of audio. Each task-specific head is a simple neural network. The WaveNet trunk architecture uses causal convolutions on raw audio waveforms sampled at 16 kHz, with a total receptive field of 190, equivalent to 12 milliseconds of audio. Each task-specific head is a neural network that shares a trunk with other tasks, allowing independent processing. Tasks have their own objective functions, optimizers, and learning rates. Supervised tasks are primary, while self-supervised tasks are auxiliary. In experiments, \"audio tagging\" is the primary supervised task. In experiments, \"audio tagging\" is the primary supervised task, with \"next-step prediction\", \"noise reduction\", and \"upsampling\" as auxiliary tasks trained on varying amounts of unlabeled data. The head architectures are designed to be simple, forcing the shared trunk to learn a representation suitable for multiple audio tasks. The next-step prediction task is formalized as predicting the next step in the audio sequence. The next-step prediction task involves predicting the next value in a sequence of audio frames using a 2-layer convolutional neural network. This task allows for the creation of large training datasets from unlabeled audio data. The model consists of two layers with nonlinearities in all but the last layer. The first layer has 128 units, and the second has a single output unit. The head takes \u03c4 frames of data from the trunk and predicts the next frame of audio. The next-step head treats this as a regression problem, using mean squared error as a loss function. The WaveNet model predicts the next frame of audio as a regression problem, using mean squared error as a loss function. Noise reduction is defined by treating noise as an additive random process on top of the true signal. The WaveNet model predicts the next frame of audio using mean squared error as a loss function. Noise reduction is achieved by treating noise as an additive random process on top of the true signal, with a model trained to predict clean samples from noisy ones. The noise reduction head has a structure similar to the next-step prediction head, minimizing a smoothed L1 loss between clean and noisy samples. Our noise reduction head, similar in structure to the next-step head, is trained to minimize a smoothed L1 loss between clean and noisy waveform inputs. The smooth L1 loss is preferred for denoising tasks over mean squared error due to its stable convergence. Additionally, an unsupervised upsampling task can be created by downsampling the audio source and using the original source as the target for upsampling. An unsupervised upsampling task can be created by downsampling the audio source and using the original source as the target. The network's job is to infer the high frequency information lost during the transform. The network's task is to infer high frequency information lost during the transform from the original signal's 16 kHz sample rate. A smooth L1 loss function was used to compare the estimated upsampled audio with the original, trained on raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets. The audio samples from FSDKaggle2018 and Librispeech datasets were cropped to two seconds and downsampled to 16 kHz. Inputs were normalized to [-1, 1] and noise was added from ChiME3 datasets at SNR levels of 10dB to 15dB for a noise-reduction task. Noise types included booth, bus, cafe, pedestrian area, and street junction. The study involved using noise from ChiME3 datasets at SNR levels of 10dB to 15dB for a noise-reduction task. Different noise types were included such as booth, bus, cafe, pedestrian area, and street junction. A hyperparameter search was conducted for the network architecture, including the number of blocks in the trunk and layers per block, with varying values tested. The network's performance was found to be largely unaffected by the specific architecture specifications. The study involved a hyperparameter search for the network architecture, focusing on the depth and width of auxiliary task heads and learning rates. The final hyperparameters were chosen based on performance on both the main and auxiliary tasks, favoring the main task. The model was trained on all tasks simultaneously by computing the loss function for each task during a forward pass. In the study, the model was trained on all tasks simultaneously by computing the loss function for each task during a forward pass. The training process involved using a uniform weighting strategy for the tasks and the \"Adam\" optimizer with specific parameters. The learning rate was decayed every 5 epochs to improve convergence, and a batch size of 48 was used across all experiments. The learning rate was decayed by a factor of .95 every 5 epochs to improve convergence. A batch size of 48 was used for all experiments, as it was the largest permissible by available computational resources. Noise reduction and upsampling tasks required separate forward propagation of noisy and downsampled audio. Important parameters can be found in TAB3."
}