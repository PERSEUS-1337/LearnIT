{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not yet been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. Adaptivity is implemented across manifolds in the cartesian product. Our generalization of adaptive optimization algorithms to Riemannian manifolds shows faster convergence and lower train loss values compared to standard algorithms. Experimentally, we demonstrate this improvement by embedding the WordNet taxonomy in the Poincare ball. Developing efficient stochastic gradient-based optimization algorithms is crucial for various applications. Developing efficient stochastic gradient-based optimization algorithms is crucial for various applications, including embedding the WordNet taxonomy in the Poincare ball. First order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD have been developed to optimize deep neural networks and large vocabularies. Recent work has focused on optimizing parameters on a Riemannian manifold, allowing for non-Euclidean geometries. These algorithms have been applied to various tasks such as solving Lyapunov equations, matrix factorization, and dictionary learning. The adaptivity of algorithms on a Riemannian manifold remains to find their respective counterparts, despite successful applications in solving various tasks like Lyapunov equations and matrix factorization. In this work, the adaptivity of algorithms on a Riemannian manifold is explored, focusing on the challenges of generalizing adaptive schemes in an intrinsic manner. The proposal includes generalizations of the algorithms and their convergence analysis in the case of a product of manifolds. The study explores the adaptivity of algorithms on a Riemannian manifold, proposing generalizations and convergence analysis for a product of manifolds. The motivation behind developing Riemannian versions of ADAGRAD and ADAM was to learn symbolic embeddings in non-Euclidean spaces, benefiting algorithms like GloVe for semantic/syntactic relationships. The GloVe algorithm benefits significantly from optimizing with ADAGRAD compared to using SGD due to different word frequencies. The absence of Riemannian adaptive algorithms could hinder the development of competitive optimization-based Riemannian embedding methods, especially in hyperbolic spaces. Elementary notions of differential geometry include manifold, tangent space, and Riemannian metric. A manifold M of dimension n can be locally approximated by a Euclidean space R^n and is a generalization of the notion of surface to higher dimensions. The tangent space T_xM at each point x in M is an n-dimensional vector space. A Riemannian manifold is a simple n-dimensional manifold with zero curvature. It has a tangent space at each point, defined by a Riemannian metric \u03c1 that varies smoothly. This metric induces a global distance function on the manifold, allowing for the calculation of geodesics. A Riemannian metric induces a global distance function on a manifold, allowing for the calculation of geodesics. In a Riemannian manifold, Riemannian SGD is defined by updating the gradient of the objective function using the Riemannian gradient. Riemannian SGD updates the gradient of the objective function using the Riemannian gradient, allowing for updates along the shortest path in the relevant direction on the manifold. The exponential map is used for this purpose, with a common approximation being the retraction map. The main algorithms of interest are ADAGRAD and ADAM. ADAGRAD rescales updates coordinate-wise based on past gradients, beneficial for sparse gradients or deep networks. ADAM includes momentum and adaptivity terms in its update rule. The ADAM update rule includes momentum and adaptivity terms, with the momentum term often yielding significant improvements. When \u03b2 1 = 0, it is similar to RMSPROP but with an exponential moving average instead of a sum. This avoids the issue of learning stopping too early in ADAGRAD due to significant accumulated squared gradients. The ADAM update rule includes momentum and adaptivity terms, with the momentum term often yielding significant improvements. BID18 identified a mistake in the convergence proof of ADAM and proposed modifications to the algorithm, such as AMSGRAD or ADAMNC. Writing coordinate-wise updates requires a choice of coordinate system, but on a Riemannian manifold, this is not always straightforward. On a Riemannian manifold, working with local coordinate systems called charts is essential. Quantities defined using a chart are intrinsic if their definition is independent of the chart used. The Riemannian gradient of a smooth function on a manifold can be defined intrinsically, but its Hessian is only intrinsically defined at critical points. The RSGD update is intrinsic as it involves objects intrinsic to the manifold, but it is unclear if Eqs. (3,4,5) can be expressed in a coordinate-free manner. One possible solution is to fix a canonical coordinate system in the tangent space at initialization and parallel-transport it along the optimization trajectory. In Euclidean space, parallel transport between two points does not depend on the path taken due to the absence of curvature. In a Riemannian manifold, parallel transport depends on the chosen path and curvature introduces a rotational component, breaking the sparsity of gradients and adaptivity benefits. The interpretation of adaptivity as optimizing different features at different speeds is lost, as the coordinate system for gradients depends on the optimization path. The techniques used to prove the theorems may not apply here. The coordinate system for gradients depends on the optimization path in a Riemannian manifold, making the interpretation of adaptivity challenging. The techniques used to prove the theorems may not apply to updates defined differently. Additional structure is assumed on the manifold as the cartesian product of Riemannian manifolds. In a Riemannian manifold, designing adaptive schemes is challenging due to the absence of intrinsic coordinates. Each component x i \u2208 M i can be seen as a \"coordinate\", simplifying the adaptation process. The adaptivity term involves rescaling the gradient using squared Riemannian norms. ADAGRAD, ADAM, and AMSGRAD were briefly discussed in section 2. In the Euclidean setting, the adaptivity term involves rescaling the gradient. ADAM combines ADAGRAD with momentum and an exponential moving average. AMSGRAD corrects ADAM's convergence proof. ADAMNC has a non-constant schedule for parameters \u03b21 and \u03b22. BID18 proposed an initial schedule. ADAMNC is a variant of ADAM with a non-constant schedule for \u03b21 and \u03b22. BID18 proposed an initial schedule for \u03b22 in ADAMNC, which allows it to recover the sum of squared-gradients of ADAGRAD. Without momentum, ADAMNC yields ADAGRAD. The product manifold (M, \u03c1) is defined as the combination of compact, geodesically convex sets X i \u2282 M i. The projection operator \u03a0 Xi minimizes distance d i between points in X i and M i. A family of geodesically convex functions (f t ) on M to R is considered, with bounded diameters and gradients. Convergence guarantees are provided. Riemannian AMSGRAD algorithm is presented in FIG1 alongside the standard AMSGRAD algorithm. RADAM and ADAM are derived by removing max operations. Convergence guarantee for RAMSGRAD is given in Theorem 1, with \u03b6 defined accordingly. The convergence guarantee for RAMSGRAD is presented in Theorem 1, where \u03b6 is defined. When (Mi, \u03c1i) = R for all i, convergence guarantees between RAMSGRAD and AMSGRAD coincide. The regret bound worsens by a multiplicative factor of approximately 1 + D when the curvature is small but non-zero. Theorem 1 presents the convergence guarantee for RAMSGRAD, with defined parameters. The regret bound worsens by a factor of approximately 1 + D when the curvature is small but non-zero. The convergence guarantee for RADAMNC is shown in Theorem 2. The convergence of RADAMNC is proven in Theorem 2, with sequences (x_t) and (v_t) obtained from RADAMNC. The role of convexity is highlighted, comparing convexity and geodesic convexity definitions. Regret bounds for convex objectives are discussed. The role of convexity and geodesic convexity in regret bounds for convex objectives is discussed. Using the cosine law, a bound on the term g_t, x_t - x* is obtained in the context of an SGD update. In the context of an SGD update, a bound on the term g_t, x_t - x* is obtained using the cosine law. This bound involves two terms to be bounded, with the second term requiring a well-chosen decreasing schedule for \u03b1. The step is generalized in Riemannian manifolds using lemma 6, applicable in all Alexandrov spaces. The curvature dependent quantity \u03b6 from Eq. (10) allows for bounding \u03c1, with significant improvements for sparse gradients. The lemma allows us to bound \u03c1 using \u03b6 from Eq. (10), with significant improvements for sparse gradients. Algorithms like RADAM, RAMSGRAD, and RADAGRAD are empirically assessed for adaptivity and convergence theorems do not require specifying \u03d5 i, suggesting potential for regret bounds improvement by exploiting momentum/acceleration. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed for adaptivity and compared to non-adaptive RSGD method. The WordNet noun hierarchy is embedded in the n-dimensional Poincar\u00e9 model of hyperbolic geometry, known for better embedding tree-like graphs than Euclidean space. The Poincar\u00e9 model is used for algorithms in BID14, BID4, and BID6 due to closed form expressions. Riemannian gradients are rescaled Euclidean gradients, with distance functions, geodesics, exponential and logarithmic maps, and parallel transport formulas derived from BID26 and BID7. The transitive closure of the WordNet taxonomy graph consists of 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in D n to minimize distances between connected words while maximizing distances otherwise. The loss function is similar to log-likelihood, approximating the partition function using sampling of negative word pairs. The loss function used in the study is similar to log-likelihood and approximates the partition function by sampling negative word pairs. The study focuses on measuring representation capacity and generalization through reconstruction and link prediction tasks. The study focuses on measuring representation capacity and generalization through reconstruction and link prediction tasks in 5-dimensional hyperbolic spaces. Training details include a \"burn-in phase\" for 20 epochs with a fixed learning rate of 0.03 using RSGD with retraction. Negative words are sampled based on their graph degree raised at power 0.75 during this phase, improving all metrics. After the burn-in phase, negatives are sampled uniformly. During optimization methods, negatives are sampled uniformly. Experimentally, RADAM showed slightly better results than RAMS-GRAD. Convergence to lower loss values was observed when using the first-order approximation of the exponential map in RSGD and adaptive methods. Retraction methods may require fewer steps and smaller gradients to escape sub-optimal points on the ball border of Dn compared to fully Riemannian methods. Results show that \"retraction\"-based methods are not directly comparable to fully Riemannian analogues, possibly due to needing fewer steps and smaller gradients to escape sub-optimal points on the ball border of Dn. Exponential and retraction-based methods were compared using various learning rates, with RSGD baseline results displayed in different colors for optimal, slower convergence, and faster overfitting settings. Results show that \"retraction\"-based methods are not directly comparable to fully Riemannian analogues. RSGD baseline results are displayed in different colors for optimal, slower convergence, and faster overfitting settings. RADAM consistently achieves the lowest training loss and outperforms other methods on the MAP metric for both reconstruction and link prediction settings in the full Riemannian setting. In the \"retraction\" setting, RADAM achieves the lowest training loss and performs on par with RSGD on the MAP evaluation for reconstruction and link prediction. RAMSGRAD converges faster in terms of MAP for link prediction, indicating better generalization capability. Various first-order Riemannian methods have emerged after the introduction of Riemannian SGD. Various first-order Riemannian methods have emerged, such as Riemannian SVRG, Stein variational gradient descent, accelerated gradient descent, and averaged RSGD, with new convergence analysis in the geodesically convex case. Stochastic gradient Langevin dynamics was generalized to optimize on the probability simplex. Riemannian counterparts of SGD with momentum and RMSprop were proposed, suggesting the transport of momentum using parallel translation. No convergence guarantee is provided for their algorithm, which compromises the possibility of obtaining convergence guarantees by performing coordinate-wise adaptive operations in the tangent space. Another version of Riemannian ADAM for the Grassmann manifold G(1, n) removes the adaptive component, resulting in no adaptivity across manifolds and no convergence analysis provided. The adaptivity term v t becomes a scalar, but no adaptivity across manifolds is discussed, and no convergence analysis is provided. Proposing to generalize adaptive optimization tools to Cartesian products of Riemannian manifolds, with convergence rates similar to Euclidean models. Outperforming non-adaptive methods on hyperbolic word taxonomy embedding tasks. Our methods outperform non-adaptive methods like RSGD on hyperbolic word taxonomy embedding tasks. Using formulas and inequalities, we show superior performance. Lemma 6 (Cosine inequality in Alexandrov spaces) is a user-friendly inequality used to prove convergence of gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. It involves sides and angles of a geodesic triangle in a space with curvature lower bounded by \u03ba. Lemma 6 (Cosine inequality in Alexandrov spaces) states that in a geodesic triangle in an Alexandrov space with curvature lower bounded by \u03ba, the sides a, b, c and angle A satisfy a certain inequality. This lemma is crucial for proving convergence in optimization algorithms for geodesically convex functions in Alexandrov spaces."
}