{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization performance and prevent overfitting in deep neural networks. Three novel observations about dropout in DNNs with ReLU activations are discussed: 1) it smooths local linear models, 2) different layers have varying neural-deactivation rates with a constant dropout rate, and 3) the rescaling factor of dropout causes inconsistencies between training and testing conditions. The proposed method \"Jumpout\" improves dropout in deep neural networks by sampling the dropout rate using a monotone decreasing distribution, leading to better performance for nearby data points. It also adaptively normalizes the dropout instead of applying a fixed rate to all samples. Jumpout improves dropout in deep neural networks by adaptively normalizing the dropout rate at each layer and training sample, maintaining consistent effective dropout rates for activated neurons. This approach also rescales outputs for a better trade-off between variance and mean of neurons, addressing incompatibility between dropout and batch normalization. Jumpout outperforms original dropout on various datasets like CIFAR10, CIFAR100, Fashion-MNIST, and ImageNet-1k. Jumpout improves dropout in deep neural networks by adaptively normalizing the dropout rate at each layer and training sample, maintaining consistent effective dropout rates for activated neurons. It outperforms original dropout on various datasets like CIFAR10, CIFAR100, Fashion-MNIST, and ImageNet-1k, with negligible additional memory and computation costs. BID17 BID7 is a technique that reduces co-adaptation amongst neurons in deep neural networks by randomly setting hidden neuron activations to 0. However, it requires tuning dropout rates for optimal performance, which can slow convergence if too high or yield no improvements if too low. Tuning rates separately for each layer and training stage is ideal. In practice, a single fixed dropout rate is often used for all layers and training stages to reduce computation. Dropout acts as a perturbation on training samples, helping the DNN generalize to noisy samples with that specific amount of perturbation. This fixed rate excludes samples with less perturbation that could potentially be more helpful. The fixed dropout rate used for all layers and training stages may lead to too much or too little perturbation, affecting generalization. Dropout is also incompatible with batch normalization. Dropout is incompatible with batch normalization due to the need for rescaling of undropped neurons, which breaks the consistency of normalization parameters. This conflict often leads to dropout being omitted in favor of batch normalization in modern DNN architectures. The proposed \"jumpout\" is an improved version of dropout for DNNs with ReLU activations. It addresses the drawbacks of dropout and aims to enhance generalization performance. Dropout in DNNs with ReLU activations involves randomly changing activation patterns to improve generalization performance by training linear models to work for data points in nearby polyhedra. Dropout in DNNs with ReLU activations improves generalization performance by training linear models to work for data points in nearby polyhedra, depending on the dropout rate used. The typical number of units dropped out is np on a layer with n units, smoothing each linear model to work on data points at a typical distance away. In jumpout, the dropout rate is a random variable sampled from a decreasing distribution, ensuring a higher probability of smaller dropout rates. This leads to a higher chance of smoothing polyhedra to other points at greater distances. In jumpout, the dropout rate is a random variable sampled from a decreasing distribution, leading to a higher chance of smoothing polyhedra to other points at greater distances. Additionally, the fraction of activated neurons in different layers can vary, affecting the effective dropout rate. In jumpout, the dropout rate is adaptively normalized for each layer and training sample, ensuring consistent neural deactivation rates across layers and samples. The outputs of jumpout are rescaled to maintain variance, allowing for compatibility with Batch Normalization (BN) layers. This enables the benefits of both dropout and BN to be utilized in training Deep Neural Networks (DNN). Jumpout, like dropout, randomly generates a mask over hidden neurons without extra training. It can be easily implemented and integrated into existing architectures with minimal modifications. In experiments on various datasets, jumpout shows similar memory and computation costs as dropout but consistently outperforms it on different tasks. Jumpout, similar to dropout, randomly masks hidden neurons without additional training. It has comparable memory and computation costs as dropout but consistently outperforms it on various tasks. Previous approaches like \"standout\" and BID24 have also addressed the fixed dropout rate issue by proposing adaptive dropout rate methods. Jumpout adjusts dropout rates based on ReLU activation patterns, without relying on additional trained models like standout and BID24. It introduces minimal computation and memory overhead and can easily be integrated into existing model architectures. Jumpout introduces minimal computation and memory overhead, can be easily integrated into existing model architectures. BID19 introduced Gaussian dropout as a faster convergence optimization. BID9 proposed variational dropout to connect global uncertainty with adaptive dropout rates for each neuron. BID11 extended variational dropout to reduce gradient estimator variance and achieve sparsity. In this paper, the focus is on modifications to the original dropout method that do not require additional training or introduce more parameters. BID11 extended variational dropout to reduce gradient estimator variance and achieve sparse dropout rates. Other recent variants include Swapout and Fraternal Dropout, which aim to generalize dropout to different neural network architectures and shrink the gap between training and test phases. Jumpout is a modification to the original dropout method that does not require extra training or introduce more parameters. It can be applied alongside other dropout variants and targets different dropout-related issues in deep neural networks. The study focuses on feed-forward neural networks and their components. The paper discusses the formalization of deep neural networks (DNNs) using ReLU activation function. It covers the representation of hidden nodes, network output prediction, and generalization of DNN architectures. The formalization can represent fully-connected networks with bias terms at each layer. The paper formalizes deep neural networks (DNNs) with ReLU activation function, covering hidden nodes, network output prediction, and generalization of architectures. Eqn. (1) includes bias terms at each layer. Convolution is a sparse matrix multiplication, average-pooling is linear, max-pooling is an activation function. The residual network block can be represented by appending an identity matrix to a weight matrix to retain input values. DNN with shortcut connections can be written as a piecewise linear function for ReLU activation functions. The DNN in a region surrounding a data point x is a linear model with a modified weight matrix W x j. ReLU activation sets units to 0 or preserves values, resulting in a piecewise linear function. The DNN can be simplified to a linear model by setting rows with activation patterns of 0 to zero vectors. This process eliminates ReLU functions and produces a linear model with weight vector \u2202x. The linear model is associated with activation patterns on all layers, defining a convex polyhedron. In the context of a DNN simplified to a linear model, ReLU activations are focused on for their computational efficiency and performance. Dropout is studied for improving generalization by considering local linear models within convex polyhedra. The text discusses how dropout improves the performance of DNNs by promoting independence among neurons and training multiple smaller networks. This is inspired by local linear models within convex polyhedra. Dropout encourages neuron independence and diversity, training multiple smaller networks for ensemble predictions. It smooths local linear models within convex polyhedra in DNNs with ReLUs. For DNNs with ReLUs, the input space is divided into convex polyhedra, where each data point behaves like a linear model. Training samples are dispersed among polyhedra, leading to distinct local linear models for each data point. Nearby polyhedra may correspond to different linear models. The input space for DNNs with ReLUs is divided into convex polyhedra, with each data point having its own local linear model. Nearby polyhedra may correspond to different linear models due to differences in activation patterns. The linear model of a polyhedron may only work for a few training data points within it, leading to instability in deep neural networks. To address dropout issues, a dropout rate is sampled from a truncated half-normal distribution, improving generalization ability. The dropout rate is sampled from a truncated half-normal distribution to improve generalization ability in deep neural networks. The rate is determined by sampling from a Gaussian distribution and taking the absolute value, then truncating it within specified limits to ensure effectiveness without compromising performance. This approach results in a monotone decreasing probability of dropout rates. Other distributions like Beta could also be explored in the future. The use of a Gaussian-based dropout rate distribution with a standard deviation \u03c3 as a hyper-parameter helps enforce generalization in deep neural networks. Smaller dropout rates are sampled more frequently to enhance the performance of local linear models on points in closer polyhedra, promoting smoothness in generalization. This method encourages effective performance while diminishing effectiveness on points further away. The dropout rate for each layer is a hyper-parameter that controls smoothness among nearby local linear models. Tuning different dropout rates for each layer can improve network performance, but setting a single global dropout rate is a common approach due to computational constraints. Setting a single global dropout rate for all layers is a common approach due to computational constraints, but it can be suboptimal as the proportion of active neurons in each layer varies. This variation leads to different fractions of active neurons being deactivated, resulting in a significantly varied effective dropout rate. To better control dropout behavior across layers and training stages, the dropout rate is normalized by the fraction of active neurons in each layer. This helps achieve a consistent activation pattern and allows for more precise tuning of the dropout rate. The activation pattern is more consistent when tuning the dropout rate as a single hyper-parameter. In standard dropout, neurons are scaled by 1/p during training and kept unchanged during testing. This scaling factor helps maintain the mean of neurons between training and testing but can cause variance differences, leading to incompatibility with batch normalization. Batch normalization (BN) can have unpredictable behavior due to differences in variance between training and test phases. One approach is to combine dropout layers with BN layers to address this issue. This involves a linear computational layer followed by a BN layer, ReLU activation layer, and dropout layer. The value of a neuron after ReLU can be treated as a random variable with a certain probability. When combining dropout layers with batch normalization (BN), the value of a neuron after ReLU can be seen as a random variable with a probability. This neuron then contributes to the next layer before undergoing BN, where the focus is on changes in mean and variance with the addition of dropout. When combining dropout layers with batch normalization (BN), the focus is on changes in mean and variance as dropout affects the scales during training. The inconsistency between trained BN parameters and testing phase due to dropout can be fixed by rescaling the output to counteract the effects on mean and variance scales. To address the inconsistency caused by dropout in batch normalization, rescaling the output y j can help recover the original scale of mean and variance. The rescaling factor for dropped neurons should be (1 \u2212 p j ) \u22121 for mean and (1 \u2212 p j ) \u22120.5 for variance. Taking into account E[w j ] can further adjust the un-dropped neurons, but this requires additional computation and memory cost. The scaling factor is specifically correct for the variance of y j. Rescaling the output y j in batch normalization after dropout can help recover the original scale of mean and variance. The rescaling factor for dropped neurons should be (1 \u2212 p j ) \u22121 for mean and (1 \u2212 p j ) \u22120.5 for variance. Additional computation and memory cost is required to adjust the un-dropped neurons based on E[w j ]. Simple scaling methods cannot resolve the shift in both mean and variance. Different dropout rescaling factors are explored, such as (1 \u2212 p) \u22120.5 and (1 \u2212 p) \u22120.75 for y when p = 0.2 in the CIFAR10(s) network. The rescaling factor for dropped neurons in batch normalization after dropout should be (1 \u2212 p) \u22121 for mean and (1 \u2212 p) \u22120.5 for variance. When the mean E(y j ) is large, a rescaling factor close to (1\u2212p j ) \u22121 should be used, while for small mean values close to 0, the second term in the variance is ignorable. The rescaling factor for dropped neurons in batch normalization after dropout should be (1 \u2212 p) \u22121 for mean and (1 \u2212 p) \u22120.5 for variance. A trade-off point of (1 \u2212 p) \u22120.75 is proposed to balance between these two factors efficiently during training. This approach ensures consistency in both mean and variance, as shown in the comparison between original dropout and dropout using the rescaling factor. Using dropout with batch normalization in convolutional networks can potentially improve performance. The rescaling factor for dropped neurons in batch normalization after dropout is proposed to be (1 \u2212 p) \u22120.75 to balance mean and variance efficiently during training. The study compares the performance of using dropout with and without batch normalization in convolutional networks. It suggests that using dropout with batch normalization can enhance performance, with larger dropout rates potentially leading to more improvement. However, the accuracy decreases significantly when using the original dropout with batch normalization at dropout rates above 0.15. In contrast, a new rescaling method called \"Jumpout\" shows continuous performance improvement with increasing dropout rates (up to 0.25), outperforming other configurations. Jumpout, a new rescaling method, overcomes original dropout drawbacks by sampling from a decreasing distribution for random dropout rates and normalizing adaptively based on active neurons for consistent regularization and generalization effects. Jumpout normalizes the dropout rate adaptively based on active neurons, enforces consistent regularization effects, and scales outputs differently during training. It requires a main hyper-parameter \u03c3 and two auxiliary truncation hyperparameters (p min , p max) to control the distribution. Jumpout has three hyperparameters: \u03c3, p min, and p max. The values of p min = 0.01 and p max = 0.6 are set to bound samples from the half-normal distribution. The input h j is considered as the features of layer j for one data point. The estimation of q + j can be done separately for each data point in a mini-batch or averaged over data points. In practice, the estimation of q + j for a mini-batch can be done separately for each data point or averaged over data points. Jumpout has similar memory costs as dropout, with minimal computation required for counting active neurons and sampling from the distribution during DNN training. In this section, dropout and jumpout are applied to various DNN architectures, comparing their performance on different benchmark datasets. The architectures include small CNNs, WideResNet, ResNet, and others applied to datasets like CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet. The study applies dropout and jumpout to different DNN architectures on benchmark datasets like CIFAR, Fashion-MNIST, SVHN, STL10, and ImageNet. Standard settings and data preprocessing/augmentation are followed for CIFAR and Fashion-MNIST experiments, while pre-trained models are used for ImageNet with dropout and jumpout training. The study compares dropout and jumpout on various DNN architectures using benchmark datasets like CIFAR, Fashion-MNIST, SVHN, STL10, and ImageNet. Pre-trained models are used for ImageNet experiments, with jumpout consistently outperforming dropout on all datasets and DNNs tested. Jumpout consistently outperforms dropout on all datasets and DNNs tested, including Fashion-MNIST and CIFAR10 where test accuracy is already high. Jumpout also achieves significant improvements on CIFAR100 and ImageNet without the need to increase model size. A thorough ablation study of proposed modifications further confirms the effectiveness of jumpout. The effectiveness of jumpout, a modified version of dropout, is confirmed through a thorough ablation study. Each modification improves the vanilla dropout, and combining all three modifications achieves the best performance. Learning curves show that jumpout outperforms dropout in early learning stages. Jumpout, a modified version of dropout, shows advantages over dropout in early learning stages and reaches good accuracy faster. Future work may focus on finding a better learning rate schedule for jumpout to improve performance. Rescaling factors are applied to y in the network CIFAR10(s). Empirical mean and variance ratios with dropout are compared to without dropout in the plots. The plots compare the mean and variance ratios of y with dropout versus without dropout. The rescaling factor (1 \u2212 p) \u22120.75 shows a good balance between mean and variance rescaling."
}