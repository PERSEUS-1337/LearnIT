{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets of the phrase's meaning by representing cluster centers in a pre-trained word embedding space. An end-to-end trainable neural model predicts the set of cluster centers directly from the input text sequence. The proposed neural model predicts cluster centers from input text during test time, providing interpretable semantic representations that outperform baselines on various NLP tasks. This approach utilizes codebook embeddings for phrases and sentences in a pre-trained word embedding space, offering a novel method for unsupervised phrase similarity, sentence similarity, hypernym detection, and extractive summarization. NLP models like word2vec, GloVe, skip-thoughts, ELMo, and BERT learn representations from raw text without supervision. These models use single embeddings for sentences or phrases, limiting their ability to capture multiple senses or topics without annotation. To address this, new approaches are needed for more nuanced analysis. Word sense induction methods and multi-mode word embeddings aim to represent target words with multiple points in a semantic space by clustering words around them. For example, real property can refer to real estate in legal documents. Real property can have multiple meanings, such as real estate in legal documents or a true characteristic in philosophical discussions. Different from topic modeling like LDA, approaches to discovering these senses require solving distinct clustering problems for each target word. Extending multi-mode representations to phrases or sentences faces efficiency challenges. Efficiency challenges arise when extending multi-mode representations to phrases or sentences due to the large number of unique sequences, making clustering-based approaches difficult. The number of parameters required for clustering increases significantly with the number of unique sequences, posing challenges in estimating and storing such a large number of parameters. Our compositional model aims to predict cluster centers' embeddings from the target phrase's word sequence to reconstruct the co-occurring distribution efficiently. This approach addresses the challenge of sparseness in co-occurring statistics, especially for long sequences with many unique sequences. In this work, a neural encoder and decoder are used to address the challenge of sparseness in co-occurring statistics. The model aims to compress redundant parameters in local clustering problems by predicting cluster centers' embeddings from the target phrase's word sequence efficiently. This approach differs from previous methods by clustering co-occurring words at test time. In this work, a neural encoder and decoder are used to address sparseness in co-occurring statistics. Instead of clustering co-occurring words at test time, a mapping between target sequences and cluster centers is learned during training for efficient prediction. A nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed word embeddings. The proposed model uses a nonnegative and sparse coefficient matrix to match predicted cluster centers with observed word embeddings during training. This allows for joint training of the model and captures compositional meanings of words better than traditional methods in unsupervised phrase similarity tasks. The proposed model outperforms traditional methods in unsupervised phrase similarity tasks by using a nonnegative and sparse coefficient matrix to match predicted cluster centers with observed word embeddings. It can measure asymmetric relations like hypernymy without supervision and demonstrates superior performance in sentence representation compared to single-mode alternatives. The training setup, objective function, and architecture of the prediction mode are detailed in Sections 2.1, 2.2, and 2.3, respectively. The model described in Section 2.2 and Section 2.3 uses codebook embeddings to represent sentences, aiming to reconstruct co-occurring words while avoiding common topics. The model described in Section 2.2 and Section 2.3 uses codebook embeddings to represent sentences, aiming to reconstruct co-occurring words while avoiding common topics. The negatively sampled words are used to avoid predicting common topics, and neighboring words are considered related to the sequence. Training signal is to reconstruct a set of neighboring words within a fixed window size for sentence representation. The model uses codebook embeddings to represent sentences and reconstruct co-occurring words while avoiding common topics. Training signal is to reconstruct neighboring words within a fixed window size for sentence representation, with different models needed for phrases and sentences. The goal is to cluster words that could possibly occur beside a sequence, rather than actual co-occurring words in the training corpus. The model uses codebook embeddings to represent sentences and reconstruct co-occurring words while focusing on semantics. It considers word order information in the input sequence but ignores the order of co-occurring words. The distribution of co-occurring words is modeled in a pre-trained word embedding space. The distribution of co-occurring words in a pre-trained word embedding space is modeled by arranging the embeddings into a matrix. The predicted cluster centers of the input sequence are represented as a matrix in the neural network model. The number of clusters is fixed in this work. The predicted cluster centers of the input sequence are represented as a matrix in the neural network model. The number of clusters is fixed to simplify the design of the prediction model. The reconstruction loss of k-means clustering in the word embedding space is discussed, where a permutation matrix matches cluster centers and co-occurring words. In this work, the relaxation of non-negative sparse coding (NNSC) is adopted to allow positive coefficient values while encouraging sparsity. This approach helps neural networks generate diverse cluster centers in an arbitrary order, unlike k-means clustering which collapses to fewer modes. The NNSC loss is smoother and better captures conditional co-occurrence distributions. The NNSC loss is smoother and easier to optimize for neural networks compared to kmeans loss, which collapses to fewer modes. The coefficient value M k,j is constrained to avoid learning to predict centers with small magnitudes, ensuring stability. The proposed loss function efficiently minimizes L2 distance in a pre-trained embedding space, avoiding the computational expense of estimating permutations between predictions and ground truth words. The coefficient value M k,j is constrained to prevent learning small magnitude centers, ensuring stability. The proposed method uses convex optimization to estimate M Ot on the fly, treating it as a constant for end-to-end training. The loss function prevents the neural network from predicting the same global topics for all inputs. The approach is a generalization of Word2Vec, encoding compositional meaning and decoding multiple embeddings. Our neural network architecture is a generalization of Word2Vec that encodes compositional meaning and decodes multiple embeddings. The encoder maps sentences with similar word distribution closer together, while the decoder outputs a sequence of embeddings instead of words, eliminating the need for discrete decisions. The decoder in our neural network model outputs a sequence of embeddings instead of words, eliminating the need for discrete decisions. By treating the embedding of <eos> as the sentence representation and passing it through different linear layers before inputting it to the decoder, we can capture different aspects in the codebook embeddings. The decoder in the neural network model outputs embeddings instead of words, capturing different aspects in the codebook embeddings. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation. The framework is flexible and allows for replacing encoder and decoder with other architectures. The framework is flexible, allowing for the replacement of encoder and decoder with other architectures. The cluster centers predicted by the model summarize the target sequence well, capturing more semantic facets of a phrase or sentence. The difficulty lies in evaluating topics conditioned on the input sequence. The model uses codebook embeddings to improve unsupervised semantic tasks by capturing semantic facets of phrases or sentences. Pre-trained GloVe embeddings are utilized for sentence and phrase representation, with the model trained on Wikipedia 2016 data. Stop words are removed from the set of co-occurring words in the experiments. Our model is trained on Wikipedia 2016 data with stop words removed from co-occurring words. In phrase experiments, only noun phrases are considered, with boundaries extracted using regular expression rules on POS tags. Sentence boundaries and POS tags are detected using spaCy. Our models do not require additional resources like PPDB or multi-lingual resources, making them practical for domains with low resources such as scientific literature. Our models, trained on Wikipedia 2016 data with stop words removed, use a GloVe embedding size of 300 dimensions. Due to limited computational resources, training is done on a single GPU within a week, leading to underfitting after a week. Comparison with BERT is challenging as it preserves more syntax information through masked language modeling loss. BERT (Devlin et al., 2019) is difficult to compare with our models trained on Wikipedia 2016 data with stop words removed using GloVe embeddings. BERT, with more parameters and computational resources, utilizes a word piece model to address out-of-vocabulary issues. However, we provide its unsupervised performances based on cosine similarity as a reference. Semeval 2013 and Turney 2012 are standard benchmarks for evaluating phrase similarity. BiRD and WikiSRS contain ground truth phrase similarities. Semeval 2013 distinguishes similar phrase pairs, while Turney aims to identify the most similar unigram to a query bigram. In Turney (5), the goal is to identify the most similar unigram to a query bigram among 5 candidates. Our model evaluates two scoring functions for measuring phrase similarity using transformer encoder embeddings. Our model, labeled as Ours Emb, computes cosine similarity between phrase embeddings by comparing reconstruction errors from normalized codebook embeddings. When ranking for similar phrases, negative distance represents similarity. Performance is compared with 5 baselines including GloVe Avg and Word2Vec Avg, which compute cosine similarity between averaged word embeddings, and BERT CLS and BERT Avg for cosine similarities. Our model, labeled as Ours Emb, outperforms baselines in 4 datasets including SemEval 2013 and Turney. The performances are presented in Table 2, showing significant improvements. Our strong performances in Turney verify the effectiveness of our encoder. Our models significantly outperform baselines in 4 datasets, including Turney. The effectiveness of our encoder in incorporating word order information for phrase embeddings is verified. Non-linearly composing word embeddings proves more effective than traditional baselines like GloVe and Word2Vec. Our model (K=1) generally performs slightly better than (K=10), supporting previous findings on multi-mode embeddings. The study found that multi-mode embeddings like Ours (K=10) did not significantly improve word similarity benchmarks, but still performed strongly compared to baselines. The number of clusters (K) did not greatly impact similarity performance, making it easier to select K in practice. The STS benchmark is a common sentence similarity task where models predict semantic similarity scores between sentence pairs. The study compares different models for predicting semantic similarity scores between sentence pairs, including BERT CLS, BERT Avg, GloVe Avg, word mover's distance (WMD), and cosine similarity between skip-thought embeddings (ST Cos). They also introduce a benchmark called STSB Low for sentences with lower similarity scores. In comparison to word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos), a method called GloVe SIF weights words in sentences based on their probability in the corpus. The post-processing step involves removing the first principal component estimated from the training distribution. This method, recommended by Arora et al. (2017), is denoted as GloVe SIF. The GloVe SIF method, which estimates sentence similarity by weighting words based on their probability in the corpus, is compared to GloVe Prob_avg. The strong performance of average embedding suggests considering word embeddings in addition to sentence embeddings for measuring sentence similarity. This is challenging with other methods where sentence and word embeddings are in different semantic spaces. The multi-facet embeddings allow for estimating word importance in a sentence by computing cosine similarity with predicted codebook embeddings and summing the positive similarities. This method differs from others as it uses the same semantic space for both word and sentence embeddings. Our method uses importance weighting for words in the query sentence, which is then multiplied with original weighting vectors to generate different results. Our approach outperforms other methods in matching topics between sets and demonstrates the benefits of multi-mode representation. The proposed method uses importance weighting for words in the query sentence, outperforming other methods in matching topics between sets and demonstrating the benefits of multi-mode representation. Additionally, a variant using a bi-LSTM as the encoder and a LSTM as the decoder showed lower performance compared to the transformer alternative. The variant using a bi-LSTM as the encoder and LSTM as the decoder outperforms the transformer alternative in equation 2 and Table 3. It significantly outperforms ST Cos, supporting the approach of ignoring the order of co-occurring words in the NNSC loss. The model is applied to HypeNet for unsupervised hypernymy detection, based on the assumption that co-occurring words are less related to some hyponyms. The predicted codebook embeddings of a hyponym cluster co-occurring words better than its hypernym. Our asymmetric scoring function outperforms baselines in detecting hypernyms. The accuracy of detecting hypernym direction is compared in Table 4, with methods outperforming baselines. Our (K=1) method performs similarly to Our (K=10). The objective is to discover a summary A with normalized embeddings that reconstruct the distribution of word embeddings in the document. The extractive summarization method optimizes sentence selection based on importance of words, generating multiple codebook embeddings to represent each sentence in the document. The method optimizes sentence selection based on word importance, generating multiple codebook embeddings to represent each sentence in the document. It compares with other ways of modeling sentence aspects, such as average word embeddings and using all words in the sentences as different aspects. The approach with a fixed number of codebook embeddings avoids certain problems. The method, denoted as W Emb, optimizes sentence selection based on word importance by avoiding the problem of fixed codebook embeddings. Results on the testing set of CNN/Daily Mail are compared using F1 of ROUGE in Table 5, with methods choosing 3 sentences following a specific setting. Unsup, No Order means methods do not use sentence position information in the documents. In unsupervised methods for sentence embeddings, sentence order information is crucial, with Lead-3 being a strong baseline. Evaluating unsupervised methods involves comparing those that do not assume the first few sentences make a good summary. Predicting more aspects is done by using higher cluster numbers in Table 5. In unsupervised methods for sentence embeddings, sentence order information is crucial. Comparing unsupervised methods that do not assume the first few sentences form a good summary, predicting more aspects by using higher cluster numbers yields better results. Setting K = 100 gives the best performance after selecting 3 sentences, demonstrating that larger cluster numbers are desired. This method allows for setting K to be a relatively large number, greatly alleviating computational and sample efficiency challenges. Topic modeling has been extensively studied and widely applied for its interpretability and flexibility in incorporating different input features. Topic modeling, extensively studied and widely applied for interpretability and flexibility, can be optimized using neural networks to discover semantically coherent topics efficiently. Sparse coding on word embedding space and parameterizing word embeddings using neural networks are utilized to model multiple aspects of words. Coding on word embedding space is used to model various aspects of a word, while parameterizing word embeddings with neural networks helps test hypotheses and save storage space. Representing words as single or multiple regions in Gaussian embeddings is used to capture asymmetric relations. However, challenges in extending these methods to longer sequences remain unaddressed. One challenge is designing a neural decoder for a set rather than a sequence while modeling dependency between elements. One challenge in extending methods to longer sequences is designing a neural decoder for sets rather than sequences. This involves matching steps between sets and computing distance loss, with options like Chamfer distance and more sophisticated matching losses. Studies focus on measuring symmetric distances between ground truth and predicted sets. Our set decoder aims to efficiently predict clustering centers for set reconstruction, different from other methods focusing on measuring distances between ground truth and predicted sets. Our goal is to efficiently predict clustering centers for set reconstruction using a neural encoder and decoder, overcoming challenges in learning multi-mode representation for long sequences like phrases or sentences. We use a non-negative sparse coefficient matrix during training to match dynamically. The proposed model uses a neural decoder to predict codebook embeddings as representations of sentences or phrases. It outperforms BERT, skip-thoughts, and GloVe in unsupervised benchmarks by learning interpretable clustering centers. Multi-facet embeddings perform best for input sequences. The experimental results show that multi-facet embeddings outperform BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings are most effective for input sequences with multiple aspects, while both multi-facet and single-facet embeddings perform similarly well for sequences with only one aspect. Future plans include training a single model to generate multi-facet embeddings for both phrases and sentences, and evaluating this method as a pre-trained embedding approach for supervised or semi-supervised settings. Additionally, the method will be applied to other unsupervised learning tasks relying on co-occurrence. The method will be applied to unsupervised learning tasks relying on co-occurrence statistics such as graph embedding or recommendation. The model is kept simple to converge training loss quickly, without fine-tuning hyper-parameters. The transformer architecture and most hyper-parameters are similar to BERT, with a sparsity penalty weight of 0.4 on the coefficient matrix. The transformer model used in this study has hyper-parameters similar to BERT, with a sparsity penalty weight of 0.4 on the coefficient matrix. Parameters such as maximal sentence size, maximal number of co-occurring words, number of dimensions in transformers, and number of transformer layers are specified. The transformer model in the study has hyper-parameters similar to BERT, with a sparsity penalty weight of 0.4 on the coefficient matrix. The number of transformer layers on the decoder side varies for different settings, with dropout on attention also changing. The window size is set to 5, and hyperparameters are determined by validation loss. The number of codebook embeddings K is chosen based on training data performance. The performance of the self-supervised co-occurring word reconstruction task is determined by the validation loss. The number of codebook embeddings K is chosen based on training data performance, with larger K potentially leading to longer training times. The hidden embedding size for skip-thoughts is set to 600, and the model has fewer parameters than BERT base. In comparison to BERT base, our model has fewer parameters and uses less computational resources for training. We evaluate BERT Large in unsupervised semantic tasks and find it performs better in similarity tasks but worse in hypernym detection. Our model outperforms BERT base in most cases, especially in phrase similarity tasks. Increasing the model size may improve BERT's performance in similarity tasks, but our method remains superior. The objective function of BERT may not be effective for short sequences like phrases. In Section 3.4, the comparison of different summarization methods is done with a focus on sentence length. The hypothesis that shorter sentences lead to poor performance for certain methods is verified by plotting R-1 performance against sentence length. Our method (K=100) outperforms other methods like W Emb (GloVe) and Sent Emb (GloVe) when summaries are short. In Figure 3, Our method (K=100) outperforms W Emb (GloVe) and Sent Emb (GloVe) when summaries have similar length. W Emb (*) generally outperforms Sent Emb (*) due to selecting more sentences, but this may not be fair as it can cover more topics. Choosing longer sentences may be preferable for fluency in extractive summarization. The figure suggests that Ours (K=100) is the best choice for shorter summaries, while W Emb (BERT) is better for longer summaries. Combining our method with BERT could lead to improved performance in extractive summarization tasks. Combining our method with BERT could improve performance in extractive summarization tasks by using contextualized word embeddings from BERT. Visualizing predicted embeddings from randomly selected sentences in the validation set shows promising results."
}