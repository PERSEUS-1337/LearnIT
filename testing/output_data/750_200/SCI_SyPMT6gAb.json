{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning is crucial for evaluating and improving policies using historical data from a logging policy. The main challenge is to develop counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization show significant improvement over conventional baseline algorithms. Off-policy learning involves evaluating and improving a deterministic policy using historic data, which is crucial due to the high costs and risks associated with on-policy evaluation in real-world scenarios. Utilizing historic data can lead to significant improvements over conventional algorithms. Off-policy learning involves utilizing historic data for safe exploration of policies before deployment, reducing costs and risks associated with on-policy evaluation. Various methods like Q learning, doubly robust estimator, and self-normalized approaches have been studied in reinforcement learning and contextual bandits. A new direction includes using logged interaction data with bandit feedback for off-policy learning. Off-policy learning with bandit feedback involves limited feedback in the form of scalar rewards for actions taken. This approach utilizes logged interaction data to explore policies safely before deployment, reducing costs and risks associated with on-policy evaluation. BID34 introduced a new counterfactual risk minimization framework for off-policy learning in bandit feedback scenarios, addressing the challenge of distribution mismatch between logging and new policies. This framework incorporates sample variance as a regularization term in the empirical risk minimization objective. Our contribution in this paper is three-fold: By connecting to the generalization error bound of importance sampling BID6, we propose a new counterfactual risk minimization framework that includes sample variance as a regularization term in the objective. In this paper, the authors propose a new learning principle for off-policy learning with bandit feedback. They suggest regularizing the generalization error of the new policy by minimizing the distribution divergence between it and the logging policy. The policy is parametrized as a neural network for end-to-end training, and the divergence minimization problem is solved using variational divergence minimization and Gumbel soft-max sampling. The policy is parametrized as a neural network and solves the divergence minimization problem using recent work on variational divergence minimization and Gumbel soft-max sampling. Experimental evaluation on benchmark datasets shows significant performance improvement over conventional baselines, with case studies supporting the theoretical proofs. The policy is parametrized as a neural network and solves the divergence minimization problem using recent work on variational divergence minimization and Gumbel soft-max sampling. We use a family of stochastic policies, where each policy defines a posterior distribution over the output space given the input x, parametrized by some \u03b8. Sampling actions from the distribution h(Y|x) allows for action selection based on probabilities. In online systems, actions are taken by sampling from a distribution h(Y|x), with each action having a probability of h(y|x) being selected. Feedback is observed by comparing the action y to an underlying 'best' y*. The expected risk of a policy h(Y|x) is defined as DISPLAYFORM1, and off-policy learning aims to find a policy with lower risk. In off-policy learning, the goal is to minimize the expected risk of a policy on test data. Data is collected from a logging policy, and the aim is to find an improved policy with lower expected risks. Challenges include skewed distribution of the logging policy and the need to optimize the policy based on observed loss feedback and logging probabilities. In off-policy learning, the goal is to minimize the expected risk of a policy on test data. Challenges include skewed distribution of the logging policy and the need for empirical estimation using finite samples, leading to generalization error and requiring additional regularization. The propensity scoring approach using importance sampling can address the distribution mismatch between different policies. In off-policy learning, challenges include skewed distribution of logging policy, empirical estimation using finite samples, and the need for additional regularization. Propensity scoring with importance sampling can address distribution mismatch. Counterfactual risk minimization introduces regularization to address flaws in the vanilla approach, such as loss scaling invariance and large variance. The authors proposed a regularization term for sample variance derived from empirical Bernstein bounds to address loss scaling and large variance in off-policy learning. They approximated the regularization term via first-order Taylor expansion to enable stochastic training, but this neglects non-linear terms from second-order and above. The authors introduced a stochastic optimization algorithm that neglects non-linear terms from second-order and above, leading to approximation errors in reducing sample variance. They aim to derive a variance bound directly from the parametrized distribution h(Y|x) instead of estimating it empirically from samples. The average loss reweighted by importance sampling function termR(h) is discussed, along with a general learning bound for importance sampling weights. An identity lemma is presented for random variables z with importance sampling weights, leading to an upper bound derivation. Theorem 1 provides an upper bound for the second moment of the weighted loss based on the R\u00e9nyi divergence. It involves random variables X and Y with a loss function \u03b4(x, y) and conditional divergence d2(h(y|x)||h0(y|x); P(x)). The bound is similar to Eq. (4) but considers a joint distribution over x, y. Detailed proofs can be found in Appendix 1. Theorem 2 provides a generalization bound between expected risk R(h) and empirical risk R(h) using distribution divergence function. The proof involves Bernstein inequality and second moment bound, with detailed proofs in Appendix 7. This result showcases bias-variance trade-offs. The proof of the theorem involves Bernstein inequality and the second moment bound, showcasing bias-variance trade-offs in empirical risk minimization problems. It suggests minimizing variance regularized objectives in bandit learning settings to control the trade-off between empirical risk and model. In a test setting, minimizing the variance regularized objectives is crucial. The model hyper-parameter \u03bb controls the trade-off between empirical risk and model variance. However, setting \u03bb empirically and optimizing the objective pose challenges. To address this, an alternative formulation of regularized ERM is explored, inspired by distributionally robust learning. This involves a constrained optimization approach, drawing from the method of Langaragian multiplier for constrained optimization. The new constrained optimization formulation, inspired by the method of Langaragian multiplier, introduces a regularization hyper-parameter \u03c1 in the objective function. This formulation provides a good surrogate for the true risk, with the difference bounded by \u03c1 and approaching 0 as N \u2192 \u221e. It eliminates the need to compute sample variance in existing bounds, especially when dealing with a parametrized distribution. The new objective function eliminates the need to compute sample variance in existing bounds when dealing with a parametrized distribution. Recent f-gan networks and Gumbel soft-max sampling can help in estimating the divergence function. The bounds also highlight the importance of stochasticity in the logging policy for counterfactual learning. The need for stochasticity in the logging policy is emphasized for counterfactual learning. A deterministic policy with peaked masses and zeros in certain regions makes learning difficult as those regions are never explored. The divergence term calculation reflects this, showing that a deterministic policy leads to an unbounded generalization bound. The unbounded generalization bound in counterfactual learning is due to the integral results being unbounded, making it not possible in this case. The objective requires minimizing the square root of the condition, with a convex function involved in the calculation. The connection between divergence and the minimization objective is highlighted. The connection between divergence and the minimization objective is highlighted in the context of counterfactual learning. The f-GAN method proposed in BID26 provides a lower bound for variational divergence minimization. The dual formulation is derived using Fenchel convex duality, showing the tightness of the bound when T 0 (x) = f (h/h 0 ). The bound is tight when T 0 (x) = f (h/h 0 ), and neural networks can approximate continuous functions with any desired precision. By choosing the family of neural networks for T, the equality condition can be satisfied theoretically. By choosing neural networks for the family of T, the final objective is a saddle point of a function mapping input pairs to a scalar value. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence. The true divergence is denoted by Df = sup T T dhdx - f*(T)dh0dx, with \u0125 and h0 as the empirical distributions obtained by sampling. The true divergence Df is denoted by dhdx - f*(T)dh0dx, with \u0125 and h0 as the empirical distributions obtained by sampling. The estimation error is decomposed into two terms, one from restricting the parametric family of T to neural networks and the other from the approximation error of empirical mean estimation to the true distribution. The estimation error is decomposed into two terms: one from restricting the parametric family of T to neural networks and the other from the approximation error of empirical mean estimation to the true distribution. The first term involves the difference between an empirical distribution and the underlying population distribution, which can be verified using the strong law of large numbers. By optimality condition, T0 = h(y|x) / h0(y|x), where both h and h0 are probability density functions. The ratio is integrable due to the bounded loss assumption, and f*(T0) = 2T0 - 1 is also integrable. Applying the strong law of large numbers, both terms approach 0. The second term can be addressed using Theorem 5, leading to it approaching zero almost surely. A generative-adversarial approach can be applied by representing the T function as a discriminator network and parametrizing the policy distribution as a generator neural network. Gumbel soft-max sampling methods are used for differential sampling in structured output problems with discrete values of y. The Gumbel soft-max sampling method is used for differential sampling in structured output problems with discrete values of y. The training procedure involves sampling from a logging policy and optimizing a generator distribution to minimize divergence. The Gumbel soft-max sampling method is utilized for differential sampling in structured output problems with discrete values of y. The training algorithm involves sampling mini-batches of 'real' and 'fake' samples from the data, updating for variance regularization, and presenting the end-to-end learning process for counterfactual risk minimization from logged data. The algorithm solves the robust regularized formulation and includes training for the original ERM formulation. The algorithm solves the robust regularized formulation by sampling mini-batches of data and updating for variance regularization. It includes training for the original ERM formulation and aims to minimize counterfactual risk from logged data. The algorithm works in two separate training steps: 1) update the policy parameters to minimize the reweighted loss, and 2) update the generator and discriminator parameters to regularize variance for improved generalization. Exploiting historical data is crucial in multi-armed bandit problems. The discriminator is used to regularize the variance and improve the generalization performance of the new policy in bandit problems. Various approaches, such as doubly robust estimators, have been proposed to exploit historical data in multi-armed bandit scenarios. These techniques have also been extended to reinforcement learning problems. Recent works in deep RL have addressed off-policy updates using methods like multi-step bootstrapping and off-policy training of Q functions. Learning from log traces involves applying propensity scores to evaluate candidate policies. Off-policy training of Q functions involves using propensity scores to evaluate candidate policies and estimate treatment effects from observational studies. Unbiased counterfactual estimators have been derived for computational advertising, while techniques to reduce bandit learning to supervised learning have shown poor generalization performance. Variance regularization aims at improving off-policy learning. Variance regularization aims to improve off-policy learning by addressing distribution mismatch in importance sampling problems and supervised learning with a convex objective function. Our divergence minimization technique can be applied to supervised learning and domain adaptation problems to address distribution match issues. Regularization for our objective function is closely connected to distributionally robust optimization techniques. To learn a classifier, the empirical risk is minimized over an ellipsoid uncertainty set. The Wasserstein distance between empirical and test distributions is a well-studied constraint for robust generalization performance. Empirical evaluation involves converting supervised learning to bandit feedback. A logging policy is constructed, predictions are sampled, and feedback is collected. Conditional random field policy is also used for benchmarks. In evaluation, two types of metrics are used for the probabilistic policy h(Y|x): expected loss ('EXP') and bandit feedback datasets are created by sampling predictions y i \u223c h 0 (y|x i ) and collecting feedback as \u03b4(y * i , y i ). The logging policy h 0 is trained on 5% of D * using a conditional random field (CRF) policy, and hamming loss is used as the loss function. Each sample x i is passed four times to h 0 to record sampled actions y i, loss value \u03b4 i, and propensity score p i = h 0 (y i |x i ). In evaluation, two types of metrics are used for the probabilistic policy h(Y|x): expected loss ('EXP') and average hamming loss of maximum a posteriori probability (MAP) prediction y MAP = arg max h(y|x). MAP predictions are faster but may not capture the diversity of predictions. In evaluation, two types of metrics are used for the probabilistic policy h(Y|x): expected loss ('EXP') and average hamming loss of maximum a posteriori probability (MAP) prediction y MAP = arg max h(y|x). MAP predictions are faster but may not capture the diversity of predictions. However, a model with high MAP performance but low EXP performance might be over-fitting, as it may be centering most of its probability masses in the regions where h 0 policy obtained good performance. Baselines Vanilla importance sampling algorithms using inverse propensity score (IPS), and the counterfactual risk minimization algorithm from Swaminathan & Joachims (2015a) (POEM) are compared, with both L-BFGS optimization and stochastic optimization solvers. The hyperparameters are selected by. Neural network policies without divergence regularization (NN-NoReg) are compared as baselines to verify the effectiveness of variance regularization. Four multi-label classification datasets from the UCI machine learning repo are used, and supervised to bandit conversion is performed. Statistics are reported in the Appendix. In the UCI machine learning repo, neural network policies are compared with divergence regularization. Three-layer feed-forward neural network is used for policy distribution, and a two or three layer feed-forward neural network for divergence minimization. Separate training version 2 is used for benchmark comparison. Networks are trained with Adam of learning rate 0.001 and 0.01 for reweighted loss and divergence minimization. PyTorch is used for implementation. The neural network policies are compared with divergence regularization using Adam for training. Two Gumbel-softmax sampling schemes are evaluated, NN-Soft and NN-Hard, showing improved results. Codes for reproducing the results are available for download. By introducing neural network parametrization of policies, test performance significantly improves compared to baseline CRF policies. Additional variance regularization further enhances testing and MAP prediction loss. No significant difference between Gumbel soft-max sampling schemes. Varying maximum number of. In the study, the effectiveness of variance regularization is quantitatively analyzed by varying the maximum number of iterations in each divergence minimization sub loop. The expected loss in test sets is plotted against epochs average over 10 runs with error bars using the dataset yeast. Models with no regularization have higher loss and slower convergence. The study analyzed the effectiveness of variance regularization by varying the maximum number of iterations in each divergence minimization sub loop. Models without regularization had higher loss and slower convergence, while adding regularization led to faster convergence and lower test loss. The stronger the regularization imposed, the better the test performance. The study showed that regularization helps the training algorithm converge faster and improve test performance. Increasing the number of training samples also enhances generalization performance. Regularized policies outperformed non-regularized ones in terms of generalization. Regularization improves test performance and generalization ability, reaching a stable level. Regularized policies outperform non-regularized ones in generalization. However, MAP performance decreases after training samples exceed 24, indicating potential overfitting. After training samples exceed 24, MAP prediction performance starts to decrease, suggesting potential overfitting. Experiments compare cotraining in Alg. 3 and the easier version Alg. 2, as well as two Gumbel-softmax sampling schemes. Blending weighted loss and distribution divergence slightly improves performance but makes training more difficult due to balancing gradient objectives. The training with regularization is more challenging due to balancing gradient objectives, but there is no significant performance difference between different sampling schemes. The effect of logging policies on learning performance is discussed, emphasizing the importance of the stochasticity of the logging policy. The algorithm's ability to learn an improved policy relies on the stochasticity of the logging policy. By introducing a temperature multiplier \u03b1 to modify the parameter h0, the policy can become deterministic as \u03b1 increases. Varying \u03b1 in the range of 2 [-1,1,...,8] showed an improvement in the algorithm's performance. NN policies outperform logging policy when h0 is sufficiently stochastic, but struggle when temperature parameter exceeds 2/3. Stochasticity does not affect loss values, but ratios decrease due to logging policy's lower loss. Stronger regularization in NN policies leads to slightly better performance. The decreased loss of the logging policy h0 resulted in improved NN policy performance. Stronger regularization in NN policies showed better performance against weaker ones, indicating robustness in learning principles. Regularization helps the model be more robust and achieve better generalization. As h0 improves, models consistently outperform baselines, but the difficulty increases with the quality of h0. More visualizations of metrics can be found in the appendix. The impact of logging policies on improved policies is discussed, showing a trade-off between policy accuracy and sampling biases. Varying the proportion of training data points used to train the logging policy affects the performance of improved policies. In this study, the impact of logging policies on improved policies is analyzed by varying the proportion of training data points. The NN and NN-Reg policies outperform the logging policy, indicating their ability to address sampling biases. The increasing ratios of test expected loss to h0 performance reflect the relative policy improvement. The study starts with the intuition that explicitly regularizing variance can enhance generalization performance in off-policy learning for logged bandit datasets. The paper proposes a new training principle for off-policy learning in logged bandit datasets by regularizing variance to improve generalization performance. The training objective combines importance reweighted loss with a regularization term measuring distribution match. Variational divergence minimization and Gumbel soft-max sampling techniques are used to train neural network policies to minimize variance regularized objective. By applying variational divergence minimization and Gumbel soft-max sampling techniques, neural network policies are trained end-to-end to minimize the variance regularized objective. Evaluations on benchmark datasets confirmed the effectiveness of the learning principle and training algorithm. Case studies further supported the theoretical discussion. The main limitation is the requirement for propensity scores, which may not always be available. Learning to estimate propensity scores and incorporating them into the training framework will enhance the applicability of the algorithms. Learning to estimate propensity scores and incorporating them into the training framework will enhance the applicability of algorithms, with theoretical guarantees for directly learning importance weights. The techniques and theorems can be extended to general supervised learning and reinforcement learning, making it interesting to study further applications. The study focuses on applying Lemma 1 to importance sampling weight functions and loss functions, utilizing Reni divergence and Bernstein's concentration bounds to bound the variance. The goal is to optimize a generator for bandit learning with theoretical guarantees. The study aims to optimize a generator for bandit learning using importance sampling weight functions and loss functions, with theoretical guarantees provided by Reni divergence and Bernstein's concentration bounds. The algorithm involves updating the discriminator and generator iteratively until convergence. The study optimizes a generator for bandit learning using importance sampling weight functions and loss functions. The algorithm involves updating the discriminator and generator iteratively until convergence. Mini-batches of samples are used to estimate the generator gradient and update the parameters. The statistics of the datasets are reported, showing the effect of stochasticity on test loss. As the logging policy becomes more deterministic, neural network policies still find improvement over the initial policy in expected loss and loss with MAP predictions. No clear trend is observed in the performance of MAP predictions, possibly due to the initial policy already having good MAP. The study explores optimizing a generator for bandit learning using importance sampling weight functions and loss functions. Neural network policies show improvement over the initial policy in expected loss and loss with MAP predictions. However, no clear trend is observed in the performance of MAP predictions, possibly due to the initial policy already having good MAP prediction performance. Further investigation is warranted. Neural network policies show improvement over the initial policy in expected loss. However, it will be difficult for NN policies to beat the logging policy in MAP predictions if it was already exposed to full training data."
}