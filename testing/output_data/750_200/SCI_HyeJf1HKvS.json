{
    "title": "HyeJf1HKvS",
    "content": "This work introduces a two-stage neural architecture for learning structural correspondences between graphs. It utilizes localized node embeddings from a graph neural network to rank soft correspondences initially, followed by synchronous message passing networks to iteratively refine the rankings for matching consensus in local neighborhoods. The message passing scheme computes a reliable measure of consensus for corresponding neighborhoods, guiding the re-ranking process effectively. The architecture is scalable to large, real-world inputs while being purely local and sparsity-aware. Graph matching involves establishing structural correspondences between nodes in graphs by considering node and edge similarities. A two-stage neural architecture uses localized node embeddings and message passing networks to rank correspondences and refine them iteratively. The method is scalable and effective for tasks in computer vision and entity alignment between knowledge graphs, outperforming current state-of-the-art approaches. Graph matching involves establishing structural correspondences between nodes in graphs by considering node and edge similarities. This problem is crucial in various real-world applications such as cheminformatics, bioinformatics, social network analysis, and computer vision tasks like object tracking and shape recognition. Graph matching, a crucial problem in various real-world applications like computer vision, can be formulated as a graph matching problem. Different approaches such as graph edit distance and maximum common subgraph problem have been explored, but solving them optimally for large-scale instances is challenging. These combinatorial approaches do not adapt well to data distribution. Various neural architectures have been proposed to tackle graph matching or similarity tasks in a data-dependent fashion, incorporating continuous node embeddings for crucial semantic information. These approaches address the limitations of purely combinatorial methods, which struggle with large-scale instances and lack adaptability to data distribution. Graph matching approaches in 2018 are limited in their ability to compute similarity scores between graphs, rely on inefficient global matching procedures, and do not generalize well to unseen graphs. The typical formulation involves an edge-preserving quadratic assignment problem with one-to-one mapping constraints. The formulation for graph matching in 2018 is based on neighborhood consensus to prevent adjacent nodes from being mapped to different regions. The problem involves supervised and semi-supervised matching of graphs with the inductive bias of neighborhood consensus. In this work, the problem of supervised and semi-supervised graph matching is addressed by incorporating the intuition of neighborhood consensus into the model. The proposed deep graph matching architecture consists of two stages and aims to generalize to unseen graph pairs in the supervised setting, while utilizing complete graph structures in the semi-supervised setting. The proposed deep graph matching architecture includes a local feature matching procedure and an iterative refinement strategy using synchronous message passing networks. The feature matching step computes initial correspondence scores based on local node embeddings, while the refinement strategy aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The method is scalable to large, real-world inputs. The deep graph matching architecture involves a two-stage neighborhood consensus approach for correspondences, scalable to real-world inputs. The deep graph matching architecture utilizes a neural network to update correspondences between nodes in the source and target graphs based on pair-wise color differences. This approach is inspired by related methods and involves computing similarities between nodes using node embeddings. The sinkhorn normalization is applied to obtain correspondence matrices that satisfy certain constraints. The deep graph matching architecture uses sinkhorn normalization to create correspondence matrices that meet specific constraints. The matrices are interpreted as discrete distributions over potential correspondences in the target graph for each node in the source graph. The neural network is trained in a supervised manner against ground truth correspondences by minimizing the negative log-likelihood of correct correspondence scores. The network is implemented as a Graph Neural Network (GNN) to obtain localized, permutation. The Graph Neural Network (GNN) is implemented to minimize the negative log-likelihood of correct correspondence scores by aggregating localized information through a neural message passing scheme. Various operators from geometric deep learning and relational representation learning are available for selection. The work in geometric deep learning and relational representation learning offers a variety of operators for precise feature control. However, the local nature of node embeddings can lead to false correspondences, violating neighborhood consensus criteria. Detecting and resolving these violations in local neighborhoods is crucial due to the NP-hard nature of finding a global optimum. The proposed algorithm aims to detect violations of neighborhood consensus criteria using graph neural networks in an iterative fashion. The soft correspondence matrix is utilized to pass node functions between graphs, refining correspondences iteratively. The algorithm detects violations of neighborhood consensus criteria using graph neural networks iteratively. The soft correspondence matrix passes node functions between graphs, refining correspondences. Using S, node indicator functions are mapped from G_s to G_t, and synchronous message passing is performed on both graphs via a shared graph neural network \u03a8 \u03b82 to measure neighborhood consensus between node pairs. The algorithm uses a shared graph neural network to measure neighborhood consensus between node pairs and update correspondence scores. This process is iteratively applied to improve consensus and resolve ambiguities in node colorings. The algorithm utilizes a shared graph neural network to enhance neighborhood consensus between nodes and adjust correspondence scores, resolving ambiguities in node colorings. The consensus stage employs local operators to distribute global node colorings and address false matchings from the initial stage. The importance of the two-stage approach is highlighted, with the necessity of an initial matching for testing neighborhood consensus. Theorems demonstrate the effectiveness of measuring local neighborhood matches between graphs G s and G t using d i,j as a metric. Theorem 1 states that if G s and G t are isomorphic graphs and \u03a8 \u03b82 is a permutation equivariant GNN, then S encodes an isomorphism between G s and G t. Theorem 2 shows that if \u03a8 \u03b82 is a permutation equivariant and T-layered GNN with injective AGGREGATE and UPDATE functions, it provides equal node embeddings. Both Theorem 1 and 2 show that permutation equivariant GNNs with injective functions provide equal node embeddings. Common GNN architectures are equivariant due to permutation invariant neighborhood aggregators, while injectivity can be achieved by using powerful GNNs like the Weisfeiler & Lehman heuristic. The proposed approach utilizes graph structures with sum aggregation and MLPs on neighboring node features. It is related to classical graph matching techniques like the graduated assignment algorithm. Starting from an initial feasible solution, a new solution is iteratively computed by solving a linear assignment problem. The proposed approach involves iteratively computing a new solution by solving a linear assignment problem using the softassign operator with sinkhorn normalization. The gradient Q is related to a neighborhood consensus scheme for a non-trainable GNN instantiation. The proposed approach involves updating correspondence scores using trainable neural networks based on the difference between node representations. This deep parameterized model is a generalization of the graduated assignment algorithm and supports continuous node and edge features through established GNN models. Our approach updates correspondence scores using trainable neural networks based on node representations, supporting continuous features. Optimizations are applied to scale the algorithm to large input domains, with sparsification of initial correspondences before neighborhood consensus. The algorithm updates correspondence scores using neural networks based on node representations and supports continuous features. Sparsification of initial correspondences is done before neighborhood consensus to reduce memory footprint and time complexity. The feature matching procedure ranks the correct correspondence within the top k elements for each node, emphasizing the importance of optimizing the initial feature matching loss. Optimizing the initial feature matching loss is crucial for updating correspondence scores using neural networks. To accelerate this process, node indicator functions can be replaced with randomly drawn node functions, maintaining injectivity. By sampling from a continuous distribution, node indicator functions remain injective. Theorem 1 holds without restrictions on function space L(G s). Theorem 2 may not hold, but the refinement strategy resolves ambiguities by re-sampling R (l) s in each iteration. Softmax normalization satisfies rectangular doubly-stochastic solutions but may lead to inconsistent integer solutions early on. Softmax normalization may lead to inconsistent integer solutions early on. To address this, row-wise softmax normalization is proposed to relax constraints and allow for supervised refinement. Experimental results show that row-wise normalization is sufficient for the algorithm to converge to the correct solution. The algorithm converges to the correct solution via neighborhood consensus by using row-wise normalization. Varying the number of refinement iterations for training and testing speeds up runtime and encourages convergence with fewer steps. Decreasing the number of iterations during training does not affect convergence abilities during testing. Our method shows that decreasing the number of training iterations does not impact convergence abilities during testing. It is validated on various tasks including synthetic graphs, supervised keypoint matching, and semi-supervised knowledge graph alignment. The implementation is in PYTORCH using PYTORCH GEOMETRIC and KEOPS libraries for processing sparse mini-batches. Our method, implemented in PYTORCH using PYTORCH GEOMETRIC and KEOPS libraries, can process sparse mini-batches with GPU acceleration and minimal memory usage. Optimization is done via ADAM with a fixed learning rate. Hits@k is used to evaluate the model's performance. In our experiment, we evaluate our model on synthetic graphs to learn matching pairs in a supervised manner. Graphs consist of Erd\u0151s & R\u00e9nyi graphs with different node sizes and edge probabilities. Training and evaluation are conducted on various graph configurations. Additional experiments are performed in Appendix E. Training and evaluation of the model is done on 1,000 graphs with different configurations. Additional experiments in Appendix E verify the robustness of the approach towards node addition or removal. The graph neural network operators are implemented by stacking three layers of the GIN operator for its expressiveness in distinguishing raw graph structures. Input features are initialized with one-hot encodings of node degrees. The model is trained and evaluated on 1,000 graphs with various configurations. Input features are initialized with one-hot encodings of node degrees. The matching accuracy Hits@1 is shown for different choices of |V s | and p, with a decrease in performance observed with increasing structural noise. The proposed two-stage architecture outperforms the purely local matching approach. The proposed two-stage architecture can recover all correspondences regardless of structural noise levels. This applies to both initial sinkhorn(\u015c (L)) and optimized softmax(\u015c (L)) variants, showcasing the benefits of matching consensus and scalability enhancements. The proposed two-stage architecture can recover all correspondences regardless of structural noise levels, showcasing the benefits of matching consensus and scalability enhancements. The refinement strategy improves performance when operating on sparsified top k correspondences, converging to the perfect solution with increasing iterations during testing. The proposed two-stage architecture can recover all correspondences regardless of noise levels, showcasing matching consensus and scalability enhancements. Experiments on PASCALVOC and WILLOW-OBJECTCLASS datasets demonstrate the effectiveness of the approach in recovering correct correspondences. The PASCALVOC dataset contains annotated images with keypoint locations, filtered to exclude difficult objects, with varying scale and number of keypoints. The WILLOW-OBJECTCLASS dataset has consistent orientations for each category with exactly 10 keypoints per image. The WILLOW-OBJECTCLASS dataset contains 40 images per category with 10 keypoints each. The model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 images per class for training. Graphs are constructed using Delaunay triangulation of keypoints, and input features are from a pre-trained VGG16 model on IMAGENET. Architecture and parameters are adjusted for fair comparison with previous studies. The features of keypoints are extracted from a pre-trained VGG16 model on IMAGENET. The graph neural network operator used is SPLINECNN, with a trainable B-spline based kernel function conditioned on edge features between node-pairs. Results are evaluated for both isotropic and anisotropic cases. The features of keypoints are extracted from a pre-trained VGG16 model on IMAGENET. For SPLINECNN, a kernel size of 5 in each dimension is used, along with a hidden dimensionality. The edge features are given as normalized relative distances and 2D Cartesian coordinates. For SPLINECNN, a kernel size of 5 is used in each dimension with a hidden dimensionality of 256. The network architecture includes two convolutional layers, dropout with a probability of 0.5, and a final linear layer. Training involves forming pairs between training examples of the same category and evaluating the model with test graph pairs. Results show that the refined architecture using isotropic and anisotropic GNNs outperforms competing methods and non-refined baselines in Hits@1 for PASCALVOC and WILLOW-OBJECTCLASS datasets. Visual qualitative results are also presented in Appendix I. Our refinement strategy significantly outperforms competing methods and non-refined baselines on the WILLOW-OBJECTCLASS dataset, reducing errors by half across all categories. Starting from a weaker initial feature matching baseline, improvements of up to 14 percentage points are seen on PASCALVOC. Task-specific isotropic or anisotropic GNNs help improve performance further in the consensus stage. In the refinement strategy, initial matchings aid in enhancing performance in the consensus stage using task-specific isotropic or anisotropic GNNs for \u03a8 \u03b81. The approach is validated by addressing the geometric feature matching problem with only point coordinates. Experimental training setup follows Zhang & Lee (2019) and generalization testing is done on the PASCALPF dataset (Ham et al., 2016) using synthetic graph pairs. Source points are randomly sampled and Gaussian noise is added to generate target points. The study involves generating synthetic graph pairs by sampling source points, adding Gaussian noise, and introducing outliers. Graphs are constructed by connecting nodes with their k-nearest neighbors. The unmodified anisotropic keypoint architecture is trained on 32,000 synthetic examples. Evaluation is done on the PASCALPF dataset, showing Hits@1 results in Table 3. The study evaluates a consensus architecture on the PASCALPF dataset, consisting of 1,351 image pairs across 20 classes. Results show improvement over previous state-of-the-art results, highlighting the benefits of the consensus stage. The model also performs well on the DBP15K datasets, linking entities from different knowledge graphs. The model is evaluated on the DBP15K datasets, linking entities from Chinese, Japanese, and French knowledge graphs to the English version. Entity input features are obtained using monolingual FASTTEXT embeddings and aligned into the same vector space. The sum of word embeddings is used as the final entity input representation. The model aligns entity input features from different languages using word embeddings and a graph neural network operator. Training is done using negative log likelihood. The model aligns entity input features from different languages using a three-layer GNN with dimensionality 256 and 32. Training is performed in a semi-supervised fashion using negative log likelihood. Results are evaluated using Hits@1 and Hits@10, comparing the model to previous work. Our model improves upon the state-of-the-art on all categories with gains of up to 9.38 percentage points. The refinement strategy consistently enhances Hits@1 results significantly, while Hits@10 results are affected by the refinement operating on sparsified top 10 initial correspondences. Our approach significantly improves initial correspondences and Hits@1 results, with scalability for multiple refinement iterations. Experimental results show effective solving of real-world problems, but limitations inherited from GNNs and WL heuristic for graph isomorphism testing. Our method inherits limitations from GNNs and WL heuristic for graph isomorphism testing. One limitation is the potential failure to converge when two nodes are assigned the same color by WL, resulting in non-convergence due to equal neighborhood sets. Adding noise to initial correspondence distributions may help resolve these ambiguities. Identifying correspondences between nodes in graphs has been extensively studied in various domains. Related problems include maximum common subgraph, network alignment, graph edit distance, and graph matching. Graph neural networks have also been recently explored in this area. Graph neural networks have become a focus of research for deep graph matching techniques. A two-stage neural architecture was presented for learning node correspondences between graphs in a supervised or semi-supervised fashion. The approach aims to reach a neighborhood consensus. Our two-stage neural architecture aims to learn node correspondences between graphs by reaching a neighborhood consensus and resolving violations iteratively. We proposed enhancements for scalability and evaluated our approach on real-world datasets, consistently outperforming the state-of-the-art. The final optimized algorithm is provided in Algorithm 1. The T-layered GNN \u03a8 \u03b82 can distinguish graph structures using injective node colorings, achieving the power to differentiate any graph structure. The T-layered GNN \u03a8 \u03b82, operating on injective node colorings, can distinguish any graph structure using a permutation matrix to describe isomorphisms between subgraphs. The T-layered GNN \u03a8 \u03b82 uses a permutation matrix to describe isomorphisms between subgraphs, extending the graduated assignment algorithm with trainable parameters. The algorithm's impact was evaluated by implementing a non-trainable GNN instantiation \u03a8 \u03b82. Using trainable neural networks \u03a8 \u03b82 improves results compared to fixed-function message passing schemes. The approach can learn to utilize node and edge features for refining procedures and offers a variety of task-dependent GNN operators. The approach discussed in the previous paragraph utilizes trainable neural networks to improve results compared to fixed-function message passing schemes. It allows for the selection of task-dependent GNN operators and the experimental validation of robustness towards node addition or removal through synthetic experiments. The consensus stage in the approach utilizes neural networks to improve matching results, showing robustness to node addition or removal in synthetic experiments. Our neural architecture addresses the challenge of identifying correspondences between nodes of two graphs by gradually decreasing false positive influences of unmatched nodes in the refinement stage. This problem is studied in graph theory under the combinatorial maximum common subgraph isomorphism problem. The combinatorial maximum common subgraph isomorphism problem in graph theory is NP-hard and remains so even in trees unless the common subgraph is required to be connected. Most variants of the problem are difficult to approximate with theoretical guarantees. Exact polynomial-time algorithms are available for specific problem variants relevant in cheminformatics. Exact polynomial-time algorithms are available for specific problem variants relevant in cheminformatics, while in bioinformatics and computer vision, techniques for network alignment or graph matching are developed for large networks without specific structural properties. In graph matching, the goal is to minimize a function for two graphs of order n with adjacency matrices A s and A t, using permutation matrices in the set P and the squared Frobenius norm. The function to be minimized involves permutation matrices in set P and the squared Frobenius norm. Minimizing Equation (12) is equivalent to solving Equation (1). Previous research has focused on minimizing Equation (12) using a Frank-Wolfe type algorithm and projecting the fractional solution to P. The applicability of relaxation and projection in minimizing Equation (12) using a Frank-Wolfe type algorithm is still poorly understood, with few theoretical results existing. The WL heuristic distinguishes two graphs G s and G t if there is no fractional S such that the objective function takes 0. The Frank-Wolfe algorithm can be modified to obtain the WL partition. The Frank-Wolfe algorithm can be modified to obtain the WL partition for graph matching. Aflalo et al. (2015) showed that the standard relaxation provides a correct solution for certain asymmetric graphs characterized by spectral properties. Bento & Ioannidis (2018) studied different relaxations for graph matching. Other approaches include spectral relaxations (Umeyama, 1988; Leordeanu & Hebert, 2005) and random walks (Gori et al., 2005). Graph matching is closely related to the quadratic assignment problem (QAP). Graph matching is closely related to the quadratic assignment problem (QAP), with recent literature considering a weighted version incorporating node and edge similarities. Zhou & De la Torre (2016) proposed factorizing the affinity matrix into smaller matrices and incorporating global geometric constraints. Several approaches have been proposed for solving the graph matching problem, including factorizing the affinity matrix, incorporating global geometric constraints, and studying kernelized graph matching with node and edge similarities expressed as kernels. Lagrangean decompositions and dual ascent algorithms have also been explored for solving this problem efficiently. Recently, functional representation for graph matching has been proposed as a generalizing concept with the goal to avoid constructing the affinity matrix. The graph edit distance measures the minimum cost to transform one graph into another by adding, deleting, and substituting vertices and edges. The graph edit distance measures the minimum cost to transform a graph by adding, deleting, and substituting vertices and edges. It is NP-hard and related to the maximum common subgraph and quadratic assignment problems. Recent exact algorithms have been proposed, but are limited to small graphs, leading to the development of heuristics based on the assignment problem. The problem of network alignment involves finding the similarity between pairs of nodes in graphs. Algorithms typically follow a two-step approach, with heuristics based on the assignment problem widely used in practice. These heuristics can reduce running time to quadratic or even linear for restricted cost functions. The problem of network alignment involves finding node similarities using a two-step approach. Algorithms compute a similarity matrix from the graphs' topology and then solve the assignment problem to align nodes. ISORANK by Singh et al. (2008) uses the adjacency matrix of the product graph, while Kollias et al. (2012) apply PAGERANK to compute the alignment. The matrix M is obtained by applying PAGERANK using a normalized version of K as the GOOGLE matrix and node similarities as the personalization vector. Efficient approximations of ISORANK have been proposed to avoid generating the product graph of quadratic size. An extension supporting vertex and edge similarities has been presented, with computation using nonexact techniques. Network alignment can be solved by linearizing the quadratic optimization problem into an integer linear program, approached via Lagrangian relaxation. A message passing algorithm for sparse network alignment has been developed. In network alignment, various techniques aim to find an optimal correspondence between vertices of two graphs. Recent approaches include learning node and edge similarity functions for specific tasks, such as using a cost model for graph edit distance. Another approach involves linearizing the optimization problem into an integer linear program and using Lagrangian relaxation. The method presented in this work is related to different lines of research in deep graph matching procedures, including utilizing local node feature matchings and cross-graph embeddings. Refining local feature matchings by enforcing neighborhood consistency has been relevant for matching in images. The functional maps framework aims to solve a similar problem for manifolds. Deep graph matching has been heavily investigated in a deep fashion, with various approaches such as supervised deep graph matching networks based on displacement and combinatorial objectives. Different models have been proposed, with some using a differentiable spectral graph matching solver while others have fully-learnable matching procedures. Our matching procedure is fully-learnable, unlike other approaches that use node-wise features and dense node-to-node cross-graph affinities. Previous methods involve mapping point coordinates into a high-dimensional space and computing pairwise inner products between point embeddings for matching, but they do not naturally resolve inconsistent neighborhood assignments like our work does. Our approach resolves inconsistent neighborhood assignments in graph matching by utilizing the Gromov-Wasserstein discrepancy. Xu et al. (2019) enhanced the optimal transport objective by learning node embeddings to account for noise in graphs, extending this concept to multi-graph partitioning and matching. Our supervised approach involves optimal transport between nodes for sets of graphs. Our approach involves learning a Gromov-Wasserstein barycenter for partitioning and matching graphs in a supervised manner. It can generalize to unseen graph instances and is different from previous work on network alignment using CYCLEGANs and NODE2VEC embeddings. The approach involves utilizing a deep graph model with global and local network topology preservation as auxiliary tasks. Different methods such as fast local matching and shared graph neural networks have been used to approximate graph edit distance and fine-tune network output. These approaches focus on local node embeddings, which may lead to matching correspondences inaccuracies. The curr_chunk discusses enhancing node embeddings in graphs by utilizing inter-graph information through cross-graph matching procedures. Wang et al. (2019b) and Xu et al. (2019d) propose methods to aggregate information from similar embeddings in other graphs to improve the performance of graph neural networks. The curr_chunk discusses improving graph node embeddings through cross-graph matching procedures. Wang & Solomon (2019) address rigid motion between point clouds using a point cloud matching approach. Intra-graph node embeddings are processed with a Transformer module before feature matching based on inner product similarity scores. However, these methods may not consistently achieve accurate matching due to localized node embeddings. Neighborhood consensus for image matching is a method to ensure consistency in correspondences within local neighborhoods in computer vision. This approach has a history of improving local feature matching results efficiently. The functional maps framework was proposed to define continuous maps between function spaces, improving local feature matching results efficiently in computer vision. A deep neural network for neighborhood consensus using 4D convolution was also introduced, but it cannot be efficiently transferred to the graph domain directly. The functional maps framework defines continuous maps between function spaces on manifolds, commonly used in computer vision for local feature matching. It cannot be directly transferred to the graph domain."
}