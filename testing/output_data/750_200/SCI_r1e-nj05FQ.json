{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, including humans, despite individual incentives conflicting with the common good. Understanding cooperative behavior in self-interested individuals is significant in multi-agent reinforcement learning and evolutionary theory. This study focuses on intertemporal social dilemmas, where the conflict between individual and group interests is pronounced. Through a combination of MARL and natural selection, the research demonstrates the emergence of cooperative behavior. By combining MARL with structured natural selection, individual biases for cooperation can be learned in a model-free way. An innovative modular architecture for deep reinforcement learning agents supports multi-level selection, showing cooperation in various environments. Nature exhibits cooperation across different scales, from genetic interactions to societal levels, despite the push of natural selection. Despite natural selection favoring individual selfish interests, cooperation is prevalent across various scales, from genetic interactions to societal levels. Altruism can be favored by selection when individuals interact with other cooperators, avoiding exploitation by defectors. Other mechanisms such as kin selection, reciprocity, and group selection also play a role in promoting cooperation among self-interested agents. The emergence of cooperation among self-interested agents is a key topic in multi-agent deep reinforcement learning. Social dilemmas involve a trade-off between collective welfare and individual utility, where self-interested agents tend to converge to defecting strategies instead of achieving the collectively optimal outcome. The goal is to find multi-agent training regimes that address this challenge. Reinforcement-learning agents struggle to achieve optimal outcomes in social dilemmas, often resorting to defecting strategies. Previous solutions include opponent modeling, long-term planning, and intrinsic motivation functions. These approaches contrast with end-to-end model-free learning algorithms known for better generalization capabilities. Evolution can be applied to remove hand-crafted intrinsic motivation in reinforcement learning, similar to its use in optimizing hyperparameters, implementing black-box optimization, and evolving neuroarchitectures, regularization, loss functions, behavioral diversity, and reward functions. These principles are driven by single-agent search and optimization or competitive multi-agent tasks. The proposed system distinguishes between optimization processes unfolding over two time-scales: fast learning and slow evolution, addressing specific challenges in interactive storytelling domains. The proposed system distinguishes between optimization processes unfolding over two time-scales: fast learning and slow evolution. It addresses challenges in interactive storytelling domains by modeling intrinsic motivation as an additional term in agent rewards, implemented through a neural network for evolution to mitigate intertemporal dilemmas. Evolutionary theory suggests that evolving intrinsic reward weights across a population does not lead to altruistic behavior. To address this, a \"Greenbeard\" strategy is implemented where agents choose interaction partners based on cooperativeness signals, termed assortative matchmaking. Assortative matchmaking is a process where agents choose interaction partners based on honest signals of cooperativeness. To address limitations of this approach, a modular training scheme inspired by multi-level selection theory is introduced, called shared reward network. In a new approach called shared reward network evolution, agents consist of policy and reward neural network modules. The policy network is trained using modified rewards from the reward network on a fast timescale, while both modules evolve separately on a slow timescale. Each agent has a unique policy network but shares the same reward network in each episode. Fitness is determined by individual rewards for the policy network. In shared reward network evolution, agents have distinct policy networks but share the same reward network. The fitness for the policy network is based on individual rewards, while the fitness for the reward network is determined by the collective return of all co-players. This approach prevents overfitting and suggests a potential evolutionary origin of social inductive biases. The study explores different parameters in evolutionary paradigms for resolving difficult ISDs without handcrafting, focusing on Markov games within a MARL setting. Parameters include environments, reward network features, matchmaking, and reward network evolution. In a MARL setting, the study examines intertemporal social dilemmas (ISDs) in 1994. ISDs are games where selfish actions benefit individuals in the short term but harm the group in the long run. Two dilemmas are studied on a 2D grid: the Cleanup game where agents collect apples based on field cleanliness. In the Cleanup game on a 2D grid, agents collect apples with spawning rate linked to aquifer cleanliness. As waste fills the aquifer, apple respawn rate decreases until no apples can spawn. Agents must leave the apple field to clean, which offers no reward. If all agents refuse to clean, no rewards are received. In the Harvest game, agents collect rewarding apples with spawn rates based on nearby apples. There is a dilemma between quick individual harvesting and long-term group yield. The reward components in the model affect agents' loss functions. See the Appendix for more details. In the model, there are three reward components for agents' loss functions: total reward for policy loss, extrinsic reward for extrinsic value function loss, and intrinsic reward for intrinsic value function loss. Player i's total reward is the sum of extrinsic and intrinsic rewards. The extrinsic reward is obtained from the environment when player i takes action a from state s, while the intrinsic reward is a social preference calculated across features. The intrinsic reward u(f) is calculated from state s, using a 2-layer neural network with parameters evolved based on fitness. The elements of v correspond to coefficients related to inequity aversion, evolved from previous work. The feature vector f is player-specific. In Markov games, social preferences should not be influenced by the temporal alignment of players' rewards. Features are player-specific quantities transformed into intrinsic rewards via a reward network. In Markov games, social preferences should not be influenced by the temporal alignment of players' rewards. Two ways of aggregating rewards are considered, using off-policy importance weighted actor-critic (V-Trace) BID10 with trajectories from 500 actors, and an architecture with intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The architecture for Markov games includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The retrospective method derives intrinsic reward from past rewards, while the prospective variant derives intrinsic reward from future expectations. The temporally decayed reward for agents is updated at each timestep using value estimates and a stop-gradient before the reward network module. Training was done with a population of 50 agents using distributed asynchronous training in multi-agent environments. Agents were sampled from a population of 50 and played in 500 arenas. Episode trajectories lasted 1000 steps and weights were updated using V-Trace. The set of weights included learning rate, entropy cost weight, and reward network weights. Parameters of the policy network were inherited in a Lamarckian fashion. Agents observed their last actions. The objective function for the agents' neural network included the value function gradient, policy gradient, and entropy regularization components, weighted by hyperparameters baseline cost and entropy cost. Evolution was guided by a fitness measure based on the total episode return, calculated as a moving average of apples collected. Evolution was guided by a fitness measure based on the total episode return, calculated as a moving average of apples collected. Matches were determined through random or assortative matchmaking methods. Random matchmaking selected agents uniformly at random for the game, while cooperative matchmaking ranked agents based on recent cooperativeness and grouped them accordingly. This ensured that highly cooperative agents played with each other, while defecting agents played with other defectors. Cooperativeness was calculated differently for Cleanup and Harvest scenarios. The agent's cooperativeness ranking was calculated based on its return compared to the mean return of all players. Cooperative metric-based matchmaking was used with individual reward networks or no reward networks, but not with the multi-level selection model. The reward network was separately evolved within its own population to allow different modules of the agent to compete only with like components. In separate populations, the reward network was evolved independently to allow different agent modules to compete with similar components. This enabled exploration of hyperparameters and generalization to various policies. Five policy networks were paired with a shared reward network in each episode. In a unique approach, 5 policy networks were paired with a shared reward network in each episode. The evolution of reward network parameters was based on total episode return across the group of co-players, distinct from previous work on intrinsic rewards. The evolution of reward network parameters is motivated by dealing with the tension in ISDs and evolving a form of communication for social cooperation, rather than learning reward-shaping in a sparse-reward environment. Shared reward networks mix group fitness on a long timescale with individual reward on a short timescale, providing a biologically principled method for multiple agents in a social setting. Shared reward networks offer a biologically principled approach that balances group fitness and individual rewards. In contrast to hand-crafted aggregation methods, using intrinsic reward networks improves performance in games like Cleanup and Harvest. Random matchmaking shows no significant advantage over PBT, while assortative matchmaking with reward networks and retrospective social features outperforms both. Assortative matchmaking with reward networks and retrospective social features outperforms random matchmaking with no significant advantage over PBT. Individual reward network agents show little improvement over PBT on Cleanup and Harvest games. Adding reward networks is beneficial when players have separate networks evolved selfishly. Agents with individual reward networks performed very well, showing that conditioning internal rewards on social features and a preference for cooperative agents playing together were crucial. Shared reward network agents performed as effectively as assortative matchmaking and handcrafted inequity aversion intrinsic reward from BID25, even with random matchmaking. This suggests that agents did not necessarily need immediate access to honest signals of other agents' cooperativeness to resolve the dilemma; having the same intrinsic reward function was sufficient. Figures 4(c) and (d) compare retrospective and prospective variants. The prospective variant of reward network evolution, although better than PBT with a shared reward network, generally results in worse performance and more instability compared to the retrospective variant. The retrospective variant depends on environmentally provided reward, while the prospective variant requires agents to learn good value estimates before the reward networks become useful. Various social outcome metrics are plotted to better understand agent behavior complexities. The prospective variant of reward network evolution leads to lower performance and more instability compared to the retrospective variant. Social outcome metrics, such as sustainability and equality, are used to analyze agent behavior complexities. Having no reward network results in quick apple collection, while reward networks promote more sustainable behavior. The prospective reward network leads to lower equality, while the retrospective variant has high equality. Tagging is more common with prospective or individual reward networks compared to the retrospective shared reward network. The weights of the final retrospective shared reward networks were best at resolving the ISDs. The final weights of the retrospective shared reward networks evolved differently for each game, suggesting unique social preferences were needed. Cleanup required a simpler reward network, while Harvest needed a more complex one. In Cleanup, a less complex reward network sufficed as finding other agents being rewarded was intrinsically rewarding. However, Harvest required a more complex reward function to prevent over-exploitation of resources. The first layer weights tended to be arbitrary due to random matchmaking, leading to little specialization. Real environments lack scalar reward signals, relying on internal drives based on primary or secondary goals. Organisms have developed internal drives based on primary or secondary goals instead of scalar reward signals. Implementing natural selection via genetic algorithms did not lead to cooperation, but assortative matchmaking generated cooperative behavior with honest signals. A new evolutionary paradigm based on shared reward networks achieves cooperation in general situations. Evolutionary paradigm based on shared reward networks promotes cooperation by improving credit assignment and exposing social signals correlating with selfishness levels. The shared reward network evolution model promotes cooperation by exposing social signals related to selfishness levels and improving credit assignment through mechanisms like competitive altruism and other-regarding preferences. This model is inspired by multi-level selection but differs in that lower level units constantly swap pairings with higher level units. The shared reward network evolution model promotes cooperation by exposing social signals related to selfishness levels and improving credit assignment through mechanisms like competitive altruism and other-regarding preferences. This form of modularity can be seen in nature through examples like free-living microorganisms forming multi-cellular structures and prokaryotes incorporating plasmids for cooperation in social dilemmas. The spread of cultural norms in humans may be represented by a reward network, independent of individual success. Future research could explore alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity, to understand their impact on a reward network's weights and evolutionary origins. It would be interesting to study how kin selection and reciprocity impact the reward network's weights, hinting at the evolutionary origins of social biases. Additionally, exploring an assortative matchmaking model and combining an evolutionary approach with multi-agent communication for cooperative behaviors like cheap talk could provide further insights. Games have episodes lasting 1000 steps, with different playable area sizes for Cleanup and Harvest. The games have episodes lasting 1000 steps with different playable area sizes. Agents can only observe via a 15\u00d715 RGB window. The action space includes moving and tagging other players, with a reward cost for tagging. Training was done via joint optimization of network parameters. The Cleanup game involves an additional action for cleaning waste. Training was done via joint optimization of network parameters and hyperparameters/reward network parameters. Gradient updates were applied for every trajectory up to a maximum length of 100 steps. Optimization was done via RMSProp with specific parameters, and the learning rates were allowed to evolve. PBT was used for evolving the entropy cost throughout training. The entropy cost was sampled from LogUniform(2 \u00d7 10 \u22124 ,0.01) and evolved using PBT during training. Learning rates were initially set to 4 \u00d7 10 \u22124 and allowed to evolve. PBT utilizes genetic algorithms to search over hyperparameters, resulting in an adaptive schedule and joint optimization with network parameters learned through gradient descent. Evolution involved a mutation rate of 0.1 with perturbations for entropy cost, learning rate, and reward network parameters. A burn-in period of 4 \u00d7 was implemented for evolution. During training, perturbations were applied to entropy cost, learning rate, and reward network parameters. A burn-in period of 4 \u00d7 10^6 agent steps allowed for accurate fitness assessment before evolution."
}