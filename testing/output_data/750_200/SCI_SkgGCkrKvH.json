{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, specifically Choco-SGD, enables data privacy and on-device learning over networks. This approach achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. The algorithm's practical performance is demonstrated in training scenarios over decentralized user devices and in datacenters. Distributed machine learning enables training deep learning models over decentralized user devices and in datacenters. It offers computational scalability and data-locality, allowing joint training while keeping training data local to each device. Decentralized schemes can be as efficient as centralized ones. Decentralized training methods can be as efficient as centralized approaches for deep neural networks. Gradient compression techniques are used to reduce data sent over communication links in distributed training. Decentralized training methods for deep neural networks aim to reduce data sent over communication links. Wangni et al. (2018) and Stich et al. (2018) introduced methods for data compression. Tang et al. (2018) proposed DCD and ECD algorithms for communication compression, but they have limitations. CHOCO-SGD, introduced by Koloskova et al. (2019), overcomes these constraints and allows for high compression ratios. The focus is on evaluating the algorithm's generalization performance on the test-set. CHOCO-SGD, introduced by Koloskova et al. (2019), focuses on generalization performance on test-set for decentralized training methods. It shows speed-ups in a peer-to-peer setting similar to federated learning. In a datacenter setting, decentralized communication patterns improve scalability over centralized approaches. CHOCO-SGD demonstrates communication efficiency and faster time-to-accuracy on large tasks like ImageNet training. However, decentralized algorithms face challenges when scaling to a larger number of nodes. Decentralized algorithms like CHOCO-SGD show communication efficiency and faster time-to-accuracy on tasks like ImageNet training. However, scaling these algorithms to a larger number of nodes presents challenges, with decentralized schemes often not reaching the same performance as centralized schemes. These findings highlight deficiencies in current decentralized training schemes and call for further research on scalable decentralized training schemes. CHOCO-SGD converges at rate O 1 / \u221a nT + n /(\u03c1 4 \u03b4 2 T ) on non-convex smooth functions, showing linear speedup in the number of workers n. A version with momentum is analyzed for practical performance on on-device training over a peer-to-peer social network. The practical performance of CHOCO-SGD with momentum is analyzed in two scenarios: on-device training over a social network and in a datacenter for computational scalability. Decentralized learning approaches face difficulties when scaling to larger nodes in communication restricted settings. Difficulties faced by decentralized learning approaches in communication restricted settings are addressed through various methods such as decentralized schemes, gradient compression, asynchronous methods, and multiple local SGD steps before averaging. This is particularly relevant for learning over decentralized data. In this paper, the focus is on combining decentralized SGD schemes with gradient compression, specifically using gossip averaging methods. Previous research has shown convergence rates dependent on the spectral gap of the mixing matrix. Combining SGD with gossip averaging has demonstrated convergence at a specific rate. Communication compression with quantization, popular in deep learning, has theoretical guarantees and has been successful in various related schemes. The spectral gap of the mixing matrix affects convergence rates, with recent observations supporting this. Compression with quantization has gained popularity in deep learning, with theoretical guarantees established for unbiased and biased compression schemes. Error correction schemes show the best practical and theoretical performance. Recent studies have also explored proximal updates and variance reduction in combination with quantized updates. Decentralized Optimization with Quantization has been a focus of recent research. Recent research has focused on decentralized optimization with quantization. Gossip averaging may not converge in the presence of quantization noise, but adaptive schemes have been proposed to converge at a higher communication cost. Recent research has focused on decentralized optimization with quantization. Adaptive schemes have been proposed to converge at a higher communication cost, with algorithms like DCD, ECD, and CHOCO-SGD being introduced for deep learning applications. These algorithms can handle arbitrary high compression and have been analyzed for both convex and non-convex functions. In decentralized optimization with quantization, algorithms like CHOCO-SGD have been introduced for deep learning applications. CHOCO-SGD achieves higher test accuracy compared to DeepSqueeze, converging with arbitrary compression ratio. The algorithm involves compression operators and gossip-based stochastic optimization. In this section, the decentralized optimization problem is formally introduced along with compression operators and the CHOCO-SGD gossip-based stochastic optimization algorithm. The distributed setup involves optimization problems distributed across n nodes with local data distributions. Communication is limited to local neighbors defined by a weighted graph representing communication links. The decentralized optimization problem involves a weighted graph representing communication links for message exchange. Compression operators are used to transmit compressed messages in the CHOCO-SGD algorithm. Compression operators are utilized in the CHOCO-SGD algorithm to transmit compressed messages, supporting a larger class of operators compared to quantization. Specific compression schemes are discussed in Section 5. The CHOCO-SGD algorithm utilizes compression operators to transmit compressed messages, supporting various schemes discussed in Section 5. Each worker updates its private variable using stochastic gradient and gossip averaging steps, preserving averages even with quantization noise. Nodes communicate with neighbors using compressed updates. The CHOCO-SGD algorithm uses compression operators for transmitting compressed messages to update private variables with stochastic gradient and gossip averaging steps. Nodes communicate with neighbors using compressed updates, allowing parallel execution of communication and gradient computation. Each node only needs to store 3 vectors at most, regardless of the number of neighbors. The CHOCO-SGD algorithm utilizes compression operators for transmitting compressed messages to update private variables with stochastic gradient and gossip averaging steps. Nodes communicate with neighbors using compressed updates, requiring each node to store a maximum of 3 vectors. Additionally, a momentum-version of CHOCO-SGD is proposed in Algorithm 2, extending the analysis to non-convex problems. Technical assumptions are made regarding the bounded variance of stochastic gradients on each worker. The CHOCO-SGD algorithm uses compression operators for efficient communication among nodes, achieving asymptotic convergence with improved constants compared to SGD. Experimental comparisons with baseline methods are provided in this section. The CHOCO-SGD algorithm utilizes compression operators for efficient communication among nodes, showing improved convergence compared to SGD. Experimental comparisons with baseline methods are conducted in this section, including a momentum version of CHOCO-SGD. In Algorithm 1, line 9 is replaced with local momentum with weight decay 10. The experiment setup includes a ring topology with 8 nodes, training ResNet20 on Cifar10 dataset, and implementing various optimization algorithms with momentum. For optimization algorithms with momentum, including DeepSqueeze and CHOCO-SGD, the momentum factor is set to 0.9 without dampening. The initial learning rate is fine-tuned and warmed up gradually, with decay at 150 and 225 epochs, stopping training at 300 epochs. Hyper-parameter tuning includes adjusting the consensus learning rate \u03b3. Compression schemes are applied to each layer of ResNet20 separately. Compression schemes are applied to every layer of ResNet20 separately. Two unbiased schemes include quantization and sparsification, while two biased schemes involve selecting a fraction of weights. The consensus learning rate \u03b3 is also tuned during hyper-parameter tuning. Compression schemes are applied to every layer of ResNet20 separately, including unbiased quantization and sparsification, as well as biased schemes involving selecting a fraction of weights. Top a and sign compression are two biased compression schemes analyzed. The combination of DCD and ECD with biased schemes is not supported by theory, while CHOCO-SGD and DeepSqueeze have only been studied with biased schemes. Results from experiments show that unbiased compression schemes ECD and DCD perform well at low compression ratios but struggle at higher ratios, aligning with theoretical and experimental findings. The performance of DCD with biased top sparsification is better than with unbiased random counterpart, despite lacking theoretical support. CHOCO-SGD shows good generalization in all scenarios with minimal accuracy drop. Sign compression achieves high accuracy with significantly fewer bits per weight compared to full precision baseline. The focus now shifts to decentralized real-world scenarios. In challenging real-world scenarios, decentralized methods are necessary due to local training data on each device, limited communication bandwidth, unknown network topology, and the need for efficient machine learning model training. In decentralized settings, communication bandwidth is limited, global network topology is typically unknown, and a large number of devices are connected. Privacy is a key motivation, keeping training data private on each device. Training data is permanently split between nodes, not shuffled during training, with each node having a distinct part of the dataset. This scenario has not been studied extensively in prior works. In decentralized settings, training data is split between nodes, each having a distinct part of the dataset. No prior works have studied this scenario for decentralized deep learning. Centralized approaches like all-reduce are not efficient in this setting, so we compare to a centralized baseline where all nodes send updates to a central coordinator for aggregation. In decentralized deep learning, training data is split between nodes. CHOCO-SGD with sign compression is compared to decentralized SGD without compression and centralized SGD without compression. Scaling properties are studied on 4, 16, 36, and 64 nodes using different network topologies. In decentralized deep learning, CHOCO-SGD with sign compression is compared to decentralized SGD without compression and centralized SGD without compression on different network topologies. Results show that CHOCO-SGD slows down due to graph topology influence and communication compression, impacting training uniformly for both topologies. In decentralized deep learning, CHOCO-SGD with sign compression is compared to decentralized SGD without compression and centralized SGD without compression on different network topologies. The train performance is similar to the test, indicating slower convergence as the reason for performance degradation. Increasing epochs improves decentralized scheme performance, but even with 10 times more epochs, the gap between centralized and decentralized algorithms remains. The focus in real decentralized scenarios is not on minimizing epochs but on reducing the amount of training needed. In real decentralized scenarios, the focus is on reducing communication to save user's mobile data. CHOCO-SGD performs best with slight degradation as nodes increase. Torus topology is beneficial for large node numbers due to good mixing properties. Experiments on a Real Social Network Graph show that both Decentralized and Centralized SGD require significantly more bits for reasonable accuracy. Training models on user devices connected by a real social network using the Davis Southern women social network with 32 nodes. Training includes a ResNet20 model on the Cifar10 dataset for image classification and a three-layer LSTM. Training includes a ResNet20 model on the Cifar10 dataset for image classification and a three-layer LSTM for a language modeling task on WikiText-2. Results are summarized in Figures 2-3 and Table 3, showing the training accuracy reached after the same number of epochs. The decentralized algorithm outperforms the centralized and quantized decentralized for image classification training accuracy. However, the centralized scheme achieves the highest test accuracy. CHOCO-SGD outperforms the exact decentralized scheme in test accuracy with less transmitted data. CHOCO-SGD shows a slight accuracy drop compared to baselines after the same number of epochs. It outperforms centralized SGD in test perplexity and performs best in terms of perplexity for a fixed data volume. In large-scale training with Resnet-50 on ImageNet-1k, CHOCO-SGD excels in performance. In large-scale training with Resnet-50 on ImageNet-1k, CHOCO-SGD utilizes \"Sign+Norm\" quantization scheme and shows benefits when scaling to more nodes. Decentralized optimization methods can outperform centralized ones, especially in scenarios with fast network connections. Assran et al. (2019) demonstrated impressive speedups for training on 256 GPUs with all nodes accessing all training data. Assran et al. (2019) presented a decentralized algorithm with asynchronous gossip updates and exact communication for training on 256 GPUs. Their approach, different from CHOCO-SGD, focuses on faster mixing through changing communication topology. The experiments were conducted on 8 machines using Resnet-50 for ImageNet-1k training. The setup for training ImageNet-1k with Resnet-50 involved using 8 machines with 4 Tesla P100 GPUs each, performing communication within machines using all-reduce and decentralized communication between machines with compressed communication. The mini-batch size on each GPU was 128, following the general SGD training scheme with hyperparameters from CHOCO-SGD. Limited computational resources prevented heavy tuning of the consensus. CHOCO-SGD, utilizing hyperparameters from (Goyal et al., 2017), benefits from its decentralized and parallel structure, achieving a slight 1.5% accuracy loss compared to All-reduce. Despite not heavily tuning the consensus stepsize due to limited computational resources, CHOCO-SGD outperforms All-reduce in terms of time per epoch. Our approach using CHOCO-SGD achieved a test accuracy of 76.37%, outperforming the common all-reduce baseline in terms of time per epoch by 20%. We suggest integrating the scheme proposed by Assran et al. for better training efficiency in decentralized deep learning environments. Our algorithm shows theoretical convergence guarantees in non-convex settings and offers a linear speedup in node numbers. It performs well in communication-restricted environments for tasks like image classification and language modeling. Our main contribution is enabling training in communication-restricted environments while respecting data locality. We demonstrate decentralized schemes for high communication compression, expanding the reach of decentralized deep learning applications. The proof of Theorem 4.1 is presented, derived from analyzing CHOCO-SGD for arbitrary stepsizes in Theorem A.2. The proof structure follows Koloskova et al. (2019). The proof of Theorem 4.1 analyzes CHOCO-SGD for arbitrary stepsizes and derives it as a special case. The structure of the proof follows a previous study. Algorithm 1 is a special case of a more general class of algorithms, involving stochastic gradient updates and averaging among nodes. Convergence is shown for algorithms with stochastic gradient updates followed by linear convergence in the averaging step. Decentralized SGD with arbitrary averaging scheme is discussed in Algorithm 3, where stochastic gradient updates are followed by an averaging step exhibiting linear convergence. The averaging scheme's convergence rate is estimated based on previous research. The algorithm assumes an averaging scheme that preserves iterates' average and converges linearly. Decentralized SGD with arbitrary averaging scheme is discussed in Algorithm 3, where stochastic gradient updates are followed by an averaging step exhibiting linear convergence. The algorithm assumes an averaging scheme that preserves iterates' average and converges linearly with a parameter 0 < c \u2264 1. An example is Exact Averaging, which converges at a rate c = \u03c1, where \u03c1 is an eigengap of the mixing matrix W. To recover CHOCO-SGD, CHOCO-GOSSIP is chosen as the consensus averaging method. Decentralized SGD with arbitrary averaging scheme is discussed in Algorithm 3, where stochastic gradient updates are followed by an averaging step exhibiting linear convergence. CHOCO-SGD is recovered by choosing CHOCO-GOSSIP as the consensus averaging scheme. The order of communication and gradient computation parts is exchanged in Algorithm 1 for better illustration, without affecting convergence rate. Decentralized SGD with arbitrary averaging scheme is discussed in Algorithm 3, where stochastic gradient updates are followed by an averaging step exhibiting linear convergence. The proof of Theorem 4.1 shows that the iterates of Algorithm 3 with constant stepsize satisfy certain conditions, leading to a linear speedup compared to SGD on one node. The proof of Theorem A.2 shows that CHOCO-SGD with CHOCO-GOSSIP averaging converges at a rate of O(1/\u221anT + n/(T\u03c1^2)), which is worse than the exact averaging case. This could be due to proof technique limitations or support for high compression. The proof of Theorem A.2 shows that CHOCO-SGD with CHOCO-GOSSIP averaging converges at a rate of O(1/\u221anT + n/(T\u03c1^2)), which is worse than the exact averaging case. This could be due to proof technique limitations or support for high compression. The theorem gives guarantees for the averaged vector of parameters x, however in a decentralized setting it is very expensive and sometimes impossible to average all the parameters. In a decentralized setting, averaging all parameters across machines is costly and sometimes impossible. Similar guarantees on individual iterates can be obtained as shown in (Assran et al., 2019). The convergence of local weights is discussed in Corollary A.3, with a proof based on the L-smoothness of f. The result holds for T larger than 64nL^2, but this constraint can be relaxed. Theorem A.4 provides further insights. Theorem A.4 discusses the convergence of Algorithm 3 in a decentralized setting with constant step sizes. The convergence rate is determined by the underlying averaging scheme, and the rate holds for any T. However, the first term is less favorable compared to another theorem due to the difference in values between \u03c3^2 and G^2. Theorem A.4 proves the convergence of Algorithm 3 with constant step sizes in a decentralized setting. The convergence rate is determined by the averaging scheme, with \u03c3^2 being much smaller than G^2. Lemma B.1 and B.3 discuss inequalities for vectors and matrices in Frobenius norm. Algorithm 4 CHOCO-SGD (Koloskova et al., 2019) is mentioned as well. Algorithm 4 CHOCO-SGD (Koloskova et al., 2019) introduces Error Feedback and mixing matrix W for stochastic gradient updates in a decentralized setting. It can be combined with weight decay and momentum, with Nesterov momentum being adaptable. The algorithm can be interpreted as an error feedback algorithm, providing insights into its workings. CHOCO-SGD in a decentralized setting is an error feedback algorithm where quantization errors are saved into internal memory and added to the compressed value at the next iteration. The algorithm corrects errors by adding internal memory before compressing the difference x (t) i \u2212 x (t\u22121) i, representing the evolution of local variable x i at step t. In this section, the procedure of model training and hyper-parameter tuning is detailed. The comparison includes CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Two models are trained: ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM. The model training and hyper-parameter tuning process involved training ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM for a language modeling task on WikiText-2. The LSTM had a hidden dimension size of 650, with a BPTT length of 30 and gradient clipping value fine-tuned to 0.4. The three-layer LSTM has a hidden dimension size of 650, with a BPTT length of 30. Gradient clipping is set to 0.4, and dropout is applied only on the output of LSTM. Both ResNet20 and LSTM are trained for 300 epochs with a per node mini-batch size of 32. The learning rate of CHOCO-SGD scales linearly with node degree. Momentum is applied only on ResNet20 training. Learning rate is gradually warmed up from 0.1. During ResNet20 training, the learning rate is gradually warmed up from 0.1 to the fine-tuned initial rate. The initial rate is decayed by a factor of 10 at 50% and 75% of training epochs. The optimal learning rate per sample\u03b7 is determined by a linear scaling rule. The best performance is ensured to be in the middle of the pre-defined grid. The optimal learning rate per sample \u03b7 is determined by a linear scaling rule, ensuring the best performance is in the middle of the pre-defined grid. Fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20 on Cifar10 and a social network topology are demonstrated in tables. The fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20/LSTM on a social network topology with 32 nodes are shown in Table 5. The training data is split between nodes without shuffling, with a per node mini-batch size of 32 and maximum node degree of 14. The base learning rate and consensus stepsize are adjusted based on the node degree. Learning curves for the social network topology are also plotted. The learning curve for the social network topology with 32 nodes is plotted, showing the local models reaching consensus towards the end of optimization. Test accuracy of the averaged model and distance of local models from the averaged model are depicted. The local models in the social network topology reach consensus towards the end of optimization, with their test performances matching that of the averaged model. Divergence from the averaged model occurs before decreasing the stepsize at epoch 225, aligning only when the stepsize decreases. This behavior was also observed in a previous study by Assran et al. (2019)."
}