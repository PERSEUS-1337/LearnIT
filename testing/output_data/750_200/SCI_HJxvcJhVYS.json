{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A model based on differential equations is used, but the inference over its parameters is challenging. A proposed generalization of Bayesian optimization helps approximate the posterior distribution for better inference. The proposed method generalizes Bayesian optimization to approximate inference by applying Stein variational gradient descent on estimates from a Gaussian process model. It shows promising results for likelihood-free inference in reinforcement learning environments, focusing on estimating parameters of a physical system from observed data when the likelihood function is unavailable. Recent methods address the challenge of limited simulations in robotics and reinforcement learning by constructing conditional density estimators from joint data, improving efficiency in simulation use. Recent methods aim to address efficiency in simulation use by constructing conditional density estimators from joint data, utilizing techniques such as mixture density networks and sequentially learning approximations to the likelihood function before running Markov chain Monte Carlo. Gutmann and Corander (2016) propose an active learning approach using Bayesian optimization to reduce the number of simulator runs significantly. This paper explores combining variational inference methods with Bayesian optimization to efficiently propose parameters for simulations. A Thompson sampling strategy is used to refine variational approximations to a black-box posterior, with parameters proposed using Stein variational gradient descent over samples from a Gaussian process. The approach also includes a method to optimally subsample the data. The approach combines variational inference with Bayesian optimization to propose simulation parameters efficiently. It uses Stein variational gradient descent over Gaussian process samples and includes a method to optimally subsample data for batch evaluations. The goal is to estimate a distribution that approximates a posterior distribution over simulator parameters given observations from a target system. The text discusses using Bayesian optimization to find the optimal distribution by minimizing the discrepancy between the target distribution and the simulator outputs. This approach does not require access to the likelihood function and utilizes a black-box method to solve the optimization problem. The algorithm includes a Gaussian process model, Thompson sampling acquisition function, and kernel herding procedure for sample selection. The algorithm utilizes a Gaussian process model, Thompson sampling acquisition function, and kernel herding procedure for sample selection. It bypasses modeling the map from parameters to KSD by learning directly via Stein variational gradient descent. The algorithm uses a Gaussian process to model the synthetic likelihood function, which approximates the simulations-observations discrepancy. The choice of discrepancy function is discussed in the ABC literature, and background details on the Kernelized Stein Discrepancy are provided in the appendix. The GP approximates the expensive and non-differentiable discrepancy \u2206 \u03b8 by providing a cheap and differentiable approximation. Candidate distributions q n \u2208 Q are selected using Thompson sampling, which samples functions from the GP posterior to account for uncertainty in the model. Thompson sampling, used in models like sparse spectrum Gaussian processes, samples functions from the GP posterior to account for uncertainty. The acquisition function is defined based on the target posterior approximation using SVGD for variational distribution q. The acquisition function in SVGD defines the variational distribution as a set of particles initialized from the prior and optimized through perturbations. It guides particles to local maxima of log posterior and encourages diversification by repelling nearby particles. The acquisition function in SVGD guides particles to local maxima of log posterior and encourages diversification by repelling nearby particles. Gradients of sample functions are defined for SSGP models with differentiable mean functions. Selecting a distribution q, evaluations of \u2206\u03b8 from samples \u03b8 \u223c q are needed to update the GP model. Representing q by a large number of particles M improves the algorithm. The GP model is improved by representing q with a large number of particles M, allowing SVGD to explore distant modes. However, using the large number of particles directly for simulations is costly. Instead, an optimal subsampling of candidate q is done to select S M query parameters for running the simulator. Kernel herding is used to select a set of samples that minimizes error on empirical estimates for expectations under a given distribution q. The procedure for SSGPs involves selecting samples based on the information encoded by the GP, rather than naively herding with the original feature map \u03c6. The distributional Bayesian optimisation (DBO) algorithm utilizes the information encoded by the GP posterior kernel to select informative samples for the model. The algorithm is summarized in Algorithm 1 and experimental results evaluating DBO in synthetic data are presented in this section. The experimental results evaluate the proposed method DBO in synthetic data scenarios, comparing it against MDNs. The method is tested on OpenAI Gym's 3 cart-pole environment with fixed physics parameters \u03b8 real. A dataset y of 10 trajectories is generated by executing randomly sampled actions, with summary statistics \u03b3 consistent with previous studies. The discrepancy is set to \u2206 \u03b8. The study tested the DBO method on synthetic data in the cart-pole environment with fixed physics parameters. Results show DBO outperformed MDNs in recovering the target system's posterior and providing better overall approximations. The method also showed improved performance in terms of MMD. An open-source implementation is available online. The paper presented a Bayesian optimization approach to inverse problems on simulator parameters, showing that DBO outperformed MDNs in approximating the posterior and MMD. Results suggest DBO is more sample-efficient for inferring parameters in reinforcement learning. Future work includes scalability and theoretical analysis. The paper introduced a Bayesian optimization method for inverse problems on simulator parameters, demonstrating that DBO performed better than MDNs in approximating the posterior and MMD. Results indicate DBO is more efficient in inferring parameters for reinforcement learning. Future work involves scalability and theoretical analysis. The method allows for fast incremental updates to reduce time complexity in updating the GP posterior with new observation pairs. The paper introduced a Bayesian optimization method for inverse problems on simulator parameters, demonstrating that DBO performed better than MDNs in approximating the posterior and MMD. Results indicate DBO is more efficient in inferring parameters for reinforcement learning. Fast incremental updates are proposed to reduce time complexity in updating the GP posterior with new observation pairs."
}