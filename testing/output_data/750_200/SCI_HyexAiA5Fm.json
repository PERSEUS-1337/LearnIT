{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models successful in modeling high-dimensional continuous measures. This paper introduces a scalable method for unbalanced optimal transport (OT) using the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the transfer of a source measure to a target measure. The formulation is theoretically justified and an algorithm based on stochastic alternating gradient updates is proposed for solving the problem. The paper introduces a scalable method for unbalanced optimal transport using a generative-adversarial framework. It proposes an algorithm based on stochastic alternating gradient updates for solving the problem of transforming one measure to another. The methodology is demonstrated through numerical experiments in population modeling. The paper presents a scalable method for unbalanced optimal transport using a generative-adversarial framework, focusing on transforming a source population into a target population. It utilizes modern approaches based on the Kantorovich formulation to find the optimal probabilistic coupling between measures. Regularizing the process is shown to be effective in population modeling. Optimal transport methods, such as BID24, utilize linear programming for discrete measures. Regularizing with entropy improves efficiency using the Sinkhorn algorithm. Stochastic methods for continuous settings have been proposed. Generative models like GANs can also learn transport maps when a cost is unavailable. Generative models like GANs can learn transport maps for various applications such as image translation, natural language translation, domain adaptation, and biological data integration. These models push a source distribution to a target distribution by training against an adversary. In this example, a sub-population is growing rapidly. Monge-like formulations of unbalanced optimal transport aim to learn a transport map and scaling factor to push source to target distributions. Various strategies using GANs have been employed to enforce correspondence between original and transported samples, but struggle with handling mass variation. Several formulations have been proposed for extending the theory of optimal transport to handle mass variation. Scaling algorithms have been developed for approximating the solution to optimal entropy-transport problems, allowing for unbalanced masses in the Kantorovich optimal transport problem. The Kantorovich OT problem is addressed by OT by BID27, which relaxes hard marginal constraints using divergences to allow for mass variation. This approach has been used in various applications such as computer graphics, tumor growth modeling, and computational biology. However, current methods cannot perform unbalanced OT between continuous measures. Inspired by GANs, a novel framework for unbalanced optimal transport is presented. The novel framework presented addresses unbalanced optimal transport by directly modeling mass variation in addition to transport. It proposes a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport. This generalizes the unbalanced Monge OT problem and offers scalable methodology for solving it. The optimal entropy-transport problem is addressed using a scalable methodology that utilizes a convex conjugate representation of divergences. The methodology is demonstrated in practice with various datasets, including handwritten digits and single-cell RNA-seq data from zebrafish embryogenesis. Additionally, a new scalable method is proposed for solving the optimal-entropy problem. In addition to proposing a new scalable method for solving the optimal-entropy transport problem, a new algorithm (Algorithm 2) is introduced in the Appendix for unbalanced OT in continuous settings. The algorithm extends previous work and provides a scalable alternative for large or continuous datasets. Optimal transport (OT) involves transporting measures in a cost-optimal manner using deterministic transport maps. The problem is formulated as finding a map T that minimizes a cost function subject to the constraint T # \u00b5 = \u03bd. The Monge problem is non-convex and challenging to solve. The Kantorovich OT problem is a convex relaxation of the Monge problem, formulating OT as a search over probabilistic transport plans. The relaxed problem is a linear program that is always feasible and can be solved in O(n^3) time for discrete measures \u00b5 and \u03bd. The relaxed Monge problem is a linear program that can be solved in O(n^3) time for discrete measures. Entropic regularization simplifies the dual optimization problem, leading to efficient solutions using the Sinkhorn algorithm. Stochastic algorithms have been proposed for computing transport plans for continuous measures, while unbalanced OT formulations handle mass variation. Existing numerical methods are based on optimal-entropy transport formulations. Existing numerical methods extend classical optimal transport to handle mass variation through optimal-entropy transport formulations. This approach relaxes marginal constraints using divergences to find a measure that minimizes a specific function. Mass variation is allowed in the resulting measure, enabling efficient solutions for transport plans. In this section, a new algorithm is proposed for unbalanced optimal transport that directly models mass variation and can be applied to high-dimensional continuous measures. The algorithm is based on a Monge-like formulation of unbalanced optimal transport, addressing a gap in practical algorithms for unbalanced optimal transport in high-dimensional spaces. The algorithm proposed for unbalanced optimal transport directly models mass variation and can be applied to high-dimensional continuous measures. It is based on a Monge-like formulation of unbalanced optimal transport, aiming to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. The algorithm for unbalanced optimal transport aims to learn a stochastic transport map and scaling factor to push a source measure to a target measure in a cost-optimal way. It considers stochastic maps for practical problems, such as in cell biology where one cell can give rise to multiple cells. In unbalanced optimal transport, a stochastic transport map and scaling factor are used to push a source measure to a target measure in a cost-optimal way. This model is suitable for practical problems like cell biology, where one cell can give rise to multiple cells. The transport map T models the movement of points from a source measure to a target measure at two distinct time points. The transport map T and scaling factor \u03be are used in unbalanced optimal transport to move points from a source measure to a target measure. Different transformation models are chosen based on the costs of mass transport and variation. An example scenario involves transporting points from a source measure to a target measure while dealing with class imbalances. In unbalanced optimal transport, a scaling factor is used to balance class imbalances when moving points from a source measure to a target measure. The scaling factor guides how samples in the source distribution should be weighted to achieve class balance with the target distribution. Relaxation techniques are employed to address optimization challenges in satisfying constraints, using a divergence penalty as a substitute. The relaxation of optimal transport problem involves using a divergence penalty instead of an equality constraint. This approach, known as the Monge-like version, specifies a joint measure \u03b3 and reformulates the objective function for optimal-entropy transport. The main difference lies in the search space, as not all joint measures can be specified by a choice of transport map. The search space for joint measures in optimal transport formulations is limited by the choice of transport map. Equivalence is only possible when restricting the joint measures to the support of the marginal measure. Equivalence in optimal transport formulations can only be established by restricting the joint measures to the support of the marginal measure. Theorem 3.4 shows that solutions of the relaxed problem converge to solutions of the original problem with a sufficiently large divergence penalty. Theorem 3.4 states that with a sufficiently large divergence penalty, solutions of the relaxed problem converge to solutions of the original problem. The proof is provided in the Appendix. The relaxation of unbalanced Monge OT allows learning the transport map and scaling factor using stochastic gradient methods with neural networks. The optimization procedure involves minimizing a penalty term represented by an adversary function, similar to GAN. The optimization of unbalanced Monge OT involves using alternating stochastic gradient updates with neural networks. The objective is to minimize the divergence between transported samples and real samples, while cost functions encourage finding a cost-efficient strategy. The optimization of unbalanced Monge OT involves using alternating stochastic gradient updates with neural networks to minimize divergence and find a cost-efficient strategy. Further practical considerations for implementation and training are discussed in the Appendix. The probabilistic Monge-like formulation is similar to the Kantorovich-like entropy-transport problem in theory but results in different numerical methods. Algorithm 1 solves the non-convex formulation using neural networks to learn a transport map and scaling factor, enabling scalable optimization. The networks are immediately useful for practical applications, requiring only a single forward pass for computation. The neural architectures of T, \u03be imbue their function classes with a particular structure, enabling effective learning in high-dimensional settings. Algorithm 1 may not find the global optimum due to non-convexity, while the scaling algorithm of BID8 solves a convex optimization problem but is limited in scalability. A new stochastic method is proposed in the Appendix for completeness. The Appendix proposes a new stochastic method for handling transport between continuous measures, overcoming scalability limitations of BID8. The output is in the form of a dual solution, which is less interpretable for practical applications compared to Algorithm 1. In the Appendix, a new stochastic method is proposed for transport between continuous measures, addressing scalability limitations of BID8. The output is a dual solution, less interpretable compared to Algorithm 1. Algorithm 1 directly learns a transport map and scaling factor, showing advantages in numerical experiments. The problem of learning a scaling factor for balancing measures arises in causal inference, where \u00b5 represents the distribution of covariates from a control population and \u03bd from a treated population. The goal is to scale the importance of different members from a control population based on how likely they are to be present in the treated population, in order to eliminate selection biases in the inference of treatment effects. Algorithm 1 performs unbalanced Optimal Transport (OT) for population modeling, with applications illustrated using MNIST data. Algorithm 1 demonstrates unbalanced optimal transport for population modeling using MNIST data, specifically comparing class distributions between source and target datasets. Algorithm 1 demonstrates unbalanced optimal transport for population modeling using MNIST data. The class imbalance between the source and target datasets reflects a scenario where certain classes become more popular while others become less popular. The scaling factor learned by Algorithm 1 reflects the class imbalances and can be used effectively. The scaling factor learned by Algorithm 1 reflects the class imbalances and can model growth or decline of different classes in a population. Experiments validate the reweighting that occurs during unbalanced optimal transport from MNIST to USPS datasets, illustrating evolution between two different time points. Algorithm 1 models the evolution from MNIST to USPS distribution using Euclidean distance as transport cost. The unbalanced transport is visualized in FIG1, showing the predicted appearance of MNIST images in the USPS dataset. The size of the image reflects the scaling factor, indicating changes in prominence between the datasets. In the USPS dataset, MNIST digits preserved their likeness during transport, with brighter digits having higher scaling factors. This is consistent with USPS digits being brighter and containing more pixels. The study applied Algorithm 1 on the CelebA dataset to perform unbalanced optimal transport from young faces to aged faces. A variational autoencoder was first trained on the CelebA dataset to encode samples into a latent space before applying the algorithm. The study utilized a variational autoencoder on the CelebA dataset to encode samples into a latent space. Algorithm 1 was then applied for unbalanced optimal transport from young to aged faces, with the transport cost being the Euclidean distance in the latent space. The transported faces generally retain key features, as shown in FIG2, with some exceptions like gender swaps. The study used a variational autoencoder on the CelebA dataset to encode faces into a latent space. The faces generally retain key features, with exceptions like gender swaps. Young faces with higher scaling factors were more likely to be male. The model predicts a growth in the prominence of male faces compared to female faces as the CelebA population ages. The study observed a strong gender imbalance between young and aged populations in the CelebA dataset, with young faces predominantly female and aged faces predominantly male. This imbalance was confirmed through ground truth labels. In biology, lineage tracing of cells during different developmental stages or disease progression involves unbalanced source and target distributions, where some cells in earlier stages are more likely to develop into cells in later stages. In biology, lineage tracing involves unbalanced source and target distributions, where some cells in earlier stages are more likely to develop into cells in later stages. Algorithm 1 was applied to single-cell gene expression data from two stages of zebrafish embryogenesis to showcase the relevance of learning the scaling factor. The results of the transport were plotted after dimensionality reduction by PCA and T-SNE. Cells from the blastula stage with higher scaling factors were compared. In single-cell gene expression data analysis, cells from the blastula stage with higher scaling factors were found to be significantly enriched for genes associated with differentiation and development of the mesoderm, indicating potential biological insights. This demonstrates the application of scaling factor analysis in uncovering meaningful biological discoveries. In this section, a stochastic method for unbalanced optimal transport based on the regularized dual formulation of BID7 is presented. The dual formulation of FORMULA2 is a constrained optimization problem that is challenging to solve, but can be made unconstrained by adding a strongly convex regularization term to the primal objective. This term has a \"smoothing\" effect on the transport plan. The regularized dual formulation of BID7 in optimal transport introduces a smoothing term to the primal objective, encouraging high entropy plans. The dual problem is expressed as a supremum over functions, with a relationship between the primal and dual optimizers. The equation is then rewritten in terms of expectations, assuming access to samples from \u00b5, \u03bd, and their normalized measures. The algorithm described in Algorithm 2 for Unbalanced Optimal Transport (OT) involves parameterizing neural networks u and v with parameters \u03b8 and \u03c6, respectively, and optimizing them using stochastic gradient descent. This algorithm is a generalization of classical OT to unbalanced OT, with a dual formulation that is the dual of the entropy-regularized classical OT problem. The dual solution learned from Algorithm 2 for Unbalanced Optimal Transport can reconstruct the primal solution based on a transport map indicating mass transport between points in X and Y. The marginals of the transport map may not necessarily be \u00b5 and \u03bd. The transport map \u03b3 * indicates mass transport between points in X and Y, with marginals not necessarily being \u00b5 and \u03bd. A deterministic mapping from X to Y can be learned from \u03b3 * , such as the barycentric projection T : X \u2192 Y. A stochastic algorithm for learning this map from the dual solution is presented in Algorithm 3. The objectives in (6) and (3) are equivalent if reformulated in terms of \u03b3 instead of (T, \u03be). The formulations are equivalent if the search space of (3) contains only joint measures specified by some (T, \u03be). This relation is formalized by Lemma 3.3. Proof shows L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) for any solution (T, \u03be) and \u03b3 defined by (12). The proof demonstrates that L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) for any solution (T, \u03be) and \u03b3 defined by (12), showing the equivalence between the objectives in (6) and (3) when formulated in terms of \u03b3. The disintegration theorem implies the existence of a family of measurable functions that pushforward measures under T x. The Radon-Nikodym derivative \u03be satisfies certain conditions, as shown in the proof. The Radon-Nikodym derivative \u03be is restricted to the support of \u00b5, satisfying certain conditions. This leads to the conclusion that W c1,c2,\u03c8 (\u00b5, \u03bd) \u2265 L \u03c8 (\u00b5, \u03bd), completing the proof. The analysis of optimal entropy-transport by BID27 yields theoretical results for (6), including an existence and uniqueness result. The analysis of optimal entropy-transport by BID27 provides theoretical results for (6), including an existence and uniqueness result for joint measure \u03b3 specified by any minimizer of L \u03c8 (\u00b5, \u03bd) when certain conditions are met. Theorem 3.3 of BID27 guarantees the existence of a minimizer for W c1,c2,\u03c8 (\u00b5, \u03bd), which implies the existence of a minimizer for L \u03c8 (\u00b5, \u03bd). Uniqueness is established when \u03c8 \u221e = \u221e, with the marginals uniquely determined by the minimizers of L \u03c8 (\u00b5, \u03bd). The product measure generated by these minimizers is also unique, proving the uniqueness of \u03b3. For certain cost functions and divergences, L \u03c8 defines a proper metric between positive measures \u00b5 and \u03bd. By choosing appropriate entropy functions, L \u03c8 corresponds to the Hellinger-Kantorovich BID27 or the Wasserstein-Fisher-Rao BID9 metric. Solutions of the relaxed problem converge to solutions of the original problem. Solutions of the relaxed problem converge to solutions of the original problem for constrained optimization with a divergence penalty. Solutions of the relaxed problem converge to solutions of the original problem for constrained optimization with a divergence penalty. The sequence of minimizers \u03b3 k is bounded and equally tight under certain assumptions, as shown by Propositions in the text. In BID27, the sequence of minimizers \u03b3 k is bounded and equally tight. By an extension of Prokhorov's theorem, a subsequence of \u03b3 k weakly converges to some \u03b3. This implies that \u03b3 is a minimizer of W c1,c2,\u03b9= (\u00b5, \u03bd). The proof of Lemma 3.3 shows that \u03b3 k is equivalent to the product measure induced by minimizers of L \u03b6 k \u03c8 (\u00b5, \u03bd), leading to the min-max problem formulation. The convex conjugate form of \u03c8-divergence is presented to rewrite the main objective as a min-max problem. For non-negative finite measures P, Q over T \u2282 R d, a lemma is provided where equality holds under certain conditions. This result has been used for generative modeling and a rigorous proof can be found in the referenced work. The text provides a simple proof of a result related to convex conjugate and optimal functions over the support of P \u22a5. The proof involves first-order optimality conditions and the subdifferential of a function \u03c8. The optimal function over the support of P \u22a5 is shown to be \u03c8 \u221e. In practice, the choice of cost functions for the problem is crucial. Proposition B.1 provides conditions on c1, c2 for the problem to be well-posed. It is common to use the Euclidean distance as the cost of transport, c1(x, y), and a convex function for the cost of mass adjustment, c2, that prevents \u03be from becoming too small or too large. The choice of cost functions for the problem is crucial. It is common to use the Euclidean distance as the cost of transport and a convex function for the cost of mass adjustment to prevent \u03be from becoming too small or too large. Any \u03c8-divergence can be used to train generative models to match a generated distribution P to a true data distribution Q. The choice of cost functions is crucial for training generative models to match a generated distribution P to a true data distribution Q. Jensen's inequality states that D \u03c8 (P |Q) is minimized when P = Q for probability measures, but not for non-probability measures. In the original GAN paper, the discriminative objective corresponds to D \u03c8 (P |Q) with specific functions. The Jensen-Shannon divergence is minimized when P = Q for probability measures. Additional constraints on \u03c8 are needed for non-probability measures to match P to Q. If \u03c8(s) has a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. The Jensen-Shannon divergence is minimized when P = Q for probability measures. Additional constraints on \u03c8 are needed for non-probability measures to match P to Q. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0, \u03c8 \u221e > 0, and P = Q over a region with positive measure, then D \u03c8 (P |Q) > 0. If \u03c8(s) does not attain a unique minimum at s = 1 or \u03c8 \u221e \u2264 0, then divergence is equal to or less than P = Q. Choice of f and neural architectures are crucial for implementing unbalanced optimal transport. The function f should map from Y to a specific range, achieved through a neural network with appropriate activation layers. In our experiments, fully-connected feedforward networks with ReLU activation were used. For experiments in Section 4, fully-connected feedforward networks with 3 hidden layers and ReLU activations were used. The output activation layers included a sigmoid function for pixel brightness and a softplus function for the scaling factor weight."
}