{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks advance 2D and 3D image classification. Wavelet Pooling reduces feature dimensions and addresses overfitting, offering an alternative to traditional pooling methods. Wavelet Pooling method discards first-level subbands to reduce feature dimensions, addressing overfitting without losing structural compactness. Experimental results show it outperforms or performs comparably to max, mean, mixed, and stochastic pooling methods on benchmark datasets. CNNs are the standard for image and object classification due to their layer structures that conform to input shapes, consistently achieving higher accuracy rates than vector-based deep learning techniques. The strength of CNNs motivates researchers to constantly upgrade foundational concepts for growth and progress. The convolutional and pooling layers undergo modifications to elevate accuracy and efficiency beyond previous benchmarks. Pooling techniques have roots in predecessors like Neocognitron and Cresceptron, introducing manual subsampling and max pooling operations in deep learning. Pooling techniques, such as max pooling and average pooling, are used in CNNs to reduce spatial dimensions, increase efficiency, and prevent overfitting. While these methods are popular and simple, they have limitations that can hinder optimal network learning. Pooling techniques in CNNs like max pooling and average pooling are deterministic, efficient, and simple but have weaknesses that limit optimal network learning. Other methods like mixed pooling and stochastic pooling use probabilistic approaches to address some of these issues. However, all pooling operations utilize a neighborhood approach to subsampling, similar to nearest neighbor interpolation in image processing, which can introduce artifacts like edge halos, blurring, and aliasing. Minimizing data discontinuities is crucial for network regularization. We propose a wavelet pooling algorithm that uses second-level wavelet decomposition to subsample features, aiming to minimize artifacts like edge halos and blurring. Our method avoids nearest neighbor interpolation, providing a more accurate representation of feature contents. Comparisons with other pooling techniques such as max, mean, mixed, and stochastic pooling show promising results in image classification datasets. The study compares different pooling methods like max, mean, mixed, and stochastic on image classification datasets like MNIST, CIFAR-10, SHVN, and KDEF. MATLAB R2016b is used for simulations. The paper is structured with background in Section 2, proposed methods in Section 3, experimental results in Section 4, and summary/conclusion in Section 5. Pooling is synonymous with subsampling. In Section 2, the background is provided, followed by the proposed methods in Section 3, experimental results in Section 4, and a summary and conclusion in Section 5. Pooling, also known as subsampling, involves condensing the output of a convolutional layer by summarizing regions into one neuron value. Max pooling selects the maximum value of a region, while average pooling calculates the average value. Pooling, also known as subsampling, condenses the output of a convolutional layer by summarizing regions into one neuron value. Max pooling selects the maximum value of a region, while average pooling calculates the average value. Both methods have their effectiveness but also come with limitations. Max pooling can erase details from an image depending on the data. Researchers have developed probabilistic pooling methods to address the shortcomings of max and average pooling. Mixed pooling combines both methods by randomly selecting one for better results. Researchers have created probabilistic pooling methods, such as mixed pooling, to address issues with traditional pooling methods. Mixed pooling combines max and average pooling by randomly selecting one method during training. This method can be applied in three different ways within a layer. Another method, called stochastic pooling, improves upon max pooling. Probabilistic pooling methods, like stochastic pooling, improve upon traditional pooling methods by randomly sampling from neighborhood regions based on activation probabilities. The pooled activation is sampled from a multinomial distribution to pick a location within the region, with activations having higher probabilities having a higher chance of selection. The stochastic pooling method selects activations based on probabilities, avoiding the limitations of max and average pooling while incorporating some advantages of max pooling. Previous studies compare wavelets in image interpolation to traditional methods. Our proposed pooling method utilizes wavelets to reduce feature map dimensions and minimize artifacts from neighborhood reduction. By discarding first-order subbands, our approach captures data compression more organically, reducing jagged edges and other artifacts that may affect image classification. The wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain. The proposed wavelet pooling scheme reduces artifacts in image classification by performing a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT). This involves approximation and detail coefficients, scaling and wavelet vectors, and resolution levels. The fast wavelet transform (FWT) is used for a 2nd order decomposition on images, applying it twice on rows and columns to obtain detail subbands (LH, HL, HH) and an approximation subband (LL) at each resolution level. The image features are reconstructed using only the 2nd order wavelet subbands, pooling them by a factor of 2 with the inverse FWT (IFWT) based on the inverse DWT (IDWT). The proposed wavelet pooling algorithm performs backpropagation by reversing the process of its forward propagation, starting with 1st order wavelet decomposition and upsampling detail coefficient subbands by a factor of 2 to create new levels of decomposition. The proposed wavelet pooling algorithm performs backpropagation by reversing the process of forward propagation. It starts with 1st order wavelet decomposition and upsamples detail coefficient subbands by a factor of 2 to create new levels of decomposition. The wavelet basis used is the Haar wavelet for its even, square subbands. The experiments are conducted on a 64-bit operating system with an Intel Core i7-6800k CPU @ 3.40 GHz processor and two GeForce Titan X Pascal GPUs with 12 GB of video memory for training. The experiments utilize a 64-bit operating system with an Intel Core i7-6800k CPU @ 3.40 GHz processor and two GeForce Titan X Pascal GPUs with 12 GB of video memory for training. Different CNN structures are used for various datasets, testing the effectiveness of pooling methods with a 2x2 window for an even comparison. The experiments compare pooling methods using a 2x2 window on different CNN structures for various datasets. The proposed method outperforms all others, with max pooling showing signs of overfitting due to the small number of epochs. Our proposed method outperforms all others in a set of 10,000 images. Different pooling methods show varying trajectories during training, with max pooling starting to overfit due to the small number of epochs. Two sets of experiments are run with and without dropout layers and batch normalization to observe the effects on the network. The CIFAR-10 experiments compare different pooling methods with and without regularization. The proposed method shows the second highest accuracy, with wavelet pooling resisting overfitting. The change in learning rate prevents overfitting and results in slower learning. The experiments compare pooling methods with and without regularization. Wavelet pooling resists overfitting, while mixed and stochastic pooling show consistent learning progression. Two sets of experiments are run, one without dropout layers and one with dropout. The network structure for the SHVN experiments is shown in FIG0. Training and test data come from the SHVN dataset, with 55,000 images used for training without dropout. Our network structure for the SHVN experiments involves using different image sets for training and testing. The method proposed in the study shows the second lowest accuracy, with max and wavelet pooling slightly overfitting the data. Our approach follows the path of max pooling but performs slightly better in maintaining stability. Our method, following the path of max pooling, performs slightly better in maintaining stability compared to mixed, stochastic, and average pooling. Validation sets trend at near identical rates. The KDEF dataset used for experiments contains 4,900 images of 35 people displaying seven basic emotions through facial expressions. The KDEF dataset consists of 4,900 images of 35 individuals showing seven basic emotions through facial expressions. The dataset includes images at various poses and angles, with errors such as missing or corrupted images being fixed by mirroring counterparts in MATLAB. Images are manually cropped to match specific dimensions. The dataset does not specify a training or test set, so the data is shuffled before use. We manually crop images to match specified dimensions (762 x 562) in the KDEF dataset. Data is shuffled, with 3,900 images for training and 1,000 for testing. Images are resized to 128x128 due to constraints. Dropout layers maintain network stability. Proposed method in TAB5 shows second highest accuracy. Max pooling leads to overfitting, while wavelet pooling resists it. Average and mixed pooling are unstable. Stochastic pooling ensures consistent progression. Wavelet pooling resists overfitting and shows a consistent progression of learning. Computational complexity of wavelet pooling is not efficient, but it serves as a proof-of-concept with potential for improvement. The code written for implementing wavelet pooling shows potential for improvement in computational efficiency. The accuracy results and novelty serve as a starting point for further research and enhancements by both the current researchers and others. Calculating efficiency is based on the mathematical operations utilized by each method. In wavelet pooling, efficiency is determined by the number of operations for each subband at every level in decomposition and reconstruction. Mathematical operations for different pooling methods are calculated based on various criteria such as worst-case scenarios, number of additions and divisions, mean value, and random selection of values. TAB7 provides a comparison of mathematical operations for one image during forward propagation. Wavelet pooling is the least computationally efficient method, using 54 to 213x more mathematical operations than average pooling. Stochastic pooling is also inefficient, using about 3x more operations than average pooling. Average pooling is the most computationally efficient method, followed by mixed pooling and max pooling. Wavelet pooling is the least computationally efficient method, using 54 to 213x more mathematical operations than average pooling. However, by implementing good coding practices, GPUs, and an improved FTW algorithm, this method can be a viable option. There are improvements to the FTW algorithm that utilize multidimensional wavelets, lifting, parallelization, and other methods to improve efficiency in speed and memory. Wavelet pooling method utilizes multidimensional wavelets, lifting, and parallelization to improve efficiency in speed and memory. It has the potential to outperform traditional methods in CNNs and performs well in various datasets. The addition of dropout and batch normalization enhances network regularization. Our proposed methods, including dropout and batch normalization, respond well to network regularization. They outperform most pooling methods in CIFAR-10 & KDEF datasets and perform competitively in the SHVN dataset. Results confirm that no single pooling method is superior, with effectiveness depending on the dataset and network structure. Future work could explore varying wavelet basis for pooling optimization. Future work and improvements in this area could involve varying the wavelet basis for pooling optimization, adjusting the upsampling and downsampling factors for better image feature reduction, retaining discarded subbands for higher accuracies, enhancing the computational efficiency of the FTW method, and comparing the structural similarity of wavelet pooling with other methods."
}