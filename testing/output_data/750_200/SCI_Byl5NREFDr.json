{
    "title": "Byl5NREFDr",
    "content": "We study model extraction in natural language processing, where an adversary reconstructs a victim model using query access. The attacker can successfully mount the attack without real training data or meaningful queries, using random word sequences and task-specific heuristics. This method works across various NLP tasks like natural language inference and question answering. Our work demonstrates how attackers can extract machine learning models with task-specific heuristics, highlighting the exploit made possible by transfer learning methods in NLP. For a small budget, an attacker can extract a model that performs slightly worse than the victim model. We also explore defense strategies like membership classification and API watermarking, which can be circumvented by clever adversaries. Machine learning models are valuable intellectual property, often only accessible through web APIs. Malicious users may attempt to steal models by reproducing them locally using queries. These attacks, known as \"model stealing\" or \"model extraction,\" involve issuing a large number of queries to collect data. Contextualized pretrained representations for transfer learning in NLP APIs like ELMo and BERT have become popular. Adversaries can steal models by issuing queries to train a local copy, potentially leaking sensitive information or creating adversarial examples. In this paper, it is demonstrated that NLP models obtained by fine-tuning a pretrained BERT model can be extracted by adversaries without access to the training data used by the API provider. Experiments show that extraction attacks on NLP models fine-tuned from BERT can be done without access to training data or well-formed queries. This contrasts with prior work that required some access to relevant data for large-scale attacks. Experiments show that extraction attacks on NLP models fine-tuned from BERT can be done without access to training data or well-formed queries. The attacks involve randomly sampling words to form queries and sending them to the victim BERT model. The attacker then fine-tunes their own BERT on these queries using the victim outputs as labels. This process is illustrated in Figure 1. The attacker fine-tunes their own BERT model using victim outputs as labels after querying a victim BERT model. Despite being nonsensical, randomly-generated queries are effective in model extraction. The effectiveness of model extraction is highlighted, with a focus on nonsensical queries and pretraining on the attacker's side. Simple defenses like membership classification and API watermarking are discussed, showing effectiveness against naive adversaries but failing against clever ones. The need for stronger defenses against model extraction is emphasized for future research. The curr_chunk discusses the need for stronger defenses against model extraction attacks, especially focusing on computer vision applications and zero-shot distillation. Prior efforts on model extraction are related, with a call for further research to understand why models and datasets are vulnerable to such attacks. Model extraction attacks have been studied against image classification APIs, but do not transfer well to text-based systems due to the discrete nature of the input space. Prior work on NLP systems attempted extraction using pool-based active learning to select natural sentences from WikiText-2. In contrast to prior work on NLP systems using pool-based active learning, our study focuses on extracting knowledge from modern BERT-large models for tasks requiring pairwise inputs like question answering. This work is related to data-efficient distillation methods that aim to distill knowledge from larger models to smaller ones, but our approach does not assume white-box access to the teacher model. Unlike prior work on NLP systems, our study focuses on extracting knowledge from BERT-large models for tasks like question answering. These methods assume white-box access to the teacher model to generate data impressions, including rubbish inputs that yield high-confidence predictions. Previous work has successfully extracted from SVMs and 1-layer networks using i.i.d noise, but scaling this idea to deeper neural networks has not been done before. Unnatural text inputs have also been shown to produce overly confident model predictions. BERT, a 24-layer transformer model, is studied for model extraction. Unnatural text inputs have been shown to train models effectively for NLP tasks without real examples. BERT-large, a 24-layer transformer model, revolutionized NLP by achieving state-of-the-art performance on various tasks with minimal supervision. The model, fbert,\u03b8, contextualizes word sequences into high-quality vector representations, learned through masked language modeling on unlabeled text data. A modern NLP system leverages fine-tuning methodology with a task-specific network and BERT. Parameters are learned end-to-end with a small learning rate. Description of extraction attacks on a black-box API for a task. Extraction attacks involve reconstructing a local copy of a black-box API model by using a task-specific query generator to create nonsensical word sequences as queries. The attacker fine-tunes a public release of a model on the resulting dataset to obtain the extracted model. The attacker fine-tunes a public release of a model on a dataset to obtain the extracted model for various NLP tasks like sentiment classification, natural language inference, and question answering. The attacker fine-tunes a public release of a model on a dataset to obtain the extracted model for various NLP tasks like sentiment classification, natural language inference, and question answering. The input for question answering tasks includes pairs of sentences, paragraphs, and questions, with outputs ranging from entailment, contradiction, neutral, answer spans, and yes/no distributions. Two query generators, RANDOM and WIKI, are studied, with RANDOM generating nonsensical sequences from a Wikipedia vocabulary and WIKI using input queries. The generator uses input queries from a Wikipedia vocabulary to construct nonsensical sequences. However, additional heuristics are applied for tasks requiring complex interactions between different parts of the input space, such as replacing words in the premise to construct the hypothesis for tasks like MNLI. The generator uses input queries from a Wikipedia vocabulary to construct nonsensical sequences. Task-specific heuristics are applied, such as replacing words in the premise to construct the hypothesis for tasks like MNLI and sampling words from the passage to form a question for tasks like SQuAD/BoolQ. Representative example queries and outputs are provided in the appendix. The extraction procedure is evaluated with different query budgets for each task, with commercial cost estimates provided using Google Cloud Platform's Natural Language API calculator. Development set accuracy of various extracted models at different query budgets is shown in Table 3, with high accuracies for some tasks even at low query budgets. The extraction procedure is evaluated with different query budgets for each task, showing high accuracies even at low budgets. Extracted models are surprisingly accurate on original development sets, even when trained with nonsensical inputs. Accuracy remains high even when extracted models are trained with nonsensical inputs, with 95% accuracy on WIKI: extracted SQuAD models. However, agreement between extracted models is only slightly better than accuracy, and even lower on held-out sets. Low agreements of 59.2 F1 and 50.5 F1 are seen on SQuAD for extracted WIKI and RANDOM models, indicating poor functional equivalence. The study conducted an ablation study on alternative query generation heuristics for SQuAD and MNLI datasets. They also analyzed the impact of using argmax labels only for classification datasets, showing a minimal drop in accuracy on WIKI experiments. The study analyzed the impact of using argmax labels for classification datasets, showing minimal accuracy drop in WIKI experiments. Query efficiency was measured with varying budgets, indicating successful extraction even with small queries. In analyzing the impact of using argmax labels for classification datasets, the study showed minimal accuracy drop in WIKI experiments. Query efficiency was measured with varying budgets, indicating successful extraction even with small queries. The results prompt questions about the properties of nonsensical input queries that make them effective for model extraction, and the effectiveness of extraction without large pretrained language models. In analyzing the impact of using argmax labels for classification datasets, the study showed minimal accuracy drop in WIKI experiments. Query efficiency was measured with varying budgets, indicating successful extraction even with small queries. The results prompt questions about the properties of nonsensical input queries that make them effective for model extraction, and the effectiveness of extraction without large pretrained language models. Specifically examining the RANDOM and WIKI extraction configurations for SQuAD to understand why models trained on nonsensical queries perform well. Different victim models' agreement on answers to nonsensical queries is explored through training five victim SQuAD models. The study analyzed the impact of using argmax labels for classification datasets and found minimal accuracy drop in WIKI experiments. Query efficiency was measured with varying budgets, showing successful extraction with small queries. Different victim SQuAD models were trained to explore their agreement on answers to nonsensical queries, with high agreement on SQuAD training and development set queries but lower agreement on WIKI and RANDOM queries. The study found that victim models show high agreement on SQuAD training and development set queries but lower agreement on WIKI and RANDOM queries. High-agreement queries may be more useful for model extraction, leading to large F1 improvements when extracting models using these queries. The study found that victim models show high agreement on SQuAD training and development set queries but lower agreement on WIKI and RANDOM queries. Large F1 improvements were observed when extracting models using high-agreement subsets, indicating that agreement between victim models is a good proxy for the quality of an input-output pair for extraction. This suggests that high-agreement queries may be more useful for model extraction. The study found that victim models exhibit high agreement on SQuAD queries but lower agreement on WIKI and RANDOM queries. To investigate the interpretability of high-agreement nonsensical queries, human annotators were asked to answer specific questions. Annotators matched victim models' answers 23% on WIKI and 22% on RANDOM subsets, while scoring 77% on original SQuAD questions. Annotators used a word overlap heuristic to select answer spans, but many nonsensical question-answer pairs remained unexplained. In practical scenarios, the attacker's lack of information about the victim's architecture can impact the extraction accuracy when fine-tuning a different base model or extracting a QA model from scratch. The extraction data's signal is partially interpreted using a word overlap heuristic, but many nonsensical question-answer pairs remain mysterious to humans. When extracting a QA model from scratch, the accuracy depends on the pretraining setup. Comparing BERT-large and BERT-base models, starting from BERT-large yields higher accuracy. Using the same model as the victim also improves accuracy. When training a QA model from scratch, starting with a model like BERT-large leads to higher accuracy. Using the same model as the victim also improves results. Fine-tuning BERT gives attackers an advantage due to the good starting representation of language. Starting with a QANet model on SQuAD without pretraining, it had 1.3 million parameters. The model performed well with original SQuAD inputs but showed a significant F1 drop with nonsensical queries. Better pretraining allows models to start with a good language representation, simplifying extraction. BERT-based models benefit from better pretraining, simplifying extraction. Defense strategies aim to preserve API utility and remain undetectable to attackers without requiring re-training. Two defenses are explored, effective against weak adversaries, using membership inference to determine classifier training. The first defense strategy uses membership inference to detect nonsensical inputs or adversarial examples, issuing random outputs to eliminate extraction signals. Membership inference is treated as a binary classification problem, with datasets constructed from MNLI and SQuAD examples. The text discusses using membership inference as a defense strategy against extraction signals, treating it as a binary classification problem with datasets from MNLI and SQuAD. Logits and final layer representations of the victim model are used as input features for training the classifier. The classifiers transfer well to a balanced development set and remain robust to the query generation process. The text discusses using membership inference as a defense strategy against extraction signals. The classifiers transfer well to a balanced development set and remain robust to the query generation process. Watermarking is another defense strategy mentioned in the text. The text discusses using watermarking as a defense strategy against extraction signals. Watermarked queries and their outputs are stored on the API side to prevent memorization by extracted models. The evaluation of watermarking on MNLI and SQuAD shows promising results. Watermarking is used as a defense strategy against extraction signals in models. Dev Acc, WM Label Acc, and Victim Label Acc are used to measure accuracy. Watermarked WIKI has high WM Label Acc and low Victim Label Acc. Watermarking is done on 0.1% of queries to minimize performance drop. Non-watermarked models perform nearly identically on the development set, but show differences on watermarked training data. Watermarking is effective in distinguishing between models on watermarked training data, with non-watermarked models performing poorly. Training with more epochs exacerbates these differences. However, watermarking can only be used after an attack and assumes public deployment of the extracted model with black-box query access. Model extraction attacks against NLP APIs serving BERT-based models can be effective even with low query budgets. Attackers may use techniques like differentially private training, fine-tuning with different queries, or issuing random outputs to prevent detection. Model extraction attacks against NLP APIs serving BERT-based models are effective with low query budgets. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, and further research is needed to develop robust defenses against adaptive adversaries. Future directions following the results in this paper include leveraging nonsensical inputs to improve model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity. The distribution of agreement between victim SQuAD models on RANDOM and WIKI queries is provided in Figure 3. In this paper, the distribution of agreement between victim SQuAD models on RANDOM and WIKI queries is shown in Figure 3. Cost estimates were calculated using Google Cloud Platform's Calculator, with Natural Language APIs allowing inputs up to 1000 characters per query. Costs for different datasets were extrapolated for tasks not covered by Google Cloud APIs. The costs of entity analysis and sentiment analysis APIs for natural language inference (MNLI) and reading comprehension (SQuAD, BoolQ) were estimated in this paper. It is challenging to provide a widely applicable estimate for the price of issuing a certain number of queries, as some API providers offer a small budget of free queries. An attacker could potentially set up multiple accounts to collect extraction data in a distributed manner. Most APIs are used implicitly on webpages and are freely available to web users. API costs can vary depending on computing infrastructure and revenue models. Attackers can emulate HTTP requests to extract data at scale for free. It is important to focus on the low costs of extracting datasets rather than actual cost estimates. In this section, the costs of extracting datasets are discussed, emphasizing the relatively low costs compared to actual estimates. For example, it costs -$430.56 to extract Switchboard LDC97S62, a large conversational speech recognition dataset, and $2000.00 for 1 million translation queries. Input generation algorithms for each dataset are also detailed, such as building a vocabulary using wikitext103 and selecting the top 10000 tokens. Input generation algorithms for datasets involve building a vocabulary using wikitext103 and selecting the top 10000 tokens. For example, in the SST2 dataset, tokens are randomly sampled from the vocabulary, while in the WIKI dataset, a random sentence is chosen from wikitext103. In input generation algorithms for datasets, words from the top-10000 wikitext103 vocabulary are preserved while others are discarded. A sentence is randomly chosen from wikitext103 and words not in the vocabulary are replaced with random words from the vocabulary. The process is repeated three times to construct the final hypothesis. The premise is sampled in the same way as in SST2 and the hypothesis is sampled in the same way as in MNLI. The final paragraph is constructed by sampling tokens from the wikitext103 vocabulary. A random integer length is chosen to build the question, which is then appended with a \"?\" The final paragraph is constructed by sampling tokens from the wikitext103 vocabulary. A random integer length is chosen to build the question, which is then appended with a ? symbol and prepended with a question starter word chosen randomly. The questions are sampled in a manner identical to SQuAD and WIKI, with question starter words chosen from a specific list. In Table 11, various extraction datasets for SQuAD 1.1 are compared. The findings show that RANDOM performs better when paragraphs are sampled based on unigram frequency in wikitext103. Starting questions with common question starter words like \"what\" helps with RANDOM schemes, especially when paragraphs are sampled from a distribution reflecting unigram frequency in wikitext103. A similar ablation study on MNLI in Table 12 shows that when the lexical overlap between premise and hypothesis is too low, the model tends to predict neutral or contradiction, limiting dataset extraction signals. Conversely, when the lexical overlap is too high, the model struggles as well. The model's prediction tendencies are influenced by lexical overlap between premise and hypothesis, with high overlap leading to unbalanced datasets. Human annotators were asked to evaluate question sets for the study. For human studies, fifteen English-speaking graduate students annotated five sets of twenty questions each. Annotators were unfamiliar with the research goals. Three annotators per question set evaluated original SQuAD questions (control), WIKI questions with high victim model agreement, RANDOM questions with high victim model agreement, WIKI questions with low victim model agreement, and RANDOM questions with low victim model agreement. Inter-annotator agreement is shown in Table 10. In an ablation study on input features for the membership classifier, two input feature candidates were considered: the logits of the BERT classifier. The ordering of inter-annotator agreement reflects the closeness to the actual input distribution. The ablation study on input features for the membership classifier compared the effectiveness of using the logits of the BERT classifier versus the last layer representations. Results showed that the last layer representations were more effective in distinguishing between real and fake inputs, but the best results were achieved by using both feature sets. The ablation study compared using BERT classifier logits versus last layer representations for the membership classifier. Results showed last layer representations were more effective in distinguishing real from fake inputs. Best results were achieved by using both feature sets."
}