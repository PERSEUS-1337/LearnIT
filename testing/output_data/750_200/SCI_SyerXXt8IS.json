{
    "title": "SyerXXt8IS",
    "content": "Auto-generate enhanced input features for ML models with limited training data. Biological neural nets (BNNs) quickly learn and extract informative features. The insect olfactory network uses competitive inhibition, sparse connectivity, and Hebbian updates for rapid odor learning. MothNet, a computational model of this network, generates new features for ML classifiers, resulting in improved performance. Neurons provide new features for ML classifiers, outperforming baseline methods on MNIST and Omniglot datasets. MothNet feature generator surpasses PCA, PLS, and NNs, showcasing the value of BNN-inspired features in ML. Neural nets with backprop require large training data, limiting ML deployment. To address this, a new architecture generates class-separating features from existing ones to improve learning from limited data. Biological neural nets can learn rapidly from few samples, suggesting effective class separation is key. The insect olfactory network, including the Antennal Lobe and Mushroom Body, is a simple but effective biological neural net that can learn quickly from just a few exposures. It features competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. The synaptic connections in this network are largely random. The MothNet model, based on the insect olfactory network, incorporates competitive inhibition in the Antennal Lobe, high-dimensional sparse layers, and a Hebbian update mechanism. It demonstrated rapid learning of vectorized MNIST digits with superior performance compared to standard ML methods, even with limited training samples per class. The MothNet model incorporates inhibitory signals in the Antennal Lobe and sparsity in the Mushroom Body, with weight updates affecting only MB\u2192Readout connections through Hebbian updates based on neural firing rates. The MothNet model uses Hebbian updates based on neural firing rates to update connection weights between the Mushroom Body and Readouts. It was tested as a front-end feature generator for a machine learning classifier, combining MothNet with a downstream ML module to automatically generate features for classification. The AL-MB acted as an automatic feature generator for a non-spatial dataset, significantly improving accuracies of ML methods like NN, SVM, and Nearest Neighbors on the test set. The Mothnet Readouts used as features encoded class-relevant information not available to ML methods alone. The AL-MB network significantly improved ML accuracies by encoding class-relevant information in a form accessible to ML methods. MothNet-generated features outperformed PCA, PLS, NNs, and transfer learning in enhancing ML accuracy. The insect-derived network produced stronger features than other methods. The vMNIST dataset, created by downsampling and preprocessing the MNIST dataset, has 85 pixels-as-features. Baseline ML methods do not achieve full accuracy at low N with vMNIST. The MothNet model, part of the AL-MB network, outperformed other methods in generating strong features for ML accuracy improvement. Full details and code for the MothNet simulations can be found in references [11] and [12]. MothNet instances were randomly generated from templates specifying connectivity parameters. Two sets of experiments were conducted: Cyborg vs baseline ML methods on vMNIST. Training samples were drawn, ML methods trained for baseline, MothNet trained using stochastic differential equation simulations, and ML methods retrained with outputs from the trained MothNet instance. The MothNet instance was trained using stochastic differential equation simulations and Hebbian updates. The ML methods were then retrained from scratch, with the Readout Neuron outputs from the trained MothNet instance fed in as additional features. Trained ML accuracies of the baselines and cyborgs were compared to assess gains. To compare the effectiveness of MothNet features vs features generated by conventional ML methods, vMNIST experiments were conducted with different feature generators such as PCA and PLS. Feature generators such as PCA, PLS, and NN were used in vMNIST experiments to compare their effectiveness with MothNet features. PCA and PLS generated new features by projecting onto the top 10 modes, while NN utilized output units. CNNs were not used due to the lack of spatial content in vMNIST. For the vMNIST experiments, feature generators like PCA, PLS, and NN were compared with MothNet features. CNNs were not utilized due to the absence of spatial content in vMNIST. A NN with weights initialized from an Omniglot dataset was trained on vMNIST data using transfer learning. Adding an extra hidden layer did not enhance performance, indicating that MothNet features were not equivalent to simply adding layers. MothNet features improved ML accuracy, showing the effectiveness of the architecture in capturing new class-relevant information. MothNet features significantly improved ML accuracy on vMNIST and Omniglot datasets compared to PCA, PLS, and NN feature generators. The architecture captured new class-relevant features, leading to accuracy gains ranging from 10% to 88% on the test sets. Adding an extra hidden layer did not enhance performance, indicating the unique effectiveness of MothNet features. MothNet features improved ML accuracy across all models, with a relative reduction in test error of 20% to 55%. NN models benefited the most, with a 40% to 55% reduction in test error. Even when ML baseline accuracy exceeded MothNet's ceiling, MothNet still improved accuracy. A MothNet front-end improved ML accuracy, even when ML baseline accuracy exceeded MothNet's ceiling. Gains were significant in almost all cases with N > 3, with clustering information in MothNet readouts leveraged more effectively by ML methods. The cyborg framework was run on vMNIST using different feature generators like PCA, PLS, and NN. The MothNet architecture utilized feature generators like PCA, PLS, and NN to improve ML accuracy. Results showed that MothNet features were more effective compared to other methods such as Nearest Neighbors and SVM. The effectiveness of the high-dimensional, sparse layer (MB) alone was tested by using a pass-through AL layer for MothNet in vMNIST experiments. The high-dimensional, sparse layer (MB) was found to be crucial for improving accuracy in the MothNet architecture. Even with a pass-through AL layer, cyborgs still showed significant improvements over baseline ML methods. The AL layer added value by generating strong features, contributing up to 40% of the total gain. Neural networks benefitted the most from the AL layer. The AL layer added value by generating strong features, contributing up to 40% of the total gain. An automated feature generator based on a simple BNN significantly improved learning abilities of standard ML methods on vMNIST and vOmniglot datasets. MothNet's pre-processing made class-relevant information accessible in raw feature distributions. The competitive inhibition layer in MothNet enhances classification by creating attractor basins for inputs, pushing similar samples of different classes towards their respective class attractors. This increases the effective distance between samples and improves classification accuracy. The insect MB, similar to sparse autoencoders, increases distance between samples of different classes by pushing them towards class attractors. MBs differ from SAs as they have more active neurons, no pre-training step, and require fewer samples to improve classification structure. The Mushroom Body (MB) has more active neurons than the input dimension, no pre-training step, and requires few samples to improve classification. Unlike Reservoir Networks, MB neurons have no recurrent connections. The Hebbian update mechanism in MB is distinct from backprop, with weight updates based on a \"use it or lose it\" basis. The dissimilarity of optimizers (MothNet vs ML) may increase total encoded information."
}