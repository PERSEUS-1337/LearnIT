{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complex nature of deep networks makes it impossible to explain their hidden features in a way that humans can understand. However, this paper proposes a knowledge extraction method using \\textit{M-of-N} rules to address this issue. In this paper, a knowledge extraction method using \\textit{M-of-N} rules is proposed to map the complexity/accuracy landscape of hidden features in a Convolutional Neural Network (CNN). The experiments show an optimal trade-off between comprehensibility and accuracy, with each latent variable having an optimal rule to describe its behavior. Rules in the first and final layer are highly explainable, while those in the second and third layer are less so. The study highlights the explainability of rules in different layers of a Convolutional Neural Network (CNN). Rules in the first and final layers are highly explainable, while those in the second and third layers are less so. This sheds light on the feasibility of rule extraction from deep networks and the importance of decompositional knowledge extraction for explainability in Artificial Intelligence. Neural network models, especially deep networks, lack explainability due to their reliance on distributed representations. Unlike symbolic AI or symbolic Machine Learning, distributed representations do not necessarily correlate with easily identifiable features of the data, making knowledge extraction challenging. Knowledge extraction aims to enhance the explainability of neural networks by revealing the implicit knowledge learned in their weights. Techniques involve translating neural networks into symbolic rules or decision trees, similar to those in symbolic AI and ML. Various rule extraction algorithms have been developed over the years to achieve this goal. Rule extraction techniques for neural networks have been developed over decades, with algorithms focusing on decompositional or pedagogical approaches. The main challenge lies in the complexity of the extracted rules, especially for large convolutional neural networks. The challenge in extracting knowledge from neural networks lies in the distributed representations found in the networks, where important concepts are not represented by single neurons but by patterns of activity over many neurons. This distributed nature plays a crucial role in the capabilities of neural networks. Distributed representations in neural networks are crucial for their capabilities. Some argue that explaining latent features with symbolic knowledge extraction is a dead end, and distillation methods should be used instead. However, the efficacy of distillation for improving robustness is debated. Other approaches focus on guarantees of network behavior rather than opening the black box. In this paper, a method is developed to examine the explainability of latent variables in neural networks by using rule extraction to describe latent variables and measuring error and complexity. This allows for mapping out a rule extraction landscape to show the relationship between complexity and extracted rules. The study examines the explainability of latent variables in neural networks through rule extraction, measuring error and complexity. A rule extraction landscape is mapped out to show the relationship between complexity and accuracy of extracted rules. The accuracy of rules depends on the variable being described, with some layers in a CNN showing more accurate rules than others. A 'critical point' on the landscape indicates an ideal M-of-N rule for each latent variable. The study explores the explainability of latent variables in neural networks through rule extraction, showing the relationship between complexity and accuracy of rules. Different layers in CNNs exhibit varying levels of accuracy in extracted rules, with convolutional layers having more complex rules compared to fully connected layers. Rules with near 0% error were found in the first and final layers, while rules from the second and third layers were less accurate. The study explores rule extraction in neural networks, showing varying accuracy levels in different layers. Rules with near 0% error were found in the first and final layers, while the second and third layers had up to 15% error. The experimental results of the extraction process are discussed in different sections before concluding. One of the first attempts at knowledge extraction in neural networks involved using a decompositional approach with Knowledge-based Artificial Neural Networks (KBANN). This algorithm extracted symbolic rules known as M-of-N rules from hidden variable weights. More advanced algorithms have since been developed, including those that generate binary trees representing these rules. These algorithms can be reduced to IF-THEN propositional logic sentences. The more recent algorithms for knowledge extraction in neural networks select an M-of-N rule based on maximum information gain with respect to the output. These methods treat the model as a black box and can be queried to generate data for rule extraction. Other extraction methods combine pedagogical and decompositional approaches. Many extraction methods for knowledge extraction in neural networks combine pedagogical and decompositional approaches, while others opt for visually oriented techniques. Decompositional rule extraction techniques have mainly been applied to shallow networks, focusing on input/output relationships rather than explaining the latent features of deep networks with multiple hidden layers. The text discusses the challenges of using decompositional techniques to explain the features of deep neural networks with multiple hidden layers. The complexity of extracted rules can make it impractical for humans to understand, as argued in previous studies. However, experiments show that some layers of deep networks may still be explainable. Experiments in this paper demonstrate that certain layers of deep networks contain explainable rules that can clarify the network's behavior in terms of specific features. This suggests that rule extraction could be used for modular explanation of network models and provide insights into the similarity and disentanglement of latent features. In logic programming, a rule is defined as an implication A \u2190 B, where A is the head and B is the body. When explaining a neural network using rules, the literals represent the states of neurons. In neural networks, literals represent neuron states. For binary neurons, X = True if x = 1, False if x = 0. For neurons with continuous values, X = True if x > a, False otherwise. Latent variables in neural networks are not well described by single rules due to various input patterns activating neurons. In neural networks, M-of-N rules are used for rule extraction to soften the conjunctive constraint on the body of logical rules by requiring only M of the variables in the body to be true for some specific value of M < N. This approach is more efficient than adding a rule for each input pattern that activates a neuron, as it prevents the network from becoming a large lookup table. M-of-N rules offer a compact representation for rule extraction in neural networks, reflecting input/output dependencies and avoiding a large lookup table. They are a subset of propositional formulas and share structural similarity with neural networks. M-of-N rules offer a compact representation for rule extraction in neural networks. They share structural similarity with neural networks, viewing M-of-N rules as 'weightless perceptrons'. Each M-of-N rule can be represented by a perceptron, with the output neuron as the head and visible neurons as the body of the rule. This paper aims to bring M-of-N rules to the forefront of knowledge extraction. This paper highlights the importance of M-of-N rules in knowledge extraction for neural networks, focusing on defining literals for rule extraction using splitting values and information gain. The paper emphasizes the significance of M-of-N rules in knowledge extraction for neural networks by defining literals for rule extraction using splitting values and information gain. To explain a target neuron, a literal is generated based on information gain with respect to output labels, maximizing entropy decrease. Input literals are then created from inputs to the target neuron by maximizing information gain with respect to the target literal. Each target literal in a layer will have its own set of input literals. In rule extraction, each target literal in a layer has its own set of input literals corresponding to the same set of input neurons with different splits. For convolution layers, each feature map has neurons with different input patches. The goal is to find the optimal split that maximizes information gain for the network output, resulting in a single rule for each feature map. The key metrics in rule extraction are comprehensibility and accuracy. In rule extraction, accuracy and comprehensibility are key metrics. Accuracy is defined in terms of soundness, measuring the expected difference between rule predictions and network outputs. Neurons in a neural network determine the truth of literals, allowing for the computation of rules relating variables. In rule extraction, accuracy is measured by the expected difference between rule predictions and network outputs. Neurons in a neural network determine the truth of literals, allowing for the computation of rules relating variables. Comprehensibility is subjective and based on the complexity of a rule. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), measured relative to a maximum complexity normalized with N possible input variables. The complexity of a rule is measured relative to a maximum complexity normalized with N possible input variables. For example, a simple perceptron with specific weights and bias is used to illustrate this concept. The most complex rule is a 1-of-2 rule. The complexity of a rule is determined by a weighted sum of soundness and complexity, with a parameter \u03b2 controlling the trade-off. By using a brute force search with different \u03b2 values, the relationship between rule complexity and accuracy can be explicitly determined. For \u03b2 = 0, the rule with the minimum loss will be the one with the lowest error. The relationship between rule complexity and accuracy is determined by a weighted sum of soundness and complexity, with a parameter \u03b2 controlling the trade-off. For \u03b2 = 0, the rule with the minimum loss will be the one with the lowest error. Neurons are split using a technique to generate sets of literals, which are then negated based on weights. Neurons are split using a technique to generate sets of literals, which are then negated based on weights. Splits are generated for each neuron to create sets of literals H j and X i. The search through M-of-N rules minimizes L(R) by reordering variables based on weight magnitude. The search procedure relies on the ordering of variables X i to generate splits for h maximizing information gain. The search procedure relies on the ordering of variables X i to generate splits for neurons, maximizing information gain to define sets of literals. A neuron with n input neurons has O(2^n) possible M-of-N rules, making exhaustive search intractable. The assumption is made that the most accurate M-of-N rules with N literals are used. The search procedure relies on ordering variables to generate splits for neurons, maximizing information gain to define sets of literals. With n input neurons, there are O(2^n) possible M-of-N rules, making exhaustive search intractable. The assumption is that the most accurate rules use the literals corresponding to neurons with the strongest weights. The high accuracy in experimental results of rules extracted from the softmax layer suggests this approach is effective. The algorithm orders literals by information gains or weights for rule extraction. Despite the computational challenges, high accuracy was achieved using weights. The algorithm was implemented in Spark on IBM cloud services for efficient rule extraction. Training set examples were used to measure rule accuracy. The algorithm was implemented in Spark on IBM cloud services for efficient rule extraction. Training set examples were used to measure rule accuracy, with 1000 examples being sufficient. Running the search in parallel allowed for mapping the accuracy/complexity graph for 50 hidden neurons in the second and third layer within a few hours. To demonstrate the extraction process for the first hidden feature in the CNN trained on the fashion MNIST dataset, 1000 random input examples were selected from the training set. Each neuron in the CNN was computed along with the predicted labels of the network. With 784 neurons per feature in the first layer, corresponding to different 5x5 patches of the input, a neuron was selected for testing. In the CNN trained on the fashion MNIST dataset, 784 neurons per feature in the first layer correspond to different 5x5 patches of the input. The neuron with the maximum information gain is neuron 96, with an information gain of 0.015 when split on the value 0.0004. This neuron corresponds to the image patch centered at (3, 12). The variable H is defined as 1 if h 96 \u2265 0.0004, and input splits are defined based on this variable. In the CNN trained on the fashion MNIST dataset, the neuron with the maximum information gain is neuron 96, with an information gain of 0.015 when split on the value 0.0004. The variable H is defined as 1 if h 96 \u2265 0.0004. Input splits are chosen for maximum information gain with respect to H. Test input consists of 1000 5x5 image patches centered at (3, 12). Optimal M-of-N rules are determined for various error/complexity tradeoffs, resulting in three different rules visualized in Figure 1. The complexity penalty is increased, resulting in the extraction of three different rules visualized in Figure 1. The rules explain the neuron's behavior, with the most complex rule being a 5-of-13 rule with a 0.025 error, agreeing with the network output 97.5% of the time. The 5-of-13 rule with a 0.025 error agrees with the network output 97.5% of the time. A penalty to complexity results in simpler rules with higher errors. The technique is applied to the DNA promoter dataset, showing the relationship between complexity and rule extraction. Training a feed forward network with a single hidden layer of 100 nodes reveals an exponential relationship between complexity and error, suggesting an ideal tradeoff. The output layer shows 100% fidelity to the network with a specific rule. The rules in question are extracted from the input layer with no complexity penalty. The rules extracted from the input layer, such as 64-of-119 for H 3 9 and 32-of-61 for H 8 0, predict the network's output. Errors propagate when stacking rules that don't perfectly approximate each layer, especially in networks with chosen splits for the input. Errors propagate through layers when stacking rules that don't perfectly approximate each layer. To replace a network with a set of hierarchical rules, a single set of splits for each layer must be decided upon. This can introduce more error, so experiments are conducted layer by layer independently. Based on experiments conducted layer by layer independently, a neural network's rule extraction landscape was examined using a basic CNN trained on fashion MNIST in tensorflow. This approach provides a baseline for the best achievable rules when extracting M-of-N rules and evaluating other extraction algorithms. The layerwise rule extraction search was tested on a basic CNN trained on fashion MNIST in tensorflow. The CNN had a standard architecture with convolutional and max pooling layers, followed by a fully connected layer. Rules were extracted and tested against random inputs from the fashion MNIST training data. For the layerwise rule extraction search on a CNN trained on fashion MNIST, features were tested on image patches with maximum information gain. 50 features were randomly chosen for the third layer, with a limit of 1000 literals for rule complexity. The final layer output was tested with 10 one-hot neurons. The search procedure was repeated for 5 different values of \u03b2. Neurons underwent rule searching procedure for each layer with 5 different values of \u03b2. This resulted in 5 sets of rules with varying error/complexity trade-offs. Averaging complexities and errors for each target neuron produced a graph showing different trade-offs for each layer. First and final layers extracted accurate rules with near 0 error, while second and third layers had different trade-offs. The first and final layers extracted accurate rules with near 0 error, while the second and third layers showed a trade-off between accuracy and complexity. The third layer performed as well as the second layer despite having more input nodes. The optimal accuracy/complexity tradeoff is not solely determined by the number of input nodes. The third layer performs similarly to the second layer despite having more input nodes. The final layer provides more accurate rules with less complexity compared to the first layer. There is a critical point where error increases rapidly as complexity penalty increases, indicating a natural set of rules for explaining latent features. The critical point where error increases rapidly with complexity penalty suggests a natural set of rules for explaining latent features. Current rule extraction algorithms do not consider complexity in optimization, leading to uncertainty in where extracted rules will fall on the Error/Complexity graph. This paper introduces rule complexity as a key factor in extraction algorithms, with empirical evaluation being crucial for validation. The limitations and potential of rule extraction algorithms are highlighted, showing that even the most complex rules can have a 15% error rate. The complexity of rules in CNN layers varies, with the final layer showing simpler rules with near 0% error rates, while earlier layers have more complex rules with a 15% error rate. Selective use of decompositional algorithms depending on the layer being explained is suggested. The black box problem of neural networks hinders their deployment into society, as the need for explainability has grown with their integration. Knowledge extraction from large neural networks remains challenging, with most networks being difficult to interpret and explain. Selective use of decompositional algorithms based on the layer being explained is recommended. The difficulty of interpreting and explaining large neural networks has led to the need for knowledge extraction. Critics argue that the distributed nature of neural networks makes rule extraction unfeasible. However, a novel search method for M-of-N rules applied to a CNN shows that latent features can be described by an 'optimal' rule representing an ideal error/complexity trade-off. The search for extracted rules in a CNN reveals that latent features can be described by an 'optimal' rule representing an ideal error/complexity trade-off. Rule extraction may not provide adequate descriptions for all latent variables, but simplifying explanations without reducing accuracy suggests its usefulness as a tool. Rule extraction in CNNs can simplify explanations of latent variables without compromising accuracy, indicating its usefulness in understanding network behavior. Further research is needed to explore the impact of different factors on the accuracy and interpretability of extracted rules."
}