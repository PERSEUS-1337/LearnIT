{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. This approach addresses the issue of vanishing gradients and justifies why searching in the negative direction minimizes the training loss. Theoretical justification of the concept of STE in training activation quantized neural networks is provided in this paper. The STE-modified chain rule introduces a coarse gradient that correlates positively with the population gradient, aiding in minimizing the population loss. The choice of STE impacts the descent direction for minimizing the loss, and the associated coarse gradient descent algorithm converges to a critical point. Deep neural networks have been successful in various machine learning applications. A poor choice of STE can lead to instability in training algorithms near local minima, as shown in CIFAR-10 experiments. Recent efforts have focused on training coarsely quantized DNNs to achieve memory savings and energy efficiency during inference, while maintaining performance compared to full-precision models. Various techniques have been explored in this area (Courbariaux et al., 2015; Rastegari et al., 2016; Cai et al., 2017; Hubara et al., 2018; Yin et al., 2018b). Training fully quantized DNN involves solving a challenging optimization problem by minimizing a nonconvex empirical risk function subject to weight quantization constraints. Weight quantization in DNN has been extensively studied in the literature. The gradient \u2207f (w) in training activation quantized DNN is almost everywhere zero, making standard back-propagation inapplicable. To address this, a non-trivial search direction is constructed by modifying the chain rule, replacing zero derivatives with a surrogate known as the straight-through estimator (STE). The straight-through estimator (STE) is a proxy derivative used in the backward pass for training activation quantized DNNs. It originates from the perceptron algorithm and is based on a modified chain rule. Other approaches include stochastic neurons and the feasible target propagation algorithm for learning binary activated networks. The perceptron algorithm from the 1950s uses a modified chain rule for calculating gradients. Hinton extended this idea to train multi-layer networks with binary activations. Bengio proposed a variant called STE which uses a derivative for training activation quantized DNNs. Binary activations in deep neural networks have been a focus of research, with various approaches proposed for training networks with constrained weights and activations. The Saturated Straight-Through Estimator (STE) method, introduced by Hubara et al., substitutes the derivative of the signum activation function in the backward pass. This technique has been extended to DNNs with quantized ReLU activations, utilizing derivatives of vanilla ReLU and clipped ReLU as proxies. Despite empirical success, there is limited theoretical understanding of the Saturated Straight-Through Estimator (STE) method in training DNNs with stair-case activations. Recent studies have explored scenarios where certain layers are not ideal for back-propagation, such as leaky ReLU activation. Convergence of algorithms like Convertron using identity STE in the backward pass has been demonstrated. Recent studies have highlighted scenarios where certain layers, like leaky ReLU, are not optimal for back-propagation. Wang et al. proposed a nonlocal Laplacian layer to enhance DNN generalization accuracy, while Athalye et al. introduced a backward pass differentiable approximation to overcome adversarial defenses. This method, similar to STE, successfully broke defenses relying on obfuscated gradients at ICLR 2018. The paper discusses the concept of the \"coarse gradient\" in relation to the Successive Threshold Estimation (STE) method, which aims to understand its optimization perspective in training quantized ReLU. It questions why searching in the negative direction of the coarse gradient minimizes training loss and explores the criteria for a good STE. The paper explores the concept of Successive Threshold Estimation (STE) in training quantized ReLU nets, considering different STEs for learning a two-linear-layer network with binary activation and Gaussian data. Proper choices of STE are shown to minimize training loss. The study focuses on minimizing population loss by choosing proper Successive Threshold Estimation (STE) models for training algorithms. It is proven that negative expected coarse gradients based on STEs of vanilla and clipped ReLUs are descent directions for minimizing population loss, leading to decreasing energy during training. However, the identity STE can result in unstable training near certain local minima. Empirical performances are also examined. The training algorithm can be unstable near certain local minima due to coarse gradients not vanishing. Empirical performances of three STEs on MNIST and CIFAR-10 show that clipped ReLU STE is best for deeper networks like VGG-11 and ResNet-20. Poor STEs lead to unstable training and inferior minima with higher loss and decreased accuracy. The identity STE is not suitable for networks with two trainable layers, as it leads to unstable training and inferior minima with higher loss and decreased accuracy. Convergence guarantees for the perceptron algorithm and Convertron algorithm were only proved for the identity STE, which makes weaker assumptions than the results discussed in this paper. The identity STE is not recommended for networks with two trainable layers due to instability and inferior minima. The quantized activation function's monotonicity is crucial for gradient descent, with STEs like clipped ReLU matching quantized ReLU at extrema to avoid instability issues. In section 2, the energy landscape of a two-linear-layer network with binary activation and Gaussian data is studied. The main results and mathematical analysis for STE are presented in section 3. Empirical performances of different STEs in 2-bit and 4-bit activation quantization are compared in section 4, highlighting instability phenomena observed in CIFAR experiments. Technical proofs and figures are deferred to the appendix. The technical proofs and figures are deferred to the appendix. Notations include Euclidean norm, spectral norm, zero vector, one vector, identity matrix, inner product, Hadamard product, and model similar to (Du et al., 2018) with trainable weights w and v for prediction DISPLAYFORM0 from input Z \u2208 R m\u00d7n. The first layer consists of trainable weights w in a convolutional layer, with Z i representing patches from Z. The second layer acts as a classifier using trainable weights v. The label is determined by y * (Z) = (v * ) \u03c3(Zw * ). The activation function \u03c3 is a binary function \u03c3(x) = 1 {x>0}, different from ReLU used in previous work. The activation function used in the model is a binary function \u03c3(x) = 1 {x>0}, different from ReLU. The learning task is framed as a population loss minimization problem with a sample loss given by a specific formula. Analytic expressions for f(v, w) and its gradient can be found under the Gaussian assumption on Z. The text discusses the loss function and gradient calculations under the Gaussian assumption on Z. It mentions the use of expected sample gradient and the idea of replacing a zero component with a non-trivial function. The text discusses replacing the zero component with a non-trivial function to train a two-linear-layer CNN using the STE \u00b5, leading to coarse gradient descent. The text introduces gradient descent for learning a two-linear-layer CNN with STE \u00b5, discussing the population loss function and its analytic expressions. It highlights the population loss function when w = 0 n and provides partial gradients w.r.t. v and w when w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0). The text concludes that (v, 0 m ) cannot be a local minimizer. The text discusses the conditions for local minimizers in the model, stating that stationary points can only be saddle points and non-differentiable points are potential global minimizers. The text discusses the conditions for local minimizers in the model, stating that stationary points can only be saddle points, and non-differentiable points are potential global minimizers. The population gradient \u2207f (v, w) is proven to be Lipschitz continuous on bounded domains. In the complex case, both saddle points and spurious local minimizers are of interest. In the complex case, both saddle points and spurious local minimizers are of interest. The main results focus on the behaviors of the coarse gradient descent algorithm using different activation functions. The algorithm converges to a critical point with ReLU or clipped ReLU derivatives as the STE, but not with the identity function. Theorem 1 states that Algorithm 1 converges to a saddle point or local minimizer with ReLU or clipped ReLU activation functions, but not with the identity function. If the learning rate is small and certain conditions are met, the objective sequence decreases monotonically. The convergence properties of Algorithm 1 do not hold with the identity function \u00b5(x) = x near local minimizers satisfying \u03b8(w, w*) = \u03c0. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples. With few data points, the empirical loss roughly descends along the negative coarse gradient direction. As sample size increases, the loss gains monotonicity and smoothness, explaining the success of STE with large datasets in deep learning. The convergence properties of Algorithm 1 do not hold with the identity function \u00b5(x) = x near local minimizers satisfying \u03b8(w, w*) = \u03c0. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples. With few data points, the empirical loss roughly descends along the negative coarse gradient direction. As sample size increases, the loss gains monotonicity and smoothness, explaining the success of STE with large datasets in deep learning. The same results hold if the Gaussian assumption on the input data is weakened to rows i.i.d. following a rotation-invariant distribution. The proof will be similar. In this section, we outline the mathematical analysis for the main results. The convergence properties of Algorithm 1 are not guaranteed near local minimizers when using the identity function \u00b5(x) = x with \u03b8(w, w*) = \u03c0. Coarse gradient descent convergence is assured with infinite training samples. As the sample size increases, the loss becomes more monotonic and smooth, explaining the success of STE in deep learning with large datasets. The Gaussian assumption on input data can be relaxed to rows i.i.d. following a rotation-invariant distribution. The mathematical analysis for these results is outlined in this section. The significance of Lemma 5 lies in guaranteeing the descent property of Algorithm 1 by redefining terms and ensuring convergence properties near local minimizers. The success of STE in deep learning with large datasets is explained by the monotonic and smooth loss with increasing sample size. The Gaussian assumption on input data can be relaxed to rows i.i.d. following a rotation-invariant distribution. The significance of Lemma 5 in ensuring the descent property of Algorithm 1 is highlighted by guaranteeing convergence properties near local minimizers. When Algorithm 1 converges using ReLU STE, it can only converge to a critical point of the population loss function. The success of STE in deep learning with large datasets is attributed to the smooth loss with increasing sample size and the relaxation of the Gaussian assumption on input data. Lemma 6 states that Algorithm 1 using ReLU STE converges to a critical point of the population loss function. The coarse partial gradient using clipped ReLU STE generally correlates positively with the true partial gradient of the population loss. Additionally, the coarse gradient only vanishes at critical points. If w = 0 and \u03b8(w, w*) \u2208 (0, \u03c0), then the inner product between the expected coarse and true gradients w.r.t. w is displayed. Lemma 8 discusses the convergence of Algorithm 1, stating that the expected coarse and true gradients vanish simultaneously at saddle points. The derivative of the identity function behaves differently from previous Lemmas, as the coarse gradient does not vanish at local minima. Lemma 9 states that the coarse gradient derived from the identity function does not vanish at local minima, leading to Algorithm 1 potentially never converging there. If 1/m * v = 0 and m > 1, the expected coarse partial gradient with respect to w does not vanish at local minimizers. Lemma 9 suggests that if 1/m * v = 0, the coarse gradient descent may not converge near spurious minimizers with \u03b8(w, w*) = \u03c0. The training loss may increase and instability arise as the iterates approach a local minimizer due to the descent property not holding. Empirical performances of vanilla and clipped ReLUs differ on deeper nets. In this section, the performances of identity, ReLU, and clipped ReLU STEs are compared on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations. The clipped ReLU is expected to be the best performer as it approximates the original quantized ReLU. The instability issue of using an improper STE in training algorithms is also discussed. The resolution \u03b1 for quantized ReLU must be carefully chosen to maintain full-precision accuracy. A modified batch normalization layer is used without scale and shift to approximate a unit Gaussian distribution. The optimal \u03b1 can be pre-computed using Lloyd's algorithm on simulated half-Gaussian data. The resolution \u03b1 for quantized ReLU is determined using a variant of Lloyd's algorithm applied to simulated half-Gaussian data. This \u03b1 value remains fixed throughout training. Batch normalization is added before each activation layer. The optimizer used is stochastic gradient descent with momentum = 0.9. Training consists of 50 epochs for LeNet-5 on MNIST and 200 epochs for other experiments. The optimizer used is stochastic gradient descent with momentum = 0.9 for all experiments. Training includes 50 epochs for LeNet-5 on MNIST, and 200 epochs for VGG-11 and ResNet-20 on CIFAR-10. Parameters are initialized with pre-trained full-precision counterparts. Experimental results are in Table 1, showing training losses and validation accuracies. Clipped ReLU derivative performs best, followed by vanilla ReLU and then the identity function for the three STEs. The derivative of clipped ReLU outperforms vanilla ReLU and the identity function in terms of accuracies. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU shows comparable performance on shallow networks like LeNet-5. The identity function leads to instability issues on ResNet-20 with 4-bit activations. The identity function leads to instability issues when using the identity STE in training, resulting in lower validation accuracies. The algorithm is tested for stability by initializing training with improved minima and using the identity STE with a tiny learning rate. The training using the identity STE leads to instability issues and results in lower validation accuracies. The algorithm starts with a tiny learning rate to test stability, but the coarse gradient with identity STE prevents reaching good minima. Similarly, ReLU STE also shows poor performance due to instability at good minima. The poor performance of ReLU STE on 2-bit activated ResNet-20 is due to instability at good minima. Coarse gradient descent using identity STE leads to repulsion from good minima. The first theoretical justification for STE is that it enables descent training algorithms. The concept of Straight-Through Estimator (STE) enables descent training algorithms. Three types of STEs were considered: derivatives of the identity function, vanilla ReLU, and clipped ReLU. Negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing population loss, while the identity STE generates incompatible gradients. CIFAR experiments confirmed instability with improper STE choices. Future work aims to address this issue. The instability issue with improper choices of Straight-Through Estimator (STE) was confirmed in CIFAR experiments. Future work aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. When using the ReLU STE with a 10^-5 learning rate on ResNet-20, the coarse gradient descent was not stable, leading to an increase in classification and training errors. Lemma 11 states that for Gaussian random vectors with nonzero vectors w and w, certain identities hold. The proof involves assumptions about the vectors and the use of polar representation for two-dimensional Gaussian random variables. Lemma 12 discusses Gaussian random vectors with nonzero vectors w and w, and their corresponding identities. The proof involves the use of polar representation for two-dimensional Gaussian random variables. Lemma 13 proves an inequality involving sin(\u03c6) and \u03be, while Lemma 14 utilizes the Cauchy-Schwarz inequality to establish a relationship between sin(x) and w. The angle between two projections is also discussed in relation to w. Lemma 1 states that if w = 0n, the population loss f(v, w) can be determined. Lemma 2 shows that if w = 0n and the angle between w and w* is between 0 and \u03c0, the partial gradients of f(v, w) with respect to v and w are given. Lemma 2 states that if w = 0n and the angle between w and w* is between 0 and \u03c0, the partial gradients of f(v, w) with respect to v and w are given. The proof of Lemma 2 shows the local optimality of the stationary points. The objective function is rewritten to check local optimality of stationary points. The Hessian matrix is found to be indefinite, indicating saddle points. The unique minimizer to the quadratic function is v. The unique minimizer to the quadratic function is v, which is the key detail in the context of checking local optimality of stationary points. The text discusses conditions for small changes in v and \u03b8, and introduces Lipschitz constants for differentiable points. The text discusses inequalities involving v and w, with references to Lemmas 14.1 and 14.2. It also presents the expected partial and coarse gradients with respect to v and w, along with proofs for the claims made. The text presents inequalities involving v and w, with references to Lemmas 14.1 and 14.2. It discusses expected partial and coarse gradients with respect to v and w, along with proofs for the claims made. Lemmas 2 and 4 are used to derive results related to inner products and angles between vectors. The text discusses inequalities involving v and w, with references to Lemmas 14.1 and 14.2. It presents proofs for claims made using Lemmas 2 and 4 to derive results related to inner products and angles between vectors. The second claim is shown assuming w = 1, and various formulas are derived based on different conditions of w and \u03b8(w, w*). If v * = 0, then expressions for v and \u03b8(w, w * ) from Proposition 1 are obtained. Lemma 7 states conditions for w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0). The inner product between expected coarse and true gradients w.r.t. w is discussed. Additionally, a constant A crelu > 0 is determined based on certain conditions. The proof of Lemma 7 involves computing E Z g crelu (v, w; Z) and making relevant calculations. Lemma 9 states that for \u00b5(x) = x, the expected coarse gradient is calculated. The proof of Lemma 8 is similar to Lemma 6, with q(\u03b8, w) being non-negative and equal to 0 only at \u03b8 = 0, \u03c0, and p(0, w) \u2265 p(\u03b8, w) \u2265 p(\u03c0, w) = 0. The core part is that q(\u03b8, w) defined in Lemma 12 is non-negative and equals 0 only at \u03b8 = 0, \u03c0, as well as p(0, w) \u2265 p(\u03b8, w) \u2265 p(\u03c0, w) = 0. Lemma 9 states that for \u00b5(x) = x, the expected coarse gradient is calculated. By using certain identities, the inner product between the expected coarse and true gradients w.r.t. w is determined."
}