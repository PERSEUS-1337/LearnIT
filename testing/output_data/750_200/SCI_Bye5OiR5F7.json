{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs is introduced using the Wasserstein-2 metric proximal on the generators. The approach utilizes the gradient operator induced by optimal transport in implicit deep generative models, connecting sample space and parameter space geometry. It provides a regularizer for parameter updates, improving speed and stability in training GANs.GANs involve a discriminator distinguishing real and generated data, while the generator aims to deceive the discriminator. The adversarial game in GANs involves a discriminator distinguishing between real and generated data, while the generator aims to deceive the discriminator by recreating the density distribution from the real source through an implicit generative model. The problem of matching a target density is formulated as the minimization of a discrepancy measure, such as the Kullback-Leibler (KL) divergence. The Wasserstein distance, also known as Earth Mover's distance, is an alternative approach to defining a discrepancy measure between densities. It has been used in defining loss functions for generative models like Wasserstein GAN, which has gained significant interest in recent years. This method can introduce structures that aid in optimization processes. The Wasserstein GAN BID4 has attracted interest by defining the loss function and introducing structures for optimization using optimal transport. The paper derives the Wasserstein steepest descent flow for deep generative models in GANs, utilizing the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. This natural gradient, like the Fisher-Rao natural gradient induced by KL divergence, is commonly found in learning problems. In GANs, the Fisher-Rao natural gradient induced by KL divergence is problematic due to low dimensional support sets. Instead, the paper proposes using the gradient operator induced by the Wasserstein-2 metric for GAN generators, with regularization as the squared constrained Wasserstein-2 distance, approximated by a neural network. The paper introduces a relaxed proximal operator for GAN generators, simplifying the computation by only involving the difference of outputs. This method can be easily implemented as a drop-in regularizer for generator updates. The paper introduces a relaxed proximal operator for GAN generators, simplifying computation by only involving output differences. It can be easily implemented as a drop-in regularizer for generator updates. The method is organized into sections introducing Wasserstein natural gradient and a proximal method in Algorithm 1. The effectiveness of the proposed methods is demonstrated in experiments with various GANs, while related work on optimal transport and its proximal operator on a parameter space is briefly reviewed. Optimal transportation defines distance functions between probability densities. Optimal transportation defines distance functions between probability densities, with W p referred to as the Wasserstein-p distance. This paper focuses on the case p = 2, denoted as W. The Wasserstein-2 distance has a dynamical formulation as a trajectory transporting densities with minimal kinetic energy. The classic theory of optimal transportation defines distance functions between probability densities, with the Wasserstein-2 distance representing the trajectory transporting densities with minimal kinetic energy. This theory is extended to cover parameterized density models, where the constrained Wasserstein-2 metric function is defined on a parameter space. The constrained Wasserstein-2 metric function is defined on a parameter space with a formulation involving feasible Borel potential functions and continuous parameter paths. This metric can be used for steepest descent optimization schemes and is different from the Wasserstein-2 distance on the full density set. The constrained Wasserstein-2 metric allows for steepest descent optimization schemes based on a Riemannian structure. The Wasserstein natural gradient is obtained from this metric, providing a gradient operator for loss functions. The Wasserstein gradient operator is defined by a loss function F on \u0398, with G(\u03b8) representing the matrix of the Wasserstein Riemannian metric. The steepest descent flow and gradient descent iteration are given by specific formulas, with the computation of matrix G(\u03b8) \u22121 often being challenging in practice. The backward Euler method, also known as the Jordan-Kinderlehrer-Otto (JKO) scheme, provides a numerical scheme for equation 2 using the proximal operator. It involves regularization of the original loss function by updating parameters with a distance approximation computed through a second order Taylor expansion. This approach is particularly useful in the parameterized setting discussed earlier. The Semi-Backward Euler method for the gradient flow of loss function F : \u0398 \u2192 R is a first-order scheme that approximates the d W distance locally through a second order Taylor expansion. It is easier to approximate than the forward Euler method and simpler than the backward Euler method, as it does not require computing and inverting G(\u03b8). The Semi-Backward Euler method in implicit generative models involves implementing a generator function g(\u03b8, z) that maps noise prior Z to output samples with density X. The method simplifies the gradient flow approximation by avoiding the computation and inversion of G(\u03b8) and handling constrained optimization over \u03a6. The constrained Wasserstein-2 metric in implicit generative models simplifies the formulation and introduces a simple algorithm for the proximal operator on generators. It involves mapping noise prior Z to output samples with density X using a neural network to approximate variable \u03a6. The constrained Wasserstein-2 metric in implicit generative models simplifies the formulation and introduces a simple algorithm for the proximal operator on generators. It involves mapping noise prior Z to output samples with density X using a neural network to approximate variable \u03a6. The constrained Wasserstein metric requires the derivative of the generator g w.r.t. \u03b8 to be a gradient vector field of \u03a6 w.r.t x. The gradient constraint is satisfied when the sample space is 1 dimensional, but in general, this is not true. Finding \u03a6 involves computational difficulties and fitting the gradient constraint remains an open problem. The computation of Wasserstein proximal operator faces difficulties due to the gradient constraint. To simplify computations, a relaxed Wasserstein metric is considered on the parameter space. This approximation is based on a new metric to obtain an infimum among all feasible continuous parameter paths. In high-dimensional sample spaces, the update does not exactly correspond to the Wasserstein proximal, but rather regularizes the generator. The relaxed Wasserstein metric is used to approximate the Wasserstein proximal operator in high-dimensional sample spaces. It regularizes the generator by minimizing the expectation of squared differences in the sample space. The Wasserstein proximal operator in GANs is illustrated using a toy example with a family of distributions. The proximal regularization for a loss function is defined, and statistical distance functions between parameters are checked. The statistical distance functions between parameters \u03b8 and \u03b8 k are checked, including Wasserstein-2 distance, Euclidean distance, and Kullback-Leibler divergence. The Wasserstein-2 and Euclidean distances are effective in measuring the difference of probability models, while the Euclidean distance is independent of the model structure. The Wasserstein proximal operator decreases the loss function using the Wasserstein-1 metric. The Wasserstein-1 metric is used as the loss function with \u03b8* = (a*, b*). The Wasserstein proximal operator outperforms the Euclidean proximal in decreasing the objective function. Numerical experiments show that the Relaxed Wasserstein Proximal algorithm is effective for Wasserstein gradient-descent on various GANs. The Relaxed Wasserstein Proximal (RWP) algorithm improves speed and stability in training GANs by applying regularization on the generator, which is novel compared to traditional methods focusing on the discriminator. The Relaxed Wasserstein Proximal (RWP) algorithm introduces regularization on the generator in GAN training, focusing on improving speed and stability. This approach modifies the update rule for the generator by introducing hyperparameters such as the proximal step-size and the number of iterations. The algorithm is tested on three types of GANs, including Standard GANs. The Relaxed Wasserstein Proximal regularization is tested on Standard GANs, WGAN-GP, and DRAGAN using CIFAR-10 and CelebA datasets with DCGAN architecture. Fr\u00e9chet Inception Distance (FID) is used to measure sample quality and convergence, with FID measured every 1000 outer-iterations for CIFAR-10 and CelebA datasets. The Relaxed Wasserstein Proximal regularization improves the speed and stability of GAN training convergence. FID is used to measure sample quality, with measurements taken every 1000 outer-iterations for CIFAR-10 and CelebA datasets. Hyperparameter choices are detailed in Appendix C, and comparisons are aligned based on wallclock time. Our regularization improves GAN training convergence speed and stability, resulting in lower FID scores for all GAN types. Specifically, DRAGAN shows a 20% improvement in sample quality. Similar results are observed for the CelebA dataset. Multiple generator iterations may hinder Standard GANs on CelebA initially, requiring algorithm restarts for successful learning. Multiple generator iterations may hinder Standard GANs on CelebA initially, requiring algorithm restarts for successful learning. This defect may be rectified with a more stable loss function or different parameters. The effect of multiple generator updates compared to discriminator updates is also examined. Using the RWP regularization in GAN training improves stability and lowers FID compared to omitting regularization. Even with the stable WGAN-GP, omitting regularization leads to high FID variance. Latent space walks show that RWP does not cause GANs to memorize. Additionally, RWP improves speed in terms of wallclock time. Samples from the models are available in Appendix E, and further details can be found in Appendix F. The use of RWP regularization in GAN training improves stability, lowers FID, and enhances speed in terms of wallclock time. Multiple generator iterations may initially fail but eventually lead to successful runs. The experiment compared the effect of performing 10 generator iterations per outer-iteration with and without RWP regularization. With RWP, convergence was achieved and FID was lower, while without RWP, training was highly variable with FID on a rising trend. The Semi-Backward Euler method was also evaluated on the CIFAR-10 dataset, showing comparable results to standard WGAN-GP training. The experiment was averaged over 5 runs. The training of Semi-Backward Euler (SBE) is more complex as it involves approximating three functions: the discriminator, generator, and potential function \u03a6 p. The algorithm and hyperparameter settings are detailed in the appendix. Optimization over the three networks is shown in FIG3, with SBE method comparable to standard WGAN-GP. The Semi-Backward Euler method is comparable to norm WGAN-GP in terms of generator iterations alignment. Many studies in the literature apply the Wasserstein distance as the loss function in optimal transport for machine learning and GANs. The Wasserstein distance is a statistical distance used for comparing probability distributions supported on lower dimensional sets. It is leveraged in Wasserstein GANs, where the loss function is the Wasserstein-1 distance. The discriminator must satisfy the 1-Lipschitz condition in its computations. The loss function in Wasserstein GANs is the Wasserstein-1 distance, with the discriminator needing to satisfy the 1-Lipschitz condition. Regularization techniques are used to meet this requirement. The Wasserstein-2 metric creates a metric tensor structure, forming an infinite dimensional Riemannian manifold known as the density manifold. Gradient flows in this manifold are linked to transport-related partial differential equations like the Fokker-Planck equation. The gradient flow in the density manifold is connected to transport-related partial differential equations like the Fokker-Planck equation. Learning communities explore leveraging this structure in probability space and studying stochastic gradient descent. Nonparametric models like the Stein gradient descent method have also been researched. Many nonparametric models, including the Stein gradient descent method, have been studied in the context of probability over parameters. Approximate inference methods for computing Wasserstein gradient flow have been considered, with a focus on constrained Wasserstein gradient with fixed mean and variance in parameter space. In this work, the focus is on applying the constrained Wasserstein gradient and its relaxations to implicit generative models, specifically on regularizing the generator. The approach differs from previous works that mainly concentrated on regularizing the discriminator. The study explores the Wasserstein gradient flow in Gaussian families and elliptical distributions, aiming to work on general implicit generative models. The focus of this work is on regularizing the generator using the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space. The proposed method leads to a better minimizer in terms of FID and faster convergence speeds. A metric function W2 is considered in the full probability set, introducing a Riemannian structure in density space. The variational formulation introduces a Riemannian structure in density space, defining smooth and strictly positive probability densities. The tangent space of the set is given by a specific elliptic operator, identifying functions modulo additive constants. The inner product endows the set with a Riemannian metric tensor. The Wasserstein gradient operator in (P+, gW) is defined for a loss function F: P+ \u2192 R. The gradient flow satisfies analytical results provided in BID3. The Wasserstein-2 metric and gradient operator are then constrained on statistical models defined by a triplet (\u0398, Rn, \u03c1). The statistical model is defined by a triplet (\u0398, Rn, \u03c1), where \u0398 \u2282 Rd and \u03c1 : \u0398 \u2192 P(Rn). The Riemannian metric g on \u03c1(\u0398) is defined by pulling back the Wasserstein-2 metric tensor. Wasserstein statistical manifold is defined for \u03b8 \u2208 \u0398 and \u03c3i \u2208 T\u03b8\u0398. The associated metric tensor G(\u03b8) is assumed to be smooth. The associated metric tensor G(\u03b8) is smooth and positive definite, forming a smooth Riemannian manifold. The constrained Wasserstein gradient operator in parameter space is studied in Theorem 2. The distance dW can be expressed as an action function in the Wasserstein statistical manifold, with the gradient operator defined on a Riemannian manifold. The gradient operator on a Riemannian manifold is defined with \u03a6(t, x) satisfying certain conditions. The derivation of the proposed semi-backward method is presented, along with the proof of a claim involving a geodesic path. Equations 6 and 7 are proven, with the maximizer on the right-hand side of equation 7 being discussed. Equation 7 is proven by comparing the left-hand side and right-hand side, leading to the derivation of a consistent numerical method known as the Semi-backward method. The proof of Proposition 4 is also presented, along with the implicit model and gradient constraint. The implicit model and gradient constraint are presented in this section, showing the probability density transition equation satisfies the constrained continuity equation. The equations are derived using gradient and divergence operators, with integration by parts utilized for simplification. The text discusses the explicit computation of the proximal operator using Wasserstein and Euclidean distances, with equations derived using integration by parts and push forward relations. The text discusses the explicit computation of the proximal operator using Wasserstein and Euclidean distances, with hyperparameter settings for experiments in Section 3.1 provided. The text discusses hyperparameter settings for experiments in Section 3.1, including values for \u03b2, latent space dimension, optimizer, learning rate, and generator iterations for different datasets and models. The Relaxed Wasserstein Proximal is highlighted as an easy-to-implement regularization method. The text discusses hyperparameter settings for experiments in Section 3.1, including values for \u03b2, latent space dimension, optimizer, learning rate, and generator iterations for different datasets and models. The Relaxed Wasserstein Proximal is highlighted as an easy-to-implement regularization method with specific algorithm steps outlined for Standard GANs. The text discusses hyperparameter settings for experiments in Section 3.1, including values for \u03b2, latent space dimension, optimizer, learning rate, and generator iterations for different datasets and models. The Relaxed Wasserstein Proximal is highlighted as an easy-to-implement regularization method with specific algorithm steps outlined for Standard GANs. In training GANs with RWP regularization, updating the discriminator and performing Adam gradient descent are key steps. The difference lies in the terms g \u03b8 (z i ) \u2212 g \u03b8 k\u22121 (z i ) 2 2 and the number of generator iterations. An outer-iteration involves updating the discriminator and generator multiple times. Samples generated from a Standard GAN with RWP regularization on the CelebA dataset are shown in FIG4. In experiments, hyperparameter settings for GANs with RWP regularization are discussed. Samples from Standard GAN and WGAN-GP on different datasets are compared, showing smooth transitions in latent space walks. Specific settings for WGAN-GP on CIFAR-10 include a batch size of 64 and DCGAN architecture. The hyperparameter settings for Semi-Backward Euler (SBE) on WGAN-GP trained on CIFAR-10 include a batch size of 64, DCGAN architecture, Adam optimizer with learning rate 0.0002, latent space dimension of 100, and updating the discriminator 5 times, the generator once, and the potential 5 times in every outer-iteration loop. In each outer-iteration loop, the discriminator was updated 5 times, the generator once, and the potential 5 times."
}