{
    "title": "SygMXE2vAE",
    "content": "BERT and other Transformer-based models have achieved state-of-the-art results in Natural Language Processing tasks. A layer-wise analysis of BERT's hidden states reveals valuable information beyond attention weights, particularly in models fine-tuned for Question Answering tasks. Our analysis focuses on how QA models transform token vectors to find answers, using probing tasks to reveal information in each layer. Hidden state visualizations offer insights into BERT's reasoning process, showing transformations related to traditional tasks. BERT can implicitly incorporate task-specific information into its token representations. The system can incorporate task-specific information into token representations. Fine-tuning has little impact on semantic abilities. Transformer models are prevalent in Natural Language Processing, surpassing RNNs in Machine Translation. Large models and extensive pre-training have led to advancements in translation. Permission is granted for personal or classroom use, with restrictions on commercial distribution. Copyrights must be respected. Request permissions for other uses. The main subject of this paper is BERT BID8, a popular Transformer model that has shown significant improvements in various Natural Language Processing tasks. The paper discusses the challenges of black box models, particularly in the context of Transformer models like BERT. While Transformers are seen as somewhat interpretable through attention values, current research suggests this may not always be the case. The paper takes a different approach to interpreting Transformer Networks by examining hidden states between encoder layers directly. It aims to address questions such as whether Transformers answer questions decompositionally like humans, if specific layers in a multi-layer network solve different tasks, the influence of fine-tuning on a network's inner state, and how evaluating network layers can help understand why and how a network functions. The paper examines hidden states in Transformer Networks to understand if specific layers solve different tasks, the impact of fine-tuning on network states, and how evaluating network layers can explain failures in predicting correct answers. The focus is on Question Answering as a complex task that requires solving various Natural Language Processing tasks. The paper explores hidden states in Transformer Networks to analyze if different layers handle various tasks, the effect of fine-tuning on network states, and how evaluating network layers can clarify incorrect answer predictions. It focuses on Question Answering as a complex task that involves solving multiple NLP tasks. Additionally, the study suggests that other NLP tasks can also be approached as QA tasks, with preliminary tests on the small GPT-2 model yielding similar results. The research introduces a layer-wise visualization of token representations to reveal insights into the internal state of Transformer networks and proposes a set of general NLP Probing Tasks extended by QA-specific tasks like Question Type Classification and Supporting Fact Extraction. The paper delves into analyzing hidden states in Transformer Networks to understand how different layers handle tasks, the impact of fine-tuning on network states, and how evaluating network layers can clarify incorrect answer predictions. It focuses on Question Answering as a complex task and introduces a set of general NLP Probing Tasks extended by QA-specific tasks like Question Type Classification and Supporting Fact Extraction. The study shows that BERT's transformations go through similar phases, with general language properties encoded in earlier layers and used to solve tasks in later layers. The analysis focuses on Transformer models, particularly BERT and GPT-2, which are part of the Transformer network group. Other notable Transformer models include Universal Transformer and TransformerXL, which aim to improve certain aspects. The analysis highlights the Universal Transformer and TransformerXL models as potential areas for future research to improve the flaws of the Transformer architecture. Research on interpretability and probing tasks in neural models is also emphasized, with a focus on post-hoc methodologies applied to trained models. Recent advances in probing tasks and methodologies have been made, with a focus on applying them to trained models like ELMo, BERT, and GPT-1. Various studies have proposed different probing frameworks and tasks, analyzing both semantic and syntactic information. Fine-tuned models are not specifically studied in these analyses. Recent studies have focused on analyzing pre-trained models like ELMo, BERT, and GPT-1 through probing tasks. Different frameworks and tasks have been proposed to analyze semantic and syntactic information, with a specific focus on BERT in the context of a Ranking task. Some studies also explore models through qualitative visual analysis, limited to CNNs. Zhang and Zhu BID41, Nagamine et al. BID24, Hupkes et al. BID14, and Li et al. BID17 explore different aspects of neural networks such as CNNs, DNNs, word vectors, and sequence tagging. Liu et al. BID20 also analyze BERT's token representations but focus solely on pre-trained models. Our work extends previous research on BERT's token representations by analyzing fine-tuned models and specific phases of BERT. Inspired by Jain and Wallace, we explore the limitations of attention for explainability and interpretability through adversarial examples and traditional methods. Our analysis focuses on fine-tuned BERT models, evaluating hidden states and token representations for explainability and interpretability. We examine transforming token vectors qualitatively and probe language abilities quantitatively through QA-related tasks. The architecture of BERT and Transformer networks enables us to follow these transformations effectively. The architecture of BERT and Transformer networks allows for analyzing token transformations qualitatively and quantitatively through QA-related tasks. Hidden states are evaluated for explainability and interpretability by collecting representations of tokens in each layer. BERT's hidden states are collected from each layer without padding, allowing for the representation of each token throughout the model's layers. Distances between token vectors are used as indicators for semantic relations, as there are no references for semantic meanings within the vector spaces. Dimensionality reduction is applied to BERT's pre-trained models with vector dimensions of 1024 (large model) and 512 (base model) to visualize relations between tokens in a two-dimensional space. Dimensionality reduction techniques such as T-SNE, PCA, and ICA are applied to BERT's hidden states from each layer to visualize token relations in a two-dimensional space. PCA is used to present the most distinct clusters, and k-means clustering is applied to verify the distribution in high-dimensional vector space. The number of clusters is chosen based on observed clusters in PCA across layers. After applying dimensionality reduction techniques like T-SNE, PCA, and ICA to BERT's hidden states, k-means clustering is used in high-dimensional vector space to verify the distribution. The number of clusters is determined based on observed clusters in PCA across layers. Semantic probing tasks are then applied to analyze information stored in transformed tokens after each layer, aiming to understand how BERT answers questions and maintains language information. The study focuses on Edge Probing for NLP tasks like Named Entity Labeling, Coreference Resolution, and Relation Classification. Additional tasks include Question Type Classification and Supporting Fact Identification for Question Answering. Named Entity Labeling involves predicting entity categories for token spans, based on Named Entity Recognition. Coreference Resolution requires predicting if two mentions refer to the same entity, with negative samples added for training. Coreference task involves predicting if two mentions refer to the same entity, with negative samples added for training. Relation Classification task requires predicting relation types connecting known entities, sourced from the SemEval 2010 Task 8 dataset. Question Type Classification task focuses on identifying question types accurately using the Question Classification dataset. Source code for all experiments will be made publicly available. The Question Type Classification task involves correctly identifying question types using the Question Classification dataset, which includes 500 fine-grained types of questions. The extraction of Supporting Facts is crucial for Question Answering tasks, especially in multihop cases, where BERT's token transformations play a role in distinguishing distracting information. BERT's token transformations play a crucial role in distinguishing distracting from important context parts in Question Answering tasks, particularly in multihop cases. A probing task is constructed to identify Supporting Facts, where the model predicts if a sentence contains relevant information regarding a specific question. This task tests the hypothesis that token representations contain information about their significance to the question. The probing task involves identifying Supporting Facts by predicting if a sentence contains relevant information for a specific question. Different tasks are constructed for each dataset to assess their ability to recognize key details. Input tokens are embedded using a fine-tuned BERT model for each probing task sample. For the probing task, input tokens are embedded using a fine-tuned BERT model for all layers. Only tokens of \"labeled edges\" within a sample are considered for classification, which are then fed into a Multi-layer Perceptron classifier to predict label-wise probability scores. The curr_chunk discusses using a two-layer Multi-layer Perceptron classifier to predict label-wise probability scores after representation extraction. The study aims to understand how BERT performs on complex downstream tasks like Question Answering, which involves tasks such as Coreference Resolution and Relation Modeling. Detention is a common punishment in schools in the UK, Ireland, and other countries. It involves students remaining in school during specific times, even on non-school days. Detention is a common punishment in schools, requiring students to stay in school during specific times, even on non-school days. HotpotQA is a Multihop QA task with 112,000 question-answer pairs designed to combine information from multiple parts of a context. The distractor-task of HotpotQA involves contexts with supporting and distracting facts, with an average size of 900 words. The distractor-task of HotpotQA involves contexts with supporting and distracting facts, with an average size of 900 words. To accommodate the input size restriction of the pre-trained BERT model, the amount of distracting facts is reduced by a factor of 2.7. Additionally, yes/no-questions are excluded from the analysis. The QA bAbI tasks are artificial toy tasks designed to test neural models' abilities in reasoning over multiple sentences, including Positional Reasoning, Argument Relation Extraction, and Coreference Resolution. The models analyzed, BERT BID8 and GPT-2 BID28, are Transformers that build upon previous Transformer models like ELMo and ULMFit. They focus on Multihop QA tasks with Positional Reasoning and Coreference Resolution, differing from other QA tasks in simplicity and artificial sentence nature. Transformer models BID36, SemiSupervised Sequence Learning BID5, ELMo BID25, and ULMFit BID13 are analyzed in this study. The models integrate into a probing setup based on Pytorch implementation of BERT. Monolingual models bert-base-uncased and bert-large, as well as GPT-2 small model, are used for experiments. In this study, the monolingual models bert-base-uncased and bert-large are fine-tuned on different datasets. Hyperparameters such as learning rate and batch size are tuned through grid search. Models are trained for 5 Epochs with evaluations every 1000 iterations. Input length is set to 384 tokens for bAbI and SQuAD tasks, and 512 tokens for HotpotQA tasks. The input length chosen is 384 tokens for bAbI and SQuAD tasks, and 512 tokens for HotpotQA tasks. Evaluation includes models trained on a single bAbI task and a multitask model trained on data of all 20 tasks. Two settings are distinguished: Span prediction and Sequence Classification. Span prediction requires appending all possible answers to the base context. For span prediction in HotpotQA, all possible answers are appended to the base context. HotpotQA tasks include Support Only (SP) and Distractor tasks, with the latter including distracting sentences up to the 512 token limit. This approach simplifies the task and enhances token vector distinction. The HotpotQA Distractor task includes distracting sentences within the 512 token limit. Evaluation results show that the model performs well on the SQuAD task but struggles with tasks derived from HotpotQA, especially the distractor setting. GPT-2 performs better on the bAbI task compared to SQuAD and HotpotQA. BERT and GPT-2 excel in different tasks. GPT-2 outperforms BERT in bAbi tasks, particularly in tasks 17 and 19 requiring positional reasoning. Qualitative analysis shows recurring patterns in vector transformations. The text reveals recurring patterns in vector transformations from samples of SQuAD and bAbI task datasets. Results from probing tasks are compared across different BERT models with varying layers. PCA representations suggest the model's progression through layers. The study compares results of two BERT-large models, one fine-tuned on HotpotQA and one without fine-tuning. PCA representations show the model's phases in answering questions across diverse tasks. The four phases include Semantic Clustering where early layers group tokens into topical clusters. In the middle layers of BERT-based models, entities are connected more by their relation within a certain input rather than by topical similarity. These layers serve as an implicit replacement for embedding layers in neural network architectures. In the middle layers of neural networks, clusters of entities are connected by their relation within a certain input context rather than by topical similarity. Task-specific clusters already filter question-relevant entities, such as a cluster with words like countries, schools, detention, and country names. Another question-related cluster involves identifying facts about Emily being a wolf and wolves being afraid of cats. The main challenge in the question-related cluster is to identify the facts that Emily is a wolf and Wolves are afraid of cats. The cluster highlights Emily as a relevant entity related to wolves, including mentions of wolves in plural form. Similar clusters are observed in the HotpotQA model, with more coreferences. The model's ability to recognize entities, identify mentions, and find relations improves in higher network layers. The ability to recognize entities, identify mentions, and find relations improves in higher network layers, as shown in FIG7. Named Entity Labeling is learned first, while coreference resolution and relation recognition require input from additional layers. This pattern is consistent in both BERT-base and BERT-large models. Matching questions with supporting facts is crucial for Question Answering and Information Retrieval, traditionally achieved by filtering context parts based on relevance. In traditional pipeline models, filtering context parts based on relevance is crucial for Question Answering and Information Retrieval. BERT models transform tokens to match question tokens with relevant context tokens, as shown in FIG4. Probing tasks reveal the models' strong ability to distinguish relevant information from irrelevant information. The models' ability to distinguish relevant information from irrelevant information is strongest in their higher layers, as shown in FIG1 for SQuAD and bAbI. However, the fine-tuned HotpotQA model in FIG2 does not perform as well without fine-tuning, especially in identifying the correct Supporting Facts. Vector representations help identify important facts matched with the question. The vector representations in the model's higher layers help identify important facts matched with the question, aiding in retracing decisions and increasing transparency. In the last network layers, the model separates correct answer tokens from the rest, forming homogeneous clusters. The vector representation at this stage is task-specific and learned during fine-tuning, leading to a performance drop in general NLP probing tasks. The vector representation in the last network layers is task-specific and learned during fine-tuning, leading to a performance drop in general NLP probing tasks. This loss of information is especially noticeable in the large BERT model fine-tuned on HotpotQA. The model without fine-tuning still performs well on tasks like NEL or COREF, but the fine-tuned model loses this ability. Analogies to Human Reasoning are drawn, comparing the phases of answering questions to the human reasoning process. The first phase of semantic clustering in human reasoning involves decomposing input into parts. The second phase focuses on building relations between these parts to connect information for answering questions. Separating important from irrelevant information and grouping potential answer candidates are also part of human reasoning. However, BERT differs in that it can process all parts of the input simultaneously, allowing for concurrent phases depending on the task at hand. BERT can process all parts of the input simultaneously, allowing for concurrent phases depending on the task at hand. Comparing insights from BERT models to GPT-2, a major difference is that GPT-2 gives particular attention to the first token of a sequence. This occurs even when it is not the question word in a QA setup. In contrast to BERT, GPT-2 focuses on the first token of a sequence during dimensionality reduction, leading to a separation of clusters. This issue affects all layers of GPT-2 except for the Embedding Layer, the first Transformer block, and the last one. To address this, the first token is masked during dimensionality reduction. GPT-2, like BERT, distinguishes between relevant Supporting Facts and the question in the vector space, as shown in FIG8. GPT-2, similar to BERT, separates relevant Supporting Facts and questions in the vector space. It also identifies another sentence with similar meaning. Unlike BERT, the correct answer is not distinctly separated in GPT-2. These findings extend beyond BERT to other Transformer networks. Future work will involve more probing tasks to confirm these observations. The focus is on understanding failure states in explainable Neural Networks. Observation of Failure States in explainable Neural Networks is crucial. Visualizations can show when, why, and how the network fails. Differentiating between correct and wrong predictions is possible through hidden state representations. For wrong predictions, the phases may resemble correct predictions but focus on the wrong answer. Inspecting early layers provides insight into failure states. When the network has confidence in its prediction, phases may resemble a correct prediction but center on the wrong answer. Inspecting early layers can reveal reasons for wrong candidate selection, such as incorrect Supporting Facts or misresolution of coreferences. Low network confidence, often seen when the predicted answer is far from the actual answer, results in different transformations in each layer. When the network has confidence in its prediction, phases may resemble a correct prediction but center on the wrong answer. In cases where the predicted answer is far from the actual answer, transformations do not go through the usual phases. The vector space is still transformed in each layer, but tokens are mostly kept in a single cluster. Low network confidence leads to different transformations in each layer, with little impact from fine-tuning on core NLP abilities. The impact of fine-tuning on core NLP abilities is minimal, as the pretrained model already contains sufficient information about words and their relations. Fine-tuning only applies small weight changes and forces the model to forget some information to fit specific tasks. However, the model retains much of its previously learned encoding when fitting the QA task, explaining the success of the Transfer Learning approach. Maintained positional embedding is crucial for Transformer network performance. The positional embedding is crucial for Transformer network performance, as it maintains its effects even into late layers. Visualizations show its importance in tasks like SQuAD dataset. Fine-tuning on Question Type probing task also yields interesting results. The performance curves show that fine-tuning on SQuAD improves the model's ability to resolve question types, while fine-tuning on bAbI tasks results in a loss of this ability due to the static structure of the samples. The study found that the model fine-tuned on HotpotQA did not outperform the model without fine-tuning, indicating that BERT-large is pre-trained to recognize question types. This sheds light on the inner workings of Transformer networks and their interpretability. The qualitative analysis of token vectors in Transformer models reveals interpretable information that can help identify misclassified examples and model weaknesses. It also provides insights into which parts of the context are important for decision-making. Lower layers of the model may be more suitable for certain problems. Further research is needed to develop methods for processing this information. Transfer Learning suggests choosing layer depth based on the specific task at hand. Further research is recommended on skip connections in Transformer layers for potential advantages in direct information transfer between non-adjacent layers solving different tasks. The modularity of Transformer networks indicates that specific layers address distinct problems, supporting the idea of different phases in the network. Our work explores the modularity of Transformer networks, suggesting that specific layers tackle different problems, hinting at potential benefits in task-specific pre-training. Further research should focus on understanding how state-of-the-art models solve downstream tasks to enhance their performance."
}