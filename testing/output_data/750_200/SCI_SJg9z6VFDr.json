{
    "title": "SJg9z6VFDr",
    "content": "Recently, various neural networks have been proposed for irregularly structured data such as graphs and manifolds. All existing graph networks have discrete depth, but a new model called graph ordinary differential equation (GODE) extends the idea of continuous-depth models to graph data. The derivative of hidden node states is parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. Two end-to-end methods for efficient training of GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. The GODE model introduces efficient training methods using direct back-propagation through the ODE solver, outperforming the adjoint method. Bijective blocks enable low memory consumption, and GODE can be easily integrated with existing graph neural networks to improve accuracy in node and graph classification tasks. The GODE model is efficient in semi-supervised node and graph classification tasks, offering continuous time modeling, memory efficiency, accurate gradient estimation, and generalizability across different graph networks. Unlike CNNs limited to grid-based data like images and text, GODE excels in irregularly structured datasets represented by a graph data structure. Graph data structures represent objects as nodes and relations as edges, widely used for irregularly structured data like social networks, protein interaction networks, and citation graphs. Traditional methods like random walk and graph embedding have limited expressive capacity, leading to inferior performance. Graph neural networks (GNN) were proposed as a new class of models to model graphs, inspired by the success of CNNs. There are two main methods to perform convolution on a graph: spectral methods and non-spectral methods. Spectral methods compute the graph Laplacian and perform filtering in the spectral domain, while non-spectral methods aim to approximate filters without computing the graph Laplacian for faster processing. Graph neural networks (GNN) utilize spectral and non-spectral methods for graph convolution. Spectral methods involve computing the graph Laplacian for filtering in the spectral domain, while non-spectral methods directly perform convolution in the graph domain. GraphSAGE introduces an inductive convolution kernel. Existing GNN models have discrete layers, limiting their ability to model continuous diffusion processes. The recently proposed Graph Ordinary Differential Equations (GODE) extend the Neural Ordinary Differential Equation (NODE) to model message propagation on graphs as an ODE. GODE allows for adaptive evaluation and overcomes the limitations of existing GNN models with discrete layers. In this work, the authors highlight the limitations of Neural Ordinary Differential Equations (NODEs) in image classification tasks compared to discrete-layer models. They attribute this to errors in gradient estimation during training and propose a memory-efficient framework for accurate gradient estimation. The authors propose a memory-efficient framework for accurate gradient estimation in Neural Ordinary Differential Equations (NODEs), addressing errors in gradient estimation during training. Their method significantly improves performance on benchmark classification tasks, reducing test error from 19% to 5% on CIFAR10. The framework is memory-efficient for free-form ODEs and achieves constant memory usage with restricted-form invertible blocks. Generalization of ODE to graph data is done with GODE models, showing improved performance on various datasets and graph models. Previous efforts have viewed neural networks as differential equations, proposing new architectures based on numerical methods in ODE solvers. Several new architectures based on numerical methods in ODE solver have been proposed, including a stable architecture by Haber & Ruthotto (2017) and neural ordinary differential equation (NODE) by Grathwohl et al. (2018). The adjoint method, widely used in optimal control and geophysical problems, has also been applied to ODE. Dupont et al. (2019) introduced augmented neural ODEs to enhance the expressive capacity of NODEs. To improve the expressive capacity of neural ordinary differential equations (NODEs), Dupont et al. (2019) proposed augmented neural ODEs. However, none of the existing methods address the issue of inaccurate gradient estimation. Empirical performances of NODE in benchmark classification tasks are notably lower compared to discrete-layer models. Graph neural networks (GNNs) can be categorized into spectral and non-spectral methods, with spectral GNNs requiring information of the entire graph for filtering in the Fourier domain, while non-spectral GNNs focus on message aggregation around neighbor nodes, making them more localized and requiring less information. Spectral methods for graph convolution involve using the graph Laplacian, while non-spectral methods focus on message aggregation around neighbor nodes, requiring less computation. Various approaches have been developed to address the heavy computation burden and non-localized filters associated with spectral methods. Defferrard et al. (2016) and Kipf & Welling (2016) introduced localized spectral filters and first-order graph convolution, respectively, for efficient graph processing. Non-spectral methods focus on node neighbors for convolution operations. MoNet (Monti, 2017) utilizes a mix of CNNs to generalize convolution on graphs. In contrast to localized spectral filters and first-order graph convolution, non-spectral methods like MoNet, GraphSAGE, Graph attention networks, and GIN focus on defining convolution operations on graphs by considering different neighbors of a node. Invertible blocks are neural network blocks with a bijective mapping function, allowing accurate reconstruction of the input. Invertible blocks are neural network blocks with a bijective mapping function, allowing accurate reconstruction of the input. They have been used in normalizing flow and for memory-efficient network structures. Gomez et al. (2017) proposed using invertible blocks for memory-efficient network structures by discarding activation of middle layers. This approach allows reconstruction of each layer's activation from the next layer. When adding more layers with shared weights, the model transitions into a neural ordinary differential equation (NODE). When adding more layers with shared weights, the model transitions into a neural ordinary differential equation (NODE). In the continuous case, z(t) represents hidden states with f(\u00b7) as the derivative parameterized by a network. In the discrete case, x_k represents hidden states with each layer having its own function f_k. The forward pass of the model with discrete layers can be written as where K is the total number of layers, followed by an output layer applied on x_K. The forward pass of a neural ordinary differential equation (NODE) involves integrating states z over time T, with an output layer applied on z(T). Various ODE solvers can be used for integration, and the adjoint method is commonly used in optimal process control and functional analysis. The adjoint method, VODE solver, and Dopris Solver are commonly used in optimal process control and functional analysis. Model parameters denoted as \u03b8 are independent of time. The adjoint method involves solving the hidden state in reverse-time, denoted as h(t), compared to the forward-time solution z(t). The adjoint method involves solving the hidden state in reverse-time, denoted as h(t), compared to the forward-time solution z(t). In direct back-propagation through ODE solver, evaluation time points are saved during the forward pass and the computation graph is re-built during the backward pass to accurately reconstruct the hidden state and evaluate the gradient. In reverse-time integration, the hidden state h(ti) is accurately reconstructed to evaluate the gradient for optimization. The loss function L is minimized using gradient descent by solving Eq. 6 with any ODE solver. Storing z(t) during the forward pass requires large memory consumption, so Eq. 2 is solved in reverse-time to determine z(t). In summary, forward pass solves Eq. 2 forward in time, while backward pass involves reverse-time evaluation. In reverse-time integration, the hidden state h(ti) is accurately reconstructed to evaluate the gradient for optimization. The backward pass involves solving Eq. 6 in reverse-time, which may lead to inaccurate gradients due to the instability of the reverse-time ODE solver. In reverse-time integration, the hidden state h(ti) may not match the forward-time hidden state z(ti) due to the instability of the reverse-time ODE solver. This mismatch can lead to errors in the gradient calculation, as explained in Proposition 1 which states that stable ODEs have eigenvalues with a real part of 0. The reverse-time ODE solver instability can lead to errors in gradient calculation. When eigenvalues of the original system have non-zero real parts, either forward or reverse ODE is unstable. High |Re(\u03bb)| makes ODE sensitive to numerical errors. This affects solution accuracy and computed gradient. To address this, direct back-propagation through ODE solver is proposed. To address errors in reverse-time ODE solving, direct back-propagation through the ODE solver is proposed. This involves back-propagating through discretized numerical integration points to accurately reconstruct hidden states. This method ensures accurate gradient calculation and solution stability. Direct back-propagation through the ODE solver involves back-propagating through discretized numerical integration points to accurately reconstruct hidden states, ensuring accurate gradient calculation and solution stability. The adjoint for each step in discrete forward-time ODE solution can be defined, leading to a numerical discretization of the optimization perspective. Detailed derivations are provided in appendix E and F. Algorithm 1 outlines the process for accurate gradient estimation in ODE solvers for free-form functions. Algorithm 1 outlines the process for accurate gradient estimation in ODE solvers for free-form functions. Details of Eq. 6-7 can be found in appendix E and F. The algorithm defines a model with a free-form function f and integration time T. It involves selecting an initial step size, iterating through time points, and adjusting step size based on error estimates. During the forward pass, the solver integrates numerically with adaptive step sizes based on error estimation and outputs the integrated value and evaluation time points. Memory is saved by deleting middle activations. During the backward pass, the computation graph is rebuilt directly. The method is summarized in Algorithm 1, discussing its properties. During the backward pass, the solver rebuilds the computation graph by directly evaluating at saved time points without adaptive searching. The solver performs a numerical version of reverse-time integration to support free-form continuous dynamics, making the algorithm a generic method. Memory consumption analysis shows that the method consumes less memory compared to a naive solver. Our method reduces memory consumption by deleting middle activations during the forward pass and not searching for an optimal stepsize in the backward pass. Implementing a step-wise checkpoint method further reduces memory usage to O(Nf + Nt). The solver can handle free-form functions and invertible blocks without needing to store z(ti). The solver can handle free-form functions and invertible blocks, reducing memory consumption to O(Nf) by splitting input x into two parts of the same size. The solver can handle free-form functions and invertible blocks, reducing memory consumption by splitting input x into two parts of the same size. A bijective block is defined by differentiable neural networks F and G, with a bijective function \u03c8(\u03b1, \u03b2). If \u03c8(\u03b1, \u03b2) is bijective w.r.t \u03b1 when \u03b2 is given, the block is a bijective mapping. Different \u03c8 functions can be applied for different tasks, eliminating the need to store activations. Theorem 1 in appendix D allows for the application of different \u03c8 functions for various tasks. Storing activations is unnecessary as x can be accurately reconstructed from y, making it memory-efficient. Details for back-propagation without storing activations are provided in appendix B. Graph neural networks are introduced with discrete layers, extending to the continuous case with graph ordinary differential equations (GODE). GNNs are typically represented in a message passing scheme, as shown in Fig. 2 with nodes and edges. Each node is assigned a unique color for visualization ease. Current GNNs are represented in a message passing scheme, where each node is assigned a unique color for visualization. The model involves message passing from neighbor nodes to a specific node, with differentiable functions and operations like mean, max, or sum. The 3-stage model of GNN involves message passing, aggregation, and node state updates using neural networks and permutation invariant operations like mean and sum. The continuous-time GNN model, known as graph ordinary differential equation (GODE), replaces discrete-time message passing with a more nonlinear function, potentially outperforming its discrete counterparts. GODE's stability is linked to over-smoothing phenomena, with graph convolution being a form of Laplacian smoothing. Graph convolution is a special case of Laplacian smoothing, demonstrated by the over-smoothing phenomena. The continuous smoothing process involves eigenvalues of the Laplacian, leading to a stable ODE with non-zero eigenvalues. In experiments with a CNN-NODE on image classification tasks, if integration time T is large enough, all nodes will have similar features, leading to a drop in classification accuracy. Our method was evaluated on various image and graph classification tasks, including CIFAR10, CIFAR100, MUTAG, PROTEINS, IMDB-BINARY, REDDIT-BINARY, Cora, CiteSeer, and PubMed datasets. Raw datasets were used for graph classification tasks without pre-processing, and transductive inference was performed for node classification tasks. Xu et al. (2018) used raw datasets without pre-processing for node classification tasks, following a train-validation-test split by Kipf & Welling (2016). They modified ResNet18 for image classification tasks and applied GODE to graph neural networks by replacing a function in the model. GODE can be easily applied to various graph neural network architectures like GCN, GAT, ChebNet, and GIN for fair comparison in training. In a study comparing different graph neural network architectures like GCN, GAT, ChebNet, and GIN, GNNs were trained with varying depths of layers. The models used the same hyper-parameters, such as channel number, for fair comparison. For graph classification tasks, the channel number of hidden layers was set as 32 for all models, while for ChebNet, the number of hops was set as 16. For node classification tasks, the channel number was set as 16. For classification tasks, different GNN structures were experimented with varying hidden layer numbers (1, 2, 3). The comparison between adjoint method and direct back-propagation showed higher accuracy with direct back-propagation. CNN-NODE modified ResNet18 into NODE18 for classification tasks. Direct back-propagation outperformed the adjoint method for both CNN-NODE and graph networks, leading to higher accuracy in classification tasks. The training method reduced error rates significantly on image classification tasks, validating the instability of the adjoint method. Our training method outperforms the adjoint method on image classification tasks, reducing error rates significantly for NODE18 on CIFAR10 and CIFAR100. NODE18, with the same parameters as ResNet18, surpasses deeper networks like ResNet101 on both datasets. Additionally, our method shows consistent superiority on benchmark graph datasets compared to the adjoint method. During inference, using different solvers is equivalent to changing model depth without re-training the network. Our method is robust to different orders of ODE solvers and supports NODE and GODE models with free-form functions. Bijective blocks defined as Eq. 8 can be easily generalized to adapt to different tasks. The study demonstrates the effectiveness of bijective blocks in general neural networks for different tasks. Various \u03c8 functions were tested, with most GODE models outperforming their discrete-layer counterparts. The results validate the effectiveness of GODE in node classification tasks. Most GODE models outperformed discrete-layer models significantly, validating GODE's effectiveness; different \u03c8 functions behaved similarly on node classification tasks, indicating the importance of the continuous-time model. Lower memory cost was also validated. Graph classification tasks results are summarized in Table 4, with experiments on different structures like GCN, ChebNet, and GIN. Paired t-tests comparing GODE and discrete-layer models were performed. The study compared GODE models with discrete-layer models, showing GODE's superior performance. Integration time in NODE and GODE models was tested, with short times leading to lack of information and long times causing oversmoothing issues. GODE was proposed to address these issues and enable continuous modeling. The study introduces GODE to address oversmoothing in models due to long integration times. GODE enables continuous diffusion process modeling on graphs and improves gradient estimation for NODEs. It relates GNN oversmoothing to ODE asymptotic stability and achieves accuracy comparable to state-of-the-art discrete layer models. The paper addresses the fundamental problem of gradient estimation for NODE and improves accuracy on benchmark tasks to be comparable with state-of-the-art discrete layer models. Experiments are conducted on various datasets including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained and experiments are conducted. The paper discusses gradient estimation for NODE and enhances accuracy on benchmark tasks. Experiments are conducted for invertible blocks, with modifications to generalize bijective blocks and propose a parameter state checkpoint method. Pseudo code for forward and backward functions in PyTorch is provided. The pseudo code for forward and backward functions in PyTorch is provided for an invertible block in Algo. 2. Memory consumption is reduced by keeping only necessary outputs in the forward function and calculating gradients efficiently in the backward function. Memory efficiency of the bijective block is demonstrated through experiments with a GODE model. The memory efficiency of our bijective block was demonstrated by training a GODE model and comparing memory consumption with a memory-inefficient method. Results showed significant reduction in memory usage with our memory-efficient function compared to conventional backpropagation. The memory efficiency of the bijective block was demonstrated by comparing memory consumption between a memory-inefficient method and our memory-efficient version. Results showed a significant reduction in memory usage with our approach. The memory-efficient bijective blocks delete computation graphs generated by F and G return cache, y1, y2. For stable ODEs, Re(\u03bb i (J f )) = 0 \u2200i, where \u03bb i (J f ) is the ith eigenvalue of J f. The forward and reverse mappings of a bijective block are defined, with stability requiring non-positive real parts of eigenvalues of J. Theorem 1 states that a bijective mapping is proven by showing injectivity and surjectivity of the forward mapping. The forward mapping of a bijective block is proven by demonstrating injectivity and surjectivity. Injectivity is shown by proving that if Forward(x1, x2) = Forward(x3, x4), then x1 = x3 and x2 = x4. Surjectivity is demonstrated by constructing x1, x2 such that Forward(x1, x2) = [y1. The text discusses proving the bijectivity of a mapping by demonstrating injectivity and surjectivity. It also includes the derivation of gradients in a neural-ODE model and extends from continuous to discrete cases. The text discusses deriving the gradient of parameters in a neural-ODE model from an optimization perspective, extending from continuous to discrete cases. Parameters, input, target, predicted output, and loss functions are defined in the context of the continuous model following an ODE. The forward pass and loss function are defined for a differentiable function represented by a neural network. The training process is formulated as an optimization problem, using the Lagrangian Multiplier Method to solve it. The analysis considers one ODE block but can be extended to multiple blocks. The Lagrangian Multiplier Method is used to solve the optimization problem defined in Eq. 15. Starting from the KKT condition, the derivative with respect to \u03bb is derived using calculus of variation. By considering a perturbation on \u03bb(t), the conditions for Leibniz integral rule are checked, leading to the result that at optimal \u03bb(t), dL/d| = 0. The Lagrangian Multiplier Method is used to solve the optimization problem. At optimal \u03bb(t), the derivative of L with respect to | is 0. In discrete cases, the ODE condition transforms into a finite sum equation, corresponding to previous analyses."
}