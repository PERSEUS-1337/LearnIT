{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where the source and target domains may not share the same categories. This paper proposes augmenting Self-Ensembling with category-agnostic clusters in the target domain to address the challenge of aligning samples from unknown classes. The paper proposes Self-Ensembling with Category-agnostic Clusters (SE-CC) to enhance domain adaptation by incorporating category-agnostic clusters in the target domain. This novel architecture utilizes clustering information to provide domain-specific visual cues, improving generalization for closed-set and open-set scenarios. Clustering is used to obtain category-agnostic clusters in the target domain, ensuring the learnt representation preserves the underlying structure. SE-CC enhances the representation with mutual information maximization. Superior results are reported in domain adaptation experiments on Office and VisDA datasets. Convolutional Neural Networks (CNNs) have driven vision technologies to new state-of-the-arts by achieving superior results in domain adaptation experiments on Office and VisDA datasets. The challenge arises when large quantities of annotated data are required for model training, leading to the impracticality of manual labeling. One alternative is to recycle off-the-shelf knowledge/models from the source domain for new domain(s), but performance often drops significantly due to \"domain shift.\" One way to address the performance drop in domain adaptation is through unsupervised domain adaptation, which uses labeled source samples and unlabeled target samples to generalize a target model. Existing models often struggle with aligning data distributions between source and target domains, limiting their applicability to closed-set scenarios. In open-set domain adaptation, distinguishing unknown target samples from known ones is a challenge. Learning a hybrid network for both closed-set and open-set domain adaptation is crucial. Employing an additional binary can help alleviate the issue. Learning a hybrid network for closed-set and open-set domain adaptation involves using an additional binary classifier to distinguish known and unknown target samples. This helps in discarding unknown samples during adaptation, but the performance may be suboptimal when target sample distribution is diverse or semantic labels are ambiguous. In closed-set and open-set domain adaptation, a hybrid network uses clustering on unlabeled target samples to model diverse semantics of known and unknown classes. The category-agnostic clusters convey discriminative knowledge specific to the target domain, aiding in domain-invariant representations for known classes. In closed-set and open-set domain adaptation, a hybrid network utilizes clustering on unlabeled target samples to capture diverse semantics of known and unknown classes. The category-agnostic clusters help in achieving domain-invariant representations for known classes in the target domain. To address issues, a new Self-Ensembling with Category-agnostic Clusters (SE-CC) approach is introduced, incorporating an additional clustering branch to refine representations and preserve the target domain's structure. The new Self-Ensembling with Category-agnostic Clusters (SE-CC) approach introduces clustering to decompose target samples into category-agnostic clusters. An additional clustering branch in the student model predicts cluster assignments using KL-divergence to model the mismatch between samples. The SE-CC framework incorporates clustering into the student model for predicting cluster assignment distribution of target samples. It utilizes KL-divergence to minimize mismatch and preserve data structure, while maximizing mutual information to enhance feature representation. The framework is jointly optimized for unsupervised domain adaptation. The SE-CC framework integrates clustering into the student model to predict cluster assignment distribution of target samples, enhancing feature representation. Unsupervised domain adaptation involves learning transferrable features in CNNs by minimizing domain discrepancy through Maximum Mean Discrepancy (MMD). Various approaches, such as integrating MMD into CNNs and incorporating a residual transfer module, have been explored in the literature. The curr_chunk discusses the incorporation of a residual transfer module in MMD-based adaptation of classifiers for unsupervised domain adaptation. It mentions the use of a domain discriminator to encourage domain confusion and enforce domain invariance in learned representations. Additionally, it touches on open-set domain adaptation. The curr_chunk discusses open-set domain adaptation, which goes beyond traditional domain adaptation by handling scenarios with new and unknown classes in the target domain. Various approaches, such as Panareda Busto & Gall (2017) and Saito et al. (2018b), are mentioned, including the use of adversarial training to learn feature representations. In open-set domain adaptation, different methods like adversarial training and factorization of source and target data have been used to handle scenarios with unknown classes in the target domain. In open-set domain adaptation, various methods have been employed to address scenarios with unknown classes in the target domain. A subspace is tailored to the target domain, with conditional entropy applied to x S t in the student pathway and self-ensembling loss used to align classification predictions between teacher and student models. Clustering is performed to decompose unlabeled target samples into category-agnostic clusters, integrated into Self-Ensembling for closed-set and open-set scenarios. An additional clustering branch in the student model infers assignment distribution over all clusters for each target sample x S t. In open-set domain adaptation, methods address scenarios with unknown classes in the target domain. A clustering branch in the student model infers assignment distribution over clusters for each target sample, aligning it with the original cluster distribution to preserve data structure. Feature representation is enhanced by maximizing mutual information among feature map, classification, and cluster assignment distributions. SE-CC utilizes unlabeled target samples for learning task-specific classifiers in the open-set scenario by leveraging category-agnostic clusters for representation learning. The learnt feature aims to preserve the target data structure during domain adaption, enabling effective alignment of sample distributions within known and unknown classes. In this paper, the preservation of target data structure during domain adaptation is achieved by utilizing category-agnostic clusters. This enables alignment of sample distributions within known and unknown classes, and discrimination of samples between known and unknown classes. The cluster probability distribution is further used to enhance representation learning by maximizing mutual information among input features, clusters, and class probability distributions. This approach is novel as it explores the advantages of category-agnostic clusters for open-set domain adaptation. In this paper, Self-Ensembling is adapted for closed-set and open-set scenarios by integrating category-agnostic clusters into domain adaptation. The SE-CC model overview is shown in Figure 2. Open-set domain adaptation involves labeled samples in the source domain and unlabeled samples in the target domain, with N classes including known and unknown classes. The goal of open-set domain adaptation is to learn domain-invariant representations and classifiers for recognizing the N \u2212 1 known classes in the target domain while distinguishing unknown target samples. Self-Ensembling, based on Mean Teacher, is used for semi-supervised learning with a student and teacher model. Self-Ensembling, a technique based on Mean Teacher, enhances semi-supervised learning by promoting consistent classification predictions between teacher and student models under small input perturbations. It aims to ensure similar classification probability distributions over all classes despite different augmentations applied to the target sample. The self-ensembling loss penalizes the difference between classification predictions of student and teacher models, aiming to ensure consistent distribution over all classes. The teacher's weights are updated as an exponential moving average of the student's weights during training. Additionally, the unsupervised conditional entropy loss is adopted to drive decision boundaries in the student's classification branch. The Self-Ensembling approach includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data. It aims to drive decision boundaries away from high-density regions in the target domain. The training loss is balanced with tradeoff parameters \u03bb1 and \u03bb2, making open-set domain adaptation more challenging than closed-set adaptation. Open-set domain adaptation is more challenging than closed-set adaptation as it involves classifying outliers into known and unknown classes. The typical approach of using a binary classifier oversimplifies the problem by assuming all unknown samples belong to one class. To address this issue, clustering is used to model the diverse semantics in the target domain explicitly. In open-set domain adaptation, clustering is used to model diverse semantics in the target domain explicitly. This involves creating category-agnostic clusters that are integrated into Self-Ensembling to guide domain adaptation. An additional clustering branch is designed in the student of Self-Ensembling to align cluster assignment distribution with inherent cluster distribution, making feature representations domain-invariant for known classes and more discriminative for unknown and known classes in the target domain. In open-set domain adaptation, clustering is utilized to create category-agnostic clusters in the target domain. K-means is used to decompose unlabeled target samples into clusters, revealing underlying structure tailored to the target domain. The obtained clusters are able to group target samples with similar semantics. In open-set domain adaptation, clustering is used to create category-agnostic clusters in the target domain. The clusters reveal the underlying structure tailored to the target domain, where target samples with similar semantics are grouped together. Target samples are represented as output features of pre-trained CNNs for clustering. The joint relations between each target sample and all clusters encode the underlying structure. Periodically refreshing the clusters did not have a significant impact. We encode the structure of target samples as joint relations with category-agnostic clusters. Each sample's inherent cluster distribution is measured through cosine similarities with cluster centroids. Clustering branch is an additional component. The temperature parameter \u03c1 scales the softmax function. The centroid \u00b5 k of each cluster is the average of samples in that cluster. The clustering branch in the student model predicts cluster assignments for target samples. It uses a modified softmax layer to infer cluster assignment distribution P clu (x S t ) over all clusters. The clustering branch in the student model predicts cluster assignments for target samples using a modified softmax layer to infer cluster assignment distribution. The KL-divergence loss measures the mismatch between the estimated cluster assignment distribution and the inherent cluster distribution. By minimizing this loss, the model learns the cluster assignments more accurately. The KL-divergence loss is used to measure the mismatch between estimated and inherent cluster distributions. By minimizing this loss, the model learns to preserve the data structure of the target domain and improve discriminative capabilities for both known and unknown classes. Additionally, inter-cluster relationships are incorporated into the loss as a constraint to maintain inherent cluster relations. The KL-divergence loss with inter-cluster relationships constraint is used in SE-CC to ensure similarity between semantically similar clusters. The student model produces classification and cluster assignment distributions for target samples in a multi-task paradigm. Mutual Information Maximization is leveraged to strengthen target features in an unsupervised manner. The student model utilizes Mutual Information Maximization (MIM) to enhance target features in an unsupervised manner by maximizing mutual information among input features and output distributions. This approach aims to tune the features for downstream tasks by estimating and maximizing local and global mutual information. The student model enhances target features through Mutual Information Maximization (MIM) by maximizing mutual information among input features and output distributions. The output feature map is encoded into a global feature vector, concatenated with classification and cluster assignment distributions, and fed into a global Mutual Information discriminator. The global Mutual Information discriminator is used to discriminate whether the input global feature vector aligns with the given classification and cluster assignment distributions. It is implemented with a stacked fully-connected network and nonlinear activation, producing an output score representing the probability of discriminating the real input feature with matched distributions. The global Mutual Information is estimated through this process. The global Mutual Information is estimated using a Jensen-Shannon MI estimator, with a softplus function and global feature of a different target image. Local Mutual Information is also utilized to analyze the input feature at each spatial location and output distributions. The distributions are spatially replicated and concatenated to construct feature maps. The local Mutual Information discriminator analyzes input features at each spatial location by replicating and concatenating distributions to construct feature maps. It consists of three stacked convolutional layers with a final output score map indicating the probability of matching input features with given distributions. The final output score map of the local Mutual Information discriminator represents the probability of discriminating real input local features with matched distributions. The objective for the MIM module combines local and global Mutual Information estimations with a tradeoff parameter \u03b1. The training objective of the SE-CC model integrates cross-entropy loss on the source data. The SE-CC model integrates various losses and Mutual Information estimation on target data with tradeoff parameters \u03bb3 and \u03bb4. Experimental verification is done on the Office Saenko et al. VisDA dataset for synthetic-real image transfer. The dataset for synthetic-real image transfer consists of 280k images from three domains: synthetic images from 3D CAD models for training, real images from COCO for validation, and video frames from YTBB for testing. The synthetic images are used as the source and COCO images as the target for evaluation in open-set adaptation. For open-set adaptation, 12 classes are known for both source and target domains, with 33 background classes as unknown in the source and 69 COCO categories as unknown in the target. The known-to-unknown ratio in the target domain is set at 1:10. Three metrics - Knwn, Mean, and Overall - are used for evaluation, representing accuracy over known classes, known & unknown classes, and all target samples. Closed-set adaptation focuses on accuracy of all 12 classes for adaptation. For closed-set adaptation, accuracy of all 12 classes is reported. ResNet152 is used as the backbone for clustering and adaptation in both closed-set and open-set scenarios. Performance of different models on Office for open-set adaptation is shown in Table 1. AODA adopts a different open-set setting without unknown source samples. A variant of SE-CC is included for fair comparison, learning classifier without unknown source samples. Our SE-CC variant (SE-CC \u2666) learns classifier without unknown source samples, achieving better performance than other closed-set and open-set adaptation models on most transfer directions. Our SE-CC model outperforms other closed-set and open-set adaptation models on most transfer directions, especially on harder transfers like D \u2192 A and W \u2192 A. It leverages category-agnostic clusters to create domain-invariant feature representations for known classes while effectively segregating target samples from known and unknown classes. By aligning data distributions between source and target domains, RTN and RevGrad perform better than Source-only. Open-set adaptation techniques (AODA, ATI-\u03bb, and FRODA) outperform RTN and RevGrad by rejecting unknown target samples as outliers and aligning data distributions for inliers. Excluding unknown target samples during domain adaptation in open-set scenarios is effective. Our SE-CC outperforms AODA, ATI-\u03bb, and FRODA in domain adaptation by injecting category-agnostic clusters for feature learning. Performance comparisons for closed-set domain adaptation on Office and VisDA datasets confirm the effectiveness of our approach. Our SE-CC outperforms other state-of-the-art closed-set adaptation techniques in domain adaptation by utilizing category-agnostic clusters for feature learning. The results on Office and VisDA datasets demonstrate the advantage of exploiting the underlying data structure in the target domain, even without diverse and ambiguous unknown samples. In an Ablation Study, the SE-CC design influences overall performance by incorporating Conditional Entropy (CE), KL-divergence Loss (KL), and Mutual Information Maximization (MIM) to refine features and drive classifier decisions away from high-density target data regions. These techniques enhance feature suitability for downstream tasks by aligning cluster assignment distributions and maximizing mutual information among input features. The SE-CC design incorporates Conditional Entropy (CE), KL-divergence Loss (KL), and Mutual Information Maximization (MIM) to enhance feature suitability for downstream tasks. CE improves Mean accuracy from 65.2% to 66.3%, while KL and MIM contribute 3.0% and 1.2% respectively in Mean metric performance gain on VisDA for open-set domain adaptation. SE-CC, a method for domain adaptation, utilizes category-agnostic clusters in the target domain to separate unknown samples from known ones. It incorporates Conditional Entropy (CE), KL-divergence Loss (KL), and Mutual Information Maximization (MIM) to improve Mean accuracy by 4.2% in total. The results validate the effectiveness of exploiting target data structure and mutual information maximization for open-set adaptation. SE-CC, a method for domain adaptation, utilizes category-agnostic clusters in the target domain to separate unknown samples from known ones. It integrates clustering to align cluster assignment distribution with inherent cluster distribution, preserving data structure in the target domain. SE-CC is a method for domain adaptation that utilizes category-agnostic clusters in the target domain to align cluster assignment distribution with inherent cluster distribution, preserving data structure. The mutual information among input features, classification outputs, and clustering branches is used to enhance the learned feature. Experiments on Office and VisDA show performance improvements compared to state-of-the-art techniques. The implementation is done with PyTorch and SGD optimization. The SE-CC method for domain adaptation uses global and local mutual information estimation frameworks. Implementation is in PyTorch with SGD optimization. Experiment settings include learning rate, mini-batch size, training iterations, and feature dimensions. Table 6 details cluster number, tradeoff parameters, and alpha values for adaptation tasks on different datasets. The tradeoff parameters \u03bb 1 , \u03bb 2 , \u03bb 3 , \u03bb 4 and \u03b1 are tuned for open-set and closed-set adaptation tasks using the Gap statistics method. \u03bb 1 = 10 is fixed based on previous studies, while the other parameters are adjusted within specific ranges. The evaluation focuses on how the loss function design in the clustering branch impacts performance, comparing KL-divergence in SE-CC with L 1. Evaluation of Clustering Branch: Comparing KL-divergence in SE-CC with L 1 and L 2 distance shows KL-divergence as a better measure of mismatch. Evaluation of Mutual Information Maximization: Different variants of MIM module in SE-CC estimate mutual information between input feature and outputs (CLS, CLU, CLS+CLU). The CLS, CLU, and CLS+CLU modules in SE-CC estimate mutual information between input features and different outputs. CLS and CLU slightly improve performance by utilizing mutual information between input features and individual branch outputs. CLS+CLU shows a larger performance boost by combining outputs from both branches for mutual information estimation. The CLS+CLU module in SE-CC demonstrates the merit of exploiting mutual information among input features and combined outputs of two downstream tasks. It shows a larger performance boost compared to Source-only without domain adaptation. However, SE may struggle to recognize unknown target samples with ambiguous semantics. SE-CC separates unknown target samples from known target samples by preserving the underlying data structure for both classes, making known samples indistinguishable in two domains."
}