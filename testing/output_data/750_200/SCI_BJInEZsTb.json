{
    "title": "BJInEZsTb",
    "content": "In this paper, the authors explore representation learning and generative modeling using three-dimensional geometric data in the form of point clouds. They propose a deep autoencoder network that outperforms existing methods in 3D recognition tasks and enables shape editing applications through algebraic manipulations. Additionally, they compare different generative models, including GANs and Gaussian mixture models, to improve the quality of generated point clouds. The study compares various generative models, such as GANs and Gaussian mixture models, for improving the quality of generated point clouds. GMMs trained in the latent space of autoencoders produce samples with the best fidelity and diversity. The study evaluates generative models for improving the quality of generated point clouds by proposing measures of fidelity and diversity based on matching point clouds. Different 3D representations like view-based projections, volumetric grids, and graphs are discussed, highlighting their effectiveness in various domains but limitations in semantics. Recent advances in deep learning offer a promising solution to the limitations of traditional high-dimensional representations in generating new objects. This includes the challenge of linking semantics to representations and the need for complex parametric models. Recent advances in deep learning have eliminated the need for hand-crafting features and models in domains with abundant data. Deep learning architectures like autoencoders and Generative Adversarial Networks are successful at learning complex data representations and generating realistic samples. In this paper, the focus is on point clouds as a 3D modality, which offer a compact representation of surface geometry and are commonly used in range-scanning acquisition pipelines. Deep architectures for 3D point clouds are essential for learning representations and generative models. Existing literature includes PointNet BID17 for classification and segmentation, BID14 for pipeline integration, and Fan et al. (2016) for extracting 3D information from 2D images. Generative models for point clouds have gained recent attention in the deep learning community. Generative models for point clouds have become a focus in deep learning, particularly with the introduction of GANs. Evaluating these models is challenging due to the difficulty in training them and the lack of a standardized evaluation method. Fidelity and coverage are key aspects in evaluating generative models, with coverage being especially important to ensure a representative sample of the data distribution. The text discusses the challenges in evaluating generative models for point clouds, focusing on fidelity and coverage. It introduces a new AE architecture inspired by recent classification architectures, capable of learning compact representations with excellent reconstruction quality. The learned representations are suitable for classification, interpolations, and improving on the state of the art. The text introduces a workflow for training generative models for point clouds. It proposes learning a compact representation using an AE with a bottleneck layer, followed by training a plain GAN in that fixed latent representation. This approach aims to generate point clouds similar to the training data and improve coverage of the dataset. The workflow involves training a GAN in a compact latent representation learned from an AE with a bottleneck layer. This method is supported by theory and shows superior performance in training and reconstruction compared to raw GANs. Multi-class GANs perform well when trained in the latent space, with GMMs achieving the best results overall. We find that multi-class GANs trained in the latent space perform almost as well as dedicated GANs per object category. Various metrics are evaluated for learning good representations and evaluating generated samples, including proposing fidelity and coverage metrics based on optimal matching between generated samples and a test set. The paper outlines evaluation metrics for generative models, introduces models for latent representations and point cloud generation, evaluates models quantitatively and qualitatively, and provides code availability. Autoencoders are deep architectures that aim to reproduce their input by compressing data points into a low-dimensional representation. Generative Adversarial Networks (GANs) are state-of-the-art models for generating data. The Decoder (D) reproduces x from its encoded version z in Generative Adversarial Networks (GANs), which are advanced generative models. GANs involve an adversarial game between a generator (G) and a discriminator (D) to create samples indistinguishable from real data. The discriminator distinguishes between synthesized and real samples using specific loss functions for the networks.\u03b8 (D) , \u03b8 (G) are the parameters for the discriminator and generator network. The improved Wasserstein GAN is used for the discriminator and generator networks, with parameters \u03b8 (D) and \u03b8 (G) respectively. Point clouds pose unique challenges for network architecture due to the lack of grid-like structure required by convolution operators. Point clouds present challenges for network architecture as they lack a grid-like structure needed for convolution operators. Recent classification work on point clouds bypasses this issue by avoiding 2D convolutions. Additionally, point clouds are unordered, making comparisons between point sets difficult and requiring feature permutation invariance. Point clouds are unordered, requiring feature permutation invariance for comparing point sets. Two metrics proposed for this are Earth Mover's distance (EMD) and Chamfer distance (CD). EMD transforms one set to another through a transportation problem, while CD measures the distance between two subsets. The Earth Mover's distance (EMD) and Chamfer distance (CD) are metrics used to compare point sets. EMD involves transforming one set to another through a transportation problem, while CD measures the distance between two subsets. EMD is differentiable almost everywhere and compares sets using a bijection, while CD is more computationally efficient and measures the squared distance between each point in one set to its nearest neighbor in the other set. These metrics are essential for evaluating representations and generative models. To evaluate the quality of representation models or generative models, metrics like Earth Mover's distance (EMD) and Chamfer distance (CD) are used to compare point sets. Coverage metrics such as COV-CD and COV-EMD measure how well a point-cloud distribution matches a ground truth distribution by finding the closest neighbors between the sets. This evaluation helps assess faithfulness, diversity, and potential mode-collapse in the models. Coverage metrics, COV-CD and COV-EMD, measure the fraction of matched point-clouds in G to A, indicating how well G is represented within A. Fidelity is captured by matching each point cloud in G to the closest one in A using minimum distance (MMD), resulting in MMD-CD and MMD-EMD. MMD measures distances in pairwise matchings, reflecting the realism of elements in A. The Jensen-Shannon Divergence (JSD) measures the similarity between point clouds A and B in a 3D space by counting points within each voxel and comparing empirical distributions. The architectures of representation and generative models for point clouds are described, starting with an autoencoder design. A GAN architecture tailored to point-cloud data is introduced, along with a more efficient pipeline that learns an AE and then trains a smaller GAN in the latent space. Additionally, a simpler generative model based on Gaussian Mixtures is discussed. The input to the AE network is a point cloud with 2048 points, representing a 3D shape. The encoder architecture follows the principle of 1-D convolutional design. The AE network input is a 2048-point point cloud representing a 3D shape. The encoder uses 1-D convolutional layers with increasing features and a symmetric function. It encodes points independently and uses a permutation-invariant function. The decoder transforms the latent vector with fully connected layers. The decoder transforms the latent vector with 3 fully connected layers to produce a 2048 \u00d7 3 output. For a permutation invariant objective, two distinct AE models, AE-EMD and AE-CD, are explored using efficient EMD-distance approximation and Chamfer-Distance as structural losses. The study constructed 8 AEs with bottleneck sizes ranging from 4 to 512 and trained them on point-clouds of a single object class. After multiple iterations, a bottleneck size of 128 showed the best generalization error on test data. The raw point cloud GAN was introduced as the first GAN for point clouds, with a discriminator architecture identical to the AE. The study introduced a GAN for point clouds, with a discriminator architecture similar to the AE but without batch-norm and with leaky ReLUs. The generator maps a 128-dimensional noise vector to a 2048 \u00d7 3 output. In the latent-space GAN (l-GAN), data is passed through a pre-trained autoencoder trained separately for each object class. In the latent-space GAN (l-GAN), data is passed through a pre-trained autoencoder trained separately for each object class with the EMD (or Chamfer) loss function. The generator and discriminator operate on the 128-dimensional bottleneck variable of the AE, producing realistic results with shallow designs. In the latent-space GAN (l-GAN), shallow designs for both the generator and discriminator are sufficient to produce realistic results. Gaussian Mixture Models (GMMs) are also trained on the latent spaces learned by autoencoders, which can be turned into point-cloud generators. The text discusses using autoencoders to generate shapes from the latent-space GMM distribution, similar to l-GANs. Shapes are reconstructed using class-specific AEs trained on ShapeNet data, with models split into training/testing/validation sets. Classification is used to evaluate unsupervised representation learning algorithms. The autoencoder was trained on ShapeNet data with models split into training/testing/validation sets. The performance of the latent features computed by the autoencoder was evaluated using linear models on supervised datasets. The experiment included using a bigger bottleneck of 512 and applying batch-norm to the decoder. For this experiment, a bigger bottleneck of 512 was used in the autoencoder, along with an increased number of neurons and batch-norm applied to the decoder. Features for input 3D shapes were obtained by feeding the point-cloud to the network and extracting a 512-dimensional bottleneck layer vector. A linear classification SVM trained on ModelNet BID32 was used to process this feature, resulting in a more intuitive and parsimonious 512-dimensional feature compared to the previous state of the art. The decoupling of latent representation from generation in the GAN allows for flexibility in choosing the AE loss, affecting the learned feature. On ModelNet10, EMD and CD losses perform similarly for larger objects with fewer categories, but CD produces better results with increased variation in the collection due to its ability to understand rough edges and high frequency geometric details. This experiment also demonstrates that the AEs were not trained on ModelNet. The experiment demonstrates the domain-robustness of learned features. Qualitative evaluation shows the ability of the learned representation to generalize to unseen shapes through reconstruction results using AEs. Our learned representation demonstrates the ability to generalize to unseen shapes through reconstruction and enables shape editing applications such as interpolations, part editing, and analogies. The generalization ability is highlighted in quantitative measurements and comparable reconstruction quality on training vs. test splits. The study compares five generative models trained on chair point-cloud data, including two AEs with 128-dimensional bottleneck trained with CD or EMD loss (AE-CD and AE-EMD). An l-GAN is trained in each AE's latent space, with additional models trained in the AE-EMD space. In the validation dataset, the training of various generative models, including l-GANs and GMMs, was conducted on chair point-cloud data. The models were trained for a maximum of 2000 epochs, with the final model selected based on how well the synthetic results matched the ground-truth distribution. The GAN was used to generate synthetic point clouds and compare them to the validation set using JSD or MMD-CD metrics. Model selection was done every 100 epochs, with criteria shown in TAB10. Gaussian components for GMM were also selected based on the same criterion. The various models selected using the JSD criterion are shown in TAB10. GMMs performed better with full covariance matrices, suggesting strong correlations between latent dimensions. When using MMD-CD as the selection criterion, models of similar quality were obtained with an optimal number of 40 Gaussians. Table 2 evaluates 5 generators on the train-split of the chair dataset based on epochs/models selected via minimal JSD on the validation-split. Table 2 evaluates 5 generators on the train-split of the chair dataset based on epochs/models selected via minimal JSD on the validation-split. The average classification score attained by the ground-truth point clouds was 84.7%. The models are compared based on their capacity to generate synthetic samples resembling the ground truth distribution. The study evaluates 5 generators on the train-split of the chair dataset by generating synthetic point clouds and comparing them to the ground truth distribution. The average classification probability for samples recognized as chairs is also measured. Additionally, a similar experiment is conducted on the test-split using models selected based on minimal JSD on the validation-split. Training a simple Gaussian mixture model in the latent space of the EMD-based AE yields the best results in terms of fidelity and coverage on the test-split dataset of the chair dataset. The experiment was repeated with three pseudo-random seeds, and the average measurements for various comparison metrics are reported in TAB10. The EMD-based AE's latent space combined with Gaussian mixture models (GMMs) produces high fidelity and coverage results. The AE-EMD achieved an MMD-EMD of 0.05 compared to the ground truth training data, similar to the GMMs' MMD-EMD value of 0.06 on test data. The models show good generalization ability, with comparable performance on training vs. testing splits. The generalization ability of the models is established by comparing their performance on training vs. testing splits. Synthetic datasets are generated for the test split experiment, as well as for validation split comparisons, to reduce sampling bias. The ground truth dataset is used for testing and validation to reduce sampling bias when measuring MMD or Coverage statistics. The MMD-CD distance to the test set is relatively small for r-GANs, but qualitative inspection shows otherwise due to the inadequacy of the chamfer distance in distinguishing pathological cases. In pathological cases, examples of behavior are shown using r-GAN and l-GAN to generate synthetic point clouds. Nearest neighbors in synthetic sets are found using chamfer distance, with distances to ground truth reported using CD and EMD. The distances between nearest neighbors in synthetic sets generated by r-GAN and l-GAN are reported using CD and EMD. r-GAN results are of lesser quality due to the concentration of points in specific areas, leading to small CD values and partial matches being overlooked. The CD metric is blind to partial matches between shapes, resulting in a smaller value for r-GAN results. EMD promotes one-to-one mapping, correlating more strongly with visual quality and penalizing r-GAN in terms of MMD and coverage. Extensive measurements during training show the behavior of models, with JSD distance plotted in FIG2. The JSD distance, EMD-based MMD, and Coverage were measured during training to understand model behavior. r-GAN struggles to provide good coverage of the test set, while l-GAN (AE-CD) performs better in terms of visual quality. The l-GAN (AE-CD) performs better in terms of visual quality with much fewer epochs, but its coverage remains low due to the CD promoting unnatural topologies. Switching to an EMD-based AE (l-GAN, AE-EMD) dramatically improves coverage and fidelity, although both l-GANs still suffer from mode collapse. Switching to a latent WGAN largely eliminates mode collapse in GANs on point-cloud data, improving coverage and fidelity. Comparisons to voxel-based methods show promising results. The authors propose GANs on point-cloud data and compare their models to a voxel-grid based approach in terms of JSD on the training set of the chair category. The r-GAN and l-GAN models outperform BID31 in terms of diversity and classification scores on synthetic results. l-GANs perform even better with less training time compared to r-GAN due to their smaller architecture. The l-GAN outperforms the r-GAN in training time and produces high-quality synthetic results, showcasing the strength of its learned representation. The 32-component GMM also performs well when trained on the AE-EMD latent space. The l-GAN outperforms the r-GAN in training time and produces high-quality synthetic results, showcasing the strength of its learned representation. The 32-component GMM also performs well when trained on the AE-EMD latent space, with results displayed in Figure 5. Extensions to multiple classes were explored by training an AE-EMD on a mixed set of point clouds from 5 categories. The multi-class AE had a bottleneck size of 128 and was trained for 1000 epochs, compared to class-specific AEs trained for 500 epochs. The AE model was selected based on minimal reconstruction error. The study compared class-specific autoencoders (AEs) with a 85-5-10 train-val-test-split, trained for 500 epochs. Additionally, six l-WGANs were trained for 2K epochs on top of the AEs. Results showed that l-WGANs based on multi-class AEs performed similarly to dedicated class-specific ones. Visual quality was not sacrificed when using a multi-class AE-EMD. The study compared class-specific autoencoders with a 85-5-10 train-val-test-split, trained for 500 epochs. l-WGANs based on multi-class AEs performed similarly to dedicated class-specific ones in terms of visual quality. Limitations included failure cases in decoding rare geometries and missing high-frequency geometric details. Some failure cases of the models include chairs with rare geometries not being faithfully decoded and missing high-frequency geometric details like a hole in the back of a chair, altering the input shape's style. The r-GAN struggles to create realistic shapes for certain classes, particularly cars. Designing more robust raw-GANs for point clouds is an interesting avenue for future work. Training Gaussian mixture models (GMM) in the latent space of an autoencoder is closely related to VAEs. One issue with VAEs is over-regularization, which can impact reconstruction quality. Methods exist to gradually increase the regularizer weight. Fixing the AE before training generative models yields good results, with novel architectures for 3D point-cloud representation learning and generation. Our novel architectures for 3D point-cloud representation learning and generation show good generalization to unseen data, encoding meaningful semantics. The best-performing generative model in our experiments is a GMM trained in the fixed latent space of an AE, suggesting that simple classic tools should not be dismissed. The AE mentioned in Section 4.1 had specific architecture details with filters in each layer, batch normalization, and data augmentation. It was trained for 1000 epochs. The AE architecture in Section 4.1 included filters in each layer, batch normalization, and data augmentation. It was trained for 1000 epochs with CD and 1100 epochs with EMD loss. The encoder had specific filter sizes, and the decoder consisted of 3 FC-ReLU layers. Different AE setups did not show significant advantages over the \"vanilla\" architecture. The experiment involving 5 shape classes showed that different AE setups did not provide noticeable advantages over the \"vanilla\" architecture. Adding drop-out layers resulted in worse reconstructions, while using batch-norm on the encoder only sped up training and improved generalization error with single-class data. The discriminator's architecture included 1D-convolutions with specific filters and leaky-ReLU, followed by a featurewise max-pool. The generator consisted of 5 layers. The r-GAN model used in the experiment consists of a generator with 5 FC-ReLU layers and a discriminator with 2 FC-ReLU layers. The generator has neurons in the range of {64, 128, 512, 1024, 2048 \u00d7 3}, while the discriminator has neurons in the range of {256, 512}. The training was done with Adam optimizer, a noise vector of 128 dimensions, and a leak of 0.2 units. The generator in the experiment consists of 2 FC-ReLUs with {128, k = 128} neurons each, while the discriminator has 2 FC-ReLU layers. The training parameters and optimization details can be found in Table 5. For classification experiments, a linear SVM classifier with an l2 norm penalty and balanced class weights was used. The training parameters for the SVMs used in the experiment are detailed in Table 5, including the structural loss models for ModelNet40 and ModelNet10 datasets. The parameters include C-penalty, intercept loss, and optimization functions. The reconstruction quality of the autoencoders (CD and EMD-based) is also compared in terms of JSD. The reconstruction quality of the autoencoders (CD and EMD-based) is comparable between training and test datasets, showcasing their ability to generalize across different shapes. The embedding learned with AE-EMD trained across all 55 object classes enables interesting applications involving various shapes. The AE latent space allows for editing parts in point clouds to modify shapes based on shape annotations. Objects can be subdivided into categories with different structural properties, enabling shape modifications. The AE latent space enables editing parts in point clouds to modify shapes based on shape annotations. Objects can be categorized into A and B, with A possessing specific structural properties. The structural difference between A and B can be modeled using the average latent representations. Transforming the latent representation of an object in A can change its properties, as shown in Figure 8. Transforming the latent representation of object A in A can change its properties, allowing for morphing between shapes of different appearances. This process is illustrated in Figure 8, showing how interpolating shapes can produce intermediate variants. The latent representation supports removing and merging shape parts, enabling morphing between shapes. Our latent representation allows for morphing between shapes of different classes, such as a bench and a sofa. Shape analogies are demonstrated by finding \"analogous\" shapes through linear manipulations and nearest-neighbor searching in the latent space. In this section, we demonstrate finding shape analogies by manipulating the latent space and using nearest-neighbor search. Preliminary results show point-cloud generators working with voxel-based autoencoders and occupancy grids for 3D shape generation using a full-GMM model. In this study, the latent space was learned using an AE with occupancy grids of 3D shapes instead of a point-cloud autoencoder. Generation was done with a full-GMM model with 32 centers on ShapeNet's chair class, comparing against voxel-based GANs and converting voxel-grids into 2048 points for evaluation. The study compared latent AE-based GMM models with Wu et al.'s voxel-based GANs, showing superior performance of the former. The use of latent representation provided a significant improvement over the \"raw\" voxel GAN architecture. The study compared latent AE-based GMM models with voxel-based GANs, showing superior performance of the former. The voxel-based GMM at 64 3 resolution performed comparably to the one at 32 3 resolution, indicating fidelity is not solely dependent on high-frequency details. Point-cloud-based models outperformed voxel-based models in fidelity, as measured by MMD. The voxel-based latent-space models had a larger coverage boost compared to MMD. The voxel-based latent-space models show a bigger coverage boost compared to MMD due to how the coverage metric is computed, matching all generated shapes against the ground truth regardless of quality. This artificially increases coverage as even partial instances are matched to an arbitrary ground truth model. The voxel-based method artificially increases coverage by matching poor quality partial shapes to ground truth models. The voxel-based method uses GMMs with full covariances and different dimensional latent codes for resolutions 32^3, 64^3, and 128^3. Mesh conversion is done using the marching cubes algorithm with an iso-surface value of 0.5. Voxel-based AEs are fully-convolutional with specific encoder and decoder layers. The fully-convolutional voxel-based autoencoders use specific encoder and decoder layers with parameters listed for each layer. The model is trained for 100 epochs with Adam optimizer and binary cross-entropy loss. The voxel-based autoencoders were trained for 100 epochs using Adam optimizer and binary cross-entropy loss. The models were compared in terms of reconstruction quality to the state-of-the-art method BID28 on the ShapeNetCars dataset. Reconstruction quality was measured by the intersection-over-union between input and synthesized voxel grids. The quality of the GMM-generator is evaluated by comparing it to a model that memorizes the training data of the chair class. Different sizes of training sets are considered, and the coverage/fidelity of the generative models is slightly lower when memorizing the training set. The generative models show slightly lower coverage/fidelity compared to memorizing the training set, indicating the validity of the metrics. Learning a representation allows for compactly representing data and generating novel shapes through interpolations, despite some mode collapse. The generative models exhibit some mode collapse, resulting in a 10% drop in coverage, but maintain excellent fidelity with almost identical MMD to the memorization case. Additional comparisons with BID32 for ShapeNet classes are provided in Tables 10, 11, and 12, showcasing JSD-based and MMD/Coverage comparisons. In TAB12, MMD/Coverage comparisons on the test split are provided following the same protocol as in Table 10. Generalization error of various GAN models is shown in FIG0, with measurements using JSD and MMD-CD metrics. GMM model selection is depicted in Figure 17. The GMM model selection in Figure 17 shows that models with full covariance achieve smaller JSD than those with diagonal covariance. A minimum of 30 clusters with full covariance is needed for minimal JSD. The 32 centers of the GMM fitted to the latent codes are depicted in Figure 18. In a typical covariance matrix of a Gaussian component, strong off-diagonal components are shown in pseudocolor. Evaluation of five generators on test-split of chair data using minimal MMD-CD for model selection. GMM-40-F represents a GMM with 40 Gaussian components with full covariances. Synthetic point-clouds are sampled for each model. The GMM-40-F model is evaluated using synthetic point-clouds to measure how well it matches the ground truth in terms of MMD-CD, complementing a previous evaluation measure shown in FIG2."
}