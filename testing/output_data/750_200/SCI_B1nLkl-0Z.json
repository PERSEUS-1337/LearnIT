{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions, such as Q-values, are commonly used in reinforcement learning algorithms like SARSA and Q-learning. A new concept of action value is proposed, defined by a Gaussian smoothed version of the expected Q-value in SARSA. Smoothed Q-values still satisfy a Bellman equation and can be learned from experience. Gradients of expected reward can be obtained from the gradient and Hessian of the smoothed Q-value function, allowing for the development of new algorithms to train a Gaussian policy directly from a learned Q-value. Gradients and Hessians of the smoothed Q-value function are used to develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. This approach allows for learning both mean and covariance during training, leading to strong results on continuous control benchmarks. Learning algorithms involve policy evaluation and policy improvement processes. Different notions of Q-value lead to distinct families of RL methods, such as SARSA and Q-learning. Soft Q-learning and PCL are also important methods in this field. In this work, a new notion of action value called the smoothed action value function Q \u03c0 is introduced. Unlike previous Q-value functions, this new concept associates a value with a specific action at each state, impacting the resulting algorithm by influencing the types of policies that can be expressed and the type of exploration that can be applied. In this work, a new notion of action value called the smoothed action value function Q \u03c0 is introduced. The smoothed Q-value associates a value with a distribution over actions, defined as the expected return of taking an action sampled from a normal distribution centered at a, then following actions from the current policy. Smoothed Q-values have properties that make them attractive for use. Smoothed Q-values have properties that make them attractive for use in RL algorithms, such as satisfying single-step Bellman consistency and allowing for training a function approximator using bootstrapping. Additionally, for Gaussian policies, the optimization objective can be expressed in terms of smoothed Q-values, and the gradient of this objective with respect to policy parameters is equivalent to the gradient and Hessian of the smoothed Q-value function. This allows for deriving updates to the policy parameters effectively. The Smoothie algorithm utilizes derivatives of a trained (smoothed) Q-value function to update policy parameters, avoiding high variance in standard policy gradient algorithms like DDPG. Unlike DDPG, Smoothie can improve exploratory behavior in reinforcement learning. Smoothie algorithm utilizes a non-deterministic Gaussian policy with mean and covariance for exploratory behavior in reinforcement learning. It can incorporate proximal policy optimization techniques by adding a penalty on KL-divergence, which is not feasible in standard DDPG algorithm. Our formulation includes a KL-penalty to improve stability and performance in the standard DDPG algorithm for continuous control benchmarks. Results are competitive with state-of-the-art, especially for challenging tasks in the low-data regime in model-free RL framework. The goal is to find an agent that achieves maximal cumulative discounted reward in a Markov decision process (MDP) with state space S and action space A. The agent interacts with the environment by observing the state, emitting an action, and receiving a reward feedback. The agent's behavior is modeled using a stochastic policy \u03c0 that produces a distribution over feasible actions at each state. The optimization objective is the expected discounted return. The agent uses a stochastic policy \u03c0 to produce a distribution over feasible actions at each state. The optimization objective is the expected discounted return, expressed in terms of the expected action value function Q \u03c0 (s, a). The policy gradient theorem expresses the gradient of O ER (\u03c0 \u03b8 ) w.r.t. \u03b8. The policy gradient theorem BID23 expresses the gradient of O ER (\u03c0 \u03b8 ) w.r.t. \u03b8 for reinforcement learning algorithms, focusing on multivariate Gaussian policies. In this paper, the focus is on multivariate Gaussian policies for continuous action spaces in reinforcement learning. The policies are parametrized by mean and covariance functions, mapping the observed state to a Gaussian distribution. New RL training methods are developed for these parametric policies, with potential generalization to other policy families. The paper focuses on multivariate Gaussian policies for continuous action spaces in reinforcement learning. New RL training methods are developed for parametric policies, with potential generalization to other policy families. The formulation reviews prior work on learning Gaussian policies and introduces the deterministic policy gradient for Gaussian policies with a policy covariance approaching zero. The key observation of BID21 is that under a deterministic policy, one can estimate the expected future return from a state. The gradient of the optimization objective for a parameterized policy can be expressed, characterizing the policy gradient theorem for deterministic policies. The Bellman equation can be re-expressed in the limit of a small policy covariance. A value function approximator can be optimized by minimizing the Bellman error for transitions sampled from interactions with the environment. Algorithms like DDPG improve the value function and policy by minimizing the Bellman error for transitions sampled from interactions with the environment. To enhance sample efficiency, off-policy distributions are used based on a replay buffer. This substitution may alter the policy gradient identity but has been found to work well in practice. In this paper, smoothed action value functions are introduced, with gradients optimizing Gaussian policy parameters. Smoothed Q-values, denoted Q \u03c0 (s, a), differ from ordinary Q-values by not assuming the first action is fully specified, but rather only the mean is known. Expectation of Q \u03c0 (s, \u00e3) for actions \u00e3 drawn near a is required to compute Q \u03c0 (s, a). Smoothed action values are defined as Q \u03c0 (s, a) where only the mean of the distribution of the first action is known. To compute Q \u03c0 (s, a), an expectation of Q \u03c0 (s, \u00e3) for actions \u00e3 drawn near a is necessary. This approach directly learns a function approximator for Q \u03c0 (s, a) instead of learning a function approximator and then drawing samples to approximate the expectation. By directly learning a function approximator for Q \u03c0 (s, a), one can bootstrap smoothed Q-values using a Gaussian policy \u03c0 \u2261 (\u00b5, \u03a3). Derivatives of Q \u03c0 can be used to learn \u00b5 and \u03a3, with a one-step Bellman equation enabling direct optimization of Q \u03c0. The text discusses how derivatives of Q \u03c0 can be used to learn the mean and covariance parameters of a Gaussian policy \u03c0 \u03b8,\u03c6. The gradient of the objective w.r.t. mean parameters follows from the policy gradient theorem, while estimating the derivative w.r.t. covariance parameters requires computing the second derivative of Q \u03c0 w.r.t. actions. The second derivative of Q \u03c0 w.r.t. actions can be used to compute the derivative of Q \u03c0 w.r.t. \u03a3. The full derivative w.r.t. \u03c6 can be optimized using samples, achieving a fixed point when Q \u03c0 w satisfies the Bellman equation recursion. The training procedure achieves a fixed point when Q \u03c0 w (s, a) satisfies the Bellman equation. A single function approximator is used for Q \u03c0 w (s, a), optimizing it by minimizing a weighted Bellman error. Sampling from a replay buffer with knowledge of the sampling probability q(\u00e3 | s) is involved in this approach. The training procedure reaches an optimum by minimizing a weighted Bellman error. It is unnecessary to track probabilities q(\u00e3 | s) as the replay buffer provides a near-uniform distribution of actions conditioned on states. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods to constrain gradient steps within a trust region. Recent work has shown benefits from ignoring or damping importance weights, but saving the probability of sampled actions along with their transitions can provide access to q(\u00e3 | s) \u2248 N (\u00e3 | \u00b5 old (s), \u03a3 old (s)). The development of trust region methods aims to stabilize policy gradient algorithms in continuous control problems by constraining gradient steps within a trust region. These methods have not been applicable to algorithms like DDPG due to deterministic policies, but a proposed formulation in this paper allows for trust region optimization by augmenting the objective with a penalty on KL-divergence from a previous policy parameterization. The paper proposes a method to stabilize policy gradient algorithms in continuous control problems by adding a penalty on KL-divergence from a previous policy parameterization. This method is a generalization of deterministic policy gradient methods that use Q-value functions to train a policy. The proposed method generalizes deterministic policy gradient methods by adding a penalty on KL-divergence from a previous policy parameterization. It can be seen as an extension of the deterministic policy gradient, with updates for training the Q-value approximator and policy mean being identical to DDPG. The method differs from SVG by providing updates for the covariance and estimating the gradient for the mean update. The proposed method extends deterministic policy gradient methods by penalizing KL-divergence from a previous policy parameterization. It differs from SVG by updating the covariance and estimating the gradient for the mean update. Covariance updates are crucial for trust region and proximal policy techniques. The proposed method extends deterministic policy gradient methods by penalizing KL-divergence from a previous policy parameterization. It introduces expected policy gradients (EPG) for updating the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. This approach avoids approximating integrals, simplifying the updates compared to previous methods. The proposed method extends deterministic policy gradient methods by penalizing KL-divergence from a previous policy parameterization. It introduces expected policy gradients (EPG) for updating the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. This approach avoids approximating integrals, simplifying the updates compared to previous methods. The novel training scheme for learning the covariance of a Gaussian policy relies on properties of Gaussian integrals. The paper introduces a new RL algorithm, Smoothie, which extends deterministic policy gradient methods by penalizing KL-divergence from a previous policy parameterization. It utilizes expected policy gradients (EPG) to update the mean and covariance of a stochastic Gaussian policy based on gradients of an estimated Q-value function. This approach simplifies updates compared to previous methods and focuses on the averaged return of a distribution of actions rather than a single action. Further investigation could explore applying this perspective to a wider class of policy distributions. The paper introduces Smoothie, a new RL algorithm that updates a Gaussian policy using insights from Section 3. It maintains a parameterized Q function and uses gradients and Hessians for training. See Algorithm 1 for pseudocode. Smoothie is a new RL algorithm that updates a Gaussian policy using gradients and Hessians for training. It is evaluated against DDPG on a synthetic task to study its behavior. Smoothie, a new RL algorithm, is evaluated against DDPG on a synthetic task to study its behavior. The task involves a one-shot environment with a reward function as a mixture of two Gaussians. Smoothie learns both the mean and variance of the policy, while DDPG only learns the mean. Smoothie learns both the mean and variance, while DDPG only learns the mean. DDPG struggles to escape local optima due to limitations in updating the mean, while Smoothie successfully solves the task. Smoothie successfully solves the task by adjusting the covariance during training, leading the policy mean towards a better Gaussian. Initially, the covariance decreases and then increases before decreasing again as the mean approaches the global optimum. Smoothie adjusts policy variance during training to escape lower-reward local optima. It increases and decreases variance as reward function changes. Standard continuous control benchmarks on OpenAI Gym BID3 using MuJoCo environment are also explored. The text discusses the implementation of Smoothie in continuous control benchmarks on OpenAI Gym using the MuJoCo environment. It compares Smoothie with DDPG and highlights competitive performance even with hyperparameter-tuned noise in DDPG. Smoothie shows competitive performance with DDPG, even outperforming in tasks like Hopper, Walker2d, and Humanoid. TRPO is not as sample-efficient. Results compared in FIG2 after hyperparameter search. Smoothie and DDPG were compared in FIG2 after a hyperparameter search. Despite DDPG having an advantage in exploration through hyperparameter search, Smoothie performed competitively or better across all tasks, with a slight advantage in Swimmer. Smoothie outperforms DDPG in various tasks, showing significant improvements in Hopper, Walker2d, and Humanoid. The results for Humanoid are the best published for a method training on millions of environment steps, while TRPO requires tens of millions of steps for comparable performance. Smoothie, a new algorithm, demonstrates improved performance compared to TRPO, which requires tens of millions of environment steps for similar results. The introduction of a KL-penalty in Smoothie enhances performance, especially on challenging tasks, providing a solution to the instability in DDPG training. Our algorithm, Smoothie, addresses the instability in DDPG training by introducing a KL-penalty for stability. It outperforms TRPO with improved performance on challenging tasks like Hopper and Humanoid, without sacrificing sample efficiency. Smoothie utilizes a new Q-value function, Q \u03c0, which has a Gaussian-smoothed version that enhances learning by establishing a relationship between the gradient of expected reward and the policy's mean and covariance. Smoothie algorithm improves DDPG training stability with a KL-penalty, outperforming TRPO on challenging tasks. It introduces Q \u03c0, a smoothed Q-value function that enhances learning by relating expected reward gradient to policy mean and covariance. The smoothed Q-values make the reward surface smoother and have a direct link to the discounted return objective. Future work should explore this relationship further. The smoothed Q-values in Q \u03c0 improve training stability by making the reward surface smoother and have a direct link to the discounted return objective. Future work should investigate these claims further and explore applying Q \u03c0 to other policy types. In standard matrix calculus, we use the fact that for symmetric A, \u2202 \u2202A ||v||. Omitting s from \u03a3(s) for succinctness, the LHS of the equation is tackled. To address the RHS, we note that DISPLAYFORM2, leading to DISPLAYFORM3."
}