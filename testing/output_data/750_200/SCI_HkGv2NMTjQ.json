{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as domain knowledge representation. However, ontology information is often overlooked in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, designed to preserve an ontological structure. The networks are trained and evaluated using common sound event classification datasets, showing improved performance by incorporating ontological information. Sounds in environments can be related to abstract categories like humans, emergency vehicles, and home, represented by ontologies. The curr_chunk discusses how sound event classification can benefit from ontologies, which provide structure and relationships for interpreting sounds. Despite neural networks being the state of the art for sound event classification, they rarely consider ontologies. Ontologies offer a formal representation of domain knowledge through categories and relationships, enhancing training data and network architecture. Ontologies provide structure and relationships for interpreting sounds in sound event classification. They offer a formal representation of domain knowledge through categories and relationships, enhancing training data and network architecture. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Another taxonomy can be defined by interactions between objects and materials, actions, and descriptors. Hierarchical relations in sound event classifiers offer benefits such as allowing the classifier to back-off to more general categories, disambiguating acoustically similar classes, and penalizing classification differently based on super classes. Ontology-based network architectures have shown improved performance in sound event classification by incorporating domain knowledge to model neural networks. This approach has been evaluated in computer vision and music, but is not commonly used in sound event classification. The deep restricted Boltzmann machine and perceptron architectures have shown improved performance in textual topic classification and image classification by incorporating hierarchical structures. These approaches reduce overfitting and improve class disambiguation. Inspired by these methods, the authors propose ontology-based networks for sound event classification, leveraging domain knowledge to enhance neural network models. In this section, the authors propose ontology-based networks for sound event classification, leveraging domain knowledge to enhance neural network models. They present a framework for dealing with ontological information using deep learning architectures, including a Feed-forward model with proposed ontological layer to compute ontology-based embeddings. The authors propose ontology-based networks for sound event classification, utilizing domain knowledge to improve neural network models. They introduce a framework for incorporating ontological information into deep learning architectures, including a Feed-forward model with an ontological layer for computing ontology-based embeddings. The framework considers training data with audio representations associated with labels from the ontology, which can be easily generalized to more levels. The training data consists of audio representations associated with labels from an ontology. The ontology includes hierarchical levels where each class is mapped to the next level. For example, C1 includes classes like cat, dog, and piano, while C2 includes nature, human, and music. Each element in C1 is related to one element in C2. The training data consists of audio representations associated with labels from an ontology. The ontology includes hierarchical levels where each class is mapped to the next level. For example, C1 includes classes like cat, dog, and piano, while C2 includes nature, human, and music. Each element in C1 is related to one element in C2. The architecture of the Feed-forward Network with Ontological Layer is shown in Figure 2, where the blue column represents the acoustic feature vector, and the red columns are the output probabilities for both levels. This probabilistic formulation allows for inferring labels in C2 based on known labels in C1. The blue column represents the acoustic feature vector, and the red columns are the output probabilities for both levels. To estimate p(y 2 |x) using a model, compute the estimation of p(y 1 |x) and sum the values corresponding to the children of y 2. Using knowledge to relate different classes in y 1 during training should improve model performance, especially for predicting classes y 2. At training time, utilizing knowledge to relate classes in y1 can enhance model performance, particularly for predicting classes y2. The proposed framework is used to design ontology-based neural network architectures, incorporating an ontological layer. The Feed-forward Network (FFN) with Ontological Layer includes a base network (Net), an intermediate vector z, and outputs for each ontology level, with the base network weights updated at each parameter update. The Feed-forward Network (FFN) with Ontological Layer consists of a base network (Net), an intermediate vector z, and two outputs for each ontology level. The base network weights are updated at every parameter update. The base network takes audio features x as input and generates vector z, which is used to produce probability vectors for C1 and C2. The ontological layer reflects the relation between super classes and sub classes in the ontology. The FFN with Ontological Layer predicts classes in C1 and C2 for input x using an ontological layer. Equation 3 represents this relationship as a directed graph with an incidence matrix M. The ontological layer defines weights for standard layer connections, which are not trainable but part of the training data. To train the model, a gradient-based method minimizes the loss function L, a convex combination of two categorical cross-entropy functions. The hyperparameter \u03bb \u2208 [0, 1] is tuned to adjust the weighting between the two functions. When \u03bb = 1, the model trains as a standard classifier using only information from the first level of the ontology. The process of learning ontology-based embeddings is described in this section. The process of learning ontology-based embeddings involves using a Siamese neural network (SNN) to create embeddings that preserve the ontological structure. The SNN architecture enforces samples of the same class to be closer while separating samples of different classes. The goal is to train a standard classifier using information from the first level of the ontology. The architecture of the SNN with the Feed-forward Network with Ontological Layer is depicted in FIG1. Blue rows show acoustic feature vectors of samples from different subclasses or superclasses. Twin networks share weights and learn simultaneously. Ontological embeddings compute a Similarity metric (Euclidean Distance) to indicate differences between samples based on ontology. The Feed-forward Model with Ontological layer uses ontology-based embeddings to measure the difference between samples from the same subclass, different subclasses but the same superclass, and different superclasses. The output probabilities for different levels are calculated during training using a gradient-based method. The study evaluates the sound event classification performance of ontological-based neural network architectures using three types of audio example pairs. The dataset for Making Sense of Sounds Challenge 2 (MSoS) aims to classify the most abstract classes in its taxonomy. The ontology has two levels with 97 classes at the lowest level and a higher level with unspecified classes. The study aims to classify the most abstract classes in its taxonomy using audio files from various databases. The dataset consists of 1500 audio files divided into five categories, with an unbalanced number of sound types. The evaluation dataset has 500 audio files, with 100 files per category. All files are in a single-channel 44.1 kHz, 16-bit .wav format. The set was randomly partitioned for training and parameter tuning. The dataset consists of 1500 audio files divided into five categories, with 100 files per category. Files are in single-channel 44.1 kHz, 16-bit .wav format. The set was partitioned for training and parameter tuning. The official blind evaluation set had 500 files distributed among 5 classes. Urban Sounds -US8K dataset evaluates urban sound classification with a taxonomy adjusted to avoid redundant levels. The ontology was adjusted to avoid redundant levels, resulting in a two-level taxonomy with 10 classes at level 1 and 4 classes at level 2. The dataset contains 8,732 audio files from Freesound, each in single-channel 44.1 kHz, 16-bit .wav format, divided into 10 subsets. Training and parameter tuning were done using 9 folds, with one fold reserved for testing. Audio recordings were represented using state-of-the-art Walnet features BID1, with a 128-dimensional logmel-spectrogram vector computed for each audio file. The experiment involved using Walnet features BID1 to represent audio recordings. A 128-dimensional logmel-spectrogram vector was computed for each audio file and transformed using a convolutional neural network (CNN). The base network architecture consisted of 4 layers: an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. The experiment involved using Walnet features BID1 to represent audio recordings. The base network architecture consisted of 4 layers: an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. The dense layers utilized Batch Normalization, a dropout rate of 0.5, and the ReLU activation function. Parameters were tuned in the Net box and for transforming z into p(y 1 |x). Baseline models were considered for different data sets, without ontological information. The models consisted of the Base Network Architecture with an additional output layer for level 1 or level 2. The baseline models for level 1 and level 2 in the MSoS and US8K datasets were evaluated. The baseline performance for level 2 in the MSoS challenge was reported to be 0.81. The study evaluated baseline models for MSoS and US8K datasets in level 1 and level 2. Different values of \u03bb were tested to validate the architecture and analyze the ontological layer's utility. Results showed that values other than 0 and 1 improved performance, with the best performance in MSoS dataset achieved using \u03bb = 0.8, obtaining 0.74 and 0.913 accuracy in level 1 and 2 respectively. Using the ontological layer improved performance in both MSoS and US8K datasets. In MSoS, \u03bb = 0.8 resulted in 5.4% and 6% accuracy improvements in level 1 and 2. In US8K, \u03bb = 0.7 led to smaller improvements of 2.5% and 0.2% in level 1 and 2. The MSoS dataset showed a 5.4% and 6% accuracy improvement in level 1 and 2 with \u03bb = 0.8. The US8K dataset had smaller improvements of 2.5% and 0.2% in level 1 and 2 with \u03bb = 0.7. Ontology-based embeddings resulted in tighter clusters for sound event classification. The architecture described in Section 2.3 was tested to evaluate ontology-based embeddings for sound event classification. t-SNE plots were used to show how the embeddings cluster at different levels. The Walnet audio features were processed, and a Siamese neural network was trained with different class pairs to generate the embeddings. The SNN was trained for 50 epochs using the Adam algorithm, and hyper-parameters were tuned for good performance. The Siamese neural network (SNN) was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for good performance. Different numbers of pairs for the input training data were tried, with 100,000 pairs yielding the best performance. The loss function values were adjusted, affecting overall performance, with specific lambda values used for classifiers in different levels. The results in Table 1 demonstrate the accuracy performance of MSoS. The results in Table 1 show that the accuracy performance of MSoS and US8K were respectively as follows, in level 1 0.736 and 0.818, and in level 2 0.886 and 0.856. The architecture performed better than the baseline but slightly underperformed the method without embeddings. Ontology-based embeddings offer better grouping benefits as shown in Figure 6. t-SNE plots were created for classes in level 2 and level 1 using MSoS data. The t-SNE plots of classes in level 2 and level 1 showed that FF + Ontology vectors and ontology-based embeddings created clustered groups, with the latter having tighter clusters. The performance on the US8K dataset was limited due to a similar number of sub classes and super classes, unlike the MSoS dataset with a larger ratio. In the Making Sense of Sounds Challenge, a framework was proposed to design neural networks for sound event classification using hierarchical ontologies. Two methods were shown to incorporate this structure into deep learning models, achieving significant performance improvements over the baseline accuracy of 0.80 for level 2. The Feed-forward Network with Ontological Layer reached 0.88 accuracy, while using ontological embeddings achieved 0.89 accuracy. In a paper, a framework was proposed for designing neural networks for sound event classification using hierarchical ontologies. Two methods were presented to incorporate this structure into deep learning models efficiently. One method involved a Feed-forward Network with an ontological layer to relate predictions across different hierarchy levels. Another method utilized a Siamese neural Network to compute ontology-based embeddings, showing clusters of super classes with different sub classes. Results from datasets and the MSoS challenge demonstrated improvements over baselines, paving the way for further exploration of ontologies. The study presented a framework for neural networks in sound event classification using hierarchical ontologies. Results showed improvements over baselines in datasets and the MSoS challenge, paving the way for further exploration of ontologies for event classification."
}