{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, adversarial perturbations are decomposed into model-specific and data-dependent components, with the latter contributing significantly to transferability. The proposed noise reduced gradient (NRG) method leverages the data-dependent component to craft adversarial examples, leading to enhanced transferability across various ImageNet classification models. Additionally, low-capacity models exhibit stronger attack capabilities compared to high-capacity models with similar test performance. Large neural network models have powerful attack capabilities, especially when compared to high-capacity models with similar test performance. This insight can help in creating successful adversarial examples and guide the development of defense strategies against black-box attacks. These models are widely used in real-world applications like speech recognition and computer vision, but recent research shows they can be easily manipulated by adversaries to produce incorrect outputs. Recent works have shown that adversaries can manipulate models to produce incorrect outputs, creating adversarial examples. Understanding this phenomenon and effectively defending against such attacks are open questions. Adversarial examples can transfer across different models, allowing for attacks on black-box systems. Adversarial examples can transfer between models, enabling attacks on black-box systems. The generation of adversarial examples is attributed to the nonlinearity of deep neural networks and the linear nature of high dimensionality. The adversarial instability in deep neural networks is caused by their linear nature and high dimensionality. Various methods like FGSM, DeepFool, and iterative gradient sign method have been proposed for crafting adversarial examples. Transferability of adversarial examples and their effectiveness in black-box attacks have been analyzed in different studies. High-confidence adversarial examples that are strongly misclassified by the original model have shown stronger transferability. Several defense mechanisms have been proposed to combat adversarial attacks, including defensive distillation and adversarial training methods. Image transformation techniques have also been utilized to mitigate the impact of adversarial perturbations. Some approaches focus on detecting adversarial examples for manual processing, but their effectiveness is limited. In this work, the transferability of adversarial examples is explained to enhance black-box attacks. Adversarial perturbation is decomposed into model-specific and data-dependent components, with the former being noisy and representing behavior off the data manifold. The transferability of adversarial perturbations across different models is mainly due to the data-dependent component, which approximates the ground truth on the data manifold. A noise-reduced gradient (NRG) method is proposed to construct adversarial examples by utilizing this component. Benchmark results on the ImageNet validation set demonstrate the effectiveness of this approach. The noise-reduced gradient (NRG) method, used in black-box attacks on the ImageNet validation set, significantly improves success rates. Model-specific factors like capacity and accuracy influence attack success, with higher accuracy and lower capacity models showing greater capability. Transferability explains this phenomenon. The high dimensionality of the model function makes it vulnerable to adversarial perturbations, as explained by transferability. The model function is denoted as f : R d \u2192 R K, obtained by minimizing empirical risk over the training set. The perturbation \u03b7 exists for each input x, making the model susceptible to attacks on unseen models. In BID5, the high dimensionality of the model function makes it vulnerable to adversarial perturbations, leading to the existence of nearly imperceptible adversarial perturbations for each input x. These adversarial examples are studied in the context of deep neural networks, as well as other models like SVM and decision trees. The attack is classified as a non-targeted attack, where the adversary aims to misclassify x without control over the output. In a non-targeted attack, the adversary aims to misclassify x without control over the output. In a black-box attack setting, the adversary constructs adversarial examples on a local model trained on a similar dataset to the target model. Adversarial examples can be crafted on a local model trained on a similar dataset to fool the target model in a black-box attack. The process involves optimizing a loss function to measure the discrepancy between prediction and ground truth, with an implicit constraint for image data. The metric used to quantify perturbation in image data is the cross entropy, with commonly used distortion measurements being \u221e and 2 norms. Ensemble-based approaches, like using a large ensemble of models, are suggested to improve the strength of adversarial examples. Ensemble-based approaches suggest using a large ensemble of models to improve the strength of adversarial examples. The most commonly used method involves averaging the predicted probability of each model. Various optimizers, including the normalized-gradient based optimizer, can be used to solve the optimization problems for non-targeted and targeted attacks. In this paper, the normalized-gradient based optimizer is mainly used to solve optimization problems for attacks. The Fast Gradient Based Method and Iterative Gradient Method are two approaches empirically shown to be fast and effective for solving these problems. The Iterative Gradient Method is a simple yet effective optimizer for solving optimization problems for attacks. It performs the projected normalized-gradient ascent for k steps, using the original clean image x 0 and a step size \u03b1. The method includes the normalized gradient g(x) for different types of attacks, such as \u221e -attack and q-attack. The Fast Gradient Based Method can be seen as a special case of the Iterative Gradient Method. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. Previous works suggest that transferability is due to similarity in decision boundaries and a contiguous subspace. Further investigation is needed to understand this transfer phenomenon. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. BID17 claimed that transferable adversarial examples span a contiguous subspace. To investigate this transfer phenomenon, the similarity between different models A and B that enables transferability of adversarial examples across them is questioned. The models A and B have a high performance on the same dataset, indicating they have learned a similar function on the data manifold. However, the behavior of the models off the data manifold can differ due to their architectures and random initializations. This suggests decomposing the perturbation into two factors on and off the data manifold. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. Models A and B have learned a similar function on the data manifold, but their behavior off the data manifold can differ due to architectures and random initializations. Perturbations can be decomposed into data-dependent and model-specific components, with the former contributing more to transferability. The perturbation crafted from model A can mislead both model A and B by aligning with the inter-class deviation on the data manifold. Decomposing the perturbation into data-dependent and model-specific components shows that the data-dependent component contributes more to the transferability between models. The perturbation from model A can mislead both model A and B by aligning with the inter-class deviation on the data manifold. The data-dependent component contributes more to transferability between models, suggesting enhancing this component for increased success rates of black-box adversarial attacks. The NRG method reduces model-specific noise to achieve this enhancement. The NRG method aims to enhance the data-dependent component by reducing model-specific noise, specifically the high-frequency noise inherited from random initialization. By applying local averaging, noisy model-specific information can be removed to approximate the data-dependent component, leading to improved success rates of black-box adversarial attacks. The NRG method reduces model-specific noise to enhance the data-dependent component, improving success rates of black-box adversarial attacks. By using local averaging, the noise-reduced gradient captures more data-dependent information than the ordinary gradient. Visualizing NRG for different sample sizes shows that larger samples lead to smoother gradients, with m=100 capturing semantic information effectively. This approach prevents overfitting to specific source models, ensuring better generalization to other models. Therefore, attacks using \u2207f in Eq.(8) instead of \u2207f can drive the optimizer towards more data-dependent solutions. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) is a method that reduces model-specific noise to enhance data-dependent components in adversarial attacks. The noise-reduced fast gradient sign method (nr-FGSM) is a special case of nr-IGSM. For q-attack, a noise-reduced version is derived for any general optimizer. To enhance transferability, the noise-reduced version of q-attack replaces equations and is derived for any general optimizer. Classification models trained on ImageNet dataset are used to analyze effectiveness. The ImageNet ILSVRC2012 validation set with 50,000 samples is utilized for attack experiments, selecting 5,000 images correctly recognized by all models. For targeted attack experiments, random wrong labels are assigned to each image. Pre-trained models like resnet, vgg, densenet, alexnet, and squeezenet are used for experiments. Different models are chosen to save computational time. For targeted attacks, specific models like resnet, vgg, densenet, alexnet, and squeezenet are used to generate adversarial examples. The white-box attack performance is evaluated based on Top-1 success rate, with Top-5 rates calculated similarly. The cross entropy is chosen as the loss function for the experiments. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with commonly-used methods like FGSM and IGSM for targeted attacks. Success rates of FGSM and nr-FGSM are compared using different optimizers. The combination of noise-reduced gradient with fast gradient-based methods, such as FGSM and nr-FGSM, shows improved success rates in targeted attacks. Nr-FGSM consistently outperforms FGSM, even in white-box attacks, indicating the effectiveness of noise-reduced gradients in enhancing transferability. The comparison between IGSM and nr-IGSM shows that adversarial examples generated by nr-IGSM transfer more easily, indicating that noise-reduced gradients guide the optimizer to explore data-dependent solutions. Large models are also found to be more robust to adversarial transfer than small models. Large models like resnet152 are more robust to adversarial transfer compared to small models. Transfer among models with similar architectures is influenced by model-specific components. IGSM generally generates stronger adversarial examples than FGSM, except in attacks against alexnet, contradicting previous claims about transfer ease between the two methods. The study found that while IGSM generally produces stronger adversarial examples than FGSM, this is not the case when attacking alexnet. The researchers attribute this discrepancy to potentially inappropriate hyperparameters used in previous studies, leading to underfitting of the source model. The study found that IGSM produces stronger adversarial examples than FGSM, but when attacking alexnet, the IGSM overfits due to different architecture and test accuracy, leading to a lower fooling rate. This indicates a need to not fully trust the objective in Eq. (2) to avoid overfitting and poor transferability. The noise reduced gradient technique removes model-specific information, allowing for a more data-dependent solution. The reduced gradient technique regularizes the optimizer by removing model-specific information, leading to better cross-model generalization. The NRG method is applied to ensemble-based approaches with a smaller evaluation set of 1,000 images. Both FGSM and IGSM attacks are tested, with IGSM showing nearly saturated success rates. Top-5 rates are reported to demonstrate method improvements more clearly. The success rates of IGSM attacks are nearly saturated, so the Top-5 rates are reported to show improvements more clearly. Generating targeted adversarial examples is harder than non-targeted ones, and single-model approaches are ineffective for this. Targeted attacks are sensitive to the step size used in optimization procedures. In targeted attacks, a large step size is necessary for generating strong adversarial examples. NRG methods outperform normal methods for both targeted and non-targeted attacks. Top-5 success rates are reported in Table 3(b). The NRG methods outperform normal methods by a large margin in both targeted and non-targeted attacks, as shown in Table 3. The success rates of ensemble-based approaches are compared, with the noise-reduced counterpart consistently performing better. Sensitivity analysis of hyper parameters m and \u03c3 is also explored in this study. In this study, the sensitivity of hyper parameters m and \u03c3 is explored for black-box attacks using NRG methods. Larger m leads to higher fooling rates, while an optimal value of \u03c3 is crucial for best performance. Overly large or small \u03c3 values can negatively impact the results. In this study, the sensitivity of hyper parameters m and \u03c3 is explored for black-box attacks using NRG methods. An optimal value of \u03c3 is crucial for best performance, with overly large or small \u03c3 values negatively impacting results. The optimal \u03c3 varies for different source models, being around 15 for resnet18 and 20 for densenet161. The robustness of adversarial perturbations to image transformations is also explored. The study explores the influence of image transformations on adversarial examples, using destruction rate to quantify the impact. Densenet121 and resnet34 are selected as the source and target models for the analysis. The study compares the robustness of adversarial examples generated by NRG methods versus vanilla methods using Densenet121 and resnet34 models. Various image transformations are applied, and results show NRG-based methods outperform vanilla methods. Decision boundaries of different models are analyzed to understand the performance difference. The study compares the robustness of adversarial examples generated by NRG methods versus vanilla methods using different models. Resnet34 is chosen as the source model, and decision boundaries of nine target models are analyzed. The \u2207f is estimated with specific parameters, and the decision boundaries are visualized in a 2-D plane. The study compares the robustness of adversarial examples generated by NRG methods versus vanilla methods using different models. Resnet34 is chosen as the source model, and decision boundaries of nine target models are analyzed. The direction of sign \u2207f is as sensitive as the direction of sign (\u2207f \u22a5 ) for resnet34, while other target models are more sensitive along sign \u2207f. Removing \u2207f \u22a5 from gradients penalizes the optimizer along the model-specific direction, avoiding overfitting solutions that transfer poorly to other target models. The study explores penalizing the optimizer along the model-specific direction to avoid overfitting solutions that transfer poorly to other target models. It is found that the minimal distance to produce adversarial transfer varies for different models, with complex models requiring significantly larger distances than small models. This provides a geometric understanding of why big models are more robust. Adversarial examples crafted from alexnet generalize worst across models. Different models exhibit varying performances in attacking the same target model. Adversarial examples crafted from alexnet generalize poorly across models, while attacks from densenet121 consistently perform well. This observation suggests that choosing a better local model to generate adversarial examples is crucial for attacking the target model effectively. The study focuses on finding the principle behind the varying performances of different models in attacking the same target model. VGG19 bn and ResNet152 are selected as target models for FGSM and IGSM attacks. Results show that models with powerful attack capability have low fooling rates, while models with large test error and parameters have low success rates in attacks. The study reveals that models with strong attack capabilities have low fooling rates, while models with higher test error and parameters are less successful in attacks. A smaller test error and lower model capacity indicate a stronger attack capability, which is explained through transferability in Section 3.1. In this study, it is shown that models with strong attack capabilities have low fooling rates. Models with higher test error and parameters are less successful in attacks. A smaller test error and lower model capacity indicate a stronger attack capability, explained through transferability. The model with small \u2207f \u22a5 and large \u2207f can provide strong adversarial examples that transfer more easily. In this paper, it is demonstrated that adversarial perturbations consist of model-specific and data-dependent components, with the latter contributing more to transferability. The proposed noise-reduced gradient (NRG) based methods for crafting adversarial examples are more effective. Models with lower capacity and higher test accuracy exhibit stronger capabilities for black-box attacks. Future work will explore combining NRG-based methods with adversarial training for defense against black-box attacks. Future research will focus on combining NRG-based methods with adversarial training to defend against black-box attacks. The transferability component is data-dependent and low-dimensional, making black-box attacks defensible. White-box attacks, originating from high-dimensional space, are more challenging to defend against. Incorporating the NRG strategy can help learn stable features for transfer learning by reducing model-specific noise. Incorporating the NRG strategy can help learn stable features for transfer learning by reducing model-specific noise, beneficial for defending against black-box attacks. The influence of hyper parameters in targeted black-box attacks using IGSM is explored, with evaluation on success rates using resnet152 and vgg16 bn models. The optimal step size \u03b1 is crucial for generating successful adversarial examples. The performance of bn models is evaluated based on the average Top-5 success rate over three ensembles. A large step size \u03b1 of 15, compared to distortion \u03b5 = 20, can harm attack performance. Small step sizes with a large number of iterations can lead to worse performance due to overfitting, while a large step size encourages exploration of model-independent areas. To further confirm the impact of model redundancy on attack capability, an experiment was conducted on the MNIST dataset using fully connected networks of varying depths. The results showed that low-capacity models have stronger attack success rates. The experiment considered models of depth 1, 3, 5, and 9. Results show low-capacity models have stronger attack capability than large-capacity models. The Top-1 success rates of cross-model attacks are reported in TAB5, demonstrating this observation. The attack success rates against resnet152 for various models are shown in Figure 9."
}