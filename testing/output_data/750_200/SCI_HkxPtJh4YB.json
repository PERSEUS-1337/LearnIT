{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for exponential family inference over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, addressing the intractability issue as the permutation size increases. The effectiveness of this approach is demonstrated in the probabilistic identification of neurons in the worm C.elegans. Identification of neurons in the worm C.elegans using Sinkhorn variational marginal inference for computing matrix expectations efficiently. The method addresses intractability by introducing a scalable alternative for exponential family inference over permutation matrices. The Sinkhorn variational marginal inference method efficiently computes matrix expectations for neural identity inference in C.elegans, overcoming the intractability of the problem. The Sinkhorn operator is applied to the matrix L to approximate \u03c1, resulting in a doubly stochastic matrix. The method is described in sections 2 and 3, showing that the Sinkhorn approximation produces the best results. The Sinkhorn variational method efficiently computes matrix expectations for neural identity inference in C.elegans. The method is based on the relation between marginal inference and the normalizing constant, valid for exponential families. The Sinkhorn approximation produces the best results by utilizing the Birkhoff polytope and achieving the supremum for a given parameter. The Birkhoff polytope is used in the dual function log Z L for marginal inference and computation of the permanent Z L. An approximate \u03c1 is obtained by replacing the variational representation of Z L with a more tractable optimization problem, impacting the quality of the approximation. The variational representation of Z L is approximated using a component-wise entropy, resulting in the Sinkhorn permanent. Bounds for this approximation are provided, showing the tightness of the approximation. The Sinkhorn permanent approximation, perm S (L), has bounds provided for its accuracy. This approximation differs from the Bethe variational inference method, which approximates the dual function A*(\u00b5) based on a tree structure in Markov random fields. The dual function A*(\u00b5) is approximated using a tree structure in Markov random fields. This approximation has been successfully applied to permutations, with the corresponding approximate marginal B(L) computed through belief propagation. The Bethe approximation of the permanent has known bounds, with computational differences compared to the Sinkhorn approximation. The Bethe approximation of the permanent has known bounds and computational differences compared to the Sinkhorn algorithms. Sinkhorn iterations involve row and column normalization, while Bethe iterations are more complex. In practice, the Bethe approximation produces better permanent approximations, as shown in Fig 1 (b). The Sinkhorn approximation produced better marginals compared to the Bethe approximation, even though the permanents may be worse. Sinkhorn also scaled better for moderate n values, with faster iteration times. The Sinkhorn approximation outperformed the Bethe approximation in producing better marginals and scaled better for moderate n values. Comparison of approximations using 1,000 submatrices showed differences in log permanent and mean absolute errors of log marginals. The Sinkhorn approximation outperformed the Bethe approximation in producing better marginals and scaled better for moderate n values. Comparison of approximations using 1,000 submatrices showed differences in log permanent and mean absolute errors of log marginals. Sampling-based methods can also be used for marginal inference, but practical appeal is limited. An elementary MCMC sampler failed to produce sensible marginal inferences for the C.elegans nervous system. Recent advances in neurotechnology have enabled whole brain imaging of the worm C.elegans, a species with a stereotypical nervous system. The challenge now is to assign canonical labels to each neuron in the volumetric images. Recent advances in neurotechnology have enabled whole brain imaging of the worm C.elegans, a species with a stereotypical nervous system. To solve the technical problem of identifying neurons in the images, a methodology for probabilistic neural identification was applied in the context of NeuroPAL, a multicolor C.elegans transgene designed for neural identification. The goal is to estimate probabilities for assigning canonical identities to observed neurons based on their position and color vectors. The methodology involves a matrix of marginal \u03c1 to estimate probabilities for assigning canonical identities to observed neurons. These probabilities provide uncertainty estimates for model predictions, using a gaussian model for each canonical neuron. The likelihood of observing data is calculated with a flat prior assumed over P. The methodology involves estimating probabilities for assigning canonical identities to observed neurons using a matrix of marginal \u03c1. This provides uncertainty estimates for model predictions, with a flat prior assumed over P. The likelihood of observing data is calculated, inducing a posterior over P with a deterministic coloring scheme for neuron names in NeuroPAL. A downstream task involves computing approximate probabilistic neural identifies \u03c1, where a human labels neurons with the most uncertain model estimates. The methodology involves estimating probabilities for assigning identities to neurons using a matrix of marginal \u03c1. A human labels neurons with uncertain model estimates to improve identification accuracy. Results are shown in Fig 3. Results of different approximation methods for assigning identities to neurons are compared, including Sinkhorn approximation, Bethe approximation, MCMC, random baseline, naive baseline, and a 'ground truth' protocol. The Sinkhorn and Bethe approximations yield similar results. The Sinkhorn and Bethe approximations for assigning neuron identities yield similar results, with Sinkhorn slightly outperforming Bethe. Both are significantly better than any baseline except for an unattainable oracle. In contrast, MCMC does not outperform the naive baseline, indicating convergence issues and comparable computational times to approximated methods. The Sinkhorn approximation is proposed as a viable alternative to sampling for marginal inference. The Sinkhorn approximation is a sensible alternative to sampling for marginal inference, providing faster and more accurate approximate marginals than the Bethe approximation. Future work will analyze the relationship between permanent approximation quality and corresponding marginals. Additionally, the (log) Sinkhorn approximation of the permanent of L can be obtained. The (log) Sinkhorn approximation of the permanent of L is obtained using positive vectors x, y turned into diagonal matrices. The dataset used consists of NeuroPAL worm heads with human labels and log-likelihood matrices computed with specific methods. The log-likelihood matrices of neurons in worms were summarized using a n \u00d7 n matrix. Sinkhorn and Bethe approximations were computed with 200 iterations, ensuring convergence. The MCMC sampler method was used with 100 chains of length 1000, starting from iteration 500. All computations were done on a desktop. In a study by Diaconis (2009), 100 chains of length 1000 were used to sample multiples of 10 starting from iteration 500. Results were obtained on a desktop computer with an Intel Xeon W-2125 processor. An efficient log-space implementation of the message passing algorithm was described by Vontobel (2013) and simplified by Pontobel (2019). Error bars were calculated using 1,000 submatrices of size n randomly drawn from available log likelihood C.elegans matrices. In a study by Diaconis (2009), 100 chains of length 1000 were used to sample multiples of 10 starting from iteration 500. Results were obtained on a desktop computer with an Intel Xeon W-2125 processor. An efficient log-space implementation of the message passing algorithm was described by Vontobel (2013) and simplified by Pontobel (2019. A number of 1,000 submatrices of size n were randomly drawn from the ten available log likelihood C.elegans matrices (see text on Appendix B, indexes were drawn with replacement). Error bars are omitted because they were too small to be noticed."
}