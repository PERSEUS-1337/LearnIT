{
    "title": "B1xFhiC9Y7",
    "content": "Predicting structured outputs like semantic segmentation requires expensive per-pixel annotations for training convolutional neural networks. To address the challenge of generalizing to new domains without annotations, a domain adaptation method is proposed. This method involves learning discriminative feature representations of patches based on label histograms in the source domain, followed by an adversarial learning scheme to refine the features. The proposed method involves learning discriminative feature representations of patches based on label histograms in the source domain and using an adversarial learning scheme to align feature distributions in target patches with those in source ones. This framework integrates global alignment with patch-level alignment and achieves state-of-the-art performance on semantic segmentation tasks. Extensive experiments on benchmark datasets demonstrate the effectiveness of the approach. Recent deep learning methods have shown progress in vision tasks like object recognition and semantic segmentation, relying on large-scale annotations for training. However, models often struggle to generalize to new domains. Domain adaptation methods aim to bridge this gap between annotated source domains and unlabeled target domains. While many image classification methods exist, domain adaptation for pixel-level prediction is an ongoing area of research. Domain adaptation is crucial for pixel-level predictions as annotating ground truth is expensive. Existing methods use feature-level or output space adaptation to align distributions between domains without labels. There is still room for improvement in this area despite recent works on domain adaptation for tasks like semantic segmentation. Existing state-of-the-art methods in domain adaptation align distributions between source and target domains using adversarial learning. Global distribution alignment may not be effective due to differences in camera pose or field of view, leading to misalignment and incorrect bias. Matching patches that are more likely to be shared is considered for better adaptation. Instead of globally aligning distributions between domains, the approach considers matching shared patches to reduce misalignment and bias during adaptation. By aligning patch distributions through adversarial learning and considering label histograms as a factor, discriminative representations are learned. Advances in learning disentangled representations by considering label histograms of patches to learn discriminative representations for patches and better align them between source and target domains. The text discusses using image representations for patches to address high-variation issues, followed by aligning patches between source and target domains using adversarial modules. The process involves aligning global and patch-level distributions through discriminative representations and K-means clustering. The text discusses using pixel-level annotations in the source domain to extract label histograms as patch-level representations. K-means clustering is then applied to group patches into clusters for training a classifier shared across domains. An adversarial loss is used to align feature representations of target patches with the distribution of source patches in a clustered space. In domain adaptation, an adversarial loss is used to align feature representations of target patches with the distribution of source patches in a clustered space. Experiments include synthetic-to-real and cross-city scenarios for pixel-level road-scene image segmentation. The proposed domain adaptation framework aligns feature representations for structured output prediction in various scenarios, showing superior performance compared to state-of-the-art methods. The framework is versatile and can be applied to other structured outputs like depth prediction in future research. The contributions of this work include proposing a domain adaptation framework for structured output prediction, developing a method for learning discriminative representations guided by label histograms, and demonstrating the effectiveness of the adaptation method on semantic segmentation tasks. The framework aligns feature representations for structured output prediction and outperforms state-of-the-art methods. In this work, domain adaptation methods for image classification and pixel-level prediction tasks are discussed. These methods aim to align feature distributions between source and target domains using hand-crafted features and deep architectures to learn domain-invariant features. Adversarial learning is a common practice in this domain adaptation approach. Recent algorithms utilize deep architectures to learn domain-invariant features and minimize discrepancies across domains. Adversarial learning schemes and Maximum Mean Discrepancy are commonly used, along with different classifiers and loss functions. Some work focuses on enhancing feature representations through pixel-level transfer and domain separation. Domain adaptation for structured pixel-level predictions, particularly in semantic segmentation for road-scene images, is an area that has not been extensively studied. The BID14 method introduces domain adaptation for semantic segmentation in road-scene images using adversarial networks to align global feature representations. The CDA method BID36 utilizes SVM classifier to capture label distributions on superpixels instead of using category-specific priors. The CDA method BID36 applies SVM classifier to capture label distributions on superpixels for domain adaptation in semantic segmentation. Class-wise domain adversarial alignment with pseudo labels and object priors from Google Street View are used to match statistics between domains. Our proposed method focuses on learning discriminative representations for patches to aid in patch-level alignment, without the need for additional priors or annotations. Unlike existing approaches that use class-level alignment, our framework preserves structured information at the patch level and enables end-to-end training. Compared to the BID31 method, which also allows end-to-end training, our algorithm emphasizes learning patch-level representations for improved alignment. Our algorithm focuses on learning patch-level representations for alignment, emphasizing a latent disentangled space for tasks like facial recognition, image generation, and view synthesis. Our algorithm focuses on learning patch-level representations for alignment, emphasizing a latent disentangled space for tasks like facial recognition, image generation, and view synthesis. In contrast, recent methods like BID35 and AC-GAN focus on synthesizing 3D objects and developing a generative adversarial network with auxiliary classifiers for specified factors. These methods show promise in learning discriminative representations but are limited to single domains. Our proposed domain adaptation framework focuses on learning discriminative representations for patches to aid in aligning distributions across domains. The framework utilizes available label distributions as a disentangled factor without the need to pre-define any factors. The proposed domain adaptation framework aims to align distributions across domains by using discriminative representations for patches. It involves aligning predicted output distribution of target data with the source distribution through supervised learning and adversarial loss. Additionally, a classification loss in a clustered space is incorporated to learn patch-level discriminative representations from the source output distribution. The adaptation framework aligns distributions by using discriminative representations for patches. It includes a classification loss in a clustered space to learn patch-level discriminative representations from the source output distribution. The goal is to align patch-level distributions between source and target data through supervised and adversarial loss functions. The adaptation framework aligns distributions using discriminative representations for patches. It involves structured prediction and discriminative representation learning on source data, with clustering on ground truth labels. Global and patch-level adversarial loss functions are used to align the target distribution. The baseline model includes a supervised cross-entropy loss and an output space adaptation module for global alignment. The baseline model includes a supervised cross-entropy loss Ls and an output space adaptation module using Lg adv for global alignment. The loss Ls is optimized by a fully-convolutional network G that predicts the structured output with the loss summed over the spatial map indexed with h, w and the number of categories C. The adversarial loss Lg adv follows GAN training by optimizing G and a discriminator Dg for binary classification. The adversarial loss Lg adv in the baseline model is optimized by a fully-convolutional network G and a discriminator Dg for binary classification. The min-max problem for G and Dg is optimized to distinguish source and target images using transferable structured output representations from smaller patches. The proposed approach involves performing patch-level domain alignment by clustering patches from source-domain examples using ground truth segmentation labels to construct structured output representations shared across source and target images. The network architecture consists of a generator G and a categorization module H for learning discriminative patch representations. The proposed method involves patch-level domain alignment by clustering patches from the source domain using ground truth segmentation labels to create prototypical patch patterns. Target domain patches are then guided to adapt to this disentangled space of source patch representations. Learning discriminative representations is achieved without the need for class labels or pre-defined factors. In this work, discriminative representations are learned without the need for class labels or pre-defined factors by using per-pixel annotations to construct a semantically disentangled space of patch representations. To construct a semantically disentangled space of patch representations, label histograms are used as the disentangled factor. Patches are randomly sampled from source images, spatial label histograms are extracted, and K-means clustering is applied to assign labels to patches based on histogram similarity. A classification module is added after the predicted output to incorporate this clustered space during training the network on source data. To incorporate the clustered space during training, a classification module is added after the predicted output to simulate label histogram construction. The learned representation is denoted as F s = H(G(I s )) through the softmax function, where each data point corresponds to a patch of the input image. The learning process involves formulating a cross-entropy loss for patch-level adversarial alignment. The learning process involves formulating a cross-entropy loss for patch-level adversarial alignment to align representations of target patches to the clustered space constructed in the source domain. This is achieved by utilizing an adversarial loss between F s and F t, reshaping F by concatenating K-dimensional vectors along the spatial map to align patches regardless of their location in the image. The adversarial objective involves reshaping data and formulating a discriminator to classify feature representations from different domains. The network optimization process includes updating the discriminator and network alternately in three steps. The standard procedure for training a GAN involves alternating optimization between updating the discriminator Dg, updating the discriminator Dl, and updating the network G and H while fixing the discriminators. The discriminator Dg is trained to distinguish between the source output distribution and the target distribution, while the discriminator Dl is trained to classify feature representations from the source or target domain. The network G and H are updated to achieve the overall goal. The goal of updating the Network G and H is to align the target distribution with the source distribution using optimized discriminators Dg and Dl. This involves minimizing a combination of supervised and adversarial loss functions, ensuring good performance on main tasks. The adversarial loss functions for updating G and H aim to align target distribution with source distribution. Back-propagation updates H and enhances feature representations in G. Discriminator Dg uses fully-convolutional layers with leaky ReLU activation. The generator network consists of 5 convolution layers with specific parameters and a leaky ReLU activation. The discriminator utilizes fully-connected layers with leaky ReLU activation. The framework follows DeepLab-v2 with ResNet-101 architecture pre-trained on ImageNet. The proposed architecture in the framework utilizes DeepLab-v2 with the ResNet-101 architecture pre-trained on ImageNet as the baseline network. A module is added to the output prediction by using an adaptive average pooling layer followed by two convolution layers to generate a feature map. The implementation is done using the PyTorch toolbox on a single Titan X GPU with 12 GB memory. The discriminators are trained using the Adam optimizer with an initial learning rate. The proposed framework utilizes DeepLab-v2 with ResNet-101 pre-trained on ImageNet. Implementation is on a single Titan X GPU with 12 GB memory. Adam optimizer is used to train discriminators, while Stochastic Gradient Descent is used for the generator. Learning rates are decreased using polynomial decay. The proposed framework for domain adaptation on semantic segmentation utilizes DeepLab-v2 with ResNet-101 pre-trained on ImageNet. Training parameters include decreasing learning rates using polynomial decay, specific values for \u03bb d, \u03bb g adv, \u03bb l adv, and K, and a two-phase training approach. Additional hyper-parameters details are provided in the appendix. The proposed framework for domain adaptation on semantic segmentation is evaluated through an extensive ablation study on the GTA5-toCityscapes scenario. The method outperforms state-of-the-art approaches on various benchmark datasets and settings, including synthetic-to-real and cross-city scenarios. Experiments involve adapting datasets like GTA5 BID27 and SYNTHIA BID28 to Cityscapes, showcasing the effectiveness of the domain adaptation method. The study evaluates domain adaptation on semantic segmentation using datasets like Cityscapes BID5 and SYNTHIA BID28. It adapts Cityscapes images to the Oxford RobotCar BID23 dataset with rainy scenes, selecting 10 sequences for training and testing. 895 images are sampled for training with 271 annotated for per-pixel semantic segmentation. In a study on domain adaptation for semantic segmentation, 895 images were used for training and 271 images were annotated for per-pixel semantic segmentation. The evaluation metric used was intersection-over-union (IoU) ratio. An ablation study was conducted on the GTA5-to-Cityscapes scenario to analyze the impact of different loss functions and design choices in the proposed framework. In an ablation study on domain adaptation for semantic segmentation, different loss functions and design choices were analyzed in the proposed framework. Results show that adding disentanglement without alignments improves performance, demonstrating enhanced feature representation. Combining global and patch-level alignments achieved the highest IoU at 43.2%. The patch-level alignment with losses Ld and Lladv is crucial for achieving the highest IoU of 43.2%. Removing either loss results in a performance drop of 1.9% and 1.5% respectively. The reshaping of features as independent data points in the clustered space is essential for effective patch-level alignment. In the clustered space, reshaping features as independent data points is crucial for patch-level alignment. Without this reshaping process, performance drops by 2.4% in IoU, highlighting the importance of aligning patches with similar representations regardless of their locations. Visualization in FIG1 shows the effectiveness of this approach compared to not adapting at the patch level. In FIG1, t-SNE visualization BID34 of patch-level features in clustered space with adaptation shows well-overlapping source/target representations. Comparison with state-of-the-art algorithms includes synthetic-to-real and cross-city scenarios, with experimental results adapting GTA5 to Cityscapes in TAB1 using VGG-16 architecture. In TAB1, experimental results show adapting GTA5 to Cityscapes using VGG-16 architecture. The proposed method outperforms state-of-the-art adaptations in feature, pixel-level, and output space alignments. Utilizing ResNet-101 base network, the method improves IoU by 1.8% and achieves the best IoU in 14 out of 19 categories. In TAB2, similar improvements are shown for adapting SYNTHIA to Cityscapes. The method improves IoU by 1.8% and achieves the best IoU in 14 out of 19 categories. Results for adapting SYNTHIA to Cityscapes show similar improvements compared to state-of-the-art methods. Visual comparisons are shown in Figure 5, with more results in the appendix. Adapting between real images across different cities and conditions is demonstrated by adapting Cityscapes to Oxford RobotCar in a challenge case with different weather conditions. The proposed method improves segmentation details and reduces noise compared to existing methods. Results show a mean IoU of 63.6%, 1.4% lower than the proposed method. The method combines global and patch-level alignments for structured output domain adaptation. The proposed method combines global and patch-level alignments for structured output domain adaptation, improving segmentation details and reducing noise. It involves output space adaptation and learning discriminative representations of patches across domains through an adversarial learning scheme. Extensive experiments validate its effectiveness under various challenges in semantic segmentation. The proposed method combines global and patch-level alignments for structured output domain adaptation in semantic segmentation. Experiments validate its effectiveness under various challenges, including synthetic-to-real and cross-city scenarios, showing favorable performance against existing algorithms. The model is trained in an end-to-end manner by randomly sampling one image from each domain in a training iteration. Image and patch sizes during training and testing are shown in TAB3, maintaining the aspect ratio without cropping and down-sampling the image accordingly. The proposed method combines global and patch-level alignments for structured output domain adaptation in semantic segmentation. The model is trained in an end-to-end manner by randomly sampling one image from each domain in a training iteration, maintaining the aspect ratio without cropping and down-sampling the image accordingly. BID12 can be used as a loss in the model to push the target feature representation to one of the source clusters, replacing the adversarial loss on the patch level with an entropy loss. The model with this entropy regularization achieves an IoU of 41.9%, lower than the proposed patch-level adversarial alignment at 43.2%. The model achieves an IoU of 41.9%, lower than the proposed patch-level adversarial alignment at 43.2%. Our model learns discriminative representations for target patches by aligning them closer to the source distribution in a clustered space guided by the label histogram. Source and target patches show high similarity in the clustered space, demonstrating the effectiveness of the patch-level alignment. In the clustered space via t-SNE, source and target patches exhibit high similarity, showcasing the effectiveness of patch-level alignment. Results for adapting Cityscapes to Oxford RobotCar are presented in TAB4, comparing the proposed method with other approaches. Visual comparisons for different scenarios are provided in figures 9 to 11. The proposed method for adapting Cityscapes to Oxford RobotCar shows better segmentation outputs with more details and less noise compared to other approaches. Results are presented in figures 9 to 11, showcasing the effectiveness of the method. In figures 10 and 11, adapted segmentation results for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes settings are shown before adaptation, output space adaptation BID31, and the proposed method."
}