{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this, a new balanced face image dataset with 108,501 images representing 7 race groups was created. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups for evaluation on existing face attribute datasets. The study created a balanced face image dataset with 108,501 images representing 7 race groups. The model trained on this dataset showed higher accuracy on novel datasets and consistent performance across race and gender groups. Comparison with commercial computer vision APIs was also conducted to evaluate balanced accuracy across gender, race, and age groups. Numerous large scale face image datasets have fostered research for automated face detection, alignment, recognition, generation, modification, and attribute classification. Existing public face datasets are biased towards Caucasian faces, with other races like Latino being underrepresented. Most large scale face databases favor \"lighter skin\" faces, such as White, over \"darker\" faces like Black. Most large scale face databases are biased towards \"lighter skin\" faces, like White, compared to \"darker\" faces, such as Black. This bias can lead to unfairness in automated systems and raise ethical concerns about the accuracy of models trained on biased data. Several commercial computer vision systems have faced criticism for their asymmetric accuracy across sub-demographics, particularly in face gender classification. Studies have shown that these systems perform better on male and light faces, indicating biases in their training data. Biases in image datasets can stem from biased selection, capture, and negative sets, with public large scale face datasets often collected from popular online media platforms. To address race bias in existing face datasets collected from popular online media platforms, a novel face dataset with balanced race composition was proposed. The dataset contains 108,501 facial images from various sources, including YFCC-100M Flickr dataset, Twitter, and online newspapers, representing 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. The dataset proposed includes 108,501 facial images from various sources, representing 7 race groups. The paper highlights three main contributions: showing existing datasets and models do not generalize well to nonWhite faces, demonstrating the new dataset's better performance on novel data across racial groups, and being the first large-scale face attribute dataset in the wild. The dataset proposed includes 108,501 facial images from 7 race groups, including Latino and Middle Eastern. This dataset significantly expands the applicability of computer vision methods to fields like economics and social sciences by including major racial groups missing in existing datasets. The goal of face attribute recognition is to classify human attributes such as gender, race, age, and emotions. Face attribute recognition aims to classify human attributes like gender, race, age, emotions, and expressions from facial appearance. Existing datasets are mainly from online sources and are biased towards the White race. This recognition is used in tasks like face verification and person re-identification. Face attribute recognition is crucial for tasks like face verification and person re-identification. It is important for these systems to perform well across different gender and race groups to maintain trust. Instances of racial bias include Google Photos mistaking African American faces for Gorillas and Nikon's cameras asking Asian users if someone blinked. Instances of racial bias in face attribute recognition include Google Photos recognizing African American faces as Gorillas and Nikon's cameras prompting Asian users with messages about blinking. These incidents often lead to service termination or dropping sensitive output categories. Commercial service providers have stopped providing race classifiers due to these issues. Face attribute recognition is also used for demographic surveys in marketing and social science research to understand human social behaviors and demographic backgrounds. Social scientists use images of people to infer demographic attributes and analyze behaviors, such as demographic analyses of social media users. Unfair classification can lead to over-or under-estimation of specific sub-populations in analysis, with significant policy implications. In the context of demographic analyses, unfair classification can have significant policy implications. Research in algorithmic fairness focuses on balanced accuracy, ensuring attribute classification is independent of race and gender. Research in algorithmic fairness focuses on the accuracy of attribute classification independent of race and gender. Studies have looked into auditing bias in datasets, improving datasets, and designing better algorithms or models. Our paper focuses on gender classification from facial images, addressing biases in commercial systems highlighted by Buolamwini & Gebru (2018). Biased results may stem from skewed datasets or associations between scene and race in images. Balancing across all possible factors is deemed infeasible. The contribution of our paper is to mitigate the limitations and biases of existing databases by collecting more diverse face images from non-White race groups. This significantly improves generalization performance to novel image datasets not dominated by the White race. Our dataset is the first large scale in-the-wild face dataset. Our dataset includes Southeast Asian and Middle Eastern races, defining 7 race groups. Race is based on physical traits, while ethnicity is based on cultural similarities. In the context of race and ethnicity, different categorizations exist, with race based on physical traits and ethnicity on cultural similarities. The U.S. Census Bureau's race classification includes White, Black, Asian, Hawaiian and Pacific Islanders, Native Americans, and Latino. Latino is considered a race rather than just an ethnicity, with subgroups like Middle Eastern, East Asian, Southeast Asian, and Indian also identified based on facial appearance. In this paper, the focus is on race classification, particularly Latino being considered a race based on facial appearance. Subgroups like Middle Eastern, East Asian, Southeast Asian, and Indian were identified as distinct. Hawaiian and Pacific Islanders, as well as Native Americans, were excluded due to limited examples. The experiments in the paper were based on a 7-race classification, with a discussion on measuring dataset bias based on skin color or race. Some recent studies use skin color as a proxy for racial or ethnic grouping, but it has limitations due to variations in lighting and within-group skin color differences. Race is multidimensional while skin color is one-dimensional. Race is multidimensional, while skin color is one-dimensional. Skin color alone cannot differentiate between many race groups like East Asian and White. Therefore, race is explicitly used and annotated by human annotators, with skin color also considered as a complement. To complement race categorization, skin color measured by ITA is used. Face datasets often sourced from public figures may be biased towards older or more attractive individuals. Professional photographers capture images in limited situations, leading to quality bias in the data collected. The dataset collection process aims to minimize selection bias by avoiding stereotypical or celebrity-focused searches. It starts with the Yahoo YFCC100M dataset, detecting faces without preselection to maximize diversity and coverage. Our dataset, derived from the Yahoo YFCC100M dataset, focuses on detecting faces without preselection to ensure diversity. It is smaller but more balanced on race compared to other datasets. The collection process incrementally increased the dataset size by detecting and annotating faces, estimating demographic compositions, and adjusting the number of images for each country sampled. The dataset focused on detecting faces without preselection for diversity. Demographic compositions of each country were estimated, and the number of images adjusted to avoid dominance by the White race. Faces from the U.S. and European countries were excluded after sampling enough White faces. The minimum face size was set to 50 by 50 pixels to maintain recognizability and robustness. Only images with \"Attribution\" and \"Share Alike\" Creative Commons licenses were used. We used images with \"Attribution\" and \"Share Alike\" Creative Commons licenses to annotate race, gender, and age group using Amazon Mechanical Turk. Three workers were assigned for each image, with ground-truth values determined by agreement among workers. Annotations were refined by training a model. After refining annotations through model training, race labels were manually verified for images with discrepancies. The race composition of datasets was assessed, with annotations added for 3,000 random samples from datasets without race information. Most face attribute datasets, especially those focusing on celebrities, were found to be skewed in terms of race composition. Most face attribute datasets, particularly those featuring celebrities or politicians, show bias towards the White race. Gender balance in datasets is relatively even, with a male ratio ranging from 40%-60%. Model performance comparison was conducted using ResNet-34 architecture trained on each dataset with ADAM optimization. Face detection was done using dlib's CNN-based face detector, and attribute classification was performed on each face. The experiment involved using a learning rate of 0.0001 and detecting faces in images with dlib's CNN-based face detector. The attribute classifier was run on each face using PyTorch. The dataset was compared with UTKFace, LFWA+, and CelebA, with UTKFace and LFWA+ suitable for race annotations comparison. CelebA was only used for gender classification as it lacks race annotations. Table 1 provides detailed dataset characteristics. CelebA lacks race annotations, while FairFace defines 7 race categories but only 4 were used for comparison. Cross-dataset classifications were performed using models trained from these datasets. FairFace is the only dataset with 7 races, so fine racial groups were merged for compatibility with other datasets. CelebA was included for gender classification. Classification results are shown in Tables 2 and 3. The classification results for race, gender, and age on various datasets were presented in Tables 2 and 3. The model performed best on the LFWA+ dataset due to its diversity and generalizability. Additionally, the generalization performance of the models was tested on three novel datasets. The study tested the generalization performance of models on three novel datasets collected from different sources than the data from Flickr. The test datasets contain people from diverse locations, including images from geo-tagged Tweets in four countries (France, Iraq, Philippines, and Venezuela) with 5,000 randomly sampled faces. The study collected data from geo-tagged Tweets in four countries (France, Iraq, Philippines, and Venezuela) and media photographs from professional outlets, sampling 5,000 faces and 8,000 faces respectively. The study collected data from geo-tagged Tweets in four countries and media photographs from professional outlets, sampling 5,000 faces and 8,000 faces respectively. The authors also used a public image dataset collected for a protest activity study, with a wide range of diverse race and gender groups engaging in different activities in various countries. 8,000 faces were randomly sampled from this dataset and annotated for gender, race, and age. The study collected data from geo-tagged Tweets and media photographs, sampling 5,000 and 8,000 faces respectively. Faces were annotated for gender, race, and age. The FairFace model outperformed other models for race, gender, and age classification on novel datasets. The FairFace model outperforms other models for race, gender, and age classification on novel datasets, even with fewer training images. It also produces more consistent results across different race groups compared to other datasets. The FairFace model shows superior performance for race, gender, and age classification on new datasets with fewer training images. It also demonstrates more consistent results across various race groups compared to other datasets. The model's fairness is measured by accuracy equality and equalized odds for different demographic groups. The FairFace model achieves the lowest maximum accuracy disparity for gender classification across different race and gender groups, with less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White. The LFWA+ model shows a strong bias towards the male category, while the CelebA model tends to exhibit a bias towards the female category due to the dataset composition. The FairFace model achieves less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White for gender classification. Other models show bias towards males and perform inaccurately on females and non-White groups. Gender performance gap was largest in LFWA+ dataset. Unbalanced representation in training data likely causes asymmetric gender biases in computer vision services. The study found asymmetric gender biases in commercial computer vision services, likely due to unbalanced representation in training data. Data diversity was measured by visualizing faces in 2D space using t-SNE, showing FairFace had well-spread race groups compared to other datasets dominated by White faces. The FairFace dataset contains diverse faces with race groups loosely separated, likely due to biased training data. Other datasets like VGG-Face and UTKFace focus more on local clusters. Pairwise distance distributions were used to measure face diversity. The diversity of faces in datasets like UTKFace and LFWA+ was measured using pairwise distance distributions. UTKFace had tightly clustered faces, while LFWA+ showed diverse faces despite containing mostly white faces. This diversity is attributed to the training of the face embedding model. The face embedding model trained on a white-oriented dataset effectively separates white faces, leading to diverse appearances in datasets like UTKFace and LFWA+. Previous studies have shown inconsistent classification accuracies across demographic groups in face analytic models. Testing with FairFace images revealed disparities in gender classification by online APIs. Inconsistent classification accuracies were found across different demographic groups. FairFace images were used to test online APIs for gender classification, showing disparities. The dataset used was diverse in race, age, expressions, head orientation, and photographic conditions, serving as a better benchmark for bias measurement. 7,476 random samples from FairFace were used, containing an equal number of faces from each race, gender, and age group, excluding children under 20. The experiments using FairFace images tested online APIs for gender classification, revealing disparities. The dataset was diverse in race, age, expressions, head orientation, and photographic conditions. The experiments were conducted on August 13th -16th, 2019. Table 6 displays gender classification accuracies of tested APIs, with Amazon Rekognition detecting all faces. Detection rates are further detailed in Table 8 of the Appendix. The gender classification accuracies of tested APIs were displayed in Table 6, with Amazon Rekognition detecting all faces. The results showed that all tested gender classifiers still favored the male category, consistent with previous reports. The results show that gender classifiers still favor males, with dark-skinned females having higher error rates. Skin color alone is not a reliable indicator of bias, and face detection can also introduce gender bias. Microsoft's model had trouble detecting male faces. This paper addresses model bias in gender classification and face detection. A novel face image dataset balanced on race, gender, and age is proposed to improve generalization classification performance for gender, race, and age on diverse image datasets. The model trained from this dataset produces balanced results. A novel face image dataset with more non-White faces than typical datasets is proposed to address model bias in gender classification and face detection. The dataset, derived from Yahoo YFCC100m dataset, allows for balanced accuracy across race groups and can be used for training new models and verifying existing classifiers for algorithmic fairness in AI systems. The novel dataset proposed in this paper aims to mitigate race and gender bias in computer vision systems, improving transparency and acceptance in society. The novel dataset aims to mitigate race and gender bias in computer vision systems for better acceptance in society. Figure 5 shows the distribution of different races based on skin color in the dataset."
}