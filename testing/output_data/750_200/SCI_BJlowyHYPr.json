{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model designed for forecasting data streams from geospatial point-cloud sources. It utilizes a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. The D-Conv operator enhances spatiotemporal forecasting models by capturing neighboring correlations and can be integrated into LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM architecture is applied to point-cloud stream forecasting for mobile service traffic and air quality indicator forecasting. Results show accurate long-term predictions, outperforming other neural network models. Point-cloud stream forecasting predicts future values and locations of data streams from geospatial point-cloud S, using historical observations. Point-cloud stream forecasting operates on irregular sets of points from various data sources like mobile network antennas and air quality sensors, requiring models to capture complex spatial correlations. Vanilla LSTMs are not sufficient for this task. Different approaches to geospatial data stream forecasting involve predicting over grid-structured data using ConvLSTMs or mapping point-cloud input to a grid for tasks like mobile network traffic analysis. The proposed PointCNN leverages spatial-local correlations of point clouds for forecasting directly over point-cloud data streams using historical information. The proposed PointCNN leverages spatial-local correlations of point clouds for forecasting over point-cloud data streams. The CloudLSTM architecture is introduced to handle temporal dependencies and combine with Seq2seq learning and attention mechanisms for precise forecasting. CloudLSTM with Seq2seq learning and attention mechanisms is used for precise forecasting over point-cloud streams. A point-cloud is defined as a set of N points, each containing value features and coordinates. Different measurements can result in U different channels of the point-cloud data. An ideal point-cloud stream forecasting model should have five key properties: (i) Order invariance, (ii) Information intactness, and (iii) Interaction. These properties ensure that the output maintains the same number of points as the input and is not affected by permutations of input points. An ideal point-cloud stream forecasting model should also be robust to transformations like scaling and shifting, capture local dependencies among neighboring points, and adapt to changing spatial correlations over time. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM to handle changing spatial correlations among points over time. DConv generalizes convolution on grids by computing weighted summations on point-clouds, inheriting properties of ordinary convolution. The Dynamic Point Cloud Convolution (DConv) operates on point-clouds, maintaining properties of ordinary convolution. It takes U in channels of a point-cloud S and outputs U out channels with the same number of elements as the input. Points in S i in are denoted as Q K n, including the K nearest points. The DConv operates on point-clouds, with S and S j as 3D tensors. Q K n is a subset of points in S i in, including the K nearest points to p n. Each p n in S contains H value features and L coordinate features. The DConv sums the element-wise product over all features and points in Q K n to obtain the values and coordinates of a point. The DConv operates on point-clouds, with shared learnable weights for element-wise product over features and points to obtain values and coordinates of a point. The weights are 5D tensors shared across anchor points in the input map. The DConv operates on point-clouds with shared learnable weights for element-wise product over features and points to obtain values and coordinates of a point. Each element in the map is a scalar weight for input and output channels, nearest neighbors, and coordinate features. Bias is defined for the output map. Sigmoid function limits predicted coordinates to (0, 1) to avoid outliers. Raw point-cloud coordinates are normalized before feeding them to the model. The DConv operates on point-clouds with shared learnable weights for element-wise product over features and points to obtain values and coordinates of a point. The coordinates of raw point-clouds are normalized to (0, 1) before feeding them to the model to improve transformation robustness. The K nearest points can vary for each channel at each location, reflecting different types of measurements in the dataset. The CloudLSTM model aims to learn spatial correlations between different measurements in datasets, such as mobile apps and air quality indicators. It allows each channel to find the best neighbor set for forecasting, improving the accuracy of predictions. The CloudLSTM model improves forecasting performance by allowing each channel to find the best neighbor set. The DConv operator weights its K nearest neighbors across all features to produce values and coordinates in the next layer, maintaining symmetry and independence from input order. The DConv function is symmetric and does not depend on input order, satisfying properties (i) and (ii). It operates on neighboring point sets to capture local dependencies and improve robustness to global transformations. Normalization over coordinate features enhances robustness, meeting properties (iii) and (iv). DConv improves robustness to transformations by coordinating features, meeting desired properties (iii) and (iv). It learns cloud-point layout and topology for dynamic positioning tailored to each channel and time step, essential for spatiotemporal forecasting neural models. DConv can be efficiently implemented using simple 2D convolution. DConv improves robustness to transformations by coordinating features and learning cloud-point layout and topology for dynamic positioning. It can be efficiently implemented using simple 2D convolution and introduces variations tailored to pointcloud structural data. DConv ensures order invariance in pointcloud structural data without introducing extra complexity or information loss, unlike PointCNN which uses X-transformation and MLPs for weight and permutation learning. The DConv operator ensures order invariance in point-cloud structural data without extra complexity or information loss. It can be seen as a DefCNN over point-clouds, with differences in how it deforms input maps and selects neighboring points for operations. DefCNN and DConv offer transformation modeling flexibility with adaptive receptive fields on convolution. DConv can be integrated into LSTMs for learning spatial and temporal correlations over point-clouds. The Convolutional Point-cloud LSTM (CloudLSTM) is formulated similar to ConvLSTM, with input, forget, and output gates (i_t, f_t, o_t), memory cell (C_t), hidden states (H_t), all as point cloud representations. Weight (W) and bias (b) tensors are learnable, with element-wise product (' ') and DConv operator (' ') used in the equations. The CloudLSTM cell combines DConv with Seq2seq learning and soft attention mechanism for forecasting, leveraging effective neural models for spatiotemporal grid-structural data modeling. The Seq2seq CloudLSTM architecture incorporates an encoder and decoder using the soft attention mechanism for forecasting. The encoder encodes historical information into a tensor, while the decoder decodes the tensor into predictions. The states of the encoder and decoder are connected via a context vector before generating final forecasts. The study employs a two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell, utilizing DConv operations to process point-cloud data before forecasting. Increasing the number of stacks and channels did not significantly improve performance. The study explores new architectures, including CloudRNN and CloudGRU, by incorporating DConv into vanilla RNN and Convolutional GRU. These architectures share a Seq2seq design with CloudLSTM but without the attention mechanism. Performance evaluation is done using traffic datasets from mobile services and network antennas. The study evaluates new architectures like CloudRNN and CloudGRU, incorporating DConv into vanilla RNN and Convolutional GRU. Performance is assessed using datasets from mobile services and air quality indicators. CloudLSTM is used to forecast future demands, compared with 12 baseline models using TensorFlow and TensorLayer in a computing cluster with NVIDIA Tesla K40M GPUs. The study evaluates new architectures like CloudRNN and CloudGRU, incorporating DConv into vanilla RNN and Convolutional GRU. Performance is assessed using datasets from mobile services and air quality indicators. Experiments are conducted on spatiotemporal point-cloud stream forecasting tasks over 2D geospatial environments using TensorFlow and TensorLayer on a computing cluster with NVIDIA Tesla K40M GPUs. Models are optimized using the Adam optimizer and mean square error. For mobile traffic forecasting, large-scale multi-service datasets from two European metropolitan areas were analyzed over 85 days. The data included traffic volume from devices connected to 792 and 260 antennas. Coordinate features were omitted as the data sources had fixed locations. The dataset includes traffic volume data from devices connected to 792 and 260 antennas in two European cities over 85 days. The antennas are non-uniformly distributed in urban areas, forming 2D point clouds. Traffic volume is measured in Megabytes and aggregated over 5-minute intervals for 38 different mobile services. More details can be found in Appendix G. Air quality forecasting performance is investigated using a public dataset comprising six air quality indicators collected by 437 air quality monitoring stations in China. The dataset includes 8,760 snapshots for each cluster, with data measured on an hourly basis. Experiments are conducted on two city clusters based on their geographic locations. Before feeding to the models, measurements for mobile service and air quality indicators are transformed into input channels of the point-cloud S. Coordinate features are normalized to the (0, 1) range. For baseline models requiring grid-structural input, point-clouds are transformed into grids using the Hungarian algorithm. Refer to Appendix G for more details. CloudLSTM is compared with baseline models like PointCNN and CloudCNN, which perform convolution over point-clouds for classification and segmentation. PointLSTM is introduced by replacing cells in ConvLSTM with X. The ratio of training plus validation, and test sets is 8:2. The study compares CloudLSTM with various baseline models like PointCNN and CloudCNN, which use convolution on point-clouds for classification and segmentation. PointLSTM is created by replacing cells in ConvLSTM with X. Other models such as MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++ are also discussed. The study discusses various models like 3D-CNN, LSTM, ConvLSTM, and PredRNN++ for mobile traffic prediction. The accuracy of CloudLSTM is evaluated using MAE and RMSE. Metrics like PSNR and SSIM are used to measure forecast fidelity and similarity with ground truth. Details about the metrics are provided in Appendix F. For mobile traffic prediction, neural networks forecast city-scale consumption over 30 minutes using past measurements. RNN-based models are evaluated over 3 hours. In air quality forecasting, models use half-day measurements to predict indicators for the following 12 hours. In air quality forecasting, RNN-based models outperform CNN-based models and MLP in predicting indicators for the following 12 hours. The models are also evaluated over 3 days with different numbers of neighboring points and attention mechanisms considered. The study compares RNN-based architectures with CNN-based models and MLP in air quality forecasting. The CloudLSTM, CloudRNN, and CloudGRU variants outperform other benchmark architectures, showing better performance in urban scenarios. CloudLSTM performs better than CloudGRU, which outperforms CloudRNN. The forecasting performance of CloudLSTM is not significantly affected by the number of neighbors (K). The CloudLSTM outperforms CloudGRU and CloudRNN in forecasting performance. The CloudLSTM's forecasting performance is not affected by the number of neighbors (K), suggesting the use of a small K to reduce model complexity. The attention mechanism improves forecasting by capturing better dependencies between input sequences and vectors in decoders. The effect of the attention mechanism has been confirmed in other NLP tasks. In Appendix H, long-term forecasting performance is evaluated for RNN-based architectures up to 36 time steps. MAE evolution is shown in Fig. 4, indicating reliable models for city 1. For city 2, low K may impact CloudLSTM's long-term performance before step 20, guiding K selection for different forecast lengths. All models are used for 12-step forecasting. The proposed CloudLSTMs show superior performance in 12-step air quality forecasting, outperforming ConvLSTM by up to 12.2% and 8.8% in MAE and RMSE. Lower K values result in better prediction performance, with CloudCNN consistently outperforming PointCNN. The CloudLSTM models demonstrate superior performance in modeling spatiotemporal point-cloud stream data, with lower K values yielding better prediction performance. CloudCNN consistently outperforms PointCNN, showcasing its effectiveness as a feature extractor. Performance evaluations for long-term forecasting up to 72 future time steps are conducted on RNN-based models. In experiments using strict variable-controlling methodology, D-Conv significantly improves performance in LSTM, ConvLSTM, PredRNN++, PointLSTM, and CloudLSTM models. CloudRNN and CloudGRU are inferior to CloudLSTM. Attention CloudLSTM shows improved effects compared to CloudLSTM. CloudRNN, CloudGRU, and CloudLSTM are compared, with CloudLSTM being superior. The attention mechanism in CloudLSTM does not have a significant impact. The core operator, RNN structure, and attention are ranked by their contribution. CloudLSTM is introduced as a neural model for spatiotemporal forecasting with pointcloud data streams, utilizing the DConv operator for spatial feature learning and maintaining permutation invariance. The DConv operator predicts values and coordinates of each point, adapting to changing spatial correlations. It can be combined with various RNN models, Seq2seq learning, and attention mechanisms efficiently using a standard 2D convolution operator. The input and output of DConv are 3D tensors, and it finds the set of top K nearest neighbors for each point. The DConv operator predicts values and coordinates of each point by finding the set of top K nearest neighbors for each point. It transforms the input into a 4D tensor and applies the sigmoid function to the output map for translation. The DConv operator reshapes the output map and applies the sigmoid function to enable translation into a standard convolution operation. The complexity is analyzed by separating the operation into finding neighboring sets and performing weighting computations. The DConv operator reshapes the output map and applies the sigmoid function to enable translation into a standard convolution operation. The complexity of finding K nearest neighbors for one point is O(K \u00b7 L log N) using KD trees. The overall complexity of computing one feature of the output point is O((H + L) \u00b7 K), similar to a vanilla convolution operator. The DConv operator introduces complexity by searching for K nearest neighbors for each point, with normalization of coordinates enabling transformation invariance with shifting and scaling. This is represented by positive scaling coefficient A and offset B. The DConv operator introduces complexity by searching for K nearest neighbors for each point, with normalization of coordinates enabling transformation invariance. The proposed CloudLSTM model combines with an attention mechanism, denoted as H j en and H i de for encoder and decoder states. The context tensor for state i at the encoder is represented by a score function e i,j. The proposal is compared against baseline models MLP and CNN. In this study, the proposal is compared against baseline models MLP, CNN, 3D-CNN, DefCNN, LSTM, ConvLSTM, and PredRNN++ frequently used in mobile traffic forecasting and time series forecasting. The PredRNN++ is considered the state-of-the-art architecture for spatiotemporal forecasting on grid-structural data, outperforming other models like ConvLSTM. CloudRNN and CloudGRU share a similar Seq2seq architecture with CloudLSTM but do not use the attention mechanism. The detailed configuration and number of parameters for each model are shown in Table 3. In Table 3, the configuration and parameter count for each model in the study are detailed. ConvLSTM, PredRNN++, and PointLSTM used 2 layers like CloudLSTMs. 3x3 filters were used for a receptive field of 9, equivalent to K=9 in CloudLSTMs. PredRNN++ has a unique structure compared to other Seq2seq models. CloudLSTM with 36 channels and K=3 was also considered. The study details different architectures of Seq2seq models, including 2-stack Seq2seq CloudLSTM with varying K values and attention mechanisms. The models are optimized using the MSE loss function and evaluated using MAE, RMSE, PSNR, and SSIM metrics. The forecast value of the h-th air quality indicator at antenna/monitoring station n at time t is evaluated using MAE, RMSE, PSNR, and SSIM metrics. PSNR is defined as 20 log v max (t) \u2212 10 log 1 where \u00b5 v (t) and v max (t) are the average and maximum traffic recorded for all services/quality indicators. Coefficients c 1 and c 2 are used to stabilize the fraction in the presence of weak denominators. Figure 5 shows anonymized locations of antenna sets in two cities, with data collected through deep packet inspection at the packet gateway using proprietary traffic classifiers. Operator and service details are not disclosed for confidentiality reasons. The data collection process was conducted under the supervision of the national privacy agency and in compliance with regulations. The dataset used for the study only includes anonymized mobile service traffic information at the antenna level, without personal subscriber information. The dataset used for the study is fully anonymized and does not contain personal subscriber information. The raw data cannot be made public due to a confidentiality agreement. The analysis includes 38 different services, with a power law observed in the demands generated by individual mobile services. Streaming is the dominant service. The measurement campaign duration is shown in Fig. 6, confirming a power law in demands from mobile services. Streaming is the top traffic type, accounting for nearly half of total consumption. Other services like web, cloud, social media, and chat also contribute significantly. Gaming only makes up 0.5% of the demand. The air quality dataset includes information from 43 cities in China, with over 2.8 million air quality records collected by Microsoft Research. The air quality dataset contains information from 43 cities in China, with 2,891,393 records from 437 monitoring stations over a year. Stations are divided into two clusters, A with 274 stations and B with 163. Missing data was filled using linear interpolation. The dataset can be accessed at https://www.microsoft.com/en-us/research/project/urban-air/. The performance of Attention CloudLSTMs in forecasting accuracy for individual mobile services is evaluated over 36 steps. MAE evaluation on a service and category basis shows similar performance in both cities. Services with higher traffic volume, like streaming and cloud, have higher prediction errors due to more frequent fluctuations in traffic evolution. The performance of Attention CloudLSTMs in forecasting accuracy for individual mobile services is evaluated over 36 steps. Services with higher traffic volume, like streaming and cloud, have higher prediction errors due to more frequent fluctuations. MAE for long-term air quality forecasting on city clusters shows error growth with time for all models. Larger K in CloudLSTM can significantly improve robustness. The CloudLSTM with different K shows improved robustness, with larger K leading to slower MAE growth over time. Evaluation of mobile traffic forecasting includes visualizing hidden features to gain insights into the model's learned knowledge. The scatter distributions of hidden states in CloudLSTM and Attention CloudLSTM at both stacks, along with input snapshots from City 2. Each scatter subplot shows features extracted at a higher level, displaying direct spatial correlations. In City Cluster A, RNN-based models show improved NO2 forecasting with stack 2 features capturing direct spatial correlations. Visual comparisons in Fig. 11 and 12 highlight the superior performance of (Attention) CloudLSTMs in predicting air quality, converting point-clouds into heat maps for better visualization. The proposed architectures of Attention CloudLSTMs capture trends in point-cloud streams, ensuring high long-term visual fidelity compared to other models. The DConv in CloudLSTM uses Sigmoid functions to regulate coordinate features, bringing distant points closer for better computation. Multiple DConv stacked via LSTM structure further enhances CloudLSTM's performance. The proposed CloudLSTM architecture uses DConv with Sigmoid functions to bring distant points closer for better computation. Stacking multiple DConv via LSTM structure enhances the model's representability, allowing each input point to refine its position at each time step. Outliers in the air quality dataset are identified using DBSCAN, demonstrating the model's ability to forecast with outlier points. The CloudLSTM architecture utilizes DBSCAN to identify outliers in the air quality dataset. It achieves the lowest prediction error compared to other models, with CloudCNN performing the best among CNN-based models. The model's robustness to outliers is further explored through outlier point analysis. The CloudLSTM architecture, utilizing the DConv operator, shows superior forecasting performance compared to other models like CNN, 3D-CNN, DefCNN, and PointCNN. The robustness of CloudLSTM to outliers is investigated by creating a toy dataset with randomly selected weather stations, including outliers moved away from the center by varying distances on both axes. The CloudLSTM architecture, using the DConv operator, outperforms other models like CNN, 3D-CNN, DefCNN, and PointCNN in forecasting. To test its robustness to outliers, a toy dataset with weather stations, including outliers moved away from the center by varying distances, was created. The positions of weather stations after moving by different distances are shown in figures. The CloudLSTM performs well in forecasting over inliers and outliers, achieving significantly better performance than its counterpart. CloudLSTM performs well in forecasting over inliers and outliers, achieving significantly better performance than PointLSTM. Comparison with simple baselines using MLPs and LSTMs with different input forms also shows CloudLSTM's superiority. The models use K nearest neighbors for prediction, with K values ranging from 1 to 100. CloudLSTM outperforms MLPs and LSTMs on the air quality dataset. The number of neighbors K impacts the model's receptive field, balancing local and global spatial dependencies. The results show that the number of neighbors K does not significantly affect the performance of the baseline models. CloudLSTM, which combines local spatial dependencies with global spatial dependencies, outperforms the simple baselines. Seasonal information in mobile traffic data can be utilized for better forecasting, but using data spanning multiple days directly is impractical. To improve forecasting performance, utilizing seasonal information in mobile traffic data is essential. However, directly inputting data spanning multiple days is impractical due to the large number of data points involved. To address this issue, concatenating 30-minute sequences with a sub-sampled 7-day window can efficiently capture seasonal information. By concatenating 30-minute sequences with a sub-sampled 7-day window, seasonal information is efficiently captured for forecasting mobile traffic data. Experimental results show improved performance of forecasting models when incorporating the 7-day window, indicating better learning of periodic information and reduced prediction errors. The model learns periodic information efficiently by concatenating 30-minute sequences with a 7-day window, reducing prediction errors. However, this increases input length and model complexity. Future work aims to fuse seasonal information more efficiently with minimal complexity increase."
}