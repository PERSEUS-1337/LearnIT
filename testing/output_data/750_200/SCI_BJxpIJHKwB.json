{
    "title": "BJxpIJHKwB",
    "content": "Few shot image classification involves learning a classifier from limited labeled data. Generating classification weights is common in meta-learning for few shot image classification. However, it is challenging to generate exact and universal weights for diverse query samples with very few training samples. The Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) addresses this issue by generating different classification weights for each query sample, allowing them to attend to the entire support set. AWGIM introduces novel contributions in few shot learning by generating adaptive classification weights for query samples through information maximization. This approach achieves state-of-the-art performance on benchmark datasets, showcasing the effectiveness of the method. AWGIM introduces novel contributions in few shot learning by generating adaptive classification weights for query samples through information maximization. The method achieves state-of-the-art performance on benchmark datasets, demonstrating its effectiveness in enabling deep models to learn from very few samples. Few shot learning aims to help deep models learn from limited samples. Meta learning is a popular approach for few shot problems, where models extract knowledge across tasks to quickly adapt to new ones. Different meta learning methods include gradient-based and metric-based approaches. Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) is introduced in this work to address the challenge of generating classification weights for different tasks conditioned on limited labeled data. This method aims to improve the effectiveness of weights generation in few shot learning. AWGIM introduces Attentive Weights Generation for few shot learning to improve classification weights generation by focusing on query samples specifically. Simple cross attention between query samples and support set is insufficient, leading to a proposal to maximize mutual information between generated weights and query data. AWGIM proposes maximizing mutual information between generated weights and query data to improve classification in few shot learning. It introduces Variational Information Maximization and eliminates inner updates without performance loss. AWGIM achieves state-of-the-art performance on benchmark datasets and undergoes detailed component analysis. The AWGIM model achieves state-of-the-art performance on benchmark datasets through maximizing mutual information between generated weights and query data. Detailed analysis validates the contribution of each component in AWGIM. Previous works in few-shot learning include meta-learning approaches like optimal initialization for all tasks and learning transformations for activations of each layer. The meta-learner LSTM optimizes few-shot classification tasks by learning transformations for activations of each layer. Various methods involve learning similarity metrics between query and support samples, considering spatial information or local image descriptors, and generating classification weights directly. In few-shot classification tasks, different methods have been explored to generate classification weights directly, such as linear combinations of weights for base and novel classes, using activations of a trained feature extractor, and generating \"fast weights\" from loss gradients. Some methods do not consider generating different weights for query examples or maximizing mutual information. Some methods for few-shot classification include using generative models to generate more data, utilizing closed-form solutions directly, and integrating label propagation on a transductive graph. Attention mechanism has shown success in computer vision and natural language processing by modeling interactions between queries and key-value pairs. In this work, attention is used for few-shot image classification by maximizing mutual information. The approach involves both self and cross attention to encode task and query-task information, similar to Attentive Neural Processes. This contrasts with regression focused on a stochastic process. In contrast to regression focused on a stochastic process, attention is utilized for few-shot image classification by maximizing mutual information. Mutual information measures the decrease of uncertainty in one random variable when another is known, with its maximum value achieved when y is a deterministic function of x. Mutual information is maximized when y is a deterministic function of x. It has various applications such as in Generative Adversarial Networks and self-supervised learning. The attentive path equips query samples with task knowledge through an attention mechanism. The weight generator generates classification weights specific for x. The weight generator generates classification weights specific for x, which are used to predict class labels and reconstruct inputs. The model aims to maximize mutual information and generate sensitive classification weights for different query samples. The problem formulation, proposed model, objective function, and theoretical analysis are provided in subsequent sections. The proposed model in Sec. 3.3 maximizes mutual information between variables. Formulated under episodic training paradigm, it involves N-way K-shot tasks with support and query sets. Meta-loss is estimated on the query set during meta-training to optimize. During meta-training, the model optimizes the meta-loss on the query set Q to adapt quickly to novel tasks. The proposed approach follows a general framework for generating classification weights. The proposed approach follows a general framework for generating classification weights using a feature extractor to output image feature embeddings. Latent Embedding Optimization (LEO) is a method related to generating classification weights, where a latent code z is generated by h conditioned on a support set S. Classification weights w can be decoded from z with l. LEO optimizes classification weights by generating a latent code z from a support set S using a feature extractor h. The weights w are decoded from z with a generating function l, avoiding high-dimensional updates in the inner loop. LEO optimizes classification weights by generating a latent code z from a support set S using a feature extractor h. The weights w are decoded from z with a generating function l, avoiding high-dimensional updates in the inner loop. The key difference between LEO and AWGIM is that LEO does not require inner updates to adapt the model, while AWGIM is a feedforward network trained to maximize mutual information for better task adaptation. The proposed method involves using a feature extractor to process images in a sampled task, generating d-dimensional vectors. Two paths, contextual and attentive, encode task context and query samples. The outputs are concatenated for classification weight generation. The encoding process involves two paths - contextual and attentive. The contextual path focuses on learning representations for the support set using a multi-head self-attention network. The outputs contain rich information about the task for later use in generating classification weights. The contextual path in the encoding process utilizes a multi-head self-attention network to learn representations for the support set, resulting in richer information for generating classification weights. The attentive path is introduced to address the issue of sub-optimal weights generated solely from the support set, providing adaptation to different query samples. The attentive path introduces adaptive classification weights by allowing individual query examples to attend to task context, improving adaptation to different query samples. This is achieved through a new multi-head self-attention network on the support set, encoding global task information distinct from the contextual path's focus on generating classification weights. The self-attention network in the contextual path focuses on generating classification weights, while the cross attention network in the attentive path produces task-aware representations for query samples. Sharing the same self-attention networks may limit the expressiveness of learned representations in both paths. Multi-head attention with h heads is used in both paths to generate different sets of queries, keys, and values. The contextual path focuses on classification weights, while the attentive path generates task-aware representations for query samples using multi-head attention with h heads. The two paths use different sets of queries, keys, and values to learn comprehensive representations. X cp\u2295ap combines latent representations for support in each query sample. The classification weights are decoded from concatenated tensors X cp\u2295ap, with each query sample having its own latent representations. The weights are adaptive to individual query samples and task-context, following a Gaussian distribution. The weights generator outputs distribution parameters, and the sampled weights are represented as W. The mean value of K classification weights is computed to reduce complexity. The classification weights are decoded from concatenated tensors X cp\u2295ap, with each query sample having its own latent representations. The weights are adaptive to individual query samples and task-context, following a Gaussian distribution. The weights generator outputs distribution parameters, and the sampled weights are represented as W. The mean value of K classification weights is computed to reduce complexity. The prediction for query data is computed by XW f inalT, and for support data by X s W f inalT. Two decoders r 1 reconstruct X cp and X ap respectively using the generated weights W. The weights generator g produces weights W, which are used by decoders r1 and r2 to reconstruct Xcp and Xap. Reconstruction is used as an auxiliary task for analysis. The classification weights are encoded with attentive and contextual paths for direct use. The weights generator produces weights used for reconstruction tasks. However, the generated classification weights are not sensitive to different query samples. To address this, the proposal is to maximize mutual information between generated weights and support/query data. To address the limitation of insensitivity of generated weights to query samples, the proposal is to maximize mutual information between weights and support/query data. This involves using Variational Information Maximization to compute a lower bound of the mutual information. Maximization is used to compute the lower bound of mutual information by approximating the true posterior distribution. The objective function involves maximizing the log likelihood of labels for support and query data. The objective function involves maximizing the log likelihood of labels for support and query data by minimizing the cross entropy between prediction and ground-truth. Gaussian distributions are assumed for p \u03b8 (x|w) and p \u03b8 (x|w), with r 1 and r 2 approximating their means. The loss function for training the network involves reconstructing x cp and x ap with L2 loss. The loss function for training the network involves deciding weightage for log likelihood and reconstruction of x cp and x ap with L2 loss. Hyper-parameters \u03bb 1 , \u03bb 2 , \u03bb 3 trade-off different terms. The generated classification weights carry information about support data and query sample. In LEO, inner update loss is cross entropy on support data. Merging inner update into outer loop results in loss as the summation of first two terms in Equation 15. Weight generation in LEO does not involve specific query. The encoding process in contextual path results in computational complexity O((N K) 2) due to self-attention. The computational complexity of attentive path is O((N K) 2 + |Q|(N K)). The total complexity is O((N K) 2 + |Q|(N K)), with the value of (N K) 2 usually being negligible in few-shot learning problems. AWGIM reduces computational overhead by avoiding inner updates, leading to significant reductions in training and inference time. Empirical evaluation on miniImageNet and tieredImageNet datasets shows its performance compared to other methods. We conduct experiments on miniImageNet and tieredImageNet datasets, subsets of ILSVRC-12 dataset, to compare with other methods. miniImageNet has 100 classes with 600 images each, while tieredImageNet has 608 classes and 779,165 images selected from 34 higher level nodes in ImageNet hierarchy. The miniImageNet dataset consists of 608 classes and 779,165 images selected from 34 higher level nodes in the ImageNet hierarchy. The image features in LEO are used, represented by a 640 dimensional vector for each image. For N-way K-shot experiments, N classes are randomly sampled from the meta-training set, each containing K support samples and 15 query samples. 5-way 1-shot and 5-shot models are trained on two datasets. During meta-testing, 600 N-way K-shot tasks are sampled from the meta-testing set, and the average accuracy for the query set is reported. TensorFlow is used for implementation, with feature embeddings of dimension 640 and 128 for dh. The number of heads in the attention module is set to 4. The code for the method implementation will be available, with feature embeddings of dimension 640 and 128 for dh. The attention module has 4 heads, and various models like Meta LSTM, Prototypical Nets, Relation Nets, SNAIL, TPN, and MTL have been compared based on their performance metrics. The accuracy comparison of different approaches on tieredImageNet shows that our AWGIM model outperforms other models with an accuracy of 63.12% in 5-way 1-shot tasks. The results of various models on tieredImageNet meta-testing set are compared, with MetaOptNet Resnets achieving the highest accuracy of 59.91% in 5-way 1-shot tasks and 72.85% in 5-way 5-shot tasks. The model is optimized with weight decay and trained for 50,000 iterations. The learning rate for 5-way 1-shot is 0.0002 and for 5-way 5-shot is 0.001, decayed by 0.2 every 15,000 iterations. Batch size is 64 for 5-way 1-shot and 32 for 5-way 5-shot. The model is trained on meta-training and meta-validation sets with fixed hyper-parameters, compared with state-of-the-art methods on two datasets. Several state-of-the-art methods like MAML, Prototypical Nets, and Relation Nets are evaluated on tieredImageNet. Results of Dynamic on miniImageNet with WRN-28-10 as the feature extractor are reported. The backbone network structure of the feature extractor is included for reference. Results on miniImageNet and tieredImageNet are shown in Tables 1 and 2, categorizing methods into metric-based, gradient-based, and graph-based meta learning categories. The methods in Tables 1 and 2 are categorized into metric-based, gradient-based, and graph-based meta learning categories. The bottom part discusses classification weights generation approaches, including Dynamic, Prediction, DAE-GNN, LEO, and AWGIM. AWGIM outperforms all other methods in the tables and shows competitive performance on tieredImageNet and miniImageNet. All methods use WRN-28-10 as the backbone network for comparison. AWGIM outperforms LEO in all settings for classification weights generation on tieredImageNet and miniImageNet using WRN-28-10 as the backbone network. Detailed analysis and comparison with LEO are provided in Table 3. AWGIM outperforms LEO in classification weights generation on tieredImageNet and miniImageNet using WRN-28-10. Detailed analysis on AWGIM is shown in Table 3, with results of LEO Rusu et al. (2019) included for reference. Different generators are studied, with \"Generator conditioned on S only\" achieving similar or slightly better results than \"Generator in LEO\" without inner update. The \"Generator conditioned on S only\" achieves similar or slightly better results than \"Generator in LEO\" without inner update, indicating that self-attention is comparable to relation networks in modeling task-context. By replacing attention modules with 2-layer MLPs in the \"MLP encoding\" approach, task-contextual information can still be encoded effectively. In the \"MLP encoding\" approach, one MLP is used for support set and another for query samples. Even without attention, \"MLP encoding\" achieves accuracy close to LEO. Ablation analysis shows that maximizing information is crucial, as setting \u03bb 1 = \u03bb 2 = \u03bb 3 = 0 significantly drops performance. In the \"MLP encoding\" approach, one MLP is used for support set and another for query samples. Even without attention, \"MLP encoding\" achieves accuracy close to LEO. Ablation analysis shows that maximizing information is crucial, as setting \u03bb 1 = \u03bb 2 = \u03bb 3 = 0 significantly drops performance. First, \u03bb 1 , \u03bb 2 and \u03bb 3 are all set to be 0. The accuracy is similar to \"generator conditioned on S only\", indicating that the generated classification weights are not fitted for different query samples. \u03bb 1 = 0 affects the performance noticeably, suggesting that support label prediction is critical for information maximization. The study focuses on the importance of support label prediction for information maximization in AWGIM. Classification weights are tailored for each query sample, and shuffling them between samples and classes is done to analyze their adaptability. The results of shuffling the weight tensor are presented in Table 3. The study analyzes the adaptability of classification weights in AWGIM by shuffling them between samples and classes. Results show that shuffling between classes degrades accuracy in limited support data scenarios, while shuffling within classes has minimal impact. With more labeled data, shuffling methods yield similar results, both worse than the original weights. In this work, Attentive Weights Generation via Information Maximization (AWGIM) is introduced for few-shot image classification. The study shows that larger support sets lead to more diverse and specific classification weights for each query sample in 5-way 5-shot experiments. The generated weights are optimized for each query example by two encoding paths. AWGIM is introduced for few-shot image classification, generating optimal classification weights for each query sample. It maximizes mutual information between generated weights and query, support data. This is the first work using mutual information techniques for few-shot learning, showing state-of-the-art performance on benchmark datasets. The multi-head attention in AWGIM is used to encode global task information to support samples for a N-way K-shot task. The attentive path employs attention to compute cross attention between query and context-aware support samples, generating optimal classification weights for each query sample. AWGIM utilizes multi-head attention to encode global task information for N-way K-shot tasks. The model generates optimal classification weights for each query sample by computing cross attention between query and context-aware support samples. During meta-training, weights are sampled from a Gaussian distribution with diagonal covariance for few shot regression tasks. During meta-training, the number of classes N is set to 1 and the cross entropy loss is adapted to mean square error. Data points (x, y) are used as inputs to AWGIM to generate weight and bias parameters for a three layer MLP with hidden dimension 40. Few shot regression tasks are constructed as either sinusoidal or linear regression tasks. Multi-head attention is replaced with single-head attention in the two paths for 5-way 1-shot and 5-way 5-shot experiments on miniImageNet dataset, showing improvement in results. The results of 5-way 1-shot and 5-way 5-shot experiments on miniImageNet dataset show that multi-head attention improves performance. Single head attention struggles with extremely scarce data. AWGIM converges faster than LEO in terms of convergence speed during meta-training on 5-way 1-shot miniImageNet. AWGIM converges faster than LEO during meta-training on 5-way 1-shot miniImageNet. Inference time of AWGIM shows minimal computational overhead compared to using \"MLP encoding\" with time complexity O(N K + |Q|). Experiments on miniImageNet with batch size 64 and 100 batches processed show self-attention usage. The experiments show that the usage of self-attention and cross attention in AWGIM incurs negligible overhead compared to MLP encoding. The values of N, K, and |Q| are relatively small, allowing for fast processing by the GPU. Classification weights are visualized using t-SNE, with 10,000 weight vectors generated from 400 tasks in a meta-validation set. In a 5-way 1-shot miniImageNet experiment, 10,000 weight vectors are visualized for classification. The generator's inputs are also plotted for comparison. The decoded weights for each class are closer together, and the generator can adapt weights for different query samples. The generator can adapt weights for different query samples from different classes within one task, as shown in the t-SNE visualization in Figure 4. Blue and red dots represent the classification weights for two query samples in the same task."
}