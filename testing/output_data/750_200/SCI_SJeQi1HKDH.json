{
    "title": "SJeQi1HKDH",
    "content": "Animals develop new skills through environmental interaction and social influence. A model incorporating social influence in reinforcement learning is proposed. A metric to measure policy distance is defined to quantify uniqueness. The Interior Policy Differentiation (IPD) algorithm encourages agents to learn unique policies while solving tasks. The Interior Policy Differentiation (IPD) algorithm encourages agents to learn unique policies while solving tasks, leading to performance improvement and a collection of distinct behaviors. The paradigm of Reinforcement Learning involves learning through interaction with the environment to maximize rewards, inspired by cognition and animal studies. Biodiversity and skill development are crucial for species evolution and continuation. Behavioral diversity is essential for species evolution and continuation. Previous works have focused on encouraging diversity in Reinforcement Learning (RL) through designing interactive environments with rich and diverse features. This approach has shown that agents can learn different skills, such as locomotion, using standard RL algorithms. However, designing complex environments manually is labor-intensive, and diversity is limited by obstacle classes. In this work, the focus is on improving the diversity of RL agents while maintaining their ability to solve tasks. The inspiration is drawn from Social Influence in animal society, aiming to increase behavioral diversity by motivating agents to explore beyond just maximizing rewards. The concept of social influence in reinforcement learning is inspired by animal society. Agents are encouraged to differentiate their actions from others while maximizing rewards. This is implemented through social uniqueness to increase diversity among RL agents. In reinforcement learning, social influence is used to encourage agents to differentiate their actions from others while maximizing rewards. This is achieved through social uniqueness motivation and is considered as a constrained optimization problem. A rigorous policy distance metric is defined to compare agent similarity, and an optimization constraint is developed using this metric to provide immediate feedback in the learning process. Interior Policy Differentiation (IPD) is introduced as a novel method to promote diversity among RL agents. Interior Policy Differentiation (IPD) is a novel method proposed for constrained policy optimization in reinforcement learning. It encourages agents to perform well in tasks while also taking different actions from other agents. This method has been benchmarked on locomotion tasks, showing the ability to learn diverse and well-behaved policies using the Proximal Policy Optimization (PPO) algorithm. Intrinsic motivation methods like VIME and curiosity-driven methods add intrinsic rewards to RL algorithms to encourage exploration and tackle sparse reward problems. Intrinsic reward methods proposed by Pathak et al. (2017) and Burda et al. (2018a) use prediction errors of neural networks as rewards. Burda et al. (2018b) introduced Random Network Distillation (RND) to quantify intrinsic reward, while Liu et al. (2019) proposed Competitive Experience Replay (CER) using two actors and a centralized critic. Competitive Experience Replay (CER) by Liu et al. (2019) utilizes two actors and a centralized critic, defining intrinsic rewards based on state coincidence. The challenge lies in balancing external rewards from the environment with intrinsic rewards from different heuristics. The Task-Novelty Bisector (TNB) learning method by Zhang et al. (2019) aims to optimize both types of rewards simultaneously. The Task-Novelty Bisector (TNB) learning method aims to optimize extrinsic and intrinsic rewards simultaneously, updating the policy based on the angular bisector of the two gradients. However, this approach requires additional neural networks for evaluating novelty, leading to increased computation expenses. The Distributed Proximal Policy Optimization (DPPO) method introduced by Heess et al. enables agents to learn complex locomotion skills in diverse environments. Their policy learned impressive skills effective in traveling terrains and obstacles, showing that rich environments can encourage different locomotion behaviors. However, designing such environments requires extra manual efforts. The research by Such et al. (2018) demonstrates that different RL algorithms can lead to varying policies for the same task. Policy gradient algorithms tend to converge to a local optimum in Pitfall, while off-policy and value-based algorithms learn more sophisticated strategies. In contrast, this paper focuses on learning different policies with a single algorithm and avoiding local optima. (Schulman et al., 2015; Kurutach et al., 2018) The paper discusses learning different policies through a single algorithm and avoiding local optima. Kurutach et al. (2018) use an ensemble of deep neural networks to maintain model uncertainty. A metric is defined to measure policy differences, laying the foundation for the proposed algorithm. Learned policies are denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}, with \u03b8 i representing parameters and \u0398 the parameter space. The paper introduces a metric space for measuring policy differences, using the Total Variance Divergence. This metric is defined on the parameter space \u0398, ensuring identity, symmetry, and triangle inequality properties are satisfied. The Total Variance Divergence (TVD) is used as a metric to measure policy differences in a metric space defined on parameter space \u0398. The goal is to maximize the uniqueness of a new policy by calculating the TVD based on Monte Carlo methods. The calculation of Total Variance Divergence (TVD) for policy differences is based on Monte Carlo estimation, which can be challenging in continuous state cases due to the difficulty in obtaining enough samples efficiently. The domain of the sampling distribution \u03c1(s) is denoted as S, while the domain of \u03c1 \u03b8 (s) is denoted as S \u03b8 \u2282 S. In finite time horizon problems, \u03c1(s|s \u223c \u03b8) represents the probability of reaching state s under policy \u03b8. In finite time horizon problems, the domain of possible states can be divided to improve sample efficiency. Approximating \u03c1(s|s \u223c \u03b8) allows for the use of different policies as long as the state space is similar. Adding noise to \u03b8 ensures Condition 1 holds, making \u03c1(s|s \u223c \u03b8) a viable choice. In practice, to satisfy the properties in Definition 1, we must sample s from S \u03b8 \u222a S \u03b8j. By adding noise on \u03b8, Condition 1 is ensured, and sufficient exploration in training can eliminate the last term in the objective function of policy differentiation. In training, enabling sufficient exploration and initialization of \u03b8 can eliminate the last term related to domain S \u03b8. The estimation of \u03c1 \u03b8 (s) using a single trajectory \u03c4 is unbiased. Developing an efficient learning algorithm involves maximizing the expectation of cumulative rewards in the traditional RL paradigm. The objective in reinforcement learning is to maximize cumulative rewards by considering both the reward from the primal task and policy uniqueness. Previous approaches combine these rewards with a weight parameter to enhance behavioral diversity among agents. In reinforcement learning, the objective is to maximize cumulative rewards by considering both the reward from the primal task and policy uniqueness. The intrinsic reward is weighted with a parameter \u03b1, which affects the balance between intrinsic and extrinsic rewards. The formulation of the intrinsic reward and the selection of \u03b1 are crucial for the agent's success in solving tasks. To address these challenges, inspiration is drawn from the motivating factor of social uniqueness in human behavior. To address challenges in reinforcement learning, the formulation of intrinsic reward is crucial. Drawing inspiration from social uniqueness, the multi-objective optimization problem is transformed into a constrained optimization problem with a threshold for minimal uniqueness. This approach acts as a penalty method to optimize task-solving success. The constrained optimization problem in Eq.(7) is tackled using Interior Point Methods (IPMs) instead of the Task Novel Bisector (TNB) approach. The difficulty lies in selecting the penalty coefficient \u03b1, which is addressed by Zhang et al. (2019). In this work, the constrained optimization problem is solved using Interior Point Methods (IPMs) instead of the Task Novel Bisector (TNB) approach. The proposed RL paradigm addresses the computational challenges and numerical instability of directly applying IPMs, especially when \u03b1 is small. In the proposed RL paradigm, the learning process is influenced by peers, allowing for a more natural way to handle computationally challenging and numerically unstable situations when \u03b1 is small. By bounding collected transitions within a feasible region using previously trained policies, new agents are terminated if they step outside this region, ensuring all samples during training are valid. During the training process, new agents are terminated if they step outside the feasible region bounded by previously trained policies. This ensures that all collected samples are valid and less likely to appear in previous policies, resulting in a new policy with sufficient uniqueness. This approach eliminates the need to deliberate on the trade-off between intrinsic and extrinsic rewards, making the learning process more robust and free from objective inconsistency. The method, named Interior Policy Differentiation (IPD), is demonstrated on the MuJoCo environment in OpenAI Gym. Three locomotion environments are tested: Hopper-v3, Walker2d-v3, and HalfCheetah-v3. All environment parameters are set to default values in the experiments. In experiments on MuJoCo environments in OpenAI Gym, the method Interior Policy Differentiation (IPD) is tested on Hopper-v3, Walker2d-v3, and HalfCheetah-v3. Different policies can be generated by selecting different random seeds before training. The proposed method is based on PPO (Schulman et al., 2017) and can improve behavior diversity. In this work, the proposed method is demonstrated based on PPO and compared with TNB and WSR approaches for combining task goals and uniqueness motivation. The uniqueness metric is used directly in learning new policies without reshaping. Implementation details are provided in Appendix D. Our method utilizes the uniqueness metric directly in learning new policies without reshaping. We compare it with TNB and WSR approaches in the same experimental settings, training 10 different policies sequentially. Qualitative results are shown in Fig.2, visualizing agent motion over time. The method visualizes agent motion by drawing frames representing pose at different time steps, with intervals proportional to velocity. The visualization starts at the beginning of each episode, showing acceleration and motion patterns clearly. Experimental results show uniqueness and performance of policies. The experimental results in Fig. 3 demonstrate the performance and uniqueness of policies. Our method outperforms others in Hopper and HalfCheetah tasks. In Walker2d, both WSR and our method improve policy uniqueness, but none surpass PPO's performance. Detailed comparisons are shown in Table 1, with performance and reward curves in Fig. 5 and Fig. 6. More detailed results are provided in Fig. 7 in Appendix C. The performance of trained policies and their reward curves are shown in Fig.5 and Fig.6 in Appendix C. Fig.7 in Appendix C provides detailed results on uniqueness. Success rate is used as a metric to compare different approaches, with a policy considered successful if it outperforms the baseline of policies trained without social influences. Our method consistently outperforms the baseline during training, ensuring reliable performance. Significant improvements were observed in the Hopper and HalfCheetah environments, indicating successful policy learning without sacrificing performance. Our method shows significant performance improvements in the Hopper and HalfCheetah environments by preventing policies from getting stuck in local minima, leading to enhanced exploration and better overall performance in reinforcement learning. Our method enhances traditional RL by promoting exploration to prevent policies from getting stuck in local minima, leading to improved performance in the HalfCheetah environment. In our method, there is no explicit termination signal in default settings. The agent initially acts randomly, leading to repeat samples and high control costs. It can receive termination signals from peers to avoid wasteful random actions. The agent learns to terminate early to save control costs, then adjusts behavior for higher rewards. The agent learns to terminate early to avoid control costs by imitating previous policies, then adjusts behavior for higher rewards. The learning process is seen as an implicit curriculum, with the difficulty of finding a unique policy increasing as the number of learned policies with social influence grows. Later policies must deviate from previous solutions. Ablation study results on performance changes under different scales of social influence are shown in Fig. 4. In this work, an efficient approach called Interior Policy Differentiation (IPD) is developed to motivate RL to learn diverse strategies inspired by social influence. The performance decrease is more obvious in Hopper environment due to its limited 3-dimensional action space, restricting the number of possible diverse policies that can be discovered. The method defines the distance between policies and introduces the concept of policy uniqueness, treating the problem as a constrained optimization problem. Our proposed method, Interior Policy Differentiation (IPD), draws key insights from Interior Point Methods to address the problem of policy uniqueness in reinforcement learning. Experimental results show that IPD can learn various well-behaved policies, helping agents avoid local minimums and facilitating implicit curriculum learning. The approach guarantees certain properties and optimizes policy uniqueness in different environments. The results show that TNB and WSR can sometimes achieve higher uniqueness than the proposed method, but direct optimization of uniqueness may lead to a decrease in task performance. Hyper-parameter tuning and reward shaping are crucial to balance this trade-off. The deterministic part of policies is used for calculating DTV, removing Gaussian noise in PPO and utilizing a specific network structure. In the implementation details, the calculation of DTV involves using the deterministic part of policies in PPO by removing Gaussian noise on the action space. A MLP with 2 hidden layers is used for actor models, with the first hidden layer having 32 units. Different unit numbers are tested for the second layer in an ablation study, with 10, 64, and 256 units chosen based on success rate, performance, and computation expense considerations. In the implementation details, the calculation of DTV involves using the deterministic part of policies in PPO. A MLP with 2 hidden layers is used for actor models. Different unit numbers are tested for the second layer in an ablation study. Taking into consideration success rate, performance, and computation expense, the training timesteps are fixed. The threshold selection in the proposed method allows for flexible control of policy uniqueness by adjusting the constraint threshold. Different thresholds lead to different policy behaviors. In the proposed method, the threshold selection allows for flexible control of policy uniqueness by adjusting the constraint threshold. Different thresholds result in varied policy behaviors, affecting the performance of the learning algorithm. The use of cumulative uniqueness as constraints focuses on long-term differences rather than forcing every single action to be different. Testing with different threshold values shows varying agent performance. In the proposed method, the threshold selection allows for flexible control of policy uniqueness by adjusting the constraint threshold. Testing with different threshold values shows varying agent performance. The use of cumulative uniqueness as constraints focuses on long-term differences rather than forcing every single action to be different. Constraints can be applied after the first t timesteps for similar starting sequences. The WSR, TNB, and IPD methods are three approaches in constrained optimization. The optimization of policy is based on batches of trajectory samples and implemented with stochastic gradient descent. The Penalty Method considers constraints by putting them into a penalty term and solving the unconstrained problem iteratively. The Penalty Method and Feasible Direction Method are approaches in constrained optimization. The Penalty Method solves the unconstrained problem iteratively by considering constraints, while the Feasible Direction Method finds a direction that satisfies the constraints. The final solution in the Penalty Method heavily depends on the selection of a fixed weight term \u03b1. The Feasible Direction Method (FDM) considers constraints by finding a direction that satisfies them. The TNB method selects a direction based on gradients, with a fixed learning stride. The shape of the function g is crucial for TNB's success. Alternatively, a barrier term can be used in optimization. The shape of function g is crucial for the success of TNB optimization. A barrier term can be used with a small positive number \u03b1 to influence the objective minimally. Choosing a sequence of decreasing \u03b1 values can bring the solution closer to the primal objective. However, directly applying this method is computationally challenging and numerically unstable. The learning process is based on sampled transitions, and a more natural way to handle the optimization is by bounding the collected transitions in the feasible region. This is done by permitting previous trained policies and sending termination signals to new agents that step outside the feasible region. This ensures that all valid samples collected during the training process are inside the feasible region. During the training process, valid samples are kept inside the feasible region by terminating agents that step outside it. This results in a new policy with sufficient uniqueness, eliminating the need to balance intrinsic and extrinsic rewards. The learning process becomes more robust and objective inconsistency is no longer an issue. The pseudo code of IPD based on PPO is shown in Algorithm.1, with additions to the primal PPO algorithm highlighted in blue."
}