{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) is a variational-inference method that achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. However, as the number of Monte Carlo samples increases, the inference-network gradients break down. This issue can be addressed by removing high-variance score-function terms, resulting in the 'sticking-the-landing' IWAE (IWAE-STL). In this work, the authors argue that directly optimizing the proposal distribution in importance sampling is preferable to optimizing IWAE-type multi-sample objectives. They introduce an adaptive importance sampling framework called AISLE. The authors introduce an adaptive importance sampling framework called AISLE, which generalizes the RWS algorithm. AISLE admits IWAE-STL and IWAE-DREG as special cases, avoiding breakdown in IWAE-gradients. The work analyzes algorithms for variational inference to learn the generative model. In this work, algorithms for variational inference are analyzed to learn the generative model and construct a tractable variational approximation. The focus is on finding values for the model parameters and approximations that closely match the true distribution. The presentation focuses on a single latent representation-observation pair (z, x) to simplify notation. Parameters are not shared between the generative model and variational approximation. The setting covers amortised inference and discusses stochastic gradient-ascent algorithms for optimization. Two classes of stochastic gradient-ascent algorithms for optimizing \u03c8 := (\u03b8, \u03c6) using K Monte Carlo samples have been proposed. IWAE-DREG removes problematic score-function terms from the IWAE \u03c6-gradient, inducing bias. RWS algorithm optimizes separate objectives for \u03b8 and \u03c6 using self-normalized importance sampling. The RWS algorithm optimizes separate objectives for \u03b8 and \u03c6 using self-normalized importance sampling with K particles. It is an adaptive importance-sampling approach that iteratively improves its proposal distribution while optimizing \u03b8 via stochastic approximation. The RWS \u03c6-gradients do not degenerate as K \u2192 \u221e, unlike IWAE. Tucker et al. (2019) found that RWS may break down due to not optimizing a joint objective for \u03b8 and \u03c6. IWAE-STL gradient performed well without a strong theoretical basis. However, IWAE faced \u03c6-gradient breakdown and had lower empirical performance than RWS. It remains unclear whether the multi-sample objective of IWAE or the adaptive importance-sampling approach of RWS is better. This study suggests that directly optimizing the proposal distribution, as in RWS, is more favorable than optimizing the IWAE multi-sample objective. In this work, it is shown that directly optimizing the proposal distribution, such as in RWS, is preferred over optimizing the IWAE multi-sample objective. This is because the multi-sample objective often relies on reparametrisations, leading to the \u03c6-gradient breakdown. Modifications like IWAE-STL and IWAE-DREG can be justified by taking an adaptive importance-sampling view. This conclusion aligns with previous findings by Le et al. (2019) that reparametrisations can make IWAE inferior to RWS, especially for discrete latent variables. Our work introduces the adaptive importance sampling for learning (AISLE) framework, which generalizes the RWS algorithm and includes IWAE-DREG and IWAE-STL gradients. Section 3 presents novel material on AISLE, from which various gradient estimators can be derived. In Section 3, the AISLE framework is introduced, presenting novel material where various gradient estimators can be derived. The derived gradient estimators from AISLE are guaranteed not to degenerate as K \u2192 \u221e. Specifically, the IWAE-STL gradient can be recovered as a special case of AISLE through a novel application of the 'double-reparametrisation' identity. This suggests that the breakdown of RWS observed in previous work may not be solely due to its lack of a joint objective. Our work provides a theoretical foundation for IWAE-STL and proves that AISLE also admits the IWAE-DREG gradient as a special case. The learning rate for IWAE \u03c6-gradient should be scaled as O(K) unless gradients are normalized by optimizers like ADAM. AISLE introduces a new family of gradient estimators for \u03b1-divergences, which generalizes previous methods. The focus is on analyzing the impact of self-normalization bias on importance-sampling based gradient approximations and comparing algorithms empirically. The focus of the work is to compare algorithms for gradient estimators in AISLE, without deriving new algorithms. The notation shorthand p(f) is used, and empirical comparisons are referenced in other works. The notation shorthand p(f): To keep the notation concise, we suppress dependence on the observation x. Expectations of test functions can be estimated by \u03c6, which are IID according to q \u03c6. Similarly, expectations of \u03c0 \u03b8 (f) can be approximated by self-normalised importance sampling estimators. The kth importance weight is denoted as w \u03c8 (z k) and its self-normalised version as s w k \u03c8. The importance weighted autoencoder (IWAE) aims to maximize a lower bound on the log-marginal likelihood by finding the generative-model parameters \u03b8. The self-normalised estimate \u03c0 \u03b8 \u03c6, z (f) may have bias that vanishes at rate O(K \u22121) under certain assumptions. The IWAE maximizes a lower bound on the log-marginal likelihood by optimizing the inference-network parameters \u03c6 and the number of samples K. As K increases, the evidence bound also increases. The IWAE extends the VAE to an auxiliary-variable construction, different from the VAE by Kingma & Welling (2014). The IWAE, like VAE, is based on an extended space using an auxiliary-variable construction. The IWAE objective involves approximating an intractable quantity with high variance, which is typically reduced using the reparametrisation trick. The IWAE objective reduces high-variance terms using the reparametrisation trick, involving a distribution q and differentiable mappings. The IWAE gradient focuses on the \u03c6-portion, as stated in Lemma 1 by Tucker et al. (2019). The IWAE gradient, specifically the \u03c6-portion, has drawbacks related to the reliance on reparametrisations and high-variance terms, which can be mitigated through control-variate approaches or continuous relaxations. Vanishing signal-to-noise ratio and inability to achieve zero variance have been identified as drawbacks of the IWAE gradient, leading to proposed modifications to address these issues. The IWAE gradient has drawbacks of vanishing signal-to-noise ratio and inability to achieve zero variance. Modifications like IWAE-STL and IWAE-DREG have been proposed to address these issues. The 'doubly-reparametrised' IWAE (IWAE-DREG) gradient removes score-function terms through Lemma 1, with gradients equal in expectation. The reweighted wake-sleep (RWS) algorithm was proposed in Bornschein & Bengio (2015). The \u03b8-and \u03c6-gradients are usually intractable and approximated using self-normalised importance sampling. The bias of (7) relative to (6) is of order O(1/K). Appendix A discusses the impact of this bias. The bias of the RWS algorithm relative to IWAE-DREG is of order O(1/K). Appendix A discusses this bias on the \u03c6-gradient in more detail. The optimization of both \u03b8 and \u03c6 is done simultaneously, sharing particles and weights. However, the lack of a joint objective for both \u03b8 and \u03c6 is seen as a drawback of RWS. The algorithm rws \u03c6 \u03b8, z in expectation is derived using Lemma 1. The function F(w) = w(1 - w) transforms self-normalised importance weights. In high-dimensional settings, the weights are mainly supported on the two particles with the largest weights. Optimizing \u03c6 reduces error in approximating the \u03b8-gradient. Adapting the proposal distribution in importance-sampling schemes does not always require minimizing KL-divergence. Adapting the proposal distribution in importance-sampling schemes may not always require minimizing the KL-divergence. Alternative techniques, such as minimizing the \u03c72-divergence, can be preferable. Generalizing the RWS-objective involves optimizing \u03c6 to minimize the \u0192-divergence from p to q. The unified framework allows for the optimization of \u03c6 to minimize the \u0192-divergence from p to q, leading to the algorithm adaptive importance sampling for learning (AISLE). This approach enables the derivation of robust \u03c6-gradient estimators that do not degenerate as K \u2192 \u221e, with optimization done through stochastic gradient ascent. The \u03b8-gradient remains the same for all algorithms discussed in this work. The \u03b8-gradient is consistent across algorithms in this work. The \u03b8-gradient is seen as an unbiased gradient in the IWAE paradigm, while AISLE interprets it as a biased approximation using importance sampling. Integrals involving \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ) can be expressed as Z. The expectation and normalizing constant Z \u03b8 can be approximated using vanilla Monte Carlo method. The \u03b8-gradient is consistent across algorithms in this work, seen as unbiased in IWAE and biased in AISLE. Integrals involving \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ) can be approximated with Z. Most \u0192-divergences used in variational inference allow optimization without knowing Z \u03b8. Simple algebra shows g(y) :=f (y) \u2212f (y)/y. The integral in (11) can be approximated using self-importance sampling without knowing Z \u03b8. Equation (10) applies to (11), leading to a reparametrised estimator where h(y) = g(y)y and g : R \u2192 R. Several particular cases are described, including AISLE-KL-NOREP/RWS. The gradient equations for AISLE-KL-NOREP/RWS and AISLE-KL are derived without reparametrisation. Proposition 1 shows that IWAE-STL can be obtained from AISLE in a principled manner, avoiding breakdowns and achieving zero variance. IWAE-STL can be derived from AISLE in a principled manner, avoiding breakdowns and achieving zero variance. This provides a theoretical basis for IWAE-STL, previously seen as biased and heuristically justified. IWAE-STL also showed good empirical performance even when RWS broke down, suggesting the breakdown may not be due to RWS' lack of optimizing a joint objective. The breakdown of RWS may not be due to a lack of optimizing a joint objective. Different methods like IWAE-STL and AISLE-KL aim to reduce bias and variance in gradient estimators. The \u03b1-divergence between two distributions can be calculated using a specific formula. The \u03b1-divergence between two distributions p and q can be expressed as a specific formula, which can help reduce bias and variance in gradient estimators. Minimizing this divergence is important in importance sampling, as it relates to the variance of the importance weights. AISLE-\u03b1-NOREP Equation (13) provides a method to achieve this without relying on reparametrization. The variance of importance weights in sampling can be expressed as \u03c7 2 (\u03c0 \u03b8 q \u03c6 ). AISLE-\u03b1 provides a method to minimize bias and variance without reparametrization, while AISLE-\u03c7 2 can be equivalent to when gradients are normalized. The implementation of AISLE-\u03c7 2 is equivalent to IWAE-DREG when gradients are normalized. The learning rate needs to be scaled as O(K) for IWAE or IWAE-DREG \u03c6-gradients. For the 'exclusive' KL-divergence, the approximation leads to a simple average over K independent replicates of the estimator for VAEs proposed in Roeder et al. (2017). The 'exclusive' KL-divergence approximation is a simple average over K independent replicates of the estimator for VAEs proposed in Roeder et al. (2017). Optimizing this divergence can lead to faster convergence of \u03c6, but caution is needed as it may negatively impact learning of \u03b8. The adaptive-importance sampling paradigm of the reweighted wake-sleep (RWS) is preferable to the multi-sample objective paradigm of importance weighted autoencoders (IWAEs) because it achieves all the goals of the latter while avoiding its drawbacks. The self-normalised importance-sampling approximation in RWS interpolates between two extremes as the number of particles, K, increases. The importance-sampling approximation in RWS varies with the number of particles, K, leading to accurate estimators as K increases. For K = 1, the estimators reduce to vanilla Monte Carlo approximations. The importance-sampling approximation in RWS varies with the number of particles, K, leading to accurate estimators as K increases. For K = 1, the estimators reduce to vanilla Monte Carlo approximations. The small-K self-normalisation bias of the reparametrisation-free AISLE \u03c6 gradients is difficult to characterize, but it may favor a minimization of the exclusive KL-divergence. The small-K self-normalisation bias of the gradients favors minimizing the exclusive KL-divergence. Using self-normalised importance-sampling with K > 1 particles reduces bias in the \u03b8-gradient. Controlling the error involves ensuring q \u03c6 is close to \u03c0 \u03b8 in Z. A small 'inclusive' KL-divergence implies well-behaved approximations. The importance of small 'inclusive' KL-divergence in ensuring well-behaved approximations when q \u03c6 is close to \u03c0 \u03b8 in the space Z with positive probability mass. The family Q must be sufficiently expressive for effective proposal distributions. When the variational family Q is sufficiently expressive, minimizing the exclusive KL-divergence can yield well-behaved importance weights. However, if Q is not flexible enough and all its members are far from \u03c0 \u03b8, minimization may not be effective. When the variational family Q is sufficiently expressive, minimizing the exclusive KL-divergence can yield well-behaved importance weights. In some cases, using a gradient-descent algorithm to minimize exclusive divergence may be more effective than minimizing inclusive divergence, as it can lead to faster convergence in certain applications. When the variational family Q is expressive, minimizing exclusive KL-divergence can yield well-behaved importance weights. In some cases, using gradient-descent to minimize exclusive divergence may lead to faster convergence in certain applications, even with a smaller number of particles K for some \u03c6-gradients. Simply setting K = 1 for approximating \u03c6-gradients is not always optimal. Increasing the number of particles K is desirable to reduce gradient approximation variance, even in scenarios where minimizing exclusive KL-divergence may lead to poorly behaved importance-sampling approximations of the \u03b8-gradient. Using all K particles and weights for approximating the \u03b8-gradient is more efficient than not utilizing the information. Setting K = 1 for reparametrisation-free AISLE \u03c6-gradients may not always be optimal. In the supplementary materials, different \u03c6-gradient estimators are illustrated, comparing AISLE-KL-NOREP, AISLE-KL-NOREP, and AISLE-\u03c7 2 -NOREP with vanilla Monte Carlo estimates. The gradient for AISLE based on the KL-divergence does not require R1 but does not achieve optimal results. The gradient for AISLE based on the KL-divergence without reparametrisation coincides with the standard RWS gradient. AISLE-KL after reparametrising and exploiting an identity from Lemma 1 coincides with IWAE-STL gradient. AISLE-\u03c7 2 -NOREP uses the \u03c7 2 -divergence without reparametrisation. AISLE-\u03c7 2 uses the \u03c7 2 -divergence after reparametrising. The gradient for AISLE based on the \u03c7 2 -divergence after reparametrising is proportional to IWAE-DREG. The gradient for IWAE using the reparametrisation trick from Kingma & Welling (2014) is also discussed. The gradient for IWAE employing the reparametrisation trick from Kingma & Welling (2014) is discussed, along with IWAE-DREG and RWS-DREG gradients. These gradients are related to the joint law parametrised by \u03b8 of the observations. The generative model is parametrised by \u03b8 and factorises with latent variables and observations. Proposal distributions are fully factored Gaussians, with parameters to optimise for the reparametrisation trick. The proposal distribution for the generative model is a fully factored Gaussian with parameters to optimize for the reparametrization trick. It is similar to benchmark models used in previous studies. The generative model and variational approximation involve isotropic Gaussians with correlated latent vectors. The variational approximation remains fully factored, potentially limiting uncertainty capture. The only randomness comes from a multivariate normal random variable, affecting the variance for any values of A, b, and K. The only source of randomness in the expression is the multivariate normal random variable. The variance of the gradients in the model goes to zero as the parameters approach their optimal values. The variance of the C-gradient portion approaches zero as the parameters converge to their optimal values. A detailed analysis of reparametrisation-trick gradients in Gaussian settings is discussed in Xu et al. (2019). Empirical comparisons of algorithms are conducted with varying numbers of particles and model dimensions, each repeated 100 times using synthetic data sets. The generative model is specified via different settings in Figures 1 and 2, with varying values of \u03c6 and \u03a3. The algorithms share the same \u03b8-gradient, focusing on optimizing \u03c6 while fixing \u03b8 throughout. The generative model is specified via \u03a3 = (0.95 |d\u2212d |+1 ) (d,d )\u2208{1,...,D} 2. The fully-factored variational approximation cannot fully mimic the dependence structure of the latent variables. Gradient-ascent algorithm is initialized by drawing initial values \u03c6 0 from a standard normal distribution. Stochastic gradient-ascent and ADAM are used for optimization. The generative model is specified via \u03a3 = (0.95 |d\u2212d |+1 ) (d,d )\u2208{1,...,D} 2. Both plain stochastic gradient-ascent and ADAM with default parameter values are used for optimization, with a total of 10,000 iterations. The learning-rate parameters follow i \u22121/2 scaling. The covariance matrix is not diagonal, with a logarithmic scaling on the second axis."
}