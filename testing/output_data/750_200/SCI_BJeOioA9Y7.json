{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The student can be trained independently of the teachers and shows superior performance on various supervised and reinforcement learning tasks. The knowledge flow approach allows the student to be independent of the teachers during training, showing superior performance in supervised and reinforcement learning tasks compared to other methods. Research communities have developed numerous deep net architectures for various tasks, with new ones constantly being introduced. These architectures can be trained from scratch or fine-tuned using a similar deep net trained on different data. In reinforcement learning, the concept of teachers has also been explored, such as in the progressive neural net which utilizes multiple teachers to extract useful features for a new task. Progressive neural net BID23, PathNet BID6, 'Growing a Brain' BID30, Actor-mimic BID20, and Knowledge distillation BID9 are techniques that leverage multiple teachers, genetic algorithms, network capacity growth, pre-training, and knowledge distillation for new tasks. However, these techniques have limitations. To address limitations in existing techniques like progressive neural nets BID23 and PathNet BID6, a new method called Knowledge distillation BID9 distills knowledge from a large ensemble of models to a smaller student model. Other methods like 'Growing a Brain' BID30 and actor-mimic BID20 have drawbacks such as reliance on a single pretrained model. Our approach, Knowledge flow, addresses limitations in existing techniques like 'Growing a Brain' BID30 and actor-mimic BID20 by allowing multiple teachers to transfer knowledge to a student during training. This ensures the student becomes independent regardless of the number of teachers used, with a constant size for the resulting student net. Our framework offers flexibility in choosing teacher models and is applicable to various tasks from reinforcement learning to fully-supervised training. Our approach, Knowledge flow, allows multiple teachers to transfer knowledge to a student during training, providing flexibility in choosing teacher models. It is applicable to tasks from reinforcement learning to fully-supervised training, aiming to find a policy that maximizes future rewards. The goal of reinforcement learning is to maximize future rewards by finding a policy through the A3C formulation. The policy mapping is obtained from a probability distribution over states, modeled by deep nets with parameters \u03b8 \u03c0. The value function is approximated by a deep net with parameters \u03b8 v. The policy parameters are optimized using a loss function based on a scaled negative log-likelihood. The value function is approximated by a deep net V \u03b8v (x) with parameters \u03b8 v. To optimize the policy parameters \u03b8 \u03c0 for a state x t, a loss function based on negative log-likelihood and negative entropy regularizer is common. The loss function includes the empirical k-step return R t and an entropy function to encourage exploration. The entropy function encourages exploration by favoring a uniform probability distribution. To optimize the value function, squared loss is commonly used. By minimizing the empirical expectation of policy and value functions alternatingly, a framework called knowledge flow is proposed to transfer knowledge from deep nets. Knowledge flow is a framework proposed to transfer knowledge from deep nets, moving 'knowledge' from multiple 'teachers' to a 'student' deep net under training. The student initially relies heavily on one teacher, but as training progresses, the student becomes independent. This process is illustrated on example deep nets in a figure. The knowledge flow framework transfers knowledge from multiple teachers to a student deep net under training. Parameters of the student net are trained using fixed parameters from pre-trained models on different tasks. Teacher representations are added to the student net to enhance learning. The knowledge flow framework enhances learning in a student deep net by adding teacher representations, which are transformed and scaled to guide the student's learning process. The student model should perform well on the target task independently after training. During training, the student model gradually becomes more independent from the teacher's knowledge, eventually performing well on the target task without relying on teachers. During training, the student gradually becomes independent from teachers, mastering tasks on its own. Two additional loss functions encourage this transfer by measuring reliance on teachers and ensuring stable behavior as teacher influence decreases. The student gradually becomes independent from teachers during training, with two additional loss functions ensuring stable behavior as teacher influence decreases. Loss\u02dc \u00b7 \u00b7 (\u03b8, w, Q) originates from the original loss \u00b7 \u00b7 (\u03b8) by transforming the deep net to include cross-connections, hence its dependence on w, Q. During training, the student gradually becomes independent from teachers with the help of parameters \u03bb 1 and \u03bb 2. The strength of teacher influence decreases as \u03bb 1 is gradually increased. Negative transfer may occur if teacher and student objectives differ. The proposed method reduces negative transfer by decreasing teacher influence as training progresses. Despite differing objectives, students can benefit from low-level knowledge transfer from teachers. The method modifies deep nets and uses loss functions to decrease teacher influence in student models. The method reduces negative transfer by decreasing teacher influence as training progresses. For each layer in the student model, a candidate set is defined containing the teachers' layers to be considered. Normalized weights are introduced to decide which representation to trust at each layer of the student net. In the student net, normalized weights are introduced for each layer to determine which representation to trust. The method reduces negative transfer by decreasing teacher influence as training progresses. The maximal number of introduced matrices in the framework is limited. In the student net, normalized weights are used to determine which representation to trust, reducing negative transfer by decreasing teacher influence as training progresses. It is recommended to link one teacher layer to one or two student layers, introducing matrices Q in the process. Additional trainable parameters Q and w are introduced in the framework but are not part of the resulting student network. In the final stage of training, the student network becomes independent and no longer relies on additional parameters Q and w or transformed representations from teachers. The influence of teachers is gradually decreased during training by encouraging normalized weights to increase to a value of 1. During training, the student network becomes independent by increasing normalized weights to 1 for all layers, reducing reliance on teachers. Rapid decrease of teacher influence can harm performance. During training, the student network becomes independent from teachers. A fast decrease in teacher influence can degrade performance as it takes time to find good transformations. To prevent rapid changes in the student's output distribution, a Kullback-Leibler regularizer is used. In supervised learning, parameters \u03b8 are updated, while in reinforcement learning, Q-values are updated. Knowledge flow is evaluated on both tasks using only the student model to avoid teacher influence. Atari games are used for reinforcement learning, with the agent learning to predict actions based on rewards and input images. The agent learns to predict actions in Atari games based on rewards and input images. It uses a fully forward architecture with three hidden layers, including convolutional and fully connected layers. The model outputs a softmax probability. The model architecture includes a 4x4 convolutional layer with stride 2, followed by a fully connected layer with 256 hidden units. It outputs a softmax probability distribution over actions and a scalar value function estimate. Adam optimizer is used with a learning rate of 10^-4, gradually decreased to zero. \u03bb 1 and \u03bb 2 are selected using progressive neural net by randomly sampling from {0.05, 0.1, 0.5}. The learning rate is set to 10^-4 and gradually decreased to zero for all experiments. \u03bb 1 and \u03bb 2 are selected by randomly sampling from {0.05, 0.1, 0.5} and {0.001, 0.01, 0.05} respectively. Each experiment is repeated 25 times with different random seeds. Top three results out of 25 runs are reported. Evaluation involves playing each game for 30 episodes with 16 agents on 16 CPU cores in parallel. The trained student models are evaluated by playing each game for 30 episodes with 16 agents on 16 CPU cores in parallel. Comparison with PathNet and progressive neural net (PNN) using experimental settings is summarized in TAB0. State-of-the-art results on Atari games are also included in TAB0 for reference. The state-of-the-art results on Atari games, including comparisons with PathNet and PNN, are summarized in TAB0. Our transfer framework shows higher scores in experiments with one teacher compared to PathNet and in experiments with two teachers compared to PNN. Knowledge flow effectively transfers knowledge from teachers to the student, leading to improved performance when the number of teachers increases. In our framework, increasing the number of teachers from one to two significantly improves student performance across all experiments. Training curves in FIG1 show our approach performing well. Different environment/teacher settings were tested, not used by PathNet or progressive neural network, with results in TAB2. \"Ours w/ expert\" has one expert teacher for the target game, \"ours w/ non-expert\" has both non-expert teachers, and \"Fine-tune\" is also evaluated. In our framework, increasing the number of teachers from one to two significantly improves student performance across all experiments. Knowledge flow with an expert teacher outperforms the baseline, showing successful transfer of knowledge. Our A3C implementation achieves better scores than those reported by BID17 for most games. Knowledge flow successfully transfers knowledge from expert teachers to students, outperforming fine-tuning methods with non-expert teachers. Students in knowledge flow can learn from multiple teachers, avoiding negative impacts from insufficiently pretrained teachers. Training curves for the experiments are shown in FIG5. Fine-tuning from an insufficiently pretrained model can slow down training and degrade performance. In knowledge flow, students benefit from intermediate representations of expert teachers, even if input/output spaces differ. The student model in the experiment achieves scores ten times larger than learning without a teacher, using Chopper Command and Space Invaders as teachers for the target game Seaquest. The model benefits from learning from teachers and achieves significantly higher scores compared to learning without a teacher. Various image classification benchmarks are used for supervised learning, with parameters determined using validation sets. Evaluation metrics include reporting top-1 error rates on test sets, with results averaged over three runs with different random seeds. The CIFAR-10 and CIFAR-100 datasets consist of colored images of size 32 \u00d7 32, with 10 and 100 classes respectively. Training and test sets contain 50,000 and 10,000 images. Experiments are conducted using Densenet (depth 100, growth rate 24) with standard data augmentation. Teachers are trained on CIFAR-10, CIFAR-100, and SVHN datasets before training the models. For our approach, teachers are trained on CIFAR-10, CIFAR-100, and SVHN datasets before training the student model. Fine-tuning from the CIFAR-100 expert improves performance by 4% over the baseline for the CIFAR-10 target task, while fine-tuning from the SVHN expert performs worse. Knowledge flow improves by 13% over the baseline when presented with both good and inadequate teachers. The CIFAR-100 deep net is a good teacher, while a deep net trained with SVHN isn't. Knowledge flow improves by 13% over the baseline when presented with both good and inadequate teachers. This demonstrates the ability of knowledge flow to leverage a good teacher's knowledge and avoid misleading influence. Results are similar on the CIFAR-100 dataset, and additional results demonstrating the properties of knowledge flow can be found in the appendix. In contrast to previous approaches like PathNet and Progressive Net, our method ensures independence of the student during training by using lateral connections. Our method ensures independence of the student during training by using lateral connections. Distral, a neologism combining 'distill & transfer learning', considers joint training of multiple tasks with a shared policy to encourage consistency between different tasks. Knowledge flow addresses a single task, while multi-task learning addresses multiple tasks simultaneously. In multi-task learning, information from different tasks is shared to improve performance, while in knowledge flow, information from multiple teachers helps a student learn a new task. Related work includes actor-mimic, learning without forgetting, growing a brain, policy distillation, domain adaptation, knowledge distillation, and lifelong learning. The authors developed a general knowledge flow approach for training deep nets from multiple teachers, showing improvements in reinforcement learning and supervised learning. Future work includes learning when to use different teachers and actively swapping teachers during student training. The approach involves distilling knowledge from a larger model (teacher) to a smaller model (student). The study focuses on distilling knowledge from larger teacher models to smaller student models for training. Experiments were conducted on various datasets like MNIST, CIFAR-100, and ImageNet using different model configurations. The study explores distilling knowledge from larger teacher models to smaller student models on datasets like CIFAR-100 and ImageNet. The student model mimics the teacher's structure but with reduced output channels in convolutional layers. Results show improved performance compared to Knowledge Distillation (KD) due to leveraging both output layer behavior and intermediate layer representations of the teacher model. The 'EMNIST Letters' dataset comprises 28x28 pixel images of handwritten letters. The 'EMNIST Letters' dataset contains 28x28 pixel images of handwritten letters with 26 balanced classes. The training and test sets have 124,800 and 20,800 images respectively. The 'EMNIST Digits' dataset includes 28x28 pixel images of handwritten digits with 10 balanced classes. The training and test sets have 240,000 and 40,000 images respectively. The study uses the MNIST model as a baseline, teacher, and student model, training teachers on different EMNIST datasets. The study used the MNIST model as a baseline, teacher, and student model, training teachers on EMNIST Digits, EMNIST Letters, and a subset of EMNIST Letters with 13 classes. The student model was trained with different teachers, showing improved performance compared to baseline and fine-tuning methods. Results are summarized in Table 5 and illustrated in FIG6 for accuracy over epochs. In the study, the STL-10 dataset with colored images of size 96 \u00d7 96 pixels and 10 balanced classes was used. The training set had 5,000 labeled images and 100,000 unlabeled images, with the test set containing 8,000 images. The experiment only utilized the 5,000 labeled images for training, comparing results to fine-tuning and a baseline model. Our approach on the STL-10 dataset involves fine-tuning a model using weights pretrained on CIFAR-10 and CIFAR-100, resulting in a reduction of test errors by more than 10% compared to the baseline. Student model training in our framework further reduces the test error by 3%. In our framework, student model training reduces test errors by 3% compared to fine-tuning. Results are obtained using fewer data and may not be directly comparable to other approaches. We compare to Distral BID26, a state-of-the-art multi-task reinforcement learning framework, on Atari games. In our framework, we used 'KL + ent 1 col' with a central model (m 0 ) and task models (m i ) for each task. Experiments were conducted on Atari games with three tasks (task 1, task 2, task 3). Distral was trained for 120M steps, while our model was trained for 40M steps. Results show that Distral's task 1 model (m 1 ) outperformed its center model (m 0 ). Distral is suboptimal for learning a multi-task agent with different target tasks. Our framework can decrease a teacher's influence and reduce negative transfer, unlike Distral which is suboptimal for learning a multi-task agent with different target tasks. In the C10 experiment, the averaged normalized weight (p w) for teachers and the student was plotted, showing that the C100 teacher had a higher p w value than the SVHN teacher, confirming the relevance of C100 to C10. The C100 teacher has a higher p w value than the SVHN teacher, confirming its relevance to C10. Learning with untrained teachers results in worse performance compared to knowledgeable teachers, as shown in the experiments. Learning with untrained teachers achieves lower rewards compared to learning with knowledgeable teachers, as confirmed by experiments. Knowledge flow leads to higher rewards in various environments and teacher-student settings. The KL term helps maintain the student's output distribution when teachers' influence decreases. The KL term in teacher-student settings prevents drastic changes in the student's output distribution as teachers' influence decreases. An ablation study with KL coefficient set to zero shows performance drops without the KL term. Learning with the KL term achieves an average reward of 2907, while learning without it achieves 1215. Training with the KL term in teacher-student settings results in higher rewards compared to training without it. Additional experiments involve using different architectures for the teacher model, specifically the BID16 model with 3 convolutional layers and a hidden fully connected layer with 512 ReLUs. The teacher model has 3 convolutional layers with 32, 64, and 64 filters, and a hidden fully connected layer with 512 ReLUs. The student model, based on BID17, has 2 convolutional layers with 16 and 32 filters, and a hidden fully connected layer with 256 ReLUs. Both models have output layers for actions and values. In experiments, the teacher's layers are linked to corresponding layers in the student model. In the experiment, teachers with different architectures were used to train the student model for the KungFu Master task. Results showed that despite architectural differences, the performance was similar to using teachers with the same architecture. In the experiment, teachers with different architectures were used to train the student model for the KungFu Master task. Despite, the results showed that knowledge flow can enable higher rewards, even if the teachers and student architectures differ. The average reward achieved with teachers of different architectures was 37520, while with teachers of the same architecture it was 35012. This indicates that using teachers with varying architectures can still lead to successful learning outcomes. The experiment also explored the use of an average network to obtain parameters, showing that it can affect performance positively. Overall, the study highlights the importance of knowledge transfer in achieving better rewards in reinforcement learning tasks. Using an exponential average to compute \u03b8 old results in similar performance as using a single model. For the Boxing task with a Riverraid expert teacher, using an average network achieves an average reward of 96.2, while using a single network achieves 96.0. Using an average network to obtain \u03b8 old achieves an average reward of 96.2, while using a single network achieves 96.0. Various techniques for knowledge transfer have been explored, such as fine-tuning, progressive neural nets, PathNet, 'Growing a Brain', actor-mimic, and learning without forgetting. PathNet allows multiple agents to train the same deep net while reusing parameters. In contrast to PathNet and Progressive Net, our method involves multiple teacher nets trained with lateral connections to avoid catastrophic forgetting. Our method introduces scaling with normalized weights to avoid catastrophic forgetting, ensuring independence of the student during training. Distral, a combination of 'distill & transfer learning', involves joint training of multiple tasks with a shared policy to encourage consistency between different tasks. Knowledge flow is different from multi-task learning as it focuses on transferring information from multiple teachers to help a student learn a new task. In contrast, multi-task learning shares information from different tasks to improve performance. Knowledge distillation BID9 distills information for learning. Our technique enables knowledge transfer between different source and target domains, allowing an agent to learn multiple tasks simultaneously and generalize the extracted knowledge to new domains. Feature regression and cross entropy loss are used to encourage the student to produce similar actions as expert teachers. The proposed technique allows for knowledge transfer between different domains, enabling an agent to learn multiple tasks simultaneously. It utilizes feature regression and cross entropy loss to guide the student in producing similar actions as expert teachers. The approach ensures that new tasks can be added to a deep net without forgetting the original capabilities, by retaining old capabilities through recording the old network's output on new data. This method differs from others by explicitly transferring knowledge from teacher networks."
}