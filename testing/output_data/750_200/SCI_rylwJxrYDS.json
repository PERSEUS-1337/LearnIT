{
    "title": "rylwJxrYDS",
    "content": "The proposed vq-wav2vec algorithm learns discrete representations of audio segments using a self-supervised context prediction task. It utilizes gumbel softmax or online k-means clustering for quantization, allowing for the application of NLP algorithms. Experiments show BERT pre-training achieves state-of-the-art results in phoneme classification and speech recognition tasks. Learning discrete speech representations has gained recent interest, with autoencoding being a popular approach. In this paper, the vq-wav2vec algorithm combines research on learning discrete speech representations via context prediction instead of input reconstruction. This allows for the application of NLP algorithms to speech data. The vq-wav2vec algorithm learns discrete representations of audio segments by predicting context, enabling the application of NLP algorithms to speech data. It maps raw audio to dense quantized representations for training acoustic models. Our new discretization algorithm, vq-wav2vec, learns discrete representations of audio segments using a Gumbel-Softmax approach and online k-means clustering. These representations are then input to a Deep Bidirectional Transformer (BERT) for training acoustic models, showing better performance than traditional inputs. The experiments demonstrate that BERT representations outperform log-mel filterbank inputs and dense wav2vec representations on TIMIT and WSJ benchmarks. Discretization of audio allows for the application of NLP algorithms to speech data, such as using a sequence to sequence model for speech recognition over discrete audio tokens. The wav2vec model learns audio representations through a self-supervised context-prediction task with a convolutional neural network. It produces representations for each time step and aggregates them to distinguish future samples from distractors, minimizing contrastive loss. The wav2vec model learns audio representations through a self-supervised context-prediction task with a convolutional neural network. It distinguishes future samples from distractors by minimizing contrastive loss using a step-specific affine transformation. The representations produced by the context network are input to the acoustic model instead of log-mel filterbank features. BERT is a pre-training approach for NLP tasks that uses a transformer encoder model to build text representations. Our approach, vq-wav2vec, learns vector quantized (VQ) representations of audio data using log-mel filterbank features. BERT is a pre-training approach for NLP tasks that uses a transformer encoder model to build text representations. The vq-wav2vec approach learns vector quantized representations of audio data by predicting if passages are from the same document. It uses convolutional networks for feature extraction and a quantization module to build discrete representations. The vq-wav2vec approach utilizes an encoder network to process audio data at a 10ms stride, followed by a quantizer that converts dense representations into discrete indices. These indices are then mapped to a reconstruction of the original representation. The quantization module employs a fixed size codebook containing different representations and utilizes Gumbel-Softmax and online k-means clustering for one-hot representations. The vq-wav2vec approach uses Gumbel-Softmax and online k-means clustering for one-hot representations. Multiple vector quantizations are applied to different parts of z to prevent mode collapse. The vq-wav2vec approach utilizes Gumbel-Softmax for one-hot representations. It involves applying linear layers and ReLU functions to output logits for the Gumbel-Softmax. During training, probabilities for variable selection are determined using uniform samples. The approach optimizes future time step prediction loss instead of autoencoder reconstruction loss. The vq-wav2vec approach uses Gumbel-Softmax for one-hot representations and optimizes future time step prediction loss. The codebook variable representation is chosen based on Euclidean distance, and gradients for the encoder network are obtained through back-propagation. The final loss includes a future prediction task and stop gradient operator with a hyper-parameter \u03b3. The final loss in the vq-wav2vec approach includes two additional terms: one for future prediction task and another for moving codebook vectors closer to encoder output. The third term ensures encoder outputs are close to a centroid. To avoid mode collapse, replacing encoder feature vector with a single entry in the codebook is not ideal. The vq-wav2vec approach includes strategies to prevent mode collapse by independently quantizing partitions of the feature vector z into larger dictionaries for improved performance. The vq-wav2vec approach prevents mode collapse by quantizing feature vectors into matrices and applying VQ approaches to groups with shared or unshared codebook variables. The vq-wav2vec approach prevents mode collapse by quantizing feature vectors into matrices and applying VQ approaches to groups with shared or unshared codebook variables. In practice, sharing the codebook variables yields competitive results to a non-shared representation. Once a vq-wav2vec model is trained, audio data can be discretized for algorithms requiring discrete inputs. One possibility is to use the discretized training data for BERT pre-training, where the task is to predict masked input tokens based on the surrounding context encoding. BERT pre-training involves predicting masked input tokens based on surrounding context. To improve speech recognition, BERT representations can be fed into an acoustic model. Recent advancements in BERT training focus on masking spans of consecutive speech tokens instead of individual tokens. This approach aims to enhance the prediction difficulty by sampling a small percentage of tokens as starting indices and masking a fixed number of tokens. To improve speech recognition, BERT representations are used in an acoustic model by masking spans of consecutive speech tokens. This involves sampling a small percentage of tokens as starting indices and masking a fixed number of tokens to make prediction harder. The models are pre-trained on the full 960h of Librispeech and evaluated on benchmarks like TIMIT. The evaluation of models is done on two benchmarks: TIMIT, a 5h dataset with phoneme labels, and Wall Street Journal (WSJ), an 81h dataset for speech recognition. The models are trained on 31 graphemes for WSJ and use fairseq implementation of wav2vec with 34 \u00d7 10 6 parameters. The fairseq implementation of wav2vec models with 34 \u00d7 10 6 parameters includes an encoder with 8 layers, each with 512 channels, and an aggregator with 12 layers. The encoder has kernel sizes (10,8,4,4,4,1,1,1) and strides (5,4,2,2,2,1,1,1), while the aggregator has stride 1 and increasing kernel sizes. Skip connections are introduced between subsequent blocks in the aggregator. Training includes wav2vec context prediction. The encoder network in the fairseq implementation of wav2vec models has 8 layers with skip connections between subsequent blocks. Training involves predicting 8 steps into the future and sampling 10 negatives, with a batch size of 10 and random cropping of 150,000 frames for each example. Training is done on 8 GPUs. The model used for ablations and experiments on the 100h Librispeech subset has a smaller architecture with specific kernel sizes and strides. It is trained for 40k updates. Gumbel-Softmax models are utilized with 2 groups and 320 latents per group, producing one-hot vectors. The temperature is annealed from 2 to 0.5 over updates. The Gumbel-Softmax model with 640 logits generates one-hot vectors for each group, with the temperature annealed from 2 to 0.5 over updates. After training on 960h of Librispeech, 13.5k unique codeword combinations are obtained. Using k-means models with 2 groups and 320 variables per group, vq-wav2vec on full Librispeech yields 23k unique codewords. A gamma value of 0.25 is found to be effective for balancing the VQ auxiliary loss in BERT base models. Following training on Librispeech, vq-wav2vec generates 23k unique codewords. A gamma value of 0.25 is used to balance the VQ auxiliary loss in BERT base models. Training involves 128 GPUs with a batch size of 3072 tokens per GPU. BERT small uses a smaller setup with model dimension 512, FFN size 2048, 8 attention heads, and dropout 0.05. BERT small models are trained with a model dimension of 512, FFN size of 2048, 8 attention heads, and dropout of 0.05. The acoustic model uses wav2letter and is trained for 1k epochs on 8 GPUs for TIMIT and WSJ datasets. Decoding on WSJ involves using a lexicon and a language model trained on WSJ data. Language models include a 4-gram KenLM model and a character-based convolutional model. The language modeling data is evaluated using a 4-gram KenLM model and a character-based convolutional model. A vq-wav2vec model is trained on Librispeech, then used to estimate a BERT model. A wav2letter acoustic model is trained on WSJ using either BERT or vq-wav2vec representations instead of log-mel filterbanks. Comparisons are made with wav2vec results from the literature. Table 1 displays the comparison of vq-wav2vec with BERT training achieving a new state of the art of 2.34 WER on nov92, showing significant gains especially without a language model. The vq-wav2vec with Gumbel-Softmax achieves 2.34 WER on nov92 without a language model, using 13.5k codewords. Comparing Gumbel-Softmax to k-means for vector quantization, a larger codeword model did not outperform the baseline. Training BERT models with a small vocabulary is enabled by the limited codewords. The study compares different models for phoneme recognition, including vq-wav2vec with k-means and BERT small configuration. Results show varying phoneme error rates, with the addition of BERT small improving performance. The study compares different models for phoneme recognition, including vq-wav2vec with k-means and BERT small configuration. Results show varying phoneme error rates, with the addition of BERT small improving performance. All models use the CNN-8L-PReLU-do0.7 architecture, with Gumbel-Softmax and k-means clustering performing comparably in the no language model setup. The large codeword model can reduce the gap to the original wav2vec model. After experimenting with vq-wav2vec and BERT on the TIMIT phoneme recognition task, a new state-of-the-art 11.67 PER was achieved, representing a 21% error reduction compared to wav2vec. Training a standard sequence-to-sequence model on discretized speech also showed promising results in preliminary experiments. After achieving a new state-of-the-art 11.67 PER on the TIMIT phoneme recognition task with vq-wav2vec and BERT, promising results were also seen in training a standard sequence-to-sequence model on discretized speech. The model was evaluated on the Librispeech dev/test sets using a 4k BPE output vocabulary, showing potential despite not matching the state of the art results that rely on data augmentation. Further investigation into vq-wav2vec's ability to compress audio data was conducted by training models with varying numbers of groups and variables. After achieving a new state-of-the-art 11.67 PER on the TIMIT phoneme recognition task with vq-wav2vec and BERT, further investigation was done on how well vq-wav2vec can compress audio data. Models were trained with different numbers of groups and variables to measure compression with varying codebook sizes. The tradeoff between bitrate and accuracy on phoneme recognition was analyzed by experimenting with different group sizes and variables, ranging from 0.53 kbit/s to 33.03 kbit/s. The quantization module was placed after the aggregator module for training. The study explored compression of audio data using vq-wav2vec models with varying codebook sizes from 40 to 1280 variables, covering a bitrate range of 0.53 kbit/s to 33.03 kbit/s. Different lossy compression algorithms were used as baselines, including Codec2, Opus, MP3, and Ogg Vorbis. The codecs were tested at variable and constant bitrate settings using ffmpeg for encoding and decoding. The study compared different compression algorithms for audio data, including vq-wav2vec, Codec2, Opus, MP3, and Ogg Vorbis. Results showed that vq-wav2vec performed best across various bitrate settings. Masking entire spans of tokens was more effective than individual tokens. BERT training on discretized audio data was robust to large input masking. Vq-wav2vec, a self-supervised algorithm, quantizes unlabeled audio data, improving the state of the art. vq-wav2vec is a self-supervised algorithm that quantizes unlabeled audio data, improving performance on benchmarks by leveraging BERT pre-training. Future work includes applying other algorithms requiring discrete inputs to audio data and exploring self-supervised pre-training algorithms. Additionally, finetuning the pre-trained model to output transcriptions is considered, along with investigating the relationship between variables and groups. Multiple groups are shown to be beneficial. The relationship between variables and groups is investigated in the context of feeding pre-trained features to a custom ASR model. Multiple groups are found to be beneficial compared to a single group with many variables, as shown in Table 6. Additionally, Table 7 demonstrates that with a single group and numerous variables, only a small number of codewords survive."
}