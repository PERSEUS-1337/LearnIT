{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging. A non-statistical framework based on identifying modular organization of the network is proposed, relaxing the need for statistical independence. Experiments show modularity between groups of channels to a certain degree. The modular organization of the network, based on counterfactual manipulations, allows for targeted interventions on complex image datasets. This enables applications like computationally efficient style transfer and automated assessment of robustness in pattern recognition systems. Deep generative models have been successful in designing realistic images across various domains. Recent advancements in space observations have led to the development of state-of-the-art approaches like Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) for creating realistic images in different domains. Efforts have been made to create models with disentangled latent representations to control interpretable properties of images, although these models may not be mechanistic or causal in attributing interpretable properties to specific parts of an image. Gaining access to a modular organization of generative models would enhance interpretability and allow for extrapolations, such as generating an object in a new background. This capability aligns with human representational abilities and the modular organization of the visual system. In this paper, a causal framework is proposed to explore modularity in deep generative architectures for extrapolations, aiming to enhance interpretability and adaptability to environmental changes. The paper proposes a causal framework to explore modularity in deep generative architectures for better interpretability and adaptability. The framework is based on the principle of Independent Mechanisms, allowing for individual modification of causal mechanisms without influencing each other. This approach can be applied to assess how well generative models capture causal mechanisms. The paper introduces a causal framework based on the principle of Independent Mechanisms, allowing for individual modification of causal mechanisms without influencing each other. This framework is applied to assess how well generative models capture causal mechanisms and analyze disentanglement in deep generative models through unsupervised counterfactual manipulations. The paper presents a causal framework for analyzing disentanglement in generative models through unsupervised counterfactual manipulations. It shows how VAEs and GANs exhibit modularity in their hidden units, allowing for editing of generated images. The work is related to interpretability of convolutional neural networks, with a focus on generative models. Generative models like InfoGANs, \u03b2-VAEs, and others aim to disentangle latent variables for transformations on data points. A new concept of intrinsic disentanglement is introduced to uncover internal network organization, suggesting that some transformations are statistically dependent and may not be easily separated in the latent space. This work builds on the interpretability of convolutional neural networks and focuses on generative models. The curr_chunk discusses different approaches to disentangling latent variables in generative models, contrasting various methods such as interventions on internal variables and group representation theory. The approach introduced in the curr_chunk is highlighted for its flexibility in handling arbitrary continuous transformations without the strict requirements of representation theory. The curr_chunk introduces a general framework for disentanglement in generative models, focusing on arbitrary continuous transformations without the need for representation theory. This approach bridges disentanglement to causal concepts and offers an interventional perspective on extrinsic disentanglement. The curr_chunk introduces a generative model framework for mapping latent space to data points in Euclidean space, with a focus on representations and disentanglement. Refer to Appendix A for mathematical details. The curr_chunk discusses a causal generative model (CGM) for mapping latent representations to data points using a non-recurrent neural network. It involves a succession of operations represented by a computational graph. The curr_chunk introduces the Causal Generative Model (CGM) for mapping latent representations to data points using endogenous variables in a causal graph. This is not about statistical independence but about how the variables influence each other. The curr_chunk discusses the paradigmatic choice of using output activation maps of each channel in a hidden layer of a convolutional neural network as endogenous variables. It also mentions using mild conditions to guarantee the left-invertibility of the internal representation of the network for latent variables. The dimensions of the variables are constrained to subsets of smaller dimension, and the endogenous image sets are denoted for a subset of variables. The curr_chunk discusses defining counterfactuals in a CGM framework for neural networks. It involves replacing subset variables with assignments to obtain an interventional model. This concept aligns with potential outcome theory. The unit-level counterfactual in a CGM framework induces a transformation of the generative model output. Faithfulness of a counterfactual mapping ensures interventions on internal variables align with the original model, preventing non-faithful counterfactuals from generating outputs outside the learned distribution. Non-faithful counterfactuals can lead to artifactual outputs by leaving the learned data distribution, potentially saturating downstream neurons. The idea of disentangled representation suggests that latent variables encode real-world transformations sparsely. This insight has driven supervised approaches to disentangling representations by manipulating relevant transformations explicitly with appropriate datasets. Supervised approaches focus on disentangling representations by manipulating relevant transformations explicitly with appropriate datasets. Unsupervised learning seeks to learn real-world transformations from unlabeled data, encoding them through changes in individual latent factors and enforcing conditional independence between them. This statistical approach presents challenges due to constraints on the prior distribution of latent variables. The statistical approach of enforcing independence between latent factors presents challenges in specifying a disentangled representation, leading to an open question on finding an appropriate inductive bias for downstream tasks. Finding an appropriate inductive bias for learning representations that benefit downstream tasks remains an open question, especially on complex real-world datasets where disentangled generative models struggle to match the visual sample quality of non-disentangled models like BigGAN. The non-statistical definition of disentanglement involves a transformation T acting on the data manifold YM, where T corresponds to a transformation of the latent space that affects only a single variable zk, leaving other variables free to encode other properties. Two transformations T1 and T2 are considered disentangled if they modify different components of the latent representation. The notion of extrinsic disentanglement involves two transformations T1 and T2 being qualified as disentangled when they modify different components of the latent representation, following the causal principle of independent mechanisms. This definition is agnostic to subjective choices and statistical notions of independence. When applied to the latent space, the functional notion of disentangled transformation implies statistical independence between factors. However, a different representation is needed to uncover related properties that are disentangled according to our definition. In a graphical model, endogenous variables may not be statistically independent due to a common latent cause, but can still reflect related properties. In a graphical model, endogenous variables are not necessarily statistically independent due to a common latent cause, but can still reflect interesting properties that can be intervened on independently. The definition of disentanglement is extended to allow transformations of internal variables, making them intrinsically disentangled with respect to a subset of endogenous variables. The internal representation space can be transformed by T, affecting variables indexed by E without common latent ancestors. Modularity is defined as a structural property allowing for disentangled transformations. The internal representation space can be transformed by T, affecting variables indexed by E without common latent ancestors. Modularity is defined as a structural property allowing for disentangled transformations, following Prop. 2. Definition 4 states that a subset of endogenous variables E is modular if any transformation applied to it within its input domain is disentangled. The proof is an extension of Proposition 1, with trivial extensions to multiple modules. This framework is based on a functional definition of disentanglement that applies to transformations, linking it with an intrinsic property of the trained network to define a disentangled representation. A disentangled representation is defined by partitioning the intermediate representation into modules, allowing valid transformations in the data space. This partition requires an additional division of latent variables into modules, not considered in classical approaches. Our framework introduces the concept of grouping neurons into modules at a \"mesoscopic\" level for meaningful representation, challenging the assumption of independence at the atomic level. The framework suggests grouping neurons into modules at a \"mesoscopic\" level to allow independent intervention. Prop. 1 and 2 hint that finding a modular structure enables various disentangled transformations, with staying within input domains being key. Assigning constant values to endogenous variables helps define counterfactual interventions. The framework suggests grouping neurons into modules for independent intervention. Assigning constant values to endogenous variables defines counterfactuals, aiming for faithful ones by constraining values. Sampling from the marginal distribution of variables in E avoids characterizing V E M. The hybridization procedure involves taking two independent examples of latent variables to generate transformations. The hybridization procedure involves taking two independent examples of latent variables to generate original examples of the output. By memorizing the values of variables indexed by E, different aspects of the generated images can be encoded, allowing for the generation of hybrid examples mixing these aspects. The choice of module E in a hybridization framework allows for the encoding of different aspects of generated images. By combining the output values of layer with concatenated tuples (v(z1), v(z2)), hybrid examples can be generated to assess how module E affects the output of the generator. This causal effect is quantified by generating pairs (z1, z2) from the latent space and collecting hybrid outputs. Repetitively generating pairs (z1, z2) from the latent space independently, hybrid outputs are collected to estimate an influence map using the mean absolute effect. The difference inside the absolute value represents a unit-level causal effect, and taking the expectation computes the average treatment effect. This approach has two specificities: taking the absolute value of the unit-level causal effect. The approach involves generating pairs (z1, z2) from the latent space to estimate an influence map using the mean absolute effect. The absolute value of unit-level causal effects is taken, and the result is averaged over different interventions corresponding to various values of z2. The challenge lies in selecting subsets E to intervene on in the hybridization approach. The hybridization approach involves selecting subsets E to intervene on, especially with networks containing many units or channels per layer. A fine to coarse approach is used to extract groups, starting with estimating elementary influence maps (EIM) for individual output channels and then grouping them by similarity to define modules at a coarser scale. Representative EIMs for channels of convolutional layers of a VAE are used in this process. In an unsupervised manner, clustering of channels is performed using their Elementary Influence Maps (EIMs) as feature vectors to group them based on similarity. This grouping reveals that channels in convolutional layers of a VAE are functionally segregated, with some influencing finer face features while others affect the background or hair. This supports the idea that individual channels can be grouped into modules dedicated to specific aspects of the output. Clustering of channels is done using EIMs as feature vectors to group them based on similarity. Pre-processing involves smoothing maps and thresholding to create a binary image. NMF algorithm is applied to get factor matrices W and H, revealing functional segregation of channels in VAE convolutional layers. The Non-negative Matrix Factorization (NMF) algorithm is used to cluster template patterns in images based on their contribution weights. The choice of NMF is justified by its success in isolating meaningful parts of images. The approach will also be compared to the classical k-means clustering algorithm. The NMF algorithm isolates meaningful parts of images. It will be compared to k-means clustering. A toy generative model is introduced to justify the NMF approach. The NMF algorithm isolates meaningful parts of images and will be compared to k-means clustering. A toy generative model is introduced to justify the NMF approach, where endogenous variables are mapped to the output with matrices W k \u2208 R m\u00d7n. The NMF algorithm isolates meaningful image parts and is compared to k-means clustering. A toy generative model justifies NMF with matrices W k \u2208 R m\u00d7n. The model has a density with respect to the Lebesgue measure, with coefficients set to zero. The condition on the I k 's in (2) encodes that an image area is influenced by only one module. Proposition 3 states that for Model 1, the hidden layer partition corresponds to a disentangled representation. The NMF algorithm isolates meaningful image parts by partitioning the hidden layer into disentangled representations. This is achieved by using a thresholded version of the influence map matrix to generate a binary matrix summarizing significant influences on each output pixel. Sliding window application enforces similarity between influence maps within the same module, favoring low-rank matrix factorization. The NMF algorithm partitions the hidden layer into disentangled representations by using a thresholded influence map matrix. Sliding window application enforces similarity between influence maps within the same module, favoring low-rank matrix factorization. In Model 1, identical support I k for all columns of W k is reflected, and the study investigates generative models trained on the CelebFaces Attributes Dataset using DCGAN, \u03b2-VAE, and BEGAN architectures. Hybridization of generator samples is done by intervening on the output of the intermediate convolutional layer. The NMF algorithm partitions the hidden layer into disentangled representations by using a thresholded influence map matrix. Hybridization of generator samples is achieved by intervening on the output of the intermediate convolutional layer. Setting the number of clusters to 3 leads to interpretable cluster templates associated with background, face, and hair. Cluster stability analysis confirms this observation. Cluster stability analysis was conducted by partitioning influence maps into 3 subsets and running clustering twice on two-thirds of the data. The results showed that 3 clusters were a reasonable choice with high consistency (>90%), dropping considerably for 4 clusters. This analysis confirmed the interpretability of cluster templates associated with background, face, and hair in the NMF algorithm. The NMF-based clustering outperforms k-means algorithm, with 3 clusters showing high consistency (>90%) and dropping significantly for 4 clusters. The robustness of clustering was assessed by cosine distance between templates, with an average cosine similarity of .9 achieved with 3 clusters. The hybridization procedure applied to 3 clusters resulted in replacement of features without introducing discontinuity in the overall image structure, as shown in Supplemental Fig. 8. In Supplemental Fig. 8, facial features of Original 2 samples are inserted into the Original 1 image while preserving the hair. The \u03b2-VAE is designed for extrinsic disentanglement, but further research suggests it may not be optimal compared to other approaches. Investigating intrinsic disentanglement in models where disentanglement is not explicitly enforced is important. Investigating intrinsic disentanglement in models not explicitly designed for it, such as GAN-like architectures, can yield interesting results. This was demonstrated in experiments using the tensorlayer DCGAN implementation. This suggests that disentanglement techniques can be applied to models not optimized for it. After experimenting with basic models, the study tested a pretrained Boundary Equilibrium GAN (BEGAN) for generating high-resolution face images. The simple generator architecture of BEGAN allowed for testing the hypothesis with minimal modifications. Increasing the number of layers required interventions to obtain counterfactuals with noticeable effects. Interventions on channels from the same cluster in two successive layers were needed to obtain counterfactuals with noticeable effects. Results from intervening on layers 5 and 6 showed selective transfer of features from Original 2 to Original 1, with a clear hair transfer observed due to the model being trained on tightly cropped face images. The study found that only one module displayed clear hair transfer, while the other modules encoded various face features. The quality of counterfactual images was evaluated using the Frechet Inception Distance, showing minimal impact on image quality. The approach was tested on high-resolution generative models and complex image datasets. The study evaluated the image quality of high-resolution generative models using the BigGAN-deep architecture pretrained on the ImageNet dataset. The architecture consists of 12 Gblocks with skip connections, allowing for the generation of hybrids by mixing features. The BigGAN-deep architecture consists of 12 Gblocks with skip connections, allowing for the generation of hybrids by mixing features. Intervening on two successive layers within a Gblock was found to be more effective for generating counterfactuals. Examples in Fig. 4 demonstrate the ability to generate high-quality counterfactuals with modified backgrounds while keeping a similar object in the foreground. In more challenging situations, meaningful combinations of original samples can still be achieved. In a challenging situation, meaningful combinations of original samples are generated, such as a teddy bear in a tree or a \"teddy-koala\" merging textures. The ability of pretrained classifiers to recognize original classes was compared using generated counterfactual images. The study compared the recognition rates of pretrained classifiers on Tensorflow-hub for original classes like teddy-bear or koala. Results showed higher rates with smaller pixel distances between hybrids and originals, especially at intermediate blocks 5-6 where Inception resnet outperformed others. Non-consensual classification examples are provided in Supplementary Table 3. The Inception resnet outperformed other classifiers in the study. Non-consensual classification examples in Supplementary Table 3 suggest different classifiers rely on different aspects of image content. A mathematical definition of disentanglement was introduced to characterize representation in deep generative architectures. Interpretable modules of internal variables were found in four different groups of channels. Our study identified interpretable modules of internal variables in various generative models trained on complex datasets, contributing to a better understanding of deep neural networks and their applications like style transfer and object recognition robustness assessment. This research direction aims to enhance the interpretability of deep neural networks and enable them to be used for tasks they were not originally trained for, contributing to more sustainable research in Artificial Intelligence. Trained generator architectures can be manipulated independently, with a mathematical representation using structural causal models (SCMs). Structural causal models (SCMs) are used to represent mechanistic models where parts can be manipulated independently. These models rely on structural equations (SEs) to assign values to variables based on other variables in the system and external influences. SEs use uppercase letters for variables and lowercase for specific values. In SCMs, SEs assign values to variables using uppercase letters for variables and lowercase for specific values. SEs remain valid even after interventions, modeling operations in computational graphs of neural networks. SCMs consist of interdependent modules represented by a directed acyclic graph G. A Causal Generative Model (CGM) M captures computational relations between input latent variables {z k} and the generator's output Y. The Causal Generative Model (CGM) M captures computational relations between input latent variables {z k}, generator's output Y, and endogenous variables forming an intermediate representation. The CGM can be decomposed into two steps: {Z k} \u2192 {V k} \u2192 Y in a feed-forward neural network. The CGM M comprises a directed acyclic graph G and a set S of N + 1 deterministic continuous structural equations that assign variables in a deterministic structural causal model. The graph exemplified in Fig. 2b consists of 3 endogenous variables, 2 latent inputs, and the output, aligning with Pearl's definition. CGMs have specificities reflecting practical model structures, where variable assignments may or may not involve latent/exogenous variables. The CGMs have specificities reflecting practical model structures, allowing for feed-forward networks with latent inputs. This ensures unambiguous variable assignments and basic properties in generative networks. The output Y is determined once z is chosen or a subset of variables V k, such as Pa y, is assigned. This allows for useful mappings in generative networks where variables and outputs typically exist on manifolds of smaller dimensions than their ambient space. Functions assigning Y from latent variables and endogenous variables are introduced. The output Y is determined by choosing z or a subset of variables V k. Functions assigning Y from latent and endogenous variables are introduced, with constraints on their values in euclidean space. An embedded CGM is defined by proper embeddings g M and g M, ensuring invertibility. The embedded CGM is defined by proper embeddings g M and g M, with image sets constrained by parameters of M. The image set Y M of a trained model should approximate the support of the data distribution being modeled. The image set Y M of a trained model is crucial as it should approximate the data distribution being modeled. Generative models aim to match Y M with the target data distribution. Topology of Y M is respected in transformations using embeddings, allowing inversion of g M. Generative models aim to match the image set Y M with the target data distribution. Topology of Y M is respected in transformations using embeddings, allowing inversion of g M. If Z of CGM M is compact, then M is embedded if and only if g M is injective. Generative models based on uniformly distributed latent variables are embedded CGMs if injective. Latent variables in many GANs are embedded CGMs if injective. Restricting VAEs' latent space to compact intervals can approximate the original CGM for most samples. The CGM framework allows defining counterfactuals in the network following Pearl (2014). The interventional CGM M h is defined by replacing structural assignments for V |E with specific assignments for a subset of endogenous variables E. This concept aligns with potential outcome theory and induces a transformation in the generative model's output. Our approach connects counterfactuals to a continuous map in the embedded CGM framework. Our approach connects counterfactuals to a form of disentanglement in a generative model, allowing transformations of internal variables. Intrinsic disentanglement is defined as a transformation of endogenous variables that only affects a subset E, leading to unambiguous assignments based on endogenous values. Intrinsic disentanglement in a generative model involves a transformation of endogenous variables that only affects a subset E, leading to unambiguous assignments based on endogenous values. This concept relates to a causal interpretation of the model's structure, showing robustness to perturbations of its subsystems like counterfactuals. Intrinsic disentanglement in a generative model involves robustness to perturbations of its subsystems, such as counterfactuals. Counterfactuals represent examples of perturbations that can be disentangled given their faithfulness. The proof of equivalence between faithful and disentangled transformations is discussed, showing the compactness and injectivity of the mappings involved. The proof shows the equivalence between faithful and disentangled transformations, ensuring robustness to perturbations in generative models. The mappings involved are compact and injective, with distinct subsets of latent variables assigned unambiguously. The subsets of latent variables, A and B, are assigned unambiguously, ensuring that the image set of this layer fully covers the Cartesian product of the subsets. This guarantees that T is an endomorphism of V M for any choice of endomorphism T E, making T well-defined and an endomorphism. The i.i.d. assumption for components of Z and the structure following the sufficient condition of Prop. 1 lead to modular subsets of endogenous variables associated with each V k, creating a disentangled representation in the hidden layer. The subsets of endogenous variables associated with each V k are modular and create a disentangled representation in the hidden layer. Increasing dimensions and i.i.d. sampling ensure an injective mapping, following embedded CGM assumptions. Counterfactual hybridization of V k components results in an influence map covering I k. Conditions on I k and thresholding guarantee a rank K binary factorization of matrix B. The \u03b2-VAE architecture, similar to DCGAN, guarantees a rank K binary factorization of matrix B with indicator vectors for each V k. Hyperparameters specified in Table 1 were used for both structures, following the method proposed by Berthelot et al. (2017) for the CelebA dataset. The pre-trained model with three blocks of convolutional layers was utilized. The pre-trained model used for the CelebA dataset by Berthelot et al. (2017) consists of three blocks of convolutional layers with skip connections for image sharpness. The architecture is based on the BigGan-deep model by Brock et al. (2018) for 256x256 ImageNet, obtained from Tensorflow-hub. No retraining was done on the model. The BigGan-deep architecture used for the CelebA dataset consists of ResBlocks with BatchNorm-ReLU-Conv Layers and skip connections. The model was pre-trained on 256x256 ImageNet and no retraining was performed. Influence maps were generated by a VAE on the CelebA dataset to show variance and strength of influence. Influence maps were generated by a VAE on the CelebA dataset, showing variance and strength of perturbations on pixels. FID analysis of BEGAN hybrids revealed small distances to generated data and each other, indicating closeness in distribution. Hybrids show small distances to generated data and each other, suggesting visually plausible images. Entropy values are larger for modules with poorer quality in Gblock 6. The results show that entropy values are larger for modules with poorer quality in Gblock 6, while hybrids based on interventions in Gblock 4 have smaller entropy values. Object texture is highlighted as key information for the classifier's decision in hybrids generated from Gblock 4. The NMF algorithm extracted modules from different Gblocks, with interventions generating hybrids. Large entropy was observed for the first module of Gblock 5, indicating a mix of cock and ostrich shape properties in the hybrid bird. Various discriminative models were used for classification outcomes. The intervention procedure generated hybrids with a mix of cock and ostrich shape properties. Different discriminative models were used to classify koala+teddy hybrids, showing the robustness of classifiers. The resultant hybrids resembled a teddy bear in a koala context, emphasizing the importance of object sensitivity in classification. The study showed that the nasnet large classifier is more robust to changes in context compared to other classifiers when classifying koala+teddy hybrids."
}