{
    "title": "HyerxgHYvH",
    "content": "We propose a Lego bricks style architecture for evaluating mathematical expressions, using small neural networks for specific operations. Eight fundamental operations are identified and learned using feed forward neural networks. Using small neural networks, fundamental arithmetic operations are learned, allowing for the development of larger networks to solve more complex computations like n-digit multiplication and division. This approach introduces reusability and generalizes for computations involving up to 7 digit numbers, including both positive and negative numbers. Our solution for computations involving n-digits extends to 7 digit numbers, including positive and negative numbers. Artificial Neural Networks (ANNs) learn from data during training, but often lack generalization, leading to performance degradation on unseen data. The performance of Artificial Neural Networks degrades on unseen data, even when data contains seen categories but is acquired under different conditions. This indicates that neural networks rely on memorization rather than understanding inherent rules, lacking quantitative reasoning and systematic abstraction. In contrast, other living species exhibit fundamental capabilities in numerical extrapolation and quantitative reasoning. When observing other living species, their intelligence is based on numerical extrapolation and quantitative reasoning. Children can memorize and extrapolate arithmetic operations from single to higher digits, showing the key to generalization is reusing memorized examples. Complex numerical operations are combinations of simple functions. The key to generalization in intelligence lies in reusing memorized examples. Complex numerical operations are combinations of simple functions, which can be developed by identifying and learning fundamental operations. Fundamental arithmetic operations are learned using simple neural networks, which are then reused to solve more complex problems like n-digit multiplication and division. This approach is a generalized solution that works for both positive and negative numbers, unlike existing methods. Our solution, unlike existing methods, can handle both positive and negative numbers. Neural networks are effective at approximating mathematical functions, including simple arithmetic operations like multiplication and division. However, the complexity of proposed networks arises from the challenge of generalizing over unseen data. The architecture of proposed networks is complex due to the challenge of generalizing over unseen data. Recent works have attempted to train networks that can generalize with minimal training data. EqnMaster uses generative recurrent networks to approximate arithmetic functions, but struggles to generalize beyond 3-digit numbers. The most recent work in this area is the Neural Arithmetic Logic Unit. The Neural Arithmetic Logic Unit (NALU) uses linear activations to predict arithmetic function outputs, but struggles with extrapolation issues. A Feed Forward Network can solve arithmetic expressions, but may not be the most efficient architecture. Optimal Depth Networks using binary logic gates can efficiently perform simple arithmetic functions inspired by digital circuits, as studied in various researches. Our work builds on previous research on digital circuits and neural networks for arithmetic operations. We propose a network that can predict the output of basic arithmetic functions for both positive and negative decimal integers, unlike existing models that only work on limited digits positive integers. Our proposed model trains smaller networks for different sub tasks in arithmetic operations such as signed multiplication, division, and cross product. By combining these smaller networks, we can perform complex tasks and generalize solutions from 1 digit to n-digit arithmetic. Neural networks can be used to design networks for complex tasks like arithmetic operations. Multiplication and division can be implemented as repeated addition and subtraction, respectively, using digital circuits known for accurate arithmetic operations. These circuits can be easily scaled by increasing shifters and accumulators. Neural networks can simulate digital circuits for arithmetic operations like n-digit multiplication and division. Six neural networks were designed to perform fundamental operations including addition, subtraction, multiplication, place value shifting, and sign calculation. Neural networks can simulate digital circuits for arithmetic operations like n-digit multiplication and division. The basic function of a neuron network is a sum transform where inputs are multiplied with weights and passed through an activation function to produce the final output. These fundamental blocks can be used to design complex functions including an arithmetic equation calculator. The addition and subtraction modules in the neural network are implemented using single neurons with specific weights. The addition module has weights {+1, +1} while the subtraction module has weights {+1, -1}. These modules facilitate shift-and-add multiplication by multiplying digits one at a time from right to left. The shift-and-add multiplication module in the neural network uses a place value shifter to combine the outputs of digit multiplications and obtain the final result. It can handle n inputs, each taking a 1-digit number as input, and utilizes fixed weights in powers of 10 for each preceding digit. The neural network for single digit multiplication uses fixed weights in powers of 10 for each digit. It has two input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. The model takes two 1-digit integers as input and produces 82 possible outcomes. The highest-ranked prediction is selected as the output. The neural network for single digit multiplication uses fixed weights in powers of 10 for each digit. It has two input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. The model takes two 1-digit integers as input and produces 82 possible outcomes. The highest-ranked prediction is selected as the output. This network computes the absolute value of a single number using a neural network with 2 hidden layers. The process involves x + x and x \u2212 x operations in the first hidden layer, a maxpool layer in the second layer, and subtraction of the input from the output of the maxpool layer. The input sign calculator extracts the sign of an input number x using a single neuron with a specific activation function. The output sign calculator computes the result from a multiplication or division of two numbers. The neural network for single digit multiplication uses fixed weights in powers of 10 for each digit. It has two input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. The model takes two 1-digit integers as input and produces 82 possible outcomes. The network computes the absolute value of a single number using a neural network with 2 hidden layers. The process involves x + x and x \u2212 x operations in the first hidden layer, a maxpool layer in the second layer, and subtraction of the input from the output of the maxpool layer. The input sign calculator extracts the sign of an input number x using a single neuron with a specific activation function. The output sign calculator computes the result from a multiplication or division of two numbers. The network also includes a neuron with an activation function x/(1 + mod(x)) and a sign and magnitude input passed on to a hidden layer. The neural network uses fixed weights for single digit multiplication, with input sign and magnitude passed to a hidden layer of 10 neurons. The output layer predicts the sign multiplication result using soft-sign activation. The module assigns a sign to complex operation outputs like multiplication and division. Signed multiplication involves converting numbers to positive integers, performing operations, and calculating input and output signs. The high-level diagram in Fig. 2(a) shows signed multiplication process. Numbers are converted to positive integers using absolute operation. Input and output sign calculators determine signs. Multiplication is done using a multiply sub module in Fig. 2(c), which tokenizes inputs into single digits for multiplication. Each digit is multiplied with the 1st digit of the other number, and results are added with carry forward. The process involves multiplying each token of the multiplicand with the 1st token of the multiplier, adding the results with carry forward, and combining them to form a single number. This process is repeated for each token of the multiplier, resulting in an output with n+m digits. The architecture for the multiplication process involves row-wise multiplication with a maximum of m+1 output digits per row. The division model separates sign and magnitude during pre-processing, inspired by the long division model. The n-digit divisor is multiplied with single digit multipliers and subtracted from the dividend chunk. Additional layers are introduced to select the smallest non-negative integer from the outputs. The division model involves selecting the smallest non-negative integer from the outputs of row-wise multiplication. The selected node represents the remainder and quotient result of division for the n-digit dividend and divisor. The quotient is combined over iterations and the remainder is carried over to the next digit in the divisor. A division model based on digital circuitry for decimal digits can be generated. The study compares the results of signed arithmetic operations using a division architecture based on digital circuitry for decimal digits. Additionally, a comparison is made with the Neural Arithmetic and Logic Unit (NALU) implementation, where the results are trained and tested on a prediction dataset. Experiment 1 shows that our model outperforms recurrent and discriminative networks in accuracy, even within their testing range. Signed multiplication is exclusive to our model. Experiment 2 compares our results. Experiment 2 compares our model to the state-of-the-art Neural Arithmetic and Logic Unit (NALU) for arithmetic operations on positive integers. NALU fails outside the range of 10-20, while our model achieves 100% accuracy with their dataset. In this experiment, the results show that complex tasks can be divided into smaller sub-tasks, which can be solved by training smaller independent neural networks. This approach allows for solving more difficult tasks by combining the outputs of these smaller networks. In this work, fundamental operations for arithmetic tasks are identified and learned using simple neural networks. These smaller networks are then combined to solve more complex problems like n-digit multiplication and division. One limitation is the use of float operation in the tokenizer, which hinders end-to-end training. The proposed work involves using smaller neural networks to solve arithmetic tasks like n-digit multiplication and division. A limitation is the use of float operation in the tokenizer, which hinders end-to-end training. Future work includes designing a cross product network and developing a point cloud segmentation algorithm using multiple smaller networks."
}