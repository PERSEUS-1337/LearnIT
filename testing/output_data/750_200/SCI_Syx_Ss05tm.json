{
    "title": "Syx_Ss05tm",
    "content": "Deep neural networks are vulnerable to adversarial attacks in computer vision, where perturbations to images can cause the network to make mistakes. Previous attacks aimed to degrade performance or manipulate specific outputs, but new attacks reprogram the model to perform tasks chosen by the attacker without specifying the desired output for each input. This attack finds a single perturbation that can be added to all inputs to cause the model to perform a specific task. Adversarial reprogramming involves finding a single perturbation that can be added to all test-time inputs to a machine learning model, causing it to perform a task chosen by the attacker. This can repurpose models to perform new tasks, such as a counting task or classifying MNIST and CIFAR-10 examples. The study of adversarial examples focuses on the danger posed by attackers causing model prediction errors. The study of adversarial examples focuses on attackers causing model prediction errors with small changes to inputs, leading to potential dangers like manipulating a self-driving car or insurance claim values. Various methods have been proposed to construct and defend against these adversarial attacks. Various methods have been proposed to construct and defend against adversarial attacks, including untargeted attacks that degrade model performance and targeted attacks that aim for specific outputs. In this work, a novel adversarial goal is considered: reprogramming the model to perform a task chosen by the attacker without needing to compute the specific desired output. An adversary can reprogram a model to perform a different task by learning adversarial reprogramming functions that map between the original task and the adversarial task. An adversary can reprogram a model by adjusting parameters to achieve a mapping between the original task and the adversarial task. The function h f draws small images in the center of large images, while h g maps output class labels. Adversarial reprogramming involves repurposing a model to perform a new task by transforming input/output formats, referred to as \u03b8 as an adversarial program. The attack does not need to be imperceptible to humans to succeed, with potential consequences including theft of computational resources. Adversarial reprogramming can lead to theft of computational resources, repurposing of AI-driven assistants, and abuse of machine learning services for unethical tasks. The attack may not need to be imperceptible to humans to succeed, with potential risks discussed in more detail. The flexibility of deep neural networks allows for repurposing through changes in inputs, as shown in studies like BID36 and BID21. This adaptability highlights the network's ability to achieve unique output patterns and high accuracy even with restricted parameter updates. An additive offset to the input can be equivalent to modifying the network's first layer biases. In this paper, the concept of adversarial reprogramming is introduced, where updates are limited to a low dimensional subspace. Crafting adversarial programs involves modifying a neural network's input to perform a new task. This is achieved by introducing new parameters through an additive offset, which corresponds to an update in a low dimensional parameter subspace. In Section 3, a training procedure is presented for crafting adversarial programs that alter neural networks to perform new tasks. Section 4 demonstrates experimental results targeting various convolutional neural networks for ImageNet classification. The networks are reprogrammed to count squares, classify MNIST digits, and classify CIFAR-10 images. The susceptibility of trained and untrained networks to adversarial reprogramming is examined, showing the possibility of reprogramming tasks with data dissimilar to the original. Adversarial reprogramming involves altering neural networks to perform new tasks using adversarial data. This process can conceal adversarial programs and data, demonstrating the limitations of transfer learning in explaining adversarial reprogramming. Adversarial examples are intentionally designed inputs to cause machine learning models to make mistakes. Adversarial attacks are designed to cause machine learning models to make mistakes by creating images that lead to incorrect predictions. These attacks can be untargeted or targeted, and have been proposed in various domains such as malware detection and generative models. When presented with adversarial images, the model predicts incorrect classes. Reprogramming methods aim to produce specific functionality rather than a hardcoded output in adversarial attacks for reinforcement learning tasks. Authors have observed that the same modification can be applied to many inputs to create adversarial examples, such as designing an \"adversarial patch\" to switch predictions of multiple models. In adversarial attacks, reprogramming methods can create specific functionality across various inputs. For instance, an \"adversarial patch\" can manipulate models to predict a specific class like a toaster. Parasitic computing exploits network communication protocols to make a system perform unintended tasks, while weird machines represent a unique class of machines. Adversarial reprogramming is a form of parasitic computing that exploits network communication protocols to run arbitrary code on a targeted computer. It can be compared to weird machines, where carefully crafted inputs can manipulate models to perform specific tasks. Adversarial reprogramming functions within the neural network paradigm without gaining access to the host computer. Transfer learning and adversarial reprogramming aim to repurpose neural networks for new tasks by leveraging knowledge from one task to perform another. Neural networks exhibit useful properties for various tasks, such as developing features resembling Gabor filters in early layers when trained on images, even with different datasets or training objectives. Transfer learning allows for repurposing neural networks for new tasks by leveraging knowledge from one task to perform another. It is possible to train a convolutional neural network for one task and then use a linear SVM classifier to make it work for other tasks. This is different from adversarial reprogramming, where model parameters can be changed for the new task. In adversarial settings, an attacker is unable to alter the model and must achieve their goals without changing the model. Adversarial reprogramming involves changing model parameters for a new task, unlike transfer learning where knowledge is leveraged from one task to another. The adversary aims to reprogram the neural network by crafting an adversarial program to be included in the input. The adversary's goal is to reprogram the model for a new task by adding an adversarial program to the network input. This program is not specific to a single image but is applied to all images. The adversarial program parameters are to be learned, and a masking matrix is used in the formulation. The adversarial program parameters are defined as W \u2208 R n\u00d7n\u00d73 to be learned, with a masking matrix M that is 0 for adversarial data locations. The mask is not necessary and is used for visualization purposes. The perturbation is bounded by tanh (\u00b7) to be within (-1, 1) range. The adversarial task involves applying perturbations to images in the dataset, with a mapping function to map labels from the adversarial task to ImageNet labels. The adversarial task involves mapping labels from an adversarial task to ImageNet labels. The optimization problem includes a weight norm penalty to reduce overfitting, optimized with Adam and exponentially decaying learning rate. Hyperparameters are provided in Appendix A. After optimizing with Adam and exponentially decaying the learning rate, the adversarial program has minimal computation cost for the adversary. It only requires computing X adv and mapping the resulting ImageNet label to the correct class. Adversarial reprogramming exploits the nonlinear behavior of the target model, unlike traditional adversarial examples based on linear approximations. Adversarial reprogramming exploits the nonlinear behavior of the target model by conducting experiments on six architectures trained on ImageNet to perform different adversarial tasks. The weights of the models were obtained from TensorFlow-Slim, and top-1 ImageNet precisions are shown. The study explored adversarial reprogramming on six ImageNet-trained models, investigating resistance to reprogramming and comparing susceptibility to random networks. The possibility of reprogramming networks with dissimilar data was examined, along with concealing adversarial programs and data. The process began with a simple task of counting squares. The study involved adversarial reprogramming on ImageNet-trained models, focusing on concealing adversarial programs and data. A simple task of counting squares was used to illustrate the procedure. Images with white squares were generated and embedded in an adversarial program, resulting in larger images with the squares at the center. The study involved adversarial reprogramming on ImageNet-trained models, focusing on concealing adversarial programs and data. Adversarial programs were created as frames around images for a counting task. Labels from ImageNet were used to represent the number of squares in each image, unrelated to the original labels. Accuracy was evaluated by comparing network predictions to the actual number of squares in sampled images. The study demonstrated the vulnerability of neural networks to reprogramming by using adversarial programs to master simple tasks like counting squares. This was shown through testing on ImageNet-trained models and more complex tasks like classifying MNIST digits. The adversarial program's accuracy was measured in both test and train scenarios. In a study on neural network vulnerability, adversarial reprogramming was demonstrated on the task of classifying MNIST digits. The adversarial program successfully reprogrammed ImageNet networks to function as an MNIST classifier by presenting an additive adversarial program. Our results demonstrate successful reprogramming of ImageNet networks to classify CIFAR-10 images using adversarial programs. The reprogramming generalizes well from training to test set and increases accuracy on CIFAR-10 from chance levels. Our adversarial program successfully increased the accuracy on CIFAR-10 from chance levels to a moderate accuracy. The programs show visual similarities despite being trained for different tasks, with ResNet architecture programs exhibiting low spatial frequency texture. The text discusses the susceptibility of models to adversarial reprogramming, specifically focusing on an Inception V3 model trained with adversarial training on ImageNet data. The results indicate successful adversarial reprogramming to classify MNIST digits. The Inception V3 model trained with adversarial training on ImageNet data was successfully reprogrammed to classify MNIST digits, showing that standard approaches to adversarial defense are ineffective against adversarial reprogramming. This is due to differences in goals and the magnitude of adversarial programs compared to traditional attacks. The goal of adversarial reprogramming attacks is to repurpose the network without causing a specific mistake. The magnitude of these attacks can be large, unlike traditional small perturbation attacks. Adversarial defense methods may not generalize to data from the adversarial task. Experiments were conducted on models with random weights using the MNIST reprogramming task. The MNIST classification task was easy for networks pretrained on ImageNet. The study focused on adversarial reprogramming attacks, showing that networks pretrained on ImageNet performed better on the MNIST task compared to randomly initialized networks. Adversarial programs differed between the two types of networks, highlighting the importance of the original task in neural network performance. The study found that networks pretrained on ImageNet outperformed randomly initialized networks on the MNIST task. This suggests that the original task the neural networks perform is crucial for adversarial reprogramming. Random networks may not perform well initially due to poor scaling of network weights, while trained weights are better conditioned. The study showed that pretrained ImageNet networks outperformed randomly initialized networks on the MNIST task due to poor scaling of network weights at initialization. Despite removing any resemblance between the adversarial data and original data, the ImageNet network was successfully reprogrammed to classify shuffled MNIST digits. Reprogramming ImageNet networks to classify shuffled MNIST and CIFAR-10 images showed comparable accuracy to standard datasets, with shuffled CIFAR-10 accuracy decreasing due to convolutional structure limitations. The convolutional structure of the network is not useful for classifying shuffled images, but accuracy is comparable to fully connected networks. Transferring knowledge between original and adversarial data does not fully explain susceptibility to reprogramming. Results suggest reprogramming across tasks with unrelated datasets and domains, with the possibility of limiting visibility of adversarial perturbations. In experiments with an Inception V3 model, adversarial reprogramming was successful even with a small program size, resulting in lower accuracy. The visibility of adversarial perturbations was further limited by nearly imperceptible adversarial programs. In experiments with an Inception V3 model, adversarial reprogramming was successful even with a small program size, resulting in lower accuracy. Adversarial reprogramming remained successful even with nearly imperceptible programs, as shown in the results. Additionally, the possibility of concealing the adversarial task by hiding both the data and program within a normal ImageNet image was tested. This involved shuffling the pixels of the adversarial data (MNIST) to hide its structure, and limiting the scale of both the adversarial program and data. In experiments with an Inception V3 model, adversarial reprogramming was successful even with a small program size, resulting in lower accuracy. The adversarial data (MNIST) structure was hidden by shuffling pixels and limiting the scale of the program and data. The resulting image was combined with a random ImageNet image, and the adversarial program was optimized. The study successfully reprogrammed a network to classify MNIST digits using adversarial images that resemble normal ImageNet images. The adversarial task was hidden by shuffling pixels and selecting an ImageNet image, demonstrating the potential for concealing adversarial intentions. The study demonstrated the effectiveness of adversarial reprogramming by using complex schemes to hide the adversarial task and optimizing the choice of ImageNet images. Trained neural networks were found to be more vulnerable to reprogramming than random networks, even when the data structure is different. This shows the flexibility of repurposing trained weights for new tasks, suggesting practical dynamical reuse of neural circuits. Our results show the potential for practical reuse of neural circuits in modern artificial neural networks, enabling easier repurposing, increased flexibility, and efficiency through shared compute. Recent machine learning advancements focus on building large dynamically connected networks with reusable components. The reduced performance targeting random networks or reprogramming for CIFAR-10 classification may be due to limitations in adversarial perturbation expressivity or optimization. Our study explored limitations in expressivity and trainability when reprogramming neural networks for CIFAR-10 classification. Future research could investigate similar attacks in different domains like audio, video, or text. The ability to reprogram trained networks to classify shuffled images without spatial structure suggests potential for practical reuse of neural circuits. Trained networks can be reprogrammed to classify shuffled images, indicating potential for reprogramming across domains. Adversarial reprogramming of RNNs, especially those with attention or memory, could be particularly interesting. If adversarial programs can be found for simple operations, they could be composed to reprogram the network. If counter is zero BID28, change input attention location. Adversarial programs can reprogram RNNs for various nefarious purposes, including theft of computational resources. For example, an attacker could create a program to make a computer vision classifier solve image captchas for spam account creation. If RNNs can be reprogrammed, this computational theft could extend to other tasks. The text discusses the potential danger of reprogramming neural networks for malicious purposes, such as using image captcha-solving for creating spam accounts. Adversaries could repurpose computational resources for unethical tasks, posing a threat to the ethical principles of ML service providers. The study introduces a new class of adversarial attacks that demonstrate the possibility of reprogramming neural networks for novel malicious tasks. The study introduces a new class of adversarial attacks that demonstrate the possibility of reprogramming neural networks for novel malicious tasks, showing surprising flexibility and vulnerability in deep neural networks. Future research should focus on understanding the properties and limitations of adversarial reprogramming and ways to defend against it. Neural networks can be reprogrammed adversarially even with unrelated data. Shuffled MNIST digits combined with an adversarial program successfully reprogram the Inception V3 model to classify the shuffled digits, despite the data being unrelated to the original task."
}