{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map observed input-output pairs to a distribution over regression functions efficiently. However, NPs suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data they condition on. This issue is addressed by incorporating attention. Neural Processes (NPs) address underfitting by incorporating attention, allowing inputs to attend to relevant context points for accurate predictions. This improves prediction accuracy, speeds up training, and expands the range of modelled functions in regression tasks. In Bayesian machine learning, non-parametric models like Gaussian Processes (GPs) are popular for computing a distribution over functions that map inputs to outputs, allowing for reasoning about multiple functions consistent with the data and capturing co-variability in outputs given inputs. Neural Processes (NPs) offer an efficient method for modeling a distribution over regression functions, with prediction complexity linear in the context set size. They can predict the distribution of an arbitrary target output based on a set of context input-output pairs of any size, allowing them to model data generated from a stochastic process. NPs have different training regimes compared to Gaussian Processes (GPs) in Bayesian machine learning. Neural Processes (NPs) model data from a stochastic process, trained on multiple realizations. Unlike Gaussian Processes (GPs), NPs can predict arbitrary target outputs based on context pairs. However, NPs tend to underfit the context set, leading to inaccurate predictions. Neural Processes (NPs) underfit the context set, resulting in inaccurate predictions. The encoder aggregates context to a fixed-length latent summary, leading to imperfect reconstructions in predictions. The encoder in Neural Processes (NPs) aggregates context to a fixed-length latent summary, causing underfitting. To address this bottleneck, inspiration is drawn from Gaussian Processes (GPs) to improve target predictions. To address underfitting in Neural Processes (NPs), inspiration is drawn from Gaussian Processes (GPs) to improve target predictions by using differentiable attention to attend to relevant contexts while preserving permutation invariance. Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by using differentiable attention to attend to relevant contexts for target prediction while maintaining permutation invariance. ANPs show better context reconstruction, faster training, and enhanced expressiveness compared to NPs, allowing them to model a wider range of functions. The Neural Process (NP) is a model for regression functions that map input to output. It defines a family of conditional distributions that can model multiple contexts and targets, invariant to ordering. The deterministic NP models these distributions using a function that aggregates input-output pairs into a finite dimensional space. The deterministic NP models conditional distributions using a function that aggregates input-output pairs into a finite dimensional space with permutation invariance. Context pairs are passed through an MLP and aggregated by taking the mean to form a representation. The likelihood is modelled by a Gaussian factorised across targets with mean and variance given by passing inputs through an MLP. The unconditional distribution is defined by a fixed vector. The NP model includes a global latent variable z to address uncertainty in predictions, modelled by a factorised Gaussian parametrised by s. The encoder consists of q, r, and s, while the decoder represents the likelihood. See FIG1 for model diagrams. The NP model includes a global latent variable z modeled by a factorised Gaussian parametrised by s. The encoder consists of q, r, and s, while the decoder represents the likelihood. The parameters of the encoder and decoder are learned by maximizing the ELBO for incorporating attention. The NP model incorporates attention by maximizing the ELBO for learning encoder and decoder parameters. The model reconstructs targets with a KL term to ensure the summary of contexts is close to the summary of targets. Neural Processes (NPs) learn a wide family of conditional distributions with scalability, flexibility, and permutation invariance properties. The model incorporates attention by maximizing the ELBO for learning encoder and decoder parameters, reconstructing targets with a KL term to ensure context and target summaries are close. Neural Processes (NPs) offer scalability, flexibility, and permutation invariance in learning conditional distributions. However, they lack consistency in contexts, as the distribution of targets may vary based on the order of generation. Maximum-likelihood learning aims to minimize the KL divergence between data-generating process and NP conditional distributions. Neural Processes (NPs) approximate conditional distributions by minimizing KL divergence. An attention mechanism computes weights for key-value pairs to form query values, with permutation invariance being crucial for NPs. The attention mechanism in Neural Processes (NPs) computes weights for key-value pairs to form query values, with permutation invariance being crucial. Differentiable addressing mechanisms have been successfully applied in Deep Learning for tasks like handwriting generation, recognition, and neural machine translation. Self-attention has been used for expressive sequence-to-sequence mappings in natural language processing and image modeling. In natural language processing and image modeling, attention mechanisms are used for expressive sequence-to-sequence mappings. Different types of attention, such as locality-based and dot-product attention, are employed to weight keys according to query values. This allows for measuring similarity and effectively weighting the keys. Dot-product attention uses the dot-product between query and keys to measure similarity and weight keys according to values. Multihead attention is a parametrised extension where keys, values, and queries are linearly transformed for each head, then dot-product attention is applied to give head-specific values. This architecture allows the query to attend to different keys for each head. The final values are produced by concatenating and linearly transforming values in a multihead architecture. Self-attention is applied to context points to compute representations of each (x, y) pair, with the target input attending to these context representations for prediction. The representation of each context pair before mean-aggregation is computed using a self-attention mechanism in both deterministic and latent paths. The self-attention mechanism is used to model interactions between context points before mean-aggregation. It helps obtain richer representations encoding relations between the points. Higher order interactions are modeled by stacking self-attention. The self-attention mechanism allows for modeling interactions between context points before mean-aggregation, enabling richer representations. In the deterministic path, a cross-attention mechanism replaces mean-aggregation, allowing each query to attend closely to relevant context points for prediction. The latent path preserves global latent dependencies for target predictions. The latent path preserves global latent dependencies for target predictions by inducing correlations in the marginal distribution of the predictions, while the deterministic path models local structure. The decoder remains the same, with a query-specific representation replacing the shared context representation. Permutation invariance is maintained with the attention mechanism, and using uniform attention throughout recovers the NP. The attention mechanism in the NP with attention preserves permutation invariance in contexts. Using uniform attention recovers the NP. The computational complexity increases due to self-attention across contexts, raising it from O(n + m) to O(n(n + m)). The (A)NP computes weights for all contexts and targets using matrix multiplication, allowing for parallel computation. ANPs learn faster than NPs in terms of training iterations and wall-clock time, despite being slower at prediction time. The (A)NP should be trained on multiple functions that are realisations of a stochastic process. The (A)NP should be trained on multiple functions that are realisations of a stochastic process, drawing a batch of realisations at each training iteration to optimize the loss. The decoder architecture remains consistent across experiments, with 8 heads for multihead. In this study, (A)NPs are trained on data generated from a Gaussian Process with a squared-exponential kernel and small likelihood noise. Two settings are explored: fixed hyperparameters of the kernel throughout training and randomly varying hyperparameters at each iteration. The number of contexts and targets are randomly chosen, and x-values are drawn uniformly at random in a specific range. The focus is on the use of cross-attention in 1D data without self-attention. For 1D data generated from a Gaussian Process, attention mechanisms like cross-attention in ANP show faster error reduction and lower values compared to NP, especially with dot product and multihead attention. This is observed during training iterations. The rapid decrease in reconstruction error and lower values at convergence compared to the NP, especially for dot product and multihead attention, is observed not only against training iteration but also against wall clock time. Learning is fast despite the added computational cost of attention. The computation times of Laplace and dot-product ANP are similar to the NP for the same value of d, while multihead ANP takes around twice the time. Raising the bottleneck size (d) in the deterministic and latent paths of the NP helps achieve better reconstructions, but there seems to be a limit in improvement. Raising the bottleneck size in the deterministic and latent paths of the NP improves reconstructions, but there is a limit to the improvement. Using ANPs offers significant benefits over simply increasing the bottleneck size in NPs. Visualizing the learned conditional distribution in FIG2 (right) allows for a qualitative comparison of attention mechanisms. The conditional distribution for attention mechanisms is compared qualitatively. The NP underfits the context, Laplace and dot-product attention show different behaviors. Dot-product attention accurately predicts most context points. Dot-product attention uses parameterized representations for keys and queries, while Laplace attention is parameter-free. In a comparison of attention mechanisms, dot-product attention, using parameterized representations for keys and queries, outperforms Laplace attention. However, dot-product attention displays non-smooth predictions, while multiple heads in multihead attention help smooth out interpolations for better context reconstruction and target prediction. The multiple heads in multihead attention help smooth out interpolations for better context reconstruction and target prediction, showing that the ANP is more expressive than the NP and can learn a wider range of functions. The ANP is shown to be more expressive than the NP in handling hyperparameter settings and learning a wider range of functions. A proof-of-concept experiment demonstrates the utility of sampling entire functions from the (A)NP and accurate context reconstructions for a toy Bayesian Optimization problem. See Appendix C for detailed results analysis. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Results of three different models are shown on both datasets. See Appendix D for experimental details. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Results of three different models are shown on both datasets, including NP, Multihead ANP, and Stacked Multihead ANP. Predictions are generated based on the mean of p(y T |x T , r C , z) for varying numbers of random context pixels. The ANP, Multihead ANP, and Stacked Multihead ANP models are compared on MNIST and CelebA datasets using self-attentional layers in the encoder. Predictions are generated based on the mean of p(y T |x T , r C , z) for varying numbers of random context pixels. The Stacked Multihead ANP shows accurate reconstructions of the whole image, while the NP provides diverse predictions but less accurate reconstructions. Attention helps achieve crisper inpaintings, enhancing the ANP's ability to model less smooth 2D functions. The diversity in faces and digits obtained with different values of z is apparent in the samples, providing evidence that z can model the global structure of the image. The model can generalize to context sizes larger than what it was trained on, such as 512 context points for half the image. The model can generalize to larger context sizes than trained on, with Multihead ANP showing improved context reconstruction error and NLL for target points. Stacked self-attention provides noticeable gains in crispness and global coherence qualitatively. Visualizing each head of Multihead ANP for CelebA shows where attention focuses on target pixels. Visualizing each head of Multihead ANP for CelebA reveals distinct roles for different heads, such as focusing on nearby pixels, specific regions, or exploiting symmetry in faces. One application of (A)NPs trained on images is mapping images from one resolution to another by predicting pixel intensities in a continuous space, exploiting symmetry in faces and focusing on specific regions. The model can predict pixel intensities in a continuous space, mapping a given resolution to a higher resolution. ANPs may provide accurate reconstructions for reliable mappings between different resolutions, as shown in FIG6. The Stacked Multihead ANP model can accurately map low resolutions (4x4 or 8x8) to realistic 32x32 outputs with diversity. It can also map 32x32 images to even higher resolutions like 256x256. The Stacked Multihead ANP model can map low resolutions to realistic 32x32 outputs with diversity and even higher resolutions like 256x256, showing evidence of learning internal representations of faces with sharper edges compared to baseline interpolation methods. The ANP model can map low resolutions to realistic 32x32 outputs with diversity and even higher resolutions like 256x256, showing evidence of learning internal representations of faces with sharper edges compared to baseline interpolation methods. The iris from the sclera is a feature that is not possible with simple interpolation. The qualitative plots in this section were given from the same model for each attention mechanism, learned by optimizing the loss over random context pixels and target pixels. The ANP is not claimed to replace state-of-the-art algorithms for image inpainting or super-resolution, but rather to highlight its flexibility in modeling a wide range of conditional applications. The ANP model demonstrates the ability to generate realistic 32x32 outputs with diversity and even higher resolutions like 256x256, showcasing its capability to learn internal representations of faces with sharper edges. Attention mechanisms in NPs show similarities to Gaussian Processes in measuring similarity between points in the same domain. The use of attention in NPs highlights flexibility in modeling various conditional distributions. In NPs, there is a parallel between GP kernels and attention, measuring similarity between points in the same domain. Attention in an embedding space is related to Deep Kernel Learning, where a GP is applied to learned data representations. Training regimes of GPs and NPs differ, making direct comparison challenging. One approach is to learn the GP via the training regime of NPs, updating kernel hyperparameters at each iteration through one gradient step of the marginal likelihood. One approach for comparison is to learn the GP via the training regime of NPs, updating kernel hyperparameters at each iteration through one gradient step of the marginal likelihood. GPs have the benefit of being consistent stochastic processes, with exact expressions for predictive uncertainties, but may require kernel approximations. Variational Implicit Processes (VIP) BID19 are related to NPs, defining a stochastic process using a decoder setup with a finite dimensional z. The process and its posterior given observed data are approximated by a GP and learned via a generalisation of the Wake-Sleep algorithm BID13. Meta-Learning (A)NPs focus on few-shot learning, using input-output pairs from a new function. The Wake-Sleep algorithm BID13 and Meta-Learning (A)NPs focus on few-shot learning, using input-output pairs from a new function. Various works in few-shot classification and Meta-RL utilize attention for tasks such as continuous control and visual navigation. Attention is also used for few-shot density estimation in numerous works. Few-shot density estimation and regression tasks in Meta-RL have been explored using attention mechanisms in various works. The Neural Statistician and Variational Homoencoder models utilize permutation invariant encoders for summarizing data sets. Vfunc also explores regression on a 1D domain without attention mechanisms. Vfunc BID1 explores regression on a 1D domain without attention mechanisms, optimizing an approximation to the entropy of the latent function. Generative Query Networks are models for spatial prediction, where x represents viewpoints and y represents frames of a scene. Rosenbaum et al. (2018) apply GQN to 3D localization with an attention mechanism applied to patches of context frames. In this work, the authors propose Attention Augmented Neural Processes (ANPs) to address underfitting issues in 3D localization tasks. By incorporating attention mechanisms, ANPs improve prediction accuracy, training speed, and the range of functions that can be modeled. Future work includes exploring model architecture with cross-attention in the latent path to capture dependencies across different contexts. Future work for ANPs includes incorporating cross-attention in the latent path to model dependencies across local latents and introducing a global latent similar to the Neural Statistician setup. Another potential application is training ANPs on text data for stochastic blank filling. The Image Transformer (ImT) BID21 shows connections with ANPs in how it predicts consecutive pixel blocks using local self-attention, similar to how ANPs attend to context pixels to predict target pixels. Replacing the MLP in the decoder of the ANP with self-attention across target pixels creates a model resembling an ImT defined on arbitrary pixel orderings. This contrasts with the original ImT, which assumes a fixed ordering and is trained autoregressively. Equipping ANPs with self-attention in the decoder may extend their expressiveness, but the ordering and grouping of targets will become crucial. In this setup, targets will affect each other's predictions, making ordering and grouping important. Architectural details of NP and Multihead ANP models for regression experiments are shown in Figure 8. MLPs have relu non-linearities except the final layer. Latent path outputs parameterize q(z|s C) and decoder outputs parameterize p(y i |z, x C, y C). The decoder outputs parameters for p(y_i|z, x_C, y_C, x_i) using a softplus function. Different forms of multihead cross-attention are used in 1D and 2D regression experiments. No dropout is used to limit stochasticity to the latent z. Self-attention has the same architecture as cross-attention but with specific output configurations. In the 2D Image regression experiments, two layers of self-attention are stacked for Stacked Multihead ANP. More layers did not result in significant improvements. Parameters for the data generating GP kernel experiments are set at l = 0.6 and \u03c3 2 f = 1. In the experiments, a length scale of 0.6 and kernel scale of 1 are used for the fixed kernel hyperparameter case. For the random kernel hyperparameter case, parameters are sampled from specific ranges. A batch size of 16 is utilized, and the Adam Optimiser with a fixed learning rate is employed. In the experiments, a fixed learning rate of 5e-5 is used with the Adam Optimiser BID14. Comparisons between trained (A)NP models and oracle GP show Multihead ANP is closer to the oracle GP but underestimates predictive variance. Variational inference used for learning ANP may lead to underestimates of predictive variance, warranting further investigation. The ANP underestimates predictive variance, especially with dot-product attention collapsing to local minimums. Investigating how to address this issue is crucial. The ANP struggles with predictive variance underestimation, particularly with dot-product attention collapsing to local minimums. The KL term in NP loss differs between fixed and random GP kernel hyperparameters, affecting model predictions. In the case of multihead ANP, the KL quickly goes to 0, indicating the deterministic path is sufficient for accurate predictions. However, in the random hyperparameter case, there is added variation in the data, leading to non-zero KL and the use of latents to model uncertainty in the stochastic process. The model believes there are multiple realizations of the process that can explain the contexts well, using latents to model this variation. The ANPs trained on 1D GP data are used to tackle the BO problem of finding the minimum of test functions drawn from a GP prior. The ANPs trained on 1D GP data use latents to model variation and tackle the BO problem of finding the minimum of test functions drawn from a GP prior. ANPs consider all previous function evaluations as context points for BO, using Thompson sampling to act according to the minimal predicted value. Results averaged over 100 test functions show in FIG0. The simple regret is consistently smallest for a NP with multihead attention, approaching the oracle GP. The cumulative regret decreases most rapidly for multihead, utilizing previous function evaluations effectively for predicting the function minimum. The initial lower cumulative regret compared to the oracle GP is due to under-exploration caused by uncertainties of ANP. The cumulative regret is initially lower than the oracle GP due to under-exploration caused by uncertainties of ANP. Random pixels of an image are taken as targets and a subset as contexts, with rescaled x and y values. Batch size of 16 is used for MNIST and CelebA datasets with specific learning rates. The batch size used for both MNIST and CelebA datasets is 16, with learning rates of 5e-5 and 4e-5 respectively. The self-attention architecture is similar to the Image Transformer BID21, without Dropout or positional embeddings. The same architecture is used for both datasets with minimal tuning of hyperparameters. A single sample of q(z|s C ) is used for MC estimation of the loss. The NP overestimates predictive variance, especially around edges, but NP with attention reduces uncertainty as context increases. Stacked Multihead ANP improves results significantly over Multihead ANP, providing sharper images. Stacked Multihead ANP improves results significantly over Multihead ANP, giving sharper images with better global coherence even when the face isn't axis-aligned. Different heads play various roles in predicting targets, even when the target is disjoint from the context. All heads become useful for target prediction in such cases. Visualisation in FIG0 shows pixels attended by each head of multihead attention in the NP given a target pixel. In the NP, different heads in the Stacked Multihead ANP play various roles in predicting targets, even when the target is disjoint from the context. All heads are useful for target prediction, as shown in the visualisation in FIG0."
}