{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for image classification, latent features can be divided into \"core\" features that remain consistent across domains and \"style\" features that can vary. Style features include aspects like position, rotation, and more complex attributes like hair color or posture for images of people. The latent features for image classification can be divided into \"core\" features that are consistent across domains and \"style\" features that can vary, such as position, rotation, image quality, and more complex attributes like hair color or posture. To guard against future domain shifts, it is ideal to use \"conditionally invariant\" features for classification, assuming the domain itself is a latent variable. The distributional change of features across different domains cannot be directly observed. In data augmentation, images can be generated from an original image with an identifier variable. This method only requires a small fraction of images to have this identifier. In data augmentation, images can be generated with an identifier variable. A causal framework is provided by adding the ID variable to the model. The domain is treated as a latent variable in settings where it cannot be observed directly. Samples with the same class and identifier are treated as counterfactuals under different style interventions. Regularizing the network using a grouping-by-ID approach improves performance in settings with changing domains. Regularizing deep neural networks by penalizing with an appropriate graph Laplacian can substantially improve performance in settings with changing domains, such as variations in image quality, brightness, and color. This approach also addresses issues of interpretability, fairness, and transfer learning. Domain shifts in test distributions can degrade predictive performance of machine learning systems deployed in production. An example is the \"Russian tank legend\" where a system trained to distinguish Russian and American tanks had high accuracy due to sampling biases in the training data not replicated in the real world. The system learned to discriminate between images of different qualities, but hidden confounding factors like image quality and tank origin can lead to indirect associations. Deep learning requires large sample sizes to average out the effects of these confounding factors. A large sample size is necessary to counteract confounding factors in deep learning, such as image quality and tank origin. It is also essential for achieving invariance to known factors like translation, point of view, and rotation through data augmentation. Adversarial examples, which are intentionally perturbed inputs misclassified by ML models, highlight the differences between human and artificial cognition. Humans do not get fooled by adversarial examples and can achieve invariance to rotations with just one rotated example of the same object. Invariance to rotations can be achieved with just one rotated example of the same object, a capability humans possess but artificial models struggle with. The goal is to mimic human learning abilities and align DNN features with human cognition, while also addressing biases in training datasets to ensure fair decision-making. Existing biases in training datasets used for machine learning algorithms can lead to replicated biases in the resulting models. For example, Google's photo app once misidentified non-white individuals as \"gorillas\" due to a lack of diverse training data. To address this issue, counterfactual regularization (CORE) is proposed to control the latent features extracted from input data. CORE proposes counterfactual regularization to control latent features extracted by machine learning algorithms, categorizing them into 'conditionally invariant' (core) and 'orthogonal' (style) features. The goal is for classifiers to use only core features related to the target of interest, making the estimator robust against adversarial domain shifts. CORE is an estimator that is robust to adversarial domain shifts by focusing on 'counterfactuals' and exploiting knowledge about grouping instances. It reduces the need for data augmentation and improves predictive performance in small sample sizes. In \u00a72, CORE is shown to reduce the need for data augmentation and enhance predictive performance in small sample size scenarios. \u00a73 reviews related work, while \u00a74 introduces counterfactual regularization and the CORE estimator for logistic regression. \u00a75 evaluates CORE's performance in various experiments, including classifying whether a person wears glasses using the CelebA dataset. The text discusses classifying whether a person wears glasses using counterfactual observations and grouping information. It introduces the concept of counterfactual regularization and the CORE estimator for logistic regression. The training set includes 10 identities with approximately 30 images each. In the training set, there are 10 identities with around 30 images each, resulting in a total sample size of 321. Grouping-by-ID is used with identity and original image as IDs. Exploiting grouping information reduces test error by 32% compared to pooling without using grouping information. Exploiting the group structure reduces the average test error from 24.76% to 16.89%, i.e. by approx. 32%, compared to pooling all images without using grouping information.CORE can also make data augmentation more efficient by creating additional samples through modifications like rotating, translating, or flipping images. Data augmentation involves creating additional samples by modifying original inputs, such as rotating, translating, or flipping images. This process results in invariance of the estimator with respect to style features. Using grouping information for CORE enforces invariance more strongly compared to normal data augmentation. Using CORE for data augmentation enforces invariance with respect to style features more strongly compared to normal data augmentation. The rotation style feature on MNIST is assessed with a total sample size of 10100, including 100 augmented training examples and 10000 original samples. The rotations are randomly sampled from [35, 70]. The average test error on rotated examples is reduced from 32.86% to 16.33% with CORE. This work is similar to the goals of BID14 and Domain-Adversarial Neural Networks (DANN) proposed in BID13. The average test error on rotated examples is reduced from 32.86% to 16.33% with CORE, which enforces invariance with respect to style features. This approach is similar to the goals of BID14 and Domain-Adversarial Neural Networks (DANN) proposed in BID13. The adversarial training procedure maximizes domain classification loss while minimizing target prediction task loss. Unlike BID14, which assumes data from different domains, our model considers different realizations of the same object under different interventions. BID14 identifies conditionally independent features by adjusting variables to minimize MMD distance between domain distributions, while our approach uses a different data basis. Our approach differs from BID14 by using a different data basis and penalizing the classifier using an identifier variable ID. Causal modeling aims to guard against adversarial domain shifts and transfer learning, ensuring valid predictions even under interventions on predictor variables. Causal models aim to protect against domain shifts and ensure valid predictions under interventions on predictor variables, particularly in the context of adversarial domain changes in image classification. The classification task is anti-causal, posing challenges in transferring results to this setting. The challenge in using causal inference for guarding against domain shifts lies in focusing on the true class of the object and style features, rather than arbitrary interventions on variables. Various approaches leveraging causal motivations in deep learning have been proposed, but they differ from our goal of anti-causal prediction and non-ancestral interventions on style variables. The Neural Causation Coefficient (NCC) is proposed for cause-effect inference between random variables X and Y, distinguishing object features from context features. Structural equation modeling and CGANs are compared, with one CGAN fitted for X \u2192 Y and another for Y \u2192 X. Bahadori et al. (2017) use generative neural networks and a regularizer with penalty weights to estimate causal features for cause-effect inference. Besserve et al. (2017) establish connections between causality detection networks and estimated probabilities for causal features. The use of causal generative models, such as GANs and causal implicit generative models, is proposed to estimate individual treatment effects and sample from conditional and interventional distributions. The generator structure must align with the causal graph, and deep latent variable models are utilized along with proxy variables for estimation. BID29 propose the use of deep latent variable models and proxy variables to estimate individual treatment effects, while BID21 exploit causal reasoning to characterize fairness considerations in machine learning by deriving causal nondiscrimination criteria. The resulting algorithms require classifiers to be constant as a function of the proxy variables in the causal graph, showing structural similarity to style features. This distinction between core and style features can be viewed as a form of disentangling factors of variation. Estimating disentangled factors of variation in generative modeling has gained interest. Matsuo et al. (2017) introduced a \"Transform Invariant Autoencoder\" to reduce dependence on specified object transforms in images. They define location as an orthogonal style feature and aim to learn a latent representation without it. This approach differs from predefining style features. The goal is to learn a latent representation that excludes orthogonal style feature X \u22a5, which could include location, image quality, posture, brightness, background, and contextual information. The approach involves a confounding situation where the distribution of style features differs based on the class. In a variational autoencoder framework, Bouchacourt et al. (2017) aim to separate style and content by assuming grouped observations share a common but unknown value for one factor of variation. They focus on a classification task without explicitly estimating latent factors as in a generative framework. In a classification task, the style can vary. We aim to solve it directly without estimating latent factors explicitly. Standard notation for classification is described, followed by a causal graph comparing adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. The target of interest is denoted as Y, typically R for regression or {1, . . . , K} for classification with K classes. X represents the predictor, such as the pixels of an image. The prediction y for y, given X = x, is in the form f \u03b8 (x) with parameters \u03b8 \u2208 R d corresponding to the weights in a DNN. The prediction y for y, given X = x, is of the form f \u03b8 (x) with parameters \u03b8 \u2208 R d corresponding to the weights in a DNN. The goal is to minimize the expected loss by penalized empirical risk minimization using training data samples (x i , y i ) and predictions \u0177 i = f \u03b8 (x i ). The prediction y for y, given X = x, is f \u03b8 (x) with parameters \u03b8 \u2208 R d. Parameter estimation involves penalized empirical risk minimization to choose weights or parameters as \u03b8 = argmin \u03b8 L n (\u03b8). The penalty pen(\u03b8) can be a ridge penalty or exploit geometries like Laplacian regularized least squares. The full structural model for all variables is shown in FIG0. The domain variable D is latent, while the ID variable is used to group observations and can change conditional on class Y. The ID variable is used to group observations and can change conditional on class Y. The prediction is anti-causal, with predictors X non-ancestral to Y. Causal effect from Y on X is mediated via core features X ci and orthogonal style features X \u22a5. External interventions \u2206 can distinguish between the two types of latent variables. The core features X ci and style features X \u22a5 have different properties regarding external interventions \u2206. The distribution of P (X ci |Y ) remains constant across domains, while P (X \u22a5 |Y ) can vary. The style features X \u22a5 and Y are confounded by the latent domain D, whereas the core features X ci are conditionally independent of D given Y. The dimension of X ci is chosen to maintain this conditional independence. The style variable includes various features such as point of view, image quality, resolution, rotations, color changes, body posture, and movement. The style intervention variable influences both the latent style and the image. In potential outcome notation, the prediction under the style intervention is denoted as f \u03b8 (X(\u2206 = \u03b4)). The text discusses using a causal graph to explain domain adaptation, transfer learning, and guarding against adversarial examples. It also mentions using potential outcome notation to predict outcomes under style interventions. The intervention magnitude is assumed to be within a certain range. The causal graph explains domain adaptation, transfer learning, and guarding against adversarial examples. The intervention magnitude is assumed to be within a certain range to minimize adversarial loss. The text discusses interventions on style features X \u22a5 to estimate the conditional distribution of Y. It explores adversarial domain shifts with strong interventions on style features, leading to an adversarial loss. The interventions can be powerful, but certain aspects of the image (mediated by core features) remain unchanged. The text discusses adversarial interventions on style features to estimate the conditional distribution of Y, with a focus on protecting against shifts in test data distribution by distinguishing between core and style features. The classical problem of causal inference is highlighted, where observing counterfactuals is impossible. The classical problem of causal inference is the inability to observe counterfactuals, where we can only see the outcome under treatment or no treatment but not both simultaneously. This makes it challenging to determine the treatment effect on the health outcome of interest. In causal inference, observing counterfactuals is generally impossible as we can only see the outcome under treatment or no treatment, but not both simultaneously. Counterfactuals involve keeping class label Y and ID constant while allowing the style intervention \u2206 to change, similar to a treatment in a medical example. In image analysis, counterfactuals are conceivable as we can observe the same object under different conditions. In image analysis, counterfactuals involve observing the same object under different conditions, allowing for changes in variables like background, posture, and image quality. Unlike in the medical setting, the focus is not on the treatment effect but on ruling out parts of the feature space. In image analysis, counterfactuals involve observing the same object under different conditions to rule out parts of the feature space for classification. The focus is not on the treatment effect but on penalizing any change in classification under different style interventions while keeping class and identity constant. The pooled estimator treats all examples identically by summing over the loss with a ridge penalty. The adversarial loss of the pooled estimator will be infinite in general. The pooled estimator uses a ridge estimator with a cross-validated penalty parameter. The adversarial loss can be infinite. Conditions (i) and (ii) ensure the estimator works well in terms of adversarial loss. The pooled estimator with a ridge estimator and cross-validated penalty parameter works well in terms of adversarial loss if certain conditions are met. The adversarial loss can be minimized by ensuring the core features are constant across all parameters. The invariant parameter space I is defined by core features x ci \u2208 R p. The adversarial loss under interventions is the same as without interventions. The optimal predictor in space I is based on core features X ci. Inference of I from data is done using empirical risk minimization. To approximate the optimal invariant parameter vector, empirical risk minimization is used to infer the invariant space I from data. The unknown invariant parameters space I is approximated by an empirically invariant space In, with a regularization constant \u03c4 \u2265 0 allowing for variations in predictions for class labels. The estimated predictions for class labels are consistent across all counterfactuals of image i, with \u03c4 values allowing for slight variations. The true invariant space I is a subset of the empirically invariant subspace In. The Lagrangian form of constrained optimization can be used with a penalty parameter \u03bb instead of \u03c4. The matrix LID is a graph Laplacian BID4 with n connectivity components. The matrix LID is a graph Laplacian BID4 with n connectivity components, formed in the sample space by the identifier variable ID. The graph Laplacian regularization penalizes the sum of variances \u03c32i(\u03b8), not strongly dependent on the penalty \u03bb chosen. The graph is formed by connecting similar features, with the penalty \u03bb not strongly affecting the outcome. It is crucial to define the graph based on the identifier variable ID. Other regularizations do not perform well against domain shifts. Adversarial loss is analyzed for binary classification in a one-layer network. The structural equation for the image X is linear in style features X \u22a5. In the context of analyzing adversarial loss for binary classification, the structural equation for the image X is assumed to be linear in style features X \u22a5. The interventions are additive, and logistic regression is used to predict class labels. The CORE estimator's adversarial loss converges to the optimal value as sample size increases, while experiments in different scenarios are conducted to assess its performance. In \u00a75.1 and \u00a75.2, CORE is tested with confounded training data and changing style features in test distributions. The level of confounding is controlled in the assessment. \u00a75.3 involves classifying elephants and horses based on color. Additional experiments in \u00a7B include gender and wearing glasses, as well as wearing glasses and brightness. Experimental results for settings introduced in \u00a72 can be found in \u00a7C.2 and \u00a7C.3. TensorFlow BID0 implementation of CORE will be provided along with code for reproducing experiments. Architecture details are also available. In addition to the details provided below, information on the employed architectures can be found in \u00a7C.7. An open question is how to set the value of the tuning parameter \u03c4 or the penalty \u03bb in Lagrangian form. Performance is typically not very sensitive to the choice of \u03bb. Stickmen images are synthetically generated, with the target of interest being Y \u2208 {adult, child} and X ci \u2261 height. Height is a core feature for differentiating between children and adults. Height is a core feature for differentiating between children and adults, and there is a dependence between age and movement in the training dataset. The data generating process is illustrated in FIG0.9, showing examples where large movements are associated with different age groups. The model's prediction of Y based on movement fails when presented with images of dancing adults. Test sets 2 and 3 intervene on X \u22a5, removing the dependence between Y and X \u22a5. Large movements are associated with both children and adults in these test sets, with heavier movements in test set 3. Misclassification rates for CORE and the pooled estimator are shown in FIG0 for c = 50 and m = 20000. The pooled estimator fails in achieving good predictive performance on test sets 2 and 3, while CORE succeeds with as few as 50 counterfactual observations. The results suggest that the pooled estimator uses movement as a predictor for age, unlike CORE which does not due to counterfactual regularization. Including more counterfactual examples would not improve the pooled estimator's performance. Including more counterfactual examples would not improve the performance of the pooled estimator as these would be subject to the same bias. The CelebA dataset is used to classify whether a person in an image is wearing eyeglasses, with image quality being lower when the person wears glasses. This mimics confounding seen in the Russian tank legend. The image quality intervention in the study is based on sampling new image quality as a percentage of the original image's quality from a Gaussian distribution. Counterfactual observations are only available for images of people wearing glasses, with the same image used but with a newly sampled image quality value. This is referred to as \"CF setting 1\". The study involves sampling new image quality values from a Gaussian distribution for counterfactual observations in \"CF setting 1\". Different test sets with varying image quality interventions are analyzed, showing misclassification rates for CORE and the pooled estimator. The study compares the performance of the pooled estimator and CORE on different test sets with varying image quality interventions. The pooled estimator outperforms CORE on test set 1 by utilizing predictive information from image quality, while CORE is restricted from doing so. However, the pooled estimator struggles on test sets 2-4 as it relies too heavily on image quality as a predictor. In contrast, CORE's performance is not significantly affected by changes in image quality distributions. In contrast to the pooled estimator, CORE's predictive performance is not affected by changing image quality distributions. The study aims to assess if CORE can exclude \"color\" from its learned representation by including a few instances of colored elephants. CORE's ability to exclude \"color\" from its learned representation is tested by adding grayscale counterfactual examples of elephants in the Animals with attributes 2 dataset. The study includes images of horses and elephants, with a total sample size of 1850. Misclassification rates for CORE are compared with the use of counterfactuals. The study tested CORE's ability to exclude color by adding grayscale counterfactual examples of elephants in the dataset. Test sets with different color modifications were used, showing that the pooled estimator did not perform well on certain sets. The study tested CORE's ability to exclude color by adding grayscale counterfactual examples of elephants in the dataset. The colorspace of all images is shifted towards red, affecting the performance of the pooled estimator on test sets 2 and 3. In contrast, the CORE estimator's predictive performance is hardly affected by changing color distributions. Adding grayscale examples can help recognize a colored elephant as an elephant, highlighting the bias in the pooled estimator towards gray elephants. The CORE estimator aims for color invariance by including grayscale images of elephants in the dataset. This approach helps to mitigate bias towards gray elephants in the pooled estimator. If \"color\" is considered a protected attribute, CORE would not include it in its learned representation, promoting fairness. The CORE estimator aims for color invariance by not including \"color\" in its learned representation, promoting fairness. It distinguishes latent features in images into core and style features, using counterfactual regularization to achieve robustness against interventions. The main idea is to demand invariance of the classifier among instances of the same object in the training data. By demanding invariance of the classifier among instances of the same object in the training data, we can achieve invariance of classification performance with respect to adversarial interventions on style features. This approach works despite sampling biases in the data and can achieve the same classification performance as standard data augmentation approaches with fewer instances. If style features are unknown, regularization can still be applied. Regularization of CORE can achieve invariance of classification performance with respect to style features, even when they are unknown. Larger models like Inception or ResNet architectures may not guard against interventions on implicit style features. Further assessment of the benefits is needed. CORE regularization can achieve classification performance invariance with respect to style features, even when they are unknown. Using Inception V3 features may not protect against interventions on implicit style features. Future directions could involve utilizing video data for grouping and counterfactual regularization, potentially aiding in debiasing word embeddings. The structural equation for image X is linear in style features X \u22a5, with logistic regression predicting class label Y. Interventions \u2206 act additively on style features X \u22a5, which linearly affect image X via matrix W. Core features X ci are conditionally invariant. The core features X ci are conditionally invariant in logistic regression for predicting Y from image data X, with a logistic loss function used for training and testing. The logistic loss function is used for training and testing with m samples to estimate \u03b8. The expected losses on test data include standard logistic loss and loss under adversarial interventions. The benchmarks are also provided. The formulation of Theorem 1 relies on specific assumptions. The formulation of Theorem 1 relies on specific assumptions including sampling conditions for training data and the requirement of a matrix W with full rank. The sampling process involves collecting independent samples that satisfy certain constraints. The sampling process involves collecting n independent samples from a distribution that satisfies certain constraints. For c = m - n samples, a new value of \u2206 is redrawn, resulting in m samples with n distinct values of (y i , id i ) and m i counterfactuals at each sample. The pooled estimator is guaranteed under Assumption 1 with probability 1 with respect to the training data. The pooled estimator, under Assumption 1, has infinite adversarial loss with probability 1. For the CORE estimator, an equivalent result can be derived for misclassification loss. To prove this, it is necessary to show that W t\u03b8pool = 0 with probability 1, where \u03b8 * is the oracle estimator orthogonal to the column space of W. The pooled estimator has infinite adversarial loss with probability 1. To show this, it is necessary to demonstrate that W t\u03b8pool = 0 with probability 1, where \u03b8 * is the oracle estimator orthogonal to the column space of W. The derivative of the loss function in the direction of \u03b4 is proportional to the counterfactual data x i,j. The oracle estimator \u03b8 * remains the same under true training data and counterfactual training data. The derivative g(\u03b4) can be expressed as a difference between two formulas. The derivative g(\u03b4) in FORMULA24 can be expressed as a difference between FORMULA24 and FORMULA25. By model assumptions, x i,j \u2212 x i,j (0) = W \u2206 i,j, where \u03b4 = W u. The eigenvalues of W t W are positive. The estimator \u03b8 * is not affected by interventions \u2206 i,j, whether trained on original or counterfactual data. Conditioning on (x i (0), y i ) for i = 1, . . . , n, the estimator remains the same. The interventions \u2206 i,j are drawn from a continuous distribution, leading to the left hand side of the equation having a continuous distribution. The probability of the left hand side not being 0 is 1, completing the proof by contradiction. With probability 1, \u03b8 core = \u03b8 * as defined in the model. The invariant space is the linear subspace where W t \u03b8 = 0. With probability 1, the estimator remains unchanged if the data without interventions is used for training, as \u03b8 core = \u03b8 * in the model where W t \u03b8 = 0 in the linear subspace. The estimator remains unchanged when using data without interventions for training, as \u03b8 core = \u03b8 * in the model where W t \u03b8 = 0 in the linear subspace. Comparing different formulas, we can see that by uniform convergence of Ln to the population loss L (0), we have c = m - n samples redrawn randomly from the empirical sample. When c = m - n samples are redrawn from the empirical sample at random, we have \u03b8 core = \u03b8 * with probability 1 under (A3). Using the CelebA dataset, we create a confounding by including mostly images of men wearing glasses and women without glasses. Counterfactuals are used by showing an image of the same person without glasses for males and with glasses for females, known as \"CF setting 2\". In \"CF setting 2\", counterfactuals are used by showing the same person without glasses for males and with glasses for females. Test set 1 follows the same distribution as the training set, while test set 2 flips the association between gender and glasses. The study compares training a four-layer CNN end-to-end versus using Inception V3 features and retraining the softmax layer. The study compares training a four-layer CNN end-to-end versus using Inception V3 features. Results show that as the number of counterfactual examples increases, the performance difference between CORE and the pooled estimator decreases. The pooled estimator performs worse on test set 2 as the number of counterfactual examples grows. The study analyzes a confounded setting where X \u22a5 indicates brightness in the CelebA dataset for classifying eyeglasses. The pooled estimator performs worse on test set 2 as m increases, exploiting X \u22a5 more as m grows. The study examines the relationship between brightness and wearing glasses in images. Test sets 1-4 show different interventions on image brightness. The pooled estimator outperforms CORE on test set 1 due to its ability to exploit predictive factors. The pooled estimator performs better than CORE on test set 1 by utilizing the brightness of images for prediction. However, it struggles on test sets 2 and 4 when brightness distributions differ from the training set. In contrast, CORE's performance is not significantly affected by changing brightness distributions. Results for different counterfactual settings, including using images of the same person or a different person, can be found in FIG0 .4 and FIG0 .5. In counterfactual settings 2 and 3, varying factors make isolating brightness challenging. Grouping images of different persons can still improve predictive performance. The value of tuning parameter \u03c4 in Eq. remains an open question. In counterfactual settings, grouping images of different persons can enhance predictive performance. The value of the tuning parameter \u03c4 or penalty \u03bb remains an open question. Performance is not highly sensitive to the choice of \u03bb, as shown in the experiment with varying numbers of identities in the training dataset. The training dataset includes sample sizes ranging from 321 to 4386, with an average of 27 to 32 counterfactual observations per person. CORE improves predictive performance compared to pooling all images, especially with small sample sizes. As sample sizes increase, the performance of CORE and the pooled estimator becomes comparable. In the experiment, varying the number of augmented training examples showed that CORE consistently had lower misclassification rates on test set 1 compared to the usual MNIST test set. The experiment showed that CORE had lower misclassification rates on test set 1 compared to the usual MNIST test set, indicating more efficient data augmentation. The performance of CORE was not sensitive to the number of counterfactual examples once a sufficient amount was present in the training set. The experiment showed that the pooled estimator failed to achieve good predictive performance on test sets 2 and 3. Counterfactual settings 1-3 were tested with c = 5000, showing that setting 1 worked best while settings 2 and 3 had similar predictive performance. Counterfactual setting 1 performs best, with small differences in predictive performance between settings 2 and 3. There is a notable performance gap between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator, suggesting image quality may not be predictive enough at \u00b5 = 50. The latter command rotates image colors cyclically, and all images in test set 3 are converted to grayscale. The models were implemented in TensorFlow, with detailed architectures in TAB1.1. CORE and the pooled estimator share the same network architecture and training. The models were implemented in TensorFlow with detailed architectures in TAB1.1. CORE and the pooled estimator share the same network architecture and training procedure, differing only in the loss function. Experimental results are based on training each model five times to assess variance, with training data shuffled in each epoch to ensure mini batches contain counterfactual observations. In experiments, data is shuffled to ensure mini batches contain counterfactual observations, with a batch size of 120. This makes optimization more challenging for small c values."
}