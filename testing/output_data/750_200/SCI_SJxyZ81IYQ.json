{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively. This approach preserves semantic content better through explicit factorization of semantics and syntax. The new image captioning paradigm preserves semantic content better through explicit factorization of semantics and syntax, using a compositional generation procedure with a recursive structure that fits human language properties. Image captioning has gained attention recently, with models using an encoder-decoder paradigm to generate captions for images. Despite its effectiveness, the sequential model used has a fundamental problem. Sequential models in image captioning struggle to reflect hierarchical structures in natural languages, leading to drawbacks such as reliance on n-gram statistics and favoring frequent n-grams. The drawbacks of sequential models in image captioning include overreliance on n-gram statistics, favoring frequent n-grams in training, and obscuring the dependency structure between syntax and semantics. A new paradigm is proposed to separate the extraction of semantics and the construction of syntactically correct captions. The new paradigm for image captioning proposes a two-stage process: extracting semantics and constructing syntactically correct captions. It involves deriving a semantic representation of the image using noun-phrases and recursively composing the caption. The composition process involves forming higher-level phrases by connecting selected sub-phrases. It is a parametric modular approach, not a hand-crafted algorithm. The compositional procedure described is a parametric modular approach involving two nets for phrase composition and evaluation. It offers advantages over conventional captioning models by preserving semantic content and capturing hierarchical dependencies in natural language. The proposed paradigm in image captioning captures hierarchical dependencies in natural language, increases caption diversity, and generalizes well to new data with limited training data. The literature in image captioning has evolved from bottom-up detection-based approaches to neural network era methods. In the neural network era, image captioning has shifted to using convolutional neural networks for image representation and recurrent neural networks for caption generation. This approach increases caption diversity and generalizes well to new data with limited training data. In the neural network era, image captioning has evolved to use convolutional neural networks for image representation and recurrent neural networks for caption generation. Vinyals et al proposed a neural image captioner using LSTM to generate words. Xu et al extended this by using an attention mechanism on feature vectors. Lu et al adjusted the attention computation to include generated text, while Anderson et al added an LSTM for better attention control. Dai et al reformulated the approach. In recent approaches to image captioning, researchers have made advancements in attention computation and semantic information extraction. For example, Dai et al reformulated latent states as 2D maps to capture semantic information better. Yao et al predicted frequent training words occurrences to improve LSTM performance. Tan et al treated noun-phrases as hyper-words to enhance decoder output efficiency. In recent advancements in image captioning, researchers have improved decoder efficiency by treating noun-phrases as hyper-words. The proposed paradigm constructs captions in a bottom-up manner, representing the input image with noun-phrases and generating captions recursively. This approach aims to address issues such as incorrect semantic coverage and lack of diversity seen in sequential caption generation models. Our proposed paradigm for image captioning represents input images with noun-phrases and generates captions recursively, aiming to preserve semantics effectively and produce diverse captions. This approach contrasts with the non-recursive composition procedure used in related work by Kuznetsova et al BID23. The proposed compositional paradigm for image captioning generates captions by selecting phrases from noun-phrases extracted from the input image. The composition procedure is not recursive, limiting the versatility of the generated captions to single objects. In a two-stage framework for image captioning, phrases are composed from noun-phrases extracted from the input image. The composition is hierarchical, following the structure of natural language. In a two-stage framework for image captioning, noun-phrases are extracted from the input image to create a hierarchical composition of phrases. The caption is constructed in a bottom-up manner using a recursive procedure called CompCap, which considers nonsequential dependencies among words and phrases. Unlike conventional captioning methods that rely on n-gram statistics, CompCap can capture the semantic representation of the image more effectively. In a two-stage framework for image captioning, noun-phrases are extracted from the input image to represent image semantics explicitly. This approach captures object categories and associated attributes, enhancing visual understanding. In a two-stage framework for image captioning, noun-phrases are extracted from the input image to represent image semantics explicitly. This approach captures object categories and associated attributes, enhancing visual understanding. The number of distinct noun-phrases in a dataset is significantly smaller than the number of images, leading to the formalization of noun-phrase extraction as a multi-label classification problem. In a two-stage framework for image captioning, noun-phrases are extracted from the input image to represent image semantics explicitly. This approach captures object categories and associated attributes, enhancing visual understanding. The task of noun-phrase extraction is formalized as a multi-label classification problem. Noun-phrases are derived from training captions and treated as classes for binary classification based on visual features extracted from images. The process involves binary classification of noun-phrases in image captioning using weight vectors and a sigmoid function. The input image is represented by selecting top-scoring noun-phrases and pruning semantically similar ones through Semantic Non-Maximum Suppression. A recursive compositional procedure called CompCap is used to construct the caption. CompCap is a recursive compositional procedure used to construct captions by connecting pairs of noun-phrases with a Connecting Module (C-Module) to generate longer phrases. The C-Module also computes a score for the generated phrase. The C-Module concatenates sequences to form phrases and computes a score for each. A parametric module can determine the new phrase P new. The Evaluation Module assesses if P new is a complete caption. If not, the pool is updated and the process repeats until a complete caption is obtained. The Connecting Module (C-Module) selects a connecting phrase P (m) based on left and right phrases P (l) and P (r), and evaluates the connecting score S(P (m) | P (l), P (r), I). LSTM decoding for filling in caption blanks is found to be ineffective, possibly due to inputs in BID29. In the task of filling in caption blanks in BID29, using LSTM to decode intermediate words was ineffective. The C-Module focuses on incomplete captions, which have a larger space compared to BID29. An alternative strategy is adopted to treat connecting phrase generation as a classification problem, as the number of distinct connecting phrases is limited in the proposed paradigm. This approach is motivated by the observation that semantic words like nouns and adjectives are not part of connecting phrases. The proposed paradigm limits connecting phrases to only distinct sequences mined from training captions, treated as different classes. A classifier is defined in the connecting module to score left and right phrases, using a two-level approach. The connecting module is defined as a classifier that scores left and right phrases using a two-level LSTM model. The model encodes phrases P (l) and P (r) with word embeddings and image features, controlling attention and evolution of the encoded state. The encoders for P (l) and P (r) share the same structure but have different parameters. The high-level LSTM drives the evolution of encoded state for phrases P (l) and P (r). Encodings z (l) and z (r) go through fully-connected layers and a softmax layer to determine connecting scores for phrases. A virtual neg is added to handle unconnectable pairs. The Evaluation Module (E-Module) is used to determine whether a phrase is a complete caption. It encodes the input phrase into a vector using a two-level LSTM model. The E-Module encodes input phrases into vectors using a two-level LSTM model to determine if they are complete captions. It can also check other properties like caption quality using a caption evaluator. Extensions include generating diverse captions using beam search or probabilistic sampling. The framework can be extended for generating diverse captions by using beam search or probabilistic sampling. This allows for multiple ordered pairs and connecting sequences to be retained at each step, forming multiple beams for beam search to avoid local minima. Additionally, diverse captions can be generated by sampling a part of the ordered pairs or connecting sequences based on normalized scores. The framework can also incorporate user preferences or other conditions. The framework can be extended to incorporate user preferences or other conditions, allowing for control over the resultant captions by filtering noun phrases or modulating scores. Experiments were conducted on MS-COCO BID4 and Flickr30k BID5 datasets, each containing a large number of images with ground-truth captions. In experiments on MS-COCO BID4 and Flickr30k BID5 datasets, there are 123,287 and 31,783 images respectively, each with 5 ground-truth captions. The vocabulary is cleaned and truncated for training data collection. The training captions are truncated to 18 words max. Ground-truth captions are parsed into trees using NLP toolkit BID31 for training data. C-Module and E-Module are separately trained for classification tasks. The compositional procedure is modularized for better generalization. Testing involves two forward passes for each module. CompCap is compared with other methods. Several methods are compared with CompCap for image captioning, including Neural Image Captioner (NIC) BID2, AdapAtt, TopDown BID1, and LSTM-A5 BID19. These methods encode images as feature vectors or predict semantical concepts as additional visual features. Predictions from noun-phrase classifiers are used in CompCap. The study compares various methods for image captioning, including NIC, AdapAtt, TopDown, and LSTM-A5. The methods use semantical concepts as additional visual features and predictions from noun-phrase classifiers. ResNet-152 is used to extract image features, with fixed parameters and a learning rate of 0.0001 for all methods during training. During training, ResNet-152 is fixed without finetuning, and the learning rate is set to 0.0001 for all methods. Parameters that yield the best performance on the validation set are selected for generating captions during testing. CompCap selects 7 noun-phrases with top scores to represent the input image, balancing semantics and syntax. Beam-search of size 3 is used for baselines and pair selection, while connecting phrase selection does not use beam-search. The quality of generated captions is compared on MS-COCO and Flickr30k test sets. CompCap with predicted noun-phrases outperforms other methods in terms of SPICE metric, but lags behind in CIDEr, BLEU-4, ROUGE, and METEOR. These results highlight the differences in methods that generate captions sequentially and compositionally, with SPICE focusing on semantical analysis. The results show that methods generating captions sequentially favor certain n-grams, while compositional methods preserve semantic content better. An ablation study on the proposed compositional paradigm is conducted, with the input image represented by groundtruth noun-phrases. The study conducted on the proposed compositional paradigm showed that representing the input image with groundtruth noun-phrases led to a significant improvement in all metrics. CompCap effectively preserves semantic content, generating better captions with a better semantic understanding of the input image. Additionally, integrating noun-phrases from a ground-truth caption further boosted metrics, except for SPICE. The proposed compositional paradigm integrates noun-phrases into captions, boosting metrics except for SPICE. CompCap excels at composing semantics into syntactically correct captions, handling out-of-domain content effectively and requiring less data to learn. Two studies were conducted to verify this hypothesis. CompCap is effective at handling out-of-domain semantic content and requires less data to learn. Two studies were conducted to verify this. The first experiment controlled the data ratio for training CompCap, resulting in curves for SPICE and CIDEr. In the second study, baselines and CompCap were trained on MS-COCO/Flickr30k and tested on Flickr30k/MS-COCO, showing significant drops for baselines but competitive results for CompCap. CompCap shows competitive results when trained with in-domain and out-of-domain data, indicating the benefit of disentangling semantics and syntax. The ability to generate diverse captions is a key property of CompCap, achieved by varying noun-phrases or composing order. The diversity of captions in CompCap is analyzed using five metrics, including the ratio of novel and unique captions, vocabulary usage, and pair-wise editing distances. These metrics evaluate the degree of diversity from various aspects. The diversity of captions in CompCap is analyzed using metrics such as vocabulary usage and pair-wise editing distances to quantify the diversity of captions at both dataset and image levels. CompCap obtained the best results in all metrics, indicating diverse and novel captions. Qualitative samples in FIG2 show captions generated with different composing orders or noun-phrases. Error Analysis in FIG4 highlights failure cases with similar errors. Error Analysis in FIG4 reveals failure cases with similar errors to those in Figure 1, but with fundamentally different causes. Errors in CompCap captions stem from misunderstanding visual content, while sequential models tend to favor frequent n-grams. Proposed solution involves applying more sophisticated techniques in noun-phrase extraction. The proposed method for image captioning involves a compositional approach, where the captioning procedure is divided into two stages: extracting explicit noun-phrases from the input image in the first stage, and assembling them into a caption using a recursive compositional procedure in the second stage. This method aims to address errors in caption generation caused by misunderstanding visual content and over-reliance on frequent n-grams. The proposed method for image captioning involves a two-stage process: extracting noun-phrases from the input image and assembling them into a caption using a recursive compositional procedure. This approach aims to improve caption generation by preserving semantics effectively, requiring less training data, generalizing better across datasets, and producing diverse captions. The key for suppression is to identify semantically similar noun-phrases based on comparisons of central nouns. The method involves comparing central nouns in noun-phrases to determine semantic similarity. Encoders in the C-Module are used to get encodings for each noun-phrase, with small normalized euclidean distance indicating semantic similarity. The C-Module uses encoders to compute normalized euclidean distances for noun-phrases, with a small sum indicating semantic similarity. A threshold of 0.002 is used to determine similarity. The C-Module uses encoders with independent parameters for P (l) and P (r) to improve performance. Additional hyperparameters can be tuned for CompCap, such as beam search size for pair selection. The C-Module with independent encoder parameters improves performance. Additional hyperparameters for CompCap include beam search size for pair and connecting phrase selection. Adjusting these hyperparameters shows minor influence on CompCap performance."
}