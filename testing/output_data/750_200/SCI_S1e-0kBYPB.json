{
    "title": "S1e-0kBYPB",
    "content": "In this work, the need for explanations of black-box AI models like neural networks is highlighted. Two issues with current explanatory methods are identified: differing perspectives lead to different explanations, and explainers have only been validated on simple models, not real-world neural networks. The current post-hoc explainers have only been validated on simple models like linear regression, not real-world neural networks. A verification framework for explanatory methods is introduced under the feature-selection perspective, based on a non-trivial neural network architecture. The efficacy of the evaluation is validated by demonstrating the failure modes of current explainers. The efficacy of post-hoc explanatory methods is validated on real-world neural networks, showcasing failure modes of current explainers. The framework aims to offer a readily available evaluation for feature-selection perspective on explanations. The text discusses two widely used perspectives on explanations in neural networks: feature-additivity and feature-selection. These perspectives provide different explanations for predicting a single input in isolation. In the context of explaining predictions in neural networks, different perspectives (feature-additivity and feature-selection) offer fundamentally different explanations. Current methods compare these perspectives, highlighting strengths and limitations in pointing out biases. Current explanatory methods are successful in pointing out biases in neural networks, but their reliability is questioned when the model being explained has a less dramatic bias. The ground-truth decision-making process of neural networks is unknown, making it difficult to evaluate explainers applied to complex models trained on real-world datasets. When evaluating explainers for neural networks trained on real-world datasets, assumptions about the model's behavior are often made, such as focusing on relevant correlations. However, recent studies have shown unexpected spurious correlations in human-annotated datasets that neural networks rely on heavily. Recent works have revealed spurious correlations in human-annotated datasets that neural networks heavily rely on. To address this issue, a framework is proposed to generate evaluation tests for explanatory methods from a feature-selection perspective. This framework identifies tokens with zero contribution to the model's predictions. The framework identifies tokens with zero contribution to the model's predictions in a pair of (target model, dataset) for multi-aspect sentiment analysis. The architecture allows for the identification of irrelevant and relevant tokens, testing if explainers rank zero-contribution tokens higher than relevant ones. The test is conducted on three pairs of models trained independently for different aspects. The framework tests explainers for critical failures by generating evaluation tests with guarantees on the target model's behavior. It does not introduce an explanation generation framework but focuses on evaluating explainers for errors. This approach penalizes explainers only when errors are guaranteed, making it a unique contribution in the field. The study introduces an automatic evaluation test for explainers that penalizes errors with guarantees on the target model's behavior. It evaluates the L2X feature-selection explainer and compares it with popular explainers like LIME and SHAP. The study compares LIME and SHAP explainers with L2X, finding that the former perform better most of the time. Error rates of these methods are provided to highlight possible failures in explanations. In some cases, the explainers predict the most relevant token incorrectly. The study compares LIME and SHAP explainers with L2X, finding that the former perform better most of the time. Error rates of these methods are provided to highlight possible failures in explanations. In certain cases, the explainers predict the most relevant token to be among the tokens with zero contribution. The evaluation test will be released for future work on explanatory methods, which are mainly feature-based explainers. Feature-based explainers provide explanations by assigning weights to input features, either additively or selectively. Additive explainers give weights to each feature, while selective explainers identify a subset of features responsible for the prediction. Other types of explanations include example-based methods. In this work, the focus is on verifying feature-based explainers, which assign weights to input features for explanations. Other types of explanations include example-based methods and human-level explanations that mimic real-world human reasoning. In this work, the focus is on verifying feature-based explainers, which assign weights to input features for explanations. Evaluations commonly performed include testing explainers on interpretable target models like linear regression and decision trees. However, these simple models may not be representative of the complex neural networks used in practice. The evaluation of explainers focuses on assessing their faithfulness to target models. Synthetic setups are used for evaluation, where important features are controlled. While these setups allow for complex target models, they may still prompt the models to learn simpler functions than needed for real-world applications. Despite being trained on synthetic setups, target models may still learn simpler functions than required for real-world applications, making the job easier for explainers. In a setup assuming reasonable behavior, intuitive heuristics are identified for high-performing models, such as relying on specific features like adjectives and adverbs in sentiment analysis. Evaluation through crowd-sourcing confirms if the explainer's features align with the model's predictions, although neural networks may uncover unexpected artifacts. The evaluation of explainers' faithfulness to the model's predictions may not be reliable due to unexpected artifacts discovered by neural networks. Another evaluation involves humans predicting the model's behavior based on explanations from different explainers. The evaluation of explainers' faithfulness to the model's predictions may involve humans predicting the model's behavior based on explanations from different explainers. This method is costly and labor-intensive. In contrast, our evaluation is fully automatic, using a non-trivial neural network trained on a real-world task. Our evaluation is fully automatic, using a non-trivial neural network trained on a real-world task. It is more challenging than previous methods and requires a stronger fidelity of the explainer to the target model. The explanation of the prediction in current explanatory methods adheres to the Feature-additivity perspective, where contributions from each feature approximate the model's prediction. Various methods follow this approach, such as LIME and Lundberg & Lee's unified method. The Shapley values from game theory provide feature contributions that satisfy desired constraints, as shown by Lundberg & Lee (2017). These values are calculated by averaging contributions over a feature's neighborhood. The contribution of each feature in an instance is an average over a neighborhood of the instance, with the choice of neighborhood being critical. Feature-selection for model explanation involves finding sufficient explanations for model predictions. Perspective 2 in model explanation involves identifying a small subset of features that lead to the same prediction as the original model. Researchers like Chen et al. (2018) and Ribeiro et al. (2018) follow this approach, with methods like L2X maximizing mutual information between the subset and the prediction. However, a drawback is the assumption of knowing the number of important features per instance, which is often not the case in practice. The perspective of model explanation involves identifying a small subset of features that lead to the same prediction as the original model. However, it assumes knowing the number of important features per instance, which is often not the case in practice. In the context of model explanation, a hypothetical sentiment analysis regression model is discussed, highlighting the potential biases in real-world neural networks. The model's behavior is compared to neural networks trained on specific tokens, showing differences in predicting target classes. The feature-additive perspective in model explanation aims to provide an average explanation of the model on a neighborhood of the instance. It highlights the relevance of certain features and their contributions to the model's predictions. The feature-additive perspective aims to explain the model's average behavior on a neighborhood of the instance, while the feature-selective perspective focuses on the specific features used by the model on the instance in isolation. The difference between the two perspectives is evident in the ranking of features on different instances. The proposed verification framework leverages the RCNN model architecture introduced by Lei et al. (2016) and prunes the original dataset to ensure accurate instance-wise explanations. The framework utilizes the RCNN model architecture by Lei et al. (2016) and prunes the dataset to ensure accurate explanations for each datapoint. The RCNN consists of a generator and an encoder, both implemented with recurrent convolutional neural networks. The RCNN model (Lei et al., 2016) includes a generator and an encoder, both using recurrent convolutional neural networks. The generator selects a subset of tokens from input text x and passes it to the encoder for making predictions without direct supervision on subset selection. Training is done jointly on the generator and encoder with supervision only on the final prediction. The RCNN model includes a generator and an encoder trained jointly with supervision only on the final prediction. Two regularizers are used to encourage the generator to select a short sub-phrase and fewer tokens. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability during training. The RCNN model uses a generator and encoder trained jointly with supervision only on the final prediction. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability during training. An intermediate hard selection in the model allows for tokens that do not contribute to the final prediction, leading to the emergence of a communication protocol called a handshake. The goal is to eliminate handshakes by gathering a dataset where the set of non-selected tokens for each prediction has zero contribution. The goal is to eliminate handshakes in the RCNN model by gathering a dataset where non-selected tokens have zero contribution to the prediction. The model's proof is in Appendix B, showing that irrelevant tokens can be identified. For example, in Figure 2, the model selects \"very\" with a score of 1, but selecting only \"very\" returns a score of 0.5, capturing the handshake. Equation 7 captures the handshake in the RCNN model by identifying non-selected tokens with zero contribution. The presence of irrelevant tokens does not necessarily imply a handshake, as some tokens may be selected in the original instance but not in the pruned dataset. After pruning the dataset to retain instances where S Sx = S x, it is uncertain if all non-selected tokens are relevant to the prediction. Some tokens may be noise selected to ensure a contiguous sequence in the RCNN model. This pruning ensures that non-selected tokens have no contribution to the prediction. After pruning the dataset to retain instances where S Sx = S x, the selection is ensured to be a contiguous sequence. To avoid penalizing explainers for not differentiating between noise and relevant features, a significant threshold \u03c4 is used to determine the relevance of selected tokens. If the absolute change in prediction after removing a token is greater than \u03c4, it is considered clearly relevant. The text discusses how a significant threshold \u03c4 is used to determine the relevance of selected tokens in a prediction. If the absolute change in prediction after removing a token is higher than \u03c4, the token is considered clearly relevant. The selected tokens are further divided into clearly relevant tokens and tokens for which relevance is unknown. Simply because a token alone did not impact the prediction significantly does not mean it is irrelevant. The procedure ensures that tokens changing the prediction by a high threshold are important and should be ranked higher. The dataset is pruned to keep datapoints with at least one clearly relevant token per instance. The procedure does not provide an explainer or ranking. The procedure prunes the dataset to include instances with at least one relevant token. Evaluation metrics are used to rank features, ensuring important tokens are ranked higher. The procedure does not provide an explainer or actual ranking. The error metrics defined include the percentage of instances where important tokens are not selected and the average number of non-selected tokens ranked higher than relevant ones. In this work, the error metrics include the percentage of times the explainer incorrectly identifies the most relevant token and the number of zero-contribution features ranked higher than relevant ones. The study focuses on the RCNN model trained on the BeerAdvocate corpus. The framework is instantiated on the RCNN model trained on the BeerAdvocate corpus, consisting of 100K human-generated beer reviews with three aspects: appearance, aroma, and palate. The RCNN predicts ratings rescaled between 0 and 1 for each aspect independently. Three separate RCNNs are trained for each aspect, resulting in three datasets. The RCNN model is trained on three aspects: appearance, aroma, and palate, with separate datasets for each aspect. Non-selected tokens have zero contribution to the model's prediction. A threshold of \u03c4 = 0.1 is chosen for significant prediction changes. Statistics of the datasets are provided in Appendix A, including average review lengths and selected tokens per review. The study provides statistics on dataset characteristics, such as average review lengths and selected tokens per review. The threshold for significant prediction changes is set at 0.1, with a focus on ensuring high guarantees on evaluation tests. Three popular explainers, LIME and SHAP, are evaluated in the study. The study evaluates three popular explainers: LIME, SHAP, and L2X. The explainers were tested with default settings for text explanations, with some modifications for L2X. The evaluation focuses on ensuring high guarantees on evaluation tests. In Section 3, LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature selection explainer. A major limitation of L2X is the need to know the number of important features per instance, which is often unknown in practice. In testing L2X under real-world circumstances, K was set as the average number of tokens highlighted by human annotators. The average K was 23, 18, and 13 for the three aspects. In Table 1, explainers often identified the most relevant feature as a token with zero contribution, with failure rates of 14.79% for LIME and 12.95% for L2X in the aroma aspect. In testing L2X under real-world circumstances, the explainers often identified the most relevant feature as a token with zero contribution, with failure rates of 14.79% for LIME and 12.95% for L2X in the aroma aspect. Metric (B) shows that both explainers can rank at least one zero-contribution token higher than a clearly relevant feature. Metric (C) shows that SHAP and L2X have different performance in ranking zero-contribution tokens ahead of clearly relevant ones. In testing L2X under real-world circumstances, explainers often identified the most relevant feature as a token with zero contribution. Failure rates were 14.79% for LIME and 12.95% for L2X in the aroma aspect. The explainers can rank zero-contribution tokens higher than clearly relevant features. SHAP and L2X have different performance in this aspect. The heatmap in Figure 6 shows rankings by each explainer for the palate aspect. Top 5 features are displayed on the right-hand side. Both LIME and SHAP explainers tend to attribute importance to nonselected tokens, ranking \"mouthfeel\" and \"lacing\" as the most important in S x. However, L2X prioritizes \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as the top tokens, with \"gorgeous\" not making the top 13. If evaluated by humans, the explainers may not align with the model's behavior. In this work, an important distinction between two perspectives of explanations is highlighted. An off-the-shelf evaluation test for post-hoc explanatory methods is introduced, offering guarantees on the behavior of a real-world neural network. Error rates on different metrics for three popular explanatory methods are presented. The first automatic verification framework offers guarantees on the behavior of real-world neural networks. Error rates on different metrics for three popular explanatory methods are presented to highlight potential failures. The methodology can be adapted to various tasks and areas, such as computer vision. The study evaluates the limitations of current post-hoc explainers by selecting super-pixels and making predictions based on blurred images. The algorithm used is domain-agnostic, providing a representative view of the explainers' fundamental constraints. Statistics of the dataset are provided in Table 2, showing the number of instances retained and average lengths of reviews. The study evaluates the limitations of current post-hoc explainers by selecting super-pixels and making predictions based on blurred images. Statistics in Table 2 show the number of instances retained and average lengths of reviews, along with average numbers of selected tokens and non-selected tokens. Percentages of instances eliminated and datapoints further eliminated are also provided. The study examines the impact of eliminating non-selected tokens on model predictions to identify instances of handshakes in the model's decision-making process. The study analyzes how removing non-selected tokens affects model predictions to detect handshakes in the decision-making process. If the RCNN prediction remains the same after eliminating tokens, it indicates no handshake in the instance. This implies zero contribution from the eliminated tokens. The equation simplifies to show that no handshake occurs when encoder(generator(S x )) equals encoder(generator(x)). This simplification highlights the importance of S Sx = S x for satisfying the condition. The study shows that no handshake occurs in x when encoder(generator(S x)) equals encoder(generator(x)). This simplification emphasizes the importance of S Sx = S x in satisfying the condition. The beer LIME has a nice brown \"grolsch\" bottle, pours a dark yellow color with a fruity smell, and tastes of fruit with a slight warming sensation. The beer LIME has a nice brown \"grolsch\" bottle, pours a dark yellow color with a fruity smell, and tastes of fruit with a slight warming sensation. The taste is smooth and better than most American lagers, with hints of apple and blueberry. The beer LIME has a nice brown \"grolsch\" bottle, pours a dark yellow color with a fruity smell, and tastes of fruit with a slight warming sensation. The taste is smooth and better than most American lagers, with hints of apple and blueberry. The beer is fizzy with a lot of head in the beginning, but lacks mouthfeel. The beer LIME has a nice brown \"grolsch\" bottle, pours a dark yellow color with a fruity smell, and tastes of fruit with a slight warming sensation. It is smooth and better than most American lagers, with hints of apple and blueberry. The beer lacks mouthfeel but is fizzy with a lot of head in the beginning."
}