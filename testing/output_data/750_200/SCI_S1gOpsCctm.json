{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are effective for control policies in reinforcement and imitation learning. A new technique, Quantized Bottleneck Insertion, is introduced to learn finite representations of memory vectors and observation features in RNNs. This allows for better analysis and understanding of memory use and behavior. Results show small finite representations in synthetic environments and Atari games. The new technique, Quantized Bottleneck Insertion, allows for learning small finite representations of memory and observation features in RNNs. This leads to improved interpretability of policies in deep reinforcement learning and imitation learning applications. In this paper, the focus is on understanding and explaining RNN policies by learning more compact memory representations. RNN policies use internal memory to encode features of the observation history, which are crucial for decision making but hard to interpret due to high-dimensional continuous memory vectors. The focus is on understanding RNN policies by learning compact memory representations. RNN memory is challenging to explain due to high-dimensional vectors updated through complex networks. Quantizing memory and observation representation can capture discrete concepts, aiding explainability. Manipulating and analyzing the quantized system can help understand memory use in RNN policies. Our main contribution is introducing a method to transform an RNN policy with continuous memory and observations into a finite-state representation called a Moore Machine using Quantized Bottleneck Network (QBN) insertion. QBNs are auto-encoders with quantized latent representations that encode memory states and observation vectors. The method introduced transforms an RNN policy into a Moore Machine using Quantized Bottleneck Network (QBN) insertion. QBNs encode memory states and observation vectors with quantized latent representations, enhancing the RNN policy. The MMN can be directly used or fine-tuned to improve QBN insertion inaccuracies. Training quantized networks is challenging, but a simple approach using \"straight through\" gradient estimators works well. Experiments in synthetic domains and benchmark grammar learning problems show accurate extraction of MMNs and insight into RNN memory use. Additionally, experiments on 6 Atari games using RNNs achieve state-of-the-art performance. Experiments on 6 Atari games using RNNs show near-equivalent MMNs can be extracted, providing insights into memory usage not obvious from observing RNN policy. Some games show RNNs using memory purely reactively, while others indicate open-loop control implementation. Our work focuses on learning finite-memory representations of continuous RNN policies, which is a novel approach compared to prior work on extracting Finite State Machines from recurrent networks. The work focuses on extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages. Previous approaches involve discretizing memory space and using query-based learning algorithms. However, these methods do not directly apply to learning policies, requiring extension to Moore Machines. The FSM approximation produced is separate from the RNN and serves as a proxy of the RNN behavior. Our approach directly inserts discrete elements into the RNN to preserve its behavior and allow for a finite state characterization. This method enables fine-tuning and visualization using standard learning frameworks, differentiating it from previous approaches that produce FSM approximations separate from the RNN. Our work extends the approach of directly learning recurrent networks with finite memory, introducing the method of QBN insertion to transform pre-trained recurrent policies into a finite representation. This differs from prior work on learning fully binary networks, which aims to create more efficient networks in terms of time and space. Our work focuses on learning discrete representations of memory and observations in recurrent neural networks (RNNs) for interpretability, rather than efficiency like prior work on fully binary networks. RNNs are commonly used in reinforcement learning to represent policies with internal memory. Our work focuses on learning discrete representations of memory and observations in RNNs for interpretability. An RNN is given an observation and must output an action based on its hidden state, which is continuously updated and influences the action choice. The RNN extracts observation features, outputs an action based on a policy, and transitions to a new state based on a transition function. Our work focuses on learning discrete representations of memory and observations in RNNs for interpretability. The RNN outputs an action based on its hidden state, transitions to a new state, and aims to extract compact quantized representations of memory and observations using Moore Machines and their deep network counterparts. In our work, we aim to capture memory and observations in a finite system using Moore Machines and their deep network counterparts. Moore Machines consist of hidden states, observations, actions, transition functions, and policies mapping states to actions. The transition function updates the hidden state based on the current state and observation. A Moore Machine Network (MMN) uses deep networks to represent the transition function and policy, mapping continuous observations to a finite discrete observation space. The MMN captures memory and observations in a finite system. In this work, an MMN uses deep networks to map continuous observations to a finite discrete observation space. The memory is composed of k-level activation units, and environmental observations are transformed to a k-level representation before processing. In this work, an MMN uses deep networks to map continuous observations to a finite discrete observation space. The memory is restricted to k-level activation units, and environmental observations are transformed to a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for non-trivial problems, such as training high-performing MMNs for Atari games. A new approach for learning MMNs is introduced to leverage the ability to incorporate quantized units into the backpropagation process. The new approach for learning Memory Mapping Networks (MMNs) leverages the ability to learn Recurrent Neural Networks (RNNs). By first training quantized bottleneck networks (QBNs) to embed continuous observation features and hidden states into a k-level quantized representation, and then inserting them into the original recurrent net with minimal behavior changes, the resulting network functions as an MMN consuming quantized features and maintaining quantized state. The new approach involves training quantized bottleneck networks (QBNs) to embed continuous features into a k-level quantized representation. This network functions as a Memory Mapping Network (MMN) consuming quantized features and maintaining quantized state. The goal is to discretize a continuous space by quantizing the activations of units in the encoding layer. A QBN is represented by a multilayer encoder E mapping inputs x to a latent encoding E(x) and a multilayer decoder D. The QBN output uses 3-level quantization of +1, 0, and -1. The tanh activation for output nodes of E(x) can make it difficult to produce quantization level 0 during learning. The goal is to discretize a continuous space by quantizing the activations of units in the encoding layer. To support 3-valued quantization, a flatter activation function \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(\u22123x) is used. The introduction of the quantize function in the QBN results in non-differentiability, posing a challenge for backpropagation. The straight-through estimator is suggested as a solution to this issue. The straight-through estimator is effective in dealing with the non-differentiability issue caused by the quantize function in the QBN. It treats the quantize function as the identity function during back-propagation, allowing for k-level encoding in the last layer of E. The QBN is trained as an autoencoder using L2 reconstruction error for a given input x. A recurrent policy can be run in the target environment to generate a large training set. The approach involves training two QBNs, b f and b h, on observed features and states respectively. If low reconstruction error is achieved, the latent \"bottlenecks\" of the QBNs can be seen as high-quality encodings of the original hidden states and features. The QBNs act as \"wires\" propagating the input. The QBNs b f and b h act as \"wires\" propagating input in the RNN, with some noise from imperfect reconstruction. Inserted between RNN units and output, they provide high-quality encodings of hidden states and features. If perfect reconstruction, RNN behavior remains unchanged. The QBNs b f and b h act as \"wires\" in the RNN, providing encodings of hidden states and features with some noise. The RNN can be seen as an MMN due to the bottlenecks of b f and b h. While perfect reconstruction would not change RNN behavior, in practice, MMN may not behave identically. Fine-tuning the MMN using original RNN data can mitigate performance degradation. During fine-tuning, the MMN is trained to match the RNN's softmax distribution over actions for stability. Visualization tools can be used to understand the memory's feature bits and their roles. The MMN is used with visualization tools to investigate memory feature bits and gain a semantic understanding of their roles. Creating a Moore Machine from the MMN helps analyze different machine states and their relationships. The Moore Machine is created using the MMN to generate a dataset of quantized states, features, and actions. The state-space corresponds to distinct quantized states, while the observation-space consists of unique feature vectors. The transition function is constructed from the data to capture transitions. The resulting Moore Machine may have more states than necessary. The Moore Machine is created using the MMN to generate a dataset of quantized states, features, and actions. The resulting Moore Machine may have more states than necessary, so standard minimization techniques are applied to arrive at the minimal equivalent Moore Machine BID19. This reduces the number of distinct states and observations. The experiments aim to extract MMNs from RNNs without significant loss in performance, determine the magnitude of states and observations in minimal machines, especially for complex domains like Atari, and evaluate if the learned MMNs are beneficial. In this section, the study addresses questions about the performance impact, the number of states and observations in minimal machines, and the interpretability of recurrent policies. Two domains with known ground truth Moore Machines are considered: a synthetic environment called Mode Counter and benchmark grammar learning problems. The Mode Counter Environments (MCEs) allow for varying memory requirements and types of memory usage. Mode Counter Environments (MCEs) allow varying memory requirements and types of memory usage. MCEs are a type of Partially Observable Markov Decision Process with M modes, transitioning over time based on a distribution. The agent receives a reward for taking the correct action associated with the active mode at each time step. In Mode Counter Environments (MCEs), the agent must infer the mode via observations and memory use to receive a reward for the correct action. Different parameterizations test memory usage for optimal performance. Three MCE instances in experiments use memory and observations in different ways, such as the Amnesia MCE designed to test memory use. In Mode Counter Environments (MCEs), different instances test memory usage for optimal performance. The Amnesia MCE does not require memory for optimal actions, the Blind MCE requires memory to track mode sequences, and the Tracker MCE uses both memory and observations for optimal actions. The Tracker MCE requires memory and observations to select optimal actions. The memory must implement counters to track key time steps where observations provide information about the mode. The architecture includes a feed-forward layer, a GRU layer, and a softmax layer for distribution over actions. Imiation learning is used for training in MCEs with 4 modes. The observation and hidden-state QBN have the same architecture with varying bottleneck units. The encoders consist of tanh nodes feeding into quantized bottleneck nodes. The decoder for both b f and b h has a symmetric architecture to the encoder. Training of b f and b h in the MCE environments was fast compared to RNN training, as QBNs do not need to learn temporal dependencies. QBNs were trained with bottleneck sizes of B f \u2208 {4, 8} and B h \u2208 {4, 8}, embedded into the RNN to create a discrete MMN. Performance of the MMN was measured before and after fine tuning, with TAB0 showing the average test score over 50 test episodes. The discrete MMN performance was measured before and after fine-tuning QBNs in the RNN. Most cases did not require fine-tuning due to low reconstruction error, resulting in optimal performance. However, for Tracker (B h = 4, B f = 4), fine-tuning improved MMN performance to 98% accuracy. Interestingly, inserting one bottleneck at a time also yielded perfect performance. After fine-tuning QBNs in the RNN, Tracker (B h = 4, B f = 4) achieved 98% accuracy. Inserting one bottleneck at a time also resulted in perfect performance, indicating combined error accumulation of the two bottlenecks affects performance. Moore Machine Extraction revealed more states and observations before minimization, showing MMN learning process. The MMN learning process does not always result in minimal state and observation representations, but after minimization, exact minimal machines were obtained for most cases. The MMNs learned via QBN insertions were found to be equivalent to the true minimal machines, showing optimal performance in most cases. The MMNs learned via QBN insertions were equivalent to the true minimal machines, showing optimal performance in most cases. Exceptions occurred when MMNs did not achieve perfect accuracy, revealing insights into memory use. For example, the machine for Blind had a single observation symbol, limiting transitions, while the machine for Amnesia showed that each observation symbol led to the same state, determining action choice solely based on the current observation. In policy learning problems, the agent's actions are determined by the current observation. The approach is evaluated on 7 Tomita Grammars, treating them as environments with 'accept' and 'reject' actions. The RNN for each grammar consists of a one-layer GRU with 10 hidden units and a fully connected softmax layer with 2 nodes. The RNNs for each grammar are trained using imitation learning with an Adam optimizer and a learning rate of 0.001. The training dataset includes accept/reject strings of lengths between 1 and 50. Test results show high accuracy for most RNNs, except for grammar #6. The RNNs were trained using imitation learning with high accuracy, except for grammar #6. MMNs were created by inserting bottlenecks in the RNNs, maintaining performance without fine-tuning. Fine-tuning results are shared in TAB1. The MMNs maintained RNN performance without fine-tuning, with minor improvements in some cases. MM extraction and minimization showed reduced state-space while maintaining performance. MMNs do not directly result in minimal machines but are equivalent to them. In this section, the technique of applying MMN learning to RNNs trained on Atari games is discussed. Unlike previous experiments with known ground truth minimal machines, for Atari games, there was no preconception of what the minimal machines might look like or how large they could be due to the complexity of input observations (i.e. images). The complexity of input observations for Atari games makes it unclear if similar results can be expected. While other efforts have been made to understand Atari agents, extracting finite state representations for Atari policies is a novel approach. All Atari agents have the same recurrent architecture, with input observations preprocessed before being fed into the network. The input observation for Atari games is preprocessed by gray-scaling, down-sampling, cropping, and normalizing. The network architecture includes 4 convolutional layers, Relu activation, GRU layer, and a fully connected layer. A3C RL algorithm is used with specific parameters for learning and discount factor. Loss is computed on the policy using softmax. The A3C RL algorithm was used to train a neural network for predicting value functions and policies. Generalized Advantage Estimation was used to compute loss on the policy. The RNN performance on six games was reported. The network architecture for QBN was adjusted for continuous observation features. The encoder and decoder had specific node configurations for the Atari domains. The encoder and decoder for the QBN in Atari domains have specific node configurations. Training data was generated using noisy rollouts to increase diversity. Bottlenecks were trained for different node sizes to learn QBNs robustly. For Atari domains, bottlenecks were trained with larger node sizes to learn QBNs robustly. The trained MMNs were inserted into the RNN for each Atari game, with performance results shown in TAB2 after fine-tuning for different combinations of node sizes. After training MMNs with larger node sizes for Atari games, fine-tuning was done for different combinations of node sizes. Results showed that for Pong, Freeway, Bowling, and Boxing, MMNs achieved similar scores to RNNs after fine-tuning. However, for Breakout and Space Invaders, MMNs scored lower even after fine-tuning. After fine-tuning MMNs for Breakout and Space Invaders, lower scores were achieved compared to RNNs due to poor reconstruction in rare game states. For example, in Breakout, the MMN failed to press the fire-button after clearing the first board, resulting in a lower score. This highlights the need for more intelligent approaches to train QBNs to capture critical information in such states. The investigation focuses on training QBNs to capture critical information in rare game states. MM Minimization reduces the number of states and observations significantly, making them easier to analyze manually. This analysis is expected to be challenging. The investigation focuses on training QBNs to capture critical information in rare game states. MM Minimization reduces the number of states and observations significantly, making them easier to analyze manually. However, analyzing moderately complex policies can be non-trivial due to the need to understand the \"meaning\" of observations and states. In Atari, three types of memory use were observed, with Pong having just three states and 10 discrete observation symbols. Each observation transitions to the same state regardless of the current state. In Pong, each observation transitions to the same state regardless of the current state, defining a set of rules mapping observations to actions without memory. In Bowling and Freeway, the minimal Markov model ignores input images, making policies open-loop controllers dependent on time-step rather than observations. The policies in Bowling and Freeway are open-loop controllers that depend on the time-step rather than observations. Bowling has a more complex open-loop policy structure compared to Freeway. The MM extraction approach provides insight into the open-loop structure of policies in Breakout, Space Invaders, and Boxing. Further analysis of memory use in RNN policies requires additional tools for semantic analysis of observations and states in Atari policies. Our approach focuses on extracting finite state Moore Machines from RNN policies to better understand memory use. By training Quantized Bottleneck Networks to produce binary encodings and inserting them into the RNN, we can extract a discrete Moore machine for analysis. Results on known environments demonstrate the effectiveness of our method. Our approach involves extracting a discrete Moore machine from RNN policies to analyze memory usage. Results on known environments and Atari games show accurate extraction of ground truth and similar performance to original RNN policies. The extracted machines reveal insights into memory usage, highlighting the small number of required memory states and cases where memory was not significantly utilized. The study analyzed policies from RNN models and found that memory usage was surprisingly small. Some policies did not use memory significantly, while others relied solely on memory and ignored observations. Future work includes developing tools for attaching meaning to observations and states, as well as analyzing finite-state machine structure for further insights. The MCE is parameterized by mode number M, mode transition function P, mode life span mapping \u2206(m), and count set C. The hidden state is a tuple (m t, c t) where m t is the current mode and c t is the count of time-steps in that mode. Mode changes occur when the lifespan is reached. The system operates in different modes based on a transition distribution. Observations are received as continuous values at each step, determining the mode when the count is within a specified set. The system operates in different modes based on a transition distribution. Observations determine the mode when the count is within a specified set. Observations are either informative or uninformative, requiring the agent to remember the current mode and track its duration for optimal performance. Experiments are conducted with different instances of Mode Counting Environments (MCE), including one with uniform random initial mode and transition. Experiments are conducted with three MCE instances: 1) Amnesia, where an optimal policy is purely reactive without using memory; 2) Blind, where optimal performance requires memory to track mode information. The experiments involve three MCE instances: Amnesia, Blind, and Tracker. Tracker is similar to Amnesia but allows for larger \u2206(m) values. It requires an optimal policy to pay attention to observations and use memory to track the current mode and mode count, making it the most challenging instance when the number of modes and their life-spans increase. The experiments involve three MCE instances: Amnesia, Blind, and Tracker. Tracker is similar to Amnesia but allows for larger \u2206(m) values. It requires an optimal policy to pay attention to observations and use memory to track the current mode and mode count, making it the most challenging instance when the number of modes and their life-spans increase. In all instances, 'A' and 'R' denote accept and reject states, respectively, with all machines being 100% accurate except for Grammar 6."
}