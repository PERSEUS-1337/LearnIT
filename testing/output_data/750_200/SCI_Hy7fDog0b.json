{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions but typically require fully-observed samples for training. In cases where only partial, noisy observations are available, a new method for training generative models using lossy measurements has been proposed. This method can recover the true underlying distribution even with per-sample information loss. The AmbientGAN method proposes a new approach for training Generative Adversarial Networks (GANs) using lossy measurements. This method can recover the underlying distribution even with per-sample information loss. AmbientGAN shows significant improvements in qualitative and quantitative results on benchmark datasets, with generative models achieving higher inception scores compared to baselines. The generator's output is passed through a simulated random measurement function, and the discriminator distinguishes between real and generated measurements. This work addresses the challenge of training generative models with limited data by directly training from noisy or incomplete samples. The approach involves using various types of measurements or projections to recover the unknown distribution, assuming the measurement process is known. The approach involves using measurements to recover the unknown distribution, assuming the measurement process is known. A new training method called AmbientGAN is used to distinguish real measurements from simulated ones in a generative model. Our method, AmbientGAN, effectively constructs generative models from noisy observations and low-dimensional projections with information loss. Results show good visual quality and improved inception scores compared to baseline methods. Theoretical analysis includes noisy, blurred measurements of desired images. Theoretical analysis shows that noisy, blurred measurements of images are not invertible for a single image due to added noise. However, the distribution of measured images uniquely determines the distribution of original images, implying that a pure Nash equilibrium for the GAN game must match the true distribution. Similar results are shown for a dropout measurement model where pixels are set to zero. Our empirical work explores measurement models such as dropout and random projection. In our experiments with the celebA dataset, we observe randomly placed occlusions on celebrity faces. In FIG1, the celebA dataset of celebrity faces is considered under randomly placed occlusions. In FIG2, learning from noisy, blurred versions of images from the celebA dataset is discussed. Incorporating the measurement process into GAN training improves sample quality. Learning a GAN on images denoised by Wiener deconvolution leads to poor sample quality, while our models produce cleaner samples. We consider learning a generative model on 2D images in the MNIST dataset from pairs of 1D projections, where measurements consist of projecting the image onto random lines. AmbientGAN recovers a lot of the image details for both variants considered. The text discusses two variants of learning generative models on 2D images from 1D projections. AmbientGAN is able to recover image details, but one variant cannot identify distribution up to rotation or reflection. Different approaches to constructing neural network based generative models are mentioned, including autoregressive and adversarial methods. The adversarial framework is highlighted for its effectiveness in modeling complex data distributions like images, videos, and 3D models. Generative adversarial networks (GANs) are powerful in modeling complex data distributions like images, videos, and 3D models. They have various applications, including solving inverse problems and translating images between domains. GANs can make synthetic data more realistic and operate on different spaces using generators and discriminators. Training stability is explored in connection with low dimensional projections of samples. Our work explores training stability by using an array of discriminators operating on different low-dimensional projections of data. It is related to previous work where 3D object shapes were created from 2D projections. The AmbientGAN framework is used, where 2D projections are created using weighted sums of voxel occupancies. The process involves creating 2D projections using weighted sums of voxel occupancies. The notation includes 'r' for real distribution, 'g' for generated distributions, 'x' for underlying space, and 'y' for measurements. Measurements are obtained from a measurement function parameterized by \u03b8, with stochastic parameters having a distribution p \u03b8. The measurements y = f \u03b8 (x) are observed on samples from a real underlying distribution p r x. The task involves creating an implicit generative model of a real distribution p r x using a known distribution p \u03b8 and a set of IID realizations {y 1 , y 2 , . . . , y s } from the distribution p r y. The goal is to create a generative model of distribution p r x using a measurement process combined with adversarial training. A random latent vector Z is used, and the generator G aims to make the distribution p g x close to p r x without access to the desired objects X \u223c p r x. The goal is to learn a generator G to mimic distribution p r x without direct access to the objects. Random measurements are simulated on generated objects X g, and a discriminator distinguishes real from fake measurements. The discriminator predicts if a given y is from the real or generated distribution. The quality function q(x) is used to define the objective. The AmbientGAN objective requires f \u03b8 to be differentiable. G and D are implemented as neural networks, making the model end-to-end differentiable for training. Our model implements G and D as neural networks, allowing for end-to-end differentiability in training. Stochastic gradients are computed using sampled Z, \u0398, and Y r, with updates alternating between D and G parameters. The approach is compatible with GAN improvements and can easily incorporate additional information. The model implements neural networks for G and D, allowing for end-to-end differentiability in training. It is compatible with GAN improvements and can easily incorporate additional information, such as per sample labels. Measurement models tailored to 2D images are used for theoretical and empirical results. The measurement models used are tailored to 2D images and include Block-Pixels, Convolve+Noise, and Block-Patch. The AmbientGAN framework is versatile and can be applied to other data formats and measurement models. Convolve+Noise: Measurements are obtained using a convolution kernel k and noise distribution \u0398. The measurements are given by f \u0398 (x) = k * x + \u0398. Block-Patch, Keep-Patch, and Extract-Patch involve manipulating randomly chosen patches. Pad-Rotate-Project involves padding the image with zeros, rotating it by a random angle, and summing each channel. In Pad-Rotate-Project, images are rotated by a random angle with padding to maintain pixel boundaries. Measurement vectors are obtained by summing pixels along the vertical axis for each channel. Gaussian-Projection involves projecting onto a random Gaussian vector to include angle measurements. The goal is to recover the true distribution using unique measurement models. The text discusses the ability to recover the true underlying distribution using unique measurement models, showing that there is a consistent mapping from distributions of samples to measurements. This consistency guarantees the effectiveness of the AmbientGAN training procedure. The text discusses the uniqueness of the true underlying distribution given the measurement distribution in the AmbientGAN training procedure. The assumption of uniqueness is satisfied under Gaussian-Projection, Convolve+Noise, and Block-Pixels. The AmbientGAN framework can recover the true underlying distribution under Gaussian-Projection, Convolve+Noise, and Block-Pixels measurement models. The assumption of a finite discrete set of pixel values is common in practical scenarios. The AmbientGAN framework can recover distributions under various measurement models, including Block-Pixels. The framework assumes a finite set of pixel values, which is common in practical scenarios. Theorem 5.4 provides a sample complexity result for learning distributions in this setting. The AmbientGAN framework can recover distributions under different measurement models, such as Block-Pixels. It assumes a finite set of pixel values. The Block-Pixels measurement model with probability p can induce a unique distribution prx that matches the measurement distribution pry. For any given dataset of IID measurement samples from pry, an optimal discriminator D implies that any optimal generator G must satisfy certain conditions. Three datasets used in experiments are MNIST for handwritten digits, CelebA for face images of celebrities, and CIFAR-10 for RGB images. The dataset CelebA consists of face images of celebrities, while CIFAR-10 contains RGB images from 10 classes. Generative models used in experiments include conditional DCGAN and unconditional Wasserstein GAN for MNIST, and unconditional DCGAN for CelebA. More details on architectures and hyperparameters can be found in the appendix. The second model is an unconditional Wasserstein GAN with gradient penalty (WGANGP) for the celebA dataset, and an Auxiliary Classifier Wasserstein GAN with gradient penalty (ACWGANGP) for the CIFAR-10 dataset. Different discriminator architectures are used for 2D and 1D projections. The baseline approaches implemented to evaluate the AmbientGAN framework involve using fully connected discriminators for 1D projections like Pad-Rotate-Project. Different discriminator architectures were used for the MNIST and celebA datasets. The goal is to create an implicit generative model for p r x from a dataset of IID samples. One crude baseline is to ignore any measurements taken. The baseline approaches for evaluating the AmbientGAN framework involve using fully connected discriminators for 1D projections like Pad-Rotate-Project. A crude baseline is to ignore any measurements taken, while a stronger baseline involves invertible measurement functions to obtain full-samples for learning a generative model. In the AmbientGAN setting, the assumptions of observing \u03b8i and having invertible functions are violated. To address this, an approximate inverse function can be used to obtain x i from y i, allowing for training a generative model with the estimated inverse samples. In the AmbientGAN setting, an approximate inverse function is used to obtain x i from y i, enabling training a generative model with estimated inverse samples. Methods for obtaining approximate inverse functions include blurring images for Block-Pixels measurements, Wiener deconvolution for Convolve+Noise measurements, and Navier Stokes based approach for Block-Patch measurements. For Block-Patch measurements, the Navier Stokes based inpainting method BID2 is used to fill in zero pixels. Inverting Extract-Patch measurements is challenging as the patch position information is lost. Pad-Rotate-Project-\u03b8 measurements require sampling multiple angles and using techniques for inverting the Radon transform BID7. Inverting Pad-Rotate-Project measurements is challenging due to the lack of information about \u03b8. Results with AmbientGAN models are reported for experiments with limited projections. Samples generated by baselines and our models are presented for comparison. In the experiment, samples from the dataset, baselines, and our models are shown for selected parameter settings. Results on MNIST are in the appendix. Results on celebA with DCGAN and CIFAR-10 with ACW-GANGP are shown. Baselines struggle to invert the measurement process, while our models produce high-quality images. Our models outperform baselines in creating high-quality images by using Convolve+Noise, Block-Patch, Keep-Patch, and 1D projections measurement models on celebA with DCGAN. The study demonstrates the ability to create coherent faces by observing parts of one image at a time. Two measurement models, Pad-Rotate-Project and Pad-Rotate-Project-\u03b8, show signal degradation. Results on MNIST with DCGAN show that the model can learn rotation and reflection, with generated digits having similar orientations and chirality within each class. The second measurement model is also discussed. The model prefers consistent orientation per class for easier learning. The second measurement model produces upright digits but with lesser visual quality. Samples from the model trained on celebA dataset show a crude outline of a face, highlighting the difficulty in learning complex distributions with just 1D projections. The model trained on celebA dataset with a DCGAN shows a crude outline of a face, emphasizing the challenge of learning complex distributions with 1D projections. Inception scores are used to evaluate the generative models in the AmbientGAN framework, with a high test set accuracy of 99.2% achieved on the MNIST dataset. For MNIST, a classification model achieved 99.2% accuracy with two conv+pool layers and two fully connected layers. Different models were trained with varying pixel blocking probabilities, showing AmbientGAN models outperform baselines as pixel blocking increases. The model performance on MNIST was evaluated with varying pixel blocking probabilities. AmbientGAN models outperformed baselines as pixel blocking increased. The Convolve+Noise measurements showed that as noise levels increased, baseline performance deteriorated. The AmbientGAN models outperform baselines as noise levels increase, maintaining high inception scores. The Pad-Rotate-Project model performs poorly with an inception score of 4.18, while the model with Pad-Rotate-Project-\u03b8 measurements achieves a score of 8.12. Comparatively, the vanilla GAN model trained with fully observed samples is also mentioned. The model with Pad-Rotate-Project-\u03b8 measurements achieves an inception score of 8.12, close to the fully-observed case. Total variation inpainting method is slow, with performance similar to unmeasured-blur baseline on MNIST. Inpainting baselines are not run on CIFAR-10 dataset. Generative models require a high-quality dataset, but our approach shows how to learn a distribution from incomplete, noisy measurements. This allows for the construction of new models with superior performance over baselines. Our approach demonstrates learning a distribution from incomplete, noisy measurements to construct new generative models with superior performance over baselines. The lemma discusses data distribution, parameter distribution, and measurement distribution in the context of the vanilla GAN model. The lemma discusses learning a distribution from incomplete, noisy measurements to construct new generative models with superior performance over baselines. It focuses on the measurement distribution in the context of the vanilla GAN model, stating that the underlying distribution must match all 1D marginals to converge in distribution. The Cramer-Wold theorem states that a unique probability distribution can match all 1D marginals obtained with Gaussian projection measurements. The Convolve+Noise measurement model requires a unique distribution to induce the measurement distribution. The Convolve+Noise measurement model requires a unique distribution to induce the measurement distribution, with a bijective map between X and Z established through continuous transformations. The measurement distribution is induced by a unique distribution, with a bijective map between X and Z established through continuous transformations. The pdfs of X and Z are related by a Jacobian, and the pdf of Y is a convolution of individual probability density functions. Taking the Fourier transform on both sides leads to a reverse map from the measurement distribution p y to a sample distribution p x. The reverse map from the measurement distribution p y to a sample distribution p x uniquely determines the true underlying distribution p x. The empirical version of the vanilla GAN objective is defined for a dataset of measurement samples. The empirical version of the vanilla GAN objective is defined for a dataset of measurement samples. The optimal discriminator for the empirical objective is determined by the empirical distribution of samples. If the discriminator is fixed to be optimal, then any optimal generator must match the empirical distribution. The proof of Theorem 5.4 shows a unique distribution for the Block-Pixels measurement model with a probability of blocking a pixel. By applying random measurement functions to samples from a discrete distribution, measurements are obtained, and a transition matrix is used for the model. The distribution over measurements p y can be written in terms of p x and the transition matrix A. If A is invertible, p x is recoverable from p y. The sample complexity is determined by the minimum eigenvalue magnitude of A. The sample complexity is determined by the minimum eigenvalue magnitude of A. For any > 0, we have DISPLAYFORM3 where we used union bound and Chernoff inequalities. Setting this to \u03b4, we get s = t 2 2\u03bb 2 2 log 2t \u03b4. With probability \u2265 1 \u2212 \u03b4, DISPLAYFORM4 =. In the specific case of Block-Pixels measurement, we divide images into classes based on the number of zero pixels. In the context of Block-Pixels measurement, images are divided into classes based on the number of zero pixels. The transition matrix A is lower triangular, and each pixel is blocked independently with probability p. The probability of no pixels being blocked is (1 - p)^n, giving each image at least a (1 - p)^n chance of being unaffected. The transition matrix A in Block-Pixels measurement is lower triangular, with each pixel blocked independently with probability p. The diagonal entries of the transition matrix are strictly positive, with a minimum value of (1 - p)^n. This proves that A is invertible, with the smallest eigenvalue being (1 - p)^n. The DCGAN model on MNIST follows a specific architecture. The DCGAN model on MNIST uses a specific architecture with a noise input of 100 dimensions sampled uniformly. Both generator and discriminator use batch-norm and concatenate labels with inputs. The WGANGP model on MNIST follows a different architecture. The generator in the WGANGP model on MNIST uses a latent vector of 128 dimensions with batch-norm, while the discriminator has three convolutional layers and one linear layer without batch-norm. The unconditional DCGAN model on celebA has a latent vector of 100 dimensions with one linear layer and four deconvolutional layers in the generator. The discriminator in the DCGAN model on celebA also uses four layers. The ACWGANGP model on CIFAR-10 utilizes a latent vector of 128 dimensions with a residual architecture. The generator consists of a linear layer followed by three residual blocks, each containing conditional batch normalization, nonlinearity, and upconvolution layers. In the ACWGANGP model on CIFAR-10, the generator uses a latent vector of 128 dimensions and a residual architecture with three residual blocks. Each block includes conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator consists of one residual block with two convolutional layers followed by three residual blocks and a final linear layer. Additional results for various measurement models are presented, assuming known parametric form and distribution of measurement function parameters for stochastic simulation. In the context of the ACWGANGP model on CIFAR-10, the generator and discriminator architectures are described. The following experiment demonstrates the robustness of the AmbientGAN approach to mismatches in parameter distribution of the measurement function, specifically using the Block-Pixels measurement model on MNIST dataset. The AmbientGAN approach is tested with the Block-Pixels measurement model on the MNIST dataset. The plot of the inception score peaks at p = 0.5, showing robustness to parameter distribution mismatch. The AmbientGAN approach, tested with the Block-Pixels measurement model on the MNIST dataset, shows robustness to parameter distribution mismatch. The generator learned through AmbientGAN captures the data distribution well, improving sensing over sparsity-based approaches. The GAN trained with p = 0.5 is used for compressed sensing, showing a plot of reconstruction error vs the number of measurements. Using AmbientGAN trained with corrupted samples, we observed a reduction in the number of measurements for compressed sensing on MNIST compared to a regular GAN trained with fully observed samples."
}