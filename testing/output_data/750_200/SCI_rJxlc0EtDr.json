{
    "title": "rJxlc0EtDr",
    "content": "Recent research on neural network architectures with external memory has focused on the bAbI question and answering dataset, which presents challenging tasks requiring reasoning. To further investigate the reasoning capacity of these architectures, a classic associative inference task from human neuroscience literature was employed. This task aims to test the ability to understand distant relationships among elements distributed across multiple facts or memories. Surprisingly, current architectures struggle with reasoning over long distance associations. MEMO architecture was developed to improve reasoning over long distance associations by introducing novel components like external memory separation and adaptive retrieval mechanism. It can solve complex tasks involving finding shortest paths between nodes. MEMO architecture utilizes an adaptive retrieval mechanism for reasoning over long distance associations. It can solve complex tasks and has been successful in bAbI tasks. This allows for connecting facts acquired at different points in time to make judgments. Inferential reasoning involves recombining experiences to infer relationships, supported by the hippocampus. Memories are stored independently through pattern separation to minimize interference. The hippocampus stores memories independently through pattern separation to minimize interference, allowing for the recall of specific events as 'episodic' memories. Recent research shows that the integration of separated experiences occurs at the point of retrieval, resolving the tension between separated memories and generalization. The integration of separated experiences at the point of retrieval through a recurrent mechanism supports inference. Neural networks like the Differential Neural Computer and end to end memory networks have shown remarkable abilities in computational and reasoning tasks. Differential Neural Computer (DNC) and end-to-end memory networks (EMN) have demonstrated strong capabilities in tackling complex computational tasks. Recent advancements in attention mechanisms and context utilization have enabled traditional neural networks to handle similar tasks. However, some tasks like bAbI present challenges due to repetitions and commonalities between training and testing sets. To address this issue, a new task called Paired Associative Inference (PAI) has been introduced. To overcome limitations in neural networks, a new task called Paired Associative Inference (PAI) was introduced, derived from neuroscientific literature. PAI aims to capture inferential reasoning by forcing networks to learn abstractions to solve unseen associations. This task is followed by finding the shortest path and bAbi tasks to investigate memory capabilities. The MEMO approach, unlike other models, retains the full set of facts in memory and learns a linear projection for memory-based reasoning. MEMO, a new approach for memory-based reasoning, retains all facts in memory and utilizes a recurrent attention mechanism for flexibility. It addresses the issue of computation time in neural networks by allowing for flexible weighting of individual memory elements. The problem of prohibitive computation time in neural networks is addressed by adapting the amount of compute time to the complexity of the task. This is achieved by adjusting input values rather than hand-tuning them, drawing inspiration from a new approach for memory-based reasoning called MEMO. The amount of compute time in neural networks is adjusted based on the complexity of the task, inspired by models like REMERGE and techniques such as adaptive computation time. In a neural network, the halting policy determines when the process terminates. Inspired by adaptive computation time, the network outputs an action to continue computing or answer the task. Unlike ACT, the binary halting random variable is trained using reinforcement learning to adjust weights based on the optimal solution. The binary halting random variable is trained using REINFORCE to adjust weights based on the optimal number of computation steps. An extra term is added to the REINFORCE loss to minimize the expected number of steps, encouraging the network to prefer specific representations and computation. The contributions of the study include a new task emphasizing reasoning, an investigation of memory representation for inferential reasoning, extensions to memory architectures for better results, and a REINFORCE loss component for learning the optimal number of iterations for task-solving. The study introduces memory architectures for reasoning tasks and a REINFORCE loss component to learn the optimal number of iterations for task-solving. Empirical results on three tasks demonstrate the effectiveness of these contributions. The study focuses on memory architectures for reasoning tasks and introduces a REINFORCE loss component to learn the optimal number of iterations for task-solving. The setup involves a set of knowledge inputs and a query, with the network predicting the answer. The length of the knowledge input sequence and each input sentence varies, with EMN embedding each word and summing them. The EMN architecture embeds words and calculates weights over memory elements to produce output in a story. The model uses embedding matrices for key, values, and query, along with positional encoding and element-wise multiplication. The EMN architecture embeds words and calculates weights over memory elements to produce output in a story. The model uses embedding matrices for key, values, and query, along with positional encoding and element-wise multiplication. In contrast, MEMO embeds input differently by deriving a common embedding for each input matrix and adapting them to be key or value. MEMO embeds input differently by deriving a common embedding for each input matrix, adapting them to be key or value, and using multiple heads to attend to the memory. The MEMO model uses multiple heads to attend to memory, with each head having a different view of the same common inputs. This allows for learning how to weight each item during a memory lookup, contrasting with hand-coded positional embeddings used in other models. The MEMO model utilizes multi-head attention to weight items during memory lookup, contrasting with hand-coded positional embeddings in other models. It also incorporates DropOut and LayerNorm for improved generalization and learning dynamics. The attention mechanism in the MEMO model utilizes matrices for transforming logits and queries, producing the answer using an output MLP. It differs from Vaswani et al. (2017) by preserving the query instead of self-attention. MEMO model utilizes matrices for attention mechanism, preserving query for computational efficiency compared to self-attention methods like Dehghani et al. (2018). It can output potential answers and learn the number of computational steps needed to effectively answer a query. MEMO model uses matrices for attention mechanism to output potential answers and determine the number of computational steps required to effectively answer a query. The decision is made by collecting information at each step and processing it with gated recurrent units (GRUs) and a MLP to define a binary policy and approximate the value function. The input to the network is the Bhattacharyya distance between attention weights of current and previous time steps. The network in the MEMO model uses matrices for attention mechanism to output potential answers and determine the number of computational steps needed. It is trained using REINFORCE and adjusts parameters using n-step look ahead values. The input is the Bhattacharyya distance between attention weights of current and previous time steps. The MEMO model's network utilizes matrices for attention mechanism to output answers and determine computational steps. Trained with REINFORCE, it adjusts parameters using n-step look ahead values. The new term introduced in the loss function, L Hop, minimizes expected number of hops, encouraging the network explicitly. The new term in the loss function, L Hop, aims to minimize the expected number of hops in the network, encouraging efficient computation. The variance issue with training discrete random variables is addressed, particularly in the case of a binary halting random variable where the variance is bounded by 1/4. The reward structure in the network is defined by the target answer and prediction. The final layer of M LP R is initialized with bias init to increase the probability of halting. A maximum number of hops, N, is set for the network, with no gradient sharing between the hop network and the main MEMO network. The network stops performing additional hops when the maximum number of hops is reached. There is no gradient sharing between the hop network and the main MEMO network. Memory-augmented networks have gained interest for their ability to solve abstract and relational reasoning tasks. Another influential model, the Differential Neural Computer (DNC), operates sequentially on inputs and learns to read and write to a memory store. The DNC is a computer model that reads and writes to memory sequentially, capable of solving algorithmic problems but difficult to scale. An extension with sparsity performed well on larger tasks like the bAbI task suite. Alternative memory-augmented architectures have been developed since, including the Dynamic Memory Network. Several memory-augmented architectures have been developed, such as the Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet, each incorporating unique features for relational reasoning over memory contents. The RelationNet enables relational reasoning over memory contents and has shown good performance on tasks like the bAbI task suite. Adaptive Computation Time (ACT) is a mechanism for dynamically adjusting computational steps based on task complexity. Adaptive Computation Time (ACT) and other methods like Adaptive Early Exit Networks and REINFORCE are approaches to dynamically adjust computational steps in neural networks. These methods allow for efficient computation by modulating the number of steps based on task requirements. Adaptive Computation Time (ACT) and other methods dynamically adjust computation steps in neural networks. These approaches optimize efficiency by modulating the number of steps based on task requirements. This technique has been applied to various neural network architectures, including recurrent neural networks and networks augmented with external memory. Our method introduces the idea of using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory. Graph Neural Networks consist of an iterative message passing process that propagates node and edge embeddings throughout a graph, with neural networks used to aggregate functions over graph components for supervised or semi-supervised learning. Graph Neural Networks (GNNs) involve a message passing process that spreads node and edge embeddings in a graph. Neural networks aggregate functions over graph components for various learning tasks. GNNs differ from traditional methods by incorporating recurrent components and unrolling the recurrence for a fixed number of steps. Our method differs from GNNs in that it performs adaptive computation to modulate the number of message passing steps and does not require message passing between memories. The model allows input queries to attend directly to memory slots, enabling paired associative inference without the need for explicit inference steps. One contribution of this paper is to introduce a task derived from neuroscience to probe the reasoning capacity of neural networks. The task involves paired associative inference (PAI) to capture the essence of reasoning by appreciating distant relationships among elements distributed across multiple facts or memories. The paired associative inference (PAI) task is a prototypical task used to study the role of the hippocampus in generalization. In this task, two images are randomly associated together to test reasoning capacity in neural networks. In the paired associative inference (PAI) task, the agent is exposed to a second pair of images, where one image is paired with a new image. Two types of queries can be asked during testing: direct queries test episodic memory, while indirect queries require inference across multiple episodes. The network is presented with a cue, image A, and two possible choices: the match that was originally paired with B or a lure that was paired with B. The paired associative inference task involves presenting a cue, image A, with two choices: the match originally paired with B or a lure paired with B. The correct answer requires linking A and C because they both were paired with B. The study compared MEMO with other memory-augmented architectures like End to End Memory Networks and DNC, as well as the Universal Transformer. For more details on the batch creation, refer to the appendix. MEMO was compared with other memory-augmented architectures like End to End Memory Networks and DNC, as well as the Universal Transformer. Table 1 shows the results of MEMO and other models on the hardest inference query for each PAI task. MEMO achieved high accuracy on the A-B-C set, along with DNC, outperforming EMN and UT. MEMO, along with DNC, achieved the highest accuracy on the A-B-C set, outperforming EMN and UT. For longer sequences, MEMO was the only architecture successful in answering complex inference queries. Further analysis on the length 3 PAI task revealed that DNC required 10 pondering steps to match MEMO's accuracy, which converged to 3 hops. MEMO achieved the same accuracy as DNC on the A-B-C set, converging to 3 hops. The attention weights of an inference query were analyzed to associate a CUE with the MATCH and avoid interference of the LURE. The original sequence A-B-C had class IDs 611-191-840, stored in different slots. In the first hop, MEMO retrieved the memory in slot 10 containing the CUE ID 611. In the second hop, MEMO assigned probability masses to slots 10, 16, and 13, supporting correct inference decisions. The activation pattern of MEMO in the last hop confirmed appropriate probability masses assigned to slots for correct inference decisions. Different patterns of memories activation were observed in another instance of MEMO using 7 hops, indicating that the algorithm used depends on the number of hops taken by the network. The algorithm used for inference in MEMO depends on the number of hops taken by the network, which can be related to knowledge distillation in neural networks. A set of ablation experiments confirmed that specific memory representations and recurrent attention mechanism support successful inference. The combination of specific memory representations and recurrent attention mechanism supports successful inference, as confirmed by ablation experiments. This conclusion applies to inference queries but not direct queries, which test episodic memory. Additionally, the adaptive computation mechanism in MEMO was found to be more data efficient compared to ACT (Graves, 2016) for this task. Our method was found to be more data efficient for the task compared to ACT (Graves, 2016). The weights analysis of an inference query in the length 3 PAI task is shown in Figure 2. The network uses 3 hops for inference, with associated retrieved slots for each probability mass. Table 2 displays the accuracy of models in finding the shortest path between nodes on graphs of varying complexity. DNC, Universal Transformer, and MEMO achieved perfect accuracy on a small graph, while MEMO outperformed EMN on more complex graphs. MEMO outperformed EMN and DNC in predicting nodes on more complicated graphs with high connectivity. It showed great scalability by considering more paths as the number of hops increased. Universal Transformer had different performance in predicting the first versus the second node of the shortest path. The Universal Transformer had different performance in predicting the first versus the second node of the shortest path. UT achieved slightly lower performance than MEMO in the latter case, showing its capability of computing operations that require direct reasoning. Test results for the best 5 hyper-parameters for MEMO are reported, along with results from the best run for EMN, UT, and DNC. Additionally, the study also focused on the bAbI question answering dataset, consisting of 20 different tasks, with the model trained on the joint 10k dataset. In the bAbI question answering dataset, MEMO achieved high accuracy on all tasks in the 10k training set, outperforming other baselines with fewer errors. The study achieved high accuracy on all tasks in the bAbI question answering dataset, outperforming other baselines with fewer errors. A combination of memory representations and recurrent attention was critical for state-of-the-art performance. The use of layernorm in the recurrent attention mechanism improved stability during training and performance. In this paper, an investigation of memory representations supporting inferential reasoning was conducted. MEMO, an extension to existing memory architectures, showed state-of-the-art results in a new proposed task, paired associative inference. MEMO, an extension to existing memory architectures, achieved state-of-the-art results in paired associative inference and graph traversal tasks. It also matched the performance of the current state-of-the-art results on the bAbI dataset. The flexible weighting of individual elements in memory allowed MEMO to solve long sequences effectively. The results achieved by MEMO were supported by the flexible weighting of individual elements in memory, combining separated storage of facts with a recurrent attention mechanism. The task involved creating datasets from the ImageNet dataset, embedding images using a pre-trained ResNet, and generating sequences of varying lengths. The study by He et al. (2016) created datasets with sequences of different lengths (3, 4, and 5 items) for training, evaluation, and testing. Each sequence in the batch consisted of a memory, query, and target, with pairwise associations between items in the sequence. The study by He et al. (2016) created datasets with sequences of different lengths (3, 4, and 5 items) for training, evaluation, and testing. N sequences from the pool, with N = 16, were used to create memory content with pair wise associations. Queries consisted of cue, match, and lure images, with two types - 'direct' and 'indirect'. The study by He et al. (2016) created datasets with sequences of different lengths for training, evaluation, and testing. Queries consisted of cue, match, and lure images, with two types - 'direct' and 'indirect'. 'Direct' queries involve finding cue and match in the same memory slot, while 'indirect' queries require inference across multiple episodes. The study by He et al. (2016) involved creating datasets with sequences of different lengths for training, evaluation, and testing. Queries included cue, match, and lure images, with 'direct' queries requiring finding cue and match in the same memory slot, and 'indirect' queries involving inference across multiple episodes. In the inference trail, the cue, match, and lure images are concatenated, with the match and lure positions randomized to avoid degenerate solutions. The lure image is always from a different sequence in the current memory, requiring an appreciation of the correct connection between the images to solve the task. The study involved creating datasets with sequences of different lengths for training, evaluation, and testing. Queries included cue, match, and lure images, with 'direct' queries requiring finding cue and match in the same memory slot, and 'indirect' queries involving inference across multiple episodes. For each entry in the batch, all possible queries that the current memory store could support were generated, with half being direct queries and the other half indirect. The targets for the network to predict are the class of the matches. Longer sequences provide more 'direct' queries but also multiple 'indirect' queries that require different connections between images. The network predicts the class of matches using longer sequences that generate both 'direct' and 'indirect' queries. 'Indirect' queries require more inference steps and overlapping images appreciation. Inputs for different architectures vary: EMN and MEMO use memory and query, while DNC embeds inputs differently. The inputs for different architectures vary: EMN and MEMO use memory and query, while DNC embeds stories and query in the same way as MEMO. UT embeds stories and query like MEMO, then uses its encoder output as the model output. Results are from the evaluation set at the end of training, with each set containing 600 items. Graph generation is used in the shortest path. Graph generation for shortest path experiments involves generating graphs by uniformly sampling two-dimensional points from a unit square. Each point represents a node, with K nearest neighbors used as outbound connections. The task is represented in three parts: a graph description, a query, and the target. The task involves representing graphs with tuples of integers for connections between nodes, queries for paths, and target paths. During training, a mini-batch of 64 graphs with queries and target paths is sampled. During training, a mini-batch of 64 graphs is used, with queries represented as a matrix of size 64 \u00d7 2 and target paths of size 64 \u00d7 (L \u2212 1). Graph descriptions are of size 64 \u00d7 M \u00d7 2, where L is the length of the shortest path and M is the maximum number of nodes allowed. The upper bound M is determined by the maximum number of nodes multiplied by the out-degree of the nodes in the graph. All networks were trained for 2e4 epochs, each consisting of 100 batch updates. During training, the graph descriptions are of size 64 \u00d7 M \u00d7 2, where M is determined by the maximum number of nodes multiplied by the out-degree of the nodes in the graph. The networks were trained for 2e4 epochs, each with 100 batch updates. For EMN and MEMO, the graph description is set to be the contents of their memory, with the query as input. The model predicts answers for nodes sequentially, with the first node predicted before the second. An important difference between MEMO and EMN is that for EMN, the ground truth answer of the first node is used as the query for the second node, while for MEMO, the answer is used. The difference between MEMO and EMN lies in how the query for the second node is determined. EMN uses the ground truth answer of the first node as the query for the second node, while MEMO uses the predicted answer by the model for the first node. This approach aims to enhance EMN's performance and test MEMO's ability to reason sequentially over multiple steps. The weights for each answer are not shared, and the Universal Transformer also embeds the query and graph description before using the encoder of the UT architecture. The Universal Transformer architecture embeds the query and graph description before using the encoder. For DNC, the graph description tuples are presented first followed by the query tuple, with pondering steps used to output the sequence of nodes. The weights for each answer are not shared. The graph description is presented first, followed by the query tuple. Pondering steps are used to output the sequence of nodes for the proposed shortest path. The models are trained using Adam with cross-entropy loss. Evaluation is done on a batch of 600 graph descriptions, queries, and targets to calculate mean accuracy over all nodes of the target path. The models DNC, UT, and MEMO have different approaches in providing answers for the second node in the target path. DNC and UT have a 'global view' while MEMO has a 'local view' on the problem. This difference affects their performance in reasoning and achieving accuracy in the second node. In contrast to DNC and UT, MEMO has a 'local view' on the problem, where the answer to the second node depends on the answer to the first node. This limits MEMO's performance if the first node's answer is incorrect. An experiment comparing MEMO and EMN showed varying results based on different query conditions. The results in Table 8 show that providing MEMO with the ground truth for node 1 as a query for node 2 improves performance compared to using the prediction of the first node. However, when EMN is trained using the same regime as MEMO, its performance drops significantly. These findings were consistent across experiments with different numbers of nodes and outbound edges. In a simpler scenario with 20 nodes and 3 outbound edges, the English Question Answer dataset was used for an experiment. The text was pre-processed by converting to lowercase, ignoring periods and interrogation marks, and treating blank spaces as word separation tokens. Commas were only used in answers and not ignored, leading to each answer having its own label. This setup ensured that each input corresponded to a single answer throughout the dataset. At training time, a mini-batch of 128 queries and corresponding stories are sampled from the test dataset. Queries are a matrix of 128 \u00d7 11 tokens, and sentences are of size 128 \u00d7 320 \u00d7 11. Padding with zeros is done for queries and stories that do not reach the max sentence and stories size. At training time, a mini-batch of 128 queries and stories are sampled from the test dataset. Queries and stories are padded with zeros to reach the max sentence and stories size. Different models use stories and query inputs in various ways for prediction. At training time, a mini-batch of 128 queries and stories are sampled from the test dataset. Different models use stories and query inputs in various ways for prediction. The encoder of UT with architecture described in Section H is used, and its output is used as the model output. Optimization steps are performed using Adam for all models with hyper parameters detailed in Appendix Section D.2. Tasks in bAbI require temporal context, so a time encoding column vector is added to the memory store in MEMO. All networks were trained for 2e4 epochs with 100 batch updates each. Evaluation is done by sampling a batch of 10,000. The memory store is trained using a time encoding derived from Vaswani et al. (2017) for 2e4 epochs with 100 batch updates. Evaluation involves sampling a batch of 10,000 elements and computing the mean accuracy over examples for each of the 20 tasks of bAbI. The models use polynomial learning rate decay and cross entropy loss for training. MEMO was trained with a cross entropy loss and had to predict class ID, node ID, and word ID in different tasks. The halting policy network parameters were updated using RMSProp with a specific learning rate. The temporal complexity of MEMO is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where n s is the number of samples processed. MEMO has a complexity of O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where n s is the number of samples processed with the network. Parameters A, N, H, I, S, d are constants. MEMO is linear with respect to the number of sentences in the input. The complexity of MEMO is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where n s is the number of samples processed. The Universal Transformer has quadratic complexity. MEMO's spatial complexity is O(I \u00b7 S \u00b7 d), with fixed memory size. ACT is implemented as specified in Graves (2016). The halting unit h is defined accordingly. In our experiments, the size is fixed and we implement ACT following Graves (2016). Our implementation of MEMO defines the halting unit h differently from the original ACT, using \u03c0 t as the binary policy. This change aims to enhance the fairness of comparison by enabling more powerful representations and making it similar to our model. The halting mechanism in MEMO+ACT uses non-linearities for more powerful representations, similar to our model. The halting probability is defined as T = min{t : where , with a fixed value of 0.01. The answer provided by MEMO+ACT is defined as a t corresponding to the answer at hop t. The architecture used is the same as described in Graves et al. (2016), with specific layer sizes detailed in Table 13. The architecture used in MEMO+ACT is similar to the one described in Graves et al. (2016), with specific layer sizes detailed in Table 13. The hyperparameters for training were found through a search, and the implementation used is 'universal_transformer_small' available at https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py. Hyperparameters used for training are described in Table 15."
}