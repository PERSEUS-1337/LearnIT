{
    "title": "rJgUfTEYvH",
    "content": "Generative models for predicting sequences of future events face challenges due to the high uncertainty of future outcomes. Recent works have explored probabilistic models for uncertain futures, but they are computationally expensive or do not optimize data likelihood directly. This work introduces multi-frame video prediction using normalizing flows, enabling direct optimization. This work introduces multi-frame video prediction using normalizing flows, allowing for direct optimization of data likelihood and producing high-quality stochastic predictions. Flow-based generative models offer a competitive approach to generative modeling of video, leveraging exponential progress in computational hardware and advancements in machine learning. Progress in machine learning has led to advancements in image classification, machine translation, and game-playing agents. However, the application of this technology is limited to situations with ample supervision or accurate simulations of the environment. An alternative to supervised learning is using large unlabeled datasets with predictive generative models to build an internal representation of the world for effective prediction of future events, such as physical interactions in video frames. Using large unlabeled datasets, a generative model can learn about real-world phenomena from videos of interactions, enabling effective decision making in applications like robotics. In this paper, the focus is on the challenge of predicting uncertain futures in videos, particularly in the case of conditional video. Existing probabilistic models are either computationally expensive or do not optimize data likelihood directly. The paper addresses the issue of stochastic prediction in videos, specifically focusing on conditional video prediction. It introduces a new class of video prediction models that can generate diverse stochastic futures and synthesize realistic video frames by extending flow-based generative models. Our approach extends flow-based generative models into conditional video prediction, addressing the challenges of modeling high-dimensional video sequences. We learn a latent dynamical system model to predict future values, inducing Markovian dynamics on the latent state of the system. Our approach extends flow-based generative models into conditional video prediction by learning a latent dynamical system model that predicts future values, inducing Markovian dynamics on the latent state of the system. We introduce VideoFlow, an architecture inspired by the Glow model for image generation, which achieves competitive results in stochastic video prediction on the BAIR dataset. VideoFlow is a flow-based generative model for conditional video prediction, achieving competitive results on the BAIR dataset. It produces high-quality results without common artifacts like blurry predictions, and offers faster test-time image synthesis for real-time applications like robotic control. VideoFlow optimizes the likelihood of training videos directly, making it practical and efficient for various applications. VideoFlow is a flow-based generative model for conditional video prediction, focusing on real-time applications like robotic control. Previous research on deterministic predictive models has explored architectural changes and pixel transformations for future video frame prediction. VideoFlow is a flow-based generative model for conditional video prediction, focusing on real-time applications like robotic control. Recent advancements in predictive models include incorporating pixel transformations, predictive coding architectures, different generation objectives, and disentangling representations. The next challenge is to address stochastic environments by building models that can effectively reason over uncertain futures in real-world videos. In real-world videos, stochastic environments present challenges for deterministic models that can only generate one future, leading to blurry predictions. Various methods incorporate stochasticity to overcome this limitation. Various methods incorporate stochasticity to overcome challenges in generating future predictions in real-world videos. These methods include models based on variational auto-encoders, generative adversarial networks, and autoregressive models. Variational autoencoders, in particular, have been widely explored for optimizing log-likelihood. Prior work has focused on maximizing log-likelihood in video prediction models, with auto-regressive models being the primary approach. However, these models are inherently sequential, leading to inefficiencies in synthesis on modern hardware. Efforts have been made to speed up training and synthesis with auto-regressive models, but challenges remain. The proposed VAE model outperforms auto-regressive models in producing better predictions, especially for longer horizons. It exhibits faster sampling, optimizes log-likelihood directly, and generates high-quality long-term predictions. The model utilizes a multi-scale architecture with stochastic variables for improved performance. The flow model in the multi-scale architecture uses stochastic variables for latent variable inference and log-likelihood evaluation. Flow-based generative models offer advantages like exact inference, log-likelihood evaluation, and parallel sampling. The model assumes a tractable prior over latent variables for transformation and composition. The text discusses inferring latent variables through invertible transformations and maximizing log-likelihood over a training set. A generative flow model for video is proposed using a multi-scale flow architecture. The text proposes a generative flow model for video using a multi-scale flow architecture, breaking up the latent space into separate variables per timestep. Each timestep's latent variable is an invertible transformation of a video frame, utilizing a multi-scale architecture for encoding information about each frame. The text describes a multi-scale architecture for encoding information about video frames using invertible transformations. The latent variable z is composed of multiple levels, each encoding information at a particular scale. The chosen transformations have simple Jacobian determinants, such as triangular, diagonal, or permutation matrices. The text discusses the use of simple Jacobian matrices like triangular, diagonal, or permutation matrices in encoding video frame information through invertible transformations. Various techniques such as Actnorm, Coupling, SoftPermute, and Squeeze are applied to manipulate the input data. The text discusses the application of techniques like SoftPermute and Squeeze to manipulate input data in deep networks for encoding video frame information through invertible transformations. Splitting the output of Flow equally across channels enables flows at higher levels to operate on lower dimensions and larger scales. The multi-scale architecture enables flows at different levels to operate on varying dimensions and scales. The architecture is composed of flows at multiple levels to infer latent variables for each video frame. The latent prior is determined using an autoregressive factorization. The architecture includes flows at multiple levels to infer latent variables for each video frame, with a latent prior determined using an autoregressive factorization. The latent prior is specified as a conditional factorized Gaussian density. The architecture involves a conditional factorized Gaussian density modeled by a deep 3-D residual network. The log-likelihood objective has two components: the invertible multi-scale architecture and the latent dynamics model. Parameters are learned by maximizing a joint objective function. The architecture includes a conditional factorized Gaussian density modeled by a deep 3-D residual network. Parameters are learned by maximizing a joint objective function that involves the latent dynamics model and the invertible multi-scale architecture. The realism of generated trajectories is compared using a real-vs-fake 2AFC Amazon Mechanical Turk with SAVP-VAE and SV2P. The VideoFlow model is conditioned with the frame at t = 1 to display generated trajectories at t = 2 and t = 3 for different shapes. Experimentation with 3-D convolutional flows was found to be computationally expensive compared to an autoregressive prior. Due to memory limits, SGD was only feasible with a small number of sequential frames per gradient step. Using 2-D convolutions in VideoFlow with autoregressive priors allows for synthesizing long sequences without introducing temporal artifacts. The generated videos display a blue border for conditioning frames and a red border for generated frames. VideoFlow models the Stochastic Movement Dataset, with the first frame of each video as the conditioning frame. More details and results can be found on the provided website. The Stochastic Movement Dataset is modeled using VideoFlow, where the first frame of each video serves as the conditioning frame. A deterministic model averages out all possible movement directions in pixel space, allowing for accurate prediction of shape position at each time step. The model uses VideoFlow to predict the future trajectory of shapes in videos by looking back at just one frame. It achieves a low bits-per-pixel of 0.04 on the holdout set and consistently predicts the shape's movement in one of eight random directions. Comparisons are made with other stochastic video generation models SV2P. VideoFlow outperforms state-of-the-art stochastic video generation models SV2P and SAVP-VAE in predicting future trajectories of shapes. The quality of generated videos is assessed using a real vs fake Amazon Mechanical Turk test, where VideoFlow consistently generates plausible \"real\" trajectories at a greater rate. VideoFlow outperforms baselines in fooling rate, consistently generating plausible \"real\" trajectories at a higher rate using the action-free BAIR robot pushing dataset. Training baseline models to generate 10 target frames conditioned on 3 input frames, VideoFlow maximizes log-likelihood of Bits-per-pixel VideoFlow 1.87. VideoFlow generates 10 target frames conditioned on 3 input frames, maximizing log-likelihood of Bits-per-pixel. Realism and diversity are measured using a 2AFC test and mean pairwise cosine distance in VGG perceptual space. Models see a total of 13 frames during training. Variational bound of bits-per-pixel on the test set is estimated via importance sampling. VideoFlow outperforms SAVP-VAE and SV2P models on bits-per-pixel, attributing the baselines' high values to their optimization objective. The presence of a \u03b2 = 1 term in their objective and scheduled sampling hinders direct optimization of the variational bound on the log-likelihood. Sampling 100 videos from each stochastic set of conditioning frames on the BAIR action-free is illustrated in Figure 4. VideoFlow outperforms SAVP-VAE and SV2P models on bits-per-pixel, with a focus on stochastic video generation models. 100 videos are sampled from conditioning frames on the BAIR action-free dataset, with the best sample chosen based on PSNR, SSIM, and VGG perceptual metrics. The models are trained with ten target frames and tested to generate 27 frames, aiming for higher accuracy in the generated videos. The BAIR robot-pushing dataset presents high stochasticity and a multitude of plausible futures. Each generated video can be realistic and represent a potential future scenario. The BAIR robot-pushing dataset is stochastic with many plausible futures. Videos generated may be realistic but differ from the ground truth. Evaluation metrics from prior work are used to assess the model's performance. 100 videos are generated from conditioning frames, and the closest to the ground truth is determined using PSNR, SSIM, and cosine similarity metrics. In prior work, evaluation metrics like PSNR, SSIM, and cosine similarity are used to compare generated videos to the ground truth. Tuning pixel-level variance and removing noise improves sample quality and training stability. The procedure involves removing pixel-level noise in the VideoFlow model to enhance video quality at the expense of diversity. Sampling videos at a lower temperature can achieve this, similar to the approach in (Kingma & Dhariwal, 2018). By scaling the standard deviation of the latent gaussian distribution with a temperature factor, frames can be sampled for a network trained with additive coupling layers. Results are reported with temperatures of 1.0 and the optimal temperature determined on the validation set using VGG. The study explores adjusting the temperature factor to scale the standard deviation of the latent gaussian distribution in the VideoFlow model. Results show that using the optimal temperature improves performance, while low-temperature sampling negatively impacts results. Additionally, hyperparameters that lead to disappearing arms perform best on certain metrics. Our model with optimal temperature outperforms SAVP-VAE and SVG-LP models on VGG-based similarity metrics, correlating well with human perception and SSIM. It is also competitive with state-of-the-art video generation models. PSNR is a pixel-level metric, where VAE models excel, while VideoFlow, modeling conditional probability of frame distribution, underperforms on PSNR. Diversity and quality in generated samples are also considered. VideoFlow outperforms diversity values reported in prior work while being competitive in the realism axis. VideoFlow outperforms diversity values reported in prior work (Lee et al., 2018) while being competitive in realism. At T = 0.6, VideoFlow has the highest fooling rate and is on par with state-of-the-art VAE models in diversity. Lower temperatures result in less random arm behavior and clear background objects, leading to higher realism scores. Higher temperatures lead to more stochastic arm motion, increased diversity scores, and noisier background objects, causing a drop in realism. VideoFlow achieves high diversity scores with stochastic arm motion, leading to noisier background objects and a drop in realism. Interpolations between different shapes and frames in the BAIR robot pushing dataset show cohesive motion of the arm between initial and final positions. Using the VideoFlow encoder, interpolations show cohesive motion of the arm between initial and final positions. Multi-level latent representation allows for interpolation of background objects at smaller scales and arm motion at different levels. Shapes with fixed type but varying size and color smoothly interpolate in the latent space. During training, colors of shapes are sampled from a uniform distribution. Interpolated colors in the latent space match those in the training set. Generated videos show frames with and without occlusions. VideoFlow detects plausibility of future frames. Model trained on 13 frames generates 100 future frames. Our VideoFlow model can predict the plausibility of future frames by generating 100 frames into the future based on 13 training frames. The generated frames maintain temporal consistency, but background objects may become noisier and blurrier in the presence of occlusions. The model has a bijection between the latent state and frame, meaning the latent state cannot store additional information beyond what is in the frame. The latent state in our VideoFlow model, represented by z t and x t, can only store information present in the frame x t. This, combined with the Markovian assumption in our latent dynamics, may cause the model to forget objects if they are occluded for a few frames. To address this, we plan to incorporate longer memory in our model, such as using a recurrent neural network in our autoregressive prior or more memory-efficient backpropagation algorithms for invertible neural networks. The VideoFlow model uses neural networks to detect the plausibility of temporally inconsistent frames in the future. By conditioning the model on the first three frames of a test-set video, a distribution is obtained for the 4th frame. The likelihood of each frame in the video occurring as the 4th time-step is computed and a decreasing log-likelihood is assigned to frames further in the future. The VideoFlow model introduces a flow-based architecture for video prediction, inspired by the Glow model for image generation. It utilizes a latent dynamical system model to predict future values, achieving competitive results with state-of-the-art VAE models in stochastic video prediction. Our empirical results show that VideoFlow achieves competitive results in stochastic video prediction, optimizing log-likelihood directly for faster synthesis. Future work includes incorporating memory for long-range dependencies and applying the model to challenging tasks. The dataset consists of 8-bit videos with rescaled dimensions. Our dataset consists of 8-bit videos with rescaled dimensions. We add uniform noise to prevent infinite densities at datapoints, enabling optimization of log-likelihood as KL divergence minimization. By applying low temperature to the latent gaussian priors of SV2P and SAVP-VAE, the performance of VAE models decreases monotonically. The VideoFlow model benefits from low-temperature sampling by balancing noise removal from the background with reduced stochasticity of the robot arm. The VAE models show a slightly blurry background with reduced stochasticity of the arm motion as temperature decreases. Lower bits-per-pixel results in higher quality arm structure modeling by the VideoFlow model. The VideoFlow model learns to model the arm structure and motion with high quality as bits-per-pixel decreases. Hyperparameters include a learning rate schedule, training steps, and latent loss multiplier values. The VideoFlow model is optimized with various hyperparameters such as latent loss multiplier values and learning rate schedules. The model's performance is evaluated by comparing P(X4 = Xt|X<4) and VGG cosine similarity metrics across different timesteps. The VideoFlow model's performance is evaluated by comparing P(X4 = Xt|X<4) and VGG cosine similarity metrics across different timesteps. A weak correlation between VGG perceptual metrics and bits-per-pixel is observed, with a correlation factor of -0.51. Additionally, evaluations with a smaller version of the VideoFlow model show competitive results with SVG-LP on VGG perceptual metrics."
}