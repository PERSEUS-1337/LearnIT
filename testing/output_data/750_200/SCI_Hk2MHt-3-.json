{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored by proposing a reconfiguration of model parameters into parallel branches at the global network level. This approach efficiently reduces the number of parameters while improving performance and introducing additional regularization. The branches are tightly coupled by averaging their log-probabilities to enhance the learning of better representations. The proposed approach involves tighter coupling of parallel branches in deep convolutional networks by averaging their log-probabilities, leading to better representations. This \"coupled ensembles\" architecture can be applied to various neural network structures. Results show improved error rates on CIFAR-10, CIFAR-100, and SVHN tasks compared to training branches independently. For the same parameter budget, DenseNet-BC outperforms with error rates of 3.46%, 17.18%, and 1.8% on CIFAR-100 and SVHN tasks. Using ensembles of coupled DenseNet-BC networks with 50M total parameters, error rates improve to 2.72%, 15.13%, and 1.42% on these tasks. The design of early convolutional architectures involved hyper-parameter choices like filter size and number of filters. VGGNet introduced a template with fixed filter size of 3x3 and N feature maps, downsampling by maxpooling or strided convolutions. The proposed template for convolutional architectures includes a fixed filter size of 3x3 and N feature maps, down-sampling by maxpooling or strided convolutions, and doubling the number of feature maps after each down-sampling operation. This approach, used by models like ResNet and DenseNet, also incorporates skip-connections between non-contiguous layers. Additionally, the template introduces \"coupled ensembling\" where the network is divided into branches, each functioning like a complete CNN. This extension achieves comparable performance to existing state-of-the-art models. The proposed template for convolutional architectures includes a fixed filter size of 3x3 and N feature maps, down-sampling by maxpooling or strided convolutions, and doubling the number of feature maps after each down-sampling operation. This approach incorporates skip-connections between non-contiguous layers and introduces \"coupled ensembling\" where the network is divided into branches, each functioning like a complete CNN. The network achieves comparable performance to existing state-of-the-art models with a significantly lower parameter count. The proposed network architecture includes fixed filter sizes and skip-connections between layers. Coupled ensembling is introduced, improving performance with reduced parameters. The paper discusses related work, introduces coupled ensembles, evaluates the approach, and concludes with future work. The proposed network architecture includes fixed filter sizes and skip-connections between layers. Coupled ensembling is introduced to improve performance with reduced parameters. The approach is compared with state-of-the-art methods, and future work is discussed in section 5. The network architecture has similarities with Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network, but differs in training methods and parameter budget allocation. The multi-branch architecture involves training each branch separately with a fixed parameter budget to improve performance. Activations of the branches are combined by their log-probabilities over target categories. The branches use the same input but different preprocessing blocks. Multi-branch architectures have been successful in various vision applications. Multi-branch architectures have been successful in vision applications. Modifications using grouped convolutions have been proposed for these architectures to improve feature extraction. Our proposal suggests a generic modification at the global model level, different from modifications at the building block level of base architectures like ResNet and Inception. Our proposal suggests a generic modification at the global model level by replicating an \"element block\" as parallel branches to form the final composite model. Shake-Shake regularization BID5 improves performance but requires more epochs for convergence and seems to depend on batch size. Our method proposes a generic rearrangement of architecture parameters for efficient parameter usage, unlike BID26's approach of modifying residual blocks locally. The technique does not require additional choices and leads to convergence with the same hyper-parameters as the base model. Ensembling is a technique to improve model performance by combining outputs of multiple trainings of the same neural network architecture. It leads to better results despite increased training and prediction costs. Our proposed model architecture involves parallel branches trained jointly, similar to ResNet and Inception modules. Arranging parameters into parallel branches improves performance. Classical ensembling can still be used for fusion. Arranging parameters into parallel branches improves performance. Ensembling approach on checkpoints during training process leads to higher performance with same time budget, but increases model size and prediction time. Our approach aims to maintain model size while improving performance. Our approach aims to maintain model size while improving performance by utilizing parallel branches in the model architecture. Each branch takes a data point as input and produces a score vector for target classes. The model comprises several branches, with the number denoted by e. The element blocks used in our experiments include DenseNet-BC and ResNet with pre-activation. CNNs are single-branch models with element blocks like DenseNet-BC and ResNet. The branches are combined using the average of their log probabilities for target classes. The classification task involves assigning samples to one class from a finite set like CIFAR, SVHN, and ILSVRC tasks. Neural network models output a score vector for target classes, followed by a SoftMax layer for probability distribution. Training includes a loss layer like negative log-likelihood. The differences among recent network architectures for image classification lie in the setup before the last FC layer. Regardless of complexity, the resulting \"element block\" always takes an image as input and produces a vector of N values as output. Fusion in ensemble models involves computing individual predictions separately for each model. Fusion in ensemble models involves computing individual predictions separately for each model instance and then averaging them. This is equivalent to predicting with a \"super-network\" that includes the instances as parallel branches with an averaging layer on top. Supernetworks are not commonly implemented due to memory constraints, but the averaging layer can be placed after the last FC layer of the element block. In ensemble models, fusion involves computing individual predictions for each model instance and averaging them. The model consists of parallel branches producing score vectors for target categories, fused through a \"fuse layer\" during training to produce a single prediction. Different options are explored for combining score vectors during training and inference. The model consists of parallel branches producing score vectors for target categories, fused through a \"fuse layer\" during training to produce a single prediction. Three options are explored for combining score vectors during training and inference: Activation average, Probability average, and Log Likelihood average. This transformation leads to improved performance with a lower parameter count in all experiments. Combining branch score vectors by averaging log probabilities of target categories improves performance with lower parameter count in all experiments. The parameter vector W of the composite branched model is the concatenation of parameter vectors W e of element blocks. Evaluation is done on CIFAR-10, CIFAR-100, and SVHN datasets. The datasets CIFAR-10, CIFAR-100, and SVHN consist of training and test images categorized into different classes. The images are all 32x32 pixels in size and normalized during training. Hyperparameters are set according to the original descriptions, without alterations to avoid bias in comparisons. During training on CIFAR datasets, standard data augmentation is used, including random horizontal flips and random crops. For SVHN, no data augmentation is applied, but a dropout ratio of 0.2 is used for DenseNet. Testing is done after normalizing the input in the same way as during training. Error rates are given in percentages and represent an average of the last 10 epochs, which is a more conservative measure than the one used by the DenseNet authors. The execution times were measured using a single NVIDIA 1080Ti GPU with optimal micro-batch 2. Experiments were conducted on the CIFAR-100 dataset with DenseNet-BC, depth L = 100, growth rate k = 12. The proposed branched architecture is compared with an ensemble of independent models. The proposed branched architecture is compared with an ensemble of independent models. Results show that the jointly trained branched configuration outperforms averaging predictions from independent models with the same number of parameters. The proposed branched architecture outperforms independent models with the same number of parameters, showing lower test error (17.61 vs. 18.42). Comparisons with single branch models also demonstrate the effectiveness of the branched configuration, with considerably lower errors (17.61 vs. 20.01) as the number of parameters increase. The proposed branched architecture is more efficient in terms of parameters compared to single branch or multiple independent models. The relation between the number of branches and model performance is analyzed, with experiments conducted to evaluate the best training and prediction fusion combinations for a branched model with e = 4. Experiments were conducted to evaluate the best training and prediction fusion combinations for a branched model with e = 4. Table 1 shows the performance of Coupled Ensembles of DenseNet-BCs with different \"fuse layer\" combinations compared to a single branch model on the CIFAR-100 test set. The architecture evaluated involved a branched model with e = 4. Table 1 displays the performance of models with various \"fuse layer\" choices during training and inference. The results are based on Coupled Ensembles of DenseNet-BCs compared to a single branch model on the CIFAR-100 test set. Table 1 displays the performance of models with different \"fuse layer\" operations for inference. The branched model with e = 4 and Avg. LSM has similar performance to a DenseNet-BC model with significantly more parameters. The error rate of \"element blocks\" trained jointly with LSM fusion is lower than individual instances. The coupling of \"element blocks\" in coupled ensembles with LSM fusion results in lower error rates compared to individual training. Averaging log probabilities helps update all branches consistently, providing a stronger gradient signal for better representations. The error gradient back-propagated from the fuse layer is the same for all branches, leading to complementary actions by all branches. When training with Avg. FC, ensemble combinations outperform single branch networks. Using 4 branches with a parameter budget of 3.2M reduces the error rate significantly. The Avg. FC training with Avg. SM prediction does not perform well due to unrelated FC instances. The Avg. FC prediction works better than Avg. SM prediction due to the non-linearity of the SM layer distorting the FC average. FC values transmit more information than SM probabilities. In this section, the optimal number of branches e for a given model parameter budget is investigated using DenseNet-BC as the \"element block\" on CIFAR-100. The number of instances e may vary depending on the network architecture, parameter budget, and dataset. The number of instances e depends on network architecture, parameter budget, and dataset. Results for different configurations of branches e, depth L, and growth rate k are shown in table 3. Parameter counts for DenseNet-BC are quantified based on L and k values. Model configurations with parameters just below the target were selected for fairness. In the study of DenseNet-BC models with CIFAR-100 and 800k parameters, it was found that using 3 branches with a depth of 70 and growth rate of 9 resulted in a decrease in error rates from 22.87 to 21.10. Additionally, utilizing 2 to 4 branches showed a significant performance improvement over the traditional single branch model. Utilizing 2 to 4 branches in the DenseNet-BC model with CIFAR-100 and 800k parameters led to a decrease in error rates from 22.87 to 21.10. Performance gains were significant compared to the single branch model, with 6 or 8 branches performing worse. The model's performance was robust to variations in parameters, showing the effectiveness of the coupled ensemble approach and DenseNet-BC architecture. The coupled ensemble approach with DenseNet-BC architecture shows robust performance gains compared to other choices, but at the cost of increased training and prediction times. Different models were evaluated, including DenseNet-BC and ResNet BID8, to assess the effectiveness of the coupled ensemble approach. The experiments evaluated ResNet BID8 with pre-activation as the element block to determine the effectiveness of the coupled ensemble approach with different architectures. The results in TAB3 show the performance of coupled ensembles compared to single models, with a focus on ResNet pre-act as the element block. Coupled ensembles with e = 2, 4 show significantly better performance than single branches. Further ensembling involving multiple models is also considered in section 4.7. In section 4.7, ensembling with ResNet pre-act as element block and e = 2, 4 outperforms single branch models. DenseNet-BC architecture is tested with 6 network sizes ranging from 0.8M to 25.6M parameters. The trade-off between depth L and growth rate k is not critical for a given parameter budget. The number of branches e also shows similar results. Our experiments show that the trade-off between depth L and growth rate k is not critical for a given parameter budget. When choosing between the number of branches e, depth L, and growth rate k for a fixed parameter budget, as long as e \u2265 3 (or even e \u2265 2 for small networks), the results are similar. Error rates for single branch DenseNet-BC were higher than reported by BID11, but our Torch7 and PyTorch implementations are equivalent. The coupled ensemble of DenseNet-BC models outperforms DenseNet-BC's reported performance, with error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. The larger models perform better than current state-of-the-art models. The larger models of coupled DenseNet-BCs outperform current state-of-the-art implementations with error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. The Shake-Shake S-S-I model BID5 performs slightly better on CIFAR 10. The coupled ensemble approach is limited by network size and training time. The coupled ensemble approach is constrained by network size and training time, limiting the model to 25M parameters due to GPU memory constraints. To overcome this limitation, a classical ensembling approach based on independent trainings was used to potentially improve performance further. However, the classical approach tends to plateau after a small number of models, with significant improvements seen from 1 to 3 models but diminishing returns from 3 to 16 models. The coupled ensemble approach showed significant improvement from 1 to 3 models but not much from 3 to 16 models. Ensembling four large coupled models resulted in a significant gain by fusing two models, with diminishing returns from further fusion. These ensembles outperformed all state-of-the-art implementations. The proposed approach involves replacing a single deep convolutional network with multiple \"element blocks\" that function as standalone CNN models. These blocks produce intermediate score vectors that are connected via a \"fuse layer\" to improve performance. The ensemble approach showed improvement with 1 to 3 models, but not much from 3 to 16 models, with ensembles outperforming state-of-the-art implementations. The proposed approach involves using multiple \"element blocks\" in a convolutional network, connected via a \"fuse layer\" to improve performance. This ensemble approach leads to significant performance improvement over a single branch configuration, with a small increase in training and prediction times. The best performance for a given parameter budget is achieved with this approach. The proposed approach involves using multiple \"element blocks\" in a convolutional network connected via a \"fuse layer\" to improve performance. It leads to the best performance for a given parameter budget, as shown in tables 3 and 4, and figure 2. The increase in training and prediction times is due to sequential processing of branches, making data parallelism less efficient on GPUs. This issue is less pronounced for larger models. The proposed approach involves using multiple \"element blocks\" in a convolutional network connected via a \"fuse layer\" to improve performance. To address the decrease in data parallelism efficiency on GPUs for larger models, two solutions are suggested: implementing parallel 2D convolutions for branches and spreading branches across multiple GPUs. Preliminary experiments on ImageNet show that coupled ensembles have lower errors compared to single branch models with the same parameter budget. Further experiments will be conducted in the future. The experiments show that coupled ensembles have lower errors compared to single branch models with the same parameter budget. The test version allows for placing the averaging layer after the last FC layer and before the SM layer, while the train version allows for placing the averaging layer after the last FC layer or after the SM layer. In the train version, the averaging layer can be placed after the last FC layer, SM layer, or LL layer. \"Element blocks\" from other groups are reused for efficiency and meaningful comparisons. Each branch is defined by a parameter vector W e, and the global network by a concatenation of all W e vectors. The global network is defined by a parameter vector W which is a concatenation of all the W e parameter vectors. When training and prediction are done separately, a script is used to split the W vector into W e ones. This allows for combining different training and prediction conditions, even if they are not all consistent. The overall network architecture is determined by global hyper-parameters specifying train versus test mode, number of branches, and placement of AVG layer. Larger models may require a batch size larger than 64 for training. For larger models, training with a batch size of 64 may not be possible. In such cases, data batches are split into \"micro-batches\" with b/m elements each, where b is the batch size. The gradient is accumulated over these micro-batches and averaged to approximate the gradient of a single batch, with the exception of BatchNorm layer due to its use of batch statistics for normalization during the forward pass. BatchNorm layer uses micro-batch statistics for normalization during forward pass, which may not make a significant difference in practice. Parameter updates are done using batch gradient, while forward passes are done with micro-batches for optimal throughput. Memory requirement depends on network depth and batch size in single branch cases. In the single branch case, memory requirement depends on network depth and mini-batch size. Using the micro-batch \"trick\" adjusts memory needs without affecting performance. Multi-branch version requires more memory only if branch width is reduced. When using branches with a constant parameter budget, reducing width or depth is necessary. Hyper-parameter search experiments showed that reducing both was the best option. Training with 25M parameters on GTX 1080 Ti used micro-batch sizes of 16 for single-branch and 8 for multi-branch versions. Splitting the network over two GPU boards can double micro-batch sizes but may not significantly increase speed. Using branches with a constant parameter budget requires reducing width or depth. Training with 25M parameters on GTX 1080 Ti used micro-batch sizes of 16 for single-branch and 8 for multi-branch versions. Splitting the network over two GPU boards can double micro-batch sizes but may not significantly increase speed. Coupled ensembles with two branches provide a significant gain over a single-branch architecture of comparable size. TAB8 is an extended version of table 2, evaluating depth L and growth rate k for a fixed parameter count. Performance remains stable with variations in (L, k) compromise. Experiment on CIFAR-100 validation set suggests (L = 82, k = 8, e = 3) as the best combination. The (L = 70, k = 9, e = 3) combination showed slightly better performance compared to (L = 82, k = 8, e = 3) on the test set, but the difference may not be statistically significant. Comparing branched coupled ensembles with model architectures recovered using meta learning techniques raises issues of experiment reproducibility and statistical significance in performance differences. The experiments showed variation in performance measures due to factors like framework used, random seed for network initialization, and CuDNN non-determinism during training. The observed variation is significant even with the same tool and seed. The observed variation in performance measures is significant even with the same tool and seed. Fluctuations in batch normalization can be observed during training, regardless of hyper-parameter settings. The choice between the model obtained after the last epoch or the best performing model impacts the final outcome. The dispersion of evaluation measures due to random initialization in neural networks is expected, but properly designed and trained networks should have similar performance. This small dispersion confirms the hypothesis, with fluctuations in batch normalization observed during training. The choice between the model after the last epoch or the best performing model impacts the final outcome. The dispersion in evaluation measures due to random initialization in neural networks is confirmed to be small, complicating comparisons between methods. Statistical significance tests may not be reliable in this case, as differences can be observed even with the same seed. Experiments in this section provide an estimation of dispersion in a moderate scale model. Experiments in this section estimate the dispersion in a moderate scale model, focusing on DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results for different configurations using Torch7 and PyTorch, as well as the same seed versus different seeds, are presented in table 8. In experiments with DenseNet-BC on CIFAR 100, the performance was evaluated using Torch7 and PyTorch with the same seed or different seeds. Various configurations were tested, including error rates at the last epoch, average error rates over the last 10 epochs, and error rates of the best-performing model. Results were presented for 2x2x3 cases, showing minimum, median, maximum, mean, and standard deviation over 10 runs. The performance of DenseNet-BC on CIFAR 100 was evaluated using Torch7 and PyTorch with the same or different seeds. Results show no significant difference between the implementations or seed usage, indicating difficulty in reproducing exact results. The performance of DenseNet-BC on CIFAR 100 was evaluated with different seeds, showing no significant difference in results. The standard deviation of measures computed on 10 runs is slightly smaller when computed on the last 10 epochs compared to the single last epoch, reducing fluctuations. The mean of measures computed on 10 runs is lower when taken at the best epoch compared to the single last epoch or last 10 epochs, as the minimum is always below the average. This involves using test data for selecting the best model. Proposing a method for ensuring the best reproducibility and fairest comparisons in selecting the best model. Choosing the minimum error rate for all models during training is not realistic or good practice as it introduces bias. When selecting the best model, using the error rate at the last iteration or the 10 last iterations introduces bias. It is recommended to use the error rate at the 10 last iterations for more accurate performance estimation. Additionally, the number of epochs used for training does not significantly impact the results. In CIFAR experiments, the average error rate of models from the last 10 epochs is used for more robust results. For SVHN experiments, the last 4 iterations are used due to fewer epochs. These observations suggest that using the average error rate from the last epochs leads to more reliable outcomes. The study compares single-branch and multi-branch architectures at a constant parameter budget, showing an advantage for multi-branch networks. However, multi-branch networks have longer training times. The research aims to determine if multi-branch architectures can still improve despite the longer training time. The study compares single-branch and multi-branch architectures at a constant parameter budget, showing an advantage for multi-branch networks. Investigating if multi-branch architectures can still improve despite longer training times. Reducing training time options include fewer iterations, fewer parameters, and increased width with reduced depth. Results for CIFAR 10 and 100 are shown based on a single branch DenseNet-BC L = 190, k = 40, e = 1 with a training time of about 80 hours. The study compares single-branch and multi-branch architectures at a constant parameter budget, showing an advantage for multi-branch networks. Results for CIFAR 10 and 100 are shown based on a single branch DenseNet-BC L = 190, k = 40, e = 1 with a training time of about 80 hours. Reducing training time options include fewer iterations, fewer parameters, and increased width with reduced depth. The study compares single-branch and multi-branch architectures at a constant parameter budget, showing an advantage for multi-branch networks. Results for CIFAR 10 and 100 are shown based on different DenseNet-BC configurations with varying parameter counts and training times. Options include reducing depth to 88, matching parameter count and training time with DenseNet-BC L = 76, k = 44, e = 4, and dividing the parameter budget by 2 or 3.7. Despite slight performance differences, all options outperform the single-branch baseline. In this section, the performance of single branch models and coupled ensembles is compared in a low training data scenario. DenseNet-BC L = 88, k = 20, e = 4 outperforms the single-branch baseline with reduced parameter count and training time. The comparison is done on two datasets: STL-10 and a 10K balanced random subset of CIFAR-100. The study compares single-branch models and coupled ensembles on datasets STL-10 and a subset of CIFAR-100. Coupled ensembles show superior performance with a fixed parameter budget. Preliminary experiments on ILSVRC2012 were conducted with resized images due to constraints. Due to memory constraints, experiments were conducted on 256\u00d7256 images with data augmentation involving random flips and crops of size 224\u00d7224. A DenseNet-169-k32-e1 single-branch model was compared with a coupled ensemble DenseNet-121-k30-e2. Results are displayed in table 11, showing the superiority of the coupled ensemble approach. Further experiments with full-sized images and increased data augmentation are planned post-deadline. The current results in table 11 demonstrate that the coupled ensemble approach with two branches significantly outperforms the baseline, even with a constant training time budget. The manuscript will be updated with these results after the deadline."
}