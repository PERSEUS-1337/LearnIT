{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a challenge for neural networks in dynamic data scenarios. A Differentiable Hebbian Consolidation model addresses this by adding a rapid learning plastic component to fixed parameters, enabling efficient learning. Hebbian Plasticity (DHP) Softmax layer adds rapid learning plasticity to fixed parameters, retaining learned representations for longer timescales. Integrated task-specific synaptic consolidation methods penalize changes in slow weights. Evaluated on various benchmarks including Permuted MNIST and Vision Datasets Mixture, with no additional hyperparameters required. Our proposed model for Permuted MNIST introduces an imbalanced variant, outperforming baselines by reducing forgetting. Human intelligence adapts in dynamic environments, a challenge for artificial intelligence. Recent ML advances excel in solving complex tasks through extensive training on large datasets. Most machine learning models excel at solving complex tasks through extensive training on large datasets. However, when deployed in real-world scenarios, they are often exposed to non-stationarity, leading to performance degradation over time due to distributional changes. This phenomenon, known as catastrophic forgetting, presents a significant challenge for deep neural networks engaged in continual learning. Interference poses a crucial problem for deep neural networks engaged in continual learning, where the goal is to adapt to consecutive tasks without forgetting previous ones. This challenges the traditional supervised learning methods that rely on independent and identically distributed samples from a stationary training distribution. For ML systems in real-world applications requiring continual learning, the iid assumption is often violated due to concept drift, imbalanced class distributions, and incomplete data. This leads to the \"stability-plasticity dilemma\" faced by neural networks, presenting a challenge for continual learning where the model must adapt to new scenarios without forgetting previous ones. In continual learning for ML systems, the balance between plasticity and stability is crucial. Synaptic plasticity in biological neural networks is essential for learning and memory. Two major theories explain human continual learning, inspired by synaptic consolidation in the neocortex. The first theory is based on synaptic consolidation in the mammalian neocortex, aiming to preserve important synaptic parameters for previously learned tasks through task-specific updates of synaptic weights. The second theory, the complementary learning system (CLS) theory, suggests that humans store high-level structural information in different brain areas while retaining episodic memories. Recent work on differentiable plasticity has shown that neural networks with \"fast weights\" can be trained end-to-end through backpropagation and stochastic gradient descent to optimize the standard \"slow weights\" and the amount of plasticity in each synaptic connection. These works use slow weights to refer to the weights normally used to train vanilla neural networks, which are updated slowly and are often associated with long-term memory. Recent work on differentiable plasticity has shown that neural networks with \"fast weights\" can be trained end-to-end through backpropagation and stochastic gradient descent to optimize the standard \"slow weights\" and the amount of plasticity in each synaptic connection. These fast weights behave as a form of short-term memory that enable \"reactivation\" of long-term memory traces in the slow weights. Miconi et al. (2018) demonstrated that networks with learned plasticity outperform those with uniform plasticity on various problems. Several approaches have been proposed to address the catastrophic forgetting problem. Differentiable Hebbian Consolidation 1 model extends work on differentiable plasticity to task-incremental continual learning, adapting quickly to changing environments and consolidating previous knowledge by adjusting synapse plasticity based on importance for retaining memories. The Differentiable Hebbian Consolidation 1 model adapts quickly to changing environments and consolidates previous knowledge by adjusting synapse plasticity. It modifies the softmax layer and incorporates plastic weights using Differentiable Hebbian Plasticity. The model can be combined with task-specific synaptic consolidation approaches to overcome catastrophic forgetting. The model integrates consolidation-based approaches like elastic weight consolidation, synaptic intelligence, and memory-aware synapses to prevent catastrophic forgetting. It combines Hebbian plasticity, synaptic consolidation, and CLS theory for rapid adaptation to new data while leveraging compressed episodic memories in the softmax layer. Tested on benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture. The proposed method integrates consolidation-based approaches like elastic weight consolidation, synaptic intelligence, and memory-aware synapses to prevent catastrophic forgetting. It combines Hebbian plasticity, synaptic consolidation, and CLS theory for rapid adaptation to new data while leveraging compressed episodic memories in the softmax layer. Tested on benchmark problems including the Imbalanced Permuted MNIST, it shows that plastic networks with task-specific synaptic consolidation methods outperform networks with uniform plasticity. Hebbian learning theory suggests that memory and learning involve modifying synaptic strength based on correlated activation of neurons. After learning, synaptic strength increases while plasticity decreases to retain knowledge. Incorporating fast weights in meta-learning has shown promising results. Recent approaches in meta-learning have introduced fast weights into neural networks for one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed a model with fast weights in FC layers to bind labels to representations using Hebbian learning-based associative memory. Rae et al. (2018) suggested a Hebbian Softmax layer to improve learning of rare classes by combining Hebbian learning with SGD updates. Miconi et al. (2018) introduced differentiable plasticity in neural networks, optimizing synaptic connections with fast weights that adjust based on activity. While powerful for training, it was mainly tested on recurrent neural networks for pattern memorization tasks. Our work introduces a new method for training neural networks using fast weights in the FC layer, implemented with DHP. We update only the softmax output layer parameters to achieve fast learning and preserve knowledge over time, focusing on overcoming catastrophic forgetting. The work introduces a method using fast weights in the FC layer to train neural networks, focusing on the softmax output layer for fast learning and knowledge retention. Strategies include Task-specific Synaptic Consolidation and CLS Theory to overcome catastrophic forgetting. The curr_chunk discusses regularization strategies inspired by task-specific synaptic consolidation for rapid learning and memory retention in neural networks. These approaches estimate the importance of each parameter or synapse to prevent catastrophic forgetting. Regularization strategies inspired by task-specific synaptic consolidation aim to prevent catastrophic forgetting in neural networks. Plastic synapses retain memories for long timescales, with less importance placed on more plastic synapses. Parameters are updated online or after learning tasks, with a regularizer added to adjust plasticity and prevent changes to important parameters of previously learned tasks. The regularizer includes hyperparameter \u03bb to control forgetting of old memories, with different strategies based on the method used. In Elastic Weight Consolidation (EWC), the importance of each parameter is computed using the diagonal of an approximated Fisher information matrix. An online variant was proposed to improve scalability by controlling the computational cost. Another online method called Synaptic Intelligence was also introduced. Our work is inspired by CLS theory, a computational framework for representing memories with a dual memory system. Memory Aware Synapses (MAS) measures parameter importance by the sensitivity of the learned function to parameter perturbations, different from Synaptic Intelligence (SI) and Elastic Weight Consolidation (EWC). Our work is inspired by CLS theory, a computational framework for representing memories with a dual memory system. Various approaches based on CLS principles involve pseudo-rehearsal, exact or episodic replay, and generative replay methods. Exact replay methods require storing data from previous tasks for later replay, while generative replay methods train a separate model to generate images for replay. iCaRL performs rehearsal. In our work, we focus on neuroplasticity techniques inspired by CLS theory to address catastrophic forgetting. Previous studies have shown how synaptic connections can store long-term knowledge with fixed weights and temporary associative memory with fast-changing weights. Recent research has explored using slow and fast weights in synaptic connections to address catastrophic forgetting during continual learning. This includes replacing soft attention mechanisms with fast weights in RNNs, incorporating Hebbian Softmax layers, augmenting slow weights with fast weights matrices, and implementing differentiable plasticity and neuromodulated differentiable plasticity. Differentiable plasticity methods like matrix, differentiable plasticity, and neuromodulated differentiable plasticity were designed for rapid learning on simple tasks or meta-learning over a distribution of tasks. These methods were not compared against neuroplasticity-inspired CLS methods due to their focus on meta-learning problems. The Hebbian Softmax layer adjusts its parameters for one-shot and few-shot learning by switching between Hebbian and SGD updates based on a scheduling scheme. However, in continual learning setups with frequent exposure to many examples from the same class, the fast weights memory storage effect diminishes as the network learns from numerous examples per class on each task. In continual learning setups, the fast weights memory storage effect diminishes as the network learns from a large number of examples per class. The goal is to metalearn a local learning rule for the fast weights using fixed weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights and a Hebbian plastic component with a scaling parameter \u03b1 and Hebbian trace Hebb. The Hebbian plastic component in the softmax layer includes a scaling parameter \u03b1 and Hebbian trace, accumulating hidden activations for each target label in a mini-batch. Post-synaptic activations are computed using pre-synaptic activations, leading to unnormalized log probabilities z, which are then passed through a softmax function to obtain predicted probabilities \u0177. The softmax function is applied on unnormalized log probabilities to obtain predicted probabilities. The \u03b7 parameter dynamically learns how quickly to acquire new experiences into the plastic component and acts as a learning rate for the plastic connections. Network parameters are optimized by gradient descent during training on different tasks in continual learning. In continual learning, \u03b1, \u03b7, and \u03b8 are optimized by gradient descent as the model is trained on different tasks. The Hebbian weight update is based on the activation levels of neurons and target classes. During continual learning, the model optimizes \u03b1, \u03b7, and \u03b8 through gradient descent while updating Hebbian weights based on neuron activations and target classes. The model accumulates hidden activations directly into softmax output layer weights for better initial representations and long-term retention of learned deep representations. The model optimizes \u03b1, \u03b7, and \u03b8 through gradient descent while updating Hebbian weights based on neuron activations and target classes. This results in better initial representations and long-term retention of learned deep representations. Fast learning with a highly plastic weight component improves test accuracy, while selective consolidation into a stable component protects old memories. The DHP Softmax model enables learning to remember by modeling plasticity over various timescales, forming a neural memory without the need for additional space or computation. This allows for easy scalability with increasing tasks. A Hebbian update for class c = 6 is illustrated, where hidden activations are averaged into one vector for the update process. The DHP Softmax model facilitates learning and memory retention by incorporating plasticity over different timescales. Hidden activations corresponding to class c = 6 are averaged into a single vector, denoted by h, to quickly store memory traces for recent experiences without interference. This compressed episodic memory formation reflects individual memory traces, similar to the hippocampus in biological neural networks. The DHP Softmax model improves learning by forming compressed episodic memory traces, similar to the hippocampus in biological neural networks. This method speeds up binding of class labels without adding hyperparameters. Hebbian Synaptic Consolidation regularizes loss and updates parameters. Incorporating strategies like EWC, Online EWC, SI, and MAS, the loss L(\u03b8) is regularized and synaptic importance parameters are updated online. The quadratic loss for Hebbian Synaptic Consolidation is derived, focusing on the network parameters \u03b8 i,j as weights between pre-and post-synaptic activities. Task-specific consolidation approaches are adapted without computing synaptic importance on the plastic component, only regularizing the slow weights of the network. The plastic component of the network's softmax layer helps prevent catastrophic forgetting by optimizing the connections' plasticity. Task-specific consolidation methods do not compute synaptic importance on the plastic component, only regularizing the slow weights of the network. SI is the only method that estimates synaptic importance parameters while training, unlike Online EWC and MAS which compute them after learning a task. Our model's softmax layer alleviates catastrophic forgetting by optimizing connection plasticity. Comparing our approach to other methods like Online EWC, SI, and MAS, we increase DNN capacity with plastic weights. Adding slow weights to the softmax layer shows that increased model capacity is not the sole factor in mitigating forgetting during sequential tasks. Our model's softmax layer alleviates catastrophic forgetting by optimizing connection plasticity. We tested our model on various benchmarks to evaluate memory retention and flexibility, focusing on test performance on the first and most recent tasks. The study focuses on evaluating memory retention and flexibility of a model by analyzing test performance on the first and most recent tasks. It also measures forgetting using the backward transfer metric, which indicates the impact of learning new tasks on previous tasks. In a benchmark study, neural networks were trained with Online EWC, SI, and MAS consolidation methods on sequential tasks. MNIST pixels were permuted differently for each task, with changing input distributions. Hyperparameters and details can be found in Appendix A. In the Permuted MNIST and Imbalanced Permuted MNIST benchmarks, a multi-layered perceptron network with two hidden layers was used. The plastic component's \u03b7 value was set to 0.001 without much tuning effort. Performance comparison was made between the network with DHP Softmax. The study compared the performance of a network with DHP Softmax to a fine-tuned vanilla MLP network called Finetune. The network with DHP Softmax showed improvement in preventing catastrophic forgetting across all tasks. Additionally, the performance with and without DHP Softmax using task-specific consolidation methods was compared, showing improved test accuracy with DHP Softmax. The study compared the performance of a network with DHP Softmax to a fine-tuned vanilla MLP network called Finetune, showing improvement in preventing catastrophic forgetting. DHP Softmax with consolidation maintained higher test accuracy throughout sequential training of tasks. An ablation study examined network parameters and Hebb traces for interpretability. During training on Permuted MNIST benchmark, \u03b7 increases rapidly in task T1, then decays after the 3rd task to reduce plasticity and prevent interference. Within each task from T4 to T10, \u03b7 initially increases then decays. The Hebb trace maintains memory of synapses contributing to recent activity. The plasticity coefficients in Hebb's model grow within each task, leveraging the structure in the plastic component. Gradient descent and backpropagation are used for meta-learning to tune the structural parameters. The Imbalanced Permuted MNIST problem introduces imbalanced distributions in each task. The Imbalanced Permuted MNIST benchmark introduces imbalanced class distributions in each task. DHP Softmax achieves 80.85% accuracy after learning 10 tasks sequentially, showing a significant 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in DHP Softmax with MAS allows rare classes to be remembered longer, resulting in a 0.04 decrease in BWT and an average test accuracy of 88.80%. This outperforms all other methods and tasks, achieving a 1.48% improvement over MAS alone. The original MNIST dataset is split into a sequence of 5 binary tasks. The MNIST dataset is split into 5 binary classification tasks with disjoint output spaces. An MLP network with two hidden layers is used, and the initial \u03b7 value is set to 0.001. Different values of \u03b7 yield similar final test performance after learning the tasks. Combining DHP Softmax with task-specific consolidation consistently improves test performance across all tasks, especially the most recent one, T5. Continual learning is performed on a sequence of 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN. DHP Softmax alone achieves 98.23% accuracy, a 7.80% improvement compared to a finetuned MLP network. In a study by Ritter et al. (2018) and Zeno et al. (2018), continual learning is conducted on a sequence of 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The datasets are adjusted to match resolution and a CNN architecture similar to previous studies is used. The network is trained with mini-batches of size 32 using plain SGD optimization with an initial \u03b7 parameter value of 0.0001. The study conducted continual learning on 5 vision datasets using a CNN architecture. The network was trained with mini-batches of size 32 and optimized with plain SGD. Results showed that DHP Softmax plus MAS decreased BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. SI with DHP Softmax outperformed other methods with an average test performance of 81.75% and a BWT of -0.04 after learning all tasks. The study demonstrated that adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. This allows new information to be learned without interference, resulting in improved test performance. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax automatically scales the magnitude of plastic connections, allowing for efficient learning without interference. This model showed significant improvement across benchmarks compared to traditional softmax layers. The neural network with DHP Softmax, which does not introduce additional hyperparameters, showed noticeable improvement compared to traditional softmax layers. The model demonstrated flexibility by incorporating Hebbian Synaptic Consolidation. The model with DHP Softmax showed improvement and flexibility by incorporating Hebbian Synaptic Consolidation. Different consolidation methods like EWC, SI, and MAS were used to alleviate catastrophic forgetting after learning multiple tasks. DHP Softmax combined with SI outperformed other methods on certain datasets, while combining DHP Softmax with MAS consistently led to superior results on other benchmarks. The local variant of MAS computes synaptic importance parameters based on Hebb's rule, leading to superior results on benchmarks. The model consistently exhibits lower negative BWT, indicating that Hebbian plasticity enables continual learning and memory retention in neural networks. Hebbian plasticity improves test accuracy and reduces catastrophic forgetting in neural networks during continual learning from sequential datasets. It enables learning from limited labeled data and scaling at long timescales, suggesting its potential for gradient descent optimized Hebbian consolidation in deep neural networks. Hebbian consolidation in DNNs enables continual learning by training on a sequence of tasks with task-specific loss for each task. After Hebbian consolidation in DNNs enables continual learning, each task has its own task-specific loss and regularizer to prevent forgetting. The model learns an approximated mapping to the true underlying function, mapping new inputs to target outputs for all tasks learned. Classes in each task can vary, as seen in SplitMNIST and Vision Datasets Mixture benchmarks. Experiments were conducted on Nvidia Titan V. Hyperparameters for training on a sequence of tasks include mini-batches of size 64, plain SGD optimization with a learning rate of 0.01, and early stopping after 10 epochs. If validation error does not improve for 5 epochs, training on the task is terminated, weights and Hebbian traces are reset, and the next task is started. For the Permuted MNIST experiments, hyperparameters such as regularization values for task-specific consolidation methods were set. Lambda values were specified for Online EWC, SI, and MAS methods. In SI, lambda corresponds to the damping parameter \u03be. In the Permuted MNIST experiments, hyperparameters were set for task-specific consolidation methods. Lambda values were specified for Online EWC, SI, and MAS methods. For each task, training samples were artificially removed based on random probabilities. In the Permuted MNIST experiments, training samples from each class were removed based on random probabilities for each task. The distribution of classes in each imbalanced dataset for tasks 1 to 10 is shown in Table 2. In the Imbalanced Permuted MNIST experiments, a total of 30609, 34515, 26587, 31120, 31781, 32000, 28714, 35622, 26012, and 25860 data points were used for tasks 1 to 10 respectively. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters \u03bb were set to 400 for Online EWC, 1.0 for SI, and 0.1 for MAS. In SI, the damping parameter \u03be was set to 0.1. Hyperparameter combinations were found through grid search using a task sequence determined by a single seed. For synaptic consolidation methods, a grid search was performed using specific task sequences and hyperparameters were set accordingly. In Split MNIST experiments, regularization hyperparameters were \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. For Online EWC, SI, and MAS synaptic consolidation methods, hyperparameters were optimized through a grid search using a 5-task binary classification sequence. Lambda values were set to 400, 1.0, and 1.5 respectively. Training was done on a sequence of tasks with mini-batches of size 64 using plain SGD with a fixed learning rate of 0.01. The Vision Datasets Mixture benchmark consists of a sequence of 5 tasks with image classification datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The notMNIST dataset includes font glyphs for letters 'A' to 'J' with 60,000 training and 10,000 testing grayscale images of size 28\u00d728. FashionMNIST has 10 categories. MAS hyperparameters range from 0.01 to 5.0, trained with mini-batches of 64 using plain SGD with a fixed learning rate of 0.01 for 10 epochs. The CNN architecture for the Vision Datasets Mixture benchmark includes 2 convolutional layers with 20 and 50 channels, and kernel sizes. FashionMNIST, SVHN, and CIFAR-10 datasets consist of grayscale and color images of various sizes for training and testing. The CNN architecture for the Vision Datasets Mixture benchmark includes 2 convolutional layers with 20 and 50 channels, LeakyReLU nonlinearities, max-pooling operations, and a final softmax output layer. A multi-headed approach was used due to different class definitions between datasets, with a trainable \u03b7 value for this benchmark. In the Vision Datasets Mixture benchmark, a trainable \u03b7 value is assigned to each connection in the final output layer, improving stability of optimization and convergence. Using separate \u03b7 parameters for each connection allows for modulation of plasticity rates, leading to better performance on tasks like SVHN and CIFAR-10. In the Vision Datasets Mixture benchmark, different hyperparameters are used for task-specific consolidation methods such as Online EWC, SI, and MAS. A random search was conducted to find the best hyperparameter combination for each method. In a random search for synaptic consolidation methods, various hyperparameters were tested for Online EWC, SI, and MAS. Sensitivity analysis was conducted on the Hebb decay term \u03b7, showing its impact on test performance after learning tasks sequentially. Plots in Figure 4 illustrate the effect of initial \u03b7 values on test performance for Permuted MNIST and Imbalanced Permuted MNIST benchmarks. Setting low values for the initial \u03b7 parameter led to the best performance in alleviating catastrophic forgetting during sequential learning tasks for Permuted MNIST and Imbalanced Permuted MNIST benchmarks. Sensitivity analysis was also performed on the \u03b7 parameter for the Split MNIST problem. Table 4 shows the average test accuracy across 5 trials for MNIST-variant benchmarks, corresponding to the sensitivity analysis plots in Figure 4. Table 4 displays the average test accuracy for MNIST-variant benchmarks, corresponding to sensitivity analysis plots in Figure 4. The code snippet includes parameters for learning rate and Hebbian traces. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to the final output layer of a neural network through plastic connections. It includes parameters for learning rate and Hebbian traces. The DHP Softmax model, implemented in PyTorch, incorporates compressed episodic memory into the final output layer of a neural network through plastic connections. It outperforms Finetune on tasks in a class-incremental setting, as demonstrated on the CIFAR-10 and CIFAR-100 datasets. The DHP Softmax model outperforms Finetune in a class-incremental learning setup, with test accuracies surpassing training from scratch on some tasks. Test accuracies of Finetune, training from scratch, and SI were referenced from von Oswald et al. (2019)."
}