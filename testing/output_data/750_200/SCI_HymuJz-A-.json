{
    "title": "HymuJz-A-",
    "content": "The text discusses the limitations of modern machine vision algorithms in learning visual relations, particularly when faced with high intra-class variability. It highlights how convolutional neural networks struggle with visual-relation problems and break down when rote memorization is not feasible. Additionally, it introduces relational networks as a potential solution for solving complex visual question answering tasks. Feedback mechanisms, including working memory and attention, are argued to be key components for abstract visual reasoning, as shown by the limitations of relational networks in solving visual question answering problems. The neural network CNN BID13 achieved remarkable accuracy in categorizing complex images after training on millions of photographs. It surpassed human observer accuracy on the ImageNet classification challenge. In contrast, a simple binary image with two curves was easily distinguishable to the human eye. The CNN failed to learn the relation between two curves in a binary image, despite being able to accurately detect a flute in another image. This raises questions about the algorithm's ability to understand simple concepts like \"sameness.\" Contemporary computer vision algorithms struggle to learn the concept of \"sameness\" as shown in an image with two curves from the SVRT challenge BID6. This difficulty has been overshadowed by the success of relational networks (RNs) on visual question answering benchmarks. Relational networks (RNs) have shown success on visual question answering benchmarks but have only been tested on toy datasets. However, they face limitations similar to CNNs for tasks like recognizing visual relations. This failure is surprising considering the ability of animals to recognize visual relations. The text discusses the limitations of existing machine learning algorithms in understanding visual relations across different species in the animal kingdom. Previous studies have shown that black-box classifiers struggle with visual reasoning tasks, despite extensive training data. This highlights the need for a systematic exploration of the capabilities of contemporary machine learning algorithms on relational reasoning problems. The text discusses the limitations of machine learning algorithms in understanding visual relations. Previous studies have shown that black-box classifiers struggle with visual reasoning tasks, highlighting the need for exploration of contemporary algorithms on relational reasoning problems. The text explores the limitations of CNNs and other visual reasoning networks on visual-relation tasks, questioning whether these limitations stem from poor hyperparameter choices or a fundamental flaw in feedforward models. Through experiments, it is shown that CNNs struggle with visual-relation tasks, and even specialized networks like RNs do not alleviate these limitations. The study also suggests that key brain mechanisms like working memory and attention play a role in visual reasoning. The text discusses the limitations of CNNs and other visual reasoning networks on visual-relation tasks, suggesting that key brain mechanisms like working memory and attention are needed to improve performance. The study includes a systematic analysis of CNN architectures on various visual reasoning problems, highlighting a distinction between hard same-different problems and easy spatial-relation problems. The text discusses limitations of CNNs and other visual reasoning networks on visual-relation tasks, proposing the need for brain mechanisms like working memory and attention to enhance performance. It includes a systematic analysis of CNN architectures on different visual reasoning problems, emphasizing the distinction between hard same-different problems and easy spatial-relation problems. The text also introduces a novel visual-relation challenge that demonstrates CNNs solve same-different tasks through rote memorization and suggests modifications to existing challenges to improve visual reasoning architectures. The SVRT challenge consists of twenty-three binary classification problems involving abstract rules for stimuli depicted as simple black curves on a white background. Positive examples in the challenges feature specific patterns, such as identical items up to translation or a larger item between two smaller ones. The SVRT challenge includes twenty-three binary classification problems with abstract rules depicted as black curves on a white background. Nine CNNs were trained on each problem, with lower accuracies on same-different compared to spatial-relation problems. CNNs from high-throughput analysis showed lower accuracies on same-different compared to spatial-relation problems. Testing involved CNNs of different depths and receptive field sizes, with pooling layers using ReLu activations. The study involved training nine networks on twenty-three problems using CNNs with specific parameters. The networks were trained using an ADAM optimizer with a base learning rate, and the best-case performance for each problem was shown in a figure. The study involved training nine networks on twenty-three problems using CNNs with specific parameters. The best networks for each problem were obtained and sorted by accuracy. Problems were colored red or blue based on their SVRT descriptions. Same-Different (SD) problems have congruent items, while Spatial-Relation (SR) problems involve phrases like \"left of\" or \"next to\". The dichotomy across the SVRT problems is evident from the results. The study found that CNNs perform much worse on Same-Different (SD) problems compared to Spatial-Relation (SR) problems. Some SD problems resulted in accuracy not much better than chance, indicating that SD tasks are particularly challenging for CNNs. This aligns with earlier evidence of a visual-relation dichotomy. Additionally, hyperparameter search showed that SR problems are generally learned equally well. Experiment 1 confirms previous findings that feedforward models struggle with visual-relation problems. Larger networks perform better on SD problems, while SR problems are equally well-learned across all network configurations. The study confirms that feedforward models struggle with visual-relation problems, with larger networks performing better on some problems. The SVRT challenge has limitations in its sample size and representation of visual relations. The study shows that feedforward models struggle with visual-relation problems, with larger networks performing better on some tasks. Sample PSVRT images depict different joint categories of SD and SR, with images categorized as Same or Different and Horizontal or Vertical based on specific criteria. The study demonstrates that feedforward models face challenges with visual-relation problems, with larger networks showing improved performance on certain tasks. PSVRT images illustrate various joint categories of Same-Different (SD) and Same-Relation (SR), categorized based on specific criteria such as size and position. Different problems require unique image structures and generation methods, making direct comparisons challenging. For instance, Problem 2 necessitates one large and one small object in an image, conflicting with Problem 1 where identical-sized items must be positioned without containment. The study highlights challenges faced by feedforward models in visual-relation problems, with larger networks showing better performance. Problems in SVRT images involve different object sizes and positions, making direct comparisons difficult. Using closed curves in image generation limits control over variability. The study addresses challenges in visual-relation problems by controlling image variability. A new dataset with two problems, Spatial Relations (SR) and Same-Different (SD), was created to tackle issues in SVRT. In SVRT, a new dataset was created with two problems: Spatial Relations (SR) and Same-Different (SD). The image generator produces gray-scale images using square binary bit patterns. The image generator in SVRT creates gray-scale images with square binary bit patterns, controlled by parameters like item size, image size, and number of items. The image generator in SVRT creates gray-scale images with square binary bit patterns, controlled by parameters like item size, image size, and number of items. It controls image variability by setting the spatial extent of item placement. The number of items (k) influences both item and spatial variability. When k \u2265 3, the SD category label is determined by the presence of at least 2 identical items, and the SR category label is determined by the average orientation of item displacements. These parameters quantify the number of possible images. The Parametric SVRT test, or PSVRT, quantifies the number of possible images in a dataset based on the average orientation of displacements between items. Each image is generated by sampling joint class labels for SD and SR, and the first item is sampled from a uniform distribution. If the SD label is Same, between 1 and k-1 identical copies of the first item are created. In PSVRT, images are generated by sampling joint class labels for SD and SR. The first item is sampled from a uniform distribution, and depending on the SD label (Same or Different), identical copies may be created. The remaining unique items are consecutively sampled and placed in an image with background spacing. This ensures identical image distribution between problem types. In this experiment, the distribution of images is the same for both same-different and spatial-relation PSVRT problems. The goal was to assess the difficulty of learning these problems with varying image parameters. A baseline architecture was able to learn both problem types easily for specific parameter values. Training sessions were conducted for different parameter combinations, measuring the number of training examples needed for the architecture to reach 95% accuracy (TTA). In each training session, the number of training examples needed for the architecture to reach 95% accuracy was measured. TTA was used as a measure of problem difficulty. The network was trained from scratch in all conditions without a holdout test set. Different image parameters were varied separately to examine their effect on learnability in three sub-experiments. The study conducted three sub-experiments varying parameters n, m, and k while training a baseline CNN from scratch with 20 million images. The CNN had four convolution and pool layers with different kernel sizes. The best-case result for each condition was reported based on 10 random initializations. The baseline convolutional network had four convolution and pool layers with varying kernel sizes. Fully-connected layers had 256 units each with dropout in the last layer. ADAM optimizer with a learning rate of 10^-4 was used, and weights were initialized with the Xavier method. The effect of network size on learnability was also examined. The study examined the effect of network size on learnability using a larger network with 2 times the units in convolution layers and 4 times in fully-connected layers. A strong dichotomy in learning curves was observed, with a sudden rise in accuracy from chance-level to 100% termed as the \"learning event\". The study observed a sudden rise in accuracy from chance-level to 100%, termed as the \"learning event\". Training runs that exhibited this event almost always reached 95% accuracy within 20 million images, showing a strong bi-modality in final accuracy. In different experimental conditions, a strong bi-modality was observed, with accuracy levels ranging from chance-level to close to 100%. In SR, no straining effect was found across all image parameters. However, in SD, a significant straining effect was noted from image size and number of items, leading to a decreased likelihood of the learning event occurring. The network learned SD in 7 out of 10 random initializations for the baseline parameter configuration. Increasing image size and the number of items in the image negatively impacted the network's ability to learn the problem. The network learned the problem in 7 out of 10 random initializations for the baseline parameter configuration, but only in 4 out of 10 on 120 \u00d7 120 images. The network never learned the problem with image sizes of 150 \u00d7 150 and above, or when there were 3 or more items in an image. Rotation by a multiple of 90\u00b0 was considered the same for items in the experiment. The relaxation of the strict same-different rule by considering items to be the same if congruent up to a 90\u00b0 rotation quadrupled the number of matching images in the dataset. However, this imposed a severe strain on CNNs, hindering their ability to learn. The increase in image size and item number contributes to image variability exponentially. Increasing image size while keeping the number of items constant results in a quadratic-rate increase in image variability, while increasing the number of items leads to an exponential-rate increase. The strain on CNNs is significant, even with a twofold difference in network width, causing a constant rightward shift in the TTA curve over image sizes. The transition to the problem becoming essentially impossible is only delayed by one step in the image size parameter. Increasing item size did not have a visible straining effect on CNNs, and learnability remained stable across different item sizes. It is possible to create feedforward feature detectors that can generalize to coordinated item variability, such as \"subtraction templates,\" although more direct methods are still being explored. When CNNs learn a PSVRT condition, they build a feature set tailored for a specific dataset rather than learning the rule itself. These features should be minimally sensitive to irrelevant image variations. The CNN in the experiment did not learn well with increasing image variability, suggesting that it did not learn invariant rule-detectors. The Relational Network (RN) was proposed as an architecture designed to detect visual relations and was tested on various VQA tasks. The Relational Network (RN) is an architecture designed to detect visual relations and outperformed a baseline CNN on various visual reasoning problems, including the \"sort-of-CLEVR\" VQA task using images. The RN sits on top of a CNN and learns a map from pairs of high-level CNN feature vectors to answers to relational questions, which can be provided as natural language processed by LSTM or hardcoded binary strings. The entire system (CNN+LSTM+RN) can be trained end-to-end. The Relational Network (RN) outperformed a baseline CNN on visual reasoning problems, including the \"sort-of-CLEVR\" VQA task. The RN was trained to answer relational and non-relational questions about scenes with simple 2D items. The sort-of-CLEVR tasks have shortcomings in requiring comparisons of cued items without learning the concept of sameness. Low item variability leads to reliance on rote memorization. To measure RN performance without these handicaps, the model was trained on a two-item sort-of-CLEVR same-different task and PSVRT stimuli. The model was trained on a two-item sort-of-CLEVR same-different task and PSVRT stimuli to measure its ability to transfer the concept of same-different. Architecture details include a convolutional network with four layers and kernel sizes of 5x5. The model's convolutional network had four layers with 5x5 kernel sizes and ReLu activations. The RN part included a 4-layer MLP with 256 units per layer and a 3-layer MLP with 256 units per layer. The system used ReLu activations, 50% dropout in the penultimate layer, and a softmax function in the final layer. Training was done with cross-entropy loss and an ADAM optimizer with a base learning rate of 2.5 x 10^-4. The final layer output was processed with a softmax function and trained using cross-entropy loss with an ADAM optimizer at a base learning rate of 2.5 x 10^-4. Weights were initialized using Xavier initialization. The model replicated results from BID22 on the sort-of-CLEVR task by constructing twelve versions of the dataset, each missing a different color+shape combination. Each dataset depicted two items, sometimes with the same color and shape. The CNN+RN architecture was trained on datasets with missing color+shape combinations, reaching 95% training accuracy. However, it did not generalize well to left-out color+shape combinations on the sort-of-CLEVR task. The CNN+RN model trained on datasets with missing color+shape combinations but struggled to generalize to left-out combinations on the sort-of-CLEVR task. The model learned quickly but did not transfer the same-different ability to the left-out condition during validation. The CNN+RN model struggled to generalize to left-out color+shape combinations on the sort-of-CLEVR task, even though the attributes were represented in the training set. The model did not transfer the same-different ability to the left-out condition during validation. In Experiment 2, the CNN+RN model showed improved performance on the sort-of-CLEVR task when using PSVRT bit patterns instead of simple shapes. Training on 20M images and varying image size from 30 to 180 pixels, the model achieved over 95% accuracy for image sizes of 120 or below. However, for image sizes of 150 and 180, the system did not learn, suggesting a limit to the RN architecture's representational capacity. The system did not learn for image sizes of 150 and 180, indicating a limit to the RN architecture's representational capacity for some same-different tasks on PSVRT. Visual-relation problems can exceed the capacity of CNNs due to the combinatorial explosion in the number of templates needed for arrangements of objects. Learning templates for arrangements of objects become rapidly intractable due to the combinatorial explosion in the number of templates needed. While biological visual systems excel at detecting relations, feedforward networks struggle with stimuli that have a combinatorial structure. This limitation has been overlooked by current computer vision scientists, despite cognitive scientists acknowledging it early on. Humans have been found capable of learning complex visual rules and generalizing them to new instances with just a few training examples. Humans can learn complicated visual rules and generalize them with just a few examples, even for challenging problems like problem 20 in SVRT. This problem involves shapes that can be reflected around the perpendicular bisector of the line joining their centers. In contrast, even after a million training examples, the best performing network could not surpass chance levels. Visual reasoning abilities are not exclusive to humans, as seen in birds. Visual reasoning ability is not limited to humans, as birds and primates have shown the ability to recognize same-different relations and transfer knowledge to novel objects. Ducklings have been shown to perform a one-shot version of a visual learning experiment from birth, demonstrating a preference for novel objects based on training. Ducklings can rapidly learn abstract concepts of same and different from a single example or possess these concepts innately. In contrast, CNN+RN in Experiment 3 struggled to transfer the concept to novel objects even after extensive training. This highlights the unique learning abilities of ducklings compared to artificial intelligence models. There is evidence that visual-relation detection in the brain may rely on feedback signals beyond feedforward processes, despite the ability to perform certain visual recognition tasks with minimal cortical feedback. The detection of natural object categories can occur with minimal cortical feedback, but object localization in clutter requires attention and processing of spatial relations between objects. The processing of spatial relations in cluttered scenes requires attention, even when objects can be detected pre-attentively. Working memory plays a role in visual relation processing, particularly in tasks involving spatial reasoning. The computational role of attention and working memory in detecting visual relations involves constructing flexible representations dynamically through attention shifts, rather than storing templates statically. This approach helps prevent capacity overload by avoiding the need to store templates for all possible relations. Representations built \"on-the-fly\" help prevent capacity overload in neural networks by avoiding the need to store templates for all possible relations. Humans excel at detecting visual relations and effortlessly construct structured descriptions about the visual world. Attentional and mnemonic mechanisms are crucial for computational understanding of visual processing. The exploration of attentional and mnemonic mechanisms is crucial for computational understanding of visual reasoning, surpassing modern computers in detecting visual relations."
}