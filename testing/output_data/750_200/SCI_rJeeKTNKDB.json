{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations and utilizing a fully autoregressive graph decoder. The model is evaluated on multiple molecular optimization tasks with promising results. Our model significantly outperforms previous baselines in molecular optimization tasks by adding new substructures and resolving their attachment to the emerging molecule. This task involves modifying compounds to improve their biochemical properties through graph-to-graph translation. The model is trained to translate input molecular graphs into better forms, facing challenges due to the vast candidate space and diverse molecular properties. The model is trained to translate input molecular graphs into better forms, facing challenges due to the vast candidate space and complex molecular properties. Graph generation is computationally challenging due to complex dependencies, similar to machine translation. Success in this task relies on the inductive biases in the encoder-decoder architecture. Prior work proposed a junction tree encoder-decoder using valid chemical substructures to generate molecular graphs. The previous work proposed a junction tree encoder-decoder using valid chemical substructures to generate molecular graphs. The approach involved representing each molecule as a junction tree over chemical substructures in addition to the original atom-level graph. However, the encoding and decoding were carried out separately, limiting the impact of predicted attachments on subsequent substructure choices. The proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation improves upon the previous junction tree encoder-decoder by interleaving the prediction of substructure components with their attachments to the molecule being generated. This non-autoregressive process allows for consistent substructure attachments across different nodes in the junction tree. The encoder-decoder model for graph generation predicts substructure components and their attachments to the molecule, enabling strong dependencies between successive attachments and substructure choices. The encoder represents molecules at different resolutions, with graph convolution supporting attachment predictions at the lowest level and substructure predictions at the highest level. Our decoding process efficiently decomposes generation steps into smaller hierarchical steps to avoid combinatorial explosion. It can handle conditional translation by taking desired criteria as input, allowing for different criteria combinations at test time. The model's tree and graph decoders are isolated, leading to the generation of invalid junction trees that cannot be assembled. The model's decoding process efficiently handles different criteria combinations at test time by interleaving tree and graph decoding steps to avoid generating invalid junction trees. Additionally, an autoregressive decoder is proposed to address inconsistent local substructure attachments during training, improving performance on molecular optimization tasks. Our new model outperforms previous graph generation methods in discovering molecules with desired properties, showing significant improvements on QED and DRD2 optimization tasks. It runs 6.3 times faster during decoding and utilizes hierarchical decoding and multi-resolution encoding for better performance. Our new model outperforms previous methods in molecular graph generation by utilizing hierarchical decoding and multi-resolution encoding, achieving faster decoding times. Various approaches have been used in previous work for generating molecular graphs based on SMILES strings. Various approaches have been used in previous work for generating molecules based on SMILES strings, including generative models that output adjacency matrices and node labels, sequential node-by-node decoding models, and hypergraph grammar methods. Our new model improves molecular graph generation through hierarchical decoding and multi-resolution encoding, leading to faster decoding times. Kajino (2018) developed a hypergraph grammar method for molecule generation, closely related to Jin et al. (2018). Jin et al. used a two-stage procedure for generating molecules based on substructures, creating a junction tree in the first step and resolving the full graph in the second step. However, their approach introduced local independence assumptions and applied the steps stage-wise during decoding. The decoder in the second step is not autoregressive, introducing local independence assumptions. Our method predicts substructures and attachments jointly with an autoregressive decoder, related to graph encoders for molecules. Our method represents molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees, differentiating it from other graph encoders for molecules. It is closely related to approaches that learn to represent graphs hierarchically using graph coarsening algorithms. Defferrard et al. (2016) and Ying et al. (2018) used graph coarsening algorithms to create multiple graph hierarchy layers. Gao & Ji (2019) suggested learning graph hierarchy during encoding. These methods aim to represent graphs as a single vector for regression. Gao & Ji (2019) proposed learning the graph hierarchy jointly with the encoding process for graph generation. Unlike previous methods focusing on representing graphs as a single vector, this approach encodes molecules into multiple sets of vectors at different resolutions. These vectors are dynamically aggregated by decoder attention modules in each graph generation step to translate molecules into ones with improved chemical properties. The function F, parameterized as an encoder-decoder with neural attention, is used for the graph translation task. Both the encoder and decoder are illustrated in Figure 2. In each generation step, the decoder of molecule X transforms it into molecule G with better chemical properties using an encoder-decoder with neural attention. The attachment prediction involves predicting attaching points in the new substructure and their corresponding attaching points in the current graph. To support hierarchical generation, a matching encoder is designed to represent molecules at multiple resolutions for each decoding step. To support hierarchical generation, a matching encoder is designed to represent molecules at multiple resolutions. The molecule X is represented by a hierarchical graph with substructure, attachment, and atom layers. The model encodes nodes into substructure, attachment, and atom vectors for decoding. The encoder represents molecules at multiple resolutions using substructure, attachment, and atom layers. The decoder utilizes substructures extracted from the molecule to make predictions. The paper discusses extracting substructures from molecules to characterize their connections, constructing a substructure tree with nodes representing substructures, and utilizing a vocabulary of substructures with high coverage on test sets. The paper discusses constructing a tree of substructures from molecules, where nodes represent substructures and edges connect nodes with common atoms. A graph decoder generates a molecule by expanding the substructure tree in a depth-first order, making predictions for new substructures and their attachments. The decoder in the model predicts new substructures and their attachments in a depth-first order based on the hidden representations of the substructures. It uses topological prediction to determine if a new substructure should be created and substructure prediction to identify the type of the new substructure. The model predicts new substructures and their attachments in a depth-first order based on hidden representations. If the probability is above 0.5, a new substructure is created with its parent set. The attachment between substructures is defined by atom pairs, which are predicted in two steps. The model predicts substructures and their attachments based on hidden representations. Predicting attaching atoms is done by enumerating possible configurations to form a vocabulary for each substructure. The prediction is treated as a classification task. The corresponding atoms in the substructure are found based on the predicted attaching points. The probability of attachment is computed using atom representations. The model predicts substructures and their attachments based on hidden representations. The probability of a candidate attachment is computed using atom representations. Predicted attachments depend on previous steps and affect subsequent predictions. Teacher forcing is applied during training with a depth-first traversal over the ground truth substructure tree. The model uses teacher forcing during training with a depth-first traversal over the ground truth substructure tree. The attachment enumeration is manageable due to small substructures, with an average attachment vocabulary size of less than 5 and fewer than 20 candidate attachments. The encoder represents a molecule as a hierarchical graph with three components: atom layer, bond layer, and substructure layer. The atom layer in the model represents the molecular graph of X, showing atom connections with labels for atom type and charge. The attachment layer is derived from the substructure tree, providing information for attachment prediction. The attachment layer in the model shows attachment configurations of substructures, while the substructure layer provides information for substructure prediction in the decoding process. This layer includes edges connecting atoms and substructures between different parts of the molecule. The hierarchical graph H X for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The MPN architecture from Jin et al. (2019) is used to encode each layer, connecting atoms and substructures to propagate information. The hierarchical graph H X for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The encoder contains three MPNs that encode each layer using the MPN architecture from Jin et al. (2019). The MPN encoding process is denoted as MPN \u03c8 (\u00b7) with parameter \u03c8. The inputs to the atom layer MPN are embedding vectors of atoms and bonds in X, propagating message vectors between atoms for T iterations to output atom representations. The attachment layer MPN takes input features of nodes and outputs concatenated embeddings for each node. The attachment layer in the hierarchical message passing network (MPN) concatenates embeddings of nodes with atom vectors for each node. The input feature for edges in this layer is an embedding vector that describes the relative ordering between nodes during decoding. T iterations of message passing are then performed to compute substructure representations. The substructure layer in the MPN computes input features for nodes by concatenating embeddings with node vectors. The hierarchical encoder computes substructure representations by concatenating embeddings and node vectors. The output is a set of vectors representing a molecule at multiple resolutions. The hierarchical MPN architecture is used during decoding to encode the hierarchical graph, generating substructure and atom vectors. The hierarchical MPN architecture encodes the hierarchical graph at each step, generating substructure and atom vectors. Future nodes and edges are masked to ensure prediction depends on previous outputs. Training set includes molecular pairs where each compound can have multiple outputs. To generate diverse outputs, a variational translation model is used with an additional input. For Figure 5: Conditional translation, a variational translation model is used with an additional input z to generate diverse outputs. The latent vector z indicates the intended mode of translation and is sampled from a Gaussian prior during testing. The model is trained using variational inference, sampling z from the posterior distribution. The structural changes from molecule X to Y are summarized at both atom and substructure levels to compute and sample z. The model computes vector \u03b4 X,Y to summarize structural changes from molecule X to Y at atom and substructure levels. Latent code z is used for reconstruction in the decoder. Limitation may arise in multi-property optimization due to fixed model behavior during testing. In a multi-property optimization setting, the limitation of the method is that the behavior of a trained model cannot be changed. To address this, the method is extended to handle conditional translation by feeding desired criteria as input. This involves computing \u00b5 X,Y and \u03c3 X,Y with an additional input g X,Y during variational inference. The latent code is augmented as [z, g X,Y] and passed to the decoder. During testing, users can specify criteria in g X,Y to control the outcome, such as making Y drug-like and bioactive. During testing, users can specify criteria in g X,Y to control the outcome, such as making Y drug-like and bioactive. The translation model is evaluated on single-property optimization tasks following an experimental design by Jin et al. A novel conditional optimization task is constructed where desired criteria are fed as input to the translation process, ensuring molecular similarity between input X and output Y is above a certain threshold at test time. The model is trained on single-property optimization tasks without g X,Y as input. For LogP Optimization, the model translates input X into penalized logP score to measure solubility and synthetic accessibility of a compound. The model is trained on single-property optimization tasks without input X,Y. LogP Optimization measures solubility and synthetic accessibility of a compound by translating input X into output Y such that logP(Y) > logP(X). Two similarity thresholds are experimented with, and different criteria for improving properties after translation are encoded as vector g. The model is trained on single-property optimization tasks without input X,Y. LogP Optimization measures solubility and synthetic accessibility of a compound by translating input X into output Y such that logP(Y) > logP(X). Evaluation metrics include translation accuracy and diversity, with compound selection based on property improvement and similarity constraints. HierG2G method is compared against baselines like GCPN for tasks involving compound translation and diversity measurement. The method selects the final translation of a compound based on property improvement and similarity constraints, reporting average property improvement and translation success rate. Diversity is measured using the average pairwise Tanimoto distance between successfully translated compounds. HierG2G method is compared against baselines like GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE for compound translation. Baselines include sequence-to-sequence and graph-to-graph models for molecule generation. The AtomG2G model is a generative model that decodes molecules atom by atom and optimizes properties using gradient ascent. It includes an atom-based translation model for comparison, predicting atom types and bond types in each generation step. The encoder only encodes the atom-layer graph, and the decoder attention only sees the atom vectors. The AtomG2G model decodes molecules atom by atom and optimizes properties using gradient ascent. It achieves state-of-the-art results on translation tasks, outperforming JTNN in accuracy and output diversity. The autoregressive decoder allows for more expressive mappings. Our model outperforms AtomG2G on three datasets, with over 10% improvement on the DRD2 task, showcasing the advantage of our hierarchical model. When compared to other translation methods like Seq2Seq, JTNN, and AtomG2G, our model demonstrates superior translation accuracy and output diversity. Our model outperforms other models in translation accuracy and output diversity, especially on criteria c = [1, 1]. Training on 1.6K examples gives a 4.2% success rate compared to 13.0% with other pairs. The conditional translation setup transfers knowledge from pairs with g X,Y = [1, 0], [0, 1]. Ablation studies on architecture choices are reported for the QED and DRD2 tasks. In ablation studies on architecture choices for the QED and DRD2 tasks, replacing the hierarchical decoder with an atom-based decoder resulted in a decrease in model performance. The DRD2 task showed more benefit from structure-based decoding due to the importance of specific functional groups in biological target binding. The removal of hierarchies in the encoder and decoder MPN slightly decreases translation accuracy by 0.8% and 2.4%. Further removal of the attachment layer significantly degrades performance on both datasets as substructure information is lost, requiring the model to infer substructures and their construction. Details of these ablations are provided in the appendix. In this paper, a hierarchical graph-to-graph translation model is developed that generates molecular graphs using chemical substructures as building blocks. The model is fully autoregressive and learns coherent multi-resolution representations. The LSTM MPN architecture is used for both HierG2G and AtomG2G baseline, outperforming previous methods despite a slight decrease in translation performance. The model generates molecular graphs using chemical substructures as building blocks, with a fully autoregressive approach and coherent multi-resolution representations. Experimental results demonstrate its superiority over previous models. The message passing network MPN and attention layer are key components in the model's architecture. AtomG2G is a bilinear attention function used in the decoding process of molecular graphs. It predicts new atoms and bond types between nodes in a queue, adding new atoms based on predictions. AtomG2G is an atom-based translation method comparable to HierG2G, representing molecules as molecular graphs. AtomG2G is an atom-based translation method that predicts new atoms and bond types in molecular graphs. The training set and substructure vocabulary sizes for each dataset are listed in Table 3. The multi-property optimization task combines the training sets of QED and DRD2, with a test set of 780 compounds that are not drug-like and DRD2-inactive. The training and test set for multi-property optimization combine QED and DRD2 tasks. Hyperparameters are set for HierG2G and AtomG2G models, with specific dimensions and regularization weights. Both models undergo 20 iterations of message passing. For multi-property optimization, models are trained with specific hyperparameters and dimensions. CG-VAE models are used for molecule generation and property prediction tasks. Compound translation follows a similar procedure as in previous studies. For multi-property optimization, CG-VAE models are trained with specific hyperparameters and dimensions. Compound X is translated using gradient ascent over its latent representation z to maximize property score. Keeping KL regularization weight low (\u03bb KL = 0.005) is crucial for meaningful results. Ablation studies show the impact of different parameters on molecule generation. The gradient ascent procedure generates dissimilar molecules from input X. Ablation studies in Figure 8 show experiments with decoder modifications and hierarchy reductions in the encoder and decoder MPN. Topological and substructure predictions are based on hidden vectors in the two-layer model. In the two-layer model, molecules are represented by c X = c G X \u222a c A X and predictions are based on hidden vector h A k. The one-layer model represents molecules as c X = c G X and predictions are based on atom vectors v\u2208S k h v, with the hidden layer dimension adjusted to match the original model size."
}