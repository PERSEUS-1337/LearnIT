{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization is a bilevel optimization problem where optimal parameters depend on hyperparameters. Regularization hyperparameters for neural networks can be adapted by fitting compact approximations to the best-response function. Scalable best-response approximations for neural networks can be constructed by modeling the best-response as a single network with gated hidden units. This approximation is justified by showing that the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. Our approach fits a model using a gradient-based hyperparameter optimization algorithm that alternates between approximating the best-response and optimizing hyperparameters without requiring differentiation of the training loss. This allows tuning of discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities online. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training, outperforming fixed values. It outperforms other optimization methods on large-scale deep learning problems. Regularization hyperparameters like weight decay, data augmentation, and dropout are crucial for neural network generalization but challenging to tune. Hyperparameter optimization methods like grid search, random search, and Bayesian optimization are commonly used but can be inefficient for high-dimensional spaces. Formulating hyperparameter optimization as a bilevel optimization problem can lead to faster convergence by exploiting structure. Formulating hyperparameter optimization as a bilevel optimization problem involves defining parameters and hyperparameters, mapping them to training and validation losses, and aiming to minimize the validation loss using gradient descent for faster convergence. The best-response function w * is challenging to compute due to the high-dimensional nature of the optimization problem. The best-response function w * in hyperparameter optimization is difficult to compute due to its high-dimensional nature. To address this, a parametric function \u0175 \u03c6 is proposed to approximate w * directly, allowing for joint optimization of \u03c6 and \u03bb. This approach offers a scalable approximation for neural network weights, reducing memory overhead. In hyperparameter optimization, constructing a compact approximation for neural network weights is challenging due to memory overhead. A proposed method involves modeling the best-response of each row in a layer's weight matrix as a rank-one affine transformation of hyperparameters, resulting in Self-Tuning Networks (STNs) that update their own hyperparameters. This approximation is justified by the structure of the exact best-response for a shallow linear network with L2-regularized Jacobian. Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering advantages over other optimization methods. They are easy to implement by replacing existing modules in deep learning libraries with \"hyper\" counterparts. This online adaption ensures computational effort is not wasted and yields hyperparameter schedules that outperform fixed ones. The STN training algorithm allows for online adaption of hyperparameters, outperforming fixed settings. It does not require differentiation of the training loss with respect to hyperparameters, enabling tuning of discrete hyperparameters like data-augmentation parameters. Empirical evaluation shows improved performance on large-scale deep-learning problems. Bilevel optimization involves solving upper-level and lower-level problems, with minimax problems being a specific example. Initially studied in economics, bilevel programs model leader/follower dynamics. Bilevel programs model leader/follower dynamics in various fields, including machine learning. These programs are NP-hard even with linear objectives and constraints, leading to focus on restricted settings for solutions. In contrast to previous work on restricted settings, this study focuses on obtaining local solutions in the nonconvex, differentiable, and unconstrained setting. The goal is to design a gradient-based algorithm for solving the optimization problem efficiently. The study aims to design a gradient-based algorithm for efficiently solving Problem 4 by considering the best-response function for updating \u03bb and w. A principled approach to solving Problem 4 involves using the best-response function and converting it into a single-level problem. Gradient descent can be used to minimize the function with respect to \u03bb, under certain conditions. Lemma 1 states that if w 0 solves Problem 4b for \u03bb 0, and f is C 2 in a neighborhood of (\u03bb 0, w 0) with a positive definite Hessian, then there exists a function w * such that w * (\u03bb) is the unique solution to Problem 4b for each \u03bb in a neighborhood of \u03bb 0. The direct gradient captures the reliance of the upper-level objective on \u03bb, while the response gradient shows how the lower-level parameter responds to changes in the upper-level parameter. Including the response gradient can stabilize optimization by converting the bilevel problem into a single-level one, ensuring a conservative gradient vector field. The solution to Problem 4b is a set, but uniqueness and differentiability of w * can lead to efficient algorithms in practice. Gradient-based hyperparameter optimization methods aim to approximate the best-response w * or its Jacobian \u2202w * /\u2202\u03bb, but struggle with discrete and stochastic hyperparameters due to computational complexity. The algorithm proposed by Lorraine & Duvenaud (2018) approximates w * as a differentiable function\u0175 \u03c6 with parameters \u03c6, allowing for efficient optimization of hyperparameters. The algorithm by Lorraine & Duvenaud (2018) approximates w* with a differentiable function \u0175 \u03c6 using gradient descent on the objective function. The algorithm by Lorraine & Duvenaud (2018) approximates w* with a differentiable function \u0175 \u03c6 using gradient descent on the objective function. It involves a factorized Gaussian noise distribution with a fixed scale parameter \u03c3 \u2208 R n +, where \u03c6 is found by minimizing an objective function. An alternating gradient descent scheme is used to update \u03c6 and \u03bb, which perturbs the upper-level parameter \u03bb to help the lower-level parameter learn how to respond. This approach has shown success with L2 regularization on MNIST (LeCun et al., 1998), but its effectiveness with different regularizers or scales on larger problems is uncertain. The method requires \u0175 \u03c6, which can be challenging for high-dimensional w, and setting \u03c3, which determines the neighborhood size for \u03c6, remains unclear. In this section, a best-response approximation \u0175 \u03c6 is constructed for memory-efficient scaling to large neural networks. An automatic method to adjust the neighborhood scale of \u03c6 is described, along with an algorithm that easily handles discrete and stochastic hyperparameters. The resulting networks update their own parameters efficiently. In this section, a best-response approximation \u0175 \u03c6 is constructed for memory-efficient scaling to large neural networks. The algorithm easily handles discrete and stochastic hyperparameters, resulting in Self-Tuning Networks (STNs) that update their own parameters efficiently during training. The architecture involves an affine transformation of hyperparameters for weight matrix and bias, enhancing the usual weight/bias computation. The best-response architecture involves an additional weight/bias scaled by hyperparameters, making it tractable and memory-efficient. It requires D out (2D in + n) parameters for \u0174 \u03c6 and D out (2 + n) parameters for b \u03c6, enabling parallelism in computing predictions. The best-response architecture involves additional weight/bias scaled by hyperparameters, enabling parallelism in computing predictions. This can be implemented by replacing existing modules in deep learning libraries with \"hyper\" counterparts. The model's best-response function is a mapping from R^n to the high-dimensional weight space R^m. Equation 10 serves as a reasonable approximation in this context. In this section, a model is presented where the best-response function can be exactly represented using a linear network with Jacobian norm regularization. The network's hidden units are modulated based on hyperparameters, and a 2-layer linear network is used to predict targets from inputs. The model uses a squared-error loss with an L2 penalty on the Jacobian, with the penalty weight \u03bb being a mapped parameter. The model presented uses a squared-error loss with an L2 penalty on the Jacobian, where the penalty weight \u03bb is mapped using exp. The best-response function can be represented by a linear network with modulated hidden units and a sigmoidal gating mechanism. The architecture shown in FIG0 uses a network with sigmoidal gating of hidden units to approximate the best-response for deep, nonlinear networks. For a narrow hyperparameter distribution, a smooth best-response function can be approximated by linear gating instead of sigmoidal gating. Replacing sigmoidal gating with linear gating allows for an affine approximation to the best-response function, ensuring convergence to a local optimum in gradient descent for quadratic lower-level objectives. The effect of the sampled neighborhood is discussed, emphasizing the importance of the neighborhood size. The text discusses the importance of the sampled neighborhood size in achieving convergence to a local optimum in gradient descent. It explains how the gradient of the approximation matches that of the best-response when the neighborhood size is appropriate, but may not match if the neighborhood is too small or too wide. The scale of the hyperparameter is controlled by the entries of \u03c3. Adjusting the scale of the hyperparameter distribution during training is crucial for capturing the best-response over samples. Varying \u03c3 based on the sensitivity of the upper-level objective to the sampled hyperparameters can help address issues with flexibility and capturing the shape locally. During training, adjusting \u03c3 based on the sensitivity of the upper-level objective to sampled hyperparameters can improve flexibility and local shape capture. An entropy term weighted by \u03c4 \u2208 R + enlarges \u03c3 entries, resulting in an objective similar to variational inference. The objective interpolates between variational optimization and variational inference as \u03c4 ranges from 0 to 1, aiding in better training and representation learning. Minimizing the first term moves probability mass towards an optimum \u03bb * , balancing \u03c3 to avoid heavy entropy penalties. Algorithm performance is evaluated at the deterministic current hyperparameter \u03bb 0 during benchmarking. The STN training algorithm can tune hyperparameters that other gradient-based algorithms cannot, such as discrete or stochastic hyperparameters. It uses an unconstrained parametrization \u03bb \u2208 R n and maps it to the appropriate constrained space using a non-differentiable discretization for discrete hyperparameters. Training and validation losses, denoted as L T and L V, are possibly stochastic. The STN training algorithm involves non-differentiable discretization for discrete hyperparameters. Training and validation losses, denoted as L T and L V, are functions of hyperparameters and parameters. The algorithm alternates between updating \u03c6 for training steps and updating \u03bb and \u03c3 for validation steps. The complete algorithm is provided in Algorithm 1. The algorithm for training involves non-differentiable discretization for discrete hyperparameters. The derivative of E with respect to \u03c6 can be estimated using the reparametrization trick. The computation paths for \u2202f /\u2202w and \u2202\u0175 \u03c6/\u2202\u03c6 do not involve the discretization r. When dealing with a discrete hyperparameter \u03bb i, two cases need to be considered. The algorithm for training involves non-differentiable discretization for discrete hyperparameters. When dealing with a discrete hyperparameter \u03bb i, two cases need to be considered: Case 1 involves regularization schemes where F does not depend on \u03bb i directly, and Case 2 involves using the REINFORCE gradient estimator for \u03bb i. The number of hidden units in a layer is an example of a hyperparameter that requires the REINFORCE approach. The study applied a method to tune hyperparameters in convolutional networks and LSTMs, resulting in self-tuning CNNs and LSTMs. The method outperformed fixed hyperparameter values and was compared to other optimization methods on CIFAR-10 and PTB datasets. The study compared STNs to common hyperparameter optimization methods on CIFAR-10 and PTB datasets. STNs dynamically adjust hyperparameters during training, outperforming fixed hyperparameters. An ST-LSTM was used to tune output dropout rate, surpassing fixed rates with a discovered schedule. The schedule discovered by an ST-LSTM for output dropout outperforms fixed rates, achieving 82.58 vs 85.83 validation perplexity. Improved performance is attributed to the schedule, ruling out stochasticity from sampling hyperparameters during training. The ST-LSTM discovered a schedule for output dropout that outperformed fixed rates, achieving 82.58 vs 85.83 validation perplexity. STNs outperformed random Gaussian and sinusoid perturbations, with evidence showing the schedule itself was responsible for the improvement in performance. The standard LSTM performed nearly as well as the STN, showing that the schedule itself was responsible for the improvement over a fixed dropout rate. Training a standard LSTM with the final dropout value from the STN did not perform as well as following the schedule. The STN discovered the same schedule regardless of initial hyperparameter values, indicating the importance of the hyperparameter schedule. The STN schedule implements a curriculum by using a low dropout rate early in training, aiding optimization, and then gradually increasing the dropout rate for better generalization. This schedule is found to be consistent regardless of initial hyperparameter values, showing the importance of hyperparameter adaptation. The ST-LSTM was evaluated on the PTB corpus with a 2-layer LSTM and 650 hidden units per layer. 7 hyperparameters were tuned including variational dropout rates, embedding dropout, DropConnect, and coefficients for activation regularization. The ST-LSTM was evaluated on the PTB corpus with hyperparameters tuned for variational dropout rates, embedding dropout, DropConnect, and coefficients for activation regularization. The best results were obtained using a fixed perturbation scale of 1 for the hyperparameters. STNs outperformed grid search, random search, and Bayesian optimization, achieving lower validation perplexity more quickly. STNs outperform other methods in achieving lower validation perplexity more quickly. The schedules for hyperparameters in STNs show nontrivial patterns of dropout usage. ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture. On the CIFAR-10 dataset, STNs were compared to other optimization methods using the AlexNet architecture. 15 hyperparameters were tuned, including dropout rates and data augmentation parameters. STNs showed the lowest validation loss compared to grid search, random search, and Bayesian optimization. The study compared STNs to other optimization methods on the CIFAR-10 dataset using the AlexNet architecture. 15 hyperparameters were considered, with STNs achieving the lowest validation loss. The experimental setup details are in Appendix E, and STNs found better hyperparameter configurations in less time. Bilevel Optimization is briefly mentioned, with references provided. A comprehensive textbook on bilevel problems was written by BID1. When objectives/constraints are linear, quadratic, or convex, a common approach involves replacing the lower-level problem with its KKT conditions as constraints for the upper-level problem. Trust-region methods, like those used by BID9, approximate the problem locally. Sinha et al. (2013) used evolutionary techniques to estimate the best-response function iteratively. Hypernetworks, first considered by Schmidhuber (1993; BID15), are functions mapping to weights. Evolutionary techniques were used to estimate the best-response function iteratively. Hypernetworks, first considered by Schmidhuber (1993; BID15), are functions mapping to the weights of a neural net. Gradient-Based Hyperparameter Optimization has two main approaches. Gradient-Based Hyperparameter Optimization has two main approaches. The first approach approximates the value of w after T steps of gradient descent on f with respect to w starting at (\u03bb 0 , w 0 ). The second approach uses the Implicit Function Theorem to derive \u2202w * /\u2202\u03bb(\u03bb 0 ) under certain conditions. The Implicit Function Theorem is used to derive \u2202w * /\u2202\u03bb(\u03bb 0 ) for hyperparameter optimization in various models, including neural networks, log-linear models, kernel selection, and image reconstruction. Differentiating gradient descent or training loss with respect to hyperparameters can be challenging, especially as the number of descent steps increases. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. Various methods can be used to construct the model iteratively, selecting the next hyperparameters to train on by maximizing an objective function. Model-Based Hyperparameter Optimization uses Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. Different methods are used iteratively to construct the model, selecting the next hyperparameters to train on by maximizing an objective function. This approach requires building inductive biases into the model, which may not hold in practice and do not take advantage of the network structure for hyperparameter optimization. Model-free hyperparameter optimization methods like random search and grid search do not scale well with the number of hyperparameters. Successive Halving and Hyperband extend random search by allocating resources to promising configurations using bandit techniques. These methods ignore problem structure but are easy to parallelize. Our approach, on the other hand, utilizes rich gradient information. Population Based Training (PBT) is a method that considers schedules for hyperparameters by training a population of networks in parallel. Under-performing networks have their weights replaced by better-performing ones, and the hyperparameters of the better network are copied and perturbed for training new network clones. This allows a single model to experience different variations. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting hidden units, allowing for gradient-based optimization to tune various regularization hyperparameters, including discrete ones. Self-Tuning Networks (STNs) optimize hyperparameters by scaling and shifting hidden units, using gradient-based optimization to tune regularization hyperparameters. STNs discover hyperparameter schedules that outperform fixed hyperparameters, achieving better generalization performance in less time. This approach offers a path towards automated hyperparameter tuning for neural networks. The study acknowledges support from various funding sources. It discusses the best-response of parameters to hyperparameters and the gradients involved in the optimization process. The Jacobian matrix decomposition is also mentioned in relation to the optimization problem. The text discusses the Jacobian matrix decomposition and the existence of a unique continuously differentiable function in a neighborhood of a given point. The text discusses the uniqueness of the solution to Problem 4b in a neighborhood of a given point, following second-order optimality conditions. It also introduces the data matrix X, its SVD decomposition, and the associated targets. The SVD decomposition of matrix X is represented as U, V, and D, with orthogonal matrices U and V, and diagonal matrix D. Simplifying the function y(x; w) leads to a constant Jacobian. The optimal solutions for regularized and unregularized versions of the problem are discussed. The SVD decomposition of matrix X is represented as U, V, and D, with orthogonal matrices U and V, and diagonal matrix D. The optimal solution u* to the unregularized version of Problem 19 is given by Q0s0 = u*, where s0 = D^-1Ut. Best-response functions Q*(\u03bb) and s*(\u03bb) are chosen as \u03c3(\u03bbv + c) row Q0 and s0 respectively. The chosen functions Q*(\u03bb) = \u03c3(\u03bbv + c) and s*(\u03bb) = s0 meet the criteria for a quadratic function. By assuming \u22022f/\u2202w2 = 0, we find the optimal solution \u0175\u03c6(\u03bb) = U\u03bb + b for the function f. After finding the optimal solution \u0175\u03c6(\u03bb) = U\u03bb + b for the function f, we can simplify expressions using linearity of expectation and properties of the Trace operator. Differentiating f involves various matrix-derivative techniques. After finding the optimal solution for the function f, we can simplify expressions using matrix-derivative techniques and properties of the Trace operator. Differentiating f involves setting the derivative equal to 0, resulting in the best-response Jacobian. Substituting parameters into the equation gives the approximate best-response as the first-order Taylor series of the optimal solution. The model parameters were updated, but hyperparameters were not. Training was stopped when the learning rate fell below 0.0003. Variational dropout was tuned for the LSTM input, hidden state, and output. Embedding dropout was also adjusted, removing certain words from sequences. DropConnect was used to regularize the hidden-to-hidden weight matrix. The text discusses the use of DropConnect to regularize the hidden-to-hidden weight matrix in a model. Activation regularization (AR) penalizes large activations, while temporal activation regularization (TAR) is a slowness regularizer. These regularization techniques were tuned for the model. The text presents additional details on the CNN experiments, including the training setup with validation data, optimization algorithm, and hyperparameters used for the baseline CNN model. The baseline CNN was trained with SGD using an initial learning rate of 0.01 and momentum of 0.9 on mini-batches of size 128. The learning rate was decayed by 10 when validation loss didn't decrease for 60 epochs, stopping training if the rate fell below 10^-5 or validation loss didn't decrease for 75 epochs. Hyperparameter search spaces were defined for dropout rates. The ST-CNN's parameters were trained similarly to the baselines. The ST-CNN's elementary parameters were trained using SGD with initial learning rate 0.01 and momentum of 0.9 on mini-batches of size 128. Hyperparameters were optimized using Adam with learning rate 0.003. Training alternated between best-response approximation and hyperparameters with the same schedule as the ST-LSTM. An entropy weight of \u03c4 = 0.001 was used in the entropy regularized objective. We used five epochs of warm-up for model parameters with fixed hyperparameters. Entropy weight of \u03c4 = 0.001 was used in the regularized objective. Cutout length was limited to {0, 24} and number of cutout holes to {0, 4}. Dropout rates and data augmentation noise parameters were initialized to 0.05. ST-CNN showed robustness to hyperparameter initialization, with low regularization aiding optimization in initial epochs. Curriculum learning is a method that optimizes non-convex functions by gradually increasing the difficulty of the training criteria. It starts with a simpler version of the problem and gradually increases the parameter \u03bb from 0 to 1. This approach helps in robust optimization and aids in the initialization of hyperparameters. In this section, hyperparameter schedules are explored as a form of curriculum learning. By gradually increasing parameters like dropout over time, the learning problem becomes more challenging. Results from grid searches show that greedy hyperparameter schedules can outperform fixed ones. The study explores hyperparameter schedules as a form of curriculum learning, showing that greedy schedules can outperform fixed ones. A grid search was conducted over 20 values each of input and output dropout, revealing that smaller dropout rates are optimal at the start of training, while larger rates perform better as training progresses. The study demonstrates that optimal performance is achieved with small input and output dropout values initially, but larger dropout rates are more effective as training progresses. A grid search was conducted to determine the best hyperparameter values, showing that a greedy dropout schedule outperforms fixed values. The study shows that a greedy dropout schedule with varying values throughout training outperforms fixed hyperparameter values. PyTorch code listings for best-response layers used in ST-LSTMs and ST-CNNs are provided in Section 4.1. In Section 4.1, PyTorch code listings for constructing ST-LSTMs and ST-CNNs are provided, including the HyperLinear and HyperConv2D classes. Optimization steps for the training and validation sets are also simplified."
}