{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences, a component lacking in current language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance by anticipating different sentence structures. Experiments show that translation performance improves with structural planning in neural machine translation. Manipulating planner codes can result in translations with different structures. Planning ahead is essential for grammatical and logical correctness in human speech, evidenced by speech errors and behaviors. Unlike humans, NMT models lack a planning phase when generating sentences. In neural machine translation, unlike humans, the model lacks a planning phase for sentence generation. This leads to uncertainty in word prediction as the model is unaware of the overall sentence structure. Research aims to enable the model to plan the coarse structure of the output sentence. In neural machine translation, the model lacks a planning phase for sentence generation, resulting in uncertainty in word prediction. Research aims to enable the model to plan the coarse structure of the output sentence by inserting planner codes at the beginning of the sentences. In neural machine translation, the model lacks a planning phase for sentence generation, resulting in uncertainty in word prediction. Research aims to enable the model to plan the coarse structure of the output sentence by inserting planner codes at the beginning of the sentences. The input sentence already provides rich information about the target-side structure. By learning a set of planner codes, uncertain information about the sentence structure can be disambiguated, potentially increasing the effectiveness of beam search. In this work, simplified POS tags are used to annotate the structure S Y. Planner codes are learned through a network that reconstructs S Y with X and C Y. The codes are merged with target sentences in training data, improving translation performance. Output sentence structure can be controlled by manipulating planner codes. In this work, the structure of output sentences is controlled by manipulating planner codes. The structural annotation S Y is extracted by simplifying POS tags, and a code learning model is used to obtain the planner codes. To reduce uncertainty in decoding, a coarse structural annotation is used to describe the overall sentence structure, such as \"NP VP\" order. Beam search or the NMT model can efficiently solve uncertainty in local structures. The extraction of structural annotations S Y involves simplifying POS tags in a two-step process. In this work, coarse structural annotations are extracted through a two-step process simplifying POS tags of the target sentence. The planner codes are then learned to remove uncertainty in sentence structure when producing a translation. The code learning model architecture involves computing discrete codes based on simplified POS tags of the target sentence, which are then discretized into approximated one-hot vectors. The code learning model architecture includes encoding with a backward LSTM, computing vectors C1 to CN, discretizing them into one-hot vectors, and using Gumbel-Softmax trick. The decoder LSTM predicts tags sequentially based on information from X and C, with the probability of emitting each tag predicted at the end. The architecture is illustrated in Fig. 2. The code learning model architecture involves a sequence auto-encoder with an extra context input X to the decoder. Parameters are optimized with crossentropy loss. After training, planner codes C are obtained for all target sentences in the training data. The training data is then transformed into a list of (X, C Y ; Y ) pairs by connecting planner codes with target sentences using an \"eoc\" token. The training data is transformed into (X, C Y ; Y ) pairs by connecting planner codes with target sentences using an \"eoc\" token. A regular NMT model is trained on this modified dataset, utilizing beam search during decoding. Methods like BID19 and BID2 have been proposed to enhance syntactic correctness in translations. The NMT decoder uses a lattice from a Statistical Machine Translation system. BID2 employs a multi-task approach, incorporating dependency tree parsing into the NMT model. Various methods integrate target-side syntactic structures explicitly. Aharoni and Goldberg (2017) train a NMT model to generate linearized constituent parse trees. BID20 proposes a model to generate words and parse actions simultaneously, conditioning word prediction on action prediction. However, none of these methods plan the structure before translation. Some works learn discrete codes for different purposes, compressing word embeddings and breaking down word dependency. Models are evaluated on translation tasks using tokenization tools. The models are evaluated on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks. Kytea is used for tokenizing Japanese texts and moses toolkit for other languages. Bytepair encoding is used in the code learning model with 256 hidden units in all hidden layers. The model is trained using Nesterov's accelerated gradient for 50 epochs with a learning rate of 0.25. Different settings of code length N and number of code types K are tested, with the information capacity of the codes being N log K bits. The learned codes are evaluated for different settings in TAB1, with accuracy measured in correctly reconstructing the source sentence. The text discusses the trade-off between accuracy in reconstructing source sentences and guessing correct codes in a code learning model. A balanced trade-off is found with N=2, K=4 settings. The NMT model used has 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with 256 units for IWSLT De-En. The NMT model uses 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with 256 units for IWSLT De-En task. Key-Value Attention is applied in the first decoder layer, and a residual connection is used to combine hidden states in two decoder layers. Dropout with a rate of 0.2 is applied outside of the recurrent function. The NMT models are trained using the NAG optimizer with a learning rate of 0.25, annealed by a factor of 10 after 20K iterations if no improvement is observed. By conditioning word prediction on generated planner codes, translation performance improves over a strong baseline. However, applying greedy search on JaEn dataset results in lower BLEU scores. Beam search followed by greedy search does not significantly change results. It is important to simultaneously explore search space. The study explores the importance of simultaneously exploring multiple candidates with different structures in translation tasks. Planning ahead to explore diverse candidates improves beam search but not greedy search. Results align with a recent study showing beam search performance depends on candidate diversity. Manual selection of planner codes can also be considered. The study discusses the importance of exploring multiple candidates with different structures in translation tasks. Results show that planning ahead for diverse candidates improves beam search performance. Manual selection of planner codes is also an option. The study demonstrates the effectiveness of using planner codes in Ja-En translation tasks, showing diverse translations can be obtained by manipulating the codes. The distribution of assigned planner codes for English sentences in the ASPEC Ja-En dataset is illustrated, with some codes being more prevalent than others. The distribution of planner codes for English sentences in the ASPEC Ja-En dataset is skewed, with some codes more prevalent than others. Learning discrete codes may not be fully utilized, leaving room for improvement. Directly predicting structural annotations like POS tags and translating based on them can degrade performance by around 8 BLEU points on the IWSLT dataset. In this paper, a planning phase is added to neural machine translation to generate planner codes for controlling the output sentence structure. An end-to-end neural network with a discretization bottleneck is designed to predict simplified POS tags of target sentences. Experiments show that this method improves translation performance and allows for sampling translations with different structures. The planning phase is beneficial in improving translation quality. The planning phase in neural machine translation generates planner codes to control sentence structure, improving translation performance and allowing for sampling translations with different structures. This framework can be extended to plan other latent factors like sentiment or topic."
}