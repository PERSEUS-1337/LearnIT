{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is proposed for two layer ReLU networks, offering a tighter generalization bound. This capacity bound correlates with test error behavior as network sizes increase, potentially explaining the generalization improvement with over-parametrization. Additionally, a matching lower bound for Rademacher complexity is presented, showing improvement over existing measures. Deep neural networks have been successful in various tasks and are often over-parametrized to fit random labels. A new lower bound for Rademacher complexity improves capacity bounds for neural networks, explaining the generalization enhancement with over-parametrization. Increasing the size of neural network models, even though they are over-parametrized, helps in improving generalization error rather than leading to overfitting. This contradicts traditional wisdom in learning where higher model capacity is expected to result in overfitting. Increasing the size of neural network models improves generalization error, even without explicit regularization techniques like weight decay or early stopping. Empirical observations show that increasing the number of hidden units leads to decreased test error in image classification tasks. This contradicts traditional beliefs that higher model capacity leads to overfitting. The improvement in generalization with over-parametrization in neural networks is not fully explained by complexity measures based on the total number of parameters. Different measures such as norm, margin, and sharpness have been suggested to capture this phenomenon. Different norm, margin, and sharpness based measures have been suggested to measure the capacity of neural networks and explain generalization behavior. Even when the network is large enough to fit the training data, the test error continues to decrease for larger networks. Training fully connected feedforward networks on CIFAR-10 shows similar phenomena as observed in ResNet18 architecture. Unit capacity and unit impact capture the complexity and impact of hidden units, respectively. The complexity and impact of hidden units in neural networks are captured by unit capacity and unit impact. Empirical observations show that both average unit capacity and average unit impact decrease faster than 1/ \u221a h, where h is the number of hidden units. Previous research has shown that the generalization bound of a network depends on the spectral norm and 1,2 norm of its layers. The complexity measures based on spectral norm and 1,2 norm of network layers fail to explain why over-parametrization helps and increase with network size, even for two layer networks. The complexity measures increase with network size, even for two layer networks. To study this phenomenon, two layer ReLU networks were chosen to simplify the architecture while preserving the property of interest. This paper proves a tighter generalization bound for two layer ReLU networks. The paper proves a tighter generalization bound for two layer ReLU networks, showing that complexity decreases with increasing hidden units. The generalization bound for two layer ReLU networks decreases with increasing hidden units, as shown by the Frobenius norm of the top layer and the difference of hidden layer weights with initialization. In the over-parametrized setting, the closeness of learned weights to initialization can be understood by training just the top layer in the extreme case of infinite hidden units. In an extreme setting with infinite hidden units, training only the top layer of the network minimizes training error by selecting the right features to minimize loss. This suggests that over-parameterizing networks reduces the need for optimization algorithms to tune hidden unit weights. Dziugaite & Roy (2017) have numerically evaluated a PAC-Bayes measure based on this initialization. In an extreme setting with infinite hidden units, training only the top layer of the network minimizes training error by selecting the right features to minimize loss. Dziugaite & Roy (2017) have numerically evaluated a PAC-Bayes measure from the initialization used by the algorithms, showing the importance of initialization in reducing the need for tuning hidden unit weights. Nagarajan & Kolter (2017) also emphasize the significance of initialization, proving an initialization dependent generalization bound for linear networks. Liang et al. (2017) proposed a Fisher-Rao metric based complexity measure that correlates with generalization behavior in larger networks. Our contributions in this paper include an empirical investigation on the role of over-parametrization in generalization of neural networks on 3 different datasets (MNIST, CIFAR10, and SVHN). We show that existing complexity measures increase with the number of hidden units but do not explain generalization behavior with over-parametrization. Additionally, we prove tighter generalization bounds for two-layer ReLU networks. Our proposed complexity measure for neural networks decreases with the increasing number of hidden units, potentially explaining the effect of over-parametrization on generalization. We also provide a lower bound for the Rademacher complexity of two-layer ReLU networks with a scalar output, improving over previous results. Our lower bound for two-layer ReLU networks with a scalar output surpasses previous bounds and is the first to exceed the Lipschitz constant of the network class. The network consists of fully connected layers with input dimension d, output dimension c, and hidden units h. The classification task involves maximizing the output score for the label. The margin operator \u00b5 is defined for c-class classification tasks, with the ramp loss defined accordingly. The expected margin loss for a predictor is bounded by a given margin \u03b3 > 0. The expected margin loss of a predictor is defined for any distribution D and margin \u03b3 > 0, bounded between 0 and 1. The empirical estimate of the expected margin loss is denoted by L \u03b3 (f). The generalization bound holds for any function f \u2208 F with probability 1 \u2212 \u03b4 over the choice of the training set of size m. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels. It increases with the complexity of the class. By bounding the Rademacher complexity of neural networks, we can obtain a bound on the generalization error. The choice of the function class considered is crucial in determining the Rademacher complexity. The Rademacher complexity of neural networks is bounded to estimate generalization error. Choosing the right function class is important to capture trained networks accurately. Experimentation on CIFAR-10 dataset explores behavior of network layers with increasing hidden units. The experiments on the CIFAR-10 dataset investigate the behavior of network layers with increasing hidden units. The spectral and Frobenius norms initially decrease but eventually increase with h, with the Frobenius norm increasing faster. The distance Frobenius norm w.r.t. initialization decreases, indicating that the increase in weights' Frobenius norm in larger networks is due to the random initialization. The increase in the Frobenius norm of weights in larger networks is attributed to the increase in random initialization. The distance to initialization per unit decreases with increasing hidden units, leading to a shift in the distribution of angles between learned and initial weights. This per unit distance to initialization is a key factor in capacity bounds. In larger networks, the Frobenius norm of weights increases due to random initialization. The unit capacity, defined as the distance to initialization per unit, plays a key role in capacity bounds. For the second layer, the Frobenius norm and distance to initialization decrease with network size, indicating a limited role of initialization. The Frobenius norm and distance to initialization decrease with network size for the second layer, suggesting a limited role of initialization. The impact of each classifier on the final decision shrinks faster than 1/ \u221a h, defined as unit impact. Unit impact is defined as \u03b1 i = v i 2. The unit impact, defined as \u03b1 i = v i 2, plays a crucial role in two-layer neural networks. Empirical observations suggest that networks from real data have bounded unit capacity and unit impact. The hypothesis class of neural networks is represented using parameters in a restricted set W. The hypothesis class of neural networks is represented using parameters in a restricted set W. Our empirical observations show that networks from real data have bounded unit capacity and unit impact. Studying the generalization behavior of this function class can enhance our understanding of these networks. We will now analyze the generalization properties of this function class, specifically focusing on two-layer ReLU networks. By bounding the Rademacher complexity of the class F W in terms of the sum over hidden units of the product of unit capacity and unit impact, we can derive a generalization bound. The generalization bound for two-layer ReLU networks is derived by bounding the Rademacher complexity of the class F W in terms of the sum over hidden units of the product of unit capacity and unit impact. The proof technique decomposes the network complexity into that of hidden units, different from previous works that decompose it into layers. The complexity of two-layer neural networks is decomposed across hidden units to provide a tighter bound on Rademacher complexity. This approach considers the linear structure of each individual layer, unlike previous works that focus on layer-wise Lipschitzness. The generalization bound in Theorem 1 applies to any function in a specific class defined by fixed \u03b1 and \u03b2 values. To obtain a bound applicable to all networks, a union bound over the space of possible \u03b1 and \u03b2 values is necessary. The generalization bound for two-layer ReLU networks is derived by covering the space of possible \u03b1 and \u03b2 values. The bound improves existing ones and decreases with increasing network width in practice. An explicit lower bound for Rademacher complexity is also provided. The generalization bound for two-layer ReLU networks improves over existing bounds and decreases with increasing network width. An explicit lower bound for Rademacher complexity is also provided, showing tightness in the bound. The additive factor in the bound results from taking the union bound over the cover of \u03b1 and \u03b2. In practice, the additive term is small and does not dominate the first term, leading to an overall decrease in capacity with over-parametrization. The generalization bound is further extended to p norms in Appendix Section B, presenting a finer tradeoff between the two terms. The generalization bound for two-layer ReLU networks improves with increasing network width and decreases with over-parametrization. In Appendix Section B, the bound is extended to p norms, showing a finer tradeoff between terms. The key complexity terms in the bounds differ, with the first term behaving similarly. The generalization bound for two-layer ReLU networks improves with increasing network width and decreases with over-parametrization. Experimental comparison shows that even a network of size 128 is enough to achieve zero training error on CIFAR-10 and SVHN datasets. In experiments on CIFAR-10 and SVHN datasets, networks of size 128 can achieve zero training error. However, larger networks show better generalization even without regularization. Unit capacity and unit impact decrease with increasing network size. The number of epochs needed to reach 0.01 cross-entropy loss decreases for larger networks. FIG6 compares the behavior of different network sizes in the same experimental setup. The effective capacity of function class decreases with network size, leading to lower generalization bounds. Our bound outperforms other norm-based data-independent bounds and even VC-dimension for networks larger than 1024. The numerical values are loose but show improvement in our bound. Our capacity bound improves over VC-dimension for networks larger than 1024, showing a decrease in size even for networks with about 100 million parameters. Applying data-dependent techniques can significantly enhance these bounds. Our capacity bound decreases with network size, even for networks with about 100 million parameters, unlike other norm-based bounds. This suggests that our bound may identify properties that help over-parametrized networks generalize. Additionally, we compare our complexity measure between networks trained on real and random labels to analyze its behavior. In this section, a lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound. The lower bound is shown on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer. This allows for comparison with existing results and extends the lower bound to the bigger class F W. Theorem 3 is presented. The lower bound for the Rademacher complexity of neural networks is proven on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer. The complexity lower bound matches the first term in the upper bound of Theorem 1, up to 1 \u03b3. The complete proof is given in the supplementary Section C.3. The upper bound in Theorem 1 is tight, even with additional information like bounded spectral norm. Previous capacity lower bounds for spectral norm bounded networks are limited. Despite the small size of the matrix bounding the Lipschitz of the network, the upper bound cannot be improved. This lower bound shows a gap between the Lipschitz constant and the capacity of neural networks, excluding those with rank-1 matrices as weights. The smaller function class excludes neural networks with rank-1 matrices as weights, showing a capacity gap between ReLU and linear networks. The lower bound does not apply to linear networks. By setting weight matrices in intermediate layers to the Identity matrix, the construction can be extended to more layers. The function class is defined by the parameter set s1s2 as the Lipschitz bound of F Wspec. Choosing \u03b1 and \u03b2 such that \u03b1^2 = s1 and max i\u2208[h] \u03b2i = s2 results in W \u2282 Wspec. The function class F Wspec has a capacity bound defined by the parameter set s1s2 as the Lipschitz bound. By choosing \u03b1 and \u03b2 such that \u03b1^2 = s1 and max i\u2208[h] \u03b2i = s2, we get a stronger lower bound for this function class. This result improves on previous lower bounds and presents a new capacity bound for neural networks. In this paper, a new capacity bound for neural networks is presented, which decreases with the increasing number of hidden units. The focus is on understanding the role of width in the generalization behavior of two-layer networks. Future studies will explore the interplay between depth and width in controlling network capacity. Additionally, a matching lower bound for capacity is provided, improving on current lower bounds for neural networks. The paper presents a new capacity bound for neural networks that decreases with more hidden units. It focuses on the role of width in generalization behavior and aims to understand the interplay between depth and width in controlling network capacity. The study also provides a matching lower bound for capacity, improving on current lower bounds for neural networks. In an experiment, a pre-activation ResNet18 architecture was trained on CIFAR-10 dataset with a convolution layer, 8 residual blocks, and a linear layer. The study aims to understand the impact of different hyperparameter choices on solution complexity and the implicit regularization effects of optimization algorithms for neural networks. The architecture consists of a convolution layer, 8 residual blocks, and a linear layer. The number of output channels and strides in the residual blocks are defined by the initial number of channels. The kernel size used is 3 in all convolutional layers. Training involves 11 architectures with varying values of k. SGD is used with specific parameters, and training stops based on loss criteria or after a set number of epochs. We trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets using specific learning rate and stopping criteria. Data augmentation was performed by random horizontal flip and random crop followed by zero padding. Thirteen architectures were trained for each dataset with increasing hidden units. For each dataset, 13 architectures were trained with increasing hidden units. The networks were trained using SGD with specific parameters and stopped when cross-entropy reached 0.01 or after 1000 epochs. Generalization bounds were calculated with a set margin. The exact generalization bounds were calculated for each dataset with a specific margin set at the 5th percentile of data points. To ensure linearity with the number of classes, adjustments were made to the bounds provided in previous studies. Random initialization was used as the reference matrix for plotting distributions with a standard Gaussian kernel. In this section, the behavior of measures on networks trained on SVHN and MNIST datasets is shown. The over-parametrization phenomenon in the MNIST dataset is illustrated in FIG10, comparing generalization bounds. Theorem 2 is generalized to p norm, with Lemma 11 constructing a cover for the p ball with entry-wise dominance. The proof introduces Lemma 11 to construct a cover for the p ball with entry-wise dominance. The generalization error is bounded for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1), and U 0 \u2208 R h\u00d7d. The bound improves on Theorem 2 with a tighter bound that decreases with h for larger values, particularly for p = ln h. The generalization error is bounded for any function f(x) = V[Ux] +, with a tighter bound that decreases with h for larger values. Lemma 7 provides a vector-contraction inequality for Rademacher complexities, and a technical result from Maurer (2016) is used in the proof. The Rademacher complexity of a class of networks can be decomposed to that of hidden units, as shown in Lemma 9. The complexity is bounded for a training set S and a given parameter \u03b3. The Rademacher complexity of the class F W is bounded for a training set S and parameter \u03b3. The proof involves induction on t and the Lipschitzness of the ramp loss. The inequality is a result of the \u221a2\u03b3 Lipschitzness of the ramp loss, which is 1/\u03b3 Lipschitz in each dimension. The loss at each point depends on the correct label score and the maximum score among other labels, making it \u221a2\u03b3-Lipschitz. Lemma 10, the Ledoux-Talagrand contraction, is used in the proof of Theorem 1, which involves a convex and increasing function f, Lipschitz functions \u03c6i, and Rademacher random variables \u03bei. The proof of Theorem 1 utilizes Lemma 9, showing the application of Lemma 10 with specific parameters. Lemma 11 introduces a covering lemma for a p ball. Lemma 11 introduces a covering lemma for a p ball with specific parameters. By construction, a set of vectors can be found to dominate the elements entry-wise. Lemma 13 bounds the generalization error with specific conditions and probabilities. The generalization error is bounded using specific conditions and probabilities, with Lemma 13 providing bounds for different cases. The proof involves applying a union bound on Lemma 12, with considerations for different norms and constants. Lemma 14 provides specific results for the case p = 2, where the generalization error is bounded for any function f(x) = V[Ux] +. The proof involves upper bounding the generalization bound from Lemma 13 for p = 2 and \u00b5 = 3 \u221a 2 4 \u2212 1. The generalization error is bounded for any function f(x) = V[Ux] +. The proof involves upper bounding the generalization bound from Lemma 13 for p = 2 and \u00b5 = 3 \u221a 2 4 \u2212 1. The proof of Theorem 2 follows from Lemma 14 and uses \u00d5 notation to hide constants and logarithmic factors. Lemma 15 provides a generalization bound for any p \u2265 2, with extra constants and logarithmic factors compared to Lemma 14 for p = 2. Lemma 15 provides a generalization bound for any p \u2265 2, with extra constants and logarithmic factors compared to Lemma 14 for p = 2. The generalization error is bounded for any function f(x) = V[Ux] +, with probability 1 \u2212 \u03b4 over the choice of the training set S = {x i } m i=1 \u2282 R d. Lemma 15 provides a generalization bound for any p \u2265 2, with extra constants and logarithmic factors compared to Lemma 14 for p = 2. The generalization error is bounded for any function f(x) = V[Ux] +, with probability 1 \u2212 \u03b4 over the choice of the training set S = {x i } m i=1 \u2282 R d. For \u00b5 = e p \u2212 1, (\u00b5 + 1) 2/p = e 2. If \u00b5 \u2265 h, N p,h = 0, otherwise ln N p,h is bounded as follows: DISPLAYFORM10 = ( h/(e p \u2212 1) \u2212 1) ln e + e h \u2212 1 h/(e p \u2212 1) \u2212 1 \u2264 ( e 1\u2212p h \u2212 1) ln (eh). The proof of Theorem 5 follows from Lemma 15 and using \u00d5 notation to hide constants and logarithmic factors. The proof of Theorem 3 starts with the case h = d = 2 k, m = n2 k for some k, n \u2208 N. V = \u03b1 = [\u03b1 1 . . . \u03b1 2 k ] for every \u03be, and DISPLAYFORM0, where x i := e i n. The dataset is divided into 2 k groups. The dataset is divided into 2k groups, each with n copies of a different element in the standard orthonormal basis. The matrix U(\u03be) is defined as Diag(\u03b2) \u00d7 F(\u03be), where F(\u03be) has a 2-norm upper bounded by 1. This leads to U(\u03be) 2 \u2264 max i \u03b2 i."
}