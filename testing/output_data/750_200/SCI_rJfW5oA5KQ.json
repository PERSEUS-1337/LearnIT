{
    "title": "rJfW5oA5KQ",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in learning complex real-world distributions, but recent works have highlighted issues such as lack of diversity and mode collapse. Theoretical work by Arora et al. (2017a) suggests a dilemma regarding GANs' statistical properties: powerful discriminators can lead to overfitting, while weak discriminators may not detect mode collapse. In contrast to previous works on GANs, this paper demonstrates that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by using discriminators tailored to specific generator classes. This approach can address issues like lack of diversity and mode collapse in GANs. The Integral Probability Metric (IPM) induced by discriminators can approximate Wasserstein distance and/or KL-divergence, showing that successful training results in distributions close to the true distribution. Preliminary experiments suggest that GANs' lack of diversity may be due to optimization sub-optimality rather than statistical inefficiency. Recent years have seen the empirical success of Generative Adversarial Networks (GANs) in generating high-quality samples across various domains. Several ideas have been proposed to enhance the quality of learned distributions and training stability. However, the understanding of GANs is still evolving, with questions about whether they truly learn the target distribution. Recent work has highlighted concerns that distributions learned by GANs may suffer from mode collapse or lack of diversity, missing significant modes of the target distribution. The paper suggests that designing proper discriminators with strong distinguishing power can alleviate mode collapse, especially against specific families of generators like special subclasses of neural network generators. The paper focuses on alleviating mode collapse in GANs by designing discriminators with strong distinguishing power against specific families of generators. It mainly discusses the Wasserstein GAN formulation and introduces the F-Integral Probability Metric for comparing distributions. The paper addresses mode collapse in GANs by utilizing discriminators with strong distinguishing power against specific generator families. It introduces the F-Integral Probability Metric for comparing distributions and discusses using parametric families like neural networks to optimize objectives via gradient-based algorithms. One main concern with GANs is \"mode collapse,\" where the learned distribution generates high-quality but low-diversity examples. The issue arises from the weaker IPM compared to W 1, allowing the mode-dropped distribution to fool the former. This problem can be mathematically represented by setting q = p N with N = R(F)/\u03b5 2, where R(F) is a complexity measure of F. The issue of mode collapse in GANs is addressed by increasing the strength of the discriminator to larger families like all 1-Lipschitz functions. However, the Wasserstein-1 distance lacks good generalization properties according to Arora et al. (2017a), as the empirical Wasserstein distance used in optimization differs significantly from the population distance. The empirical Wasserstein distance used in optimization lacks good generalization properties, even for distributions like a spherical Gaussian. This poses a dilemma in establishing GAN theories: powerful discriminators lead to overfitting, while weak discriminators result in diversity issues. This paper addresses the dilemma in GAN theories caused by powerful discriminators leading to overfitting and weak discriminators resulting in diversity issues. It proposes designing a strong discriminator class F against a specific generator class G to resolve this conundrum. The paper focuses on the restricted approximability of IPM with a generator class G and data distribution p, aiming to distinguish p and any q \u2208 G as well as all 1-Lipschitz functions. It explores discriminators F that can approximate the Wasserstein distance W1 for the data distribution p. The paper discusses discriminators F that can approximate the Wasserstein distance W1 for the data distribution p and any q \u2208 G, focusing on the realizable case where p \u2208 G. A discriminator class F with restricted approximability resolves the dilemma by avoiding mode collapse. A discriminator class F with restricted approximability avoids mode collapse by ensuring that if the IPM between p and q is small, then p and q are also close in Wasserstein distance. This allows for passing from population-level guarantees to empirical-level guarantees, as long as the capacity of F is bounded. The Rademacher complexity of F relates the statistical properties of Wasserstein GANs, providing insights into diversity, generalization, and distance guarantees. This theoretical framework addresses the statistical theory of GANs with polynomial samples, offering a new perspective on the training process. The paper introduces a theoretical framework for the statistical theory of GANs with polynomial samples. It focuses on designing discriminator class F with restricted approximability for various generator classes, including Gaussian distributions and exponential families. Properly chosen F provides diversity guarantees for different distribution classes. In Section 3, one-layer neural networks with ReLU activations can distinguish pairs of Gaussian distributions, while linear combinations of sufficient statistics can distinguish exponential families. In Section 4, invertible neural networks generate distributions with a special type of neural network discriminator that guarantees restricted approximability. In Section 4, invertible neural networks use a special type of discriminator with an additional layer for restricted approximability. This class of discriminators ensures certain guarantees, including the production of exponentially large modes due to non-linearities. The invertibility assumption, however, limits distributions to the entire space, which may not align with the low-dimensional manifold where natural images are believed to reside. The distribution of natural images is believed to reside on a low-dimensional manifold, which contradicts the assumption that distributions are supported on the entire space. The KL-divergence is not an appropriate measurement of statistical distance when both distributions have low-dimensional supports. The paper focuses on approximating the Wasserstein distance using IPMs. The paper focuses on approximating the Wasserstein distance using IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE approach. The main proof technique involves developing tools for approximating the log-density of a smoothed neural network generator. Synthetic and controlled experiments demonstrate the correlation between IPM and Wasserstein distance. The theory suggests that the test IPM could be an alternative for measuring diversity and quality of learned distributions when KL-divergence or Wasserstein distance is not measurable in complex settings. On real datasets, the optimizer is often tuned to balance learning carefully. The test IPM is considered as an alternative for measuring diversity and quality of learned distributions in complex settings. The lack of diversity in real experiments may be due to sub-optimality of the optimization process. Various empirical tests have been developed to measure diversity, memorization, and generalization in GANs, indicating that lack of diversity is a common issue. Arora et al. (2017a; b) formalized this problem. The lack of diversity is a common issue in GANs, with various proposed solutions such as the \"birthday paradox\" to address mode collapse. Different architectures and algorithms have been suggested to tackle this problem, with some showing promising results. Feizi et al. (2017) demonstrated guarantees when training GANs with quadratic discriminators. Feizi et al. (2017) demonstrated provable guarantees for training GANs with quadratic discriminators using Gaussian generators. Zhang et al. (2017) showed that the IPM is a proper metric under certain conditions and provided a KL-divergence bound with finite samples. Our work in Section 4.1 extends these results by developing statistical guarantees in Wasserstein distance. Our Section 4.1 extends previous work by providing statistical guarantees in Wasserstein distance for distributions like injective neural network generators on low-dimensional manifolds. Liang (2017) discusses GANs in a non-parametric setting, highlighting that the sample complexity improves with generator smoothness, but the derived rate is non-parametric-exponential in dimension unless the target family's Fourier spectrum decays rapidly, which may not be practical. The rate of derivation in non-parametric settings is exponential in dimension unless the Fourier spectrum decays rapidly. The invertible generator structure in Flow-GAN shows that successful GAN training implies learning in KL-divergence when data can be generated by an invertible neural net. This suggests that real data cannot be generated by an invertible neural network. The theory implies that if data can be generated by an injective neural network, the closeness between the learned distribution and the true distribution can be bounded in Wasserstein distance. The notion of IPM includes statistical distances like TV and Wasserstein-1 distance. When F is a class of neural networks, the F-IPM is referred to as the. The F-IPM, or neural net IPM, considers distances between distributions such as TV and Wasserstein-1 distance. Other distances of interest include KL divergence and Wasserstein-2 distance. The Rademacher complexity of a function class is also discussed. The Rademacher complexity of a function class is discussed in relation to the training IPM loss for the Wasserstein GAN. Generalization of the IPM is governed by the quantity R n (F, G), as stated in Theorem 2.1. Theorem 2.1 discusses the generalization of the IPM for Wasserstein GANs. It states that one-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability. One-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees, considering Gaussian distributions with bounded mean and well-conditioned covariance. The discriminators induce IPM W F with restricted approximability w.r.t. G, as shown in Theorem 3.1. The discriminators induce IPM W F with restricted approximability w.r.t. Gaussian distributions in G, bounded by a factor of 1/ \u221a d. The 1/ \u221a d factor is not improvable without using more sophisticated functions than Lipschitz functions of one-dimensional projections of x. The discriminators induce IPM W F with restricted approximability w.r.t. Gaussian distributions in G, bounded by a factor of 1/ \u221a d. The linear combinations of sufficient statistics in exponential families form a family of discriminators with restricted approximability. The discriminator family consists of linear functionals over features T(x). The log partition function log Z(\u03b8) must satisfy certain conditions. If X has diameter D and T(x) is L-Lipschitz in X, then F has a Rademacher complexity bound. The log partition function is always convex, requiring a strictly positive lower bound on the curvature. The log partition function log Z(\u03b8) is always convex, with assumptions requiring a positive lower bound on curvature. Geometric assumptions on sufficient statistics are necessary for the bound eq. (8) due to the intrinsic dependence of the Wasserstein distance on the underlying geometry of x. The proof of eq. FORMULA15 follows standard theory, while the proof of eq. (8) is deferred to Section B.2 for further development. Discriminators with restrictions are designed in this section. In Section 4, discriminators with restricted approximability for neural net generators are designed. The focus is on invertible neural networks generators with proper densities in Section 4.1, and then extended to injective neural networks generators in Section 4.2, where latent variables can have lower dimensions than observable dimensions. In this section, generators parameterized by invertible neural networks are considered. The distributions no longer have densities, and latent variables can have lower dimensions than observable dimensions. The variances can be non-spherical, allowing each hidden dimension to impact the output distribution differently. The case where the standard deviation of hidden factors is [1 k , \u03b41 d\u2212k ] can model data around a \"k-dimensional manifold\" with noise level \u03b4. The invertible neural networks G \u03b8 can model data around a \"k-dimensional manifold\" with noise level \u03b4. The family G consists of standard -layer feedforward nets with invertible parameters. The neural networks G \u03b8 are parameterized by constants \u03c3, \u03b2, \u03b4, and \u03b3, with activation function \u03c3 being twice-differentiable. The standard deviation of hidden factors must satisfy \u03b3 i \u2208 [\u03b4, 1]. The network is invertible, with its inverse also being a feedforward neural net with activation \u03c3 \u22121. The generator networks must meet certain assumptions to ensure effectiveness. The generator networks must meet specific assumptions to ensure effectiveness in implementing pseudo-random functions that cannot be distinguished from random functions by polynomial time algorithms. The family of neural networks with activation functions of at most + 2 layers contains all functions log p \u2212 log q for p, q \u2208 G. The family F of neural networks with activation functions of at most + 2 layers contains all functions log p \u2212 log q for p, q \u2208 G. The proof involves the change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian in G \u22121 \u03b8 involves computing the determinant of weight matrices. This computation is non-trivial, but can be represented by adding a bias on the final output layer. The proof of Lemma 4.1 is deferred to Section D.2. Theorem 4.2 states that if G = {p \u03b8 : \u03b8 \u2208 \u0398} is a set of invertible-generator distributions satisfying Assumption 1, then the discriminator is defined. The proof of Theorem 4.2 involves invertible-generator distributions in G satisfying Assumption 1. The discriminator class F has restricted approximability w.r.t. G. Lemma 4.3 relates KL divergence to IPM for log densities in F. The proof of Theorem 4.2 involves invertible-generator distributions in G satisfying Assumption 1. The discriminator class F has restricted approximability w.r.t. G. Lemma 4.3 relates KL divergence to IPM for log densities in F. For every q \u2208 G, there exists f \u2208 F such that f \u2212 (log p \u2212 log q) \u221e \u2264 \u03b5, and all functions in F are L-Lipschitz. The discriminator class is chosen as in Lemma 4.1, implementing log p \u2212 log q for any p, q \u2208 G. The proof sketch outlines the steps to lower bound the quantity by the Wasserstein distance and upper bound W F (p, q) by the Wasserstein distance. Transportation inequalities by Bobkov-G\u00f6tze and Gozlan are used to establish these bounds. Lemma D.3 states that if X \u223c p (or q) and f is 1-Lipschitz, then f (X) is sub-Gaussian. In the case of an invertible generator with X = G \u03b8 (Z), where Z are independent Gaussians, as long as G \u03b8 is suitably Lipschitz, f (X) is a sub-Gaussian random variable. The upper bound (2) can be achieved by using a truncation argument or ensuring functions in F are Lipschitz globally. Theorem D.2 provides workarounds for achieving immediate bounds in the case where functions in F are not Lipschitz globally. By using a truncation argument or ensuring a linear growth of the Lipschitz constant in x 2, a W 2 bound can be obtained. This extends the result in (Polyanskiy & Wu, 2016) and shows that if training succeeds with small expected IPM, the estimated distribution q is close to the true distribution p in Wasserstein distance. Corollary 4.4 follows from the combination of restricted approximability and generalization bound. With small expected IPM, the estimated distribution q is close to the true distribution p in Wasserstein distance. The training error is measured by Eqm [W F (p n ,q m )], which is a measurable value. Designing efficient algorithms to achieve a small training error is an important open question for future work. In this section, injective neural network generators are considered for generating distributions on a low dimensional manifold, which is more realistic for modeling real images. A novel divergence between two distributions is designed that is sandwiched by Wasserstein distance and can be optimized as IPM. In this section, a novel divergence between two distributions is introduced, sandwiched by Wasserstein distance and optimized as IPM. The key idea is to design a variant of the IPM that approximates the Wasserstein distance. A smoothed F-IPM between distributions p and q is defined, which can be optimized as W F with an additional variable \u03b2. Certain discriminator classes are shown to be effective in this approach. In this section, a novel divergence between two distributions is introduced, optimized as W F with an additional variable \u03b2. Certain discriminator classes are effective in approximating the Wasserstein distance. Theorem 4.5 states that for any pair of distributions p, q \u2208 G, a generalization bound holds when n poly(d). Theorem E.1 defines dependencies on parameters for avoiding mode collapse in neural network generators. The discriminator family F must have restricted approximability with respect to the generator family G to prevent mode collapse. Specific discriminator classes are designed to guarantee this. In GAN training, specific discriminator classes are designed to ensure restricted approximability with the generator family to prevent mode collapse. Synthetic experiments confirm that the practice aligns with theory, showing correlation between IPM and Wasserstein / KL divergence. In GAN training, specific discriminator classes are designed to prevent mode collapse by ensuring restricted approximability with the generator family. Synthetic experiments confirm a correlation between IPM and Wasserstein / KL divergence, suggesting that the optimization difficulty, rather than statistical inefficiency, may be the main challenge in GAN training. In synthetic experiments with GANs, invertible neural net generators are trained with discriminators of restricted approximability. The IPM is found to be correlated with the KL divergence, indicating that optimization difficulty, not statistical inefficiency, is the main challenge in GAN training. In synthetic experiments with WGANs, GANs are trained to learn the unit circle and a \"swiss roll\" curve in two dimensions. The Wasserstein distance is used to measure the quality of the learned generator, showing strong correlation with IPM W F. The ground truth distributions are not covered in Theorems 4.2 and 4.5, but evidence suggests restricted approximability is likely to hold. The ground truth distributions are set as a unit circle or Swiss roll curve, sampled from DISPLAYFORM0 Generators and discriminators using standard two-hidden-layer ReLU nets. The architectures for the generator and discriminator are 2-50-50-2 and 2-50-50-1 respectively. The RMSProp optimizer is used with learning rates of 10^-4 for both, and 10 steps are performed on the discriminator between each generator step. In 2012, the learning rates for the generator and discriminator are both 10^-4, with 10 steps on the discriminator between each generator step. Two metrics are compared between the ground truth distribution p and the learned distribution q during training: the neural net IPM WF(p, q) and the Wasserstein distance W1(p, q). The empirical Wasserstein distance W1(p, q) is used as a proxy for the true Wasserstein distance. The empirical Wasserstein distance W1(p, q) is a good proxy for the true Wasserstein distance W1(p, q). In experiments, the learned generator closely matches the ground truth distribution at iteration 10000. The neural net IPM and Wasserstein distance show a strong correlation. However, at iteration 500, the generators have not fully learned the true distributions yet, resulting in large IPM and Wasserstein distances. Sample complexity bounds for learning are also discussed. The first polynomial-in-dimension sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence are presented. Discriminators with restricted approximability are designed to avoid mode collapse and improve generalization. These techniques may be extended to other distribution families in the future. The techniques presented aim to improve generalization and avoid mode collapse in GANs by designing discriminators with better restricted approximability bounds. The goal is to extend these methods to other distribution families with tighter sample complexity bounds, exploring and generalizing approximation theory results in the context of GANs. The techniques aim to enhance generalization and prevent mode collapse in GANs by creating discriminators with improved restricted approximability bounds. The upper bound W F (p1, p2) \u2264 W1(p1, p2) is derived from the 1-Lipschitz property of functions in F, while the lower bound is established by recovering the mean distance using linear discriminators as the sum of two ReLU discriminators. The lower bound is established by recovering the mean distance using linear discriminators as the sum of two ReLU discriminators. The neuron distance between the two Gaussians is computed, showing that at least one term is greater than a certain value. The function R is strictly increasing, with R(0) = 1/ \u221a 2\u03c0. By manipulating v, we can compare different values of v. Using perturbation bounds, we can bound a quantity in the supremum, leading to a final expression involving the W 2 distance. The lower bound is derived by combining bounds in mean difference and using the W 2 distance between two Gaussians. The W 2 distance bridges the KL and F-distance, with the goal of bounding the growth of \u2207 log p 1 (x) 2. The upper bound involves bounding the growth of \u2207 log p 1 (x) 2 and log p 2. By the Rademacher contraction inequality, we can bound the right hand side directly. The exponential family properties and assumption on \u2207 2 A are also considered. Proof of Theorem 3.2 involves KL bounds and Wasserstein bounds. The exponential family property and assumption on \u2207 2 A are used to show the bounds. Rademacher complexity is computed for generalization. The Rademacher complexity is computed for generalization in the context of learning mixture of k Gaussians using a one-hidden-layer neural network. The Gaussian concentration result is utilized in the proofs. The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) is used in the proofs for generalization in learning mixture of k Gaussians with a one-hidden-layer neural network. The upper bound is shown by proving each function is D-Lipschitz, while the lower bound is established using the KL divergence implementation. The KL divergence is used to analyze the regularity properties of distributions in the context of learning mixture of Gaussians with a neural network. By applying Gaussian concentration and Rademacher complexity, the generalization of the neural network model is established. The Rademacher complexity of a one-hidden-layer neural net is bounded for Lipschitz continuity in \u03b8. The expected supremum over a covering set is analyzed using a discretization bound. The covering number is upper bounded by the product of separate coverings. The covering number of each \u00b5 i , c j separately can be upper bounded by the product of separate coverings. For each process Y \u03b8, the i.i.d. average of random variables is \u03b5 i log k j=1 exp(\u00b5 j X+c j ). The log-sum-exp part is D-Lipschitz in X, allowing for reuse of analysis from the Bobkov-Gotze part. This shows that the term \u03b5 i log is 1)-subGaussian, leading to sub-Gaussian maxima bounds. By the 1-step discretization bound and combining equations, choosing \u03b5 = c/n for small c gives Theorem D.2 (Upper. Choosing \u03b5 = c/n for sufficiently small c gives an upper bound on f-contrast by Wasserstein. Let p, q be distributions on Rd with positive densities. If certain conditions are met, then a bound on Wasserstein distance can be obtained. The proof involves a truncation argument and bounding terms using Cauchy-Schwarz inequality. It also extends a previous proposition by Polyanskiy & Wu (2016). The proof presented here extends Proposition 1 by Polyanskiy & Wu (2016) by using a truncation argument and Cauchy-Schwarz inequality. It involves representing log p \u03b8 (x) with a neural network and computing the inverse of x = G \u03b8 (z) using a feedforward net with activation \u03c3 \u22121. The problem of representing log p \u03b8 (x) with a neural network is considered, using a feedforward net with activation \u03c3 \u22121. The inverse network implementing G \u22121 \u03b8 has layers, parameters, and \u03c3 \u22121 activation. By adding branches, the log determinant of the Jacobian can also be computed. By adding branches to the network, the log determinant of the Jacobian can be computed. This results in a neural network that computes log p \u03b8 (x) with no more than + 1 layers and O( d 2 ) parameters, using a choice of activations within {\u03c3}. By adding branches to the network, the log determinant of the Jacobian can be computed, resulting in a neural network that computes log p \u03b8 (x) with no more than + 1 layers and O( d 2 ) parameters, using a choice of activations within {\u03c3}. The theorem follows by combining lemmas showing a lower bound and the Gozlan condition for any \u03b8 \u2208 \u0398. The Gozlan condition is satisfied for any \u03b8 \u2208 \u0398, and the network G \u03b8 is also L-Lipschitz. The random variable is L 2 -sub-Gaussian, satisfying the Gozlan condition with \u03c3 2 = L 2. The mapping is L-Lipschitz, and for any \u03b8 1 , \u03b8 2 \u2208 \u0398, we have an upper bound on W F. The Gozlan condition is satisfied for any \u03b8 \u2208 \u0398, and the network G \u03b8 is also L-Lipschitz. The random variable is L 2 -sub-Gaussian, satisfying the Gozlan condition with \u03c3 2 = L 2. The mapping is L-Lipschitz, and for any \u03b8 1 , \u03b8 2 \u2208 \u0398, we have an upper bound on W F. When swapping p \u03b81 and p \u03b82, we obtain a bound on Wasserstein distances through Theorem D.2. The Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 needs to be upper bounded. The Lipschitz property of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 needs to be bounded. Each layer h k is C(R W , R b , k)-Lipschitz in x. The bound eq. (22) implies a tail bound for X 2. By choosing D = K \u221a d, the tail bound P( X 2 \u2265 D) \u2264 exp(\u221220d) is obtained. The tail bound P(X^2 \u2265 D) \u2264 exp(\u221220d) is obtained by bounding the Lipschitz property of log p\u03b8(x) for all \u03b8 \u2208 \u0398. Reparameterizing the log-density neural network allows for the weights and biases to belong to a specific set, leading to a bound on the Rademacher complexity of the network. The Rademacher complexity of the network F is at most two times a certain quantity. A re-parametrization is done to simplify the log-density network. The constant C(\u03b8) includes the normalizing constant for Gaussian density and the sum of log det(W i ). A parameter K is introduced to handle this term. A metric is defined for any reparametrized \u03b8. The Rademacher complexity of the network F is bounded by a certain quantity. A re-parametrization simplifies the log-density network by introducing a parameter K. A metric is defined for any reparametrized \u03b8. Lemmas D.6 and D.7 provide bounds on discretization error and expected max over a finite set. Lemma D.7 provides bounds on the expected maximum over a finite set, with constants \u03bb 0 and C. Substituting these bounds into the equation, for \u03b5 \u2264 min {R W , R b } and \u03bb \u2264 \u03bb 0 \u03b4 2 n, the generalization error is dominated by the term n log n/\u03b4 4 when n/ log n \u2265 \u03b4. This term is crucial in showing the empirical average over n samples. The inverse network G \u22121 is denoted by the k-th hidden layer h k (x). The inverse network G \u22121 is denoted by the k-th hidden layer h k (x). It suffices to show Lipschitzness of hidden layers for all k, with bounds verified through induction. The claim holds for the k-th layer, verifying the result for layer k. Lipschitzness and quadratic term bounds are combined to show tail decay at a single \u03b8. The random variable is proven to be sub-exponential by examining a single x and using rules for independent sums. The random variable DISPLAYFORM32 is sub-exponential, proven by examining a single x and using rules for independent sums. DISPLAYFORM33 is sub-Gaussian with mean O(Cd). The term h, A \u03b3 h, is a quadratic function of a sub-Gaussian random vector and is subexponential with a mean bounded by Cd/\u03b4^2. The term h, A \u03b3 h, is a quadratic function of a sub-Gaussian random vector, making it subexponential with a mean bounded by Cd/\u03b4^2. The parameter K is upper bounded, and Y \u03b8 is shown to be mean-zero sub-exponential with a MGF bound. The expected maximum of Y \u03b8 is bounded using a covering argument, with the covering number of \u0398 being bounded by the product of independent covering numbers. Jensen's inequality is used to show that for any \u03bb \u2264 \u03bb 0 \u03b4 2 n, a certain bound holds. The theorem quantitatively specifies relevant quantities of the generator class. The theorem quantitatively specifies relevant quantities of the generator class, including a truncated version of the convolution of p and a Gaussian distribution to obtain p \u03b2 \u03b8. This distribution adds Gaussian noise with variance \u03b2 2 to a sample from G \u03b8 and truncates it to a high-probability region in both the latent variable and observable domain. The text discusses introducing regularity conditions for the family of generators G, including bounds on partial derivatives of f and defining various quantities such as S, \u03bb min, t(z), T, and R. The text introduces regularity conditions for the generator class G, defining quantities like S, \u03bb min, t(z), T, and R. It presents a theorem stating that for certain F,d, F approximates the Wasserstein distance. The text discusses regularity conditions for the generator class G and introduces quantities like S, \u03bb min, t(z), T, and R. It presents a theorem showing that for certain F,d, F approximates the Wasserstein distance. Theorem E.1 states that for every p, q \u2208 G, DISPLAYFORM49. The main ingredient in the proof involves a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G, as shown in Theorem E.2. The text discusses regularity conditions for the generator class G and introduces quantities like S, \u03bb min, t(z), T, and R. It presents a theorem showing that for certain F,d, F approximates the Wasserstein distance. Theorem E.1 states that for every p, q \u2208 G, DISPLAYFORM49. The main ingredient in the proof involves a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G, as shown in Theorem E.2. The assumptions in Assumption E.1 state that for \u03b2 = O(poly(1/d)), there exists a family of neural networks F of size poly(1/\u03b2, d) such that for every distribution p \u2208 G, there exists N \u2208 F satisfying: (1) N approximates log p for typical x, (2) N is globally an approximate lower bound of p, and (3) N approximates the entropy. The approach involves approximating p \u03b2 (x) using Laplace's method of integration. The proof of Theorem E.1 involves neural networks N1 and N2 from family F approximating log densities of p\u03b2 and q\u03b2 respectively. The proof relies on a variant of Laplace's method of integration and a greedy \"inversion\" procedure to calculate maximum values. The proof of Theorem E.1 involves neural networks N1 and N2 from family F approximating log densities of p\u03b2 and q\u03b2. By Theorem E.2, N1 and N2 approximate log p\u03b2 and log q\u03b2 respectively. The lower bound is proven using Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W 1/6. The optimal coupling C of p, q is considered for the claim. The proof of Theorem E.1 involves neural networks N1 and N2 approximating log densities of p\u03b2 and q\u03b2 from family F. By setting \u03b2 = W 1/6, the necessary bound is obtained. The claim considers the optimal coupling C of p, q and the induced coupling Cz on the latent variable z in p, q. The rest of the section is dedicated to the proof of Theorem E.2. The proof of Theorem E.2 involves helper lemmas and induction to produce estimates iteratively. The claim trivially holds for i = 0, and the proof proceeds by reverse induction on l. The proof involves induction to produce estimates iteratively. The claim holds for i = 0, and by induction, we show that it holds for i as well. The size/Lipschitz constant of the neural network is determined by the (unnormalized) cdf of a Gaussian with covariance. The algorithm presented approximates the integral using a small, Lipschitz network and can be implemented efficiently. The proof involves induction to produce estimates iteratively, with the claim holding for i = 0 and extending to all i. The size/Lipschitz constant of the neural network is determined by the (unnormalized) cdf of a Gaussian with covariance. The algorithm efficiently approximates the integral using a small, Lipschitz network. It involves calculating gradients and finding the nearest matrix with separated eigenvalues. The algorithm efficiently approximates the integral using a small, Lipschitz network by calculating gradients and finding the nearest matrix with separated eigenvalues. The algorithm selects the smallest i that satisfies the separation condition and calculates approximate eigenvector/eigenvalue pairs of H + E i. It uses an approximate version of Lemma E.8 and defines the set B based on a net S. Lemma E.4 states that matrices E i are chosen from a \u03b2 2 -net S with spectral norm bounded by O(1/\u03b2 2). There exist matrices E 1 , E 2 , ..., E r such that if M \u2208 S, at least one of the matrices M + E i has eigenvalues that are \u2126(\u03b2)-separated. The proof of Theorem E.9 shows that the approximation serves the purpose, with an integral that can be evaluated similarly. The approximation in the curr_chunk serves the purpose, with an integral that can be evaluated similarly to Theorem E.9. Synthetic WGAN experiments are performed with invertible neural net generators and discriminators designed with restricted approximability. The goal is to demonstrate the correlation between empirical IPM W F (p, q) and the KL-divergence on synthetic data. The data is generated from a ground-truth invertible neural net generator using Leaky ReLU activation function. The goal is to show the correlation between empirical IPM W F (p, q) and KL-divergence on synthetic data. The Leaky ReLU activation function with negative slope 0.5 is used in a feedforward net with a spherical Gaussian Z. Weight matrices are well-conditioned with singular values between 0.5 to 2. The discriminator architecture is chosen based on restricted approximability guarantee. A trainable one-hidden-layer neural network is used to model the non-differentiable function log \u03c3 \u22121. Constraints are added to all parameters. Training involves generating data for the generator and discriminator networks. The neural network maps reals to reals with constraints on parameters. Training involves generating batches for the generator and discriminator networks, solving the min-max problem in Wasserstein GAN formulation. The RMSProp optimizer is used for updates. Evaluation metrics include KL divergence between true and learned generator. The evaluation metrics for the true and learned generator include the KL divergence and the training loss (IPM W F train). The KL divergence is considered a strong criterion for distributional closeness, while the training loss is the unregularized GAN loss during training. During training, the unregularized GAN loss is used to balance the number of steps for discriminator and generator. The neural net IPM evaluates a separately optimized WGAN loss with no regularization for the discriminator. This approach aims to find the generator that maximizes contrast. The discriminator is trained in norm balls without any other regularization to find f \u2208 F that maximizes contrast. The theory shows that WGAN can learn the true generator in KL divergence, and F-IPM is indicative of the KL divergence. In the first experiment, G is a two-layer net in d = 10 dimensions, making the estimation problem non-trivial due to the presence of nonlinearity. The generator in a two-layer net in d = 10 dimensions is trained with a shallow neural net. The discriminator is trained using Vanilla WGAN or WGAN-GP with different initializations. Results show that WGAN training with a restricted approximability discriminator design can learn the true distribution in KL divergence, with the best run achieving a KL lower than 1. The discriminator design of restricted approximability allows GANs to learn the true distribution in KL divergence, with the best run achieving a KL lower than 1. The W F (eval) and KL divergence are highly correlated, and adding gradient penalty improves optimization significantly. W F can serve as a good metric for monitoring convergence. The quantity W F can serve as a good metric for monitoring convergence, as shown by the KL curve. Results with vanilla discriminators also correlate well with the KL-divergence, indicating the effectiveness of the discriminator design. The KL-divergence between the true distribution p and learned distribution q at different training steps correlates well with the estimated IPM in evaluation. The inferior performance of the WGAN-Vanilla algorithm in KL does not come from statistical properties. In this synthetic case, the inferior performance of the WGAN-Vanilla algorithm in KL divergence is not due to statistical properties but rather the convergence of the IPM during training. The correlation between p and its perturbations is directly tested by comparing KL divergence and neural net IPM on pairs of perturbed generators. In a synthetic case, the WGAN-Vanilla algorithm's inferior performance in KL divergence is attributed to the convergence of the IPM during training. The correlation between generators and their perturbations is tested by comparing KL divergence and neural net IPM. A clear positive correlation is observed between the KL divergence and neural net IPM, with points falling around the line W F = 100D kl, indicating a linear scaling in the neural net distance. In a synthetic case, the WGAN-Vanilla algorithm's inferior performance in KL divergence is attributed to the convergence of the IPM during training. Majority of points fall around the line W F = 100D kl, indicating a linear scaling in the neural net distance. Outliers with large KL are due to perturbations causing poorly conditioned weight matrices. Experiments with vanilla fully-connected discriminator nets show a positive correlation between KL divergence and neural net IPM. The three-layer net with hidden dimensions 50-10 has more parameters than the architecture with restricted approximability. Results in FIG7 show that generators converge well in KL divergence, but correlation is slightly weaker. Vanilla discriminator structures may be satisfactory for a good generator, but specific designs could improve the quality of the distance W F. The left-most figure displays the KL divergence between true distribution p and learned distribution q during training. Specific designs could enhance the quality of the distance W F, as shown in the figures displaying KL divergence, estimated IPM, and training loss. The correlation between KL and neural net IPM is consistent, regardless of the type of discriminator used."
}