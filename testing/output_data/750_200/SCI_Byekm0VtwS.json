{
    "title": "Byekm0VtwS",
    "content": "Uncertainty is a key feature in intelligence, making the brain flexible and powerful. Crossbar-based neuromorphic computing chips, which use analog circuits, have uncertainty and can mimic the brain. However, current deep neural networks do not consider this uncertainty, leading to lower performance on neuromorphic chips compared to CPUs/GPUs. A proposed uncertainty adaptation training scheme (UATS) aims to address this issue by incorporating uncertainty into the neural network training process. The uncertainty adaptation training scheme (UATS) incorporates uncertainty into neural network training on neuromorphic computing chips, improving performance compared to traditional platforms. Uncertainty reasoning is essential for human intelligence, with fuzziness and stochasticity being key aspects in intelligent systems. The fuzziness and stochasticity are two types of uncertainties in intelligent systems. Fuzziness helps the brain efficiently deal with the real world by ignoring redundant information, while stochasticity enables creativity and adaptability in unfamiliar situations. These characteristics are lacking in most current artificial intelligence systems. The characteristics of fuzziness and stochasticity are essential for intelligent systems, lacking in current AI systems. Researchers have found that 8-bit integers are sufficient for many applications, despite the common use of 32-bit or 64-bit floating numbers. Methods like network quantization and Bayesian networks address issues in AI systems. Neuromorphic computing chips offer hardware solutions. In recent years, nanotechnology and crossbar structure based neuromorphic computing chips have advanced significantly, offering hardware solutions to address uncertainties in Deep Neural Networks (DNN). The crossbar structure, efficient for vector-matrix multiplication (VMM) due to Ohm's and Kirchhoff's laws, utilizes nanoscale nonvolatile memory (NVM) devices for additional storage capacity. The crossbar structure with nanoscale nonvolatile memory devices provides additional storage capacity for vector-matrix multiplication (VMM) in neuromorphic computing chips. This computing in memory (CIM) architecture can alleviate memory bottlenecks in von Neumann architecture, making neuromorphic computing more energy and area efficient for AI applications. The uncertainty in neuromorphic computing chips arises from fuzziness due to analog to digital converters (ADCs) and stochasticity from NVM devices. The uncertainty in computing chips is caused by analog to digital converters (ADCs) and stochasticity from NVM devices. The VMM result is a summarization of currents, converted by ADC to digital voltages for data transfer. The stochasticity of NVM devices is due to random particle movement, causing varied conductance and different output currents. The stochasticity of NVM devices, caused by random particle movement, leads to varied conductance and different output currents. A training scheme utilizing this stochasticity is proposed to enhance the performance of neuromorphic computing chips. Various types of NVM devices are available, including phase change memories, filamentary migrating oxide devices, and ferroelectric tunnel junction synapses. Various types of NVM devices exhibit different stochasticity due to varying physical mechanisms. The Gaussian distribution is used to model device stochasticity, with a stable state and lower probability farther away from it. The mean of the distribution represents the conductance value of the stable state. The Gaussian distribution is used to model device stochasticity, with the mean representing the conductance value of the stable state. The variance is typically correlated to the mean, and the standard deviation is assumed to be linearly and positively related to the mean. Conductances below a certain value are not considered. The conductance of devices is modeled using a Gaussian distribution, with the mean representing the stable state conductance. Conductances below a minimum value are cut off. Stochastic conductance is denoted by Gs, sampled from N(\u00b5, \u03c3^2) with a mean of \u00b5 and variance of \u03c3^2. Device fuzziness is a result of the writing process. When using neuromorphic computing chips for AI applications, writing the conductance of each device in the crossbar is crucial. The target conductance of each device is determined based on the weights of the neural network through a mapping process. The difference between two devices' conductances is used to express a weight, which can be positive or negative. Lower device conductance values lead to higher energy efficiency. To express weights in neuromorphic computing chips, the difference between device conductances is used. Lower conductance values improve energy efficiency. However, accuracy is affected by device stochasticity and circuit fuzziness. The conductance of the device is affected by stochasticity and fuzziness, leading to inaccurate measurements. A model using Gaussian distribution is used to describe the fuzziness, with G representing the sampled value from the distribution. In neuromorphic computing, fuzzy target conductance Gf is determined by a Gaussian distribution with mean \u00b5 and variance \u03c3^2. Uncertainty in DNN programming on chips can decrease classification accuracy, but this can be mitigated by the proposed methods. The uncertainty in DNN programming can decrease classification accuracy, but can be alleviated by using the proposed uncertainty adaptation training scheme (UATS) which introduces stochasticity in the feed forward process to guide neural networks in learning how to handle uncertainty. The uncertainty adaptation training scheme (UATS) introduces stochasticity in the feed forward process to guide neural networks in handling uncertainty. During training, weights are replaced by a sample of random variable w f, which impacts the conductances G pf and G nf according to a fuzziness model. The uncertainty adaptation training scheme (UATS) introduces stochasticity in the feed forward process to guide neural networks in handling uncertainty. The target conductances G ptarget and G ntarget are calculated based on variable w f. The loss function is calculated by the average output of n FF processes with the same input batch, not just one. The effectiveness of UATS was evaluated on multiple models and datasets. The uncertainty adaptation training scheme (UATS) introduces stochasticity in the feed forward process to guide neural networks in handling uncertainty. The effectiveness of UATS was evaluated on multiple models and datasets, including the MNIST dataset with different models such as multilayer perceptron (MLP) and convolutional neural network (CNN). The models were trained using specific conductance values and tested on a set of 10,000 images to calculate the test error. The models were trained with specific conductance values and tested on 10,000 images to calculate test error. Different uncertainty levels were applied, showing that without using the UATS, uncertainty increased test errors for both MLP and CNN models. The uncertainty levels affected test errors of MLP and CNN models, with CNN model (LeNet-5) showing the best performance without uncertainty but being most affected by it. The 'mlp2' model was more robust to uncertainty than 'mlp1'. UATS was used to tune weights and retrain models, with k=5, n=5 for fine-tuning and 25 epochs. UATS was utilized to tune weights and retrain models, with k=10, n=5 for retraining and 100 epochs. Results showed that UATS significantly improved accuracies with consistent uncertainty levels in both experiments. Retraining outcomes were generally better than fine-tuning using UATS, especially at low uncertainty levels. Additionally, UATS was validated on the CIFAR-10 dataset with a ResNet-44 DNN model, demonstrating its effectiveness. The study validated the effectiveness of UATS on the CIFAR-10 dataset with a ResNet-44 DNN model, achieving lower error rates than ideal cases with proper hyper-parameters. UATS acts as a regularization method, particularly beneficial for DNNs with more layers, making training easier by leveraging uncertainty in the system. The Bayesian network is a useful method for building uncertain neural networks, but it requires controllable weight distributions. Neuromorphic computing chips struggle with this as weight distribution is device-determined. While methods exist to manipulate conductance distribution, they are not as convenient as UATS, which requires no additional circuit. Various distributions, like Laplacian and uniform, have been explored to model device stochasticity. The VMM can model device stochasticity using different distributions like Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli. Despite the varied behavior, network performance remains similar when distributions have the same mean and variance. The VMM transforms individual device distributions into a summary of random parameters, leading to similar network performance with different distributions. To reduce the computation intensity of UATS, methods like sampling weights for inputs or batches instead of every VMM can be used. Using the uncertainty model of VMM results instead of weights can accelerate simulation speed while achieving similar results."
}