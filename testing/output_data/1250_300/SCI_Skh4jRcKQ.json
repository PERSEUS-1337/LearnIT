{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified by showing that the expected coarse gradient, obtained through the modified chain rule, correlates positively with the population gradient. This makes its negation a descent direction for minimizing the population loss in neural network training. The deployment of Deep Neural Networks (DNN) requires significant memory and computational resources. Recent efforts focus on training coarsely quantized DNN to achieve memory savings and energy efficiency during inference. The choice of Straight-Through Estimator (STE) impacts the stability of the training algorithm and convergence to critical points. Poor STE selection can lead to instability near local minima, as demonstrated in CIFAR-10 experiments. Recent efforts have been made to train coarsely quantized Deep Neural Networks (DNN) for memory savings and energy efficiency during inference. Weight quantization of DNN has been extensively studied in the literature, while training fully quantized DNN poses a challenging optimization problem. The gradient in training activation quantized DNN is almost zero, requiring a non-trivial search direction to address this issue. Training activation quantized DNNs presents challenges due to the near-zero gradient, making standard back-propagation inapplicable. To address this, a non-trivial search direction is needed, such as using the straight-through estimator (STE) or stochastic neurons. Other approaches include the target propagation algorithm for learning hard-threshold networks. The idea of STE traces back to the perceptron algorithm from the 1950s. The perceptron algorithm, developed in the 1950s, uses a modified chain rule for learning single-layer perceptrons. It has been extended to train multi-layer networks with binary activations and variants like the saturated STE for DNN training with quantized ReLU activations. The saturated Straight-Through Estimator (STE) is used in the backward pass for training Deep Neural Networks (DNN) with quantized ReLU activations. Various proxies, including derivatives of vanilla ReLU and clipped ReLU, have been employed. Limited theoretical understanding exists for training DNN with stair-case activations using STE. Different scenarios where certain layers are not ideal for back-propagation have been discussed, such as using an implicit weighted nonlocal Laplacian layer as a classifier for improving generalization accuracy. In the context of training DNN with quantized ReLU activations, the use of Straight-Through Estimator (STE) in the backward pass is discussed. The derivative of a pre-trained fully connected layer is used as a surrogate in the backward pass to improve generalization accuracy. The concept of coarse gradient is introduced, highlighting the non-uniqueness of STE and the importance of understanding its optimization perspective. In the optimization perspective, understanding Straight-Through Estimators (STE) in training quantized ReLU nets is explored. Three representative STEs for learning a two-linear-layer network with binary activation and Gaussian data are considered. Proper choices of STE lead to descent training algorithms, with negative expected coarse gradients of vanilla and clipped ReLUs being descent directions for minimizing population loss. The identity STE can result in unstable training near certain local minima. The identity Straight-Through Estimator (STE) can lead to unstable training near certain local minima. Empirical performances of different STEs on MNIST and CIFAR-10 show that clipped ReLU STE performs best for deeper networks like VGG-11 and ResNet-20. Training with identity or ReLU STE can be unstable, leading to inferior minima with higher loss and decreased accuracy. Convergence guarantees have been proven for the identity STE in perceptron and Convertron algorithms. The Convertron algorithm makes weaker assumptions than the identity STE, but results do not generalize to networks with two trainable layers. The monotonicity of quantized activation functions plays a role in gradient descent, with clipped ReLU matching quantized ReLU at the extrema to avoid instability issues. The study focuses on the energy landscape of a two-linear-layer network with binary activation and Gaussian data, comparing empirical performances of different STEs in activation quantization. The study compares the empirical performances of different Straight-Through Estimators (STEs) in activation quantization for a two-linear-layer network with binary activation and Gaussian data. Technical proofs and figures are deferred to the appendix due to space limitations. Key notations and the model architecture are outlined, with details on the trainable weights and activation function. The study examines the performance of Straight-Through Estimators (STEs) in activation quantization for a two-linear-layer network with binary activation and Gaussian data. The model architecture includes trainable weights and an activation function. The first layer acts as a convolutional layer, while the second layer serves as the classifier. The label generation and loss function are defined, with assumptions made about the Gaussian distribution of the input data. The study focuses on using Straight-Through Estimators (STEs) for activation quantization in a two-linear-layer network with binary activation and Gaussian data. The approach involves replacing a component in the gradient with a non-trivial function \u00b5 to train the network effectively. The study utilizes Straight-Through Estimators (STEs) for activation quantization in a two-linear-layer network with binary activation and Gaussian data. It introduces a non-trivial function \u00b5 in the gradient for effective network training. The coarse gradient descent for learning the CNN with STE \u00b5 is described, along with preliminaries about the population loss function and analytic expressions of f (v, w) and \u2207f (v, w). The population loss function f (v, w) is detailed for w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0), with insights on possible local minimizers of the model. The study focuses on utilizing Straight-Through Estimators (STEs) for activation quantization in a network with binary activation and Gaussian data. It introduces a function \u00b5 in the gradient for effective training and discusses the population loss function f (v, w) for different scenarios. The main results highlight the presence of saddle points and spurious local minimizers in the complex case. The study explores the use of Straight-Through Estimators (STEs) for activation quantization in a network with binary activation and Gaussian data. It focuses on the behaviors of coarse gradient descent in the presence of saddle points and spurious local minimizers. The main results show that using the derivative of vanilla or clipped ReLU in Algorithm 1 leads to convergence to a critical point, while using the identity function does not. Theorem 1 states that under certain conditions, the objective sequence is monotonically decreasing and converges to a saddle point or local minimizer of the population loss minimization. The study focuses on the behaviors of coarse gradient descent in the presence of saddle points and spurious local minimizers. The convergence guarantee for the descent is established under the assumption of infinite training samples. As sample size increases, the empirical loss gains monotonicity and smoothness, explaining the effectiveness of Straight-Through Estimators with large datasets in deep learning. The results hold even if the Gaussian assumption on input data is weakened. The mathematical analysis in this section outlines the key results for different sample sizes. The study shows that the coarse gradient descent direction is effective in minimizing the population loss, especially when the input data assumption is relaxed. The expected coarse gradient has a non-negative correlation with the population gradient, forming a descent direction for optimization. The study demonstrates that the coarse gradient descent direction effectively minimizes the population loss, with a non-negative correlation between expected coarse and population gradients forming a descent direction for optimization. The algorithm converges to critical points of the population loss function, ensuring monotonically decreasing energy until convergence. The study shows that Algorithm 1 converges to critical points of the population loss function, with the coarse gradient vanishing only at these points. The coarse gradient descent direction has a positive correlation with the true gradient, forming a descent direction for optimization. Lemma 8 states that Algorithm 1 converges when the coarse gradient and the crelu gradient vanish simultaneously at saddle points. However, Lemma 9 suggests that the coarse gradient descent may not converge near spurious minimizers. The text discusses the issue of spurious minimizers and instability in training algorithms when using improper STEs for quantized activations. It compares the performances of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks. The experiment results show differences in empirical performances on deeper nets. The resolution \u03b1 for quantized ReLU needs careful selection to avoid instability. In experiments, weights are kept float, and resolution \u03b1 for quantized ReLU is crucial. A modified batch normalization layer is used to determine \u03b1, which remains fixed during training. The original LeNet-5 is augmented with batch normalization. The quantization approach used is similar to HWGQ, with uniform quantization. Stochastic gradient descent with momentum = 0.9 is the optimizer for all experiments. The quantization approach used in the experiments is similar to HWGQ, with uniform quantization. Stochastic gradient descent with momentum = 0.9 is the optimizer for all experiments. Training details for LeNet-5, VGG-11, and ResNet-20 on MNIST and CIFAR-10 datasets are provided, along with the schedule of the learning rate in the appendix. Experimental results are summarized in Table 1, showing training losses and validation accuracies for different STEs. Among the three STEs, the derivative of clipped ReLU performs the best overall, followed by vanilla ReLU and then the identity function. The study compares the performance of different activation functions on shallow and deep neural networks. While vanilla ReLU performs well on shallow networks, the identity function leads to instability on deeper networks. Training with clipped ReLU results in higher validation accuracies. Switching to the identity function during training leads to a significant increase in training loss and validation error. The study compares the performance of different activation functions on shallow and deep neural networks. Training with a tiny learning rate of 10^-5 initially leads to an increase in training loss and validation error within the first 20 epochs. Switching to a normal learning rate schedule at epoch 20 improves training speed. The identity STE results in a worse minimum due to coarse gradients, while ReLU STE shows instability at good minima. The concept of STE is justified theoretically, with three types considered: identity function, vanilla ReLU, and clipped ReLU. The study compared different activation functions on neural networks, showing that using a tiny learning rate initially led to increased loss and errors. The concept of STE was justified theoretically, with three types considered: identity function, vanilla ReLU, and clipped ReLU. The identity STE resulted in worse minima, while ReLU STE showed instability. Further work aims to understand coarse gradient descent for large-scale optimization problems. Figure 4 shows that using the weights from clipped ReLU STE on ResNet-20 with 2-bit activations led to instability in coarse gradient descent. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors. Lemma 12 further explores Gaussian random vectors and angles, showing specific identities and properties. Lemma 13 discusses inequalities involving angles and random vectors, while Lemma 14 explores projections and angles between vectors. Lemma 1 states that if w = 0n, the population loss f(v, w) can be calculated. Lemma 2 discusses partial gradients of f(v, w) with respect to v and w when w = 0n and \u03b8(w, w*) \u2208 (0, \u03c0). The text also touches on saddle points and local optimality of stationary points. The text discusses the local optimality of stationary points, showing that the stationary points are saddle points. It also demonstrates that perturbed objective values increase when moving away from the minimizer, supporting the claim. Additionally, it introduces Lemma 3, which establishes Lipschitz continuity for differentiable points. Lemma 4 states the expected partial gradient of (v, w; Z) with respect to v and the expected coarse gradient with respect to w. The proof involves invoking various Lemmas and defining terms to validate the claims. Additionally, Lemma 5 discusses the inner product between expected gradients when certain conditions are met. Lemma 5 states that if specific conditions are met, the inner product between expected gradients can be determined. The proof involves invoking Lemmas and defining terms to support the claim. Lemma 7 provides expressions for v and \u03b8(w, w*) under certain conditions, along with a proof involving calculations and inequalities. Lemma 9 states that the expected coarse partial gradient with respect to w is calculated using a specific formula. Lemma 10 discusses the inner product between the expected coarse and true gradients with respect to w under certain conditions."
}