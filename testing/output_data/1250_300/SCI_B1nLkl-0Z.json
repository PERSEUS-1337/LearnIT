{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions (Q-values) are fundamental in reinforcement learning, with algorithms like SARSA and Q-learning. A new concept of action value is introduced, defined by a Gaussian smoothed version of the expected Q-value in SARSA. Smoothed Q-values still satisfy a Bellman equation, making them learnable from experience. Gradients of expected reward can be derived from the gradient and Hessian of the smoothed Q-value function, enabling new algorithms for training a Gaussian policy directly from a learned Q-value approximator. This approach, allowing learning of both mean and covariance, achieves strong results on continuous control benchmarks. The choice of Q-value function in reinforcement learning algorithms has a significant impact on the resulting algorithm, affecting the types of policies that can be expressed and the type of exploration that can be applied. Different notions of Q-value, such as expected Q-value, hard-max Q-value, and soft-max Q-value, have led to distinct families of RL methods. This approach allows learning of both mean and covariance, achieving strong results on continuous control benchmarks. In this work, a new notion of action value called the smoothed action value function Q \u03c0 is introduced. Unlike previous notions, the smoothed Q-value associates a value with a distribution over actions rather than a specific action at each state. It is defined as the expected return of first taking an action sampled from a normal distribution centered at a, then following actions sampled from the current policy. Smoothed Q-values have properties that make them attractive for use in RL algorithms, such as satisfying single-step Bellman consistency and allowing bootstrapping to train a function approximator. Smoothed Q-values have attractive properties for RL algorithms, satisfying single-step Bellman consistency and enabling bootstrapping for training function approximators. The algorithm Smoothie utilizes derivatives of a trained Q-value function to train a policy, avoiding high variance in standard policy gradient algorithms like DDPG. Smoothie is a novel approach in reinforcement learning that utilizes a non-deterministic Gaussian policy to improve exploratory behavior and reduce the need for hyperparameter tuning. It can incorporate proximal policy optimization techniques by adding a penalty on KL-divergence, leading to improved stability and performance. Results show competitiveness with state-of-the-art methods, especially for challenging tasks with limited data. In the low-data regime, the standard model-free RL framework involves an agent interacting with a stochastic environment to maximize cumulative discounted reward. This is formulated as a Markov decision process with state and action spaces. The agent's behavior is modeled using a stochastic policy, and the optimization objective is the expected discounted return expressed in terms of the action value function. The text discusses reinforcement learning algorithms, including policy gradient and actor-critic variants, which balance variance and bias in estimating the action value function. It focuses on multivariate Gaussian policies for continuous action spaces and parametrizes the policy using a feature vector. The paper focuses on multivariate Gaussian policies for continuous action spaces, parametrizing the policy using a feature vector. It introduces new RL training methods for this policy family, building on prior work on learning Gaussian policies. The key observation is the deterministic policy gradient for Gaussian policies when the policy covariance approaches zero. The BID21 method focuses on estimating future returns under a deterministic policy with a Gaussian mean. It involves optimizing the value function approximator by minimizing the Bellman error for sampled transitions. Algorithms like DDPG alternate between improving the value function and policy. To improve sample efficiency, BID5 and BID21 use an off-policy distribution based on a replay buffer. This substitution may alter the policy gradient identity but has shown practical success. Incorporating off-policy distribution from a replay buffer, BID5 and BID21 replace the on-policy state distribution \u03c1 \u03c0 with \u03c1 \u03b2. This substitution may affect the policy gradient identity but has proven effective in practice. Introducing smoothed action value functions, the method utilizes gradients for optimizing Gaussian policy parameters. Smoothed Q-values, denoted Q \u03c0 (s, a), differ from ordinary Q-values by assuming only the mean of the first action is known, not the full specification. This approach re-expresses the expected reward objective for a Gaussian policy \u03c0 as a new insight differentiating it from prior work. The method introduces smoothed action values, Q \u03c0 (s, a), for optimizing Gaussian policy parameters. By directly learning a function approximator for Q \u03c0 (s, a), it enables direct bootstrapping of smoothed Q-values. Parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 in terms of mean and covariance parameters allows for optimization using the Bellman equation. The method introduces smoothed action values, Q \u03c0 (s, a), for optimizing Gaussian policy parameters by parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 in terms of mean and covariance parameters. The Bellman equation enables direct optimization of Q \u03c0, with the gradient of the objective w.r.t. mean parameters following from the policy gradient theorem. Estimating the derivative of the objective w.r.t. covariance parameters involves computing the second derivative of Q \u03c0 w.r.t. actions to derive the derivative w.r.t. \u03a3. Two ways to optimize Q \u03c0 are discussed, one using fixed target values and the other requiring a training procedure to achieve a fixed point satisfying the Bellman equation recursion. The method introduces smoothed action values for optimizing a Gaussian policy by parameterizing mean and covariance parameters. Two approaches are discussed for optimizing Q \u03c0, one using fixed target values and the other requiring a training procedure to achieve a fixed point satisfying the Bellman equation recursion. Sampling from a replay buffer with knowledge of sampling probability is used to optimize Q \u03c0. The training procedure for optimizing Q \u03c0 in a Gaussian policy involves reaching an optimum by satisfying the Bellman equation recursion. It is unnecessary to track probabilities q(\u00e3 | s) when using a replay buffer with near-uniform action distribution. Stabilizing techniques like trust region methods and penalty on KL-divergence have not been applicable to deterministic policies like DDPG. The paper proposes a formulation easily amenable to trust region optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. This method is a generalization of the deterministic policy gradient, allowing for stable learning of a policy using Q-value functions. The proposed method is a generalization of the deterministic policy gradient, allowing for stable learning of a policy using Q-value functions. It introduces updates for training the Q-value approximator and policy mean, with key differences from other methods like Stochastic Value Gradient (SVG) and Expected Policy Gradients (EPG). The proposed method introduces updates for training the Q-value approximator and policy mean, avoiding noisy Monte Carlo estimates. It relies on neural network function approximators to estimate the smoothed Q-value function directly, simplifying the updates. The training scheme for learning the covariance of a Gaussian policy is based on properties of Gaussian integrals. The novel training scheme proposed in this paper introduces updates for training the Q-value approximator and policy mean, utilizing neural network function approximators to estimate the smoothed Q-value function directly. The approach for learning the covariance of a Gaussian policy is based on properties of Gaussian integrals. The perspective presented in this paper focuses on Q-values representing the averaged return of a distribution of actions, distinct from recent advances in distributional RL. The new RL algorithm, Smoothie, maintains a parameterized Q \u03c0 w and trains a Gaussian policy \u00b5 \u03b8 , \u03a3 \u03c6 using the insights from Section 3. Smoothie is a new RL algorithm introduced in this paper, which maintains a parameterized Q \u03c0 w and trains a Gaussian policy \u00b5 \u03b8 , \u03a3 \u03c6 using gradient and Hessian updates. Algorithm 1 provides a simplified pseudocode for the algorithm, with inputs including environment EN V, learning rates, discount factor, KL-penalty, batch size, number of training steps, and target network lag. Evaluations of Smoothie compared to DDPG show promising results, with a focus on sample-efficient performance on continuous control benchmarks. Smoothie, a new RL algorithm, is compared to DDPG on a synthetic task with a reward function of two Gaussians. Smoothie learns both mean and variance, while DDPG struggles to escape local optima due to limited updates on the mean. Smoothie successfully solves the task by adjusting the policy mean and covariance during training, while DDPG struggles to escape local optima due to limited mean updates. Smoothie's smoothed reward function guides the mean towards the better Gaussian, allowing it to escape lower-reward local optima. Smoothie successfully adjusts its policy mean and covariance during training to escape lower-reward local optima. It competes with DDPG, even outperforming it in final reward performance by learning the optimal noise scale during training. Smoothie competes with DDPG, learning optimal noise scale during training. Significant advantages in final reward performance, especially in difficult tasks like Hopper, Walker2d, and Humanoid. TRPO not sample-efficient. Comparison of results in FIG2 shows Smoothie performing competitively or better across all tasks. Smoothie performs competitively or better across all tasks compared to DDPG, with a slight advantage in Swimmer and Ant, and significant improvements in Hopper, Walker2d, and Humanoid. The introduction of a KL-penalty further enhances Smoothie's performance, especially on harder tasks. The results for Humanoid are the best published for a method training on millions of environment steps, outperforming TRPO which requires tens of millions of steps for comparable reward. The introduction of a KL-penalty improves Smoothie's performance on harder tasks, providing stability in training. Smoothie's Q-value function, Q \u03c0, allows for successful learning of mean and covariance, matching or surpassing DDPG's performance. The algorithm shows significant improvement in Hopper and Humanoid tasks without sacrificing sample efficiency. Smoothie's Q-value function, Q \u03c0, successfully learns mean and covariance during training, matching or surpassing DDPG's performance. The smoothed Q-values make the reward surface smoother and have a direct relationship with the expected discounted return objective. Future work should investigate these claims and apply the underlying motivations to other policies."
}