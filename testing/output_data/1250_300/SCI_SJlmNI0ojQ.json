{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are gaining popularity due to their ease of training, scalability, and lack of a lexicon requirement. This paper discusses constructing contextual acoustic word embeddings from a supervised sequence-to-sequence model, showing competitive performance on standard sentence evaluation tasks and spoken language understanding. The focus is on learning fixed-size representations for variable length data like words or sentences in both text and speech-based applications. The curr_chunk discusses the challenge of learning word representations from variable length acoustic frames in speech recognition. Various methods like word2vec, GLoVE, CoVe, and ELMo have gained popularity in natural language processing tasks. Prior work involved aligning speech and text or chunking input speech into fixed-length segments. Our work introduces a method to obtain acoustic word embeddings from utterance-level acoustics, addressing the limitations of existing techniques. By training an attention-based sequence-to-sequence model for direct Acoustic-to-Word speech recognition, we can automatically segment and classify input speech into individual words without the need for pre-defined boundaries. This approach allows us to learn acoustic word embeddings in the context of their containing sentence, improving contextual dependencies in speech recognition. Our A2W model eliminates the need for pre-defined word boundaries by learning acoustic word embeddings at the utterance level. We demonstrate the effectiveness of attention in aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE). These embeddings are competitive with text-based word2vec embeddings and show utility in speech-based downstream tasks like Spoken Language Understanding. The CAWE model shows the potential for transfer learning in speech-based tasks, similar to VGG in vision and CoVe in natural language understanding. Prior work required large amounts of training data and vocabulary, but recent progress allows training with smaller data while restricting vocabulary. Solutions for out-of-vocabulary words involve using smaller units like characters or sub-words. A recent S2S model focuses on pure-word large vocabulary A2W recognition with a 300-hour Switchboard corpus. The models BID21, BID16, BID17, BID22, and BID19 address rare word generation but are no longer pure-word models. BID24 and BID12 introduce S2S models for large vocabulary A2W recognition using the Switchboard corpus. BID12 demonstrates the best pure-word S2S model that can learn word boundaries without supervision. Various methods, except BID6, use unsupervised learning for acoustic word embeddings. BID6 employs a supervised CNN-based speech recognition model with short speech frames. BID6 uses a supervised Convolutional Neural Network for speech recognition with short speech frames as input. BID4 proposes an unsupervised method for learning speech embeddings using fixed word contexts. Our work combines A2W speech recognition with learning contextual word embeddings from speech, similar to the Listen, Attend and Spell model. Our work combines A2W speech recognition model with learning contextual word embeddings from speech. The S2S model consists of an encoder network, a decoder network, and an attention model. The encoder is a pyramidal multi-layer bi-directional LSTM network that maps input acoustic features to higher-level features. The decoder, also an LSTM network, generates targets using an attention mechanism that enforces monotonicity in alignments. Our model utilizes an attention mechanism to enforce monotonicity in alignments, leading to a peaky distribution. Acoustic word embeddings are obtained from an end-to-end trained speech recognition system, using hidden representations from the encoder and attention weights from the decoder. The method is similar to CoVe for text embeddings but addresses the challenge of alignment between input speech and output words. Our method utilizes a location-aware attention mechanism to automatically segment continuous speech into words and obtain word embeddings. Attention weights on acoustic frames reflect their importance in classifying a word, allowing us to construct word representations based on their importance. Our method uses attention weights to segment speech into words and obtain word embeddings. The attention weights reflect the importance of acoustic frames in classifying a word, allowing us to construct word representations based on their importance. Our method utilizes attention weights to segment speech into words and generate word embeddings. The attention weights indicate the significance of acoustic frames in word classification, enabling the creation of word representations based on their importance. Additionally, we introduce Contextual Acoustic Word Embeddings (CAWE) techniques, including unweighted combination, attention-weighted average, and maximum attention, which leverage attention scores for contextual word embeddings. We evaluate our approach on the 300-hour Switchboard corpus and a subset of the How2 dataset, showcasing its effectiveness in different speech recognition setups. The second dataset used is a 300-hour subset of the How2 BID27 dataset of instructional videos, containing free speech recorded with distant microphones. It includes 13,662 videos with 3.5 million words. The A2W achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome from the Switchboard Eval2000 test set, and 24.3% on the dev5 test set of How2. The embeddings are evaluated on 16 benchmark sentence evaluation tasks covering various aspects like Semantic Textual Similarity, classification, sentiment analysis, question type, and paraphrase detection. The tasks measure correlation between embedding-based similarity and human scores. The SentEval toolkit is used to evaluate the performance of CAWE-M in comparison to U-AVG and CAWE-W on STS tasks. CAWE-M outperforms the other models by a significant margin on Switchboard and How2 datasets. Logistic regression is used for classification tasks to abstract away model complexities. CAWE-M outperforms U-AVG and CAWE-W by 12% on Switchboard and How2 datasets for STS tasks. CAWE-W performs worse than CAWE-M due to noisy word embeddings. U-AVG performs even worse on STS and SICK-R tasks. Training set embeddings are compared to CBOW word2vec model for fair evaluation. The embeddings obtained from the training set of the speech recognition model are compared to text-based word embeddings trained on all transcripts. Despite the limited vocabulary captured by the A2W speech recognition model, the performance of CAWE is competitive with word2vec CBOW. Evaluations show that the concatenated embeddings outperform word2vec on multiple tasks, especially in the Switchboard dataset. The concatenated acoustic embeddings outperform word2vec embeddings on 10 out of 16 tasks, showing improved performance in Switchboard dataset. CAWE is also evaluated on the ATIS dataset for Spoken Language Understanding, similar to Switchboard in domain. The model architecture includes an embedding layer, RNN variant, dense layer, and softmax. Our model architecture, similar to a simple RNN-based model, includes an embedding layer, RNN variant, dense layer, and softmax. We train the model for 10 epochs with RMSProp and compare performance using different word embeddings. We show that speech-based word embeddings can match text-based embeddings in downstream tasks, highlighting the utility of our approach. Additionally, we introduce a method to learn contextual acoustic word embeddings from a speech recognition model. The curr_chunk discusses the development of contextual acoustic word embeddings from a speech recognition model, showing competitive performance with text-based embeddings. These embeddings outperform traditional methods in semantic textual similarity tasks and spoken language understanding. The study suggests that contextual audio embeddings can enhance downstream tasks despite the challenges of noisy audio input. Future work will focus on scaling the model and comparing it with non-contextual acoustic word embedding methods. The curr_chunk discusses exploring ways to scale the model to larger corpora and vocabularies, comparing with non-contextual acoustic word embedding methods. Supported by the Center for Machine Learning and Health at Carnegie Mellon University and Facebook."
}