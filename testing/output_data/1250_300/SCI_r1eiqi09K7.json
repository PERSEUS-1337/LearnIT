{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. Experimentally, Riemannian adaptive methods show faster convergence and lower train loss values compared to their baselines. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when dealing with a large number of parameters. Recent advancements have led to successful first-order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD. While these algorithms are designed for parameters in a Euclidean space, there is a growing interest in optimizing parameters on a Riemannian manifold. Riemannian adaptive methods have shown faster convergence and lower train loss values compared to their Euclidean counterparts, as demonstrated in embedding the WordNet taxonomy in the Poincare ball. Recent advancements in optimization algorithms have focused on parameters on a Riemannian manifold, allowing for non-Euclidean geometries. These algorithms have shown success in various applications such as matrix factorization, dictionary learning, and hyperbolic taxonomy embedding. While first-order stochastic methods have been adapted to this setting, there is a need for Riemannian counterparts for adaptive algorithms that assign learning rates per coordinate. In this work, the authors explain the challenges of generalizing adaptive algorithms to Riemannian manifolds and propose generalizations for a product of manifolds. They also provide convergence analysis and empirical support for their claims using hyperbolic taxonomy embedding. Their initial motivation was to develop Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces. The absence of Riemannian adaptive algorithms could hinder the development of competitive optimization-based Riemannian embedding methods, especially in hyperbolic spaces. A manifold is a space that can be locally approximated by a Euclidean space and is a generalization of the notion of surface to higher dimensions. The authors refer to elementary notions of differential geometry for more detailed explanations. A Riemannian manifold is a space that can be locally approximated by a Euclidean space, with a Riemannian metric defining the geometry locally. It consists of a pair (M, \u03c1) where M is the manifold and \u03c1 is the metric. The metric induces a global distance function on M, allowing for the calculation of distances between points. In a Riemannian manifold (M, \u03c1), paths between x and y are defined by integrating the size of the speed vector in the tangent space. Riemannian SGD updates involve the gradient of the objective function and a step-size. The exponential map allows for updates along the shortest path in the relevant direction while staying in the manifold. When the exponential map is unknown, a retraction map is commonly used as a first-order approximation. Notable algorithms include ADAGRAD with rescaled coordinate-wise updates based on past gradients. ADAGRAD, ADAM, and AMSGRAD are key algorithms in optimization. ADAGRAD rescales updates based on past gradients, while ADAM includes momentum and adaptivity terms. AMSGRAD corrects a mistake in previous algorithms. ADAGRAD, ADAM, and AMSGRAD are key optimization algorithms. BID18 identified a mistake in ADAM's convergence proof and proposed AMSGRAD or ADAMNC as solutions. Intrinsic updates on a Riemannian manifold require a coordinate system. Intrinsic updates on a Riemannian manifold require a coordinate system, with local coordinate systems called charts. Quantities defined using a chart are intrinsic if their definition does not depend on the chart used. The RSGD update is intrinsic as it involves objects intrinsic to the manifold. It is unclear if Eqs. (3,4,5) can be expressed in a coordinate-free or intrinsic manner. In a Riemannian manifold, parallel transport depends on the chosen path and curvature introduces a rotational component, breaking the sparsity of gradients. Adaptivity loses its meaning as the coordinate system for gradients changes with the optimization path. Updates defined differently from Eq. (6) are not covered by the theorems presented. Additional structure is assumed on (M, \u03c1). In a Riemannian manifold, adaptivity loses meaning due to changing coordinate systems for gradients. Updates not covered by the theorems are proposed with additional structure assumed on (M, \u03c1). Each component x i \u2208 M i is seen as a \"coordinate\", leading to a simple adaptation of Eq. (3) with squared Riemannian norms in the adaptivity term. In a Riemannian manifold, adaptivity loses meaning due to changing coordinate systems for gradients. Updates not covered by the theorems are proposed with additional structure assumed on (M, \u03c1). Each component x i \u2208 M i is seen as a \"coordinate\", leading to a simple adaptation of Eq. (3) with squared Riemannian norms in the adaptivity term. In section 2, ADAGRAD, ADAM, and AMSGRAD were briefly presented. ADAM combines ADAGRAD with momentum (parameter \u03b2 1) and an exponential moving average for past squared-gradients with exponent \u03b2 2. AMSGRAD corrects ADAM's convergence proof, while ADAMNC has a non-constant schedule for \u03b2 1 and \u03b2 2. The schedule proposed by BID18 for \u03b2 2 in ADAMNC allows v t to recover the sum of squared-gradients of ADAGRAD. ADAMNC without momentum yields ADAGRAD. Assumptions and notations are made for geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0. Riemannian AMSGRAD is presented in FIG1, alongside the comparison with ADAGRAD. Assumptions and notations are made for geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0. The set of feasible parameters is defined as X := X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xn. The convergence guarantees will bound the regret after T rounds as defined in DISPLAYFORM1. Riemannian AMSGRAD is compared with standard AMSGRAD in FIG1. RADAM and ADAM are derived by removing max operations. Convergence guarantees for RAMSGRAD are presented in Theorem 1, with \u03b6 defined in DISPLAYFORM6. Convergence guarantees between RAMSGRAD and AMSGRAD coincide when (Mi, \u03c1i) = R for all i. Regret bound worsens by a factor of approximately 1 + D\u221e|\u03ba|/6 when curvature is small but non-zero. The convergence theorems of AMSGRAD and RADAMNC are discussed, with details on regret bound worsening with small non-zero curvature. The convergence proof for RADAGRAD is also highlighted. The role of convexity in the theorems is compared, emphasizing geodesic convexity in Theorem 1. The notion of convexity in Theorem 5 is replaced by geodesic convexity in Theorem 1. Regret bounds for convex objectives involve bounding the difference between current and optimal values. In Riemannian manifolds, this involves a term \u03c1 xt (g t , \u2212 log xt (x * )). Using a cosine law, the bound simplifies into two terms, with the second requiring a well-chosen decreasing schedule for \u03b1. This step is generalized in Riemannian manifolds using lemma 6 for geodesically convex subsets. In Riemannian manifolds, lemma 6 introduces a curvature dependent quantity \u03b6, allowing us to bound \u03c1. The benefits of adaptivity are highlighted, especially for sparse gradients. Empirical assessment of RADAM, RAMSGRAD, and RADAGRAD algorithms is compared to non-adaptive RSGD method. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed compared to the non-adaptive RSGD method. The Poincar\u00e9 model is chosen for optimization tools due to closed form expressions for quantities used in the algorithm. Riemannian gradients are rescaled Euclidean gradients, and various mathematical concepts like distance functions, geodesics, exponential and logarithmic maps are discussed. The transitive closure of the WordNet taxonomy graph contains 82,115 nouns and 743,241 hypernymy Is-A relations. The words are embedded in Dn to minimize distances between connected words. The loss function used is similar to log-likelihood, with negative word pairs sampled for approximation. Metrics include loss value and mean average precision (MAP) for ranking distances among ground truth negative examples. The study focuses on evaluating the performance of different optimization methods in hyperbolic spaces for link prediction tasks. They use a 5-dimensional hyperbolic space and sample negative words based on their graph degree during the \"burn-in phase.\" RADAM is reported to yield slightly better results than RAMS-GRAD. The study compares optimization methods in hyperbolic spaces for link prediction tasks. RADAM outperforms RAMS-GRAD, showing lower loss values with a first-order approximation of the exponential map. Results are shown for different learning rates, with RSGD baseline in orange and RADAM and RAMSGRAD in their best settings. In the study comparing optimization methods in hyperbolic spaces for link prediction tasks, RADAM consistently achieves the lowest training loss and outperforms other methods on the MAP metric. RAMSGRAD shows faster convergence for the link prediction task, indicating better generalization capability. Various first-order Riemannian methods have emerged after the introduction of Riemannian SGD. After the introduction of Riemannian SGD, several first-order Riemannian methods have been developed, including Riemannian SVRG, Riemannian Stein variational gradient descent, Riemannian accelerated gradient descent, and averaged RSGD. Stochastic gradient Langevin dynamics was also generalized to optimize on the probability simplex. Additionally, Riemannian counterparts of SGD with momentum and RMSprop have been proposed, but without convergence guarantees due to compromising operations in the tangent space. Another version of Riemannian ADAM for the Grassmann manifold G(1, n) has been introduced, utilizing parallel translation for the momentum term. In section 3.1, a version of Riemannian ADAM for the Grassmann manifold G(1, n) was introduced by BID3, but it removes the adaptive component and lacks adaptivity across manifolds. The proposal is to generalize adaptive optimization tools to Cartesian products of Riemannian manifolds, deriving convergence rates similar to Euclidean models. Experimental results show outperformance of non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. The text discusses hyperbolic word taxonomy embedding and presents mathematical formulas and inequalities to prove convergence of gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. The focus is on proving convergence using a user-friendly inequality developed by the author. The text presents mathematical formulas and inequalities to prove convergence of optimization algorithms for geodesically convex functions in Alexandrov spaces. Lemma 6 discusses the cosine inequality, Lemma 7 presents an analogue of Cauchy-Schwarz inequality, and Lemma 8 introduces a lemma used in convergence proofs."
}