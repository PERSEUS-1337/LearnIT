{
    "title": "BJxpIJHKwB",
    "content": "Few shot image classification involves learning a classifier from limited labeled data. Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) addresses the challenge of generating diverse classification weights for query samples by maximizing mutual information between weights and data. This approach achieves state-of-the-art performance on benchmark datasets. This is the first attempt to unify information maximization into few shot learning, showing that AWGIM achieves state-of-the-art performance on benchmark datasets. Few shot learning enables deep models to learn from very few samples, with meta learning being the most popular approach for this problem. In AWGIM, classification weights are generated for each query sample specifically using two encoding paths where the query sample attends to the task context. This approach addresses limitations in few shot learning by adapting to new tasks quickly and achieving state-of-the-art performance on benchmark datasets. In this work, AWGIM introduces Variational Information Maximization for few shot learning, generating classification weights specifically for each query sample. By maximizing the mutual information between weights and query/support data, AWGIM achieves state-of-the-art performance without compromising efficiency. The approach eliminates inner updates and is evaluated on benchmark datasets, showcasing its effectiveness. AWGIM achieves state-of-the-art performance in few-shot learning by introducing Variational Information Maximization. It generates classification weights for each query sample by maximizing mutual information between weights and query/support data. The approach eliminates inner updates and is evaluated on benchmark datasets, showcasing its effectiveness. Some methods for few-shot classification include generating classification weights directly from activations of a trained feature extractor, using graph neural network denoising autoencoders, and proposing to generate \"fast weights\" from the loss gradient for each task. Other approaches involve generative models to generate more data, closed-form solutions for few-shot classification, and integrated label propagation on a transductive graph. Attention mechanisms are also utilized in these methods. In Wang et al., 2018; Chen et al., 2019, methods for few-shot classification include generative models and closed-form solutions. Attention mechanisms, effective in computer vision and natural language processing, are used to model interactions between queries and key-value pairs. Self and cross attention are employed to encode task and query-task information, maximizing mutual information for few-shot image classification. Attentive Neural Processes also use attention, but our approach focuses on regression and stochastic processes. Mutual information measures uncertainty decrease in one random variable when another is known. It is widely used in applications like Generative Adversarial Networks and self-supervised learning. The attentive path equips query samples with task knowledge using an attention mechanism. The weight generator generates classification weights specific for x. The proposed model utilizes an attention mechanism to equip query samples with task knowledge. A weight generator generates classification weights specific for x, maximizing mutual information. The problem is formulated under an episodic training paradigm for few-shot classification tasks. The proposed model uses an attention mechanism to enhance query samples with task knowledge and a weight generator to generate classification weights for x. The approach is formulated within an episodic training paradigm for few-shot classification tasks, following a general framework for generating classification weights. The proposed model utilizes an attention mechanism to improve query samples with task knowledge and a weight generator to produce classification weights for x. It operates within an episodic training paradigm for few-shot classification tasks, following a general framework for generating classification weights. Latent Embedding Optimization (LEO) is a related method that generates classification weights based on a latent code z generated by h conditioned on the support set S. The model does not require inner updates to adapt, unlike LEO, and is a feedforward network. The proposed model utilizes an attention mechanism and a weight generator to produce classification weights for query samples in few-shot classification tasks. It differs from Latent Embedding Optimization (LEO) by avoiding inner updates and using a feedforward network approach. The framework includes a feature extractor processing images into d-dimensional vectors, with contextual and attentive paths encoding task context and query samples. The generator then produces classification weights based on the concatenated outputs of both paths. The model utilizes contextual and attentive paths to encode task context and query samples, generating classification weights for few-shot classification tasks. The contextual path focuses on learning representations for the support set, while the attentive path aims to maximize mutual information between variables. The classification weights are generated based on the concatenated outputs of both paths, improving the accuracy of weight estimation from limited labeled data. The model introduces an attentive path to improve classification weights for few-shot tasks. Unlike the contextual path, the attentive path adapts to different query samples by attending to task context. A multi-head self-attention network is used to encode global task information, enhancing the adaptability of classification weights. The attentive path focuses on generating classification weights by utilizing self-attention to provide context for query samples in cross attention. Using multi-head attention, different sets of queries, keys, and values are produced to learn comprehensive representations. The tensors are replicated and concatenated to create specific latent representations for each query sample to generate classification weights. The tensors X cp and X ap are concatenated to form X cp\u2295ap, which represents specific latent representations for each query sample. The weights generator g outputs classification weights sampled from a Gaussian distribution. The final classification weight matrix W f inal is computed by taking the mean value of K classification weights for each class. The prediction for query and support data is computed using XW f inalT and X s W f inalT, respectively. The support data X is replicated for |Q| times and reshaped as X s \u2208 R |Q|\u00d7N K\u00d7d. The prediction for support data can also be computed as X s W f inalT. Two decoders r1 and r2 reconstruct X cp and X ap respectively. Reconstruction is used as auxiliary tasks. The weights generator g outputs query-specific weights. However, experiments show that weights generated from two paths are not sensitive to different query samples. The weights generated from two paths are not sensitive to different query samples. To address this, the proposal is to maximize the mutual information between generated weights and support as well as query data. This involves using Variational Information Maximization to compute a lower bound for the true mutual information. The proposal suggests maximizing mutual information between generated weights and support/query data using Variational Information Maximization to compute a lower bound for true mutual information. This involves approximating posterior distributions and maximizing log likelihood to minimize cross entropy. The loss function for training the network involves reconstructing data with L2 loss. The proposal involves maximizing mutual information between generated weights and support/query data using Variational Information Maximization. The loss function for training the network includes reconstructing data with L2 loss and deciding weightage for different terms using hyper-parameters. The proposed method differs from LEO by involving specific query samples in weight generation and making reconstructing x ap possible. The encoding process in contextual path results in computational complexity O((N K) 2) while the attentive path complexity is O((N K) 2 + |Q|(N K)). AWGIM introduces a method that avoids inner updates without compromising performance, reducing training and inference time significantly. Empirical evaluation is conducted on miniImageNet and tieredImageNet datasets, comparing with other methods. miniImageNet has 100 classes with 600 images each, following a specific train/test split. The computational complexity of the encoding and attentive paths is O((N K) 2) and O((N K) 2 + |Q|(N K)), respectively. The induced computational overhead is negligible due to parallel implementation via matrix multiplication. The miniImageNet dataset contains 100 classes with 600 images each, while tieredImageNet has 608 classes and 779,165 images. Image features from LEO are used, with a 640-dimensional vector representing each image. Experiments are conducted with N-way K-shot tasks, training 5-way 1-shot and 5-shot models on both datasets. During meta-testing, 600 N-way K-shot tasks are performed. During experiments, N-way K-shot tasks were conducted on miniImageNet and tieredImageNet datasets using image features from LEO. The models were trained on 5-way 1-shot and 5-shot setups. Meta-testing involved 600 N-way K-shot tasks, with TensorFlow used for implementation. Key model parameters were set, and performance comparisons with other works were provided. Table 2 shows accuracy comparison of different approaches on tieredImageNet, with results averaged on 600 tasks from meta-testing set. Best results are highlighted, with models using various feature extractors for 5-way 1-shot and 5-shot setups. MetaOptNet Resnets (2017) was used for network optimization with weight decay 1 \u00d7 10 \u22126. AWGIM is compared with state-of-the-art methods on two datasets, tieredImageNet and miniImageNet. The model is trained on meta-training and meta-validation sets with fixed hyper-parameters. Results of MAML, Prototypical Nets, and Relation Nets on tieredImageNet are evaluated. Additionally, Dynamic on miniImageNet with WRN-28-10 as the feature extractor is reported. The backbone network structure of the used feature is also included. The results of Dynamic on miniImageNet with WRN-28-10 as the feature extractor are reported, along with other results in corresponding papers. Table 1 and 2 show the results on miniImageNet and tieredImageNet, categorizing methods into meta learning categories and classification weights generation approaches. AWGIM outperforms all methods in the top parts of the tables and shows competitive performance in the bottom part, being the best on tieredImageNet and close to the state-of-the-art on miniImageNet. AWGIM outperforms LEO in all settings, with detailed analysis provided in Table 3. Different generators are studied, including contextual path only and conditioning on support set. Results show that AWGIM is optimal for different query samples. Comparisons with LEO are made, where \"Generator in LEO\" indicates no inner update. The effect of attentive path is also examined, with variations in training methods highlighted. The study compares AWGIM and LEO generators, with AWGIM outperforming LEO in all scenarios. Different generator configurations are analyzed, including contextual path only and conditioning on support set. Results indicate that AWGIM is optimal for various query samples. The impact of the attentive path is also explored, with different training methods discussed. The use of information maximization in the generator leads to improved performance compared to LEO. Ablation analysis on the importance of maximizing information is conducted by varying \u03bb values. The study compares AWGIM and LEO generators, with AWGIM outperforming LEO in all scenarios. Ablation analysis on the importance of maximizing information is conducted by varying \u03bb values, showing the significance of maximizing mutual information between weights and support.\u03bb 1 = 0 affects performance noticeably, indicating the critical role of support label prediction in information maximization. The study compares AWGIM and LEO generators, with AWGIM outperforming LEO in all scenarios. Ablation analysis on the importance of maximizing information is conducted by varying \u03bb values, showing the significance of maximizing mutual information between weights and support. Prediction is critical for information maximization, with classification weights generated specifically for each query sample in AWGIM. Shuffling weights between query samples within and between classes reveals that weights for query samples from the same class are very similar, while distinct for different classes. Random shuffling affects accuracy in 5-way 1-shot experiments, but not in 5-way 5-shot experiments with more labeled data in the support set. In this work, Attentive Weights Generation via Information Maximization (AWGIM) is introduced for few-shot image classification. AWGIM learns to generate optimal classification weights for each query sample by maximizing mutual information between generated weights and query, support data. This approach outperforms LEO generators in all scenarios and demonstrates state-of-the-art performance on benchmark datasets. The study shows that larger support sets provide more knowledge to estimate optimal classification weights for each query example. AWGIM is the first work to use mutual information techniques for few-shot learning, showing state-of-the-art performance on benchmark datasets. The model utilizes multi-head attention to encode global task information and compute cross attention between query and support samples. Classification weights are generated following a Gaussian distribution with diagonal covariance. The weights generator decodes cp\u2295ap using a Gaussian distribution with diagonal covariance. During meta-training, weights are sampled from this distribution. Few shot regression tasks are modified by setting N=1 and using mean square error loss. The model generates weight and bias parameters for a three-layer MLP with hidden dimension 40. Sinusoidal and linear regression tasks are constructed, with multi-head attention improving performance in experiments on miniImageNet dataset. The results of 5-way 1-shot and 5-way 5-shot experiments on miniImageNet dataset show that multi-head attention improves performance. Single head attention struggles with extremely scarce data. AWGIM converges faster than LEO and outperforms it, with minimal computational overhead. Inference time comparisons show that AWGIM is efficient. The usage of self-attention and cross attention in AWGIM incurs negligible overhead compared to MLP encoding due to small values of N, K, and |Q|. Visualization of classification weights using t-SNE shows that the decoded weights are clustered closer for each class, indicating improved performance. The inputs to g are displayed in (a, b) and the generated classification weights in (c, d). Comparison shows that decoded weights for each class in (c) are clustered closer than (a). Red and blue dots in (b, d) denote classification weights for query samples from two classes within one task. g can generate adapted weights for different query samples, consistent with distinct classification weights for query samples from different classes."
}