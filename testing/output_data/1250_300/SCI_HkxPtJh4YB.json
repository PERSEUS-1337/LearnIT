{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for marginal inference in exponential families defined over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, addressing the intractability of computing the matrix of expectations \u03c1 := E(P). The Sinkhorn variational marginal inference method efficiently approximates the matrix of expectations \u03c1 := E(P) by using the Sinkhorn operator applied to L. This approach overcomes the intractability of computing the permanent of L and produces the best results for probabilistic inference of neural identity in C.elegans. The Sinkhorn variational marginal inference method approximates the matrix of expectations \u03c1 using the Sinkhorn operator applied to L, linking marginal inference of \u03c1 and computation of the permanent Z L. The approximation is based on replacing the intractable dual function with a component-wise entropy, producing an approximate \u03c1 that depends on the tightness of the approximation to Z L. The approximation of the normalizing constant, known as the Sinkhorn permanent, is based on replacing the intractable dual function with a component-wise entropy. Bounds for this approximation are provided, with a comparison to a heuristic approach proposed independently. The Bethe variational inference method offers a general rationale for obtaining variational approximations in graphical models. The Bethe variational inference method approximates the dual function in graphical models with a tree structure, leading to better theoretical guarantees than the Sinkhorn approximation. It has been successfully applied to permutations through belief propagation, with computational differences in message computations. The Bethe approximation produces better permanent approximations in practice, as shown in Fig 1 (b). In practice, the Bethe approximation produces better permanent approximations compared to the Sinkhorn approximation. However, the Sinkhorn approximation often produces qualitatively better marginals by putting more mass on non-zero entries. Additionally, for moderate n values, the Sinkhorn approximation scales better in terms of computational efficiency. In the study, 1,000 submatrices of size n = 8 were randomly sampled from the C.elegans dataset. The differences between approximate and true log permanent, as well as the mean absolute errors of log marginals for two approximations, were analyzed. Sampling-based methods for marginal inference were also discussed, with mention of the limitations of an elementary MCMC sampler. The unique characteristics of the C.elegans species, with a stereotypical nervous system, were highlighted. Recent advances in neurotechnology have enabled whole brain imaging of the unique C.elegans species with a stereotypical nervous system. A technical problem to be solved is the identification of worm neurons in volumetric images, where probabilistic neural identification is applied. This methodology aims to estimate the probability of identifying observed neurons with canonical identities, providing uncertainty estimates for the model. With neurons represented as vectors in R6, the goal is to estimate the matrix of marginal \u03c1 for identifying observed neurons with canonical identities. These probabilities offer uncertainty estimates for model predictions, using a gaussian model for each canonical neuron. The likelihood of observing data is determined by a flat prior over P, inducing a posterior with a deterministic coloring scheme for NeuroPAL worms. The downstream task involves computing approximate probabilistic neural identifies \u03c1. In the context of NeuroPAL, a downstream task involves computing approximate probabilistic neural identifies \u03c1. A human manually labels neurons with uncertain model estimates, leading to increased identification accuracy. Results are compared to simple baselines in Fig 3, with details in the Appendix. Alternative methods considered include Sinkhorn approximation, Bethe approximation, MCMC, and random or naive baselines. The study compared different methods for neural identification accuracy, including Sinkhorn and Bethe approximations, MCMC, and various baselines. Sinkhorn was found to be slightly better than Bethe, both outperforming other baselines except for an unattainable oracle. MCMC did not perform better than a naive baseline, indicating convergence issues. Sinkhorn was suggested as a faster and more accurate alternative to sampling for marginal inference. The Sinkhorn approximation for marginal inference is proposed as a faster and more accurate alternative to sampling than the Bethe approximation. The (log) Sinkhorn approximation of the permanent of L is obtained by evaluating S(L) in the problem it solves. The study used a dataset of NeuroPAL worm heads with available human labels and log-likelihood matrices computed with specific methods. The dataset used in the study consists of ten NeuroPAL worm heads with human labels and log-likelihood matrices. The Sinkhorn and Bethe approximations were computed with 200 iterations each, leading to efficient computation times. The MCMC sampler method described in Diaconis (2009) was used with 100 chains of length 1000. The results were obtained on a desktop computer with an Intel Xeon W-2125 processor. The Bethe approximation algorithm was implemented based on Vontobel (2013) and simplified by Pontobel (2019). The message passing algorithm described in Vontobel (2013, Lemma 29) and simplified by Pontobel (2019) was efficiently implemented in log-space. Error bars were not shown due to their insignificance."
}