{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models. This vulnerability allows attackers to exploit black-box systems. In this work, the adversarial perturbation is decomposed into model-specific and data-dependent components, with the latter contributing mainly to transferability. The proposed approach crafts adversarial examples using noise reduced gradient (NRG) to approximate the data-dependent component, enhancing transferability significantly across various ImageNet models. Low-capacity models show stronger attack capability compared to high-capacity models with similar test performance. These findings offer a principled method for constructing successful adversarial examples and potential insights for defending against black-box attacks in the era of large neural network models used in real-world applications. Large neural network models are increasingly used in real-world applications like speech recognition and computer vision. However, recent research has shown that these models can be easily manipulated to produce incorrect outputs, known as adversarial examples. Understanding this phenomenon and effectively defending against it are still open questions. Adversarial examples can transfer across different models, allowing for attacks on black-box systems. The vulnerability to adversarial examples is attributed to the strong nonlinearity of deep neural networks. The authors studied adversarial example generation as an optimization problem using box-constraint L-BFGS. Different views on the cause of adversarial instability were discussed, with BID5 attributing it to the linear nature and high dimensionality. Various methods for crafting adversarial examples and defense strategies were proposed by different authors. In the realm of adversarial attacks and defenses, various methods have been proposed, including defensive distillation, adversarial training, and image transformation. Some approaches focus on detecting adversarial examples, but they can be easily overcome by stronger adversarial examples. This work delves into the transferability of adversarial examples, highlighting the decomposition of perturbations into model-specific and data-dependent components to enhance black-box attacks. The transferability of adversarial perturbations across different models is mainly influenced by the data-dependent component, which approximates the ground truth on the data manifold. A noise-reduced gradient (NRG) method is proposed to construct adversarial examples by utilizing this component. The NRG method, when combined with other known methods, significantly increases the success rate of black-box attacks on the ImageNet validation set. Additionally, the success rate of black-box attacks is shown to depend on model-specific factors such as model capacity and accuracy, with models of higher accuracy and lower capacity demonstrating stronger capability to attack unseen models. In this work, the success rate of black-box attacks on unseen models is explored, showing that models with higher accuracy and lower capacity are more effective. Adversarial perturbations make models vulnerable, creating imperceptible changes that lead to adversarial examples. The study focuses on deep neural networks but acknowledges their existence in other models like support vector machines. Adversarial perturbations create imperceptible changes in models, leading to adversarial examples. These attacks can be non-targeted or targeted, with black-box attacks fooling models without knowledge of the target model. Adversarial perturbations create imperceptible changes in models, leading to adversarial examples. Black-box attacks deploy adversarial examples to fool the target model, while white-box attacks target the source model itself. Crafting adversarial perturbation involves optimizing a loss function J to measure prediction discrepancy. Commonly used loss functions include cross entropy and manipulating output logit. Distortion measurement commonly uses \u221e and 2 norms. Ensemble-based approaches suggest using a large ensemble of models to strengthen adversarial examples. Ensemble-based approaches suggest using a large ensemble of models to strengthen adversarial examples by averaging predicted probabilities. Different optimizers can be used to solve the problem, with a focus on the normalized-gradient based optimizer. The Fast Gradient Based Method attempts to solve the problem by performing only one step iteration, showing good transferability but not optimal. The Fast Gradient Based Method (FGBM) uses normalized gradient vectors for attacks, showing good transferability. The Iterative Gradient Method performs normalized-gradient ascent for k steps, with g(x) chosen based on the attack type. Transferability between models is crucial for black-box attacks and defenses. Transferability between models is crucial for black-box attacks and defenses. The similarity between decision boundaries of source and target models enables transferable adversarial examples. Models A and B, with high performance on the same dataset, learn a similar function on the data manifold. Perturbations can be decomposed into data-dependent and model-specific components. The perturbation can be decomposed into data-dependent and model-specific components, with the former contributing more to transferability between models A and B. The model-specific component has little impact on transferability due to different behaviors off the data manifold. The decision boundaries of the models are similar in the inter-class area, allowing for attacks using adversarial perturbations. The perturbation consists of data-dependent and model-specific components, with the former being more influential in transferring attacks between models A and B. The model-specific component has minimal impact on transferability due to its behavior off the data manifold. To enhance success rates of black-box attacks, the data-dependent component should be strengthened, leading to the proposal of the NRG method which reduces model-specific noise. The NRG method reduces noisy, model-specific information in gradients to capture more data-dependent information, improving transferability in black-box attacks. The noise-reduced gradient (NRG) yields a smoother, more data-dependent gradient, enhancing the success rates of attacks between models. The proposed nr-IGSM method utilizes NRG to drive the optimizer towards more data-dependent solutions. The proposed Noise-reduced Iterative Sign Gradient Method (nr-IGSM) uses \u2207f in Eq.(8) to drive the optimizer towards data-dependent solutions. The method is effective in enhancing transferability in black-box attacks, as demonstrated on ImageNet dataset with 50,000 samples. Targeted attacks are conducted on 5,000 images that are correctly recognized by all models. For targeted attack experiments, 5,000 images are randomly selected with a random wrong label. Various pre-trained models are used including resnet, vgg, densenet, alexnet, and squeezenet. The performance of the targeted attack is evaluated based on the Top-1 success rate. The targeted attack experiments evaluate performance based on the Top-1 success rate using pre-trained models like resnet, vgg, densenet, alexnet, and squeezenet. The effectiveness of noise-reduced gradient technique combined with fast gradient based methods is demonstrated, showing consistent and significant improvements over original FGSM for both blackbox and white-box attacks. The noise-reduced gradient technique combined with fast gradient-based methods shows consistent and significant improvements over original FGSM for both blackbox and white-box attacks. Additionally, large models like resnet152 are more robust to adversarial transfer compared to small models. Large models like resnet152 are more robust to adversarial transfer than small models. Transfer among models with similar architectures is influenced by model-specific components. IGSM generally generates stronger adversarial examples than FGSM, except for attacks against alexnet. This contradicts previous claims, with higher confidence adversarial examples more likely to transfer to target models. The inappropriate choice of hyperparameters may lead to underfitting. Alexnet differs significantly from source models in architecture and accuracy. The inappropriate choice of hyperparameters in BID6 leads to underfitting, especially when attacking alexnet. IGSM overfits more than FGSM, resulting in lower fooling rates. The noise reduced gradient technique removes model-specific information, improving cross-model generalization. NRG is applied in ensemble-based approaches with a reduced evaluation set of 1,000 images. Both FGSM and IGSM attacks are tested, with IGSM showing nearly saturated success rates. Due to the high computational cost, 1,000 images are chosen for evaluation instead of 5,000. Both FGSM and IGSM attacks are tested, with IGSM showing nearly saturated success rates. Targeted attacks are more challenging and sensitive to the step size used in optimization procedures. A large step size is necessary for generating strong targeted adversarial examples. After finding that a large step size is crucial for targeted adversarial examples, the NRG methods outperform normal methods by a significant margin in both targeted and non-targeted attacks. The Top-5 success rates of ensemble-based approaches are reported in Table 3(b), showing the superiority of NRG methods. Sensitivity analysis of hyper parameters m and \u03c3 is explored for black-box attacks using the nr-FGSM approach on a selected evaluation set. In this section, the sensitivity of hyper parameters m and \u03c3 is explored for black-box attacks using the nr-FGSM approach. Larger m leads to higher fooling rates due to better gradient estimation, while an optimal value of \u03c3 is crucial for performance. The optimal \u03c3 varies for different source models, being around 15 for resnet18 and 20 for densenet161. Additionally, the robustness of adversarial perturbations to image transformations is preliminarily investigated. The robustness of adversarial perturbations to image transformations is explored in this section. Adversarial examples generated by NRG methods are found to be more robust than those generated by vanilla methods. The study considers rotation, Gaussian noise, Gaussian blur, and JPEG compression as image transformations. In this section, the decision boundaries of different models are studied to understand why NRG-based methods perform better. Resnet34 is chosen as the source model, and various target models are considered. The \u2207f is estimated, and the decision boundaries are shown in FIG8. The direction of sign \u2207f is as sensitive as sign (\u2207f \u22a5 ) for the source model resnet34, but other target models are more robust. The direction of sign \u2207f is as sensitive as sign (\u2207f \u22a5 ) for the source model resnet34, but other target models are more robust. Adversarial examples crafted from alexnet generalize worst across models, with attacks only achieving 19.3 percent transfer to resnet152. The distances for complex models to produce adversarial transfer are significantly larger than those of small models, providing a geometric understanding of model robustness. In experiments, it was found that big models are more robust than small models. Adversarial examples crafted from alexnet generalize worst across models, with attacks achieving only 19.3 percent transfer to resnet152. Different models exhibit varying performances in attacking the same target model, with densenet121 performing well. The study aims to understand this phenomenon to guide the selection of a better local model for generating adversarial examples. Target models vgg19 bn and resnet152 were selected for FGSM and IGSM attacks, with results summarized in FIG11 showing models with powerful attack capability concentrated in the bottom left corner. The study found that models with smaller test error and lower capacity have stronger attack capability. This is explained by the model's bias for approximating the ground truth and its complexity. The smaller the model-specific component \u2207f \u22a5, the stronger the adversarial examples it can provide. The contribution from vgg-style models was not considered in the analysis. In this study, it was found that adversarial perturbations can be broken down into model-specific and data-dependent components, with the latter being the main contributor to transferability. The noise-reduced gradient (NRG) based methods proposed were more effective in crafting adversarial examples. Models with lower capacity and higher accuracy were shown to be better for black-box attacks. Future work will focus on combining NRG-based methods with adversarial training to defend against such attacks. In future research, the focus will be on combining NRG-based methods with adversarial training to defend against black-box attacks. The data-dependent component contributing to transferability is low-dimensional, making defense against black-box attacks feasible. On the other hand, white-box attacks originating from high-dimensional ambient space are more challenging to defend against. Additionally, exploring stable features for transfer learning by incorporating the NRG strategy is another avenue of interest. The influence of hyperparameters like the number of iterations and step size on the quality of adversarial examples generated using IGSM for targeted black-box attacks is being studied. The performance evaluation is based on the average Top-5 success rate over three ensembles using ResNet152 and VGG16 BN as target models. The performance of ResNet152 and VGG16 BN models is evaluated using the average Top-5 success rate over three ensembles. Experiment results show that an optimal step size is crucial for attack performance, with both too large and too small step sizes yielding harm. More iterations with a small step size can lead to worse performance due to overfitting, while a large step size encourages exploration of model-independent areas. An additional experiment on the MNIST dataset confirms the influence of model redundancy on attack capability. In an additional experiment on the MNIST dataset, models of different depths are considered, with the results showing that low-capacity models have stronger attack capabilities compared to large-capacity models. The Top-1 success rates of cross-model attacks are reported, demonstrating the influence of model redundancy on attack capability."
}