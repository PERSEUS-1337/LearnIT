{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this, a balanced face image dataset with 7 race groups was created, showing improved accuracy across different races and genders. Commercial computer vision APIs were also compared for balanced accuracy. Several large-scale face image datasets have fostered research and development in automated face detection, alignment, recognition, generation, modification, and attribute classification. These systems have been applied in various fields such as security, medicine, education, and social sciences. However, existing public face datasets are biased towards Caucasian faces, with other races significantly underrepresented. Existing public face datasets are biased towards Caucasian faces, with other races significantly underrepresented. Biased data leads to biased models, raising ethical concerns about fairness in automated systems. Commercial computer vision systems have been criticized for their asymmetric accuracy across sub-demographics. Existing public face datasets are biased towards Caucasian faces, leading to concerns about fairness in automated systems. Commercial vision systems have been criticized for their accuracy across different demographics, with biases in training data causing better performance on male and light faces. To address race bias, a novel face dataset with balanced representation of 7 race groups has been proposed, containing 108,501 facial images collected from various sources. The new dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino, with balanced representation. It outperforms existing datasets on novel data, especially for non-White faces, and is the first to include Latino and Middle Eastern groups. This expands the applicability of computer vision methods to various fields. Face attribute recognition aims to classify human attributes like gender, race, age, emotions, and expressions from facial appearance. Existing datasets in this field have been predominantly focused on the White race, but the inclusion of major racial groups like Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino in a new dataset enhances the applicability of computer vision methods. It is crucial for these systems to perform equally well across different gender and race groups to maintain public trust in machine learning and computer vision technologies. Face attribute recognition is essential for classifying human attributes like gender, race, age, emotions, and expressions. It is crucial for these systems to perform equally well across different gender and race groups to maintain public trust in machine learning and computer vision technologies. Instances of racial bias in technology have led to the termination of services or features, prompting commercial providers to stop offering race classifiers. Face attribute recognition is also used in demographic surveys for marketing and social science research. Social science research uses images of people to infer demographic attributes and analyze behaviors. Algorithmic fairness and dataset biases are increasingly important in AI and machine learning communities. The focus is on balanced accuracy in attribute classification, ensuring independence of race and gender. Fairness research aims for models to produce fair outcomes, such as in loan approval. In this paper, the focus is on balanced accuracy in attribute classification, specifically on gender classification from facial images. Previous studies have shown bias in commercial gender classification systems, especially towards dark-skinned females. The research falls into the categories of discovering bias in datasets and improving dataset quality. The study by Gebru (2018) highlighted biases in gender classification systems, particularly towards dark-skinned females. Biased datasets and associations between scene and race contribute to these results. The paper aims to address limitations by collecting more diverse face images from non-White race groups, improving generalization performance. Their dataset includes Southeast Asian and Middle Eastern races, filling a gap in representation and combating discrimination. The dataset includes Southeast Asian and Middle Eastern races, aiming to address biases in gender classification systems and combat discrimination by collecting diverse face images from non-White race groups. The dataset defines 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino, with Latino considered a race based on facial appearance. In the study, the dataset includes 7 race groups for classification, with Latino being considered a distinct race based on facial appearance. The use of skin color as a proxy for race grouping has limitations due to factors like lighting conditions and within-group variations. The dataset includes 7 race groups for classification, with skin color variations being significant. Skin color alone is not sufficient to differentiate between race groups, so race is annotated by human judgment. Existing face datasets sourced from public figures may introduce bias in age and attractiveness. The dataset aims to minimize selection bias by avoiding filtering based on stereotypes or celebrities, focusing on diversity and coverage. It is derived from the Yahoo YFCC100M dataset and is more balanced in terms of race compared to other datasets. The collection process incrementally increased the dataset size by detecting faces without preselection. The dataset, called Diversity in Faces (DiF), is smaller but more balanced on race compared to other datasets. Faces were incrementally collected from the YFCC100M dataset, with demographic compositions of each country considered. The dataset excluded the U.S. and European countries to prevent dominance by White faces. Images with specific Creative Commons licenses were used, and a minimum face size of 50 by 50 pixels was set to ensure attributes are recognizable. The dataset, Diversity in Faces (DiF), is balanced on race and collected from YFCC100M dataset. Faces were annotated for race, gender, and age using Amazon Mechanical Turk. Annotations were refined using a model and manual verification. Skewness in race composition was measured for each dataset. The dataset, Diversity in Faces (DiF), is balanced on race and collected from YFCC100M dataset. Faces were annotated for race, gender, and age using Amazon Mechanical Turk. Annotations were refined using a model and manual verification. Skewness in race composition was measured for each dataset. Most existing face attribute datasets are biased towards the White race, while gender balance ranges from 40%-60% male ratio. Model performance was compared using ResNet-34 architecture trained on each dataset with ADAM optimization. Face detection was done using dlib's CNN-based face detector, and the experiment was conducted in PyTorch. The dataset was compared with UTKFace, LFWA+, and CelebA for evaluation. The dataset Diversity in Faces (DiF) was compared with UTKFace, LFWA+, and CelebA. UTKFace and LFWA+ have race annotations, while CelebA was used only for gender classification. FairFace defines 7 race categories but only 4 were used for comparison. Cross-dataset classifications were performed using models trained from these datasets. Results for race, gender, and age classification across subpopulations were shown in Tables 2 and 3. Each model tended to perform better on the dataset it was trained on. The study compared the Diversity in Faces dataset with UTKFace, LFWA+, and CelebA. CelebA was used for gender classification, while UTKFace and LFWA+ had race annotations. Results for race, gender, and age classification across subpopulations were presented in Tables 2 and 3. The model performed best on the LFWA+ dataset due to its diversity and generalizability. To test generalization, novel datasets from Twitter and media photographs were used. The study utilized images from Twitter users with geo-tags, media photographs from professional outlets, and a protest dataset. Faces were randomly sampled from each set for analysis. The dataset used in the study was obtained from Google Image search using keywords like \"Venezuela protest\" and \"football game\". It includes a diverse range of race and gender groups engaging in various activities across different countries. 8,000 faces were randomly sampled and annotated for gender, race, and age by Amazon Mechanical Turk workers. The FairFace model outperformed other models in terms of accuracy for race, gender, and age on novel datasets, even with smaller training image sizes. This suggests that dataset size is not the only factor influencing performance improvement. The study used a diverse dataset obtained from Google Image search and annotated for gender, race, and age. The FairFace model outperformed others in accuracy for race, gender, and age even with smaller training image sizes. The model also showed more consistent results across different race groups compared to other datasets. The model's fairness was measured by accuracy equality and maximum accuracy disparity. The FairFace model achieved the lowest maximum accuracy disparity in gender classification, with less than 1% accuracy discrepancy between male and female, and White and non-White groups. Other models showed a strong bias towards males, with the LFWA+ model having the biggest gender performance gap at 32%. Commercial computer vision services also exhibit asymmetric gender biases, likely due to unbalanced representation in training data. The dataset used in the experiment had the smallest gender performance gap at 32%. Asymmetric gender biases in computer vision services are likely due to unbalanced training data. The dataset contains diverse faces, with race groups loosely separated in 2D space. FairFace has non-typical examples, while LFWA+ focuses on clusters for face recognition. UTKFace also emphasizes local clusters. The dataset used in the experiment had the smallest gender performance gap at 32%. It contains diverse faces with race groups loosely separated in 2D space. FairFace has non-typical examples, while LFWA+ focuses on clusters for face recognition. UTKFace emphasizes local clusters. The diversity of faces in these datasets was measured by examining the distributions of pairwise distances between faces. UTKFace had tightly clustered faces, while LFWA+ showed diverse faces despite mostly containing white faces. This diversity is attributed to the training data used for face embedding. The face embedding was trained on a white-oriented dataset, effective in separating white faces. Previous studies showed inconsistent classification accuracies across demographic groups. FairFace images were used to test gender classification APIs, with a diverse dataset in terms of race, age, and expressions. 7,476 random samples were used, excluding children under 20. The experiments were conducted in August 2019. FairFace dataset includes faces from various demographic groups for gender classification API testing. Experiments conducted in August 2019. Gender classification accuracies of tested APIs shown in Table 6, with Amazon Rekognition detecting all faces. Table 6 displays gender classification accuracies of tested APIs, with Amazon Rekognition detecting all faces. Two sets of accuracies are reported, with findings showing a bias towards male category and higher error rates for dark-skinned females. Skin color alone is not a sufficient guideline for classification accuracy. The study found that skin color alone is not a reliable indicator of model bias in facial recognition. Gender bias was also observed, with Microsoft's model failing to detect many male faces. A new face image dataset balanced on race, gender, and age was proposed, showing improved classification performance for gender, race, and age compared to existing datasets. This dataset was derived from the Yahoo YFCC100m dataset. The dataset proposed in this study aims to address bias in facial recognition models by providing balanced accuracy across race groups. Derived from the Yahoo YFCC100m dataset, it allows for training new models and verifying classifier accuracy. This dataset can help mitigate race and gender bias in computer vision systems, promoting algorithmic fairness in AI applications."
}