{
    "title": "HyerxgHYvH",
    "content": "We propose a Lego bricks style architecture for evaluating mathematical expressions, using small independent neural networks for specific operations. By reusing these networks, we can solve more complex tasks like n-digit multiplication and division. This approach allows for reusability and generalization up to 7 digit numbers. Our solution introduces reusability and generalization for computations involving n-digits, including multiplication, division, and cross product for up to 7 digit numbers. Unlike existing methods, it also works for both positive and negative numbers. The success of feed-forward Artificial Neural Networks lies in their ability to learn and adapt to specific tasks, but they often lack generalization and struggle with unseen data. This indicates that neural networks primarily rely on memorization rather than understanding inherent rules, leading to a lack in decision-making processes. The learning process in neural networks is based on memorization rather than understanding inherent rules, leading to a lack of quantitative reasoning and systematic abstraction. Generalization can be achieved by reusing memorized examples for complex operations, similar to how children learn arithmetic. In this work, fundamental operations commonly used in arithmetic, such as multiplication and addition, are identified and learned using simple neural networks. These operations are then reused to develop a more complex network capable of solving problems like n-digit multiplication and division. This approach is the first to propose a generalized solution for arithmetic operations, working for both positive and negative numbers. Our solution for arithmetic operations works for both positive and negative numbers, unlike previous methods. Neural networks can approximate mathematical functions, but generalizing over unseen data makes network architecture complex. Recent works have tried to train networks to generalize over minimal training data. EqnMaster uses generative recurrent networks for arithmetic functions, but still struggles with generalization. Recent works in the field of neural networks have focused on generalizing arithmetic functions over minimal training data. EqnMaster utilizes generative recurrent networks for arithmetic functions but struggles with generalization, particularly with 3-digit numbers. The Neural Arithmetic Logic Unit (NALU) employs linear activations to predict arithmetic function outputs, highlighting the challenge of extrapolation in end-to-end learning tasks. Additionally, simple Feed Forward Networks and Optimal Depth Networks offer alternative approaches to solving arithmetic expressions efficiently. Our work builds on the concept of Optimal Depth Networks and Binary Multiplier Neural Networks to design a neural network that can predict the output of basic arithmetic functions for both positive and negative decimal integers. Unlike existing models that only work on limited digits, our proposed network aims to predict outputs for various functions by training several smaller networks. Our proposed network trains several smaller networks to perform different subtasks needed for complex arithmetic operations like signed multiplication, division, and cross product. We use a combination of these smaller networks to design networks capable of handling complex tasks. Digital circuits implement these operations using shift and accumulator methods, known for their accuracy and scalability. Initial work shows neural networks can also be utilized for these operations. Neural networks can simulate digital circuits for arithmetic operations like multiplication and division. By designing smaller networks for fundamental operations such as addition, subtraction, and multiplication, complex functions like arithmetic equation calculation can be achieved. The basic function of a neuron network involves a sum transform where inputs are multiplied by weights. The neural network can perform arithmetic operations like addition and subtraction using single neurons with specific weights. These modules simplify tasks like shift-and-add multiplication and place value shifting to create complex functions like arithmetic equation calculation. The neural network simplifies arithmetic operations like multiplication by combining outputs of multiplications and using a place value shifter. It has a single neuron with linear activation and can handle n inputs with 1 digit each. The network uses fixed weights for each digit and has 2 input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. The model computes the result of single digit multiplication and absolute value using a neural network with 2 hidden layers. The neural network simplifies arithmetic operations like multiplication by combining outputs of multiplications and using a place value shifter. It computes the absolute value of a single number using a neural network with 2 hidden layers. The network includes an input sign calculator and an output sign calculator to extract and compute signs of input numbers. The neural network simplifies arithmetic operations like multiplication by combining outputs of multiplications and using a place value shifter. It computes the absolute value of a single number using a neural network with 2 hidden layers. The network includes an input sign calculator and an output sign calculator to extract and compute signs of input numbers. The network then uses modulus activation in the second layer and subtraction of 1 in the final layer to predict the output of sign multiplication. Signed multiplication is performed by converting numbers to positive integers, using a multiply sub module for single digit multiplication. The multiplication process involves tokenizing inputs into single digits and performing single digit multiplication operations. Each result is combined to form a single number, with the final output assigned a sign using 1-digit sign multiply. The division model also separates sign and magnitude during pre-processing, inspired by the long division model. The architecture for division model is inspired by long division, where n-digit divisor controls the output computation. Single digit multipliers are used to subtract from selected n-digit chunk of dividend. The selected node represents the remainder and quotient result for each chunk. The quotient is combined over iterations and the remainder is knitted to the next digit in the divisor. The division model is based on digital circuitry for decimal digits. The division model is based on digital circuitry for decimal digits. A comparison is made with the Neural Arithmetic and Logic Unit (NALU) implementation, showing superior performance in signed arithmetic operations. The model achieves 100% accuracy within the testing range and includes signed multiplication exclusively. Experiment 1 demonstrates the model's superiority over recurrent and discriminative networks, achieving 100% accuracy within the testing range. The signed multiplication feature is exclusive to this model. Experiment 2 compares results with the Neural Arithmetic and Logic Unit (NALU), showcasing performance in division operations. The NALU network fails outside the range of 10-20. The study suggests breaking down complex tasks into smaller sub-tasks for efficient training. In this paper, the approach of breaking down complex tasks into smaller sub-tasks is discussed. Fundamental arithmetic operations are identified and learned using simple neural networks. These smaller networks are then combined to solve more complex problems like n-digit multiplication and division. The use of float operation in the tokenizer is a limitation, but it does not hinder the current work as only pre-trained smaller networks are used. The proposed work involves using float operations in the tokenizer for training complex networks. This does not hinder the current work as only pre-trained smaller networks are used. A cross product network has been designed and its accuracy is being tested. Future work includes developing a point cloud segmentation algorithm using a larger number of identical smaller networks."
}