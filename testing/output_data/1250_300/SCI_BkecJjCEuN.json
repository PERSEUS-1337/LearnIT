{
    "title": "BkecJjCEuN",
    "content": "The aim of this work is to enhance the label efficiency of large neural networks for audio data by combining multitask and self-supervised learning on unlabeled data. Training an end-to-end audio feature extractor based on WaveNet, along with self-supervised tasks on unlabeled audio corpus, improves supervised classification tasks by up to 6%. Data augmentation in multitask setting further boosts performance. Deep neural networks are crucial for modeling auditory data but require large labeled datasets. Incorporating data augmentation into our multitask setting leads to further performance gains in modeling and classifying auditory data using deep neural networks. While labeled datasets are scarce, leveraging unlabeled data for self-supervised learning shows promise in improving model generalization. Our contributions include identifying effective self-supervised audio tasks and demonstrating their joint training with supervised tasks to enhance performance. Additionally, we showcase the use of WaveNet as a versatile feature extractor. The paper demonstrates the successful use of self-supervised audio tasks in conjunction with supervised tasks to improve performance. WaveNet is utilized as a feature extractor for rich audio representations. The framework is applied to three classification tasks and shows the potential of leveraging unlabeled data for performance enhancement. The proposed self-supervised tasks can also serve as a pre-training stage for transfer learning. Multitask learning can improve single-task performance by uncovering underlying structure common to tasks. Shared representations help pool data from different datasets, but labeled datasets are scarce. Self-supervised learning can leverage unlabeled data to address label scarcity. Self-supervised learning has shown promising results in the visual domain, but little previous work has explored its potential in the audio domain. An end-to-end audio processing network was implemented, utilizing a common embedding of the acoustic waveform within a \"trunk\" network modeled after the WaveNet architecture. The trunk and head networks are trained jointly for each experiment, with models consisting of a single supervised \"main\" task trained alongside 0 to 3 self-supervised \"auxiliary\" tasks. The experiments involve training a WaveNet trunk with 3 blocks of 6 dilation stacks each, resulting in an effective receptive field length of 190 samples. Three supervised tasks - audio tagging, speaker identification, and speech command recognition - are tested with up to three self-supervised tasks. The audio tagging task is trained on the FSDKaggle2018 dataset from Freesound. The audio tagging task is trained on the FSDKaggle2018 dataset from Freesound, containing 11,073 audio files. Each segment is cropped to 2 seconds, padded with zeros if needed, and fed into a network with a fully-connected layer for classification. Speaker identification task is trained on VoxCeleb-1 dataset with data from 1251 speakers sourced from interviews. The dataset BID12 contains 336 hours of data from 1251 speakers sourced from interviews with celebrities. Each audio segment is cropped to 2 seconds and normalized before being fed into the network. The speech command recognition task is trained on the Speech Commands dataset BID21, consisting of 65,000 utterances of 30 short words. The head architecture features a global average pooling layer, 2-layer perceptron, batch normalization, ReLU nonlinearity, and softmax layer with cross-entropy loss evaluation. The speech command recognition system processes 30 short words in one-second WAVE format files, categorized into 12 groups. It uses a stack of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The convolution layers have widths of 100, 50, and 25, with strides of 16, 8, and 4. Self-supervised tasks like next-step prediction, noise reduction, and upsampling are implemented alongside the main supervised tasks, trained on both labeled and unlabeled data from the Librispeech dataset. The primary goal was to develop a multitask framework for audio using waveform inputs instead of high-level feature representations like spectrograms. Convolutional architectures trained on raw waveforms allow for a wider range of audio processing tasks compared to spectral/cepstral representations. This approach avoids restricting the network architectures for different tasks, enabling a better understanding of the interaction between learning dynamics of disparate tasks. The study focused on improving performance through multitask learning with raw audio inputs instead of spectral representations. By incorporating additional unsupervised tasks, such as joint training with three self-supervised tasks, improvements were seen in supervised tasks like audio tagging. The inclusion of larger unlabeled datasets further enhanced performance metrics. Multitask learning with unlabeled data improved performance metrics across various tasks. Incorporating larger datasets led to increased performance, with a MAP@3 increase of up to .056 with an additional 500 hours of unlabeled data. Speech command classification and speaker identification tasks also showed improvements with more unlabeled data. Multitask learning proved to enhance supervised tasks without the need for additional labeled data. Multitask learning improves performance without extra labeled data. Comparing with data augmentation, pitch-shift augmentation increased MAP@3 by .066, noise augmentation by .024. Noisy data augmentation results similar to self-supervised noise-reduction task. Combining pitch-shift augmentation and self-supervised tasks yielded highest MAP@3 increase of .089, showing complementary label efficiency methods. Transfer learning in computer vision now includes knowledge transfer from self-supervised tasks on unlabeled data. Transfer learning experiments were conducted on self-supervised tasks trained on unlabeled data, followed by fine-tuning with a small amount of labeled data for a supervised task. Results favored transfer learning over training all tasks simultaneously. The study showed that jointly training a supervised task with multiple self-supervised tasks using a WaveNet-based model on raw audio waveforms led to performance gains. Our approach involves training a WaveNet-based model on raw audio waveforms with multiple self-supervised tasks to improve performance on supervised tasks. The methodology shows promise for a broad range of audio classification tasks and raises questions about the representation learned by the multitasking model. Further exploration could lead to handling a wider range of auditory tasks. The WaveNet model is used to process raw audio signals with high temporal resolution. It employs causal dilated convolutions for parallel processing, making it faster than RNNs. Task-specific neural networks are built on top of a task-agnostic trunk following the WaveNet architecture. The WaveNet trunk architecture consists of stacked dilated causal convolutions with residual connections and nonlinearities. Each block contains S dilated convolution layers with increasing dilation factors. Residual atom computations involve \"Filter\" and \"Gate\" operations to produce hidden state vectors and layer outputs. The WaveNet trunk architecture utilizes stacked dilated causal convolutions with residual connections and nonlinearities. Each block consists of S dilated convolution layers with increasing dilation factors, resulting in an effective receptive field of 1 + b(2S-1). After a hyperparameter search, N=3 blocks with S=6 layers each were chosen, giving the trunk a total receptive field of \u03c4=190, equivalent to about 12 milliseconds of audio sampled at 16kHz. Each task-specific head processes input data after passing through the shared trunk. The task-specific heads in the WaveNet architecture are simple neural networks that process input data after passing through a shared trunk. Each head operates independently with its own objective function, optimizer, and learning rates. Supervised tasks are designated as primary, while self-supervised tasks are considered auxiliary. The head architectures are designed to be simple, forcing the shared trunk to learn a representation suitable for multiple audio tasks. The next-step prediction task is a key component of the architecture. The head architectures in the WaveNet model are designed to be simple, with a focus on using as few layers as necessary. The next-step prediction task involves predicting the next value in a sequence of audio frames, allowing for the creation of large training datasets from unlabeled audio data. The prediction head consists of a 2-layer stack of 1x1 convolutional layers with ReLU nonlinearities, treating the task as a regression problem with mean squared error as the loss function. The WaveNet model's prediction head uses a 2-layer stack of 1x1 convolutional layers for next-step prediction in audio sequences, treating it as a regression problem with mean squared error as the loss function. Noise reduction task involves predicting clean audio samples from noisy ones by treating noise as an additive random process. The noise reduction task in the WaveNet model involves training a structure similar to the next-step prediction head to minimize a smoothed L1 loss between clean and noisy waveform inputs. Additionally, an unsupervised upsampling task can be created by downsampling the audio source and using the original signal as the target, akin to the \"super-resolution\" task in computer vision. The unsupervised upsampling task involves downsampling the audio source and using the original signal as the target. The network's job is to infer the high frequency information lost during the transform, with a structure similar to the next-step prediction and noise reduction tasks. The model is trained using raw audio waveform inputs from specific datasets in the PyTorch framework. The model was trained using raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets in the PyTorch framework. Audio samples were cropped to two seconds, downsampled to 16 kHz, and normalized. Noise-reduction task involved adding noise from ChiME3 datasets at varying SNR levels. Hyperparameter search was conducted for the number of blocks, layers, units, and learning rate. The study conducted hyperparameter search for the model architecture, including the number of blocks, layers, units, and learning rate. The performance of the network was not significantly impacted by specific architecture choices. Additionally, the model was trained on multiple tasks simultaneously using a weighted sum of the losses for each task. In the study, the model architecture was optimized through hyperparameter search. The model was trained on multiple tasks simultaneously using a weighted sum of losses. The optimization process included using the \"Adam\" optimizer with specific parameters and decaying the learning rate every 5 epochs. The batch size used was 48, and additional tasks required separate forward propagation. Important parameters can be found in TAB3."
}