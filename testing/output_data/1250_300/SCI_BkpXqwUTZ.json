{
    "title": "BkpXqwUTZ",
    "content": "In vanilla backpropagation, the choice of activation function is crucial for non-linearity and differentiability. This study introduces a method using iterative temporal differencing with fixed random feedback weight alignment to replace the derivative of the activation function. This approach makes error backpropagation possible without the need for a differentiable activation function, leading to a more biologically plausible way of learning deep neural network architectures. The study introduces iterative temporal differencing with fixed random feedback weight alignment as a biologically plausible approach for error backpropagation in deep learning. It aims to integrate spike-time dependent plasticity (STDP) into deep learning, inspired by the success of deep reinforcement learning mimicking dopamine effects in the brain. Additionally, hierarchical convolutional neural networks have been biologically inspired by the visual cortex. The study explores integrating spike-time dependent plasticity (STDP) into deep learning by using fixed random synaptic feedback weights alignments (FBA) in error backpropagation. It introduces the concept of segregated dendrites for modeling STDP successfully. The implementation includes using Tanh activation function on the MNIST dataset and comparing VBP, FBA, and ITD using maximum cross entropy (MCE) as the loss function. Additionally, ITD with MCE is compared to ITD with least squared error (LSE) with equal hyperparameters. In this study, the focus is on integrating spike-time dependent plasticity (STDP) into deep learning using fixed random synaptic feedback weights alignments (FBA) in error backpropagation. The experiments compare VBP, FBA, and ITD using maximum cross entropy (MCE) as the loss function. The hyperparameters for both experiments are equal, with 5000 iterations/epochs, 0.01 learning rate, 100 minibatch size, and 32 hidden layers in a 2-layer deep feed-forward neural network architecture. The goal is to move towards a more biologically plausible backpropagation for deep learning, with future steps involving investigating STDP processes, dopamine-based unsupervised learning, and generating Poisson-based spikes."
}