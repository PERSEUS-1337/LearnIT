{
    "title": "Byl5NREFDr",
    "content": "Model extraction in natural language processing involves an adversary reconstructing a victim model using only query access. The attacker can successfully extract the model without real training data by using random word sequences and task-specific heuristics. This exploit is possible due to the prevalence of transfer learning methods in NLP. Defense strategies like membership classification and API watermarking can be effective but may be circumvented by clever adversaries. Machine learning models are valuable intellectual property, often accessed through web APIs. Adversaries may attempt model extraction by querying the API to train a local copy of the model. Extracted models can leak sensitive information or be used to generate adversarial examples. Defense strategies like membership classification and API watermarking can help protect against model extraction but may be circumvented by clever adversaries. NLP APIs based on ELMo and BERT have gained popularity for transfer learning. Pretrained representations enhance performance and reduce complexity. Fine-tuned BERT models can be extracted without access to training data, even with random queries. Extraction attacks on BERT models are possible without access to training data or well-formed queries. Using randomly sampled words or sentences from Wikipedia, attackers can extract information cheaply, with the most expensive attack costing around $500. The process involves querying the victim BERT model and fine-tuning the attacker's own model using the victim's outputs as labels. The attacker samples words to form queries and sends them to the victim BERT model. They then fine-tune their own BERT using the victim's outputs as labels. The process works even with random sequences of words. The effectiveness of the randomly-generated queries for model extraction is explored, showing that despite being nonsensical, they work well. Pretraining on the attacker's side makes model extraction easier. Simple defenses like membership classification and API watermarking are ineffective against clever adversaries. The study evaluates defenses against model extraction attacks, finding that simple defenses like membership classification and API watermarking are ineffective against clever adversaries. The research aims to inspire stronger defenses and a better understanding of vulnerabilities in models and datasets. The work is related to prior efforts on model extraction in computer vision applications and zero-shot distillation in NLP systems. The study evaluates defenses against model extraction attacks, finding that simple defenses like membership classification and API watermarking are ineffective against clever adversaries. Prior work on model extraction in computer vision applications and zero-shot distillation in NLP systems is related. In contrast, recent work focuses on extracting nonsensical inputs on modern BERT-large models for tasks like question answering. BERT, a 24-layer transformer model, is studied for model extraction. Unnatural text inputs have been shown to train models effectively for NLP tasks without real examples. BERT, a 24-layer transformer model known as Bidirectional Encoder Representations from Transformers, has revolutionized NLP by achieving state-of-the-art performance on various tasks with minimal supervision. It utilizes contextualized vector representations learned through masked language modeling on unlabeled text data. Fine-tuning methodologies are commonly used with task-specific networks to further enhance performance. A 1-layer feedforward network is used to construct a composite function g T = f T,\u03c6 \u2022 f bert,\u03b8, with parameters \u03c6 and v as input. The parameters \u03c6 T , \u03b8 T are learned end-to-end using training data for task T with a small learning rate. Description of extraction attacks involves reconstructing a local copy g T (the \"extracted model\") from a commercially available black-box API for task T. The attacker uses a task-specific query generator to construct nonsensical word sequences as queries to the victim model, then fine-tunes the public release of f bert,\u03b8 * on the resulting dataset to obtain g T. The extraction attacks involve extracting models from diverse NLP tasks using different query generators: RANDOM and WIKI. The tasks include binary sentiment classification, ternary NLI classification, extractive question answering, and boolean question answering. The input queries for RANDOM are nonsensical word sequences, while WIKI uses actual sentences or paragraphs. The extraction attacks involve using query generators RANDOM and WIKI to extract models from NLP tasks. These generators are found insufficient for tasks requiring complex interactions between different parts of the input space. Task-specific heuristics are applied to improve the generation process. Representative example queries and outputs are provided in Table 1, with more examples in the appendix. What is the evaluation process for the extraction procedure using query budgets for different tasks? Representative example queries and outputs are shown in Table 1, with more examples in the appendix. Evaluation metrics include accuracy of extracted models on the original development set and agreement with victim models. Commercial cost estimates for query budgets are provided using Google Cloud Platform's Natural Language API calculator. High accuracies are achieved even at low query budgets, with diminishing gains at higher budgets. In a controlled setting, extracted models show high accuracy on original development sets across tasks, even when trained with nonsensical inputs. However, agreement between extracted and victim models is only slightly better than accuracy, with lower agreement on held-out sets. For SQuAD, extracted models have low agreements despite being trained on the same data distribution. An ablation study with alternative query generation heuristics is conducted for SQuAD and MNLI. The study conducted an ablation study with alternative query generation heuristics for SQuAD and MNLI. They also analyzed the impact of classification with argmax labels only on model extraction, showing minimal drop in accuracy. Additionally, the effectiveness of extraction algorithms with varying query budgets was measured, indicating successful extraction even with small query budgets. In a study analyzing model extraction algorithms with varying query budgets, even small budgets can lead to successful extraction. The effectiveness of extraction with nonsensical input queries for NLP models based on BERT is explored, raising questions about the properties of these queries and their impact on model performance. The study specifically examines the RANDOM and WIKI extraction configurations for SQuAD. In this section, the study focuses on the RANDOM and WIKI extraction configurations for SQuAD to determine if certain queries are more representative of the original data distribution. Different victim models show varying levels of agreement on nonsensical queries, with higher agreement on SQuAD training and development set queries compared to WIKI and RANDOM queries. The results suggest that victim models are often brittle on nonsensical inputs, but high-agreement queries may be more useful for model extraction. High-agreement queries are closer to the original data distribution, showing that they are more useful for model extraction. Extracting models using high-agreement subsets results in large F1 improvements, beating random and low-agreement subsets. This indicates that agreement between victim models is a good proxy for input-output pair quality in extraction. The interpretability of high-agreement nonsensical queries to humans remains a topic for future exploration. The study explores the interpretability of high-agreement nonsensical queries to humans by comparing human annotators' responses to SQuAD questions from different subsets. Annotators showed lower exact match scores on nonsensical queries compared to original SQuAD questions, indicating a reliance on word overlap heuristics for answer selection. This suggests that while some interpretation is possible, the majority of the signal in nonsensical queries remains uninterpretable. In practical scenarios, the attacker's lack of information about the victim's architecture can impact the extraction accuracy when fine-tuning different base models. The study compares the accuracy of different model configurations on MNLI and SQuAD, showing higher accuracy when the attacker starts from BERT-large. The study compares the accuracy of different model configurations on MNLI and SQuAD, showing higher accuracy when the attacker starts from BERT-large. Fine-tuning BERT gives attackers a significant headstart due to the good starting representation of language. QANet achieves high accuracy on SQuAD with BERT-large labels, indicating sufficient model capacity. The study explores the impact of pretraining on model accuracy, highlighting the vulnerability of BERT-based models to model extraction. Two defense strategies are discussed, focusing on preserving API utility and remaining undetectable to attackers without requiring re-training the victim model. The study discusses two defenses against model extraction without re-training the victim model. The first defense uses membership inference to detect nonsensical inputs or adversarial examples, issuing random outputs to eliminate extraction signals. Membership inference is treated as a binary classification problem using MNLI and SQuAD datasets. Classifiers trained on victim model features transfer well to a balanced development set. The study discusses using model confidence scores and rare word representations as input features for training the classifier. The classifiers transfer well to a balanced development set and are robust to query generation processes. Another defense strategy discussed is watermarking, where a fraction of queries are modified to detect extracted models. Watermarking is a defense strategy to detect extracted models by modifying a fraction of queries. Results show that models perform similarly on the development set with or without watermarking, but non-watermarked models struggle with watermarked queries. Watermarking is effective in detecting extracted models, with non-watermarked models struggling with watermarked queries. However, limitations exist as it can only be used after an attack has occurred and assumes public deployment of the extracted model. Attackers can evade detection through various methods such as differentially private training or issuing random outputs on queries. Model extraction attacks against NLP APIs serving BERT-based models are shown to be highly effective. Model extraction attacks against NLP APIs serving BERT-based models are effective, even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, requiring further research for robust defenses against adaptive adversaries. Future directions include leveraging nonsensical inputs for model distillation, using query efficiency to diagnose dataset complexity, and exploring victim model agreement for active learning in model extraction. In this paper, the authors discuss the use of input data for model extraction attacks against NLP APIs serving BERT-based models. They diagnose dataset complexity using query efficiency and investigate victim model agreement for active learning in model extraction. The cost estimate for Google Cloud Platform's Natural Language APIs is also considered, with inputs limited to 1000 characters per query. The costs for different datasets are calculated by counting input instances with more than 1000 characters multiple times. The authors extrapolated costs for tasks not covered by Google Cloud APIs, such as entity analysis and sentiment analysis, based on similar models' FLOPs requirements. The cost of issuing queries for model extraction attacks against NLP APIs can vary depending on factors like computing infrastructure and revenue models. Attackers could exploit free query limits, set up multiple accounts, or use web scraping techniques to extract data at scale. It is important to focus on the relatively low costs of extracting datasets rather than specific cost estimates, considering the variability in API costs. The cost of extracting datasets is relatively low compared to specific cost estimates for tasks like machine translation and speech recognition. For example, it costs -$430.56 to extract a large conversational speech recognition dataset and $2000.00 for 1 million translation queries. Different input generation algorithms are used for each dataset, such as building a vocabulary using wikitext103 and randomly sampling tokens. The top-10000 wikitext103 vocabulary is used to build vocabularies for various tasks like SST2, MNLI, and SQuAD. Words not in the top-10000 are replaced randomly. In MNLI, the premise is sampled similarly to SST2, while the hypothesis is sampled randomly. For SQuAD, a vocabulary is built using wikitext103, and paragraphs are constructed by sampling tokens from the full vocabulary. The final paragraph is constructed by sampling tokens from the unigram distribution of wikitext103. Questions are sampled by randomly selecting paragraph tokens and appending them with a question starter word. (SQuAD, WIKI) -A paragraph is chosen at random from wikitext103 to generate questions. (BoolQ, RANDOM) -Questions are generated by sampling paragraph tokens without appending a question mark. (BoolQ, WIKI) -Questions are generated similarly to SQuAD, WIKI without adding a question mark. In this section, additional query generation heuristics are studied. A comparison of extraction datasets for SQuAD 1.1 is presented in Table 11, showing that starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. An ablation study on MNLI in Table 12 reveals that low lexical overlap between premise and hypothesis leads to neutral or contradiction predictions, while high overlap results in entailment predictions. The study found that low lexical overlap between premise and hypothesis leads to neutral or contradiction predictions, while high overlap results in entailment predictions. Human annotators were asked to evaluate different question sets for extraction datasets, including original SQuAD questions, WIKI questions with high and low victim model agreement, and RANDOM questions with high and low victim model agreement. The study evaluated different question sets for extraction datasets, including original SQuAD questions, WIKI questions with high and low victim model agreement, and RANDOM questions with high and low victim model agreement. An ablation study on input features for the membership classifier was conducted, comparing the effectiveness of logits from the BERT classifier and the last layer representations. The results showed that the last layer representations were more effective than the logits. The study compared the effectiveness of logits and last layer representations in a membership classifier. Results in Table 9 show that last layer representations are more effective in distinguishing between real and fake inputs. Using both feature sets yielded the best results."
}