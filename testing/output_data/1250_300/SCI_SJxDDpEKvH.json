{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. A non-statistical framework based on identifying modular organization of the network allows for targeted interventions on image datasets, enabling applications like style transfer and robustness assessment in pattern recognition systems. Deep generative models, such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE), aim to create realistic images with disentangled latent representations for interpretability. However, current models lack a mechanistic or causal understanding of image properties. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, like generating objects in new backgrounds. A modular organization of generative models can improve interpretability and enable extrapolations, supporting adaptability and decision making. The proposed causal framework explores modularity, aiming to leverage deep generative architectures for such extrapolations. The paper proposes a causal framework to explore modularity in generative models, focusing on Independent Mechanisms. It discusses the effect of direct interventions in the network and the use of counterfactuals to assess the role of internal variables. The study analyzes disentanglement in VAEs and GANs trained on image databases, showing modularity in their hidden units. The paper explores disentanglement in generative models like VAEs and GANs, focusing on modularity of hidden units through unsupervised counterfactual manipulations. It introduces the concept of intrinsic disentanglement to uncover internal network organization. Related work includes interpretability of convolutional neural networks and works on supervised or unsupervised disentanglement of latent variables. The paper introduces the concept of intrinsic disentanglement to uncover the internal organization of networks, contrasting with other approaches that require semantic information or rely on group representation theory. It also discusses an interventional approach to disentanglement in a classical graphical model setting. Suter et al. (2018) focus on extrinsic disentanglement in a graphical model setting, developing measures of interventional robustness based on labeled data. They introduce a framework to define disentanglement and connect it to causal concepts, presenting the theory informally for easier understanding. The generative model M maps a latent space Z to a manifold Y M where data points live, with a representation mapping r from Y M to a representation space R. The generative model M maps a latent space Z to a manifold Y M where data points live, with a representation mapping r from Y M to a representation space R. A Causal Generative Model (CGM) is used to implement the mapping g M through a succession of operations. Endogenous variables are chosen to compute the mapping g M by composing the endogenous variable assignment v M with the endogenous mapping g M. The CGM framework allows defining counterfactuals in the network following Pearl (2014). Unit level counterfactuals are obtained by replacing assignments of subset variables with a vector of assignments. This intervention model allows for the analysis of latent inputs in the network. Unit-level counterfactuals in a CGM involve replacing subset variable assignments with a vector of assignments, allowing for the analysis of latent inputs. Counterfactuals induce a transformation of the generative model output, with non-faithful counterfactuals potentially leading to artifactual outputs or extrapolation to unseen data. The concept of disentangled representation suggests that individual latent variables sparsely encode real-world transformations. The concept of disentangled representation involves individual latent variables encoding real-world transformations, driving supervised approaches to manipulating relevant transformations explicitly. Unsupervised learning approaches aim to learn these transformations from unlabeled data, with SOTA methods encoding changes in latent factors to enforce conditional independence. This statistical approach faces challenges due to constraints on the prior distribution of latent variables, imposing statistical independence between disentangled factors on the data distribution. The statistical approach to disentangled representation faces challenges with i.i.d. constraints on latent variables, leading to issues with independence and ill-posed problems. Current unsupervised approaches are limited to synthetic datasets and CelebA, with disentangled generative models showing lower visual quality on complex real-world data. A non-statistical definition of disentanglement is proposed based on transformation-based insights. The text proposes a non-statistical definition of disentanglement based on transformation-based insights, focusing on the transformation of latent variables to encode different properties independently. This notion, called extrinsic disentanglement, aims to improve visual sample quality in generative models. The notion of extrinsic disentanglement focuses on transforming latent variables to encode properties independently, allowing for interventions on interesting data properties. This extends the definition of disentanglement beyond statistical independence to uncover related but disentangled properties. The concept of extrinsic disentanglement involves transforming latent variables to encode independent properties for interventions on data. This definition extends beyond statistical independence to reveal related yet disentangled properties, allowing for the implementation of arbitrary disentangled transformations through modularity in the internal representation. The concept of modularity in disentangled transformations involves defining a subset of endogenous variables as modular, allowing for transformations within its input domain to be disentangled. This framework extends to multiple modules and requires a partition of latent variables into modules for a truly disentangled representation. Our framework suggests that a disentangled representation requires partitioning latent variables into modules, which was not considered in traditional approaches. This modular structure allows for a broad class of disentangled transformations and emphasizes the importance of grouping neurons into modules for meaningful representations. Propositions 1 and 2 suggest that finding a modular structure in a network enables various disentangled transformations. Counterfactual interventions define transformations, and by assigning a constant value to endogenous variables, faithful counterfactuals can be achieved. Sampling from the joint marginal distribution of variables in E avoids characterizing VEM. In a feed-forward neural network, endogenous variables are chosen as output activations of channels of a layer. The hybridization procedure involves generating original examples of output using independent latent variables z1 and z2. The text discusses a framework for hybridizing examples in a neural network to assess the impact of a modular structure on the output. By generating original examples using independent latent variables, the framework allows for the creation of hybrid examples by combining features from different generated images. This approach helps quantify the causal effect of a given module on the output of the generator network. The framework quantifies the causal effect of a module on the output of the generator by generating pairs of latent vectors and estimating an influence map through hybrid outputs. The absolute unit-level causal effects are averaged over different interventions to create a grayscale heat-map pixel map. A scalar quantity is defined to measure the magnitude of the causal effect. The framework quantifies the causal effect of modules on the output image by generating influence maps through hybrid outputs. Influence maps are grouped to define modules at a coarser scale, showing functional segregation in channels of convolutional layers. The VAE trained on the CelebA face dataset shows functionally segregated channels in its layers, influencing different aspects of the output image. To group channels into modules, clustering is performed using EIMs as feature vectors. Pre-processing steps include spatial smoothing and thresholding before applying NMF algorithm to obtain cluster template patterns. The NMF algorithm is used to factorize S = WH, resulting in K cluster template patterns. Each influence map is assigned a cluster based on the template pattern with maximum weight. The choice of NMF is justified by its success in isolating meaningful parts of images. The approach will be compared to k-means clustering, and a toy generative model is introduced to further justify the NMF based approach. The model involves endogenous variables mapped to output with matrices, random choice for model parameters, and specific conditions on indices to encode influences in image areas. The identifiability result for Model 1 is provided. The model involves endogenous variables mapped to output with matrices and specific conditions on indices to encode influences in image areas. For Model 1, the partition of the hidden layer corresponds to a disentangled representation, justifying the use of NMF and sliding window to enforce similarity between influence maps. Investigating modularity of generative models on the CelebA dataset using basic architectures like \u03b2-VAE. We investigated modularity of generative models on the CelebA dataset using a basic architecture like \u03b2-VAE. The results showed that setting the number of clusters to 3 led to highly interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed these findings. The cluster stability analysis showed that using 3 clusters led to high consistency (>90%) and outperformed k-means clustering. The cosine similarity between matching clusters was .9, indicating robustness with 3 clusters. The results of the cluster stability analysis showed high consistency with 3 clusters, achieving an average cosine similarity of .9 between matching clusters. Applying the hybridization procedure to the resulting modules obtained by clustering led to a replacement of features while maintaining the overall image structure. Further research is needed to explore better extrinsic disentanglement approaches. Further research is needed to explore better extrinsic disentanglement approaches in models not explicitly optimized for disentanglement, such as GAN-like architectures. Experiments with basic models and a pretrained Boundary Equilibrium GAN (BEGAN) showed promising results in visual quality for higher resolution face images. Using a pretrained Boundary Equilibrium GAN (BEGAN), interventions on layers 5 and 6 resulted in selective transfer of features from Original 2 to Original 1, with clear hair transfer observed. The model, trained on face images with a tighter frame, showed modules encoding different face features. The study observed selective feature transfer in a pretrained BEGAN model, with clear hair transfer seen in one module. Other modules encoded various face features. Evaluation using FID showed minimal impact on image quality. The approach was tested on a BigGAN-deep architecture pretrained on ImageNet, showing scalability to high-resolution generative models. The study demonstrated the ability to generate hybrids by mixing features of different classes using a conditional GAN architecture. Counterfactuals were effectively generated by intervening on two successive layers within a Gblock. Examples showed high-quality counterfactuals with modified backgrounds while maintaining similar foreground objects. Meaningful combinations of objects of different nature were also generated, such as a teddy bear in a tree or a \"teddy-koala\" merging textures. The generated counterfactual images can be used to probe and improve classifier robustness. The study generated hybrids by mixing features of different classes using a conditional GAN architecture. Counterfactual images were created by intervening on two successive layers within a Gblock, resulting in high-quality images with modified backgrounds while maintaining similar foreground objects. The ability of several pretrained classifiers to recognize the original classes was compared, showing that recognition rates tend to increase with layer depth. Different classifiers rely on different aspects of the image content to make their decisions. The study introduced a mathematical definition of disentanglement and applied it to characterize the representation encoded by different groups of channels in deep generative architectures. Evidence for interpretable modules of internal variables in generative models was found, contributing to a better understanding of complex generative architectures and applications such as style transfer and automated assessment of object recognition systems. This research direction focuses on enhancing the interpretability of deep neural networks and their ability to be used for tasks they were not originally trained for. It explores the use of trained generator architectures as mechanistic models, allowing for manipulation of different parts independently. Structural causal models are utilized to represent these models, enabling a better understanding of the system under study and potential external influences. The text discusses the use of structural causal models to represent computational graphs in neural network implementations, focusing on a Causal Generative Model capturing the relations between input latent variables, generator output, and endogenous variables. This approach aims to enhance interpretability and manipulation of deep neural networks for tasks they were not originally trained for. The text discusses the use of structural causal models in neural networks, specifically focusing on a Causal Generative Model (CGM) that decomposes the generator's output into intermediate representations. The CGM comprises a directed acyclic graph and a set of deterministic structural equations. This approach aligns with Pearl's definition of a deterministic structural causal model, with specificities reflecting practical model structures. The text discusses the use of structural causal models in neural networks, specifically focusing on a Causal Generative Model (CGM) that decomposes the generator's output into intermediate representations. This approach aligns with Pearl's definition of a deterministic structural causal model, with specificities reflecting practical model structures such as variable assignments involving latent/exogenous variables in feed-forward networks. The model guarantees unambiguous assignments of endogenous variables once latent inputs are chosen, allowing for useful mappings and defining outputs on manifolds of smaller dimensions. The text discusses embedded CGMs, which decompose the generator's output into intermediate representations. These models guarantee unambiguous assignments of endogenous variables once latent inputs are chosen, defining outputs on manifolds of smaller dimensions. The image sets are constrained by the parameters of the model and are typically not easy to characterize. Embedded CGMs decompose the generator's output into intermediate representations, ensuring unambiguous assignments of endogenous variables. The image sets are constrained by the model's parameters and are difficult to characterize. The image set Y M is crucial as it should approximate the support of the target data distribution. Learning the generator parameters to match Y M with the data distribution support is a key goal for generative models. Topology of Y M is respected in transformations, using embeddings for inversion of g M. Injectivity of g M is a key requirement for embedded CGMs. If the CGM M's Z is compact, then M is... The continuous inverse function in embedded CGMs requires injectivity of g M. If Z of CGM M is compact, then M is embedded only if g M is injective. This framework allows defining counterfactuals in the network following Pearl (2014). The CGM framework defines counterfactuals in the network following Pearl (2014). It involves unit-level counterfactuals for endogenous variables and their assignments. Counterfactuals induce a transformation of the generative model output, related to disentanglement and intrinsic disentanglement within the network. Intrinsic disentanglement in a CGM model involves a transformation T that affects only a subset E of endogenous variables, ensuring robustness to perturbations. This concept relates to a causal interpretation of the model's structure and is supported by the compactness of Z and the Hausdorff property of g M. Armstrong (2013) proves that a continuous and injective mapping g M is an embedding when Z is compact and the codomain of g M is Hausdorff. The equivalence between faithful and disentangled transformations is discussed, with Proposition 5 showing that T is an endomorphism of V M when there is no common latent ancestor between subsets E and E. This ensures unambiguous assignment of values by non-overlapping subsets of latent variables. The image set of the layer fully covers the Cartesian product of subsets A and B of latent variables, ensuring T is an endomorphism of V M. The i.i.d. assumption for Z components leads to modular subsets of endogenous variables, creating a disentangled representation. Increasing dimensions and i.i.d. sampling guarantee an injective mapping, with counterfactual hybridization resulting in an influence map covering I k. The conditions on I k and thresholding approach ensure a rank K binary factorization of matrix B. The \u03b2-VAE architecture is similar to DCGAN, with hyperparameters specified in Table 1. The model consists of three blocks of convolutional layers with skip connections for image sharpness. The pretrained model used is from Tensorflow-hub. The pretrained BigGan-deep model from Tensorflow-hub was used without retraining. It consists of ResBlocks with BatchNorm-ReLU-Conv Layers and skip connections for signal flow. Influence maps were generated using a VAE on the CelebA dataset, showing variance impact on pixels. FID analysis of BEGAN hybrids measured distances between real and generated data classes. The FID analysis of BEGAN hybrids showed that hybrids have a small distance to the generated data and each other, indicating visually plausible images. Entropy values varied based on the Gblock intervened, with Gblock 6 showing poorer quality leading to larger entropy values. Hybrids based on interventions on Gblock number 4 had smaller entropy values. Overall, the results suggest that object texture plays a role in the quality of the generated images. The entropy values for hybrids vary based on the Gblock intervened, with Gblock 6 showing larger entropy values for poorer quality modules. Hybrids from Gblock 4 have smaller entropy values, indicating that object texture is crucial for the classifier's decision. The entropy is computed for hybrids between classes \"cock\" and \"ostrich\", with Gblock interventions generating bird hybrids with mixed shape properties. The experiment investigates the robustness of classifiers using koala+teddy hybrids. The hybrids mix shape properties of cock and ostrich, with Gblock interventions generating bird hybrids. The classifiers must focus on the object present, not the context. Nasnet large is more robust to contextual changes compared to other classifiers."
}