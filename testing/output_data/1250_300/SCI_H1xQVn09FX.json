{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. Generative Adversarial Networks (GANs) can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. GANs outperform WaveNet baselines on evaluation metrics and generate audio much faster. Neural audio synthesis is a challenging problem that requires modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine-scale details but have slow sampling speeds. Generative Adversarial Networks (GANs) have been successful in generating high-resolution audio quickly. Large models operate at a fine timescale, but autoencoder variants are limited to modeling local latent structure due to memory constraints. Generative Adversarial Networks (GANs) have been successful in generating high-resolution images efficiently. Adapting GAN architectures for audio generation has potential for domain transformations, but struggles to achieve the same level of fidelity as image counterparts. Frame-based techniques for audio waveforms focus on locally-coherent waves with periodicity. Frame-based techniques for audio waveforms, such as transposed convolutions or STFTs, struggle with aligning phase due to differences in periodicity. STFT allows for unwrapping phase and deriving instantaneous radial frequency, maintaining phase coherence. GAN researchers have progressed from limited datasets to less constrained domains, like the CelebA dataset, to improve image modeling. GAN researchers have advanced image modeling by starting with focused datasets like CelebA for faces and NSynth for audio notes, both aligned and cropped to enhance fine-scale details and reduce variance. Later models expanded to less constrained domains for broader generalization. The original NSynth paper introduced autoregressive WaveNet and bottleneck spectrogram autoencoders for conditional generation. Follow-up work explored various approaches including frame-based regression models, inverse scattering networks, VAEs with perceptual priors, and adversarial regularization for domain transfer. This work introduces adversarial training and effective representations for noncausal convolutional generation in GANs, focusing on maintaining the regularity of periodic audio waveforms. Maintaining the regularity of periodic signals over short to intermediate timescales is crucial for human perception. A synthesis network faces challenges in learning the appropriate frequency and phase combinations to produce coherent waveforms. Phase precession, similar to a short-time Fourier transform, occurs when the stride of frames does not align with a waveform's periodicity. Another approach inspired by the phase vocoder involves unwrapping the phase to generate coherent waveforms. In Figure 1, a method for generating coherent waveforms inspired by the phase vocoder is depicted. Unwrapping the phase allows for linear growth, with the derivative representing the instantaneous frequency. The paper explores the synthesis of coherent audio with GANs, highlighting the generation of log-magnitude spectrograms and phases directly. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Key findings include generating log-magnitude spectrograms and phases directly with GANs for more coherent waveforms, estimating IF spectra for even more coherent audio, and the importance of preventing harmonics from overlapping by adjusting STFT frame size and switching to mel frequency scale. GANs on the NSynth dataset outperform WaveNet in speed and quality, with global conditioning allowing for smooth timbre interpolation and consistent timbral identity across pitch. The study focuses on using GANs to generate coherent audio on the NSynth dataset, which contains 300,000 musical notes from various instruments. The dataset is structured with labels for pitch, velocity, instrument, and acoustic qualities. Training was done on acoustic instruments and fundamental pitches to ensure natural sound. The dataset was split into a new 80/20 train/test split for better performance. The study focuses on using GANs to generate audio on the NSynth dataset, which contains musical notes from different instruments. A new 80/20 train/test split was created from shuffled data. The model samples a random vector and uses transposed convolutions to generate output data. Progressive training methods were adapted for audio generation, showing slightly better convergence time and sample diversity. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. An auxiliary classification BID24 loss is added to the discriminator to predict the pitch label. Spectral representations are computed using STFT magnitudes and phase angles. The STFT has a 256 stride and 1024 frame size, resulting in 75% frame overlap and 513 frequency bins. The resulting \"image\" size is (256, 512, 2) with channels for magnitude and phase. The STFT has a 256 stride and 1024 frame size with 75% frame overlap and 513 frequency bins. The resulting \"image\" size is (256, 512, 2) with channels for magnitude and phase. The magnitude is logged and scaled between -1 and 1, while the phase angle is also scaled to match the tanh output nonlinearity. Instantaneous frequency models are created by unwrapping the phase angle and taking the finite difference. High frequency resolution variants are obtained by doubling the STFT frame size and stride. IF-Mel variants transform both log magnitudes and instantaneous frequencies to a mel frequency scale without dimensional compression. To enhance frequency separation, the STFT is transformed to a mel frequency scale without compression. WaveGAN and WaveNet are state-of-the-art models for audio generation, with WaveGAN adapted to accept pitch conditioning. WaveNet has been used on the NSynth dataset for sound interpolation, but we introduce a new WaveNet model for generative audio modeling. WaveNet is the current state of the art in generative audio modeling. It has been adapted to accept pitch conditioning signals and outperforms other models. Evaluation metrics include human evaluation for audio quality. The evaluation metrics for audio quality in generative audio modeling include human evaluation using Amazon Mechanical Turk and the Number of Statistically-Different Bins (NDB) metric proposed by BID27 to measure diversity. The evaluation metrics for audio quality in generative audio modeling include the Number of Statistically-Different Bins (NDB) metric proposed by BID27 to measure diversity, the Inception Score (IS) proposed by BID28 for evaluating GANs, and Pitch Accuracy. The Inception Score (IS) metric is used to evaluate GANs, but in this study, features from a pitch classifier trained on spectrograms are used instead of Inception features. Pitch Accuracy (PA) and Pitch Entropy (PE) are also measured separately to account for models producing distinct pitches. Fr\u00e9chet Inception Distance (FID) is another metric proposed for evaluating GANs based on the distance between multivariate Gaussians. The study shows a clear trend in audio quality decreasing as output quality decreases. The study uses pitch-classifier features instead of Inception features to evaluate audio quality. Results show a clear trend of decreasing quality as output representations move from IF-Mel to Waveform. The highest quality model, IF-Mel, is slightly inferior to real data. WaveNet baseline occasionally breaks down, leading to comparable scores with IF GANs. Sample diversity correlates with audio quality, with high frequency resolution improving scores. Autoregressive sampling in TAB0 lacks diversity, showing peaky distributions in sample histograms. The model assigns high likelihood to training data, but autoregressive sampling in TAB0 lacks diversity, leading to peaky distributions. FID scores are lower for models with high frequency resolution. Mel scaling has less effect on FID compared to listener study. Phase models have high FID, indicating poor sample quality. Classifier metrics like IS, Pitch Accuracy, and Pitch Entropy are good due to explicit conditioning. High-resolution models generate examples classified similarly to real data, but lack discriminative information due to mode collapse. The metrics provide a rough measure of models generating classifiable pitches, with low frequency models and baselines showing less reliability. Visualizing qualitative audio concepts is recommended, with examples showing differences in waveform coherence among models. For more detailed comparisons, listening to the accompanying audio examples is advised. The WaveGAN and PhaseGAN models show phase irregularities, creating a blurry web of lines, while the IFGAN is more coherent with small variations. Visualizations demonstrate the phase coherence of different GAN variants, with real data and IF models producing consistent waveforms, PhaseGAN showing some discontinuities, and WaveGAN being irregular. Rainbowgrams depict the log magnitude of frequencies, showing clear phase coherence in real data and IFGAN, speckled noise in PhaseGAN, and irregularity in WaveGAN. The visualization shows clear phase coherence of harmonics in real data and IFGAN, while PhaseGAN displays discontinuities and WaveGAN appears largely incoherent. GANs allow conditioning on the same latent vector for the entire sequence, unlike autoregressive models like WaveNet. WaveNet autoencoders learn local latent codes controlling generation on a millisecond scale but have limited scope. Interpolating between examples in raw waveform, latent code, and global code of IF-Mel GAN shows improvements in mixing sounds and timbre. Interpolating between waveforms in IF-Mel GAN results in high-fidelity audio examples by mixing sounds and timbre, unlike WaveNet which produces less realistic in-between sounds due to its failure modes. The GAN model's global conditioning allows for perceptual attribute interpolation while staying along the prior at all intermediate points. The IF-Mel GAN model allows for smooth interpolation of perceptual attributes in audio examples, producing high-fidelity sounds with consistent timbre and pitch variations. The timbre morphs smoothly between instruments while maintaining the unique character of each, and varying pitch conditioning over five octaves demonstrates the model's ability to preserve timbral identity. The IF-Mel GAN model maintains consistent timbre and pitch variations across different instruments, creating a unique identity for each. It can generate audio samples in parallel, making it significantly faster than WaveNet autoencoders, with a latency drop from 1077.53 seconds to 20 milliseconds on a TitanX GPU. The IF-Mel GAN model drastically reduces synthesis latency compared to WaveNet, making it around 53,880 times faster. This opens up the possibility for real-time neural network audio synthesis on devices, allowing for a broader range of expressive sounds. The focus on deep generative models for audio has mainly been on speech synthesis, while music generation remains relatively under-explored. In comparison to speech, music audio generation is relatively under-explored. Previous work has focused on autoregressive models and GANs for synthesizing musical instrument sounds, with improvements in training stability and generation quality. The NSynth dataset was used for interpolating between timbres of musical instruments, but with slow sampling speeds. The NSynth dataset was used for timbre transformations between audio sources, achieving significant speedups in sampling. By employing GANs, high-quality audio generation was demonstrated, surpassing WaveNet fidelity and generating samples much faster. The study focused on high-quality audio generation using GANs on the NSynth dataset, surpassing WaveNet fidelity and generating samples much faster. Further work is needed to validate and expand to a broader class of signals, including speech. Possible applications include domain transfer and addressing issues like mode collapse and diversity. The study implemented a pitch classifier in the discriminator, trained models with the ADAM optimizer, and experimented with different learning rates and classifier loss weights. They found that a learning rate of 8e-4 and classifier loss of 10 performed the best across all model variants. The networks used box upscaling/downscaling, pixel normalization, and Tanh output nonlinearity for the generator. Real data was normalized before passing to the discriminator, and the log-magnitudes and phases were scaled to [-0.8, 0.8]. Each GAN variant was trained accordingly. The real data is normalized before passing to the discriminator. The log-magnitudes and phases are scaled to [-0.8, 0.8]. Each GAN variant is trained for 4.5 days on a single V100 GPU with a batch size of 8. Progressive models train on 1.6M examples per stage (7 stages) and continue training until 4.5 days complete. The WaveNet baseline uses a Tensorflow implementation with a decoder composed of 30 layers of dilated convolution. The conditioning stack in the WaveNet model consists of 5 layers of dilated convolution followed by 3 layers of regular convolution, all with 512 channels. A one-hot pitch conditioning signal is used, distributed in time, and passed through a 1x1 convolution for each layer of the decoder. The model uses mulaw encoding for the 8-bit version and a quantized mixture of 10 logistics for the 16-bit version. WaveNets converged in 150k iterations over 2 days using 32 V100 GPUs with synchronous SGD training."
}