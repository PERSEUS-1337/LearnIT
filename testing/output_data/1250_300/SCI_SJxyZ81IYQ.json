{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively in a bottom-up manner. This approach preserves semantic content better and follows a natural human language structure. The current text chunk discusses the limitations of sequential models in image captioning, which fail to capture hierarchical structures in natural languages. These models rely too heavily on n-gram statistics and lack the ability to represent hierarchical dependencies among words in a caption. Sequential models in image captioning have drawbacks as they rely on n-gram statistics and favor frequent n-grams in training, leading to syntactically correct but semantically irrelevant captions. To address this, a new paradigm decomposes semantics extraction and caption construction into two stages, first deriving explicit semantic representations like noun-phrases before constructing the caption recursively. The proposed paradigm decomposes semantics extraction and caption construction into two stages, deriving explicit semantic representations like noun-phrases before recursively constructing the caption. It consists of two parametric modular nets for phrase composition and completeness evaluation, offering advantages over conventional captioning models. The proposed paradigm consists of two parametric modular nets for phrase composition and completeness evaluation, offering advantages over conventional captioning models. It preserves semantic content, allows for hierarchical dependencies, increases caption diversity, and generalizes well to new data. The literature in image captioning has evolved from bottom-up and detection-based approaches to neural network era methods. Recent works in image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. Different approaches have been proposed, such as using a single feature vector or a set of feature vectors to represent the input image, along with attention mechanisms to extract relevant information. These advancements have improved caption diversity and generalization to new data. Recent advancements in image captioning involve using convolutional neural networks for image representation and recurrent neural networks for caption generation. Various approaches have been proposed, including attention mechanisms to extract relevant information from feature vectors. Recent methods focus on extracting phrases or semantic words directly from the input image, improving caption diversity and generalization to new data. Despite improvements in model architectures, sequential caption generation tends to favor frequent n-grams. The proposed paradigm in image captioning involves a bottom-up approach, representing the input image with noun-phrases and constructing captions recursively. This method aims to preserve semantics effectively, require less data to learn, and generate more diverse captions compared to sequential approaches. The proposed compositional paradigm in image captioning involves selecting noun-phrases from an input image and recursively composing them into captions using neural networks. The approach aims to generate diverse captions with a hierarchical structure, different from traditional sequential methods. The proposed two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner using a recursive compositional procedure called CompCap. This approach considers nonsequential dependencies among words and phrases, explicitly representing image semantics with noun-phrases like \"a black cat\" and \"two boys\". In the context of image captioning, noun-phrases like \"a black cat\" and \"two boys\" are extracted from input images to represent object categories and attributes. This process involves parsing captions to identify distinct noun-phrases, treating each as a class for multi-label classification. The number of distinct noun-phrases in datasets is significantly smaller than the number of images, leading to the formalization of noun-phrase extraction as a classification problem. In image captioning, noun-phrases are extracted from training captions to represent object categories and attributes. Each noun-phrase is treated as a class for classification. Visual features are extracted from images using a Convolutional Neural Network, and binary classification is performed for each noun-phrase. The input image is represented using top scoring noun-phrases, pruned through Semantic Non-Maximum Suppression. A recursive compositional procedure called CompCap is used to construct the caption. CompCap is a recursive compositional procedure used to construct captions by connecting noun-phrases through a Connecting Module and evaluating the completeness of the resulting caption using an Evaluation Module. The process involves scanning pairs of phrases, generating longer phrases, and selecting the one with the maximum connecting score as the new phrase. If the new phrase is incomplete, the pool of phrases is updated and the process is repeated. The Connecting Module in CompCap selects connecting phrases to construct captions by evaluating the completeness of the resulting caption. It treats the generation of connecting phrases as a classification problem due to dealing with incomplete captions. The C-Module focuses on incomplete captions, treating connecting phrase generation as a classification problem. A set of distinct connecting sequences is mined from training captions and used as classes for the connecting module, which acts as a classifier. A two-level LSTM model encodes left and right phrases to output a normalized score. The C-Module treats connecting phrase generation as a classification problem, using a two-level LSTM model to encode phrases and output a normalized score. The model controls attention with visual features and evolves the encoded state. Phrases are connected based on the highest connecting score, with a virtual neg class added for unconnectable pairs. The Evaluation Module (E-Module) determines if a phrase is a complete caption by encoding it into a vector and evaluating its probability. It can also check for other properties like caption quality using a caption evaluator. The framework can be extended to generate diverse captions for an image using beam search. The framework can be extended for generating diverse captions by using beam search or probabilistic sampling. This allows for multiple beams in beam search to avoid local minima and diverse captions through sampling ordered pairs or connecting sequences based on normalized scores. User preferences can also be incorporated by filtering noun phrases or modulating their scores. The text chunk discusses the controllability and interpretability of operations in generating captions, showcasing examples in the Experimental section. Experiments are conducted on MS-COCO and Flickr30k datasets with specific details on image counts, ground-truth captions, vocabulary sizes, and data preprocessing. Training data for modules is collected by parsing ground-truth captions into trees using NLP toolkit. The C-Module and E-Module are separately trained as standard classification tasks in all experiments. In experiments, training data for modules is collected by parsing ground-truth captions into trees using NLP toolkit. C-Module and E-Module are separately trained as standard classification tasks. The recursive compositional procedure is modularized for better generalization. Testing involves two forward passes for each module. CompCap is compared with NIC, AdapAtt, TopDown, and LSTM-A5 for image captioning. In comparison to other methods like NIC, AdapAtt, TopDown, and LSTM-A5 for image captioning, CompCap encodes images as semantical feature vectors and predicts noun-phrases as additional visual features. All methods are re-implemented and trained with the same hyperparameters using ResNet-152 pretrained on ImageNet to extract image features. Beam-search of size 3 is used for baselines, while CompCap selects 7 noun-phrases with top scores to represent the input image. Beam-search of size 3 is used for pair selection in CompCap. CompCap uses beam-search of size 3 for baselines and selects 7 noun-phrases with top scores to represent input images. The quality of generated captions is compared using various metrics on MS-COCO and Flickr30k datasets. CompCap performs well under the SPICE metric but lags behind baselines in terms of other metrics like CIDEr, BLEU-4, ROUGE, and METEOR. This reflects the differences in sequential and compositional caption generation methods. The study compares CompCap's performance in caption generation using different metrics like CIDEr, BLEU-4, ROUGE, and METEOR. It shows that CompCap excels in preserving semantic content, especially when input images are represented by groundtruth noun-phrases. By following a compositional generation procedure, CompCap generates captions that are more semantically accurate, leading to improved metric scores. Additionally, integrating noun-phrases from ground-truth captions further enhances the quality of generated captions, except for SPICE metric. CompCap excels in preserving semantic content by integrating noun-phrases from ground-truth captions, leading to improved metric scores except for SPICE. The proposed compositional paradigm disentangles semantics and syntax, making CompCap good at handling out-of-domain semantic content and requiring less data to learn. Studies show that training CompCap on MS-COCO/Flickr30k and testing on Flickr30k/MS-COCO yields positive results in terms of SPICE and CIDEr. In the second study, baselines and CompCap were trained on MS-COCO/Flickr30k and tested on Flickr30k/MS-COCO. Results show significant drops for baselines but competitive results for CompCap, indicating the benefit of disentangling semantics and syntax. CompCap's ability to generate diverse captions was analyzed using various metrics, showing the ratio of novel and unique captions. The diversity of generated captions is quantified using various metrics, including the ratio of novel and unique captions, vocabulary usage, and pair-wise editing distances. Diversity is measured at both the dataset and image levels, with different criteria for single and multiple captions per image. CompCap obtained the best results in all metrics, indicating diverse and novel captions. Error analysis revealed that misunderstandings of visual content led to errors in captions, which could be improved with more sophisticated techniques in noun-phrase extraction. Sequential models tend to favor frequent n-grams, even with a perfect understanding of visual content. The proposed method for image captioning involves a compositional approach, dividing the captioning process into two stages. The first stage extracts an explicit representation of the input image with noun-phrases, while the second stage assembles these noun-phrases into a caption using a recursive compositional procedure. This hierarchical structure preserves semantics effectively, requires less training data, generalizes well across datasets, and produces diverse captions by finding semantically similar noun-phrases. The proposed method for image captioning involves a compositional approach, dividing the captioning process into two stages. The first stage extracts an explicit representation of the input image with noun-phrases, while the second stage assembles these noun-phrases into a caption using a recursive compositional procedure. This hierarchical structure preserves semantics effectively, requires less training data, generalizes well across datasets, and produces diverse captions by finding semantically similar noun-phrases. To suppress noun-phrases, the method compares central nouns in noun-phrases to determine semantic similarity, using encoders in the C-Module to compute normalized euclidean distances between their encodings. The method for image captioning involves a compositional approach, dividing the process into two stages. The C-Module uses encoders to compute normalized euclidean distances between noun-phrases' encodings, determining semantic similarity. The C-Module contains two encoders for ordered pairs, with independent parameters for better performance. Additional hyperparameters for CompCap can be tuned for pair selection and connecting phrase selection. The C-Module for image captioning uses encoders with independent parameters for better performance. Additional hyperparameters for CompCap, such as beam search sizes for pair selection and connecting phrase selection, can be adjusted. Experimental results show that these hyperparameters have minor influence on CompCap's performance."
}