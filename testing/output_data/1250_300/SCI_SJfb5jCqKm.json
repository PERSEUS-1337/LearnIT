{
    "title": "SJfb5jCqKm",
    "content": "We propose a new uncertainty estimation algorithm for deep neural classification that addresses biased estimates for highly confident predictions. By using earlier snapshots of the trained model, our algorithm selectively estimates uncertainty for these points before they are jittered. Extensive experiments show that our method outperforms all known techniques in providing more accurate uncertainty estimates. This is crucial for applications like autonomous driving and medical diagnosis that rely on controlling statistical uncertainties in deep learning models. The Bayesian framework offers a principled approach to infer uncertainties from deep learning models, but implementing it for deep neural networks faces computational hurdles. Current uncertainty estimation methods for deep learning, such as raw softmax response, entropy, embedding layers, and MC-dropout, are based on signals from standard networks. An ensemble of softmax response values from multiple networks has been shown to outperform other approaches in uncertainty estimation. The curr_chunk discusses a method of confidence estimation that improves upon existing approaches by addressing erroneous confidence estimates in deep classifiers. The algorithm learns an improved confidence score function based on observations that confidence scores from ordinary deep classifiers can be inaccurate, especially for highly confident instances. This inaccuracy is attributed to the training process with stochastic gradient descent optimizers. The curr_chunk discusses improving confidence estimation in deep classifiers using stochastic gradient descent optimizers. It focuses on decoupling uncertainty estimation into ordinal ranking and probability calibration tasks, with a main emphasis on ranking uncertainties for classification. The goal is to learn a pair (f, \u03ba) where f is a classifier and \u03ba is a confidence score function that assigns lower confidence values to misclassified points. The curr_chunk discusses two methods to enhance confidence scoring functions for deep neural networks. The first method involves selecting an early stopped model for each instance to improve uncertainty estimation, while the second method approximates the first without additional examples. These methods aim to boost known confidence scoring functions for DNNs. The curr_chunk introduces a new performance measure for scoring functions based on selective prediction concepts. Extensive experiments with baseline methods and image datasets show consistent improvements. Results are validated using calibrated uncertainty estimates measured with negative log-likelihood and Brier score. The work focuses on uncertainty estimation for supervised multi-class classification problems, where uncertainty is considered as negative confidence. The curr_chunk discusses the training process of deep neural classification models using softmax activation for multi-class classification. It introduces a confidence score function, \u03ba(x, i, |f), to quantify confidence in predicting x belonging to class i based on signals from the model. The function induces a partial order over points in X and is not required to distinguish between points with the same score. The curr_chunk discusses the uncertainty estimation in deep learning models, highlighting the lack of consensus on performance measurement. Different studies have used various metrics such as Brier score, negative-log-likelihood, and area under the ROC curve to evaluate ordinal estimators. The curr_chunk proposes a unitless performance measure for \u03ba functions, borrowing elements from other approaches. It introduces the concept of selective classification and suggests measuring performance using the area under the risk-coverage curve of a selective classifier induced by \u03ba. The curr_chunk introduces the terms selective classifier, selective risk, and coverage, proposing to measure performance using the area under the risk-coverage curve. A normalization of this measure, called \"excess AURC\" (E-ARUC), allows for meaningful comparisons across problems. A selective classifier is defined as a pair (f, g), where f is a classifier and g is a selection function. Performance is quantified using coverage and risk, which can be empirically evaluated over any finite labeled set. The curr_chunk discusses the empirical evaluation of selective risk and coverage measures using labeled sets. It introduces the risk-coverage curve (RC-curve) to measure the overall performance of selective classifiers. The performance of a confidence score function \u03ba is evaluated using an independent set of labeled points. The area under the (empirical) RC-curve (AURC) is defined as a performance measure for \u03ba. The curr_chunk discusses the performance evaluation of a confidence score function \u03ba using the area under the (empirical) RC-curve (AURC) as a measure. It shows how a better \u03ba leads to a faster decrease in the RC-curve, indicating improved selective classifier performance. An example with a DNN trained on CIFAR-100 dataset is provided, demonstrating the relationship between selective risk and coverage. The selective risk increases with coverage, with a significant decrease in risk when rejecting points with low confidence. The RC-curve for the CIFAR100 dataset shows the optimal curve achievable with a confidence score function \u03ba*. The optimal function \u03ba* reduces selective risk to zero below a certain coverage rate. A unitless performance measure is obtained by normalizing the AURC of \u03ba*. The Excess-AURC is defined as the difference between AURC(\u03ba, f |V n) and AURC(\u03ba*, f |V n), with the optimal \u03ba having E-AURC = 0. The Excess-AURC (E-AURC) is a unitless measure used as the main performance metric. It focuses on non-Bayesian methods in deep neural classification, particularly the Monte-Carlo dropout technique for uncertainty estimation. Confidence scores for DNNs are commonly obtained by measuring the classification margin, where large values indicate high confidence levels. This approach has been shown to outperform other methods in neural networks. In the context of neural networks, various methods have been proposed for uncertainty estimation, including reject options in linear models and SVMs, K-nearest-neighbors algorithm in DNN embedding space, ensemble-based uncertainty scoring, and leveraging information from the network's training process. These methods aim to improve predictive performance and provide confidence estimates using different approaches. The literature on uncertainty estimation in neural networks is sparse. BID14 and BID15 proposed ensemble methods using fully converged models, while our method utilizes \"premature\" ensemble members for classification performance. This section presents an example motivating our algorithms for deep classification models. In this section, an example is presented to motivate algorithms for deep classification models. A deep classification model f trained over set S m through T epochs is considered, along with an independent validation set V n. The quality of softmax response generated from f is monitored on points in V n, allowing meaningful statements about confidence estimation for unseen test points. The example involves two groups in V n based on confidence assessment using softmax response values: a green group with the highest confidence points and a red group with the lowest confidence points. The softmax response values from a deep classification model f are used to assess confidence in two groups of points in validation set V n: green (highest confidence) and red (lowest confidence). Green points are predicted accurately earlier in training, stabilizing around Epoch 80, while red points continue to improve. This suggests that an intermediate model like f 130 can estimate the confidence of green points accurately. The E-AURC measure is used to assess the quality of confidence estimation in a deep classification model. Green points show improving confidence scores with intermediate models like f 130, while red points improve monotonically. Surprisingly, the final model f [T] is a poor estimator for green points but the best for red points. This behavior is consistent across all datasets. The text discusses the improvement of confidence scores for red points as training progresses. The final model is the best estimator for red points, while an algorithm using early stopping is proposed to address degradation in confidence estimation. Additionally, a supervised algorithm is presented for learning an improved scoring function for a deep neural classifier. The Pointwise Early Stopping (PES) algorithm for confidence scores operates by extracting the most uncertain points from a training set at each iteration. The algorithm does not rely on additional training examples and aims to improve confidence estimation for a neural classifier. The Pointwise Early Stopping (PES) algorithm extracts the most uncertain points from the training set at each iteration. The algorithm finds the best model using a set of uncertain points and determines the confidence rate for each point at test time. It produces a partition of layers based on confidence levels and iterates until the training set is empty. The Pointwise Early Stopping (PES) algorithm determines confidence levels for points at test time by finding the best performing \u03ba function based on models from F. The Averaged Early Stopping (AES) is a simplified version of PES that leverages the observation that \"easy\" points are learned earlier during training. It approximates the area under the learning curve by averaging evenly spaced points on the curve. The Averaged Early Stopping (AES) algorithm approximates the area under the learning curve by averaging evenly spaced points on the curve. It works well due to the computational burden of running the PES algorithm. The AES algorithm is applied over four known confidence scores and evaluated on standard image datasets. The results are reported in Table 4, showing performance across different datasets and baselines. The Averaged Early Stopping (AES) algorithm approximates the area under the learning curve by averaging evenly spaced points on the curve. Results are reported in Table 4 for four standard image datasets: CIFAR-10, CIFAR-100, SVHN, and Imagenet. The table shows performance of different baseline methods, with E-AURC measures quantifying the difficulty level of the learning problems. For Imagenet, only results for the softmax response and ensemble are presented due to computational constraints. The E-AURC measure quantifies the difficulty level of learning problems for different datasets. Our method consistently outperformed baseline methods in 39 out of 42 experiments. Applying AES with k = 30 always reduced the E-AURC of the baseline method. The ensemble estimation approach of BID17 is currently state-of-the-art for all datasets, and the application of AES improves upon this. The ensemble estimation approach of BID17 is the best among baselines for each dataset, currently state-of-the-art. AES improves the state-of-the-art for all datasets, although computationally intensive. In CIFAR-10, softmax response is the best single-classifier method, improved by 6% with AES. NN-distance in CIFAR-10 shows marked improvement with AES. In CIFAR-100, NN-distance is the top single-classifier method, improved by 22% with AES. AES method with Platt scaling on various datasets compared to baseline method. The AES algorithm improves performance by 22% on CIFAR-100 when applied with Platt scaling. Probability calibration is also examined, showing consistent results with raw uncertainty estimates. E-AURC serves as a reliable proxy for calibrated probabilities. The PES algorithm is implemented on the softmax response method for various datasets. The PES algorithm is implemented on the softmax response method for different datasets, requiring an independent training set split from the original validation set. E-AURC values are compared for Pointwise Early Stopping (PES) and softmax response (SR) on CIFAR-10, CIFAR-100, and SVHN. Time complexity for PES over NN-distance and MC-dropout is discussed, with specific parameters leading to a complexity of 175,000,000. The PES algorithm significantly reduced E-AURC on all datasets, with the best improvement seen on CIFAR-100. Challenges were faced when applying PES to various confidence methods, prompting further research for algorithm enhancement. Novel uncertainty estimation algorithms were introduced, inspired by observations during DNN training with SGD. The PES algorithm requires an additional labeled set and extensive computational resources. The PES algorithm presented in the previous paragraphs requires additional labeled data and computational resources for training. An approximated version, AES, is simpler and scalable, improving confidence scores on evaluated datasets. Both PES and AES utilize snapshot models generated during training to overcome confidence score deformations. Future research could focus on developing a loss function to prevent confidence deformations while maintaining high classification performance and reducing inference time through distillation methods. To reduce inference time, distillation BID13 could be incorporated. Another approach is to approximate PES using a single model per instance based on an early stopping criterion similar to BID19. Various methods such as Softmax Response, NN-distance, MC-dropout, Ensemble, and Platt scaling BID23 were implemented with specific parameters for uncertainty estimation, but some proposed extensions were not included to avoid performance degradation. The Ensemble method averages softmax values from 5 DNNs. Platt scaling is applied for calibration using logistic regression on a validation set. Performance is evaluated using negative log likelihood and Brier score. Results for AES method on CIFAR-10, CIFAR-100, SVHN, and ImageNET are provided in a table. Standard errors for other methods were not computed due to computational complexity. In Section 5, the AES method is compared to baseline on CIFAR-10, CIFAR-100, SVHN, and ImageNET for various k values. The method divides the domain into \"easy points\" and \"hard points\" and shows that \"easy points\" exhibit overfitting during training. This motivates the strategy of extracting information from early training stages to improve uncertainty estimates. Overfitting is observed in all cases, but to a lesser extent with MC-dropout, consistent with AES algorithm results. In Figures 3 and 4, plots similar to FIG0 (b,c) show overfitting in all cases, but to a lesser extent with MC-dropout. Results are consistent with AES algorithm, where E-AURC improvement over MC-dropout was smaller. NN-distance also shows overfitting, affecting hard instances more severely. Correction strategy proposed is potentially useful."
}