{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The teachers and student can have different structures and be trained on different tasks. The student becomes independent of the teachers after training, outperforming fine-tuning and other knowledge exchange methods in various learning tasks. Research communities have developed various deep net architectures for different tasks, with some architectures trained from scratch and others fine-tuned using structurally similar deep nets. In reinforcement learning, different approaches like progressive neural nets, PathNet, 'Growing a Brain', Actor-mimic, and Knowledge distillation have been explored to transfer knowledge from multiple teachers to a new student model. Knowledge flow addresses limitations of existing techniques by transferring knowledge from multiple teachers to a student model, ensuring independence of the student at the final training stage and maintaining a constant size. This approach overcomes issues such as parameter limitations, computational intensity, and reliance on a single pretrained model. Our framework allows for knowledge transfer from multiple teachers to a student model, ensuring independence of the student at the final training stage. It is applicable to various tasks from reinforcement learning to fully-supervised training, with flexibility in choosing teacher models. The approach maintains a constant size for the resulting student net and imposes no restrictions on the deep net size of the teachers and student. The goal of reinforcement learning is to find a policy that maximizes future rewards from each state. The asynchronous advantage actor-critic (A3C) formulation is followed, where the policy and value function are approximated by deep nets with parameters. The policy optimization involves a loss function based on negative log-likelihood and entropy regularization. The value function optimization commonly uses squared loss. The text discusses the use of entropy function and squared loss in optimizing the policy and value function in reinforcement learning. It introduces a framework called knowledge flow, where knowledge is transferred from 'teachers' to a 'student' deep net during training. This process is illustrated with example deep nets in a figure. During training, the student relies on teacher one whose weight decreases as the student becomes independent. Knowledge transfer from multiple teachers to the student is achieved by adding transformed representations from teacher deep nets to the student net. This is done by modifying the student net and scaling teacher representations with trainable parameters. The student model is trained to rely on teachers initially for better performance, but as training progresses, it becomes more independent. The goal is for the student to eventually master the task on its own without relying on teachers. This process is encouraged by increasing the weight on the student representation during training. During the final stages of training, the student is encouraged to become independent and master the task on its own. Two additional loss functions are introduced to measure the student's reliance on teachers and ensure stable behavior as teacher influence decreases. The modified student net includes cross-connections, denoted by Loss\u02dc, for supervised and reinforcement learning tasks. The modified deep net includes cross-connections to incorporate teacher influence, controlled by parameters \u03bb 1 and \u03bb 2. The student gradually becomes independent during training, with a focus on reducing negative transfer effects. Low level knowledge transfer from teachers is observed in experiments. The modified deep net incorporates teacher influence through cross-connections, with parameters \u03bb 1 and \u03bb 2 controlling the process. Low level knowledge transfer from teachers is observed in experiments, where a normalized weight is introduced to decide which representation to trust at each layer of the student net. The deep net incorporates teacher influence through cross-connections, with parameters controlling the process. In practice, it is recommended to link one teacher layer to one or two student layers to introduce a limited number of matrices. Additional trainable parameters are introduced but are not part of the resulting student network. In the final stage of training, the student becomes independent and no longer relies on additional parameters Q and w introduced in the framework. The influence of teachers is gradually decreased to encourage the student to rely less on them, leading to increased independence. This is achieved by minimizing the dependence cost, which encourages the student's weights to increase. Ultimately, the student becomes independent of the transformed representations obtained from teachers. During the final stage of training, the student becomes independent of the transformed representations obtained from teachers. To prevent performance degradation, a Kullback-Leibler regularizer is used to slow down the decrease of teacher influence. Knowledge flow is evaluated on reinforcement and supervised learning tasks, with results reported using only the student model to avoid any influence from teacher nets. The student model is evaluated on reinforcement learning tasks using Atari games without any influence from teacher nets. The model uses raw images from the environment to predict actions based on rewards. It has a fully forward architecture with three hidden layers and outputs a probability distribution over actions and an estimated value function. The hyper-parameter settings are the same as previous work, except for the learning rate. The third hidden layer consists of two sets of output: a softmax output for action probabilities and a scalar output for the estimated value function. Different hyper-parameter settings are used compared to previous work, with Adam optimizer and a learning rate of 10^-4. Progressive neural net framework is used to select \u03bb 1 and \u03bb 2, with experiments repeated 25 times. Evaluation is done by running 16 agents on 16 CPU cores in parallel, following the procedure of BID16. The study evaluates a transfer reinforcement learning framework by comparing it with PathNet and progressive neural net (PNN) using experimental settings. Results show that the framework outperforms PathNet in 11 out of 14 experiments and PNN in five out of seven experiments with fewer parameters. The knowledge transfer from teachers to the student is effective, as shown by the higher scores achieved. The study demonstrates effective knowledge transfer from teachers to students in a transfer reinforcement learning framework. Results show higher scores in five out of seven experiments with 16M parameters. Increasing the number of teachers from one to two significantly improves student performance. Different environment/teacher settings are experimented with, showing better scores than the A3C baseline reported by BID17. Knowledge flow with expert teachers outperforms the A3C baseline reported by BID17. The student can benefit from multiple teachers in knowledge flow, avoiding negative impacts from insufficiently pretrained teachers. Training curves are shown in FIG5, demonstrating the effectiveness of knowledge transfer from expert teachers to students. The training curves in FIG5 show that knowledge transfer from expert teachers to students benefits the student model, achieving scores ten times larger than learning without a teacher. Various image classification benchmarks are used for supervised learning, with evaluation metrics reported on the test set of each dataset. The evaluation metrics for the trained student model include reporting the top-1 error rate on the test set of each dataset. CIFAR-10 and CIFAR-100 datasets consist of colored images of size 32 \u00d7 32, with 10 and 100 classes respectively. Training and test sets contain 50,000 and 10,000 images. Experiments are conducted using standard data augmentation and Densenet as a baseline. Teachers are trained on CIFAR-10, CIFAR-100, and SVHN, with the student model trained using different combinations of teachers. Fine-tuning from the CIFAR-100 expert improves performance over the baseline for the CIFAR-10 target task. Fine-tuning from CIFAR-100 expert improves performance by 4% on CIFAR-10 target task. Knowledge flow enhances by 13% over baseline with good and inadequate teachers. Results are similar on CIFAR-100 dataset. Additional details on knowledge flow are in the appendix. Comparison with related work and details in Sec. 8 are discussed. PathNet BID6 allows multiple agents to train the same deep net, while Progressive Net BID23 leverages transfer and avoids catastrophic forgetting. Progressive Net BID23 introduces lateral connections to previously learned features to avoid catastrophic forgetting. In contrast, our method ensures the independence of the student during training, addressing a limitation in BID23. Distral combines distill and transfer learning for joint training of multiple tasks, while knowledge flow focuses on a single task. Both approaches involve the transfer of information, but in different ways. The curr_chunk discusses a general knowledge flow approach that leverages information from multiple teachers to help a student learn a new task. Results show improvements in reinforcement learning and supervised learning compared to training from scratch or fine-tuning. Future plans include learning when to use different teachers and actively swapping them during student training. Related work includes actor-mimic, learning without forgetting, growing a brain, policy distillation, domain adaptation, knowledge distillation, and lifelong learning. The curr_chunk discusses distilling knowledge from larger teacher models to smaller student models, with experiments conducted on various datasets like MNIST, CIFAR-100, and ImageNet. The student models have significantly fewer parameters than the teacher models. Results show that the framework consistently outperforms traditional knowledge distillation methods. The study compares a 50-layer ResNet BID8 model with an 18-layer ResNet student model on the 'EMNIST Letters' dataset. The student model benefits from both the output layer and intermediate layer representations of the teacher model. The dataset includes images of handwritten letters and digits, with balanced classes and specific training and test set sizes. Teachers were trained on different EMNIST datasets, with the target task being EMNIST Letters. Results are compared to fine-tuning and state-of-the-art methods on EMNIST. The study compares the performance of student models trained with different teachers on EMNIST Letters dataset. Results show that student learning with expert, semi-expert, and non-expert teachers outperforms baseline and fine-tuning methods. The STL-10 dataset, similar to CIFAR-10 and CIFAR-100, is used for training with 5,000 labeled images. Teachers were trained on CIFAR-10 and CIFAR-100, showing improved performance compared to fine-tuning and baseline models. In the study, teachers trained on CIFAR-10 and CIFAR-100 are compared to fine-tuning and baseline models. Results show that using pretrained weights from these datasets reduces test errors by over 10%. Student model training in this framework further decreases test error by 3%. The approach only uses labeled data, unlike semi-supervised methods, and achieves comparable results with fewer data. The accuracy over training epochs is illustrated in Fig. 5, and comparison is made with the state-of-the-art multi-task reinforcement learning framework Distral BID26. In Fig. 5, accuracy over training epochs is shown, comparing to Distral BID26, a multi-task reinforcement learning framework. The experiments are conducted on Atari games with three tasks. Our model is trained for 40M steps, while Distral is trained for 120M steps. Results show that our framework can reduce negative transfer by decreasing a teacher's influence. Our framework can decrease a teacher's influence to reduce negative transfer. Averaged normalized weight (p w) for teachers and student in the C10 experiment is plotted, showing higher p w value for the C100 teacher compared to the SVHN teacher. Ablation study confirms that learning with knowledgeable teachers outperforms learning with untrained teachers. Learning with knowledgeable teachers outperforms learning with untrained teachers in various environments and teacher-student settings. The KL term prevents drastic changes in the student's output distribution when the teachers' influence decreases. Ablation study shows that without the KL term, performance drops significantly, while with the KL term, performance remains stable. Learning with the KL term achieves higher rewards compared to learning without it. At the end of training, learning with the KL term achieves an average reward of 2907, while learning without it achieves 1215. Using different architectures for the teacher and student models, the teacher model has 3 convolutional layers with 32, 64, and 64 filters, followed by a fully connected layer with 512 ReLUs. The student model has 2 convolutional layers with 16 and 32 filters, followed by a fully connected layer with 256 ReLUs. Training with the KL term consistently achieves higher rewards than training without it. In the experiments, teachers with different architectures achieve similar performance as teachers with the same architecture. The target task is KungFu Master, with teachers being experts for Seaquest and Riverraid. Learning with teachers of different architectures results in an average reward of 37520, while learning with teachers of the same architecture yields 35012. Knowledge flow enables higher rewards, as shown in the results. Using an average network to obtain parameters \u03b8 old achieves similar performance as using a single model. For the target task Boxing with a Riverraid expert teacher, the average reward is 96.2 with an average network and 96.0 with a single network. More results on using an average network are shown in FIG0 (b, c). Using an average network to obtain parameters \u03b8 old achieves similar performance as using a single model, with an average reward of 96.2 and 96.0 respectively. Various techniques for knowledge transfer have been explored, such as fine-tuning, PathNet, 'Growing a Brain', actor-mimic, and learning without forgetting. These methods are contrasted with the approach of using multiple teacher nets in the discussed method. The discussed method introduces scaling with normalized weights to ensure independence of the student during training, addressing a limitation in BID23. Distral combines distillation and transfer learning by jointly training multiple tasks with a shared policy, encouraging consistency between policies. In contrast to multi-task learning frameworks like Distral, knowledge flow focuses on a single task, with a transfer of information to boost performance. In multi-task learning, information from different tasks is shared to improve performance, while knowledge flow leverages information from multiple teachers to help a student learn a new task. Knowledge distillation involves transferring information from a larger deep net to a smaller one trained on the same dataset. Actor-mimic enables an agent to address multiple tasks simultaneously and generalize knowledge to new domains. Our proposed technique allows knowledge transfer between different source and target domains, utilizing teachers' representations at the beginning of training. Our proposed technique, Learning without forgetting BID13, allows for adding a new task to a deep net without losing original capabilities by using only data from the new task. Knowledge is transferred more explicitly from teacher networks in contrast to other techniques like regression and cross entropy loss."
}