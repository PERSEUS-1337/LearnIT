{
    "title": "SJeQi1HKDH",
    "content": "In this work, social influence is integrated into reinforcement learning to enable agents to learn from both the environment and peers. The Interior Policy Differentiation (IPD) algorithm encourages agents to develop unique policies while solving tasks, leading to performance improvement and diverse behaviors. Reinforcement Learning is a learning paradigm inspired by cognition and animal studies, focusing on maximizing cumulative rewards through interaction with the environment. Biodiversity and skill development are essential for evolution and continuation. Reinforcement Learning (RL) involves learning through interaction with the environment to maximize cumulative rewards. Biodiversity and skill development are crucial for species evolution. Two approaches to increase behavioral diversity in RL include designing rich environments and motivating agents to explore beyond reward maximization. In this work, the focus is on policy differentiation in RL to enhance agent diversity while maintaining task-solving abilities. Inspired by social influence in animal societies, the concept is applied to reinforcement learning. The learning scheme involves agents maximizing rewards while differentiating actions to be unique from others. Social uniqueness motivation is implemented as a constrained optimization problem, with a policy distance metric used to compare policies in the policy space. In this work, social uniqueness motivation is applied as a constrained optimization problem in reinforcement learning. A novel method called Interior Policy Differentiation (IPD) is introduced to encourage agents to perform well in tasks while taking different actions from other agents. The method is benchmarked on locomotion tasks, showing diverse and well-behaved policies can be learned using the Proximal Policy Optimization (PPO) algorithm. In this work, various intrinsic motivation methods are explored to tackle sparse reward problems in reinforcement learning. These methods include Variational Information Maximizing Exploration (VIME), curiosity-driven methods, Random Network Distillation (RND), and Competitive Experience Replay (CER). These methods add intrinsic reward terms to encourage exploration and improve learning performance. Random Network Distillation (RND) and Competitive Experience Replay (CER) introduce intrinsic rewards based on prediction differences and state coincidence. Task-Novelty Bisector (TNB) optimizes external and intrinsic rewards by updating policies along the angular bisector of their gradients. However, the foundation for joint optimization is questioned, highlighting the challenge of balancing external and intrinsic rewards in reinforcement learning. TNB updates policy by optimizing external and intrinsic rewards along the angular bisector of their gradients. Additional neural networks like auto-encoders are needed for evaluating novelty, leading to extra computation expenses. DPPO method enables agents to learn complex locomotion skills in diverse environments. Different RL algorithms may converge to different policies for the same task. The research explores how different RL algorithms converge to various policies for the same task. The focus is on learning different policies through a single algorithm to avoid local optima. Kurutach et al. (2018) maintain model uncertainty using an ensemble of deep neural networks. A metric is defined to measure policy differences, laying the foundation for the proposed algorithm. Learned policies are denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}, with \u03b8 i representing parameters and \u0398 the parameter space. The proposed algorithm introduces learned policies denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}. A metric space is defined with the Total Variance Divergence used to measure policy distance. The method aims to maximize the uniqueness of a new policy by maximizing \u03b8 U(\u03b8|\u0398). The proposed algorithm introduces learned policies denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}. To improve sample efficiency, the method suggests approximating the domain of possible states between different policies. The proposed algorithm introduces learned policies {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...} and suggests approximating the domain of possible states between different policies to improve sample efficiency. This approximation requires that the domain of possible states be similar between policies, enabling the use of \u03c1(s|s \u223c \u03b8) as the choice of \u03c1(s). In practice, this condition is typically met by adding noise to \u03b8 and ensuring a limited state space. By sampling from S \u03b8 \u222a S \u03b8j, the properties in Definition 1 can be satisfied, with N representing random action in untrained state domains. The objective function of policy differentiation involves terms related to policy \u03b8 and the domain S \u03b8, with sufficient exploration and initialization leading to the disappearance of the last term. The proposed algorithm introduces learned policies to approximate the domain of possible states between different policies for improved sample efficiency. By enabling sufficient exploration and initialization, the last term related to the domain disappears. The estimation of \u03c1 \u03b8 (s) using a single trajectory is unbiased. The learning objective in RL considers both reward from the primal task and policy uniqueness. Previous approaches often combine the rewards from the primal task and intrinsic rewards to improve behavioral diversity. The algorithm introduces learned policies to approximate the domain of possible states for improved sample efficiency. It considers a weighted sum of the reward from the primal task and intrinsic reward, with a focus on the selection of parameters like \u03b1 and formulation of intrinsic reward. To address these issues, the optimization problem is transformed into a constrained problem, incorporating social uniqueness as a constraint rather than an additional target. The optimization problem is transformed into a constrained problem by incorporating social uniqueness as a constraint. This is achieved by using a penalty method with a penalty term and coefficient, with the difficulty lying in the selection of the coefficient. Different approaches, such as Task Novel Bisector and Interior Point Methods, are proposed to solve this constrained optimization problem. In the proposed RL paradigm, the constrained optimization problem is solved by bounding collected transitions within the feasible region using trained policies, ensuring all samples are valid during the training process. This approach overcomes the computational challenges and numerical instability of directly applying Interior Point Methods. During the training process of new agents, valid samples are collected within a feasible region, ensuring uniqueness in the new policy. This Interior Policy Differentiation (IPD) method eliminates the need to balance intrinsic and extrinsic rewards. Tested on MuJoCo-based environments, the approach shows robustness in locomotion tasks. In experiments on locomotion environments like Hopper-v3, Walker2d-v3, and HalfCheetah-v3, uniqueness in policies can be generated by selecting different random seeds. The proposed method is based on PPO and compared with TNB and WSR approaches. More implementation details are in Appendix D. The proposed method, along with TNB and WSR approaches, combines task goals and uniqueness motivation. The uniqueness metric is used directly in learning new policies without reshaping. In experiments, 10 policies are trained sequentially to be unique. Qualitative results are shown in Fig.2, depicting agent motion with fixed settings for highlighted frames and velocity intervals. The proposed method, TNB, and WSR approaches combine task goals and uniqueness motivation. In experiments, 10 policies are trained sequentially to be unique, with qualitative results shown in Fig.2. The visualization in Fig.3 displays experimental results in terms of uniqueness and performance, with our method outperforming others in Hopper and HalfCheetah. Detailed comparison on task-related rewards is provided in Table 1, and performance of trained policies is depicted in Fig.5 and Fig.6 in Appendix C. In Table 1, a detailed comparison of task-related rewards is presented. Figures 5 and 6 in Appendix C show the performance of each trained policy and their reward gaining curve. Additionally, Figure 7 in Appendix C provides more detailed results from the perspective of uniqueness. Success rate is also used as a metric to compare different approaches, with the success rate of all methods, including the PPO baseline, shown in Table 1. The results indicate that the proposed method consistently surpasses the average baseline during training, ensuring performance improvement. Our method consistently outperforms the average baseline in training, ensuring improved performance. Notable enhancements were observed in the Hopper and HalfCheetah environments, where our method prevents policies from getting stuck in local minima. This feature encourages exploration of different action patterns, leading to performance improvements. Our method prevents policies from getting stuck in local minima, encouraging exploration in the HalfCheetah environment. The agent learns to terminate itself early to avoid control costs and then adapts to pursue higher rewards, resembling an implicit curriculum. The learning process involves terminating early to avoid control costs, then adapting for higher rewards, resembling an implicit curriculum. As the number of policies learned with social influence grows, finding unique policies becomes more challenging. An efficient approach is developed to motivate RL to learn diverse strategies inspired by social influence, introducing Interior Policy Differentiation (IPD) as a method to encourage policy uniqueness. The Interior Policy Differentiation (IPD) method is proposed to encourage policy uniqueness in RL learning diverse strategies inspired by social influence. Experimental results show IPD can help agents avoid local minimum and act as implicit curriculum learning. Comparison with other methods like TNB and WSR shows trade-offs between uniqueness and task-related performance. Our proposed method addresses the trade-off between task-related performance and cost through hyper-parameter tuning and reward shaping. We use deterministic policies in PPO and MLP with 2 hidden layers for actor models. Training timesteps are fixed, and we carefully select the number of hidden units based on success rate, performance, and computation expense. In our proposed method, we control policy uniqueness by adjusting the constraint threshold. Different thresholds lead to varying policy behaviors, affecting agent performance. We use cumulative uniqueness as constraints instead of forcing every action to be different. Testing with different threshold values shows varying agent performance. In our method, we adjust the constraint threshold to control policy uniqueness, which affects agent performance. Testing with different thresholds shows varying agent performance under constraints. The Penalty Method considers constraints for optimization. The optimization of policy is based on trajectory samples and implemented with stochastic gradient descent. The Penalty Method considers constraints by using a penalty term and solving the unconstrained problem iteratively. The TNB method selects a direction based on the bisector of gradients. The learning stride in TNB is fixed. The TNB method selects a direction based on the bisector of gradients. The learning stride is fixed, which can lead to issues when the gradient of the objective function approaches zero. To address this, a barrier term can be used to ensure convergence towards the optimal solution. The choice of the barrier term parameter, \u03b1, is crucial for the success of the optimization process. The learning process in IPD based on PPO involves bounding collected transitions in the feasible region by terminating new agents that step outside it. This ensures that all valid samples collected during training are inside the feasible region, leading to a new policy with sufficient uniqueness. This approach eliminates the need to balance intrinsic and extrinsic rewards and improves robustness in the learning process. The pseudo code of IPD based on PPO is shown in Algorithm.1 with additional steps highlighted in blue."
}