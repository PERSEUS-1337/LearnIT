{
    "title": "rkeNfp4tPr",
    "content": "Stochastic momentum in SGD is popular for training deep neural networks as it helps escape saddle points faster and find a second order stationary point more quickly. Theoretical justification for its use has been elusive, but empirical evidence shows it improves convergence time in deep network training. In this paper, the authors propose that stochastic momentum improves deep network training by helping SGD escape saddle points faster and find a second order stationary point more quickly. The ideal momentum parameter should be close to 1, according to their analysis. Experimental findings further support these conclusions. SGD with stochastic momentum is widely used in nonconvex optimization and deep learning, with applications in computer vision, speech recognition, natural language processing, and reinforcement learning. In this paper, the authors highlight the benefits of using stochastic momentum in training deep neural networks. The success of momentum in achieving faster convergence compared to standard SGD has made it a crucial tool in optimization and deep learning. Despite the widespread use of stochastic momentum, the empirical improvements and guidelines for setting the momentum parameter remain unclear. Large values, such as \u03b2 = 0.9, have been observed to work well in practice. In this paper, a theoretical analysis is provided for stochastic gradient descent (SGD) with momentum. The algorithm involves setting the momentum parameter \u03b2 and step size parameter \u03b7, updating the iterate based on stochastic gradients, and maintaining a weighted average for stochastic heavy ball momentum. The analysis shows that these updates can help escape saddle points faster than standard SGD. The paper focuses on finding a second-order stationary point for smooth non-convex optimization using stochastic heavy ball momentum in SGD. It aims to obtain an approximate second-order stationary point with additional assumptions like Lipschitzness in gradients and the Hessian. The paper aims to show the benefits of using momentum in reaching a ( , )-second-order stationary point in nonconvex optimization. It introduces a condition, similar to a model assumption in previous works, to ensure updates align with negative curvature directions. The stochastic momentum helps escape saddle points faster by amplifying the escape signal. The recursive dynamics of SGD with heavy ball momentum satisfy Correlated Negative Curvature (CNC) at t with parameter \u03b3 > 0, helping in escaping saddle points faster. Under CNC assumption and constraints on parameter \u03b2, SGD with momentum takes T = O((1 \u2212 \u03b2) log(1/(1 \u2212 \u03b2) ) \u221210 ) iterations to reach a ( , ) second order stationary point. A larger momentum parameter \u03b2 enables faster escape from saddle points, shedding light on the benefits of using momentum in optimization and deep learning. The benefits of using stochastic momentum in optimization and deep learning lie in its ability to help escape saddle points faster by moving in the direction of the smallest eigenvector of the Hessian matrix. This ensures a quicker escape from regions where the gradient is small and the Hessian has negative eigenvalues. The use of stochastic momentum in optimization helps escape saddle points faster by moving in the direction of the smallest eigenvector of the Hessian matrix, ensuring a quicker escape from regions with small gradients and negative eigenvalues. Daneshmand et al. (2018) studied non-momentum SGD and made an assumption that each stochastic gradient is strongly non-orthogonal to the direction of large negative curvature, driving updates out of the saddle point region. In the present paper, stochastic momentum requires the update direction to be strongly non-orthogonal to the direction of escape, leading to updates escaping the saddle point region for similar reasons as discussed in previous research. The use of stochastic momentum in optimization accelerates the escape from saddle points by leveraging the analysis of Daneshmand et al. (2018). Momentum updates amplify the escape process in successive iterations, providing a clear benefit in speeding up saddle-point escape. However, the choice of momentum parameter \u03b2 is constrained and cannot be arbitrarily close to 1. Empirical evidence supports the effectiveness of stochastic momentum in improving performance. The use of stochastic momentum in optimization accelerates the escape from saddle points by a factor of 1 \u2212 \u03b2. Empirical evidence demonstrates the benefits of stochastic momentum in solving non-convex optimization challenges with significant saddle points. In optimization, convergence in function value is plotted with different algorithms using the same step size. The second objective involves phase retrieval, where one aims to find an unknown vector with limited samples. Empirical findings show that larger choices of beta accelerate convergence significantly for both objectives. The empirical findings in Figure 2 show that larger choices of beta accelerate convergence significantly for both objectives. Trajectories with large momentum escape saddle points more quickly than those with smaller momentum, as seen in Figures 4a and 4b. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases. The curr_chunk discusses specialized algorithms designed to exploit negative curvature and escape saddle points faster. It mentions pioneer works by Ge et al. (2015) and Jin et al. (2017) in this category. The text also touches on the nonconvex nature of phase retrieval and the strict saddle property. The curr_chunk discusses the assumption of Correlated Negative Curvature (CNC) for stochastic momentum in escaping saddle points. It compares results with related works and assumes properties like Lipschitz gradient and Hessian. The algorithm avoids perturbing updates with isotropic noise and ensures bounded noise and momentum. The curr_chunk discusses the properties of stochastic momentum, including Almost Positively Aligned with Gradient (APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature Exploitation (GrACE). These properties are argued to hold in natural settings and are later demonstrated empirically. The SGD with momentum exhibits Gradient Alignment or Curvature Exploitation (GrACE) if the momentum term is not significantly misaligned with the gradient. This condition ensures progress in the algorithm and is related to Almost Positively Aligned with Gradient (APAG) and Almost Positively Correlated with Gradient (APCG) properties. The momentum term is viewed as a biased estimate of the gradient, and the analysis requires these properties when the gradient is large. Empirical evidence supports these properties on natural problems. The PSD matrix M t measures the local curvature of the function with respect to the trajectory of SGD with momentum. Empirical evidence supports the APAG and APCG properties on natural problems, especially in saddle regions with significant iterations. The analysis does not require APCG to hold when the gradient is large or the update is at an ( , implying that SGD with momentum exhibits GrACE properties. The analysis discusses the alignment between stochastic momentum and gradient, as well as curvature exploitation in the phase retrieval problem. It shows that a small sum of these terms allows for bounding the function value of the next iterate. Experimental results related to APAG, APCG, and GrACE properties when solving problems using SGD with momentum are also reported. The analysis focuses on bounding the function value of the next iterate by aligning stochastic momentum and gradient, as well as curvature exploitation in the phase retrieval problem. The proof is structured into three cases based on the gradient and Hessian properties. The algorithm analyzed is similar to Algorithm 1 but with a larger step size parameter. The analysis focuses on bounding the function value of the next iterate by aligning stochastic momentum and gradient, as well as curvature exploitation in the phase retrieval problem. The algorithm makes progress in different cases and ultimately reaches a second-order stationary point with high probability. Theorem 1 states conditions for reaching a second-order stationary point with high probability using stochastic gradient descent with momentum. The theorem highlights the advantage of using stochastic momentum for SGD, with higher \u03b2 leading to faster convergence to a second-order stationary point. Constraints on \u03b2 are necessary to prevent it from being too close to 1. The analysis also shows that the algorithm can escape saddle points faster with momentum, as demonstrated in the comparison with CNC-SGD. In the high momentum regime, Algorithm 2 outperforms CNC-SGD, showing that higher momentum helps find second-order stationary points faster. Empirically, conditions for a wide range of \u03b2 are easily satisfied in the phase retrieval problem. The analysis focuses on escaping saddle points with SGD using momentum, showing that it takes at most T thred iterations to escape a region with small gradient and large negative eigenvalue of the Hessian. In the high momentum regime, Algorithm 2 outperforms CNC-SGD, showing that higher momentum helps find second-order stationary points faster. The analysis focuses on escaping saddle points with SGD using momentum, showing that it takes at most T thred iterations to escape a region with small gradient and large negative eigenvalue of the Hessian. The technique used is proving by contradiction, showing that the function value must decrease at least by F thred in T thred iterations on expectation. In the high momentum regime, Algorithm 2 outperforms CNC-SGD, showing that larger momentum helps escape saddle points faster. The analysis provides upper and lower bounds on the expected distance, with Lemmas 1, 2, and 3 detailing the recursive dynamics of SGD with momentum. The critical component for ensuring the lower bound is larger than the upper bound is highlighted, emphasizing the importance of momentum in optimization. The text discusses the importance of momentum in optimization, specifically in reaching a second-order stationary point faster. Lemmas 1, 2, and 3 provide bounds on the expected distance in SGD with momentum, emphasizing the critical component for ensuring the lower bound surpasses the upper bound. The analysis shows that larger momentum helps escape saddle points faster, with the lower bound growing exponentially with time and momentum parameter \u03b2. The text discusses the importance of momentum in optimization, specifically in reaching a second-order stationary point faster. It identifies three properties that guarantee SGD with momentum reaches a second-order stationary point faster with a higher momentum parameter \u03b2. The results shed light on the success of SGD with momentum in non-convex optimization and deep learning. The heavy ball method, proposed by Polyak in 1964, is also mentioned. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases, except for convex quadratic objectives. Recent works have analyzed the method for other optimization problems but have not found significant improvements in convergence speed compared to standard SGD. Some variants of stochastic accelerated algorithms have been proposed, but they do not capture the practical use of stochastic heavy ball momentum. Additionally, a negative result has been shown for the heavy ball momentum in specific scenarios. Recent works have analyzed the heavy ball momentum method but have not found significant improvements in convergence speed compared to standard SGD. Kidambi et al. (2018) show that for specific problems, SGD with heavy ball momentum fails to achieve the best convergence rate. Specialized algorithms aim at reaching a second order stationary point by exploiting negative curvature explicitly. Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates. The algorithm discussed in the work can help escape saddle points faster than standard SGD under certain conditions. The study compares different SGD variants and focuses on the effectiveness of stochastic heavy ball momentum. The analysis framework is based on previous research, aiming to show the advantage of using stochastic momentum in optimization algorithms. The analysis framework is built on previous research to demonstrate the advantage of using stochastic momentum in optimization algorithms. Lemmas 6, 7, and 8 show that SGD with momentum can decrease the function value and make progress under certain conditions. Lemma 7 states that if SGD with momentum has the APAG property and the step size \u03b7 satisfies a certain condition, then a specific inequality holds. Lemma 8 discusses the GrACE property of SGD with momentum and provides a proof for an update rule. Lemma 2 and Lemma 3 provide a quadratic approximation at time t0 and discuss the update rule for SGD with momentum. Lemma 2 and Lemma 3 discuss the quadratic approximation at time t0 and the update rule for SGD with momentum. The proof involves rewriting equations and using specific notations to show the properties of the algorithm. Additionally, Lemma 5 establishes conditions for the APCG property of SGD with momentum. Lemma 5 establishes conditions for the APCG property of SGD with momentum, including constraints on the parameter \u03b2 to ensure certain properties hold. The proof involves a series of lemmas with specific parameter choices. To prove Lemma 5 for the APCG property of SGD with momentum, a series of lemmas with specific parameter choices are needed. The upper bounding of certain expressions is established by applying triangle inequality and Lipschitz gradient assumptions. The analysis involves deriving upper bounds for various terms based on specific conditions and parameter choices. Lemma 11 and Lemma 12 provide lower bounds for specific expressions under certain conditions. The analysis involves bounding terms based on parameter choices and using the tower rule. Lemma 11 and Lemma 12 provide lower bounds for specific expressions under certain conditions, involving parameter choices and the tower rule. Lemma 13 and Lemma 14 continue this analysis by bounding terms and defining matrices, with a focus on the properties of symmetric positive semidefinite matrices. Lemma 14 provides a proof that if SGD with momentum has the APCG property, then the function value must decrease at least F thred in T thred iterations on expectation. This is achieved by showing that the lower bound is larger than the upper bound, leading to a contradiction. Lemma 14 proves that if SGD with momentum has the APCG property, the function value must decrease by at least F thred in T thred iterations on expectation. To obtain the contradiction, it is necessary to show that certain inequalities hold, which can be achieved by choosing T thred sufficiently large. Lemma 14 proves that if SGD with momentum has the APCG property, the function value must decrease by at least F thred in T thred iterations on expectation. In the subsequent subsection, a sketch of the proof of Theorem 1 is provided, which guarantees reaching a second-order stationary point in T iterations with high probability. The complete proof is available in Appendix G. The lemma in (Daneshmand et al. (2018)) ensures that uniformly sampling a w from {w kT thred}, k = 0, 1, 2, ..., T/T thred gives a second-order stationary point with high probability. The proof relies on Lemma 15 to show that by satisfying certain conditions, the function value decreases by at least F thred in T thred iterations with high probability. This is achieved by selecting w from a specific set and ensuring specific conditions are met, allowing the application of Lemma 15 to conclude the proof of the theorem. The proof of the theorem relies on Lemma 15, which guarantees a decrease in function value by at least F thred in T thred iterations. By satisfying certain conditions, Algorithm 2 is shown to be superior to previous methods in finding a second order stationary point efficiently."
}