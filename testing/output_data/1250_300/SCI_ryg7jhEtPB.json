{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) is a variational-inference method that achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. However, the method relies on reparametrizations and faces challenges with inference-network gradients as the number of samples increases. Alternative approaches like the 'sticking-the-landing' IWAE gradient and the 'doubly-reparametrised' IWAE gradient have been proposed to address these issues. The authors argue for directly optimizing the proposal distribution in importance sampling, as in the reweighted wake-sleep algorithm, over IWAE-type multi-sample objectives. The proposal distribution in importance sampling, as in the reweighted wake-sleep algorithm, is preferred over optimizing IWAE-type multi-sample objectives. An adaptive importance sampling framework called AISLE is introduced, which generalizes the RWS algorithm and includes IWAE-STL and IWAE-DREG as special cases. The work focuses on variational inference algorithms to learn the generative model and construct a tractable variational approximation. In variational inference, the maximum likelihood estimate (MLE) is used to find the parameter \u03b8 that maximizes the probability p\u03b8(x). A variational approximation q\u03c6,x(z) of p\u03b8(z|x) is constructed to approximate the true distribution. Different stochastic gradient-ascent algorithms have been proposed to optimize the parameters (\u03b8, \u03c6) using Monte Carlo samples. The IWAE-DREG method drops problematic terms from the IWAE gradient, introducing bias for the IWAE objective. The IWAE-DREG method introduces bias by dropping problematic terms from the IWAE gradient, while the RWS algorithm optimizes separate objectives for \u03b8 and \u03c6 using self-normalized importance sampling with K particles. RWS is an adaptive importance-sampling approach that iteratively improves its proposal distribution and optimizes \u03b8 via stochastic approximation. The IWAE is more popular, but RWS can break down as it does not optimize a joint objective for \u03b8 and \u03c6. In contrast to the IWAE-DREG method, the RWS algorithm optimizes separate objectives for \u03b8 and \u03c6 using self-normalized importance sampling with K particles. RWS is an adaptive importance-sampling approach that iteratively improves its proposal distribution and optimizes \u03b8 via stochastic approximation. The IWAE is more popular, but RWS can break down as it does not optimize a joint objective for \u03b8 and \u03c6. The IWAE suffers from \u03c6-gradient breakdown and exhibits inferior empirical performance compared to RWS. It is unclear whether the multi-sample objective approach of IWAE or the adaptive importance-sampling approach of RWS is preferable. The conclusion is that directly optimizing the proposal distribution, as done by RWS, is preferable to optimizing the IWAE multi-sample objective. The breakdown of IWAE can be justified by an adaptive importance-sampling view, as shown in previous work. Our work introduces the AISLE framework, a generic adaptive importance-sampling framework for variational inference. This framework encompasses RWS, IWAE-DREG, and IWAE-STL gradients as special cases. The derived gradient estimators from AISLE are guaranteed not to degenerate as K \u2192 \u221e. The AISLE framework introduces new gradient estimators that do not degenerate as K \u2192 \u221e. It connects IWAE-STL and IWAE-DREG gradients as special cases, providing a theoretical foundation for IWAE-STL. The learning rate scaling differs between IWAE \u03c6-gradient and AISLE, leading to a new family of gradient estimators for \u03b1-divergences. The AISLE framework introduces new gradient estimators for \u03b1-divergences that do not degenerate as K \u2192 \u221e. It provides insights into the impact of self-normalization bias on importance-sampling based gradient approximations and compares algorithms empirically. The focus is not on deriving new algorithms but on comparing existing ones. The importance weighted autoencoder (IWAE) aims to maximize a lower bound on the generative-model parameters \u03b8 introduced by Burda et al. (2016). The estimator for expectations of test functions can be unbiasedly estimated by \u03c6, which are IID according to q \u03c6. The self-normalised estimate \u03c0 \u03b8 \u03c6, z (f) is typically not unbiased, but its bias vanishes at rate O(K \u22121) under mild assumptions. The IWAE maximizes a lower bound on the generative-model parameters \u03b8. The bound tightens with optimization of the inference-network parameters \u03c6. For K > 1, IWAE extends VAE on an auxiliary-variable space. The gradient of the IWAE objective can be approximated using a Monte Carlo approach. The IWAE objective maximizes a lower bound on generative-model parameters \u03b8 by optimizing the inference-network parameters \u03c6. The gradient of the IWAE objective can be approximated using a Monte Carlo approach, but the high variance of this approximation is typically noisy. To address this, the reparametrisation trick is employed, requiring certain assumptions. The IWAE uses a vanilla Monte Carlo estimate and focuses on the \u03c6-portion of the gradient, which has three drawbacks. The IWAE gradient has drawbacks related to reparametrisations, vanishing signal-to-noise ratio, and inability to achieve zero variance. These issues stem from the high variance term G \u03c8 (z) and the self-normalised importance-sampling approximation of \u03c0 \u03b8 ( \u03c8 \u2212 \u2207 \u03c6 log q \u03c6 ) = 0. Various modifications have been proposed to address these limitations. The IWAE gradient has limitations in achieving zero variance and stable signal-to-noise ratio. Modifications like IWAE-STL and IWAE-DREG have been proposed to address these issues by removing score-function terms and introducing biases. The RWS algorithm is used to approximate intractable quantities. The RWS algorithm, proposed in Bornschein & Bengio (2015), approximates intractable quantities using self-normalised importance sampling. The optimization of both \u03b8 and \u03c6 is done simultaneously, but the lack of a joint objective is seen as a drawback. The algorithm allows for reusing Monte Carlo samples to approximate the \u03b8-gradient. The RWS algorithm uses self-normalised importance sampling to approximate intractable quantities. Optimization of both \u03b8 and \u03c6 is done simultaneously, with the key insight that optimizing \u03c6 can reduce error in the \u03b8-gradient approximation. Different techniques for adapting the proposal distribution q \u03c6 exist in the literature, including minimizing the \u03c7 2 -divergence. The RWS-objective is slightly generalized as maximizing log Z \u03b8 for \u03b8 and minimizing D\u0192(\u03c0\u03b8 q \u03c6) for \u03c6, where D\u0192 is some \u0192-divergence from p to q. Alternative approaches for optimizing \u03c6 could be considered. The RWS algorithm uses self-normalised importance sampling to approximate intractable quantities. Optimization of both \u03b8 and \u03c6 is done simultaneously, with the key insight that optimizing \u03c6 can reduce error in the \u03b8-gradient approximation. The unified framework allows for a principled derivation of robust \u03c6-gradient estimators that do not degenerate as K \u2192 \u221e. The \u03b8-gradient is the same for all algorithms discussed, interpreted differently by IWAE and AISLE/RWS. Integrals involving \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ) naturally appear in the derivations. The text discusses the biased approximation of the gradient for the 'exact' objective using integrals involving \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ). It mentions approximating expectations and normalizing constants with the vanilla Monte Carlo method, showing bias and standard deviation orders. The optimization of \u0192-divergences in intractable models is explored without relying on Z \u03b8 knowledge, leading to a reparametrised estimator. The text discusses the biased approximation of the gradient for the 'exact' objective using integrals involving \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ). It mentions approximating expectations and normalizing constants with the vanilla Monte Carlo method, showing bias and standard deviation orders. The optimization of \u0192-divergences in intractable models is explored without relying on Z \u03b8 knowledge, leading to a reparametrised estimator. The reparametrised estimator can be approximated with self-importance sampling, possibly multiplied by an additional importance-sampling approximation Z \u03b8 \u03c6, z of Z \u03b8 raised to some power. This leads to Equation (10) applying to (11), resulting in the reparametrised estimator where h(y) = g(y)y and g : R \u2192 R given immediately above (11). Several particular cases are described, including AISLE-KL-NOREP/RWS and AISLE-KL, which demonstrate the derivation of IWAE-STL in a principled manner from AISLE without the need for a multi-sample objective. Theoretical basis for IWAE-STL is derived from AISLE without multi-sample objective, reducing bias and variance. \u03b1-divergence between distributions p and q is expressed as Z (p(z)/q(z)) \u03b1 q(z) dz for \u03b1 > 1. The \u03b1-divergence between two distributions p and q can be expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz, where \u03ba = \u2212\u03b1 and f (y) = y \u03b1. Minimizing this divergence is important in importance sampling, as it relates to the variance of the importance weights. AISLE-\u03b1-NOREP and AISLE-\u03b1 provide methods for minimizing this divergence, with AISLE-\u03b1 using reparametrisation. IWAE-DREG can be derived from AISLE in a principled manner without the need for a multi-sample objective. Equation (12) can be derived from AISLE to obtain IWAE-DREG without the need for a multi-sample objective. The learning rate for IWAE or IWAE-DREG \u03c6-gradients needs to be scaled as O(K). Optimizing the 'exclusive' KL-divergence can lead to faster convergence of \u03c6 compared to optimizing the 'inclusive' KL-divergence. Optimizing the 'exclusive' KL-divergence can lead to faster convergence of \u03c6 compared to optimizing the 'inclusive' KL-divergence. Care must be taken as minimizing the exclusive divergence may not lead to well-defined importance weights, affecting the learning of \u03b8. The adaptive-importance sampling paradigm of RWS is preferred over IWAEs as it achieves the same goals while avoiding drawbacks. The self-normalised importance-sampling approximation adjusts the number of particles, K, to provide accurate approximations of the target distribution. The number of particles, K, affects the accuracy of approximations in a range from vanilla Monte Carlo to increasingly accurate estimators as K increases. The small-K self-normalisation bias of AISLE \u03c6 gradients favors minimizing the exclusive KL-divergence. The use of importance-sampling approximations with K > 1 particles in IWAEs aims to reduce bias in the \u03b8-gradient compared to \u2207 \u03b8 log Z \u03b8. The error of these approximations can be controlled by ensuring q \u03c6 is close to \u03c0 \u03b8, particularly by minimizing the 'inclusive' KL-divergence KL(\u03c0 \u03b8 q \u03c6). The family of proposal distributions Q can be flexible or insufficiently expressive. In the former case, minimizing the exclusive KL-divergence can yield well-behaved importance weights, while in the latter case, it could lead to poorly-behaved weights. In Scenario 1, for a flexible Q, using a gradient-descent algorithm to minimize exclusive divergence may be preferable for faster convergence. A smaller number of particles, K, could be better for some \u03c6-gradients due to self-normalization bias. Increasing K is still desirable to reduce gradient approximation variance. The \u03c6-gradients approximation may not be optimal even in Scenario 1. Increasing K is desirable to reduce variance, and not utilizing all particles and weights for gradient approximation is wasteful. Different \u03c6-gradient estimators are compared, including AISLE-KL-NOREP. In this work, various gradient approximations for AISLE are compared based on different divergence measures such as KL and \u03c72, with and without reparametrization. These approximations do not require R1 but may not achieve zero variance, even if q\u03c6 = \u03c0\u03b8. The gradients can be normalized to cancel out proportionality constants, leading to similar results. The gradient approximations for IWAE, IWAE-DREG, and RWS-DREG are compared based on different divergence measures such as KL and \u03c72. The gradients can be normalized to cancel out proportionality constants, leading to computationally the same algorithm. The joint law of observations and latent variables is parametrized by \u03b8 and factorizes accordingly. The joint law of observations and latent variables, parametrized by \u03b8, factorizes. Latent variable-observation pairs are modeled, with known matrix D. Proposal distributions are fully factored Gaussians. The model is similar to benchmarks in previous studies. In a more realistic scenario, the latent vectors z can be correlated under the generative model, while the variational approximation remains fully factored. The 'score-function free' \u03c6-gradients achieve near zero variance for proposal mean parameters when variance parameters are optimal. The 'score-function free' \u03c6-gradients approach achieves near-zero variance for proposal mean parameters when variance parameters are optimal. Empirical comparisons of algorithms are conducted for various particle numbers and model dimensions, with results showing the benefits of reparametrisation-trick gradients in Gaussian settings. The generative model is specified via different covariance matrices, leading to two scenarios where the variational approximation may or may not mimic the dependence structure of the latent variables. Gradient-ascent algorithms are initialised with standard normal distributions and different optimisation techniques are used for parameter updates. The generative model uses different covariance matrices, leading to scenarios where the variational approximation may or may not mimic the dependence structure of latent variables. The gradient-ascent algorithms are initialised with standard normal distributions, and ADAM is used for parameter updates with default values. The total number of iterations is 10,000, with learning-rate parameters at each step being i^(-1/2). The covariance matrix is not diagonal, with a logarithmic scaling on the second axis."
}