{
    "title": "HJ3d2Ax0-",
    "content": "The paper analyzes the impact of depth on the ability of recurrent networks to express correlations over long time-scales. It introduces a measure called the Start-End separation rank to quantify the information flow across time supported by the network. Deep recurrent networks are shown to support Start-End separation ranks that reflect the distance from a function that models no interaction between the beginning and end of the input sequence. Deep recurrent networks, specifically Recurrent Arithmetic Circuits (RACs), demonstrate exponentially higher Start-End separation ranks compared to shallow counterparts. This depth advantage allows deep networks to correlate different parts of input sequences as they extend, while shallow networks do not adapt to sequence length. The ability of deep recurrent networks to model long-term dependencies is significantly enhanced, providing a quantifiable advantage that can be extended to other RNN architectures like LSTM networks. Recurrent Arithmetic Circuits (RACs) merge hidden state with input via Multiplicative Integration operation. Recurrent Neural Networks (RNNs) are widely used for modeling sequential data. The success of recurrent networks in learning complex dependencies implies support for long-term correlations in inputs. However, formal understanding of how network structure influences expressiveness is lacking. Ongoing efforts include augmentations like Long Short Term Memory (LSTM) to handle tasks of increasing complexity and temporal extent. In this paper, the focus is on deep recurrent networks, which show empirical superiority over shallow ones in processing information hierarchically at every time-step. Experiments suggest that deep networks model correlations corresponding to longer time-scales. The advantage of depth in complexity and temporal capacity of recurrent networks lacks a theoretical explanation, which this paper aims to address. The paper addresses the advantage of depth in complexity and temporal capacity of recurrent networks, with a focus on deep recurrent networks showing superiority over shallow ones in processing information hierarchically. It explores the exponential boost depth provides to the ability of recurrent networks in expressing functions efficiently and handling dependencies over longer time-scales. The study explores how depth enhances long-term memory in recurrent networks, specifically deep recurrent networks' ability to model long-term dependencies efficiently. It introduces a recurrent arithmetic circuit (RAC) that shares architectural features with RNNs but differs in non-linearity. This connection between machine learning algorithms and arithmetic circuits has precedence in neural networks, with depth efficiency results proven by Delalleau and Bengio (2011) and theoretical analysis by Cohen et al. (2016) on Convolutional Arithmetic Circuits. These findings extend to common ConvNets as well. The study introduces Convolutional Arithmetic Circuits, which differ from common ConvNets similar to how RACs differ from standard RNNs. Empirical results show connections to common ConvNets and performance boosts similar to Multiplicative RNNs. The analysis connects RACs to Tensor Train decomposition, suggesting a relationship to a generalized TT-decomposition. The notion of Start-End separation rank is introduced as a measure of modeling long-term dependencies in recurrent networks. The notion of Start-End separation rank is introduced as a measure of modeling long-term dependencies in recurrent networks. It analyzes correlations between the beginning and end of input sequences, with high separation rank indicating strong correlation. Deep networks with depth L = 2 RACs have exponentially higher separation ranks than shallow networks, allowing them to model more elaborate input dependencies over longer periods of time. Deep recurrent networks with depth L = 2 RACs show exponentially higher Start-End separation ranks compared to shallow networks, allowing them to model more complex input dependencies over longer sequences. Shallow networks are inadequate for modeling correlations in long input sequences as their dependencies do not adapt to sequence length. A quantitative conjecture suggests that the separation rank of recurrent networks grows exponentially with network depth, providing deeper insights into the advantages of depth in recurrent networks. In this section, Recurrent Arithmetic Circuits (RACs) are introduced as a class of recurrent networks that operate similarly to standard RNNs. The key difference lies in the type of non-linearity used in the calculation. The algebraic properties of RACs are utilized to prove their ability to model long-term dependencies in input sequences. The basic framework of shallow recurrent networks is presented, highlighting their commonalities with RNNs and RACs in modeling discrete-time dynamical systems. The text introduces Recurrent Arithmetic Circuits (RACs) as a type of recurrent network for modeling discrete-time dynamical systems. It focuses on a sequence to sequence classification task with input sequences {xt \u2208 X} and output class scores vectors. The shallow recurrent network with hidden channels is depicted in fig. 1(a), where the hidden state ht at time t is calculated using a specific formula. The input space X can be discrete or continuous, and an initial mapping f: X \u2192 RM is performed on the input for encoding purposes. The text introduces Recurrent Arithmetic Circuits (RACs) as a type of recurrent network for modeling discrete-time dynamical systems. It focuses on a sequence to sequence classification task with input sequences {xt \u2208 X} and output class scores vectors. The shallow recurrent network with hidden channels is depicted in fig. 1(a), where the hidden state ht at time t is calculated using a specific formula. The output at time t of the shallow recurrent network is obtained via a final dense layer using some trained ConvNet. The learned parameters \u0398 represent the input, hidden, and output weights matrices, and a non-linear operation g is applied. For common RNNs, the non-linearity is typically a point-wise function like sigmoid or tanh, while for RACs, the operation involves element-wise multiplication between vectors, known as Multiplicative Integration. The extension to deep recurrent networks follows a common approach where each layer acts as a recurrent unit. The extension to deep recurrent networks involves each layer acting as a recurrent network, with the output at each time step constructed using learned parameters and non-linear operations. The input and hidden weights matrices are defined for each layer, with the final output weights matrix determining the scores for all classes. The type of deep recurrent network is determined by the non-linear operation applied, such as a common deep RNN or a deep RAC. The newly presented class of Recurrent Arithmetic Circuits (RACs) is considered a good surrogate for common Recurrent Neural Networks (RNNs). There is a structural resemblance between RACs and RNNs, with both networks having a similar recurrent aspect in their calculations. RACs, which include Multiplicative Integration, have shown superior performance compared to existing RNN models. Additionally, arithmetic circuits have been successfully used as surrogates for convolutional networks. The potential for extending proof methodologies from convolutional arithmetic circuits to common ConvNets with ReLU activations suggests that similar adaptations could be made in the recurrent network analog, making the newly proposed class of recurrent networks intriguing. The algebraic properties of Recurrent Arithmetic Circuits (RACs) are utilized to analyze the benefits of depth in recurrent networks. The Start-End separation rank is introduced as a measure of information flow across time in recurrent networks, tied to the concept of grid tensors for modeling long-term temporal dependencies. Depth exponentially boosts the ability of recurrent networks to capture complex temporal relationships. The Start-End separation rank quantifies a function's distance from separability with respect to two disjoint subsets of inputs in recurrent networks. It is defined as the minimal number of summands that together give the function, where each summand is separable with inputs from the first and last halves of the time-steps. The separation rank quantifies a function's distance from separability with respect to two disjoint subsets of inputs in recurrent networks. It is the minimal number of summands that together give the function, where each summand is separable with inputs from the first and last halves of the time-steps. This concept has been applied in various fields such as chemistry, particle engineering, machine learning, and quantum entanglement measures. If the Start-End separation rank is 1, the function is separable and cannot model interactions between inputs at the beginning and end of the sequence. In the context of recurrent networks, the Start-End separation rank quantifies the ability of a function to model interactions between inputs at the beginning and end of a sequence. A higher separation rank indicates a greater ability to capture dependency between the inputs. Deep RACs are shown to support higher Start-End separation ranks, allowing for more elaborate long-term temporal dependence to be learned. Deep RACs support higher Start-End separation ranks, enabling them to model long-term temporal dependencies more effectively. The use of grid tensors helps evaluate these separation ranks in deep and shallow RACs, with tensor theory providing the necessary background for analysis. The tensor product is a fundamental operator in tensor analysis, denoted by \u2297, which combines two tensors to create a new tensor. Matricization of a tensor A w.r.t. a partition (S, E) arranges tensor elements into a matrix. Shallow RACs with hidden channels compute class scores using a recursive function, represented by a tensor A T,1,\u0398 c. The entries of this tensor are polynomials in network weights \u0398. The shallow RAC weights tensor, denoted as T,1,\u0398 c, is constructed using a Tensor Train (TT) decomposition method. This method involves tensor products related to the Multiplicative Integration property of RACs and is analogous to a Matrix Product State (MPS) Tensor Network in quantum physics. The construction process is detailed in eq. 7 and involves the multiplication of hidden states by hidden weights at each time-step. The concept of grid tensors is introduced as a form of function discretization, where the function is evaluated on a large grid in the input space and stored in a tensor. Grid tensors of functions realized by recurrent networks help calculate separation ranks and assess the benefits of depth in these networks. The tensorial structure of a shallow RAC is tied to its grid tensor to determine its separation rank. The tensorial structure of a shallow RAC is linked to its grid tensor to establish the Start-End separation rank. This relationship is defined by specific template vectors, showing an equality between the separation rank and the matrix rank obtained from the grid tensor matricization. The use of grid tensors is crucial in assessing the benefits of depth in recurrent networks. The rank of the matrix obtained by the grid tensor matricization is crucial in assessing the separation rank of functions realized by deep RACs. The relationship between the separation rank and the matrix rank is defined by specific template vectors, showing a fundamental relation for all functions. The relation between separation rank and matrix rank is crucial for functions realized by deep RACs. Claim 2 provides a lower bound on Start-End separation rank, showing an exponential enhancement with depth in recurrent networks. Theoretical contributions include a result separating memory capacity of deep and shallow recurrent networks, with implications discussed and a proof outline provided. Theorem 1 highlights the complexity difference between deep and shallow recurrent networks in modeling correlations within input sequences. It shows that deep networks can exhibit exponentially more complex correlations than shallow ones. The formal proof is detailed in the appendix, with a quantitative conjecture on memory capacity left for future work. Theorem 1 demonstrates the superior ability of deep recurrent networks over shallow ones in capturing long-term temporal dependencies in sequential inputs. It proves that deep networks can model exponentially more complex correlations compared to shallow networks. Theorem 1 highlights the advantage of deep recurrent networks in capturing long-term dependencies compared to shallow networks. Deep networks can model more complex correlations due to their increasing Start-End separation rank with time, while shallow networks are limited in modeling long-term correlations. The proof sketch of theorem 1 shows that shallow recurrent networks are limited in modeling long-term correlations compared to deep networks, which have an exponentially increasing Start-End separation rank. The rank of the function realized by a shallow network is equal to the rank of the matrix obtained from the weights tensor, and it is crucial to prove that the rank is bounded by the matrix dimension. The TT-decomposition of the tensor is used to demonstrate this, relying on a result by Levine et al. (2017) regarding the combinatorial coefficient's exponential dependence on the minimum of M and R. The rank of the function realized by a deep recurrent network is lower bounded by the rank of the matrix obtained from the corresponding grid tensor matricization. This is supported by a lemma from Sharir et al. (2016) stating that the entries of the network's weights are polynomials, thus finding a single example is sufficient to determine the rank. The rank of the matricized grid tensor is upper-bounded by M T /2. Using a lemma from Sharir et al. (2016), we find an example where the rank exceeds the desired lower bound, showing that the desired inequality holds for most network parameter values. Our weight assignment results in a grid tensor with a rank achieving the upper bound. Theorem 1 establishes a lower bound on the Start-End separation rank of depth L = 2 recurrent networks, distinguishing deep from shallow networks. Theorem 1 establishes a lower bound on the Start-End separation rank of depth L = 2 recurrent networks, distinguishing deep from shallow networks. Conjecture 1 suggests that the memory capacity of deep recurrent networks grows exponentially with network depth, supported by a combinatorial perspective visualized through Tensor Networks. Full details of this construction are provided in appendix A. The construction of Tensor Networks for deep RACs provides a combinatorial perspective on the memory capacity of deep recurrent networks. The Tensor Network graphically represents algebraic operations involving higher order tensors, showcasing the computation of a depth L = 3 RAC after T = 6 time-steps. The Start-End separation rank of deep recurrent networks is estimated using a similar strategy. The Tensor Network construction for deep RACs offers insight into the memory capacity of deep recurrent networks. By finding specific network parameters, the Start-End separation rank can be estimated, with the Tensor Network taking the form of a basic unit connecting \"Start\" and \"End\". The challenge lies in proving that the upper bound on the rank is tight. The Tensor Network construction for deep RACs reveals insights into the memory capacity of deep recurrent networks. The conjecture suggests an exponential advantage in memory capacity between networks of different depths, leaving the proof of this result as an open problem. The paper discusses the memory capacity of deep recurrent networks and the need for a quantifier of 'time-series expressivity'. A measure called Start-End separation rank is proposed to quantify the ability of recurrent networks to model long-term temporal dependencies. This measure adjusts to the temporal extent of input series and correlates with the network's performance on long input sequences. The proposed measure, Start-End separation rank, quantifies the ability of recurrent networks to model long-term dependencies. Deep Recurrent Arithmetic Circuits show exponential increase in separation rank with input sequence length, highlighting the advantage of depth in modeling sequential data. The analysis combines tools from measure theory, tensorial analysis, combinatorics, graph theory, and quantum physics, with potential application to other recurrent network architectures like LSTM. The proposed notion of Start-End separation rank can be applied to different variants of LSTM networks to quantify their memory capacity. Experimental findings suggest that shallow layers of recurrent networks are related to short time-scales, while deeper layers support correlations of longer time-scales. This opens the door to further investigations on the role of each layer in modeling temporal correlations in recurrent networks. The findings suggest that deeper layers of recurrent networks support correlations of longer time-scales, leading to further investigations on the role of each layer in modeling temporal correlations. Theoretical observations by Levine et al. (2017) provide practical conclusions on choosing hidden channels for deep convolutional networks. The Start-End separation rank of recurrent networks grows exponentially with depth, offering insights into enhancing memory capacity. This work aims to match recurrent network architecture with temporal correlations in sequential data sets. In section A.1, a brief introduction to Tensor Networks (TNs) is provided. Section A.2 discusses TNs for calculating shallow Recurrent Autoencoders (RACs) using Matrix Product State (MPS) architecture. Section A.3 presents TN construction for deep RACs, highlighting their ability to model complex temporal dependencies. Finally, in section A.4, the construction of TNs is used to support the conjecture that the Start-End separation rank of RACs grows exponentially with depth. TNs are weighted graphs where nodes represent tensors, and edges represent tensor modes. The edges of a node in a Tensor Network (TN) represent different modes of the corresponding tensor, with each edge's weight equal to the dimension of the tensor mode. Each edge is represented by an index running between 1 and its bond dimension. TNs show connectivity properties through edges connecting nodes, representing operations between tensors. Contracted indices involve summation over all possible values, while open indices indicate the order of the tensor represented by the TN. In Tensor Networks, contracted indices involve summation over all values, while open indices indicate the tensor's order. The entire TN can be calculated by summing over contracted indices. Contraction in TNs is a generalization of matrix multiplication, as shown in FIG4. The computation in a shallow recurrent network can be written in terms of a TN, as depicted in FIG5, representing a temporal concatenation of unit cells. The computation in a shallow recurrent network can be represented by a Tensor Network (TN), which is essentially a temporal concatenation of unit cells. Each unit cell consists of input weights, hidden weights, and a triangular tensor \u03b4. The recursive relation defined by the unit cell is shown in the TN. The Tensor Network (TN) represents the operation of the shallow Recurrent Associative Classifier (RAC). The recursive relation in the TN defines the operation of the shallow RAC, with the restricted \u03b4 tensor enabling element-wise multiplication. After T repetitions of the unit cell calculation, the output vector y T,1,\u0398 is obtained by multiplying the hidden state vector h T with the output weights matrix W O. The TN representing the shallow RAC weights tensor A T,1,\u0398 c can be drawn by a simple contraction of indices. The Tensor Network (TN) representing the shallow Recurrent Associative Classifier (RAC) weights tensor A T,1,\u0398 c can be drawn in the form of a standard MPS TN, allowing for a linear amount of parameters in the Tensor Train (TT) decomposition. This TN enables the min-cut analysis for quantifying information flow in the shallow recurrent network. The computation of a deep recurrent network in TN language is more complex than that of the shallow case. The construction of a Tensor Network (TN) representing the computation of a deep recurrent network is more complex than that of the shallow case due to the reuse of hidden states in each layer. Duplicating a vector and sending it to be part of two different calculations, a common practice in deep recurrent networks, is impossible to represent in the framework of TNs. This limitation is formulated in Claim 3, stating that duplicating a node in TN notation cannot be achieved. Claim 3 states that duplicating a node in Tensor Network notation is impossible, posing a challenge in representing deep recurrent networks. This limitation can be overcome by a simple 'trick' to model the network. To represent a deep recurrent network in Tensor Network notation, a cloning operation is avoided by setting v = 1. A workaround involves duplicating the input data itself to model the inherent duplication in the network computation. This technique allows for the construction of deep RACs in TNs, depicted in FIG6. These TNs grow exponentially in size as the network depth increases, serving as a theoretical tool for analysis. The actual network is implemented using a simpler scheme shown in fig. 1(b), which grows linearly in size despite the TN's exponential growth. The deep recurrent network is represented in Tensor Network notation without cloning operations by duplicating input data. This allows for modeling intricate correlations over longer periods of time. The network's size grows linearly despite the exponential growth of Tensor Networks. In a depth L = 2 recurrent network, the input duplication process is explained through the calculation of hidden state vectors. The deep recurrent network is represented in Tensor Network notation without cloning operations by duplicating input data, allowing for modeling intricate correlations over longer periods of time. The network's size grows linearly despite the exponential growth of Tensor Networks, with a fractal structure involving many self similarities in deeper layers. The deep recurrent network, represented in Tensor Network notation, shows a fractal structure with self-similarities in deeper layers. The complexity of the network increases with depth, allowing for modeling long-term temporal dependencies. The construction of Tensor Networks corresponding to deep RACs motivates a combinatorial lower bound on the Start-End separation rank. This analysis, supported by TN visualizations, formalizes the network's ability to capture intricate correlations over extended periods. The analysis employs TN visualizations to formalize the network's ability to capture correlations. Conjecture 1 relies on finding specific network parameters to establish a lower bound on the Start-End separation rank. Claim 2 and lemma 1 are combined to show that the rank of the matrix obtained by grid tensor matricization provides a lower bound. Lemma 1 states that finding a single example where the rank exceeds the lower bound implies the desired inequality holds for most parameter values. The analysis uses TN visualizations to formalize the network's ability to capture correlations by finding specific network parameters to establish a lower bound on the Start-End separation rank. By choosing a weight assignment that separates the first layer from higher layers, the computation in deeper layers only contributes a constant factor to the matricized grid tensor. The example of the TN corresponding to an RAC of depth L = 3 after T = 6 time-steps is shown in FIG3 and FIG9. Graph segments involving only indices from the \"Start\" set do not affect the rank of the matrix under mild conditions on W I,1 and W H,1. The analysis uses TN visualizations to establish a lower bound on the Start-End separation rank in a network. Graph segments involving only \"Start\" set indices do not affect the matrix rank under certain conditions. The effective TN after T = 6 time-steps in an RAC of depth L = 3 is shown in FIG9, with the number of repetitions of the basic unit increasing exponentially with depth. The number of repetitions of the basic unit in the TN graph increases exponentially with the depth of the RAC. Claim 4 states that the computation after T time-steps by an RAC with L layers and R hidden channels per layer involves the function computing the output. The tensor V reflects the contribution of the \"Start\" set indices, and the chain of products forms an order t2 tensor representing the computation of a depth L = 1 RAC after t2 time-steps. The tensor V reflects the contribution of the \"Start\" set indices in the computation of a depth L = 1 RAC after t2 time-steps. The number of repetitions of the basic unit in the TN graph increases exponentially with the depth of the RAC, and the lower bound presented in conjecture 1 is obtained by considering a rank R matrix raised to the Hadamard. The lower bound presented in conjecture 1 is obtained by considering a rank R matrix raised to the Hadamard. The proof strategy outlined in section 4 shows an exponential advantage of deep recurrent networks over shallow ones in modeling long-term dependencies. The bounds on the Start-End separation rank of shallow and deep RACs are proven in sections B.3.1 and B.3.2. The text discusses the bounds on the Start-End separation rank of shallow and deep RACs, with a focus on the Tensor Network construction of a shallow RAC. The construction involves a Matrix Product State (MPS) Tensor Network with specific tensor building blocks. The rank of matrices obtained from matricizing tensors is related to a min-cut separating different parts of the Tensor Network graph. The text discusses the bounds on the Start-End separation rank of shallow and deep RACs using a Tensor Network construction. The minimal cut in the MPS Tensor Network is related to the bond dimension R, with exceptions for larger values. Claim 2 states that the rank of the function in a depth L = 2 RAC is lower bounded by the matrix rank from tensor matricization. An assignment of weight matrices and initial hidden states can achieve the desired rank for all configurations of the recurrent network. The text discusses achieving the desired rank for all configurations of the recurrent network by providing an assignment of weight matrices and initial hidden states. This assignment ensures the rank is achieved for all configurations with exceptions for larger values. The text discusses setting weight matrices and initial hidden states to achieve the desired rank for all configurations of the recurrent network. The output for a specific class after T time-steps is calculated under this assignment. Grid tensor evaluation involves substituting fj(x(i)) \u2261 Fij. The grid tensor is defined with Z such that Zrd = 0 for r \u2265 min{R, M}. The tensor is split into two parts mapped to a vector a and a matrix B under matricization w.r.t. the Start-End partition. The text discusses matricization w.r.t. the Start-End partition, where the left part is mapped to a vector a with non-zero entries, and the right part is mapped to a matrix B. It is shown that B can be written as a sum of N rank-1 matrices. The accumulated reward of the optimal strategy is also mentioned in relation to the trajectory. The text discusses the optimal strategy's accumulated reward and trajectory in relation to matricization. It proves the existence of a value \u2126 for maximal reward in a sequence of colors. A claim regarding the maximal matrix rank for matrices with polynomial functions is quoted, showing the importance of specific weight assignments for grid tensor matricization. The text proves the existence of a value \u2126 for maximal reward in a sequence of colors and discusses the importance of specific weight assignments for grid tensor matricization. It shows that if a matrix with polynomial entries has a single contributor with the highest degree of x, it is fully ranked for all values of x except for a finite set. The text discusses the importance of specific weight assignments for grid tensor matricization and proves that a matrix with polynomial entries is fully ranked for all values of x except for a finite set. This is shown by demonstrating that the determinant of the matrix is not the zero polynomial. The text discusses the vector rearrangement inequality and its application in proving the full rank of a matrix denoted \u016a. This inequality ensures that the matrix meets certain conditions and is fully ranked, as shown through a specific identity. The vector rearrangement inequality in lemma 2 guarantees that the matrix \u016a satisfies the conditions of lemma 1 and is fully ranked. An identity is presented to simplify a complex expression, showing a contribution less than \u03c1*."
}