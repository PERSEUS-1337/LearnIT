{
    "title": "SJg9z6VFDr",
    "content": "Recently, a new model called graph ordinary differential equation (GODE) has been proposed for graph data, inspired by neural ordinary differential equation (NODE) for Euclidean data. GODE uses continuous-depth models and two efficient training methods. It outperforms existing graph networks and can be easily adapted to different graph neural networks, improving accuracy in various tasks. Our GODE model, inspired by NODE for Euclidean data, outperforms existing graph networks in accuracy for irregularly structured datasets. Graphs represent objects as nodes and relations as edges, widely used in social networks, protein interactions, and knowledge graphs. Traditional methods like random walk are used in early works. Graph neural networks (GNN) are a new class of models that generalize convolution operations to graphs, inspired by the success of CNNs. There are two main methods for performing convolution on a graph: spectral methods and non-spectral methods. Spectral methods compute the graph Laplacian and filter in the spectral domain, while non-spectral methods directly perform convolution in the graph domain by aggregating information from neighboring nodes. Graph neural networks (GNN) aim to generalize convolution operations to graphs. Spectral methods compute the graph Laplacian for filters, while non-spectral methods perform convolution in the graph domain by aggregating information from neighboring nodes. Existing GNN models have discrete layers, making it challenging to model continuous diffusion processes. The neural ordinary differential equation (NODE) extends to graphs as graph ordinary differential equations (GODE), modeling message propagation as an ODE. NODEs offer adaptive evaluation and accuracy-speed control. In this work, a memory-efficient framework is proposed to improve gradient estimation during training of graph ordinary differential equations (GODE). The framework enhances the performance of NODEs in benchmark image classification tasks, reducing test error from 19% to 5% on CIFAR10. The proposed framework improves gradient estimation for free-form NODEs, enhancing performance in benchmark tasks like image classification on CIFAR10. It is memory-efficient and extends to graph data with GODE models, demonstrating improved performance on various datasets. Previous studies have explored neural networks as differential equations, with NODE treating the network as a continuous ODE. The NODE framework treats neural networks as continuous ODEs and has been used in generative models. Studies have focused on training NODE, with the adjoint method being widely used. GNNs can be categorized into spectral and non-spectral methods, with non-spectral GNNs being localized and requiring less computation. Spectral methods like graph convolution in the Fourier domain have also been introduced. Non-spectral GNNs focus on message aggregation around neighbor nodes, requiring less computation. Spectral methods like graph convolution in the Fourier domain have heavy computation burdens. Various approaches have been proposed to localize spectral filters and accelerate running speed. Non-spectral methods define convolution operations on graphs by considering only neighboring nodes. MoNet uses a mixture of CNNs to generalize convolution on graphs. Defferrard et al. (2016) introduced fast localized spectral filtering on graphs. MoNet (Monti, 2017) utilizes a mix of CNNs for graph convolution. GraphSAGE (Hamilton et al., 2017) samples fixed neighbors for localized inference. Graph attention networks (Veli\u010dkovi\u0107 et al., 2017) learn varying weights for node neighbors. Invertible blocks are used in normalizing flow models for accurate data reconstruction. Jacobsen et al. (2018) applied bijective blocks in building invertible networks. Invertible blocks are utilized in normalizing flow models for accurate data reconstruction. Jacobsen et al. (2018) used bijective blocks to construct invertible networks, enabling backpropagation without storing activation. This approach discards activation of middle layers as each layer's activation can be reconstructed from the next layer with invertible blocks. The discrete-layer models with residual connections can be represented as neural ordinary differential equations (NODE) when adding more layers with shared weights. In the discrete-layer models with residual connections, the forward pass of a NODE is defined with z(0) as input and T as the integration time. The transformation of states z is modeled as the solution to the NODE, with an output layer applied on z(T). Integration in the forward pass can be done with various ODE solvers, and the adjoint method is commonly used in optimal process control and functional analysis. Model parameters are denoted as \u03b8, which are independent of time. The adjoint method, commonly used in optimal process control and functional analysis, involves denoting model parameters as \u03b8. It compares two methods for back-propagation on NODE, showing the challenges of reverse-time solutions and the benefits of direct back-propagation through the ODE solver. This method ensures accurate gradient computation by reconstructing the hidden state. During forward pass, evaluation time points are saved and during backward pass, the computation graph is rebuilt by evaluating at the same time points to accurately reconstruct the hidden state for precise gradient evaluation. Gradient descent is then performed to optimize \u03b8 and minimize the loss function. The reverse-time integration in Eq. 6 can be solved with any ODE solver, but storing z(t) during forward pass requires large memory consumption. In summary, forward pass solves Eq. 2 forward in time, while backward pass solves Eq. 2 and 6 reverse in time with initial condition from Eq. 5 at time T. The reverse-time ODE solver can lead to inaccurate gradients in adjoint methods. The reverse-time ODE solver can cause inaccurate gradients in adjoint methods due to the instability of the reverse-time ODE, leading to errors in the gradient calculation. Proposition 1 states that if the Jacobian of the original system has eigenvalues with non-zero real parts, either the forward-time or reverse-time ODE is unstable. This instability can result in sensitivity to numerical errors, affecting the accuracy of the gradient calculation. The instability of eigenvalues with non-zero real parts can lead to inaccurate gradients in adjoint methods when using the reverse-time ODE solver. To address this issue, a direct back-propagation method is proposed, ensuring accurate hidden states and gradient computations. The direct back-propagation method ensures accurate hidden states and gradient computations by evaluating the model at the same time points in forward-time. Adjoint for discrete forward-time ODE solution is defined, and Eq. 7 is a numerical discretization of Eq. 6. Detailed derivations are in appendix E and F. Algorithm 1 outlines accurate gradient estimation in ODE solver for free-form functions. During forward pass, the solver integrates numerically with adaptive stepsize based on error estimation and outputs the integrated value and evaluation time points. All middle activations are deleted to save memory. During backward pass, the solver rebuilds the computation graph by directly evaluating at saved time points without adaptive searching, performing a numerical reverse-time integration. The algorithm supports free-form continuous dynamics with no constraints on the form of f, making it a generic method. Memory consumption analysis is also discussed. The algorithm discussed in the previous section supports free-form continuous dynamics without constraints on the form of f, making it a generic method. Memory consumption analysis reveals that the method is more memory-efficient compared to a naive solver, especially when using invertible blocks, reducing memory consumption to O(N f ). The algorithm discussed in the previous section supports free-form continuous dynamics without constraints on the form of f. By using invertible blocks, memory consumption can be reduced to O(N f), making it more memory-efficient. In this case, z(t i ) does not need to be stored, and input x can be split into two parts for bijective mapping.\u03c8 functions can be applied for different tasks, eliminating the need to store activations and further improving memory efficiency. The algorithm discussed in the previous section supports free-form continuous dynamics without constraints on the form of f. By using invertible blocks, memory consumption can be reduced, making it more memory-efficient. The block defined by Eq. 8 is a bijective mapping when \u03b2 is given. Different \u03c8 functions can be applied for various tasks, eliminating the need to store activations. The proof of Theorem 1 is provided in appendix D. The details for back-propagation without storing activation are in appendix B. Graph neural networks with discrete layers are introduced first, followed by an extension to the continuous case with graph ordinary differential equations (GODE). GNNs can generally be represented in a message passing scheme, where differentiable functions parameterized by neural networks are used. A GNN can be viewed as a 3-stage model: message passing, message aggregation, and update. Message passing involves neighbor nodes sending information to a specific node using a neural network function. Message aggregation combines messages from neighbors using permutation invariant operations. The node's state is then updated based on its original state and the aggregated messages. A discrete-time GNN can be converted to a continuous-time GNN using graph ordinary differential equations (GODE), which can capture highly non-linear functions. Graph Ordinary Differential Equation (GODE) combines message passing with a continuous smoothing process, demonstrating asymptotic stability related to over-smoothing phenomena. GODE can outperform discrete-layer counterparts by capturing highly non-linear functions. Graph convolution is a special case of Laplacian smoothing, showing that the ODE is asymptotically stable with only negative eigenvalues. In experiments, the ODE is shown to be asymptotically stable with negative eigenvalues, leading to nodes having similar features over time. The method was evaluated on image and graph classification tasks without pre-processing, showing promising results on various benchmark datasets. The study evaluated the ODE method on image and graph classification tasks without pre-processing, showing promising results on various benchmark datasets. The datasets used included Cora, CiteSeer, and PubMed for graph classification tasks, and a ResNet18 model was modified for image classification tasks. GODE can be easily applied to existing GNN architectures like GCN and GAT by replacing specific structures with free-form functions. The study demonstrated the generalizability of GODE to existing GNN architectures such as GCN, GAT, ChebNet, and GIN. Different depths of layers were trained for fair comparison, with consistent hyper-parameters like channel numbers. Various experiments were conducted on graph and node classification tasks with different GNN structures and hidden layer configurations. The study compared different GNN structures for node classification tasks, varying channel numbers and number of hidden layers. Direct back-propagation outperformed the adjoint method, showing instability in the reverse-time ODE. NODE18, a modified ResNet18, achieved lower error rates than the adjoint method on image classification tasks. Our training method for NODE18 reduces error rates on image classification tasks compared to the adjoint method. NODE18 outperforms deeper networks like ResNet101 on CIFAR10 and CIFAR100 with the same number of parameters as ResNet18. The method also shows robustness to different orders of ODE solvers and supports free-form functions in NODE and GODE models. Our method supports NODE and GODE models with free-form functions, demonstrating the effectiveness of GODE models over their discrete-layer counterparts. Different \u03c8 functions behaved similarly on node classification tasks, indicating the importance of the continuous-time model. Results for different models on graph classification tasks are summarized in Table 4, showing lower memory cost and outperformance of GODE models. The importance of coupling function \u03c8 is highlighted, along with the validation of lower memory cost in the appendix. Results from various models on graph classification tasks are presented in Table 4, showing the superiority of GODE models. Integration time's impact on NODE and GODE models during inference is discussed in Table 5, emphasizing the need for a balance to avoid accuracy drop. GODE is proposed as a method for modeling continuous diffusion processes on graphs, with a memory-efficient back-propagation technique for NODEs. The paper proposes GODE to model continuous diffusion processes on graphs and introduces a memory-efficient back-propagation method for NODEs. It addresses the gradient estimation problem for NODE and demonstrates superior performance on image classification and graph data tasks. Experiments are conducted on various datasets, including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained with modifications for improved performance. The structure and experiments for the invertible block are detailed in Table 1 and Fig. 1. Modifications include generalizing to bijective blocks with different functions and implementing a parameter state checkpoint method for accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch. Memory efficiency is demonstrated by only keeping necessary variables in the forward function and efficiently calculating gradients in the backward function. The bijective block in the backward function calculates x1, x2 from y1, y2 and gradients, demonstrating memory efficiency. Memory consumption comparison between memory-efficient and conventional methods showed significant differences, with the memory-efficient version showing lower memory usage as the depth of ODE blocks increased. The bijective block only requires O(1) memory as it stores outputs in cache and deletes activations of middle layers. The memory-efficient bijective block in the backward function demonstrates memory efficiency by calculating x1, x2 from y1, y2 and gradients. It only requires O(1) memory as it stores outputs in cache and deletes activations of middle layers. The algorithm for memory-efficient bijective blocks deletes computation graphs generated by F and G. The stability of an ODE in both forward-time and reverse-time is determined by the eigenvalues of the Jacobian of f. The stability of an ODE in both forward-time and reverse-time is determined by the eigenvalues of the Jacobian of f. For a bijective block, the forward and reverse mappings need to be defined. The proof for Theorem 1 shows that the mapping is both injective and surjective, making it bijective. The text discusses the bijective property of a mapping, demonstrating injectivity and surjectivity. It also delves into deriving gradients in a neural-ODE model and extends from continuous to discrete cases. The setup involves defining a continuous model following an ODE with differentiable functions. The text discusses the optimization problem in a neural-ODE model, using the Lagrangian Multiplier Method to solve it. The Karush-Kuhn-Tucker (KKT) conditions are necessary for optimality, and the derivative with respect to \u03bb is derived at the optimal point. The KKT conditions are necessary for optimality in the Lagrangian Multiplier Method. Derivatives are derived with respect to \u03bb at the optimal point, considering continuous and differentiable perturbations. The analysis transitions from continuous to discrete cases, replacing integration with finite sums."
}