{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as domain knowledge representation. However, ontology information is often overlooked in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, showing improved performance by incorporating ontological information. Ontologies provide a formal representation of domain knowledge through categories and relationships, aiding in structuring training data and neural network architecture for sound event classification. Ontologies are essential for sound event classifiers as they provide a formal representation of domain knowledge through categories and relationships. Hierarchical relations in classifiers allow for back-off to more general categories and disambiguation of classes. Various datasets like ESC-50, UrbanSounds, and DCASE are examples of ontologies used in sound event classification. Hierarchical relations in sound event classifiers offer benefits such as back-off to general categories, disambiguation of acoustically similar classes, and penalizing classification differently. Ontology-based network architectures have shown improved performance in sound event classification, with examples like a deep restricted Boltzmann machine and perceptrons for each node in the hierarchy. Authors proposed ontology-based networks to model the transformation from a super class to its sub classes, showing improved performance and reduced overfitting. They used a perceptron for each node in the hierarchy, leading to better class disambiguation. The framework includes assumptions about ontologies, a Feed-forward model with ontological constraints, and ontology-based embeddings computed using Siamese Neural Networks. The proposed framework includes a Feed-forward model with ontological constraints and ontology-based embeddings computed using Siamese Neural Networks. It considers ontologies with two levels and training data with audio representations associated with labels from the ontology. Each possible class is mapped hierarchically, with higher levels indicating higher ontology levels. The proposed framework involves a Feed-forward model with ontological constraints and ontology-based embeddings computed using Siamese Neural Networks. It maps classes hierarchically with higher levels indicating higher ontology levels. Each element in one level is related to an element in the next level. By estimating probabilities using a model, we can infer labels in the second level based on the first level and input data. The proposed framework involves a Feed-forward model with ontological constraints and ontology-based embeddings. It utilizes a base network to generate probabilities for different classes, improving model performance in making predictions. The architecture includes an ontological layer that leverages the ontology structure to map classes hierarchically. The base network in the proposed framework generates two outputs for different ontology levels using an input vector of audio features. The ontological layer reflects the relation between super classes and sub classes in the ontology, with weights defined by an incidence matrix. This layer is used to predict classes in C1 and C2 for any input x. The text chunk discusses the use of an ontological layer with weights defined by an incidence matrix to train a model using a gradient-based method. The model minimizes a loss function that is a combination of two categorical cross-entropy functions. A hyperparameter \u03bb is tuned to adjust the balance between the two functions. The goal is to create embeddings that preserve the ontological structure using a Siamese neural network. The network enforces samples of the same class to be closer while separating samples of different classes based on their superclass relationships. The Siamese neural network (SNN) architecture with the Feed-forward Network with Ontological Layer enforces samples of the same class to be closer, while separating samples of different classes based on their superclass relationships. The network uses ontological embeddings to compute a Similarity metric (Euclidean Distance) to indicate the differences between samples. The study evaluates the sound event classification performance of ontological-based neural network architectures using datasets with two levels of ontology. The dataset, designed for classifying the highest level in its taxonomy, consists of 97 classes at level 1 and 5 classes at level 2. Audio files were sourced from Freesound data for the challenge. The dataset for the challenge aims to classify the highest level in its taxonomy, with 97 classes at level 1 and 5 classes at level 2. Audio files were sourced from Freesound data base, ESC-50 dataset, and Cambridge-MT Multitrack Download Library. The development dataset includes 1500 audio files divided into five categories, each with 300 files. The evaluation dataset consists of 500 audio files, 100 per category, all in a single-channel 44.1 kHz, 16-bit .wav format. The training set is 80%, tuning parameters 10%, and testing 10%. The Urban Sounds -US8K dataset evaluates urban sound classification with a taxonomy containing more nodes than classes. The dataset Urban Sounds -US8K contains 8,732 audio files with 10 classes at level 1 and 4 classes at level 2. The audio files are sourced from Freesound database and are in a single-channel 44.1 kHz, 16-bit .wav format. The dataset is divided into 10 subsets for training and testing using state-of-the-art features for audio representation. The architecture used is a feed-forward multi-layer perceptron network with 4 layers. The architecture used in the experiment is a feed-forward multi-layer perceptron network with 4 layers. It includes an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. The dense layers utilize Batch Normalization, a dropout rate of 0.5, and the ReLU activation function. Parameters were tuned in the network and for transforming the output vector. Baseline models were considered for different data sets at levels 1 and 2, without ontological information. The architecture used in the experiment is a feed-forward multi-layer perceptron network with 4 layers. It includes an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. Baseline models were considered for different data sets at levels 1 and 2, without ontological information. The addition of an output layer for level 1 or level 2 affects the model's performance, with different values of \u03bb showing improvements in accuracy. In the MSoS dataset, the best performance was achieved with \u03bb = 0.8, reaching 0.74 and 0.913 accuracy in levels 1 and 2 respectively. Using ontological information with different values of \u03bb improves classification performance. In the MSoS dataset, \u03bb = 0.8 resulted in 5.4% and 6% accuracy improvement in levels 1 and 2 respectively. In the US8K dataset, \u03bb = 0.7 led to a smaller improvement of 2.5% and 0.2% in levels 1 and 2. Ontology-based embeddings showed tighter and better-defined clusters in t-SNE plots. The ontology-based embeddings resulted in tighter and better defined clusters, improving sound event classification performance. The Siamese neural network was trained with Walnet audio features using different super and sub class pairs. The SNN was trained for 50 epochs with 100,000 pairs yielding the best performance. The embeddings were passed to the base network architecture and tuned for optimal results. The study utilized different numbers of training data pairs and adjusted lambda values in the loss function to optimize performance. Results showed improved accuracy with the ontology-based embeddings, creating well-clustered groups in level 2 classes. The architecture outperformed the baseline but slightly lagged behind methods without embeddings. T-SNE plots illustrated the grouping benefits of the ontology-based embeddings. The study utilized different numbers of training data pairs and adjusted lambda values in the loss function to optimize performance. Results showed improved accuracy with ontology-based embeddings, creating well-clustered groups in level 2 classes. The Feed-forward Network with Ontological Layer achieved 0.88 accuracy, while using ontological embeddings achieved 0.89, outperforming significantly the baseline. The framework proposed in this paper designs neural networks for sound event classification using hierarchical ontologies. In this paper, a framework was proposed for designing neural networks for sound event classification using hierarchical ontologies. Two methods were presented to incorporate structure into deep learning models without adding more parameters. A Feed-forward Network with an ontological layer and a Siamese neural Network were used to improve classification accuracy. Results showed improved performance over baselines, paving the way for further exploration of ontologies in sound event classification."
}