{
    "title": "rJgUfTEYvH",
    "content": "Generative models for video prediction face the challenge of uncertain futures, with past observations leading to multiple possible outcomes. Current probabilistic models are either computationally expensive or do not optimize data likelihood directly. This work introduces multi-frame video prediction using normalizing flows, enabling direct data likelihood optimization and high-quality stochastic predictions. By modeling latent space dynamics, flow-based generative models show promise in video generation. The advancement in computational hardware and machine learning methods has propelled the field forward. The field of machine learning has seen exponential progress due to advancements in computational hardware and better methods. While current applications are mostly limited to supervised learning with large amounts of data, there is a growing interest in utilizing unlabeled datasets with predictive generative models. These models aim to build an internal representation of the world to effectively predict future events. Predictive generative models utilize large unlabeled datasets to build internal representations of the world for effective future event prediction. This approach is particularly useful in video prediction, where the future is highly uncertain, and a short sequence of present observations can imply multiple potential futures. Such models can be trained on abundant real-world interaction videos to learn about a wide range of phenomena, enabling applications in robotics and decision-making tasks. In video prediction, the future is uncertain, and probabilistic models are studied to represent this uncertainty. This paper focuses on stochastic prediction, specifically conditional video prediction, proposing new models for generating diverse and realistic video frames. The approach extends flow-based generative models into conditional video prediction, aiming to provide exact likelihoods and high-quality results. Our approach extends flow-based generative models into conditional video prediction, addressing the challenges of modeling high-dimensional video sequences. We introduce VideoFlow, a practical architecture inspired by the Glow model for image generation, achieving competitive results in stochastic video prediction on the BAIR dataset. VideoFlow, a model inspired by Glow for image generation, achieves competitive results in stochastic video prediction on the BAIR dataset. It produces high-quality results without common artifacts like blurry predictions, making it practical for real-time applications like robotic control. VideoFlow optimizes likelihood directly, allowing for evaluation based on likelihood values. The research on deterministic predictive models for future video frames focused on architectural changes, pixel transformations, predictive coding architectures, generation objectives, and disentangling representations. The next challenge is to address stochastic environments by building models that can effectively reason over uncertain futures in real-world videos. In real-world videos, stochastic events can lead to uncertain futures. Deterministic models struggle to generate accurate predictions in such cases, leading to blurry outcomes. Various methods, including variational auto-encoders, generative adversarial networks, and autoregressive models, have been developed to incorporate stochasticity and improve future prediction accuracy. Among these, variational autoencoders have been widely explored for optimizing future video frame predictions. The text discusses the exploration of variational autoencoders for optimizing video frame predictions, contrasting them with auto-regressive models. The proposed VAE model is shown to produce better predictions, especially for longer horizons, and exhibits faster sampling while directly optimizing log-likelihood. The proposed VAE model shows faster sampling and better long-term predictions compared to autoregressive models. It uses a multi-scale architecture with stochastic variables and exact latent-variable inference. The proposed generative flow model for video utilizes invertible transformations in a multi-scale architecture to infer latent variables per timestep, enabling exact latent-variable inference and faster sampling compared to autoregressive models. The architecture for g \u03b8 (z t) involves multiple levels of latent variable z t encoding information about frame x t at different scales. Invertible transformations such as Actnorm, Coupling, SoftPermute, and Squeeze are used to compute z = f \u03b8 (x t) efficiently. The architecture involves multiple levels of latent variable encoding information about the frame at different scales. Invertible transformations like Actnorm, Coupling, SoftPermute, and Squeeze are used to efficiently compute the latent variables for each individual frame of the video. The multi-scale architecture described above is used to infer latent variables for each frame of the video. An autoregressive factorization is used for the latent prior, with a conditional prior specified. A deep 3-D residual network is used to predict the mean and log-scale. The architecture involves invertible transformations to compute latent variables efficiently. The architecture described involves dilations, gated activation units, and invertible transformations to predict mean and log-scale. The log-likelihood objective has two parts: log Jacobian determinants for mapping video frames to latent variables, and a latent dynamics model. Parameters are learned by maximizing this objective, with the prior modeling temporal dependencies and the flow acting on separate frames. Realism of generated trajectories is compared with other models, and trajectories are displayed for different shapes. The VideoFlow model uses 2-D convolutions with autoregressive priors to generate long sequences without temporal artifacts. The model conditions on the frame at t = 1 and generates trajectories at t = 2 and t = 3 for different shapes. The use of 3-D convolutions was found to be computationally expensive, leading to memory limitations and changes in input distribution between training and synthesis. The generated videos show blue borders for conditioning frames and red borders for generated frames. The model is applied to the Stochastic Movement Dataset. The VideoFlow model uses 2-D convolutions with autoregressive priors to generate long sequences without temporal artifacts. The model conditions on the frame at t = 1 and generates trajectories at t = 2 and t = 3 for different shapes. The generated videos show blue borders for conditioning frames and red borders for generated frames. The model is applied to the Stochastic Movement Dataset. In the generated videos, a blue border represents the conditioning frame, while a red border represents the generated frames. The shape moves in one of eight directions with constant speed, and the model averages out all directions in pixel space. By looking back at just one frame, the model achieves a low bits-per-pixel of 0.04 on the holdout set. VideoFlow maximizes loglikelihood of the second frame given the first, achieving a low bits-per-pixel of 0.04 on the holdout set. It consistently predicts the future trajectory of shapes in eight random directions. Compared to SV2P and SAVP-VAE, VideoFlow outperforms in generating plausible \"real\" trajectories. The model is evaluated using a real vs fake Amazon Mechanical Turk test, showing higher fooling rates. The BAIR robot pushing dataset is used for unsupervised video generation. The BAIR robot pushing dataset is used for unsupervised video generation. Models like SAVP-VAE, SV2P, and SVG-LP are trained to generate target frames based on input frames. VideoFlow is trained to maximize log-likelihood and achieve low bits-per-pixel. Realism and diversity are measured using a 2AFC test and mean pairwise cosine distance. VideoFlow outperforms SAVP-VAE and SV2P models in bits-per-pixel on the test set. The high values of bits-per-pixel in the baselines are attributed to their optimization objective. Models were trained on 10 target frames but tested to generate 27 frames. The best sample was chosen based on PSNR, SSIM, and VGG perceptual metrics. The BAIR robot-pushing dataset is highly stochastic, and models are evaluated based on PSNR, SSIM, and VGG perceptual metrics to determine the closest generated video to the ground truth. In prior work, researchers effectively tune pixel-level variance as a hyperparameter to improve sample quality and training stability in video models. By removing pixel-level noise, higher quality videos can be generated at the cost of diversity. Sampling videos at a lower temperature can achieve this, as shown in Figure 4. Additionally, low-temperature sampling was applied to latent gaussian priors in other models for improved results. Our model with optimal temperature performs better or as well as the SAVP-VAE and SVG-LP models on VGG-based similarity metrics, which correlate well with human perception and SSIM. The model with temperature T = 1.0 is also competitive with state-of-the-art video generation models on these metrics. PSNR is a pixel-level metric, while VideoFlow models the conditional probability of the joint distribution of frames, resulting in underperformance on PSNR. Diversity and quality in generated samples are also discussed. VideoFlow models the conditional probability of the joint distribution of frames, leading to underperformance on PSNR. Diversity and quality in generated samples are evaluated by generating 10 videos for each set of conditioning frames in the test set and computing the mean distance in VGG perceptual space. VideoFlow outperforms diversity values from prior work and achieves high realism scores at lower temperatures. The state-of-the-art VAE models in diversity were evaluated by generating videos at different temperatures. Lower temperatures resulted in less random behavior and higher realism scores, while higher temperatures led to more stochastic motion and higher diversity scores. Interpolations between different shapes and frames in the BAIR robot pushing dataset showed cohesive motion of the arm. The multi-level latent representation allowed for interpolations at specific levels while keeping others fixed. In a study on VAE models, lower temperatures produced more realistic videos, while higher temperatures resulted in increased diversity. Interpolations in the BAIR robot pushing dataset showed cohesive motion of the arm. The multi-level latent representation allowed for specific level interpolations while keeping others fixed. Shapes with different sizes and colors were smoothly interpolated in the latent space. Colors were sampled from a uniform distribution, and all interpolated colors matched those in the training set. VideoFlow was used to assess the plausibility of future frames, with results displayed in Figure 7. Our VideoFlow model can detect the plausibility of future frames, generating 100 frames ahead with maintained temporal consistency. Despite occlusions, the arm remains sharp while background objects become noisier. The model's bijection between latent state and frame means it can forget occluded objects after a few frames. Future work aims to incorporate longer memory in the model for improved object retention. Our VideoFlow model, conditioned on 3 frames, detects the plausibility of a temporally inconsistent frame occurring in the immediate future. It assigns a decreasing log-likelihood to frames further in the future. The architecture is inspired by the Glow model for image generation and introduces a latent dynamical system model for predicting future flow values. VideoFlow is a flow-based video prediction model inspired by the Glow model for image generation. It introduces a latent dynamical system model for predicting future flow values and achieves competitive results with VAE models in stochastic video prediction. The model optimizes log-likelihood directly for faster synthesis, making it suitable for practical purposes. Future work includes incorporating memory for long-range dependencies and applying the model to challenging tasks. The dataset consists of 8-bit videos with added uniform noise for discretization. VideoFlow is a flow-based video prediction model that optimizes log-likelihood directly for faster synthesis. It introduces a latent dynamical system model and achieves competitive results with VAE models in stochastic video prediction. The dataset consists of 8-bit videos with added uniform noise for discretization. Additive noise is necessary to prevent infinite densities at datapoints, allowing for better optimization of the log-likelihood. Lowering the temperature from 1.0 to 0.0 in the latent gaussian priors of SV2P and SAVP-VAE decreases the performance of VAE models. The VideoFlow model benefits from low-temperature sampling by balancing noise removal from the background and reduced stochasticity of the robot arm. The VideoFlow model gains from low-temperature sampling by balancing noise removal from the background and reducing stochasticity of the robot arm. Lowering the temperature in VAE models decreases performance, while VideoFlow learns to model the arm structure and motion with high quality as bits-per-pixel decreases. Training details include a learning rate schedule, optimization with Adam, and tuning based on VGG cosine similarity metric. The training details for the VideoFlow model include using a learning rate schedule with linear warmup and linear-decay, training for 300K steps with the Adam optimizer, and tuning based on VGG cosine similarity metric. Different values of latent loss multiplier were used, and for the SAVP-VAE model, a linear decay on the learning rate was applied. The SAVP-GAN model had the gan loss multiplier and learning rate tuned on a logscale. Results showed a weak correlation between VGG perceptual metrics and bits-per-pixel. In Figure 13, results for videos in the test set show a weak correlation (-0.51) between VGG perceptual metrics and bits-per-pixel. Evaluations with a smaller VideoFlow model with 4x parameter reduction remain competitive with SVG-LP on VGG metrics."
}