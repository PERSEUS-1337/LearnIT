{
    "title": "rygVV205KQ",
    "content": "High-dimensional sparse reward tasks in reinforcement learning are challenging. This work uses imitation learning to address these challenges, focusing on learning a representation of the world from pixels and exploring efficiently with rare reward signals. Adversarial imitation can work well in high-dimensional observation spaces with a small adversary acting as the reward function. The proposed agent can solve a robot manipulation task using only video demonstrations and sparse rewards, outperforming non-imitating agents and competing approaches. Our agent learns faster than competing approaches with dense reward functions and standard GAIL baselines. A new adversarial goal recognizer allows the agent to learn stacking without task rewards. GAIL can handle high-dimensional pixel observations with a single-layer discriminator network. Using a Deep Distributed Deterministic Policy Gradients (D4PG) agent improves efficiency in environment interactions. The Policy Gradients (D4PG) agent utilizes a replay buffer for control and can successfully use various types of features with a single-layer adversary. The approach can solve a challenging robotic block stacking task from pixels using only demonstrations and a sparse binary reward. This method reduces the reliance on hand-crafted rewards and achieves faster stacking compared to traditional approaches. The paper introduces a 6-DoF Jaco robot arm agent that learns block stacking from demonstration videos and sparse rewards. It also presents an adversary-based early termination method for actor processes to improve task performance and learning speed. Additionally, the agent learns with no task reward using an auxiliary goal recognizer adversary, achieving 55% stacking success from video imitation only. Ablation experiments on Jaco stacking and a 2D planar walker benchmark are conducted to understand the reasons for the agent's improvement. The paper discusses a 6-DoF Jaco robot arm agent learning block stacking from demonstration videos and sparse rewards. It introduces an adversary-based early termination method for actor processes to enhance task performance and learning speed. Ablation experiments on Jaco stacking and a 2D planar walker benchmark are conducted to understand the agent's improvement reasons. The agent uses random projections and value network features for better performance in some cases. The Markov Decision Process (MDP) is defined with states, actions, reward function, transition distribution, and discount. DDPG is an actor-critic method with neural networks representing the actor and critic. Transitions are added to a replay buffer for better exploration. The policy and critic are neural networks with parameters \u03c6 and \u03b8. New transitions are added to a replay buffer by sampling from the policy. The action-value function is trained to match 1-step returns by minimizing the transition sampled from the replay buffer. Target networks are updated every K learning steps for stability. The policy network is trained via gradient descent to maximize the action-value function. D4PG builds on the basic DDPG agent with several improvements. GAIL learns a reward function by training a discriminator network to distinguish between agent and expert transitions. GAIL involves training a discriminator network to distinguish between agent and expert transitions. The objective includes an entropy regularizer and is related to MaxEnt inverse reinforcement learning. A D4PG agent is used for off-policy training with experience replay. The reward function is jointly trained with actor and critic updates. The reward function interpolates imitation reward and sparse task reward. Actions are not used in the discriminator, only expert videos are assumed for access. In GAIL, the reward function combines imitation reward and sparse task reward. Actions are not considered in the discriminator, only expert videos are used. The actor process includes early termination based on discriminator score to prevent drifting from expert trajectories. Multiple CPU actor processes run in parallel with a single GPU learner process. The actor process in GAIL includes early termination based on the discriminator score to prevent drifting from expert trajectories. The type of network used in the discriminator is a critical design choice, as it affects the agent's ability to fool the discriminator. Expert demonstrations provide valuable data for feature learning due to their coverage of the state space. The discriminator architectures studied in this work utilize expert demonstrations as a valuable data source for feature learning. Behavior cloning is not an option due to the lack of access to expert actions. High-resolution images are used, ruling out learning features in pixel space. Contrastive predictive coding (CPC) is chosen as a representation learning technique, allowing for long-term structure learning through a probabilistic contrastive loss. CPC is a representation learning technique that enables long-term predictions by mapping observations into a latent space. It uses a probabilistic contrastive loss with negative sampling, allowing joint training of the encoder and autoregressive model without the need for a decoder. To address issues with sparse rewards, a neural network goal recognizer can replace traditional rewards, but a discriminator is needed to detect goal states to prevent exploitation by the agent. To address sparse rewards, a secondary goal discriminator network can replace traditional rewards by detecting goal states in the latter 1/M proportion of expert demonstrations. This modified reward function allows agents to surpass the demonstrator by learning to reach the goal faster, as observed in combined imitation and sparse task rewards training. The environment includes a Kinova Jaco arm with 9 degrees of freedom and two blocks on a tabletop. Observations are 128x128 RGB images, and hand-crafted reward functions are used. Demonstrations are collected using a SpaceNavigator 3D motion controller, with 500 episodes gathered for each task. Additional trajectories were collected for validation and CPC diagnostics. The curr_chunk discusses the collection of trajectories for validation and CPC diagnostics using different behaviors in two environments. It compares an imitation method to D4PG and GAIL agents, showing favorable results. The proposed method utilizes a tiny adversary and models future observations well. The curr_chunk discusses how the CPC model performs well for expert sequences but not for non-expert ones. It also shows that conditioning on k-step predictions improves performance on stacking tasks. Additionally, it compares the learning pace of D4PG with sparse and dense rewards, highlighting the quick learning of imitation methods despite using sparse rewards. The agent using value network features takes off more quickly than with CPC features. The discriminator network has only 128 parameters with CPC features. The discriminator network with CPC features has 128 parameters, while the value network features are 2048-dimensional. GAIL with tiny adversaries on random projections has limited success. Norm clipping in the critic optimizer may explain why GAIL value features work while pixel features do not. Experimenting with CPC temporal predictions and ablation experiments on Jaco stacking are also discussed. In ablation experiments on Jaco stacking, it was found that adding layers to the discriminator network did not improve performance, and early termination was crucial. Even with fewer demonstrations, the agent could learn stacking as effectively as with more demonstrations. A small discriminator on meaningful representations was shown to be advantageous, as a more powerful discriminator led to degraded performance. An early termination criterion was introduced to stop episodes when the discriminator score dropped too low. In ablation experiments on Jaco stacking, it was found that adding layers to the discriminator network did not improve performance. An early termination criterion was crucial for better learning. The model learned slower without early stopping enabled. The episode length was high initially due to the discriminator's inability to distinguish expert from agent trajectories. With more episodes, the agent improved at imitating the expert, leading to longer episodes. Data efficiency was evaluated with 60, 120, 240, and 500 demonstrations, showing good performance even with 60 demonstrations. In the third ablation experiment, the proposed method's data efficiency is evaluated in terms of expert demonstrations. Results show that even with 60 demonstrations, good performance is achieved. Outliers were observed with 120 demonstrations. The proposed method outperformed conventional GAIL on pixels in both Jaco and planar walker experiments. Agents trained without any rewards showed success rates, with the best seed achieving 55%. The curr_chunk discusses the success of an agent in learning without task rewards, achieving a 55% success rate. It compares the efficiency of the agent in stacking to a human teleoperator and mentions the exploitation of rolling a block to appear stacked. The use of expert demonstrations to improve agent performance is highlighted, with a focus on leveraging pixel observations for training. Imitation learning using deep networks focuses on taking actions in environments based on expert demonstrations. Approaches like supervised imitation and one-shot imitation leverage encoder and decoder networks to replicate behaviors. Some methods use attention mechanisms, while others use gradient-based meta learning for one-shot learning of observed behaviors, such as stacking blocks or picking and placing objects into containers. The approach in the curr_chunk focuses on using a gradient-based meta learning approach for one-shot learning of observed behaviors, specifically in picking and placing novel objects into containers. Unlike supervised learning, this method aims for the agent to learn by interacting with the environment. It contrasts with behavior cloning, which requires a large number of demonstrations and limits generalization. Instead, inverse reinforcement learning is proposed, where a reward function is learned from demonstrations and then optimized using reinforcement learning. The curr_chunk discusses various approaches such as deep Q-Learning from demonstration (DQfD), deterministic policy gradients from demonstration (DPGfD), and Generative Adversarial Networks (GAIL) applied to imitation learning. These methods aim to train agents using expert demonstrations and reinforcement learning. Despite advancements, challenges remain in adapting GAIL for high-dimensional input spaces and sparse reward environments. GAIL BID16 applies adversarial learning to imitation, facing challenges in high-dimensional input spaces and sparse rewards. Our contribution involves using minimal adversaries to solve sparse reward tasks. Other work focuses on learning compact representations for imitation learning without actions. BID30 and BID3 learn self-supervised features from third person observations to bridge domain gaps. Unlike them, we use features to learn tasks from all expert trajectories, aiming to generalize all possible initializations of hard exploration tasks. We utilize static self-supervised features like contrastive predictive coding and dynamic value network features. Our approach involves utilizing static self-supervised features like contrastive predictive coding and dynamic value network features to train block stacking agents from sparse rewards on pixels. The stacking accuracy achieved is approximately 15%. Additionally, a visualization of CPC on video data is shown in Figure 9. The model consists of an encoder mapping observations to a latent representation and an autoregressive model summarizing past latents into a context vector. The weights for the bilinear mapping depend on the number of latent steps predicted in the future. By optimizing the loss function, the mutual information between the context and target is maximized, resulting in compact representations. This approach is useful for extracting slow features. Our proposed approach involves model learning via contrastive predictive coding (CPC) to extract slow features. The agent is trained using CPC future predictions without the need to predict in pixel space. Reward functions are modified from previous work, with dense staged rewards defined in five stages and sparse rewards in two stages. The actor and critic share a residual network for training. The actor and critic share a residual network with twenty convolutional layers, instance normalization, and exponential linear units between layers. They use Distributional Q functions instead of a scalar state-action value function, with a categorical representation of Z. The bootstrap target is computed with N-step returns, constructing Z based on sub-sequences. The bootstrap target Z is constructed based on sub-sequences using a categorical representation. A loss function L N is used for training distributional value functions, with cross entropy represented by H. Distributed prioritized experience replay is utilized for increased stability and learning efficiency."
}