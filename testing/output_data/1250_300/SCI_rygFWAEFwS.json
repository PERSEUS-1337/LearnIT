{
    "title": "rygFWAEFwS",
    "content": "We propose SWAP, an algorithm to speed up DNN training using large mini-batches and weight averaging. The resulting models generalize well and are produced faster than with small mini-batches. SGD is commonly used for DNN training, with larger mini-batches allowing for higher learning rates and faster loss reduction. In a distributed setting, multiple nodes can be used. Increasing the mini-batch size with available computational resources allows for more precise gradient estimates, higher learning rates, and larger reductions in training loss. In a distributed setting, multiple nodes can compute gradient estimates simultaneously and produce a consensus estimate through averaging. Training with larger mini-batches requires fewer updates and synchronization events, leading to good scaling behavior. However, there is a maximum batch size that can result in worse generalization performance. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging weights from a set of models. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging weights from a set of models. A new approach called Stochastic Weight Averaging in Parallel (SWAP) accelerates DNN training by utilizing compute resources more efficiently. This method involves generating multiple independent SGD sequences and averaging models from each, resulting in comparable generalization performance to models trained solely with small-batches. SWAP is simple to implement, fast, and produces good results with minor tuning. SWAP is a method to accelerate DNN training by better utilizing compute resources. It achieves generalization performance comparable to small-batch models in a similar time frame to large-batch training. SWAP reduces training times significantly and outperforms the state of the art for CIFAR10, training in 68% of the time of the winning entry in the DAWNBench competition. The impact of training batch size on generalization performance is still unknown. In (Keskar et al., 2016), it is argued that models trained with larger mini-batches may get stuck in sharper global minima, which are sensitive to data variations. However, flatness, measured by the curvature of the loss, can contradict this. Studies by (Dinh et al., 2017) and (Li et al., 2018) show transformations between flat and sharp minimizers. (McCandlish et al., 2018) suggest a critical batch size for maintaining accuracy, with a drop observed for batch sizes exceeding 1k samples in image classification on CIFAR10. Increasing batch size beyond a certain point does not significantly improve signal to noise ratio. In (Hoffer et al., 2017), the authors argue that using a larger batch size implies fewer model updates, impacting weight initialization and generalization performance. Training with large batches for longer times improves generalization but takes more time than small batches. In (Ma et al., 2017), it is shown that for convex functions, there is a critical batch size below which an iteration with a batch size of M is roughly equivalent to M iterations with a batch size of one. In the optimization process, Ma et al. (2017) found that for convex functions in the over-parameterized setting, a critical batch size exists where an iteration with a batch size of M is roughly equivalent to M iterations with a batch size of one. Methods with adaptive batch sizes exist but often require specific datasets or extensive hyper-parameter tuning. Local SGD and Post-local SGD are distributed optimization algorithms that trade off gradient precision with communication costs, resulting in better generalization compared to large-batch training. Stochastic weight averaging (SWA) is a method where models are sampled from later stages of an SGD training run and their weights are averaged to improve generalization. SWA differs from Post-local SGD in the timing of model averaging, with SWA averaging models after tens of thousands of updates compared to Post-local SGD's averaging after at most 32 iterations. This difference suggests that the mechanisms behind the success of SWA and Post-local SGD are distinct in DNN optimization. Stochastic Weight Averaging (SWA) involves averaging models sampled from later stages of an SGD training run to enhance generalization. SWA is effective in various domains such as deep reinforcement learning, semisupervised learning, Bayesian inference, and low-precision training. The SWA algorithm consists of three phases: large mini-batch updates with synchronization in the first phase, independent model refinement with smaller batch size and lower learning rate in the second phase, and averaging weights of resulting models in the last phase. In the final phase of Stochastic Weight Averaging (SWA), models are averaged and new batch-normalization statistics are computed to produce the final output. The training process is terminated before reaching perfect accuracy to prevent optimization from getting stuck. In the second phase, small-batch training is performed independently by each worker, resulting in different models. In the final phase of Stochastic Weight Averaging (SWA), models are averaged and new batch-normalization statistics are computed to produce the final output. During the small-batch phase of SWAP, workers have the same learning rates but diverging testing accuracies due to stochasticity. The averaged model outperforms individual models consistently. During the final phase of Stochastic Weight Averaging (SWA), models are averaged, and new batch-normalization statistics are computed to produce the final output. The error achieved by the test network is visualized on a plane containing the outputs of the algorithm's phases. The training and testing error for the CIFAR10 dataset are plotted, showing that the final model ('SWAP') is closer to the center of the convex basin compared to the outputs of the previous phases. During phase 2 of Stochastic Weight Averaging (SWA), the model traversed to a different side of the basin, with the final model ('SWAP') being closer to the center. The variations in the basin's topology caused 'LB' and 'SGD' points to have higher errors, while 'SWAP' was less affected. The workers 'SGD1', 'SGD2', 'SGD3' were at different sides of the basin, with 'SWAP' closer to the center and experiencing lower testing errors. In (Mandt et al., 2017), authors argue that in later stages of SGD weight iterates behave like an Ornstein Uhlenbeck process. By maintaining a constant learning rate, SGD iterates should reach a stationary distribution similar to a high-dimensional Gaussian centered at the local minimum. The distribution's covariance grows with the learning rate, inversely proportional to batch size, and depends on the Hessian of the mean loss and gradient covariance. Sampling weights from an SGD run will choose weights spread out on the ellipsoid's surface, closer to the center. The authors argue that sampling weights from an SGD run will choose weights spread out on an ellipsoid's surface, closer to the center. By averaging samples from different sides of the basin, faster progress towards the center can be made, as shown in Figure 4. In this section, the authors evaluate the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. They found the best hyper-parameters using grid searches and trained using mini-batch SGD with Nesterov momentum and weight decay. Experiments were conducted on one machine with 8 NVIDIA Tesla V100 GPUs using Horovod for distribution. SWAP phase one involved 4096 samples per batch, terminated at 98% training accuracy. The experiments involved using different batch sizes and GPU configurations for training models on CIFAR10, CIFAR100, and ImageNet datasets. Results showed improvements in test accuracies with SWAP compared to using only small or large batches. Using SWAP with different batch sizes and GPU configurations resulted in significant improvements in test accuracies for CIFAR10 and CIFAR100 datasets. SWAP achieved accuracies comparable to small batch training but terminated in a time similar to large-batch runs. Additionally, SWAP was able to train CIFAR10 to 94% test accuracy in 27 seconds using 8 Tesla V100 GPUs, outperforming the front-runner in the DAWNBench competition. Using SWAP with different batch sizes and GPU configurations resulted in significant improvements in test accuracies for CIFAR10 and CIFAR100 datasets. SWAP achieved accuracies comparable to small batch training but terminated in a time similar to large-batch runs. The experiments involved training ImageNet with modified schedules on Tesla V100 GPUs, showing that doubling the batch size reduced accuracies initially, but SWAP recovered generalization performance with reduced training times. The results in Table 3 show significant improvements in test accuracies for CIFAR10 and CIFAR100 datasets using SWAP with different batch sizes and GPU configurations. Accelerations were achieved without tuning other than adjusting learning rates proportionally to batch size increases. SWAP compared with SWA using the CIFAR100 dataset, showing similar accuracies with different training approaches. The study explores the use of SWA and SWAP models with different batch sizes and GPU configurations, showing significant improvements in test accuracies for CIFAR10 and CIFAR100 datasets. SWA samples models with 10 epochs in-between, while SWAP runs 8 independent workers for 10 epochs each. SWA was unable to improve the lower training accuracy of large-batch training runs, but SWA using small-batches after a large-batch training run was able to reach the test accuracy of a small-batch run. The study compares SWA and SWAP models on CIFAR10 and CIFAR100 datasets, showing improvements in test accuracies. SWA uses small-batches after large-batch training to reach small-batch test accuracy. SWA achieves better accuracy than SWAP with longer training time. SWAP aims to speed up SWA precision by relaxing constraints. SWAP achieves a test accuracy of 79.11% in 241 seconds, which is 3.5x faster than SWA. SWAP is an algorithm that uses a variant of SWA to improve model generalization performance by refining weights from models trained with large mini-batches. This approach allows for quicker training and good generalization performance. The variant of SWA, known as SWAP, achieves good generalization performance in a shorter training time by refining weights from models trained with large mini-batches. This method shows that models can perform as well as those trained with small-batches only, confirmed on datasets like CIFAR10, CIFAR100, and ImageNet. Visualizations suggest that averaged weights are closer to the center of a training loss basin, indicating a possible connection between regions of bad and good generalization performance. The SWAP method achieves good generalization performance by refining weights from models trained with large mini-batches. It is possible that regions with varying generalization performance are connected through regions of low training loss. The method requires choosing a transition point between large-batch and small-batch training, which can be determined through grid search. Future work will focus on a principled method for selecting this transition point and exploring SWAP with other optimization schemes. The hyperparameters used in the experiments of Section 5.1 were obtained through independent grid searches. For CIFAR experiments, momentum and weight decay constants were set at 0.9 and 5 \u00d7 10 \u22124 respectively. Tables 5 and 6 detail the remaining hyperparameters, with a stopping accuracy of 100% indicating maximum epochs were used."
}