{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters and improved performance. This approach, known as \"coupled ensembles\", involves tighter coupling of branches by averaging their log-probabilities. The method is applicable to various neural network architectures and shows promising results on tasks like CIFAR-10, CIFAR-100, and SVHN. The study explores network architecture with coupled ensembles of DenseNet-BC, achieving low error rates on CIFAR-10, CIFAR-100, and SVHN tasks. The design of early convolutional architectures involved hyper-parameter choices, evolving towards a template with fixed filter size and skip-connections in models like ResNet and DenseNet. The work extends this template by adding another element. The study extends the template of network architecture with coupled ensembles, achieving low error rates on CIFAR and SVHN datasets. The proposed approach splits the network into branches, each functioning like a complete CNN, and combines their activations through arithmetic mean for improved performance with reduced parameters. Further ensembling of coupled ensembles leads to additional improvements. The paper introduces coupled ensemble networks, which significantly improve convolutional network performance on CIFAR and SVHN datasets with reduced parameters. The network architecture is similar to previous models but differs in training a single model composed of branches and having a fixed parameter budget for the entire model. Further ensembling of coupled ensembles leads to additional improvements. The paper introduces coupled ensemble networks, which improve convolutional network performance on CIFAR and SVHN datasets by training a single model composed of branches with a fixed parameter budget. Multi-branch architectures have been successful in vision applications, with modifications using grouped convolutions for spatial and depth-wise feature extraction. The paper introduces a generic modification of CNN structure at the global model level, using a template of \"element blocks\" replicated as parallel branches. This approach aims to improve performance without the need for extensive training epochs or batch size dependency. Previous studies have explored parallel paths in ResNet for information exchange between layers, but required local modifications at a residual level. Our method involves rearranging a given architecture's parameters without introducing additional choices, leading to efficient parameter usage. Ensembling neural networks can improve performance by combining outputs from multiple trainings of the same architecture. Our model architecture consists of parallel branches, not independent networks, resulting in a single trained model. The proposed model architecture involves rearranging parameters into parallel branches for efficient parameter usage. This approach is similar to ResNet and ResNeXt but at the global network level. Ensembling can still be applied for fusion of independently trained models, leading to performance improvement. Snapshot ensembles at BID10 and BID16 use checkpoints during training for higher performance with the same time budget, but increase model size and prediction time. The goal is to maintain model size while improving performance. The proposed model architecture involves rearranging parameters into parallel branches for efficient parameter usage. This approach aims to maintain model size while improving performance. The model comprises several branches, each using an element block like DenseNet-BC or ResNet. The branches are combined using a fuse layer, with different operations explored in Section 4.4. The model architecture involves parallel branches with element blocks like DenseNet-BC or ResNet, combined using a fuse layer. Different operations for the fuse layer are explored in Section 4.4. The model outputs a score vector for classification tasks, followed by a fully connected layer and SoftMax layer for probability distribution. The differences among recent network architectures lie in the setup before the last FC layer. The differences among recent network architectures for image classification lie in the setup before the last FC layer. Ensemble models are created by independently training instances and averaging their predictions, equivalent to a \"super-network\" with parallel branches. Implementing an averaging layer after the last FC layer and before the SoftMax layer is another approach. In our model setup, parallel branches produce score vectors for target categories. These vectors are fused through a \"fuse layer\" during training to create a single prediction. Three options for combining score vectors are explored: FC average, LSM average, and LL average. Averaging log probabilities of target categories from multiple branches leads to improved performance. The proposed architecture involves multiple branches producing score vectors for target categories, which are fused through a \"fuse layer\" by averaging log probabilities. This leads to improved performance with a lower parameter count in experiments on CIFAR-10, CIFAR-100, and SVHN datasets. All hyperparameters are set according to the original descriptions of the \"element block\" used. The study involves experiments on CIFAR-10, CIFAR-100, and SVHN datasets with hyperparameters set according to the original descriptions. Data augmentation is used for CIFAR datasets, while SVHN does not use data augmentation but applies a dropout ratio of 0.2 for DenseNet. Testing is done after normalizing input, and error rates are given in percentages. DenseNet-BC's PyTorch implementation is used, and execution times are measured using a single NVIDIA 1080Ti GPU. Experiments in sections 4.3 and 4.4 are conducted on the CIFAR-100 dataset. For DenseNet-BC, the PyTorch implementation by BID0 was used, and experiments were conducted on the CIFAR-100 dataset with a DenseNet-BC configuration. Results show that a jointly trained branched configuration outperformed averaging predictions from independent models with the same number of parameters. The study compared a jointly trained branched configuration with a single branch model of similar parameters, showing lower test error for the branched model. The branched configuration was found to be more efficient in terms of parameters, outperforming single branch models. Additionally, the study analyzed the relationship between the number of branches and model performance, evaluating different choices for the \"fuse layer\" in the proposed branched model. In this section, the performance of a branched model with different \"fuse layer\" combinations is compared to a single branch model. The study analyzes the relationship between the number of branches and model performance, evaluating various choices for the \"fuse layer\" in the proposed branched model. The performance of branched models with different \"fuse layer\" combinations is compared to a single branch model. The study shows that coupling \"element blocks\" in ensembles with LSM fusion leads to lower error rates compared to training them separately. This indicates that coupling forces them to learn complementary features and improve performance. The coupling of \"element blocks\" in ensembles with LSM fusion leads to lower error rates compared to training them separately. Averaging the log probabilities helps update all branches consistently, resulting in stronger gradient signals. Ensemble combinations outperform single branch networks, with a reduction in error rate from 20.01 to 17.61 using 4 branches within a parameter budget of 3.2M. However, training with Avg. FC does not perform well as it may reach a similar FC average with unrelated FC instances. The optimal number of branches for a given model parameter budget is investigated in this section. Using 4 branches reduces the error rate to 17.61. Training with Avg. FC does not perform well due to unrelated FC instances, while Avg. FC prediction works better than Avg. SM prediction. The experiments are conducted on CIFAR-100 with DenseNet-BC as the \"element block\" and a parameter budget of 0.8M. The optimal number of branches for a given model parameter budget is investigated using DenseNet-BC on CIFAR-100 with a parameter budget of 0.8M. The results are shown in table 3, with different configurations of branches, depth, and growth rate. DenseNet-BC parameter counts are quantified based on the depth and growth rate values, making it critical for moderate size models like the 800K one targeted here. Model configurations with parameters just below the target were selected for fair comparison. The optimal number of branches for DenseNet-BC on CIFAR-100 with 800k parameters is investigated. Using 2 to 4 branches shows significant performance gains over the single branch case. However, using 6 or 8 branches performs worse. Model performance is robust to slight variations in parameters, but increased performance comes with longer training and prediction times. The DenseNet-BC architecture shows robust performance with 2 to 4 branches on CIFAR-100. However, using 6 or 8 branches leads to worse performance. Coupled ensembles with ResNet pre-act as element block are evaluated, showing gains in performance but increased training and prediction times. The DenseNet-BC architecture performs well with 2 to 4 branches on CIFAR-100. Coupled ensembles with ResNet pre-act as element block show improved performance but longer training and prediction times. Different network sizes were considered, ranging from 0.8M to 25.6M parameters, with experiments showing that the trade-off between depth and growth rate is not critical for a given parameter budget. The study experimented with single-branch and multi-branch versions of the DenseNet-BC model, achieving better performance with coupled ensembles. Larger models of coupled DenseNet-BCs outperformed current state-of-the-art implementations on CIFAR 10, CIFAR 100, and SVHN datasets. Only the Shake-Shake S-S-I model performed slightly better on CIFAR 10. The study compared the performance of coupled ensembles with model architectures learned in a meta-learning scenario. The results showed that larger models of coupled DenseNet-BCs outperformed current state-of-the-art implementations on CIFAR 100 and SVHN datasets. The Shake-Shake S-S-I model performed slightly better on CIFAR 10. The coupled ensemble approach was limited by GPU memory and training time constraints. The classical ensembling approach based on independent trainings was used for models beyond 25M parameters. The ensemble approach with coupled ensemble models showed significant improvements in performance on CIFAR-100. By fusing two models, a significant gain was achieved, but further fusion did not yield much improvement. The ensembles of coupled ensemble networks outperformed all state-of-the-art implementations. The study proposed replacing a single deep convolutional network with \"element blocks\" resembling standalone CNN models. The proposed approach involves replacing a single deep convolutional network with \"element blocks\" that resemble standalone CNN models. These blocks are coupled via a \"fuse layer\" to improve performance. This leads to the best performance for a given parameter budget, as shown in tables 3 and 4, and in figure 2. The individual \"element block\" performance is also better when trained together. The increase in training and prediction times is due to sequential processing of branches during forward and backward passes. The proposed approach involves using \"element blocks\" that are coupled via a \"fuse layer\" to improve performance. Training and prediction times increase due to sequential processing of branches, especially for smaller models. To address this, data parallelism can be extended to branches or spread over multiple GPUs. Preliminary experiments show that coupled ensembles have lower error rates compared to single branch models. Further experiments will be conducted in the future. The proposed approach involves using \"element blocks\" coupled via a \"fuse layer\" to enhance performance. Figure 3 illustrates the structure of test and train network versions, with the possibility of placing an averaging layer in the test version after the last FC layer and before the SM layer. The train version allows for placing the averaging layer after the last FC layer, after the SM layer, or after the LL layer. \"Element blocks\" from other groups are reused for efficiency and meaningful comparisons. Each branch is defined by a parameter vector W e, while the global network is defined by a concatenated parameter vector W. The network architecture involves using \"element blocks\" connected via a \"fuse layer\" to improve performance. The global network is defined by a concatenated parameter vector W, with each branch defined by a parameter vector W e. Different training and prediction conditions can be combined by splitting the parameter vector W accordingly. The global hyper-parameters control the number of branches and the placement of the AVG layer. Larger models may require data batches to be split into micro-batches for training. Gradient accumulation and averaging over micro-batches are used to approximate processing data as a single batch. BatchNorm layer uses micro-batch statistics, which may slightly differ from whole batch statistics but does not significantly impact results. To ensure optimal throughput, parameter updates are done using gradient for a batch while forward passes are done with micro-batches. Memory requirements depend on network depth and mini-batch size. Using the micro-batch \"trick\" adjusts memory needs without affecting performance. The multi-branch version requires more memory only if branch width is reduced. Hyper-parameter search experiments indicated that maintaining branch width or depth yielded the best results. In experiments with hyper-parameter search, reducing both branch width and depth was found to be the best option for optimal performance. Training with 25M parameters on a GTX 1080 Ti was done within 11GB memory using micro-batch sizes of 16 for single-branch and 8 for multi-branch versions. Doubling micro-batch sizes by splitting the network over two GPU boards did not significantly increase speed or improve performance. Comparing FC average and logsoftmax showed similar results for coupled ensembles with two branches, providing a significant gain over using only one branch. In experiments with hyper-parameter search, reducing both branch width and depth was found to be the best option for optimal performance. Training with 25M parameters on a GTX 1080 Ti was done within 11GB memory using micro-batch sizes of 16 for single-branch and 8 for multi-branch versions. Comparing FC average and logsoftmax showed similar results for coupled ensembles with two branches, providing a significant gain over using only one branch. The performance is quite stable against variation of the (L, k) compromise, with the combination of (L = 82, k = 8, e = 3) predicted to be the best on the test set. When comparing parameter usage and performance of branched coupled ensembles with model architectures recovered using meta learning techniques, issues of experiment reproducibility and statistical significance arise. Variations in performance measures stem from framework differences, random seed initialization, CuDNN non-determinism, and fluctuations in batch normalization. These factors significantly impact observed results. Fluctuations in batch normalization can be observed even with all hyper-parameters set to 0. The choice between the model obtained after the last epoch or the best performing model affects evaluation measures due to random initialization. Despite small dispersion, comparisons between methods are complicated. The dispersion in neural network performance complicates comparisons between methods, as differences below the dispersion may not be significant. Statistical tests are not helpful due to variations caused by different seeds. Experiments on a moderate scale model show the challenges in quantifying effects. Results for DenseNet-BC with L = 100, k = 12 on CIFAR 100 are presented for different seed combinations using Torch7 and PyTorch. Results for DenseNet-BC with L = 100, k = 12 on CIFAR 100 show no significant difference between Torch7 and PyTorch implementations. Different seed combinations were tested, and variations in performance were observed. The study compared Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Different seed combinations were tested, showing variations in performance. Observations on the last 10 epochs revealed no significant differences between implementations or seed usage. Averaging measures over the last 10 epochs reduced fluctuations in standard deviation and improved consistency in results. The study compared Torch7 and PyTorch implementations of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Observations on the last 10 epochs showed that averaging measures reduced fluctuations in standard deviation and improved result consistency. The mean of measures on 10 runs is lower when taken at the best epoch compared to single last epoch or last 10 epochs. Proposing a method for fair comparisons and reproducibility, avoiding bias in absolute performance estimation. In experiments comparing Torch7 and PyTorch implementations of DenseNet-BC on CIFAR 100, averaging measures over the last 10 epochs reduced standard deviation fluctuations and improved result consistency. The choice between using the error rate at the last iteration or the 10 last iterations did not affect the mean significantly, but the latter had a smaller standard deviation. For CIFAR experiments, the average error rate of the models obtained in the last 10 epochs was preferred for robustness. In SVHN experiments with fewer epochs, the last 4 iterations were used. These observations aim to ensure fair comparisons and avoid bias in performance estimation. In this study, comparisons were made between single-branch and multi-branch architectures at a constant parameter budget, showing an advantage for multi-branch networks. However, the training time for multi-branch networks is currently longer. Ways to reduce training time include reducing iterations, parameter count, or increasing width while reducing depth. The use of the average error rate from the last epochs is recommended for more robust results. The study compared single-branch and multi-branch architectures at a constant parameter budget, favoring multi-branch networks. Ways to reduce training time include reducing iterations, parameter count, or adjusting width and depth. Results for different options are shown for CIFAR 10 and 100 datasets. In this section, the performance of single branch models and coupled ensembles is compared in a low training data scenario. Both models have the same number of parameters and are trained on STL-10 and a 10K balanced random subset of CIFAR-100. DenseNet-BC models with different parameter budgets are evaluated, showing that even with reduced parameters, they outperform the single-branch baseline. Results from experiments comparing single-branch models and multi-branch coupled ensembles on two datasets, STL-10 and a subset of CIFAR-100, show that coupled ensembles outperform single-branch models with the same parameter budget. Preliminary experiments on ILSVRC2012 were conducted with image size constraints and limited data augmentation. Further experiments with full-sized images and increased data augmentation are planned. The study used DenseNet-121-k30-e2 as a baseline due to constraints. Experiments with full-sized images and increased data augmentation are planned but will be after the deadline. Results in table 11 show that a coupled ensemble approach with two branches significantly improves over the baseline, even with a constant training time budget."
}