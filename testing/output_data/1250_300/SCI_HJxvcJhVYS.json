{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A proposed method uses Bayesian optimization to approximate inference by applying Stein variational gradient descent on estimates from a Gaussian process model. This approach shows promise for likelihood-free inference in reinforcement learning environments. The problem involves estimating parameters of a physical system using a computational model when the likelihood function is not available. Recent methods focus on improving simulation efficiency through conditional density estimators or approximations to the likelihood function. Gutmann and Corander (2016) propose an active learning approach using Bayesian optimization. This paper explores combining variational inference methods with Bayesian optimization to efficiently estimate parameters of a physical system. A Thompson sampling strategy is used to refine variational approximations to a black-box posterior, with parameters proposed using Stein variational gradient descent over Gaussian process samples. The approach includes optimal subsampling of variational approximations for batch evaluations of simulator models. The approach involves estimating a distribution q that approximates a posterior distribution p(\u03b8|y) over simulator parameters \u03b8 given observations y. It utilizes a Bayesian optimization approach to minimize the discrepancy between q and the target p using a kernelized Stein discrepancy. The method does not require gradients of the target distribution p and consists of a GP model, Thompson sampling acquisition function, and kernel herding procedure. The Bayesian optimization algorithm involves estimating a distribution q to approximate the posterior distribution p(\u03b8|y) using a GP model, Thompson sampling, and kernel herding. To bypass modeling the map from q's parameters to KSD, Stein variational gradient descent (SVGD) is used. Gradients of the target log p are not required as a GP models g : \u03b8 \u2192 \u2212\u2206 \u03b8, defining a synthetic likelihood function. The simulations-observations discrepancy \u2206 \u03b8 is costly to evaluate and non-differentiable. The Bayesian optimization algorithm involves using a GP model to approximate the posterior distribution, with Thompson sampling for candidate selection. Thompson sampling accounts for uncertainty by sampling functions from the GP posterior. For models like SSGPs, weights are sampled from a multivariate Gaussian to constitute a sample from the posterior. In Bayesian optimization, Thompson sampling is used for candidate selection by sampling weights from a multivariate Gaussian for models like SSGPs. The acquisition function is defined based on an approximation to the target posterior using SVGD with particles initialised from the prior and optimised through perturbations. The gradients of sample functions are available for SSGP models with differentiable mean functions. The text discusses the use of SVGD to explore distant modes in the approximate posterior surface for Bayesian optimization. It emphasizes the importance of selecting a distribution q and optimizing the acquisition function to find optimal query parameters for simulations. The text also mentions the use of kernel herding for subsampling candidate q to improve efficiency. Kernel herding is used to select a subset of query parameters {\u03b8 n,j } S j=1 \u2282 \u0398 to minimize error in empirical estimates for expectations under distribution q. The procedure leverages GP information to choose informative samples, guided by the GP posterior kernel. This sampling scheme improves efficiency in simulations. The posterior kernel provides an embedding for q by accounting for previously observed locations in the GP data. The distributional Bayesian optimisation (DBO) algorithm is summarised in Algorithm 1 and evaluated in synthetic data scenarios, comparing it against mixture density networks (MDNs). Experimental results are presented for OpenAI Gym's 3 cart-pole environment, with details on the dataset generation and prior specifications. Further experimental setup details can be found in Appendix B. The study by Ramos et al. (2019) presents a Bayesian optimization approach for inverse problems on simulator parameters. Results show that the method outperforms MDN in recovering the target system's posterior and provides better approximations. DBO is more sample-efficient for inferring parameters in reinforcement learning environments compared to other likelihood-free inference methods. Future work includes scalability and theoretical analysis of the method. OpenAI Gym and code for the implementation are available online. The study presents a Bayesian optimization approach for inverse problems on simulator parameters, outperforming MDN in recovering the target system's posterior. DBO is more sample-efficient for inferring parameters in reinforcement learning environments. Future work includes scalability and theoretical analysis. The method allows for fast incremental updates of the GP posterior with time complexity O(M^2)."
}