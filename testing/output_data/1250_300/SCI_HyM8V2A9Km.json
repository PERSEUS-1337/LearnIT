{
    "title": "HyM8V2A9Km",
    "content": "Sparse reward is a challenging problem in reinforcement learning. Hindsight Experience Replay (HER) addresses this by converting failure experiences into successful ones. However, HER has limited applicability due to a lack of universal goal representation. Augmenting experienCe via TeacheR's adviCE (ACTRCE) extends HER using natural language as the goal representation. ACTRCE efficiently solves difficult reinforcement learning problems in 3D navigation tasks, where HER fails. Language goal representations allow the agent to generalize to unseen instructions and lexicons. Hindsight advice is crucial for solving challenging tasks, but a small amount is sufficient for learning to progress practically. Using hindsight advice is crucial for solving challenging tasks, with a small amount being sufficient for learning to progress practically. Deep reinforcement learning applications often rely on complex reward functions, but designing them can be nontrivial and lead to biased learning. Sparse and binary reward functions can make learning difficult, but Hindsight Experience Replay (HER) addresses this issue by utilizing failed experiences. Hindsight Experience Replay (HER) addresses the challenge of sparse rewards in deep reinforcement learning by converting failed experiences into successful ones using fake goals. However, representing goals using the state space can be inefficient and redundant. A goal representation is needed that is expressive, flexible, compact, and informative. In response to the challenge of sparse rewards in deep reinforcement learning, Hindsight Experience Replay (HER) converts failed experiences into successful ones using fake goals. However, representing goals using the state space can be inefficient and redundant. A goal representation that is expressive, flexible, compact, and informative is needed. Natural language representation of goals satisfies these requirements by flexibly describing goals across tasks and environments, compressing redundant information in states, and providing transferable features for generalizing across goals. The proposed technique, Augmenting experienCe via TeacheR's adviCE (ACTRCE), combines the HER framework with natural language goal representation to address a broad range of reinforcement learning problems. This method involves a teacher giving advice in natural language to the agent after each episode, allowing the agent to form new experiences based on the advice received. The ACTRCE method combines Hindsight Experience Replay with natural language goal representation to address reinforcement learning problems. A teacher provides advice to the agent after each episode, enabling the agent to form new experiences and alleviate sparse reward issues. The language goal representation allows for efficient problem-solving in challenging environments, generalization to unseen instructions, and the use of hindsight advice to tackle difficult tasks. The practical aspect of the method is highlighted by the effectiveness of even a small amount of hindsight advice. Our work combines reinforcement learning and rich language advice to efficiently address language grounding problems. By integrating reinforcement learning techniques with natural language advice, we provide a practical method for achieving goals described in natural language. This approach is essential for a more general understanding of natural language and has shown effectiveness in scaling beyond original domains. Discounted Markov Decision Process involves state space S, action space A, discount factor \u03b3, transition model P, and reward function r. The agent follows a policy \u03c0 \u03b8 parameterized by \u03b8, choosing actions to maximize expected cumulative return. Q-function Q \u03c0 calculates expected rewards for actions under a policy. Q-learning is an off-policy, model-free RL algorithm based on the Bellman equation. The Q-learning algorithm is based on the Bellman equation and uses semi-gradient descent to minimize the squared Temporal Difference error. Deep Q-Network builds on Q-learning by using a neural network to approximate the optimal Q-function. Goal-oriented reinforcement learning framework augments the Markov Decision Process with a goal space G, where a goal induces a reward function conditioned on the given goal. The agent's objective is to maximize the cumulative return by choosing actions based on the current state and goal. The goal in reinforcement learning is to maximize the expected return by choosing actions based on the current state and goal. A specific family of goal conditioned reward functions assigns rewards of either 0 or 1, making it difficult for the agent to learn. The Hindsight Experience Replay (HER) method addresses this issue by transforming experiences under a new goal if the terminal reward is zero, allowing for learning from off-policy experiences. In reinforcement learning, the authors propose a method to flexibly relabel experiences with a desirable goal by mapping states to goals. However, constructing such a mapping is challenging, as a simple mapping like G = S is redundant and limits the algorithm's applicability. For example, in a goal-oriented MDP, raw pixel observations in a 3-D environment may have many possible states that satisfy the goal of walking towards an object, making it difficult to abstractly represent the goal. In reinforcement learning, the authors propose using natural language to represent the goal space in a goal-oriented MDP. This approach aims to reduce redundancy in representation by providing an abstract representation for the goal, which can be challenging to achieve with raw pixel observations. The authors propose using natural language to represent the goal space in a goal-oriented MDP. A teacher provides a natural language description of the goal for each state, which can be used to convert a failure trajectory to a successful one. This approach includes positive reward signals and the necessity of negative experiences for training. The current state is \"Go to blue torch\". Positive and negative reward signals are used for training. Multiple goals can be satisfied in a state. Off-policy RL algorithms and a replay buffer are utilized. Teachers provide goal descriptions in natural language. Training is performed with a minibatch from the replay buffer. The approach involves receiving natural language advice from teachers to achieve different goals in a state. Each goal description corresponds to a different teacher, leading to a variety of advice. The original goal is relabeled with each advice, and the replay buffer is augmented with these trajectories. The proposed algorithm formalizes this process, considering both MDP and POMDP settings for teacher advice. The challenge lies in handling the natural language goal representation as a sequence of discrete symbols. In the POMDP setting, the teacher provides advice based on the history of states and actions. Two ways to convert language goals into continuous vectors are explored: using a recurrent neural network or a pre-trained language component. The latter allows for better natural language understanding and robustness to teacher advice. The architecture of the model includes three modules: a language component converting instructions into vectors, an observation processing component using convolutional neural networks, and a fusion component combining goal information and observations. Experimental results in KrazyGrid World and ViZDoom environments demonstrate the effectiveness of the proposed method compared to the baseline. The proposed method's effectiveness is demonstrated through a comparison of goal representations in hindsight advice, generalization, and semantic similarities. Different representations are compared, showing that the GRU and pre-trained embedding scale better than the one hot vector as the number of instructions increases. The pre-trained embedding can generalize to goals with out-of-training vocabulary words. Significant improvement in sample efficiency is shown when using hindsight language advice from teachers, especially in challenging tasks where the agent struggles to learn without it. In challenging tasks, significant improvement in sample efficiency is shown by using hindsight language advice from teachers. Even limited advice can lead to improvement, with low burden in practice. The method was tested in both a 2D grid world and a 3D environment based on the game Doom. ViZDoom BID14 BID4 is a 3D learning environment based on the game Doom, with raw pixel images and natural language instructions for tasks. Singleton tasks are introduced in KGW and ViZDoom sections, with compositions of goals like \"A and B\" or \"A or B\". DQN algorithm is used for reinforcement learning in training. In our experiments, we compared language-based goal representations with non-language goal representations. We found that language goal representations were more effective as tasks became more difficult, providing better learning signals and allowing for generalization to unseen goals in training. The DQN algorithm was used for reinforcement learning in both environments, with details provided in the appendices. The language goal representations proved more effective as tasks became harder, providing better learning signals and enabling generalization to unseen goals in training. Three goal representations were described, including Language Sentence Representation with GRUs using a pre-trained sentence embedding like InferLite BID16. InferLite is a lightweight sentence encoder trained for natural language inference, similar to InferSent but without RNNs. It uses a learned linear layer to project the original 4096-dimensional vector down to 256. One-Hot Representation is used for non-language baseline, embedding each instruction as a vector with the same dimensions as the GRU representation. Three goal representations are used for learning tasks in ViZDoom environment, with comparisons shown in FIG1. Agents using one-hot goal representations perform as well as those using other representations in easier tasks, but struggle in more challenging tasks. In a ViZDoom environment, three goal representations were used for learning tasks. One-hot representations performed well in easier tasks but struggled in more challenging ones. Language representations showed superior generalization ability, achieving a 97% success rate compared to 24% with one-hot representations. The agent could generalize to unseen instructions 83% of the time with GRU language goal representation. Visualization analysis revealed statistical relations between learned embeddings of goals. The visualization analysis showed significant differences in learned embeddings for the three goal representations used in the ViZDoom environment. Correlation matrices were calculated for each representation, revealing that GRU and InferLite embeddings had similar structures, while one-hot goal embeddings showed little correlation. t-SNE embeddings demonstrated meaningful clustering for language goals and sporadic embeddings for one-hot goals. Pre-trained embeddings were utilized to enhance the model's ability to generalize to unseen lexicons. The study utilized t-SNE embeddings to observe clustering with language goals and sporadic embeddings for one-hot goals. Pre-trained embeddings were used to enable the model to generalize to unseen lexicons at test time. By replacing words with synonyms, the model achieved tasks above 66% of the time, showcasing the importance of understanding synonyms for robust learning in noisy settings. In this section, the effectiveness of language goal representation is demonstrated from various perspectives. Hindsight advice is shown to play a crucial role in learning, with the method \"ACTRCE\" compared to the algorithm DQN. Results indicate that even a small amount of advice (1%) can significantly improve learning outcomes. Recurrent neural networks are used for embedding language goals in both methods. Experiments on singleton tasks with different grid configurations show promising results in terms of success rates. In experiments on singleton tasks with various grid configurations, the method ACTRCE outperformed the baseline DQN, achieving around 80% success rate. In more challenging environments with 7 objects, only the agent trained with ACTRCE was able to learn. The agent's multitask and zero-shot generalization performance is summarized in Table 1. In challenging environments with multiple goals and obstacles, ACTRCE outperformed the baseline DQN in terms of training success rates. The agent also showed superior performance in compositional tasks with hindsight advice, while the baseline DQN struggled to learn efficiently. In a variant of ACTRCE, the teacher provides hindsight advice only in the first {10%, 1%} of frames during training, and stops giving any advice for the remaining portion. The experiment was conducted in the ViZDoom environment single target mode with 7 objects, using GRU as the sentence embedding. Results show that the agent can still learn well even with minimal (1%) advice, demonstrating the method's robustness. Previous approaches have utilized natural language advice in reinforcement learning, such as translating advice into a short program or shaping rewards based on human feedback. The use of natural language advice in reinforcement learning has been explored in various studies. For example, BID17 used natural language advice for a RL agent learning to play a soccer game, while BID21 shaped the reward of an image captioning RL agent with human feedback. BID13 introduced an agent that learns to use English instructions to self-monitor its progress in Atari games. Additionally, language has been proposed as the latent parameter space for few-shot learning problems in policy search. Task-oriented language grounding has also been studied, where reinforcement learning is used to learn grounded language for specific tasks. In contrast to previous works using natural language advice in reinforcement learning, the authors transfer unseen synonym words from a pre-trained sentence embedding model to execute instructions in a 3D environment. They also build on prior research by incorporating experience replay to accelerate the credit assignment process. The ACTRCE method uses natural language as a goal representation for hindsight advice in reinforcement learning. It shows that language goal representations can efficiently solve challenging 3D navigation tasks and enable generalization to unseen instructions and lexicons. The ACTRCE method utilizes natural language as goal representations for hindsight advice in reinforcement learning. It demonstrates the agent's ability to generalize to unseen instructions and lexicons, showcasing practicality with minimal advice. The KrazyGrid World environment consists of a 2D grid with different functionality tiles and colors, where the agent's goal is to reach goals of various colors using a global view of the grid state. The grid state in the KrazyGrid World environment includes functionality and color attributes represented in one hot vectors. The environment has a grid size of 9x9 with 3 goals of distinct colors and various lava obstacles. Episodes terminate when the agent reaches a goal or lava, or after a maximum of 25 time steps. The goal space was expanded to include compositions of goals, with episodes ending when the agent reaches 2 different goals or after 50 time steps. An extra action called \"flag\" was added for the agent to signal goal accomplishment. The ViZDoom BID14 BID4 3D learning environment is based on the Doom game, with raw pixel images as states. The agent can navigate using turn left, turn right, and move forward actions. The goal is to follow natural language instructions like \"Go to the green torch\" within 30 time steps, with a reward of 1 for reaching the correct object. An extra action called \"flag\" allows the agent to end the episode when it believes the goal is achieved. The ViZDoom BID14 BID4 3D learning environment involves the agent navigating to objects based on natural language instructions within 30 time steps. The episode ends when the agent reaches an object or when the maximum time step is reached. Rewards are given based on reaching the correct object. Difficulty modes include easy, where objects are evenly spaced in front of the agent, and hard, where objects and the agent are randomly spawned. Instructions consist of two single object instructions joined by \"and\". No superlative instructions are included to ensure clarity. In the ViZDoom composition task, instructions are designed to be unambiguous, with mutually exclusive sets of objects satisfying each instruction. A HUD displays thumbnail images of reached objects, with the episode ending once the agent reaches a second object. Synonym instructions are generated by replacing words in the original instruction. In the ViZDoom composition task, instructions are designed to be unambiguous, with sets of objects satisfying each instruction. Positive and negative feedback is generated based on whether the agent reaches an object or not. For compositional tasks, a similar process is followed. In the ViZDoom composition task, instructions are designed to be unambiguous, with sets of objects satisfying each instruction. Positive and negative feedback is generated based on whether the agent reaches an object or not. For compositional tasks, a similar process is followed. The agent's performance did not improve when it did not reach any objects at the end of the episode. For singleton tasks, a series of convolution layers with ReLU activation functions are used for prepossessing the grid observation. For singleton tasks, a series of convolution layers with ReLU activation functions are used for prepossessing the grid observation. The language sentences are input as word level one hot vectors to a LSTM with hidden size 128. The LSTM's last hidden vector is passed into a fully connected layer with 128 output units with sigmoid activation, acting as the attention vector. Gated feature maps are flattened and passed through a fully connected layer with ReLU activation with 256 units, then into a linear layer to predict the 4 action values. For compositional tasks, a series of convolution layers with ReLU activation functions are used for prepossessing the grid observation. The language sentences are input as word level one hot vectors to a bidirectional LSTM with hidden size 40. After obtaining the preprocessed observation, it is augmented with the history vector provided. The curr_chunk describes the architecture used for processing observations in a neural network model. It involves multiple convolution layers with different kernel sizes and strides, followed by LSTM and attention mechanisms. The final output predicts 5 action values. The architecture is similar to a previous model but with a linear output layer and without dueling architecture. The input to the network is an RGB image. The architecture for processing observations in the neural network model involves convolution layers with different kernel sizes and strides, followed by LSTM and attention mechanisms. The final output predicts 5 action values. The input is an RGB image. The architecture is similar to a previous model but with a linear output layer and without dueling architecture. For KrazyGrid World experiments, hyperparameters such as learning rate, replay buffer size, and training frequency were tuned. Episodes were generated using an -greedy policy with Double DQN and Huber loss for stable gradients. In the ViZDoom environment, training instructions were used from a previous study, and DQN was implemented on top of an existing codebase with a cyclic buffer replay buffer. In the implementation of DQN for KrazyGrid World experiments, the cyclic buffer replay buffer contains recent transitions for easy and hard modes. Episodes are generated with an -greedy policy, and Double DQN is used to reduce Q-value overestimation. The network is updated every 4 frames on easy mode and 16 frames on difficult mode for better performance. Sampling from the replay buffer involves selecting 32 consecutive frames from a random episode. When sampling from the replay buffer, 32 consecutive frames are selected from a random episode to improve LSTM state estimates. Running 16 parallel threads helps alleviate sample correlation in the mini-batch for n-step Q learning. The target network is synchronized with the current model every 500 time steps, and an additional thread evaluates multi-task success rate during training. Teacher types are described, including both favorable and unfavorable behaviors in KGW. In the KGW, a subset G d \u2286 G denotes desired goals for the agent. Each episode, a goal g \u2208 G d is sampled, and the agent explores conditioned on this goal. A teacher provides advice based on the terminal state, with three types: Optimistic, Knowledgeable, and Discouraging. The teacher's advice varies depending on the agent's performance. In the KGW, teachers provide advice to the agent based on its performance with different types of advice: Optimistic, Knowledgeable, and Discouraging. The method \"ACTRCE \u2212 \" using only optimistic and discouraging teachers quickly learned and achieved good results in KrazyGrid World, outperforming the baseline DQN algorithm. The baseline DQN failed to learn, while \"ACTRCE \u2212\" quickly learned and achieved good results in KrazyGrid World. However, performance dropped when the number of lavas increased. Knowledgeable teachers helped speed up learning, maintaining a similar rate even in more difficult settings. This led to a significant performance gap between \"ACTRCE\" and \"ACTRCE \u2212\" after 32 million frames of training. In a transfer learning experiment, agents were pretrained with a pessimistic teacher who only gave advice when undesirable goals were achieved. Despite no overlap between pretraining and actual training goals, the agents learned much faster than unpretrained ones. In experiments with ViZDoom, pretrained agents using pessimistic teachers learned faster than unpretrained ones, even with different goals. Pretrained agents outperformed unpretrained ones in environments with 3 and 6 lavas. Additionally, on easy tasks, DQN and ACTRCE performed similarly, but on harder tasks with 5 objects, DQN's learning was less consistent compared to ACTRCE. In experiments with ViZDoom, pretrained agents using pessimistic teachers learned faster than unpretrained ones, even with different goals. Pretrained agents outperformed unpretrained ones in environments with 3 and 6 lavas. On the easy task, DQN and ACTRCE performed similarly. However, on harder tasks with 5 objects, DQN's learning was less consistent compared to ACTRCE. A3C baseline from BID4 was also tested, showing lower sample efficiency compared to DQN/ACTRCE implementation. The average episode length decreased when using ACTRCE, while DQN remained fairly flat for harder scenarios. The GRU hidden state representation of sentences was used in the plots in FIG0, showing a decrease in average episode length with ACTRCE compared to baseline DQN for harder tasks. A cumulative success rate curve was constructed to evaluate model performance, with a larger area under the curve indicating a better model. The Multi-task cumulative success rate for 3 ViZDoom environment tasks using GRU hidden state language encoding is shown in FIG0. In the 5 objects hard mode, ACTRCE outperformed baseline DQN after episode length 20. In the 7 objects hard mode, ACTRCE maintained better performance while baseline DQN only succeeded in very short episodes. For the 5 objects composition task, ACTRCE had two groups of trajectories based on proximity of target objects, with some requiring less than 10 time-steps and others over 20 time-steps. In the 5 objects composition task, trajectories can be grouped based on proximity of target objects, with some requiring less than 10 time-steps and others over 20 time-steps."
}