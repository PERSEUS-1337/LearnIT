{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"conditionally invariant\" features that do not change across domains and \"orthogonal\" features that can vary across domains. The goal is to use the invariant features for classification to guard against future domain shifts. The distributional change of features across different domains is not directly observable due to the latent nature of the domain variable. In data augmentation, images are generated from an original image, with an ID variable referring to the original image. Only a small fraction of images need to have an ID variable. In a causal framework, the ID variable is added to the model to treat the domain as a latent variable. Samples with the same class and identifier are treated as counterfactuals under different style interventions. Regularizing the network with a graph Laplacian improves performance in settings with changing domains. Deep neural networks have excelled in prediction tasks, but issues can arise from dependencies that vanish in test distributions due to domain shifts caused by changing conditions. Predictive performance in machine learning tasks can degrade due to domain shifts caused by changing conditions like color, background, or location changes. An example is the \"Russian tank legend\" where a system trained to distinguish between Russian and American tanks failed in practice due to sampling biases in the training data. Hidden confounding factors, like image quality, can lead to indirect associations, highlighting the need for large sample sizes in deep learning. Hidden confounding factors, like image quality, can lead to indirect associations in deep learning, necessitating large sample sizes to average out these effects. Adversarial examples, imperceptibly perturbed inputs misclassified by ML models, highlight the difference between human and artificial cognition in achieving invariance. The question arises of mimicking human ability to learn invariances from few instances and aligning features in DNNs. The text discusses the challenges of achieving invariance in deep learning models and the need to align features with human cognition. It also highlights the issue of biases in training datasets leading to discriminatory outcomes. The proposed solution is counterfactual regularization (CORE) to control latent features extracted from input data. CORE proposes counterfactual regularization to control latent features extracted by classifiers, focusing on 'conditionally invariant' core features. This approach ensures stability and coherence in targeting specific factors of interest, making the estimator robust against adversarial domain shifts. By leveraging knowledge of grouping within datasets, CORE exploits counterfactual observations to enhance performance. The manuscript introduces CORE, a method that leverages grouping information in datasets to improve predictive performance. It proposes counterfactual regularization to control latent features and ensure stability in targeting specific factors of interest. The approach is evaluated on the CelebA dataset for classifying whether a person wears glasses, using grouping information to make consistent predictions for images of the same person. The study introduces CORE, a method utilizing grouping information in datasets to enhance predictive accuracy. It employs counterfactual observations of the same person to improve model performance, resulting in significant reductions in test errors for both CelebA and MNIST datasets. Exploiting grouping information in datasets with CORE reduces test errors significantly, by 32% in one case and 50% in another. This method enhances data augmentation efficiency by enforcing invariance with respect to style features. Using CORE with grouping information reduces test errors significantly, by 32% in one case and 50% in another, enhancing data augmentation efficiency by enforcing invariance with respect to style features. This approach requires grouped observations, unlike other methods that rely on unlabeled data from the target task. The BID13 and BID14 models rely on unlabeled data from the target task. BID13 aims to learn a representation without discriminative information about the input's origin through adversarial training. In contrast, BID14 identifies conditionally independent features by adjusting transformations to minimize MMD distance between distributions in different domains. The main difference is that BID14's domain identifier is observable, while it is latent in our approach. Our approach differs from BID14 in that we use a latent domain identifier. We penalize the classifier for using latent features outside the set of conditionally independent features. Causal modeling aims to guard against adversarial domain shifts and interventions on predictor variables. However, transferring these results to adversarial domain changes in image classification faces challenges due to the anti-causal nature of the classification task and the need to guard against shifts in style features. The challenge in causal modeling is guarding against domain shifts and interventions on predictor variables. Recent approaches leverage causal motivations for deep learning, focusing on cause-effect inference. The Neural Causation Coefficient (NCC) estimates the probability of X causing Y, distinguishing between object features and context features. Structural equation modeling and CGANs show similarities. The NCC is used to estimate the probability of X causing Y and distinguish between object features and context features. CGANs are used to find causal relations between image features, with one fitted for X \u2192 Y and another for Y \u2192 X. Various approaches use generative neural networks for cause-effect inference and orienting graph edges. Regularizers combine penalties with estimated causal probabilities, obtained from causality detection networks or scores like the NCC. GANs and causal generative models are connected, with causal implicit generative models proposed for sampling from conditional and interventional distributions. Kocaoglu et al. (2017) propose causal implicit generative models for sampling from conditional and interventional distributions using a conditional GAN architecture. BID29 use deep latent variable models and proxy variables to estimate individual treatment effects, while BID21 incorporate causal reasoning to address fairness in machine learning by deriving causal nondiscrimination criteria. The distinction between core and style features in generative modeling is akin to disentangling factors of variation, with interest in estimating these factors growing. Matsuo et al. (2017) propose a \"Transform Invariant\" approach in this context. Estimating disentangled factors of variation in generative modeling has gained interest. Matsuo et al. (2017) introduce a \"Transform Invariant Autoencoder\" to reduce dependence on specific object transforms in images. They focus on orthogonal style features, such as location, but other factors like image quality and background could also be included. Their approach involves a confounding situation where style feature distributions differ based on class. In a variational autoencoder framework, Bouchacourt et al. (2017) aim to separate style and content by exploiting grouped observations. They assume samples within a group share a common but unknown value for one factor of variation while the style can differ. This approach contrasts with estimating latent factors explicitly in a generative framework for classification tasks. The standard notation for classification is described before developing a causal graph to compare adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. The text discusses parameter estimation in machine learning, focusing on minimizing expected loss through penalized empirical risk minimization. It involves choosing weights or parameters to minimize the empirical loss, with penalties like ridge penalty or Laplacian regularized least squares. The goal is to optimize the function f\u03b8(x) to predict the target variable Y based on predictor X. The text discusses the structural model for parameter estimation in machine learning, incorporating latent variables like ID and domain variable D. The prediction is anti-causal, with the class label Y influencing the image X through core features Xci and style features X\u22a5. External interventions are possible on style features but not on core features. The text explains the distinction between core features Xci and style features X\u22a5, where interventions are possible on the style features but not on the core features. The style features are influenced by the latent domain D, while the core features remain conditionally invariant. The style intervention variable influences both the latent style and the image. In potential outcome notation, X \u22a5 (\u2206 = \u03b4) represents the style under intervention \u2206 = \u03b4, with X(Y, ID, \u2206 = \u03b4) denoting the image for class Y and identity ID under style intervention \u2206. The prediction under style intervention \u2206 = \u03b4 is given by f \u03b8 (X(\u2206 = \u03b4)). The goal is to guard against adversarial domain shifts using a causal graph to explain domain adaptation, transfer learning, and protection against adversarial examples. The intervention \u2206 is assumed to be within a certain norm around the origin, causing imperceptible changes in the input to lead to misclassification. The objective is to minimize adversarial loss in the classification graph. The goal is to devise a classification in a graph that minimizes adversarial loss by considering arbitrarily strong interventions on style features. The term \"adversarial\" refers to interventions on style features, distinct from domain adversarial neural networks. The objective is to protect against adversarial domain shifts using causal graphs to explain domain adaptation and transfer learning. In contrast to BID13, the term \"adversarial\" is used for interventions on style features, while in domain adversarial neural networks, it describes the training procedure. The motivation is to protect against shifts in test data distribution by distinguishing between core and style features. Causal inference faces the challenge of never observing a counterfactual, where we can only see the outcome under one treatment but not both simultaneously. The term counterfactual is used here to refer to situations where class label and ID are constant but other variables are allowed to change. Counterfactuals in image analysis involve changing style interventions while keeping class label and ID constant. This allows for observing the same object under different conditions. The style intervention includes variables like background, posture, and image quality. Unlike in the medical setting, the focus is not on the treatment effect of the style intervention. The text discusses using style interventions in image analysis to observe the same object under different conditions, focusing on variables like background, posture, and image quality. Unlike in the medical setting, the emphasis is not on the treatment effect of the style intervention but on ruling out parts of the feature space for classification. The goal is to penalize any change in classification under different style interventions while keeping class label and identity constant. The text discusses using style interventions in image analysis to observe the same object under different conditions, focusing on variables like background, posture, and image quality. The goal is to penalize any change in classification under different style interventions while keeping class label and identity constant. The pooled estimator treats all examples identically by summing over the loss with a ridge penalty. It works well in terms of adversarial loss if certain conditions are met regarding the relationships between variables. The text discusses using style interventions in image analysis to observe the same object under different conditions. The pooled estimator works well in terms of adversarial loss under certain conditions regarding the relationships between variables. To minimize adversarial loss, the optimal predictor in the invariant space is determined by the core features, which are not directly observable. Empirical risk minimization is used to approximate the optimal invariant parameter vector. The text discusses inferring the invariant space from data to approximate the optimal invariant parameter vector using empirical risk minimization. The unknown invariant parameters space is approximated by an empirically invariant space, with a regularization constant allowing for variations. The true invariant space is a subset of the empirically invariant subspace, and under certain assumptions, they converge. The Lagrangian form of constrained optimization can also be used with a penalty parameter to achieve the optimal predictor in the invariant space. The text discusses using a penalty parameter \u03bb in the Lagrangian form of constrained optimization to obtain the optimal predictor in the invariant space. The graph Laplacian regularization penalizes the sum of variances \u03c32i(\u03b8) and is formed based on the identifier variable ID. The experiments highlight the importance of defining the graph in terms of ID to guard against adversarial domain shifts. Other regularizations do not perform as well in this scenario. The experiments emphasize the importance of defining the graph with the identifier variable ID to guard against adversarial domain shifts. The CORE estimator outperforms other regularizations in handling confounded training data sets and changing style features in test distributions. The CORE estimator can handle confounded training data sets and changing style features in test distributions. Experimental results for classifying elephants and horses, as well as additional experiments involving gender, wearing glasses, and brightness, are discussed. A TensorFlow BID0 implementation of CORE will be provided, along with details on tuning parameters. The target of interest is differentiating between children and adults based on height, which is considered a core feature. The target is differentiating between children and adults based on height, a core feature. There is a dependence between age and movement in the training dataset due to a hidden common cause. The data generating process is illustrated in a figure. Large movements are associated with children and small movements with adults in the training set. Test sets show interventions where the dependence between movement and age vanishes. In test sets 2 and 3, interventions remove the dependence between movement and age, with heavier movements in test set 3 than in test set 2. Results show that CORE outperforms the pooled estimator in predictive performance with as few as 50 counterfactual observations. Including more counterfactual examples does not improve the pooled estimator's performance due to bias. The study uses the CelebA dataset to classify whether the person in the image is wearing. The study uses the CelebA dataset to classify whether the person in the image is wearing eyeglasses. Counterfactual observations are created by sampling new image quality values. Different interventions are discussed, with misclassification rates shown in the training set. The study uses the CelebA dataset to classify whether the person in the image is wearing eyeglasses. Counterfactual observations are created by sampling new image quality values. Different interventions are discussed, with misclassification rates shown in the training set. In test sets 2-4, the pooled estimator performs poorly due to its reliance on image quality as a predictor, unlike CORE which is less affected by changing image quality distributions. In contrast to the pooled estimator, CORE's predictive performance is not significantly impacted by changing image quality distributions. The study aims to assess if CORE can exclude \"color\" from its learned representation by including counterfactual examples of different colors using the AwA2 dataset. The study uses the AwA2 dataset to classify images of horses and elephants, including counterfactual examples to test the models. Test sets with different image color modifications show that the pooled estimator struggles with grayscale and shifted color images, while CORE's performance remains stable. The pooled estimator struggles with color variations in test sets 2 and 3, benefiting from the predictive value of \"gray\" in training. In contrast, CORE's performance is unaffected by color changes, emphasizing color invariance for accurate predictions. CORE ensures fairness by not including \"color\" in its learned representation, unlike the pooled estimator which relies on color for decisions. CORE, a counterfactual regularization approach, aims to achieve robustness by distinguishing core and style features in images. It ensures fairness by excluding \"color\" from its learned representation and achieves invariance to style features like image quality and fashion type. The training is effective even with sampling biases, offering improved classification performance with fewer instances compared to standard data augmentation methods. CORE, a counterfactual regularization approach, aims to achieve robustness by distinguishing core and style features in images. It ensures fairness by excluding \"color\" from its learned representation and achieves invariance to style features like image quality and fashion type. The training is effective even with sampling biases, offering improved classification performance with fewer instances compared to standard data augmentation methods. The regularization of CORE penalizes features that vary strongly between different instances of the same object, potentially benefiting larger models like Inception or ResNet architectures. Further research could explore the benefits of using CORE for training Inception-style models end-to-end, focusing on sample efficiency and generalization performance. CORE aims to achieve robustness by distinguishing core and style features in images, excluding \"color\" from its learned representation. It offers improved classification performance with fewer instances and penalizes features that vary strongly between different instances of the same object. Future research could explore using video data for grouping and counterfactual regularization, potentially debiasing word embeddings. The CORE model distinguishes core and style features in images, excluding \"color\" from its representation. It uses logistic regression for prediction and estimates \u03b8 with logistic loss for training and testing. The model considers losses under standard and adversarial interventions on style variables, with corresponding benchmarks. The formulation of Theorem 1 relies on specific assumptions. The second loss is the loss under adversarial style interventions with large interventions allowed on X \u22a5. The formulation of Theorem 1 relies on specific assumptions regarding the sampling process for counterfactual examples. Theorem 1 states that the pooled estimator has infinite adversarial loss under specific assumptions about counterfactual sampling. The estimator is constrained to be orthogonal to a certain space, leading to implications for the estimator's behavior. The pooled estimator is constrained to be orthogonal to the column space of W. By contradiction, if W t\u03b8pool = 0, then the constraint W t \u03b8 = 0 becomes non-active, implying \u03b8pool = \u03b8*. The derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8*. The interventions only affect the column space of W, making the oracle estimator \u03b8* identical under true and counterfactual training data. The derivative g(\u03b4) in FORMULA24 can be written as DISPLAYFORM10 by taking the difference between FORMULA24 and FORMULA25. By model assumptions, \u03b4 = W u where u \u2208 R q. The eigenvalues of W t W are positive. The estimator \u03b8 * is not affected by interventions \u2206 i,j. The left hand side of (10) has a continuous distribution, and the probability of it not being 0 is 1. With probability 1, \u03b8 core = \u03b8 * with \u03b8 * defined as in (6). The proof of the first part is completed by contradiction, and for the second part, it is shown that with probability 1, \u03b8 core = \u03b8 * as defined in (6). The linear subspace I = {\u03b8 : W t \u03b8 = 0} is the invariant space for this model. By the definitions, with probability 1, I n = {\u03b8 : W t \u03b8 = 0}. Therefore, \u03b8 core = \u03b8 * . The estimator remains unchanged if data without interventions is used as training data. The population-optimal vector is defined, and by uniform convergence, FORMULA3 and FORMULA3 can be compared. The proof shows that with probability 1, \u03b8 core = \u03b8 * as defined in the model. The CelebA dataset is used to classify gender based on images, creating confounding by including mostly men wearing glasses. Counterfactuals are used to address this, with test set 2 having a flipped association between gender and glasses. The study uses the CelebA dataset to classify gender based on images, introducing counterfactuals to address confounding. Test set 2 has a reversed association between gender and glasses. Results show similar trends when training a CNN end-to-end or using Inception V3 features. The pooled estimator performs worse on test set 2 with larger m values, indicating exploitation of counterfactual examples. The study analyzes data augmentation using the CelebA dataset for classifying eyeglasses wearing. A confounded setting is explored where a hidden common cause affects brightness in images. Test sets vary in brightness interventions, affecting classification performance. The pooled estimator performs worse on test set 2 as m increases, indicating exploitation of counterfactual examples. In test sets with varying brightness interventions, the pooled estimator outperforms CORE on test set 1 by utilizing brightness information for prediction. However, it struggles on test sets 2 and 4 when brightness distributions differ significantly from training data. In contrast, CORE's predictive performance is not affected by changing brightness distributions. Different approaches for creating counterfactual observations are explored, including using a different image of the same person. In exploring different counterfactual settings, using a different image of the same person as a counterfactual (\"CF setting 2\") and using an image of a different person as a baseline (\"CF setting 3\") were evaluated. Results showed that counterfactual setting 1 worked best in isolating brightness as the invariant factor. The misclassification rates of CORE on subsampled data and the challenge of setting tuning parameters were also discussed. The performance of CORE in mitigating confounders and improving predictive accuracy is demonstrated through experiments varying the number of identities in the training dataset. Results show that as sample sizes increase, the performance of CORE becomes comparable to the pooled estimator, indicating successful mitigation of confounding factors. The experiment demonstrates that as sample sizes increase, the performance of CORE in mitigating confounders becomes comparable to the pooled estimator. Varying the number of augmented training examples also shows that CORE makes data augmentation more efficient, with lower misclassification rates on rotated digits. The experiment introduced in \u00a75.1 shows results for different numbers of counterfactual examples. The CORE estimator's performance is not sensitive to the number of counterfactual examples once there are enough in the training set. The pooled estimator struggles with predictive performance on test sets 2 and 3. Counterfactual setting 1 works best, with small differences between settings 2 and 3. There is a notable performance difference between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator, possibly due to image quality not being predictive enough for the target. The experiment results show that the pooled estimator has a significant performance difference between \u00b5 = 40 and \u00b5 = 50, possibly due to image quality not being predictive enough for the target. The models were implemented in TensorFlow and trained five times to assess variance. Training data was shuffled in each epoch to ensure mini batches contain counterfactual observations. The training data is shuffled to ensure mini batches contain counterfactual observations, with a batch size of 120. This setup makes optimization more challenging for small c values."
}