{
    "title": "BJlA6eBtvH",
    "content": "Continual learning is a challenge for neural networks due to catastrophic forgetting. A Differentiable Hebbian Consolidation model is proposed to address this issue by adding a rapid learning plastic component to the fixed parameters, allowing retention of learned representations for a longer timescale. The method is evaluated on various benchmarks including Permuted MNIST, Split MNIST, and Vision Datasets Mixture, as well as an imbalanced variant of Permuted MNIST. Our proposed model addresses catastrophic forgetting in neural networks by adapting to dynamic environments without additional hyperparameters. It outperforms baselines on benchmarks like Permuted MNIST and Vision Datasets Mixture, including an imbalanced variant of Permuted MNIST. The ability to continually adapt in changing data distributions is a key aspect of human intelligence, challenging to embed in artificial intelligence. Catastrophic forgetting in neural networks occurs when models trained with new data experience performance degradation over time. This phenomenon poses a challenge for deep neural networks tasked with continual learning, where the goal is to adapt to new tasks without forgetting previously learned ones. In real-world applications, the assumption of independent and identically distributed samples is easily violated due to concept drift and imbalanced class distributions. In applications requiring continual learning, the iid assumption is often violated due to concept drift and imbalanced class distributions. This leads to a stability-plasticity dilemma for learning systems, where they must balance integrating new knowledge with preserving existing knowledge. Synaptic plasticity in biological neural networks plays a crucial role in learning and memory, with theories inspired by synaptic consolidation in the neocortex explaining human ability for continual learning. Two major theories explain human ability for continual learning: synaptic consolidation in the neocortex and the complementary learning system (CLS) theory. The former involves preserving important synaptic parameters through task-specific updates, while the latter suggests storing high-level structural information in different brain areas. Recent research on differentiable plasticity shows that neural networks with \"fast weights\" can be trained to optimize both \"slow weights\" and the plasticity in synaptic connections. Differentiable plasticity involves training neural networks with \"fast weights\" to optimize both \"slow weights\" and the plasticity in synaptic connections. This approach outperforms networks with uniform plasticity and addresses the catastrophic forgetting problem by dynamically adjusting synaptic plasticity based on importance for retaining past memories. Differentiable Hebbian Consolidation 1 model extends work on differentiable plasticity to task-incremental continual learning. It adapts quickly to changing environments and consolidates previous knowledge by selectively adjusting synapse plasticity. The model combines traditional softmax layer modification with plastic weights in the final fully-connected layer using Differentiable Hebbian Plasticity. It also integrates task-specific synaptic consolidation approaches to overcome catastrophic forgetting. The model proposed in the curr_chunk integrates various approaches to address catastrophic forgetting, leveraging Hebbian plasticity, synaptic consolidation, and CLS theory. It utilizes compressed episodic memories in the softmax layer to retain previous knowledge and adapt to new data. The model is tested on benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture, showing superior performance with task-specific synaptic consolidation methods. Additionally, the concept of Hebbian learning is highlighted as a key factor in continual learning. Hebbian learning theory suggests that synaptic plasticity is key to learning and memory. Recent studies have incorporated fast weights in neural networks for one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed a model with fast weights based on Hebbian learning. Recent studies have introduced innovative approaches to neural network training, such as augmenting FC layers with fast weights based on Hebbian learning. These methods include Hebbian Softmax layers and differentiable plasticity, which optimize synaptic plasticity for improved learning. While these techniques have shown promise in tasks like pattern memorization and maze exploration, they have mainly been applied to meta-learning challenges rather than continual learning. Our work introduces a method to overcome catastrophic forgetting by augmenting FC layers with fast weights using DHP. We update only the softmax output layer parameters for fast learning and knowledge preservation. This approach leverages task-specific synaptic consolidation and CLS theory to protect and retain memories while enabling rapid learning and storage of new instances. The hippocampus performs rapid learning and individuated storage to memorize new instances or experiences. Regularization strategies inspired by task-specific synaptic consolidation help overcome catastrophic forgetting in continual learning. These approaches estimate the importance of each parameter or synapse to prevent changes to important parameters of previously learned tasks. Regularization strategies like Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS) compute the importance of parameters to prevent forgetting in continual learning. EWC uses Fisher information matrix values, while SI measures cumulative changes in synapses, and MAS considers sensitivity for parameter importance. Our work is inspired by CLS theory and focuses on measuring parameter importance using Memory Aware Synapses (MAS). Various approaches based on CLS principles involve pseudo-rehearsal, exact or episodic replay, and generative replay methods. Exact replay requires storing data from previous tasks, while generative replay trains a separate model to generate images for replay. iCaRL combines rehearsal and regularization by using an external memory to store exemplar patterns from old task data. Our work focuses on neuroplasticity techniques inspired by CLS theory to alleviate catastrophic forgetting. Previous research has shown how synaptic connections can have fixed weights for long-term knowledge and fast-changing weights for temporary memory. Recent studies have explored using fast weights in RNNs, Hebbian Softmax layers, and differentiable plasticity to address continual learning challenges. In (Ba et al., 2016), Hebbian Softmax layer (Rae et al., 2018) and differentiable plasticity methods (Miconi, 2016; Miconi et al., 2018) were designed for rapid learning on simple tasks or meta-learning. These techniques aim to achieve fast binding for rarer classes by modifying parameters based on an engineered scheduling scheme. However, they may switch to SGD updates when a large number of examples are observed frequently from the same class in continual learning setups. Our work focuses on metalearning a local learning rule for fast weights in continual learning setups. The model includes slow weights and a Hebbian plastic component with a scaling parameter \u03b1 and Hebbian traces accumulating mean hidden activations. The Hebbian trace is a scaling parameter for adjusting the magnitude of the Hebb. It accumulates mean hidden activations of the final hidden layer for each target label in the mini-batch. The network parameters are optimized by gradient descent as the model is trained sequentially on different tasks in continual learning setups. The \u03b7 parameter dynamically learns how quickly to acquire new experiences into the plastic component and acts as a decay term for the Hebb to prevent instability. The network parameters are optimized by gradient descent in continual learning setups. The model updates Hebbian traces during training and uses them for predictions at test time. Hidden activations are accumulated into the softmax output layer weights for better initial representations. Our model utilizes Hebbian traces for predictions, optimizing hidden activations directly into softmax output layer weights for improved initial representations and long-term retention. Fast learning with a plastic weight component enhances test accuracy, while selective consolidation into a stable component protects old memories. DHP Softmax simplifies implementation without requiring additional space or computation, allowing scalability with increasing tasks. The Softmax method simplifies implementation by scaling easily with increasing tasks. It utilizes Hebbian traces to improve learning of rare classes and bind class labels to deep representations without interference. This method enhances test accuracy and long-term retention by rapidly storing memory traces for recent experiences. The DHP Softmax method utilizes Hebbian traces to improve learning of rare classes and bind class labels to deep representations without interference. It enhances test accuracy and long-term retention by rapidly storing memory traces for recent experiences. Additionally, Hebbian Synaptic Consolidation regularizes the loss and updates synaptic importance parameters of the network in an online manner. Our model adapts task-specific consolidation approaches by only regularizing the slow weights of the network, not the synaptic importance parameters on the plastic component. The plastic component of the softmax layer helps prevent catastrophic forgetting by optimizing the plasticity of connections. Comparing our approach to vanilla neural networks with Online EWC, SI, and MAS, we show that adding plastic weights increases the DNN's capacity. Our approach enhances DNN capacity by adding plastic weights to the softmax output layer. We tested the model on various benchmarks and introduced the Imbalanced Permuted MNIST problem. Evaluation was based on classification accuracy for all tasks trained so far. Memory retention and flexibility were assessed by performance on the first and most recent tasks, as well as measuring forgetting using the backward transfer metric. The study evaluates forgetting in neural networks using the backward transfer metric. Different consolidation methods were compared on sequential tasks, with hyperparameters remaining consistent. The benchmarks involved permuted MNIST datasets with concept drift. In the study, a multi-layered perceptron network with two hidden layers was used on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The network with DHP Softmax showed improvement in alleviating catastrophic forgetting compared to a baseline network. Task-specific consolidation methods were also compared with and without DHP Softmax. In the study, a multi-layered perceptron network with two hidden layers was used on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The network with DHP Softmax showed improvement in alleviating catastrophic forgetting compared to a baseline network. Task-specific consolidation methods were also compared with and without DHP Softmax. The performance with DHP Softmax and consolidation maintained higher test accuracy throughout sequential training of tasks. An ablation study examined the structural parameters of the network and Hebb traces to provide interpretability into the behavior of the proposed model. The behavior of \u03b7 during training as tasks are learned continually was analyzed, showing an increase in plasticity to acquire new information followed by a decay to prevent interference between learned representations. The Frobenius Norm of the Hebb trace suggested growth without interference. The study focused on reducing plasticity to prevent interference between learned representations in a multi-layered perceptron network. The Frobenius Norm of the Hebb trace indicated controlled growth without runaway positive feedback for each new task learned. The plasticity coefficients within each task increased, showing the network leveraging the structure in the plastic component. Gradient descent and backpropagation were used for meta-learning to tune structural parameters. An Imbalanced Permuted MNIST problem was introduced to address class imbalance and concept drift challenges during sequential training. The study focused on reducing interference between learned representations in a multi-layered perceptron network by controlling plasticity. DHP Softmax achieved 80.85% test accuracy after learning 10 tasks with imbalanced class distributions, showing a 4.41% improvement over standard neural networks. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered longer. DHP Softmax with MAS achieved an 88.80% test accuracy with a 1.48% improvement over MAS alone, outperforming all other methods. The MNIST dataset was split into 5 binary classification tasks. In a study focusing on reducing interference in a multi-layered perceptron network, DHP Softmax achieved 98.23% test accuracy, showing a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and improved average test accuracy across all tasks, especially the most recent one, T5. The study performed continual learning on a sequence of 5 vision datasets, including MNIST and notMNIST 1. Consolidation consistently decreases BWT, leading to higher average test accuracy across all tasks, especially the most recent one, T5. Continual learning was performed on a sequence of 5 vision datasets, including MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The datasets were zero-padded to be of size 32\u00d732 and replicated 3 times to create grayscale images with 3 channels. A CNN architecture similar to previous works was used, with an initial \u03b7 parameter value of 0.0001. The network was trained with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. DHP Softmax plus MAS decreased BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. SI with DHP Softmax outperformed other methods with an average test performance. The addition of compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters alleviates catastrophic forgetting in continual learning environments. This approach improves average test accuracy by 2.14% over MAS alone and outperforms other methods with an average test performance of 81.75%. The \u03b1 parameter in the plastic component automatically adjusts synaptic parameters based on their importance for solving previously learned tasks, allowing the model to generalize across experiences. The DHP Softmax in the neural network improves performance by automatically adjusting synaptic parameters based on importance for solving tasks, without introducing additional hyperparameters. The model can generalize across experiences and avoid interference when learning new information. The model's flexibility is demonstrated by combining DHP Softmax with Hebbian Synaptic Consolidation methods like EWC, SI, and MAS to alleviate catastrophic forgetting after learning multiple tasks. DHP Softmax with SI outperforms other methods on Split MNIST and 5-Vision Datasets, while combining DHP Softmax with MAS shows superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The model consistently exhibits lower negative BWT and higher test accuracy with DHP. Our model with Hebbian plasticity consistently shows lower negative BWT and higher test accuracy across benchmarks, indicating its ability to learn continually and remember distant memories. This suggests that continual synaptic plasticity can aid in learning from limited labeled data and adapt at long timescales. The model is trained on a sequence of tasks in a continual learning setup, enabling it to learn from sequential datasets and reduce catastrophic forgetting. In a continual learning setup, the model learns a sequence of tasks with associated training data and task-specific losses to prevent forgetting. The learned mapping approximates the true underlying function, mapping new inputs to target outputs for all learned tasks. Experiments were conducted on Nvidia GPUs with mini-batches and plain SGD optimization. In a continual learning setup, tasks are learned sequentially with specific training data and losses to prevent forgetting. Experiments were conducted on Nvidia GPUs with mini-batches and plain SGD optimization. The network is trained on tasks T n=1:10 with mini-batches of size 64 and optimized using plain SGD with a learning rate of 0.01. Training continues for at least 10 epochs with early stopping implemented. Hyperparameters for Permuted MNIST experiments include regularization values for different consolidation methods. For synaptic consolidation methods, hyperparameters were optimized through grid search using specific task sequences. In the Imbalanced Permuted MNIST problem, training samples were artificially removed based on random probabilities for each class in the dataset. The distribution of classes for tasks T n=1:10 is shown in Table 2. In the Imbalanced Permuted MNIST experiments, the distribution of classes for tasks T n=1:10 is shown in Table 2. The regularization hyperparameter \u03bb for each task-specific consolidation method is specified. In the Imbalanced Permuted MNIST experiments, the regularization hyperparameter \u03bb for each task-specific consolidation method is specified. For Online EWC, \u03bb = 400, for SI \u03bb = 1.0, and for MAS \u03bb = 0.1. Hyperparameter values were determined through grid search using a task sequence determined by a single seed. Similar hyperparameter settings were used for the Split MNIST experiments. For Online EWC, SI, and MAS, the regularization hyperparameters \u03bb were set to 400, 1.0, and 1.5 respectively. A grid search was conducted using a 5-task binary classification sequence to find the best hyperparameter combination. The Vision Datasets Mixture benchmark includes tasks from MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 datasets. The notMNIST dataset contains font glyphs for letters 'A' to 'J'. The dataset has 60,000 training and 10,000 testing grayscale images of size 28\u00d728. The notMNIST dataset contains font glyphs for letters 'A' to 'J' with 60,000 training and 10,000 testing grayscale images of size 28\u00d728. SVHN consists of digits '0' to '9' with 73,257 training and 26,032 testing color images of size 32\u00d732. CIFAR-10 has 50,000 training and 10,000 testing color images from 10 categories. The CNN architecture includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling operations before a final softmax output layer. The CNN model includes LeakyReLU nonlinearities, max-pooling, and a final softmax output layer. A multi-headed approach was used with separate \u03b7 parameters for each connection in the final output layer to improve stability and optimization. Different regularization hyperparameters were used for task-specific consolidation methods in the 5-Vision Datasets Mixture experiments. In the 5-Vision Datasets Mixture experiments, different regularization hyperparameters were used for task-specific consolidation methods. Hyperparameters included \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. Sensitivity analysis was performed on the Hebb decay term \u03b7 to evaluate its impact on test performance in continual learning setups. The effect of initial \u03b7 value on final test performance was analyzed for different benchmarks. Low \u03b7 values were found to alleviate catastrophic forgetting. Sensitivity analysis was also done for the Split MNIST problem. Average test accuracy for MNIST-variant benchmarks was presented in Table 4. Hebbian traces were updated for the next iteration. Initial learning rate values of plastic connections were considered. Fixed weights were initialized with He initialization. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to the final output layer of a neural network through plastic connections. The network was trained on the full CIFAR-10 dataset and sequentially on 5 additional tasks from the CIFAR-100 dataset. The simplicity of implementation using popular ML frameworks is emphasized. The DHP Softmax model, implemented in PyTorch, incorporates compressed episodic memory into the final output layer of a neural network. It was trained on the full CIFAR-10 dataset and sequentially on 5 additional tasks from CIFAR-100. The simplicity of implementation using popular ML frameworks is highlighted. Zenke et al. (2017b) reported that DHP Softmax outperforms Finetune on class-incremental learning tasks, sometimes even performing as well as training from scratch. Test accuracies were compared with results from von Oswald et al. (2019)."
}