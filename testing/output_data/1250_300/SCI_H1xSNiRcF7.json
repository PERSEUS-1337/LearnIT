{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. A novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes, improving optimization robustness. The novel hierarchical embedding model is inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions. It improves optimization robustness and demonstrates increased performance on various tasks, especially in cases of sparse data. Embedding methods have been a key technique in machine learning, converting semantic problems into geometric ones. Recent years have seen an interest in structured or geometric representations, such as Gaussian embeddings, order embeddings, and box embeddings. These geometric structures better express ideas of asymmetry, entailment, ordering, and transitive relations. The probabilistic Box Lattice model is focused on for its strong empirical performance in modeling transitive relations and complex joint probability distributions. Box embeddings are a generalization of order embeddings. Box embeddings are a generalization of order embeddings and probabilistic order embeddings that replace vector lattice ordering with overlapping boxes. However, the \"hard edges\" of boxes can cause issues for gradient-based optimization, especially in sparse data scenarios like market basket models. The curr_chunk discusses a new model inspired by the challenges of overlapping boxes in box embeddings. The model uses Gaussian convolution to smooth the hard edges of boxes, showing superior results in modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. The approach outperforms existing state-of-the-art results and improves in the pseudosparse regime. The work is related to order embeddings and box lattice models. The curr_chunk discusses various models related to order embeddings and box embeddings, including probabilistic and deterministic approaches. It also mentions hyperbolic space models for learning hierarchical embeddings. These models optimize energy functions and aim to capture tree structures efficiently. Our approach focuses on smoothing the energy landscape of models using Gaussian convolution, common in mollified optimization methods. This approach is increasingly used in machine learning models like Mollifying Networks, diffusion-trained networks, and noisy activation functions. We aim to learn an embedding model that maps concepts to subsets of event space, with an inductive bias suited for transitive relations and fuzzy concepts of inclusion and entailment. Our approach involves representing ontologies as geometric objects, utilizing order theory and vector and box lattices. A non-strict partially ordered set (poset) allows for elements to be incomparable, making it suitable for acyclic directed graph data with transitive relations. A lattice, a type of poset, ensures each subset has a unique least upper bound and greatest lower bound. This formalism is equipped with binary operations and additional elements for bounded lattices. A lattice is a type of poset where each subset has a unique least upper bound and greatest lower bound. In a bounded lattice, additional elements (top and bottom) denote the overall least upper bound and greatest lower bound. The lattice is equipped with binary operations \u2228 (join) and \u2227 (meet). A bounded lattice must satisfy specific properties, including absorption. The dual lattice can be formed by swapping the meet and join operations. A semilattice has either a meet or join operation, but not both. In the context of real numbers, \u2227 and \u2228 can represent min and max operations. A vector lattice, also known as a Riesz space, is a vector space with a lattice structure. The lattice structure in a vector lattice is defined by a partial order using the product order from the underlying real numbers. Order Embeddings of BID20 represent partial orders as vectors using the reverse product order, creating a dual lattice. The vector of all zeroes represents the origin, and objects become more specific as they move away from it. FIG0 illustrates a two-dimensional example of the Order Embedding vector lattice representation. The Order Embedding vector lattice representation uses a box lattice structure to associate concepts in a knowledge graph with two vectors, creating a partial order and lattice structure. The lattice meet is the largest box contained within both x and y, while the lattice join is the smallest box containing both x and y. The lattice structure in the Order Embedding vector representation uses boxes to associate concepts in a knowledge graph. Marginal probabilities of events are determined by the volume of boxes and their intersections under a probability measure. Gradient-based optimization for learning box embeddings faces challenges when incorrectly identifying concepts as disjoint. When learning box embeddings using gradient-based optimization, a key issue arises when two concepts are incorrectly labeled as disjoint, leading to zero gradient signal flow. This problem is especially problematic in sparse lattices, where most boxes have little to no intersection. To address this, the authors propose a surrogate function, but a more principled framework is needed to develop alternate measures that improve optimization and model quality. The authors propose a more principled framework to develop alternate measures that avoid gradient sparsity issues in box embeddings, improving optimization and model quality. They demonstrate this with a one-dimensional example showing the overlap of intervals before and after applying a smoothing kernel. The approach relaxes the assumption of \"hard edges\" in standard box embeddings to enable better optimization while preserving geometric intuition. The joint probability between intervals is rewritten as an integral of the product of indicator functions, addressing the issue of zero gradient signal flow in sparse lattices. The authors propose a framework using kernel smoothing to improve optimization in box embeddings. By replacing indicator functions with functions of infinite support, they address gradient sparsity issues. The joint probability between intervals is rewritten as an integral of the product of indicators, allowing for better optimization. The solution involves convolution with a Gaussian kernel and antiderivatives of the standard normal CDF. The integral with smoothed indicators f and g has a closed form solution. The solution involves the antiderivative of the standard normal CDF and the softplus function. In the zero-temperature limit, the formula simplifies to the original equation. However, multiplication of Gaussian-smoothed indicators does not meet the idempotency requirement. The multiplication of Gaussian-smoothed indicators does not meet the idempotency requirement for a function lattice. A modification of equation 3 can retain smooth optimization properties while ensuring p(x \u2227 x) = p(x). The hinge function m h satisfies a specific identity, but not the softplus function. In the zero-temperature limit, equations 3 and 7 are equivalent, but equation 7 is idempotent for overlapping intervals. This leads to defining probabilities using equation 7 instead of equation 3. Softplus function upper-bounds the hinge function, requiring normalization for values greater than 1 in experiments with different approaches. In experiments with different approaches, normalization is required for values greater than 1. The final probability is calculated by the product over dimensions. The softplus function shows better behavior for highly disjoint boxes compared to other functions. Our approach, compared in FIG2, shows better behavior for highly disjoint boxes with the softplus overlap function. Experiments on the WordNet hypernym prediction task demonstrate the performance of these improvements. The WordNet hypernym hierarchy contains 837,888 edges. Positive examples are randomly chosen, while negative examples are generated by swapping terms. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, requiring less hyper-parameter tuning. Further experiments are conducted using different numbers of positive and negative examples from the WordNet mammal subset to confirm the performance in the sparse regime. The dev and test sets contain 209 positive examples, with negative examples generated randomly. Models like OE baseline, Box, and Smoothed Box nearly match the full transitive closure with balanced data. Smoothed Box outperforms OE and Box on imbalanced data, which is crucial for real-world entailment graph learning. Experiments were conducted on the Flickr entailment dataset, using the same dataset as BID22 and applying the softplus function before calculating the volume of the boxes. In order to compare results accurately, the same dataset from BID22 was used with boxes constrained to a unit cube. The experimental setup was similar to BID22, with the addition of applying the softplus function before calculating box volumes. Results show a slight performance improvement, especially on unseen captions. The method was also applied to a market-basket task using the MovieLens dataset, predicting users' movie preferences based on ratings. 8545 movies were included in the dataset after filtering for significant statistics. In the MovieLens-20M dataset, 8545 movies with more than 100 user ratings were selected for analysis. Various models were compared, including low-rank matrix factorization and hierarchical embedding methods. The smoothed box embedding method showed superior performance, especially in Spearman correlation, a key metric for recommendation systems. Additional analysis on the model's robustness to initialization conditions was conducted. The smoothed box embedding method outperforms other baselines, particularly in Spearman correlation, a crucial metric for recommendation systems. The model is easier to train with fewer hyper-parameters and has achieved state-of-the-art results on various datasets, especially effective with sparse data and robust to poor initialization. The research on learning problems posed by complex embedding structures like unions of boxes is ongoing, exploring function lattices and constraint-based approaches. The research focuses on exploring function lattices and constraint-based approaches to learning complex embedding structures like unions of boxes. A proof of Gaussian overlap formula is presented for evaluating lattice elements with smoothed indicators. The MovieLens dataset is highlighted for its suitability for optimization by the smoothed model due to its proportion of small probabilities. The research explores function lattices and constraint-based learning for complex embedding structures like unions of boxes. Fubini's theorem is used to derive equations for optimization on the MovieLens dataset, known for its small probabilities. Experiments test the robustness of the smoothed box model to initialization, adjusting parameters to control the proportion of disjoint boxes. Results are presented in table 8. The research explores function lattices and constraint-based learning for complex embedding structures like unions of boxes. Results show that the smoothed box model is robust to disjoint initialization, outperforming the original box model significantly. Methodology and hyperparameter selection details are provided for each experiment, with code available for reproduction on GitHub. WordNet experiments involve evaluating the model on the development set and selecting the best parameters for optimal performance. The best development model is used to score the test set, with baseline models trained using parameters of BID22. Negative examples are generated randomly based on the ratio for each batch of positive examples. A single-layer LSTM architecture is used to read captions and produce box embeddings. The model is trained for a fixed number of epochs and tested on the development data at each epoch. Hyperparameters are determined on the development set, and the model is evaluated every 50 steps on the development set for MovieLens experiments. The model is evaluated every 50 steps on the development set for MovieLens experiments, and optimization is stopped if the best development set score fails to improve after 200 steps. The best development model is used to score the test set."
}