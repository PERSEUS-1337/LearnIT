{
    "title": "Hy7fDog0b",
    "content": "Generative models, like AmbientGAN, aim to model complex distributions using partial, noisy observations instead of fully-observed samples. This approach allows for the recovery of the true underlying distribution even with per-sample information loss. AmbientGAN shows significant improvements in training Generative Adversarial Networks (GANs), achieving higher inception scores on benchmark datasets. The method involves passing the generator's output through a simulated random measurement function for the discriminator to distinguish between real and generated measurements. Trained with our method, generative models can achieve significantly higher inception scores than baselines. Our approach, AmbientGAN, trains a generative model directly from noisy or incomplete samples, allowing for the recovery of the underlying distribution. This method involves passing the generator's output through a simulated random measurement function for the discriminator to differentiate between real and generated measurements. Our approach, AmbientGAN, trains generative models from noisy or incomplete samples by distinguishing real measurements from simulated ones. It can construct good generative models from noisy observations and low-dimensional projections with per-sample information loss. In the study, the authors explore different measurement models for generating images, such as convolving with a Gaussian kernel and adding noise, dropout, and random projection. They show that the distribution of measured images uniquely determines the distribution of original images, leading to the need for a generative model that matches the true distribution. Empirical results on the celebA dataset with randomly placed occlusions are also presented. In the study, the authors explore different measurement models for generating images, such as convolving with a Gaussian kernel and adding noise, dropout, and random projection. They show that the distribution of measured images uniquely determines the distribution of original images, leading to the need for a generative model that matches the true distribution. The authors defer the full exploration to Section 8, where they consider the celebA dataset with randomly placed occlusions and demonstrate the importance of incorporating the measurement process into GAN training for producing better samples. The measurements involve projecting images onto random lines to observe pixel sums. Two variants are considered, with AmbientGAN recovering underlying structure. Different approaches to neural network generative models exist, including autoregressive and adversarial methods. Adversarial frameworks are powerful for modeling complex data distributions like images, videos, and 3D models. Generative models have various applications, such as solving inverse problems and enhancing realism of synthetic data. Our work explores the utility of generative priors in solving inverse problems and enhancing realism of synthetic data. We demonstrate the use of GANs to translate images between domains and propose training stability improvements by operating generators and discriminators on different low-dimensional projections of data. Our approach is related to creating 3D object shapes from 2D projections and is a special case of the AmbientGAN framework. Our work explores using generative priors for solving inverse problems and improving realism of synthetic data. We utilize GANs to translate images across domains and suggest enhancing training stability by operating generators and discriminators on different low-dimensional data projections. This approach is connected to generating 3D object shapes from 2D projections and falls under the AmbientGAN framework. The task involves creating an implicit generative model of a real distribution p_r_x using a known distribution p_theta and a set of IID realizations from the distribution p_r_y. The main idea is to create an implicit generative model of a real distribution p_r_x using a known distribution p_theta and a set of IID realizations from the distribution p_r_y. This involves combining the measurement process with adversarial training to learn a generator G that approximates p_r_x using simulated random measurements on generated objects X_g. The discriminator is used to distinguish real measurements from fake ones. The AmbientGAN objective involves using a discriminator to distinguish real measurements from fake ones, with a quality function q(x) defined based on the discriminator output. The model is trained using stochastic gradients computed with backpropagation in each iteration, alternating between sampling Z, \u0398, and Y_r. The AmbientGAN learning framework involves training with stochastic gradients computed by sampling Z, \u0398, and Y_r in each iteration. Updates are alternated between parameters of D and G, and the approach is compatible with various GAN improvements. The framework can easily incorporate additional information, such as per sample labels, through conditional versions of the generator and discriminator. The measurement models used are tailored for 2D images in the AmbientGAN framework. Models include Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, and Pad-Rotate-Project. These models manipulate image pixels in various ways for data processing. The Pad-Rotate-Project function involves padding the image with zeros, rotating it by a random angle, and summing pixels along the vertical axis to create a measurement vector. Gaussian-Projection projects onto a random Gaussian vector, allowing for the recovery of the true underlying distribution in certain measurement models. The approach aims to show the invertibility of the mapping from image samples to measurements, ensuring consistency with the AmbientGAN framework. The lemma guarantees consistency with the AmbientGAN training procedure by assuming uniqueness of the true underlying distribution given the measurement distribution. Theorems show this assumption holds under various measurement models, allowing recovery of the true distribution with the AmbientGAN framework. The AmbientGAN framework can recover the true distribution under different measurement models like Gaussian-Projection and Convolve+Noise. Theorems ensure consistency with assumptions like finite pixel values, making it practical for image representation. The Block-Pixels model guarantees unique distribution induction with a dataset. The Block-Pixels measurement model assumes x \u2208 P n \u2282 R n with p as the probability of blocking a pixel. A unique distribution p r x can induce the measurement distribution p r y if p < 1. Three datasets used for experiments are MNIST, CelebA, and CIFAR-10. Generative models include conditional DCGAN and unconditional Wasserstein GAN with gradient penalty for the MNIST dataset. More details on architectures and hyperparameters can be found in the appendix. For the MNIST dataset, two GAN models are used: a conditional DCGAN and an unconditional Wasserstein GAN with gradient penalty. For the celebA dataset, an unconditional DCGAN is used. The CIFAR-10 dataset utilizes an Auxiliary Classifier Wasserstein GAN with gradient penalty. Different discriminator architectures are used for 2D and 1D projections. Baseline approaches are implemented to evaluate the AmbientGAN framework's performance on IID samples from the measurement distribution. The AmbientGAN framework evaluates baseline approaches on IID samples from the measurement distribution. A crude baseline ignores measurements, while a stronger baseline assumes invertible measurement functions, which is not the case in the AmbientGAN setting. In the AmbientGAN setting, the assumptions of observing \u03b8 i and invertible functions are violated. To address this, an approximate inverse function is used to obtain x i from y i = f \u03b8i (x i ). Different methods are employed for various measurement models to approximate the inverse functions. In the AmbientGAN setting, an approximate inverse function is used to obtain x from y = f(x). Various methods are employed for different measurement models to approximate the inverse functions, such as total variation inpainting, Wiener deconvolution, Navier Stokes based inpainting, and techniques for inverting the Radon transform. Only results with the AmbientGAN models are reported in this subset of experiments. Samples generated by baselines and models are presented for each experiment. In the AmbientGAN setting, results with the AmbientGAN models are reported. Samples generated by baselines and our models are shown for each experiment. Results on celebA with DCGAN and CIFAR-10 with ACW-GANGP are presented, demonstrating the challenges faced by baselines in inverting the measurements process. Our models produce images with good visual quality. Quality of measurements: Gaussian kernel and IID Gaussian noise used. Results on celebA with DCGAN show drowned measurements in noise. Models create coherent faces by observing parts of one image at a time. Pad-Rotate-Project models exhibit signal degradation. MNIST results with DCGAN show model learning rotation and reflection. Second model produces upright digits. Generated images are of good visual quality. The model prefers consistent orientation per class for easier learning. The rotation angle model produces upright digits but lacks details in face generation. Difficulty in learning complex distributions with 1D projections highlights the need for better methods in GAN training. Inception scores are used to quantify generative model quality in the AmbientGAN framework. In the AmbientGAN framework, Inception scores are used to evaluate model quality. Different experiments were conducted on the MNIST dataset, including blocking pixels and adding noise, showing the performance of AmbientGAN models compared to baselines. The study evaluated model performance on MNIST using Inception scores. Different noise levels were tested, showing that AmbientGAN models outperformed baselines. The Pad-Rotate-Project model had a low score of 4.18, while the Pad-Rotate-Project-\u03b8 model achieved a score of 8.12, close to the fully-observed case's score of 8.99. The second model, trained on 1D projections, achieves an inception score close to the fully-observed case. Total variation inpainting method is slow and not run on CIFAR-10 dataset. Results show the superiority of the approach over baselines, allowing for new generative models with incomplete datasets. The text discusses relaxing the requirement of having complete datasets by learning a distribution from incomplete, noisy measurements. This approach aims to construct new generative models for distributions without high-quality datasets. The lemma presented involves data distribution, parameter distribution, and measurement distribution, highlighting the uniqueness of the probability distribution. The text also mentions the importance of matching 1D marginals in the underlying distribution for projection vectors. The text discusses the uniqueness of the probability distribution in matching 1D marginals for projection vectors in the Convolve+Noise measurement model. Theorem states that there is a unique distribution that induces the measurement distribution. The text discusses the uniqueness of probability distribution in matching 1D marginals for projection vectors in the Convolve+Noise measurement model. It states that there is a unique distribution that induces the measurement distribution. The probability density functions are denoted by p with the variable name. A bijective map between X and Z is established through continuous transformations. The pdfs of X and Z are related through a Jacobian function. The pdf of Y, a sum of two random variables, is a convolution of individual pdfs. The reverse map from the measurement distribution to the sample distribution uniquely determines the true underlying distribution. The reverse map uniquely determines the true underlying distribution p x, concluding the proof. We define the empirical version of the vanilla GAN objective for a dataset of measurement samples. The optimal discriminator for the empirical objective is determined, and any optimal generator must satisfy p g y = p r y. The proof of Theorem 5.4 shows that the distribution p x can be recovered from measurements p y using a transition matrix A. If A is invertible, p x is recoverable from p y. The sample complexity is determined by the minimum eigenvalue magnitude of A. The distribution p x can be recovered from measurements p y using a transition matrix A. The sample complexity is determined by the minimum eigenvalue magnitude of A. For Block-Pixels measurement, images are divided into classes based on the number of zero pixels, and a transition matrix A is considered. The distribution p x can be recovered from measurements p y using a transition matrix A. For Block-Pixels measurement, images are divided into classes based on the number of zero pixels, and a lower triangular transition matrix A is considered. The diagonal entries of A are strictly positive with a minimum value of (1 \u2212 p) n, making A invertible with the smallest eigenvalue being (1 \u2212 p) n. The DCGAN model on MNIST follows a specific architecture. The DCGAN model on MNIST and WGANGP model on MNIST have different architectures. The DCGAN model uses a noise input with 100 dimensions and two linear layers followed by two deconvolutional layers. The discriminator also has two convolutional layers followed by two linear layers. Batch-norm is used in both generator and discriminator. On the other hand, the WGANGP model uses a latent vector of 128 dimensions and applies one linear layer and three deconvolutional layers in the generator. The discriminator has three convolutional layers followed by one linear layer, and batch-norm is not used. The unconditional DCGAN model on celebA has a latent vector with 100 dimensions and applies one linear layer followed by four deconvolutional layers. The ACWGANGP model on CIFAR-10 follows a residual architecture with a latent vector of 128 dimensions. The generator consists of a linear layer followed by three residual blocks, each containing conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator includes one residual block with two convolutional layers, followed by three residual blocks and a final linear layer. In the presented experiment, the AmbientGAN approach is shown to be robust to mismatches in the parameter distribution of the measurement function. The Block-Pixels measurement model is used on the MNIST dataset, where pixels are blocked with a certain probability to create measurements. The study explores scenarios where the parameter distribution is only approximately known, aiming for the training process to be resilient. The study utilizes the Block-Pixels measurement model on the MNIST dataset, blocking pixels with a probability of 0.5. AmbientGAN models are trained with various blocking probabilities, showing robustness to parameter distribution mismatch. The generator learned through AmbientGAN captures the data distribution well, as evidenced by improved sensing in compressed sensing tasks. Using AmbientGAN trained with corrupted samples on MNIST with p = 0.5, a plot comparing Lasso with AmbientGAN shows a reduction in the number of measurements needed for reconstruction error."
}