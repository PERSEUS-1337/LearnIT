{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization in deep neural networks. This paper discusses novel observations about dropout in DNNs with ReLU activations, leading to the proposed method \"Jumpout.\" Jumpout samples the dropout rate using a decreasing distribution to train local linear models better for nearby data points. Jumpout is a new method that adapts the dropout rate at each layer and training sample, improving performance on various datasets like CIFAR10 and ImageNet-1k. It addresses the overfitting problem in deep neural networks by normalizing the dropout rate and rescaling outputs for better consistency between training and test phases. Deep learning has achieved remarkable success on various machine learning tasks. Dropout is a technique to mitigate overfitting in deep neural networks by randomly setting hidden neuron activations to 0. However, tuning dropout rates for optimal performance can be challenging, as too high a rate can slow convergence and too low a rate may not improve generalization. Ideally, dropout rates should be tuned separately for each layer and training stage, but in practice, a single dropout rate is often used for all layers throughout training. In practice, a single dropout rate is often used for all layers throughout training, but this approach may lead to too much or too little perturbation for different layers and samples. Dropout is not compatible with batch normalization, as it randomly shuts down activated neurons. The drawbacks of dropout include perturbation inconsistencies for different layers and samples, as well as its incompatibility with batch normalization. Rescaling undropped neurons to match the original activation gain breaks normalization parameters, leading to poor behavior when used with BN. As BN is crucial for DNN stability, dropout is often dropped in favor of BN. Three modifications are proposed to address these issues, resulting in an improved version called \"jumpout.\" The drawbacks of dropout are addressed by modifications leading to an improved version called \"jumpout.\" This approach is motivated by observations on how dropout enhances generalization performance for DNNs with ReLU activations. Dropout randomly changes ReLU activation patterns, improving performance by training linear models to work for data points in nearby polyhedra. However, a fixed dropout rate may lead to dropping out a typical number of np units on a layer with n units. In \"jumpout,\" the dropout rate is a random variable sampled from a decreasing distribution, unlike fixed dropout rates in traditional methods. This ensures a higher probability of smaller dropout rates, leading to smoother transitions between data points in polyhedra as they move farther away. In \"jumpout,\" the dropout rate is adaptively normalized for each layer and training sample, ensuring consistent neural deactivation rates. This addresses the incompatibility issue between dropout and BN by rescaling outputs to maintain variance. This allows for the benefits of both dropout and BN in training a DNN. Jumpout, similar to dropout, randomly generates a 0/1 mask over hidden neurons to maintain variance during neural deactivation. It can be easily implemented without extra training and consistently outperforms dropout on various tasks. Previous approaches like \"standout\" have also addressed adaptive dropout rates. Recent work has proposed various methods for adaptive dropout rates in neural networks. BID1 introduced \"standout\" to adjust dropout rates for different layers and training stages using a binary belief network. BID24 extended this to learn adaptive dropout rates for individual neurons or groups. BID23 linked the Rademacher complexity of a DNN to dropout rates and suggested adapting them accordingly. In contrast, jumpout adjusts dropout rates based on ReLU activation patterns without additional models, offering low computational overhead. BID19 explored Gaussian dropout as a faster convergence optimization, while BID9 studied it from a variational perspective. In contrast to previous methods like Gaussian dropout and variational dropout, recent work introduces Jumpout, which adjusts dropout rates based on ReLU activation patterns without additional models, offering low computational overhead. It combines dropout with random skipping connections and trains two identical DNNs using different dropout masks to reduce the gap between training and test phases. Jumpout is a dropout variant that modifies dropout rates based on ReLU activation patterns without extra training costs. It combines dropout with random skipping connections and trains two identical DNNs with different dropout masks to bridge the training-test gap. The DNN architecture involves weight matrices, activation functions, input data points, output predictions, and hidden nodes. The DNN formalization can represent various architectures, including fully-connected networks and convolutional layers. Bias terms can be incorporated using dummy dimensions. Convolution is a sparse matrix multiplication, average-pooling is linear, and max-pooling acts as an activation function. Residual blocks can be represented by appending an identity matrix. Short-cut connections can be expressed in the DNN form. The DNN can be represented with short-cut connections in the form of Eqn. (1) using piecewise linear activation functions like ReLU. By modifying weight matrices based on activation patterns, the DNN can be simplified into a linear model as shown in Eqn. (2). The gradient \u2202x represents the weight vector of the linear model. The DNN with ReLU activations can be simplified into a linear model by modifying weight matrices based on activation patterns. The linear model is associated with activation patterns on all layers, defining a convex polyhedron. ReLU units are cost-effective and widely used, making them suitable for various tasks. Dropout improves generalization by extending local linear models to nearby convex polyhedra, inspiring modifications to the original dropout technique. The generalization performance of a complicated DNN is improved by extending local linear models to nearby convex polyhedra. Dropout prevents neuron co-adaptation, encourages diversity, and trains smaller networks for ensemble prediction, leading to improved generalization performance. Dropout improves generalization performance by smoothing each local linear model in a DNN with ReLUs. The input space is divided into convex polyhedra, where each data point behaves as a linear model. Training samples are dispersed among different polyhedra, creating distinct local linear models. Nearby polyhedra may correspond to different linear models due to activation patterns. The proposed method aims to address the fragility and lack of smoothness in deep neural networks by sampling a dropout rate from a truncated half-normal distribution. This approach helps to improve generalization performance by smoothing local linear models within the network. The method addresses fragility in deep neural networks by sampling a dropout rate from a truncated half-normal distribution, ensuring smoothness in local linear models. The Gaussian-based dropout rate distribution encourages generalization performance by controlling the amount of generalization enforcement through the standard deviation \u03c3. Smaller dropout rates are sampled with higher probabilities, enhancing the training sample's contribution to closer polyhedra's linear models. The Gaussian-based dropout rate distribution promotes smoothness in local linear models, enhancing generalization performance by controlling the dropout rate for each layer. Tuning dropout rates separately for different layers is ideal but computationally expensive, leading to the common practice of using a single global dropout rate. However, this approach is suboptimal as it results in varying effective dropout rates for active neurons in different layers. The effective dropout rate for active neurons in different layers can vary significantly. To address this, a normalized dropout rate is proposed by dividing the dropout rate by the fraction of active neurons in each layer. This approach ensures a consistent activation pattern and allows for precise control of dropout behavior across training stages. The dropout rate is tuned as a single hyper-parameter, affecting the scaling of neurons during training. Combining dropout layers with batch normalization (BN) can lead to unpredictable behavior due to variance differences between training and test phases. One proposed approach is to use a normalized dropout rate by dividing it by the fraction of active neurons in each layer for consistent activation patterns. The text discusses the impact of dropout layers on the scaling of neurons during training, particularly in relation to batch normalization (BN). It highlights the inconsistency between training and test phases due to dropout affecting the mean and variance of neurons. To address this, a proposed solution is to rescale the output to counteract dropout's impact on mean and variance scales. During testing, the trained BN is inconsistent due to dropout not being applied. To address this, rescaling the output y j can counteract dropout's impact on mean and variance scales. Rescaling dropped neurons by (1 \u2212 p j ) \u22121 recovers the original mean scale, while (1 \u2212 p j ) \u22120.5 is used for the variance scale. Taking into account E[w j ] for un-dropped neurons requires additional computation. Simple scaling methods cannot resolve the shift in both mean and variance, as mean rescaling does not solve the variance shift. During testing, rescaling the output y j can counteract dropout's impact on mean and variance scales. Different rescaling factors like (1 \u2212 p) \u22120.5 and (1 \u2212 p) \u22120.75 are used for variance and mean rescaling. The rescaling factor choice depends on the magnitude of the mean E(y j). Ideally, a trade-off point should be used to balance mean and variance rescaling efficiently. During testing, rescaling the output y j can counteract dropout's impact on mean and variance scales. A trade-off point (1 \u2212 p j ) \u22120.75 is proposed for efficient mean and variance rescaling in DNN with ReLU. The rescaling factor choice depends on the magnitude of the mean E(y j). Using dropout with BN in convolutional networks can potentially improve performance. Using dropout with Batch Normalization in convolutional networks can potentially improve performance. The original dropout with BN leads to a decrease in accuracy with dropout rates over 0.15, while rescaling dropout rates can lead to continuous improvement until 0.25. The proposed \"Jumpout\" algorithm overcomes original dropout limitations by sampling from a decreasing distribution for random dropout rates and normalizing adaptively. Jumpout is a novel approach that samples from a decreasing distribution for random dropout rates and normalizes adaptively based on the number of active neurons. It also scales the outputs differently during training to balance mean and variance shifts. Jumpout has three hyperparameters, but tuning only one (\u03c3) has shown good performance. In practice, setting p min = 0.01 and p max = 0.6 for Jumpout works consistently well across datasets and models. Jumpout has three hyperparameters, but tuning only \u03c3 has shown good performance. The input h j is considered as the features of layer j for one data point. For a mini-batch, estimating q + j separately for each data point or using the average q + j over data points is possible. Utilizing the latter option gives comparable performance with less computation and memory. Jumpout has similar memory cost as original dropout, with minimal computation required. Dropout and Jumpout are applied to various DNN architectures and compared on six benchmark datasets. In this section, dropout and jumpout are applied to different popular DNN architectures and compared on six benchmark datasets of varying scales. The DNN architectures include small CNNs, WideResNet, ResNet, and more, applied to datasets like CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet. The experiments follow standard settings and data preprocessing/augmentation. On ImageNet, pre-trained ResNet18 models are used and trained with dropout and jumpout for the same number of epochs. Starting from a pre-trained ResNet18 model, two copies were trained with dropout and jumpout for the same number of epochs. Jumpout consistently outperformed dropout on all datasets tested, including Fashion-MNIST and CIFAR10. Even on datasets with high test accuracy, jumpout still showed improvements. On CIFAR100 and ImageNet, jumpout achieved significant improvements without the need to increase model size. These results confirm the effectiveness of jumpout over dropout. Jumpout consistently outperformed dropout on various datasets, including Fashion-MNIST and CIFAR10, without the need to increase model size. A thorough ablation study confirmed the effectiveness of each modification, with jumpout achieving the best performance when all three modifications were applied together. Learning curves showed that jumpout with adaptive dropout rate per minibatch reached a good accuracy faster than dropout. Future improvements may include finding a better learning rate schedule specifically for jumpout. In the future, a better learning rate schedule method may be found for jumpout to reach final performance earlier than dropout. Dropout rescaling factors of (1 \u2212 p) \u22120.5 and (1 \u2212 p) \u22120.75 were applied to y, with (1 \u2212 p) \u22120.75 showing a nice trade-off between mean and variance rescaling in the CIFAR10(s) network."
}