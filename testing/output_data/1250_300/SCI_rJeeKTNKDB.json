{
    "title": "rJeeKTNKDB",
    "content": "The paper extends graph-to-graph translation methods for molecular optimization by incorporating multi-resolution representations and autoregressive graph decoding. The model significantly outperforms previous baselines in various molecular optimization tasks by improving biochemical properties of compounds through graph translation. The model is trained to translate molecular graphs into better forms by incorporating multi-resolution representations and autoregressive graph decoding. The task is challenging due to the vast space of potential candidates and complex dependencies in graph generation. Prior work utilized valid chemical substructures to generate graphs, but the approach remains limited. The proposed approach improves graph generation by integrating multi-resolution representations and autoregressive decoding. Unlike previous methods, the encoder-decoder model predicts substructure components and their attachments simultaneously, allowing for stronger dependencies between them. This new method aims to address limitations in existing approaches by enhancing the generation process. The encoder-decoder model in the proposed approach enhances graph generation by incorporating multi-resolution representations and autoregressive decoding. It predicts substructure components and their attachments simultaneously, enabling strong dependencies between them. The decoding process is more efficient by breaking down each generation step into smaller hierarchical steps to avoid combinatorial explosion. Conditional translation is also supported, allowing for different criteria combinations at test time. Interleaving the tree and graph decoders prevents the generation of invalid junction trees that cannot be assembled into any molecule. Our new model improves molecular optimization tasks by interleaving substructure prediction with attachments, outperforming previous methods by 3.3% and 8.1% on QED and DRD2 tasks. It runs 6.3 times faster during decoding and shows significant advancements in discovering molecules with desired properties. Our new model improves molecular optimization tasks by interleaving substructure prediction with attachments, outperforming previous methods by 3.3% and 8.1% on QED and DRD2 tasks. It runs 6.3 times faster during decoding and shows advancements in discovering molecules with desired properties. Various approaches have been adopted for generating molecular graphs, including methods that generate molecules based on SMILES strings and generative models that output adjacency matrices and node labels of graphs at once. Ma et al. (2018) and You et al. (2018b) developed generative models for graphs, while Samanta et al. (2018) and Liu et al. (2018) focused on molecule generation. Kajino (2018) used hypergraph grammar for molecule generation. Our work is related to Jin et al. (2018) who used a two-stage procedure for graph realization, but our method predicts substructures and attachments jointly with an autoregressive decoder. Our method differs from previous approaches by jointly predicting substructures and their attachments using an autoregressive decoder. It represents molecules as hierarchical graphs, spanning from atom-level graphs to substructure-level trees. This work is closely related to methods that learn to represent graphs hierarchically. Our approach involves predicting substructures and their attachments using an autoregressive decoder, representing molecules hierarchically from atom-level to substructure-level. This differs from previous methods that learn to represent graphs hierarchically. Our approach involves predicting substructures and their attachments using an autoregressive decoder in a graph translation task to improve chemical properties. The molecule X is represented hierarchically by a graph with substructure, attachment, and atom layers for each decoding step. Our model represents molecule X with a hierarchical graph consisting of substructure, attachment, and atom layers. The nodes in the graph are encoded into substructure vectors, attachment vectors, and atom vectors for prediction. Substructures are defined as subgraphs of the molecule induced by atoms and bonds, with two types considered: rings and bonds. The vocabulary of substructures is denoted as S. The paper discusses extracting substructures from a molecule to construct a substructure tree, with nodes representing rings and bonds. The substructure tree is used to generate a molecule by incrementally expanding it in a depth-first order. Predictions are made for new substructures and their attachment to the graph. The model encodes input X to make these predictions. The model predicts new substructures and their attachment to the graph based on the encoding of input X. It first predicts if a new substructure should be created, then determines the type of substructure, and finally decides how it should be attached to the existing structure. The model predicts new substructures and their attachment to the graph based on the encoding of input X. It first predicts the atoms to be attached, then finds the corresponding atoms in the substructure S k. The predictions are based on atom pairs and their representations learned by the decoder, providing an autoregressive factorization of the distribution over the next substructure and its attachment. The model predicts new substructures and their attachment based on atom representations learned by the decoder. The decoding process depends on previous steps, and attachments affect subsequent predictions. The encoder represents a molecule X with a hierarchical graph to support decoding. The graph includes an atom layer showing atom connections and labels, and an attachment layer. The atom layer of molecule X represents atom connections with labels for atom type and charge. The attachment layer is derived from the substructure tree, providing information for attachment prediction. The substructure layer, identical to the substructure tree, is crucial for substructure prediction in the decoding process. The attachment points in each configuration must be consecutive. The substructure layer provides essential information for substructure prediction in the decoding process. Edges connect atoms and substructures between layers to propagate information. The hierarchical graph HX is encoded by a hierarchical message passing network (MPN) with three layers. The MPN encodes the atom layer first, propagating message vectors between atoms for T iterations. The hierarchical encoder encodes the atom layer, attachment layer, and substructure layer of the graph HX using a message passing network. The network propagates message vectors between atoms and nodes for T iterations to compute atom representations, substructure representations, and substructure predictions. The hierarchical encoder computes node representations by concatenating embeddings in each layer. Message passing is used to obtain substructure representations. The decoder uses the same architecture to encode the hierarchical graph. Training data includes molecular pairs for generating diverse outputs. A variational translation model is extended to include an additional input for generating molecule Y. In order to generate molecule Y from molecule X and z, a variational translation model is used with an additional input z indicating the intended mode of translation. The model is trained using variational inference and encodes structural changes from X to Y to compute and sample z. The latent code z is passed to the decoder to reconstruct output Y. During testing, users cannot change the behavior of the trained model. The model extends to handle conditional translation by allowing users to specify desired criteria during testing. This involves feeding the translation criteria as input to the process, enabling control over the outcome. The experimental design evaluates the model on single-property optimization tasks and a novel conditional optimization task where desired criteria are provided as input. The model allows for conditional translation by inputting desired criteria during testing, ensuring the output meets specific requirements. It is evaluated on single-property optimization tasks and a new conditional optimization task where criteria are provided as input. The model prevents arbitrary translations by requiring a certain molecular similarity between input and output molecules. The model allows for conditional translation by inputting desired criteria during testing, ensuring the output meets specific requirements. Evaluation metrics include translation accuracy and diversity, with a focus on improving drug-likeness and activity. Multiple translations are generated for each input molecule, with the final output selected based on property improvement and molecular similarity. The HierG2G method translates molecule X i by sampling different latent codes and selecting compound Y i with the highest property improvement and satisfying sim(X i , Y i ) \u2265 \u03b4. Translation success rate is reported for other tasks, with diversity measured by average pairwise Tanimoto distance. HierG2G is compared against baselines including GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE for molecule generation. Our model, compared to JTNN and CG-VAE, generates molecules by decoding atom by atom and optimizing properties in the latent space. We also developed AtomG2G for atom-based generation, achieving state-of-the-art results on translation tasks. The encoder encodes the atom-layer graph, and the decoder predicts atom and bond types autoregressively. Our model significantly outperforms JTNN in single-property optimization. Our model achieves state-of-the-art results on translation tasks, outperforming JTNN and AtomG2G in both translation accuracy and output diversity. It runs 6.3 times faster than JTNN during decoding and shows over 10% improvement on the DRD2 task compared to AtomG2G. Our hierarchical model outperforms other translation methods like Seq2Seq, JTNN, and AtomG2G, especially on tasks with strong constraints. Our model outperforms other models in translation accuracy and output diversity, especially on tasks with strong constraints. Training on a small subset of examples shows the effectiveness of our conditional translation setup. Ablation studies reveal the importance of structure-based decoding, with the DRD2 task benefiting more from this approach due to biological target binding dependencies. In this study, the model's performance is analyzed through various experiments. Removing hierarchies in the encoder and decoder leads to a decrease in translation accuracy. Replacing the LSTM MPN with the original GRU MPN results in a slight decrease in performance, but the model still outperforms JTNN. The hierarchical graph-to-graph translation model developed in the paper generates molecular graphs efficiently. Our hierarchical graph-to-graph translation model, utilizing the LSTM MPN architecture, outperforms previous models in generating molecular graphs. The model is fully autoregressive, learning coherent multi-resolution representations. Experimental results demonstrate its superiority under various settings. The message passing network MPN \u03c8 over graph H is defined, and an attention layer with a bilinear function is utilized. The AtomG2G decoding process involves predicting new atoms attached to frontier nodes in the queue Q. AtomG2G is an atom-based translation method that directly compares to HierG2G. The training set and substructure vocabulary sizes for each dataset are listed in Table 3. The multi-property optimization dataset is created by combining the training sets of QED and DRD2 tasks. The test set includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters are set for HierG2G. The training and test set for multi-property optimization includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters are set for HierG2G and AtomG2G models, with specific dimensions and regularization weights. CG-VAE models are used for generating molecules and predicting properties. Each compound is translated at test time following a specific procedure. The CG-VAE models are used for molecule generation and property prediction. Three models are created for logP, QED, and DRD2 optimization tasks. At test time, compounds are translated using a procedure similar to Jin et al. (2018). Gradient ascent is performed over the latent representation to maximize property scores, with KL regularization weight kept low for meaningful results. Ablation studies show modifications to the decoder, including using an atom-based decoder and adjusting layer dimensions. The decoder of AtomG2G was modified to include both atom and substructure vectors in the input of the decoder attention. The hidden layer and embedding layer dimensions were set to 300 to match the original model size. Experiments were conducted to reduce the number of hierarchies in both the encoder and decoder MPN, resulting in two-layer and one-layer models with adjustments made to the hidden layer dimension to match the original model size."
}