{
    "title": "Byekm0VtwS",
    "content": "Uncertainty is a crucial aspect of intelligence, aiding in flexibility and creativity. Neuromorphic computing chips, which mimic the brain, have uncertainty but current neural networks do not consider this, leading to lower performance on these chips compared to CPUs/GPUs. A proposed uncertainty adaptation training scheme (UATS) improves neural network performance on uncertain chips, highlighting the importance of uncertainty in human thinking and intelligent systems. The essence of human thinking involves uncertainty reasoning, with fuzziness and stochasticity being key aspects. Fuzziness helps the brain efficiently process information by ignoring irrelevant details, while stochasticity enables creativity and adaptability. These characteristics are lacking in current artificial intelligence systems like deep neural networks, which use 32-bit or 64-bit floating numbers for weights and activations. Some researchers suggest that 8-bit integers are sufficient for many applications. Additionally, existing AI systems produce consistent results regardless of how many times they are run, albeit with minor variations. The 8-bit integer is found to be sufficient for many applications in weights and activations. Methods like network quantization and Bayesian network address issues, while neuromorphic computing chips supplement uncertainty in DNN. Crossbar structures with nanotechnology devices enable efficient VMM with additional storage capability. Computing in memory architecture alleviates memory bottleneck in AI systems. The crossbar structure with nanotechnology devices enables efficient VMM with additional storage capability, relieving the memory bottleneck in AI systems. Neuromorphic computing chips supplement uncertainty in DNN, utilizing fuzziness from analog to digital converters and stochasticity from NVM devices. The uncertainty in neuromorphic computing chips arises from analog to digital converters (ADCs) and NVM devices, leading to fuzziness and stochasticity. The VMM result is a summarization of currents, converted by ADC for data transfer. The stochasticity of NVM devices is due to random particle movement, affecting conductance and output current variability. A training scheme utilizing stochasticity is proposed to enhance chip performance. Various NVM devices, such as phase change memories, contribute to the variability in neuromorphic computing chips. In this work, a training scheme utilizing stochasticity is proposed to enhance the performance of neuromorphic computing chips. Different types of NVM devices exhibit varying levels of stochasticity due to intrinsic physical mechanisms. The Gaussian distribution is used to model device stochasticity, with the mean representing the conductance value of the stable state. The variance of the distribution is typically correlated with the mean. Simplifying the relationship between mean and variance, a standard model is assumed for various devices. The Gaussian distribution is used to model device stochasticity in neuromorphic computing chips. The standard deviation is assumed to be linearly correlated to the mean. Conductances below a minimum value are cut off. The model includes stochastic conductance, a mean value, variance, stable state conductance, and device fuzziness. Writing the conductance of each device is crucial for AI applications. The conductance of each device in a neuromorphic computing chip is essential for AI applications. The mapping process determines the target conductance based on neural network weights. To express weights, the difference between two device conductances is used. Lower conductances are preferred for energy efficiency. However, due to device stochasticity and circuit fuzziness, accurate conductance writing is challenging. The conductance of devices in a neuromorphic computing chip is crucial for AI applications. The accuracy of writing conductance is hindered by device stochasticity and circuit fuzziness. A model using Gaussian distribution is employed to describe fuzziness, with parameters like fuzzy target conductance and level of devices fuzziness. Uncertainty impacts the performance of a DNN programmed into the chip directly. The uncertainty in neuromorphic computing chips can affect the performance of a DNN, leading to decreased classification accuracy. However, using the uncertainty adaptation training scheme (UATS) can help alleviate this issue. The core idea of UATS is to incorporate uncertainty into the training process to guide neural networks in learning how to handle uncertainty. This involves introducing a stochasticity model in the feed forward process, where a sample of random variable is used for calculations instead of the actual weight. The fuzziness model is also introduced to address the conductance of stable states. The uncertainty adaptation training scheme (UATS) incorporates stochasticity and fuzziness models to address uncertainty in neuromorphic computing chips. It involves replacing weights with random variables during training and calculating conductances of stable states. Additionally, the loss function is calculated based on the average output of multiple feed forward processes. UATS was evaluated on various models and datasets, including the MNIST dataset. The uncertainty adaptation training scheme (UATS) was tested on multiple models and datasets, including the MNIST dataset. Different levels of uncertainty were applied to MLP and CNN models, with 20 trials conducted for each model. The test errors were calculated, showing that without UATS, uncertainty increased the errors in both models. The stochasticity model was used to simulate read variation in 20 trials for each model with different uncertainty levels. Without UATS, uncertainty increased test errors for both MLP and CNN models. The CNN model (LeNet-5) performed best without uncertainty but was most affected by it. UATS significantly improved accuracies in both retraining and fine-tuning experiments, with retraining showing better results. The number of epochs is 100. UATS improves accuracies with the same uncertainty level in retraining and fine-tuning experiments. Results show UATS outperforms finetuning. UATS achieves lower error rates than ideal cases on CIFAR-10 dataset with ResNet-44 model. UATS acts as a regularization method for DNN training, especially effective with more layers. Bayesian network is useful for uncertain neural networks but requires controllable weight distributions. The uncertainty in intelligent systems is crucial. Bayesian network is a useful method for uncertain neural networks, but controlling weight distribution is challenging. Various distributions have been explored to model device stochasticity, with similar network performance despite different distributions. VMM transforms individual device distributions into random parameters. The VMM transforms individual device distributions into random parameters, reducing the need for a large number of random numbers in UATS computation. Methods to reduce random number requirements include sampling weights for every input or batch, and using the uncertainty model of VMM results instead of weights, leading to accelerated simulation speed with similar results."
}