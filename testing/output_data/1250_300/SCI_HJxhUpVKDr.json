{
    "title": "HJxhUpVKDr",
    "content": "In the context of multi-task learning, neural networks with branched architectures are often used to jointly tackle tasks. These networks start with shared layers, then tasks branch out into their own layers. Prior methods for determining layer sharing have been ad hoc or expensive. This paper proposes a principled approach to automatically construct branched multi-task networks based on task affinities. Shallow layers are task-agnostic, while deeper layers become more task-specific. Experimental analysis shows that this method consistently yields high-performing networks for a given budget. Our method for constructing multi-task networks involves task-agnostic shallow layers and task-specific deeper layers. Experimental analysis demonstrates that our approach consistently produces high-performing networks within a given budget. This contrasts with traditional deep neural networks trained in isolation, as humans and biological data processing also exhibit multi-tasking strategies. Multi-task networks aim to enhance generalization and processing efficiency by jointly learning related tasks. They offer advantages such as lower memory footprint, faster inference speed, and potential outperformance of single-task networks. Designing multi-task networks involves the challenge of determining shared layers among tasks, with the number of possible configurations growing rapidly with the number of tasks. In designing multi-task networks, a significant challenge is deciding on shared layers among tasks. The number of possible network configurations grows quickly with the number of tasks, making it difficult to define the optimal architecture. Researchers have explored alternatives like routing, stochastic filter grouping, and feature partitioning. Previous works on hard parameter sharing shared initial layers in the network, branching out tasks simultaneously, which can hurt performance if tasks are suboptimally grouped. In this paper, a novel approach is proposed to determine the degree of layer sharing between tasks in a network, based on measurable levels of task affinity. Previous methods of sharing layers in multi-task networks have led to suboptimal task grouping and negative transfer of information. The proposed approach aims to eliminate the need for manual exploration by considering task relatedness and performance metrics. The proposed approach uses representation similarity analysis (RSA) to assess task affinity in a neural network, constructing a branched multitask network automatically. It clusters similar tasks together and separates dissimilar tasks to reduce negative transfer. The method allows for trading network complexity for task similarity and shows superior multi-task performance with fewer computational resources. Multi-task learning involves jointly learning multiple tasks under a single model, with early approaches using sparsity constraints to select shared features. Clustering tasks based on similarity can prevent negative transfer. In deep learning, MTL models can use soft or hard parameter sharing mechanisms. Cross-stitch networks and sluice networks are examples of soft parameter sharing methods. In multi-task learning, various methods like cross-stitch networks and sluice networks are used for parameter sharing among tasks. Soft parameter sharing allows for feature sharing among tasks, while hard parameter sharing divides parameters into shared and task-specific ones. Hard parameter sharing often involves a shared encoder and task-specific decoders. Multilinear relationship networks extend this framework by incorporating tensor normal priors on fully connected layers. In contrast to existing approaches like cross-stitch and sluice networks, our branched multi-task networks determine layer sharing based on task affinities, rather than ad hoc branching points. This method clusters tasks using feature affinity scores, leading to more optimal task groupings compared to example difficulty-based approaches. Our method clusters tasks based on feature affinity scores, determining the tree structure offline for optimal task groupings. It outperforms existing approaches, especially on challenging datasets like Taskonomy. Neural architecture search aims to automate network architecture construction, with different algorithms characterized by search space, strategy, and performance estimation. Most existing works are limited to task-specific models, as layer sharing must be jointly optimized with layer types in MTL. Neural architecture search (NAS) for multi-task learning (MTL) is challenging due to the need to jointly optimize layer sharing with layer types. Recent works have explored alternatives like evolutionary search, routing, stochastic filter grouping, and feature partitioning to reduce computation burden. Unlike traditional NAS, these methods do not start from scratch but use predefined backbone networks with automatically determined layer sharing schemes. Transfer learning is loosely related to measure task affinity levels by leveraging knowledge from one task to another. Transfer learning is used to measure levels of task affinity by applying knowledge from one task to another. A more efficient alternative to task transfer learning taxonomy was proposed by Dwivedi & Roig (2019), using RSA to compute correlations between models pretrained on different tasks. Loss weighting challenges in jointly learning multiple tasks have been addressed through methods like homoscedastic uncertainty weighting, gradient normalization, and dynamic task prioritization. In this paper, the focus is on jointly solving multiple tasks within a computational budget by utilizing a shared encoder-decoder architecture with a tree-like layer sharing structure. The authors aim to find a Pareto optimal solution for N different tasks, using a uniform loss weighing scheme for experiments. Various methods such as gradient normalization and dynamic task prioritization have been proposed to address challenges in multi-task learning. The proposed method involves a shared encoder-decoder architecture with a tree-like layer sharing structure to jointly solve multiple tasks within a computational budget. Task affinity scores are derived to group related tasks together in the same branches of the tree, leading to the automated construction of a branched multi-task network. RSA is used to measure task affinity at predefined locations in the encoder. The proposed method utilizes RSA to measure task affinity scores at predefined locations in the sharable encoder. This process involves calculating representation dissimilarity matrices for features at these locations using images, resulting in a three-dimensional tensor. The output is a branched multi-task network, similar to NAS techniques, where tasks are grouped based on minimal dissimilarity within a computational budget. The proposed method uses RSA to measure task affinity scores at predefined locations in the sharable encoder. Tasks are then grouped in the same or different branches of a branched multi-task network based on minimal dissimilarity within a computational budget. The method utilizes RSA to assess task affinity scores at specified locations in the shared encoder. Tasks are grouped in the same or different branches of a branched multi-task network based on minimal dissimilarity within a computational budget. The decoder contains task-specific operations and is smaller in size compared to the encoder. Operations such as fully connected layers, softmax, and upscaling are part of the task-specific decoder. Task affinities are calculated by comparing representation dissimilarity matrices of single-task networks trained at selected locations in the encoder. The method utilizes RSA to assess task affinity scores at specified locations in the shared encoder. Task affinities are calculated by comparing representation dissimilarity matrices of single-task networks trained at selected locations. This involves filling a tensor with dissimilarity scores between feature representations, using the Pearson correlation coefficient. The similarity between the RDMs of different single-task networks is measured using Spearman's correlation coefficient. The method uses RSA to evaluate task affinity scores by comparing representation dissimilarity matrices of single-task networks at specific locations in the shared encoder. The similarity between RDMs is measured using Spearman's correlation coefficient, resulting in a symmetrical matrix of task affinity. The focus is on features used to solve tasks rather than task examples' difficulty. The sharable encoder's layers are shared among tasks based on a computational budget, with each layer represented as a node in a tree structure. The method evaluates task affinity by comparing representation dissimilarity matrices of single-task networks in the shared encoder. The encoder is split into branches to separate tasks, with task-specific decoders at the leaves of the tree. Task dissimilarity scores are minimized to build a branched multi-task network, allowing offline determination of task clustering. The task affinity scores are calculated a priori to determine task clustering offline. All possible trees within a computational budget are enumerated, and the tree minimizing task dissimilarity score is selected. The task dissimilarity score of a tree is defined as the average maximum distance between dissimilarity scores of elements in each cluster. The procedure aims to find an optimal task grouping globally by considering clustering costs at all depths. A top-down approach is proposed for deriving the tree, starting at the outer layer and performing spectral clustering at each step for different numbers of groups. The text discusses a method for deriving task groupings in a network using a top-down approach and spectral clustering. The proposed method is evaluated on various multi-tasking datasets, including the Cityscapes dataset for urban scene understanding. The dataset contains real images taken in European cities and involves dense prediction tasks. The Cityscapes dataset focuses on urban scene understanding with tasks like semantic segmentation, instance segmentation, and monocular depth estimation. A ResNet-50 encoder with dilated convolutions and a Pyramid Spatial Pooling decoder are used. Results show task affinity decreasing in deeper layers of the model. The task affinity decreases in the deeper layers of the ResNet-50 model, with features becoming more task-specific. Performance of task groupings generated by the method is compared with other approaches, showing higher performance within a given computational budget. The proposed method for task grouping achieves higher performance within a given computational budget compared to other approaches. It can select the best performing task grouping for a fixed budget, striking a balance between performance and number of parameters. The method outperforms cross-stitch networks and NDDR-CNNs in terms of this trade-off. Soft parameter sharing becomes less scalable as the number of tasks increases greatly. The Taskonomy dataset contains annotated images for various tasks. The Taskonomy dataset contains annotated images for 26 tasks, including scene categorization, semantic segmentation, edge detection, monocular depth estimation, and keypoint detection. The dataset is split into 275k train, 52k validation, and 54k test images. The architecture used is based on ResNet-50 with a fully convolutional decoder for pixel-to-pixel prediction tasks. Task affinity is measured after every ResNet block, and the method limits itself to three task groupings due to the increased number of tasks. The Taskonomy experiments focus on training setup and results. Task affinity is measured after each ResNet block, with three architectures generated based on parameter budget. Results show outperformance of models compared to previous methods. Multi-task performance is affected by task groupings, with some models struggling with larger task dictionaries. The study shows that cross-stitch networks and NDDR-CNNS struggle with larger task dictionaries, leading to decreased performance with increased parameters. In contrast, branched multi-task networks handle diverse tasks positively, demonstrating consistent performance across various scenarios and datasets. Their approach separates dissimilar tasks to limit negative transfer, allowing for simultaneous solving of heterogeneous tasks. This is a departure from existing approaches tailored for specific cases, showing stable performance across different experimental setups. The CelebA dataset contains over 200k real images of celebrities labeled with 40 facial attribute categories. Various architectures are optimized for different parameter budgets, showing different accuracies on the test set. The CelebA dataset contains over 200k real images of celebrities labeled with 40 facial attribute categories. The training, validation, and test sets contain 160k, 20k, and 20k images respectively. The prediction of each facial attribute is treated as a single binary classification task. The study uses a branched multi-task network that outperforms earlier works on the CelebA test set. Our branched multi-task networks outperform earlier works on the CelebA test set, including the VGG-16 baseline and the ResNet-18 model from Sener & Koltun (2018). The Thin-32 model shows comparable performance to VGG-16 while using significantly fewer parameters. Additionally, the Thin-64 model performs better than the ResNet-18 model with a uniform loss weighing scheme and matches the state-of-the-art ResNet-18 model with a lower parameter count. This paper introduces a method for constructing effective task groupings in branched multi-task networks. The paper introduces a method for constructing branched multi-task networks that outperform existing models like ResNet-18 with fewer parameters. The approach leverages task affinities for layer sharing, optimizing multi-tasking performance while reducing the number of parameters. Extensive experiments show consistent results across various scenarios and datasets. After experimenting with re-implementing the MTAN model using a ResNet-50 backbone and extensive hyperparameter tuning, meaningful results were not achieved on the Cityscapes dataset for all three tasks jointly. Results were only shown for semantic segmentation and monocular depth estimation. The setup included rescaling input images to 256 x 256 pixels, using a ResNet-50 encoder with specific modifications, and employing various loss functions for different tasks. The model uses Kaiming He's initialization for encoder and decoder, with L1 loss for depth, edge detection, and keypoint tasks. Scene categorization is learned with KL-divergence loss. Performance is measured by overlap in top-5 classes. Multi-task models have task weights and linearly rescaled heatmaps. Single-task models use Adam optimizer, decayed learning rate, 120000 iterations, batch size 32, no data augmentation, and weight decay. Baseline and branched multi-task models follow similar optimization procedures. The baseline and branched multi-task models use the same optimization procedure as single-task models. The architectures generated by the method are shown in figures, with predictions for qualitative evaluation. Cross-stitch networks reuse hyperparameter settings and the CNN architecture is based on the VGG-16 model. The CNN architecture is based on the VGG-16 model with a minimum number of convolutional features. The fully connected layers contain 2 \u00b7 \u03c9 features. The model is trained using stochastic gradient descent with momentum 0.9 and initial learning rate 0.05, with batches of size 32 and weight decay 0.0001. Training is done for 120000 iterations with the learning rate divided by 10 every 40000 iterations, using a sigmoid cross-entropy loss function with a uniform weighing scheme."
}