{
    "title": "H1-IBSgMz",
    "content": "Self-normalizing discriminative models approximate class probabilities without computing the partition function, benefiting neural network classifiers. Recent studies show language models trained with Noise Contrastive Estimation exhibit self-normalization. This study provides a theoretical explanation by viewing NCE as a low-rank matrix approximation. Empirical comparison with explicit approaches reveals a negative correlation between self-normalization and other factors. The ability of statistical language models to estimate word probabilities is crucial for NLP tasks. RNN language models have scalability issues due to softmax computation, impacting training and testing. Various methods like importance sampling, hierarchical softmax, BlackOut, and NCE have been proposed to address this issue. NCE has been used to train neural LMs with large vocabularies. At test time, self-normalization is proposed to address the high run-time complexity of predicting normalized probabilities. A self-normalized discriminative model is trained to produce near-normalized scores, allowing for the avoidance of costly exact normalization at test time without sacrificing prediction accuracy. The self-normalization approach aims to produce near-normalized scores during training, allowing for the avoidance of costly exact normalization at test time without sacrificing prediction accuracy. Two main approaches are used: explicit self-normalization using softmax and encouraging the normalization term to be close to one, and an alternative approach based on NCE where fixing the normalization term to a constant still results in self-normalization. The study provides a theoretical justification for the self-normalization property of NCE, showing that it can be self-normalizing without explicitly imposing it on training instances. The empirical focus is on language modeling, comparing NCE's self-normalization performance with explicit self-normalization on two datasets. The study focuses on the empirical contribution of NCE in language modeling. Results show that models with better perplexities may have worse self-normalization properties. The NCE algorithm transforms parameter learning into a binary classifier training problem for efficient language model training. The NCE approach assumes word sampling from a mixture distribution with noise samples being more frequent. A binary classifier is trained to decide which distribution was used to sample words. The NCE approach involves word-context co-occurrences in the learning corpus, with noise samples drawn from the word unigram distribution. The normalization factor Zc is computed using the word vocabulary. Setting Zc=1 at train time doesn't affect model performance, but at test time, log p(w|c) still requires explicit normalization. An alternative interpretation of NCE as a low-rank matrix approximation is presented. The NCE language modeling algorithm involves computing the score w \u00b7 c + b w over all vocabulary words to obtain a proper distribution. An alternative interpretation of NCE as a low-rank matrix approximation explains the self-normalization property. The Pointwise Conditional Entropy (PCE) matrix of a conditional word distribution is defined as pce(w, c) = log p(w|c). The NCE modeling can be written as a matrix m(w, c) = log p nce (w|c) = w\u00b7 c+b w \u2212log Z c, with the rank of the matrix m being at most d + 2. The NCE score is a corpus-based approximation of an expectation-based score. The NCE score is a corpus-based approximation of an expectation-based score. The score reaches its global maximum at the PCE matrix, with a concrete interpretation at its maximum point. The function S nce (m) has a clear interpretation at the PCE matrix. The NCE score reaches its global maximum at the PCE matrix, with a clear interpretation. The Jensen-Shannon divergence between distributions is used to find the best approximation of the PCE matrix in the NCE algorithm. The NCE algorithm aims to minimize the difference between distributions by finding the best low-rank approximation of the PCE matrix. Unlike traditional NCE, our model does not require a normalization factor. Previous studies have used different approaches for setting the normalization factor Z, which can impact the NCE score. The NCE algorithm aims to minimize distribution differences by approximating the PCE matrix. Using a fixed value for Z at train time can improve stability and convergence speed. The NCE model is self-normalized, and close to the PCE matrix. The NCE algorithm aims to approximate the PCE matrix and ensure self-normalization. Theorem 2 shows that if the matrix m is close to the PCE matrix, the NCE model is approximately self-normalized. The analysis demonstrates that the LM learned by NCE is self-normalized. The NCE algorithm aims to approximate the PCE matrix for self-normalization. The standard LM learning method is not self-normalized, but BID4 proposed a method to encourage self-normalization by penalizing deviation. BID0 provided an efficient approximation by eliminating costly computations, showing that self-normalization can be achieved on a subset of the corpus. The NCE algorithm approximates the PCE matrix for self-normalization. BID0 proposed a method to encourage self-normalization by penalizing deviation, showing it can be achieved on a subset of the corpus. Importance sampling (IS) is an efficient alternative closely related to NCE, where words are sampled based on a given context. The IS objective function in language modeling cancels out the normalization factor Zc, eliminating the need to estimate it during training. Unlike NCE, IS requires explicit computation of the normalization factor at test time. The study compares self-normalizing NCE-LM with non-self-normalizing softmax LM, showing the need for explicit self-normalization in language models. The study compares self-normalizing NCE-LM with non-self-normalizing softmax LM. The models were initialized to be self-normalized at the beginning. NCE-LM used k=100 negative samples. Implementation details followed previous work using LSTM models. Two datasets were used for evaluation, including the Penn Tree Bank dataset. The evaluation of self-normalization in language models used two datasets: PTB and WIKI. Metrics included mean log value (\u00b5 z) and standard deviation (\u03c3 z) to determine self-normalization. \u03c3 z was considered more important for practical purposes. The evaluation of self-normalization in language models focused on metrics like mean log value (\u00b5 z) and standard deviation (\u03c3 z). \u03c3 z was deemed more crucial for practical purposes. The study also examined perplexity as a measure of model prediction quality, showing NCE-LM's self-normalization compared to SM-LM's lack thereof. The study evaluates self-normalization in language models, focusing on metrics like mean log value (\u00b5 z) and standard deviation (\u03c3 z). Perplexity is used as a measure of model prediction quality, showing that NCE-LM's self-normalization degrades with higher dimensionality. Table 2 compares self-normalization and perplexity performance of DEV-LM for different values of the constant \u03b1 on validation sets, with higher \u03b1 values improving self-normalization but potentially at the expense of perplexity. The study evaluates self-normalization in language models, focusing on metrics like mean log value (\u00b5 z) and standard deviation (\u03c3 z). The larger the value of \u03b1, the better the self-normalization becomes, but at the expense of perplexity. The self-normalization worsens with the size of the model, and is negatively correlated with the improvement in perplexity. A technique is proposed to center the log(Z) values of a self-normalizing model's scores around zero. Table 3 shows the results of evaluating the shifted NCE-LM and DEV-LM models with d = 650. DEV-LM with \u03b1 = 1.0 provides an optimal trade-off between self-normalization and perplexity performance. The study evaluates self-normalization in language models, focusing on metrics like mean log value (\u00b5 z) and standard deviation (\u03c3 z). DEV-LM with \u03b1 = 1.0 achieves near perfect self-normalization and comparable perplexity to NCE-LM. NCE-LM has an advantage in training time as it does not include a normalization term in its objective. Pearson's correlation between entropy and log(Z) is shown in Table 4. The study evaluates self-normalization in language models, focusing on metrics like mean log value (\u00b5 z) and standard deviation (\u03c3 z). NCE-LM does not include the normalization term in its training objective, leading to faster training time. Pearson's correlation between entropy and log(Z) is shown in Table 4, indicating a negative correlation between log(Z) and entropy. Low-entropy distributions are more concentrated, while high-entropy ones are more spread out. The study explores the negative correlation between log(Z) value and entropy in language models, with a stronger correlation observed in larger models. Low entropy distributions can lead to high log(Z) values, deviating from the self-normalization objective. This observation may contribute to larger models having higher variance in normalization terms. The study aims to improve self-normalization algorithms based on this regularity. NCE-LM performs reasonably well but not as effectively as explicitly trained self-normalizing language models. The study suggests improving self-normalization algorithms by incorporating explicit self-normalization components in NCE's training objective. Unexpected correlations between self-normalization, perplexity performance, and partition function were discovered, providing insights for future work on self-normalizing models."
}