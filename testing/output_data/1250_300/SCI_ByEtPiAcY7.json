{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. A new method using \\textit{M-of-N} rules was proposed to map the complexity/accuracy landscape of rules describing hidden features in a Convolutional Neural Network (CNN). The experiments showed an optimal trade-off between comprehensibility and accuracy, with rules in the first and final layers being highly explainable. The study explores the trade-off between comprehensibility and accuracy in neural networks, finding optimal \\textit{M-of-N} rules for each latent variable. Rules in the first and final layers are highly explainable, while those in the second and third layers are less so. This sheds light on the feasibility of rule extraction from deep networks and the importance of decompositional knowledge extraction for explainability in AI. The distributed representation in neural networks may not correlate with easily identifiable features of the data, making knowledge extraction crucial for increasing explainability. Rule extraction techniques, either decompositional or pedagogical, aim to translate neural networks into symbolic rules for better understanding. Rule extraction techniques aim to translate neural networks into symbolic rules for better understanding. The complexity of extracted rules poses a challenge, as distributed representations in neural networks make it difficult to extract important concepts. The distributed nature of neural networks, identified as a fundamental property of connectionism, challenges the traditional approach of explaining latent features using symbolic knowledge extraction. Distillation methods are proposed as an alternative for improving robustness, but their efficacy is debated. Rule extraction techniques aim to make neural networks more explainable by translating them into symbolic rules, with a focus on balancing error and complexity trade-offs. Empirically examining the explainability of latent variables in neural networks through rule extraction. Mapping out a landscape of error/complexity trade-offs to describe the behavior of a network. Some layers have accurate rules while others do not, with an ideal M-of-N rule for each latent variable. Explainability trends vary between layers and architectures, with convolutional layers extracting more complex rules compared to fully connected layers. The study explores the explainability of neural networks through rule extraction, showing varying trends in complexity between layers. Convolutional layers extract more complex rules compared to fully connected layers, with different levels of accuracy observed in different layers. The research includes an overview of previous algorithms, definitions of accuracy and complexity for M-of-N rules, experimental results, and conclusions. The first attempts at knowledge extraction used a decompositional approach applied to feedforward networks, specifically the Knowledge-based Artificial Neural Networks (KBANN). These algorithms extract symbolic rules known as M-of-N rules from hidden variables. More recent algorithms select M-of-N rules based on input units as concepts, focusing on maximum information gain with respect to the output. These methods treat the model as a black box for rule extraction, with some being pedagogical and others eclectic in nature. Many methods for rule extraction from neural networks treat the model as a black box. Decompositional techniques aim to explain hidden features but can become too complex in deep networks. Some methods focus on input/output relationships, while others use visually oriented approaches. The paper discusses the challenges of rule extraction from deep neural networks, highlighting the complexity that arises with many hidden layers. Despite this, experiments show that some layers may have explainable rules that can shed light on the network's behavior. This opens up the possibility of using rule extraction for modular explanations and comparing latent features. The formal definition of rule explainability and optimality is also discussed. In logic programming, a rule is an implication A \u2190 B, where A is the head and B is the body. Rules explain neural network behavior by referring to neuron states. Different rules are used for neurons with binary or continuous activation values. Latent variables in neural networks are described by M-of-N rules to avoid creating a large lookup table. In neural networks, M-of-N rules are used to describe latent variables, offering a compact representation that reflects input/output dependencies. These rules soften the conjunctive constraint and share structural similarity with neural networks. They are more general than simple conjunctions and prevent the network from becoming a large lookup table. M-of-N rules are a subset of propositional formulas and share structural similarity with neural networks. They can be represented as 'weightless perceptrons' where the output neuron represents the head and visible neurons represent the body of the rule. These rules have been used for knowledge extraction and are brought back into the debate on explainability. When dealing with continuous activation values, splitting values are chosen for each neuron to define the literals for rule extraction. When dealing with continuous activation values in a network, splitting values are chosen for each neuron to define literals for rule extraction. Information gain is used to select splits for target neurons, maximizing decrease in entropy of network outputs. Input literals are generated based on splits for each input that maximize information gain. Each target literal in a layer has its own set of input literals, corresponding to the same input neurons with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches. In rule extraction, the focus is on the accuracy and comprehensibility of the extracted rules. Accuracy is measured by the expected difference between rule predictions and network outputs. By defining rules based on the state of input neurons and using them to determine the truth of target neurons, a single rule can be generated for each feature map in a layer. The network can determine the truth of H using rule R and input x. Discrepancy between rules and network output is measured. Comprehensibility is assessed based on rule complexity in DNF form. Complexity is normalized relative to maximum complexity. The complexity of a rule in disjunctive normal form (DNF) is measured relative to a maximum complexity. A loss function for a rule is defined as a weighted sum balancing soundness and complexity. The complexity of a rule in disjunctive normal form (DNF) is measured relative to a maximum complexity. A loss function for a rule is defined as a weighted sum balancing soundness and complexity. Using a brute force search procedure with various values of \u03b2, the relationship between the allowed complexity of a rule and its maximum accuracy is explicitly determined. By generating splits for neurons and searching through M-of-N rules, the minimum loss rule can be found. The complexity of a rule in disjunctive normal form (DNF) is measured relative to a maximum complexity. A loss function for a rule is defined as a weighted sum balancing soundness and complexity. By generating splits for neurons and searching through M-of-N rules, the minimum loss rule can be found. The search procedure relies on the ordering of variables X i to find the most accurate M-of-N rules with N literals. The complexity of a rule in disjunctive normal form (DNF) is measured relative to a maximum complexity. A loss function for a rule is defined as a weighted sum balancing soundness and complexity. By generating splits for neurons and searching through M-of-N rules, the minimum loss rule can be found. The search procedure relies on the ordering of variables X i to find the most accurate M-of-N rules with N literals. A neuron with n input neurons has O(2 n ) possible M-of-N rules, making an exhaustive search intractable. The algorithm was implemented in Spark and run on IBM cloud services to complete the rule extraction in a reasonable time. The algorithm for rule extraction was implemented in Spark and run on IBM cloud services to reduce the search space complexity. The accuracy of the extracted rules was evaluated using examples from the training set, with a focus on the network's output behavior. By running the search in parallel, the accuracy/complexity graph for hidden neurons was mapped efficiently. The extraction process for the first hidden feature in a CNN trained on the fashion MNIST dataset was demonstrated using 1000 random input examples. The extraction process for the first hidden feature in a CNN trained on the fashion MNIST dataset involves selecting 1000 random input examples to compute neuron activations and predicted labels. Neuron selection is based on information gain, with the optimal neuron for the first feature being neuron 96. This neuron corresponds to a specific image patch, and input splits are defined based on maximizing information gain. The test input consists of 1000 5x5 image patches centered at a specific location. The input splits are defined based on maximizing information gain with respect to variable H, which is determined by neuron 96. The extraction process results in three different rules of decreasing complexity explaining the neuron, with the most complex rule being a 5-of-13 rule with a 0.025 error rate. The extraction process results in rules of varying complexity to explain the neuron, with penalties for complexity affecting the error rates. The technique is applied to the DNA promoter dataset, showing an exponential relationship between complexity and error in the first layer. The output layer reveals a rule with 100% fidelity to the network. In the output layer, a rule with 100% fidelity to the network is found. The rules for hidden layer splits are determined by information gain. Errors propagate through layers when rules do not perfectly approximate each layer. Different splits may be chosen for the same layer when treated as output. To replace a network with hierarchical rules, decisions must be made. In most networks, different splits may be chosen for the same layer when treated as output. To replace a network with hierarchical rules, decisions must be made by selecting a single set of splits for each layer. This can introduce more error, so experiments are conducted layer by layer independently to provide an idealized complexity/error curve for rule extraction with M-of-N rules. The layerwise rule extraction search was tested on a basic CNN trained on fashion MNIST in tensorflow, with a standard architecture including a 32 feature convolutional layer with a 5 \u00d7 5 convolutional window. The rule extraction landscape of a neural network was examined using a basic CNN trained on fashion MNIST in tensorflow. The CNN had standard architecture with convolutional and max pooling layers, as well as a fully connected layer. Rules were extracted and tested on random inputs from the training data for each layer, with a limit on the number of literals for efficiency. The output layer was tested with one-hot neurons, and the search procedure was repeated for different values. The rule extraction process for a neural network trained on fashion MNIST involved testing different values of \u03b2 to extract rules with varying error/complexity trade-offs for each layer. The first and final layers produced accurate rules with minimal error, while the second and third layers showed a trade-off between accuracy and complexity. The third layer had the lowest error but could not be improved with more complex rules, and trivial rules performed worse on the second layer. The rule extraction process for a neural network trained on fashion MNIST involved testing different values of \u03b2 to extract rules with varying error/complexity trade-offs for each layer. The second layer showed a slight improvement in accuracy with more complex rules, while the third layer had the lowest error but could not be improved further with complex rules. The optimal accuracy/complexity tradeoff was not solely based on the number of input nodes, as the third layer performed similarly to the second layer despite having more input nodes. The final layer provided more accurate rules with less complexity compared to the first layer. There is a critical point where error starts to increase rapidly as complexity penalty increases, indicating a natural set of rules for explaining latent features. Ultimately, the trade-off between accuracy and complexity is subjective, and rules at critical points cannot be significantly improved by increasing complexity. This paper introduces a novel approach to rule extraction algorithms by incorporating rule complexity as a key factor in the optimization process. The analysis highlights the importance of considering complexity in rule extraction algorithms and emphasizes the need for empirical evaluation to validate their performance. In some cases, like the second and third layers of a CNN, complex rules may not provide simple explanations for features, while in other cases, simpler rules can achieve higher accuracy with less complexity. The results show that in certain cases, like the final layer of a CNN, simple rules can accurately explain the behavior of output neurons with near 0% error. However, in other cases, such as the second and third layers, even complex rules have a 15% error rate. This suggests that decompositional rule extraction may not be a general method of explainability for neural networks, but selective use of algorithms depending on the layer could be beneficial. The black box problem of neural networks remains a challenge for their integration into society, highlighting the need for explainability as they become more prevalent. The need for explainability in neural networks has grown as they become more integrated into society. Knowledge extraction has been traditionally used but critics argue that the distributed nature of neural networks makes rule extraction unfeasible. A novel search method for M-of-N rules was applied to explain the latent features of a CNN, showing that an 'optimal' rule can represent an ideal error/complexity trade-off. The discrepancy in this trade-off between neurons in different layers suggests that rule extraction may not be a general technique for explainability. The discrepancy in the trade-off between neurons in different layers suggests that rule extraction may not be a general technique for explainability in neural networks. However, simplifying explanations without reducing accuracy can still make rule extraction a useful tool for understanding network behavior. Further research is needed to explore the effects of different factors on the accuracy and interpretability landscape."
}