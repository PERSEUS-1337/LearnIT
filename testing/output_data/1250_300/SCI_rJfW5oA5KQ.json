{
    "title": "rJfW5oA5KQ",
    "content": "Recent works have shown that Generative Adversarial Networks (GANs) suffer from lack of diversity or mode collapse, as powerful discriminators cause overfitting while weak discriminators cannot detect mode collapse (Arora et al., 2017a). Recent works have shown that Generative Adversarial Networks (GANs) suffer from lack of diversity or mode collapse. However, this paper demonstrates that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by designing discriminators with strong distinguishing power against specific generator classes. This approach can approximate the Wasserstein distance and/or KL-divergence, ensuring that the learned distribution is close to the true distribution and cannot drop modes. Preliminary experiments suggest that the lack of diversity in GANs may be due to optimization sub-optimality rather than statistical inefficiency. The lack of diversity in GANs may be due to optimization sub-optimality rather than statistical inefficiency. Various ideas have been proposed to improve the quality of learned distributions and training stability, but understanding of GANs is still in its early stages. Recent work has highlighted mode collapse in GANs, where the learned distribution misses significant modes of the target distribution. The paper discusses how mode collapse in GANs can be alleviated by designing discriminators with strong distinguishing power against specific families of generators, focusing on the Wasserstein GAN formulation. It introduces the F-Integral Probability Metric (F-IPM) and explains the setup of generators and discriminators in WGAN to learn data distributions. The paper introduces the F-Integral Probability Metric (F-IPM) for Wasserstein GANs to address mode collapse by optimizing Lipschitz functions using gradient-based algorithms. The issue arises from the weaker IPM compared to the Wasserstein-1 distance, leading to low-diversity examples in the learned distribution. The paper discusses the challenges of mode collapse in Wasserstein GANs and the limitations of using the Wasserstein-1 distance for optimization. It highlights the difficulty in generalizing empirical Wasserstein distance to population distance, even when distributions are equal. This poses a dilemma in establishing GAN theories. The paper addresses the challenges of mode collapse in Wasserstein GANs and the limitations of using the Wasserstein-1 distance for optimization. It discusses the difficulty in generalizing empirical Wasserstein distance to population distance, even when distributions are equal, posing a dilemma in establishing GAN theories. The paper proposes a solution by designing a strong discriminator class F against a specific generator class G, with restricted approximability w.r.t. G and the data distribution p. The paper focuses on designing a discriminator class F with restricted approximability to address mode collapse in Wasserstein GANs. It aims to approximate the Wasserstein distance W1 for data distribution p and any q \u2208 G, ensuring that p and q are close in Wasserstein distance. The paper introduces a discriminator class F to prevent mode collapse in Wasserstein GANs by ensuring that data distributions p and q are close in Wasserstein distance. It provides theoretical insights on the statistical properties of Wasserstein GANs with polynomial samples. The paper introduces a theoretical framework for GANs with polynomial samples, focusing on designing discriminator class F to prevent mode collapse in Wasserstein GANs. It explores various generator classes, including Gaussian distributions, exponential families, and distributions generated by invertible neural networks, showcasing techniques for designing F to provide diversity guarantees. In Section 4, the paper discusses using one-layer neural networks with ReLU activations as discriminators and linear combinations of sufficient statistics for exponential families. It highlights that invertible neural networks can produce distributions with exponentially large modes, but the invertibility assumption limits the support to the entire space, which may not align with the low-dimensional manifold of natural images. The paper discusses the limitations of using KL-divergence for distributions with low-dimensional supports and highlights the advantage of using IPMs in approximating Wasserstein distance for generators with low-dimensional supports. It demonstrates the superiority of GANs over MLE in learning distributions with measure-zero support. The main proof technique involves approximating the log-density of a neural network generator in synthetic experiments. The paper introduces the use of Integral Probability Metrics (IPMs) as an alternative to KL-divergence and Wasserstein distance for measuring the diversity and quality of learned distributions in more complex settings. It also discusses the challenges of optimizing the IPM in real datasets due to the need to balance the learning of generators and discriminators in GANs. The lack of diversity in distributions learned by GANs can be attributed to sub-optimal optimization rather than statistical inefficiency. Various tests have been developed to assess diversity, memorization, and generalization in GANs. Mode collapse, caused by a weak discriminator, has been formalized and addressed through different architectures and algorithms. Mode collapse in GANs is a real phenomenon caused by a weak discriminator. Several architectures and algorithms have been proposed to address this issue with varying success. While some solutions have been shown to work for specific cases, there are no provable solutions in more general scenarios. Recent work has shown that the IPM is a proper metric under certain conditions, providing a KL-divergence bound with finite samples. This study extends on previous work by developing statistical guarantees in Wasserstein distance for distributions with injective neural network generators. The study provides statistical guarantees in Wasserstein distance for distributions generated by injective neural networks, showing that successful GAN training implies learning in KL-divergence. It suggests that real data cannot be generated by an invertible neural network. The study suggests that real data cannot be generated by an invertible neural network. It also shows that successful GAN training implies learning in KL-divergence and provides statistical guarantees in Wasserstein distance for distributions generated by injective neural networks. The notion of IPM includes various statistical distances like TV and Wasserstein-1 distance. The KL divergence and Wasserstein-2 distance are key in GAN training for distributions with finite second moments. The Rademacher complexity plays a role in generalization, as stated in Theorem 2.1. Notation includes Gaussian distributions and universal constants. Discriminators with restricted approximability are designed for GAN training. We design discriminators with restricted approximability for simple parameterized distributions like Gaussian distributions using one-layer neural networks with ReLU activation. The discriminators have restricted approximability guarantees w.r.t. Gaussian distributions with bounded mean and well-conditioned covariance. The set of one-layer neural networks has restricted approximability w.r.t. Gaussian distributions in the sense that the lower and upper bounds differ by a factor of 1/ \u221a d. The text discusses the restricted approximability of discriminators for Gaussian distributions using one-layer neural networks with ReLU activation. It mentions that the lower and upper bounds differ by a factor of 1/ \u221a d and extends the concept to mixture of Gaussians and exponential families. The text discusses the approximability of discriminators for exponential families using linear functionals. It presents conditions for the log partition function and Lipschitz properties, along with Rademacher complexity bounds. The proof relies on convexity and geometric assumptions on the sufficient statistics. In this section, discriminators with restricted approximability for neural net generators are designed. The focus is on invertible neural networks generators with proper densities and injective neural networks generators with lower dimensional latent variables. The generators are parameterized by invertible neural networks. Generators in this section are parameterized by invertible neural networks, allowing for non-spherical variances to model data around a \"k-dimensional manifold\" with noise. The family of invertible neural networks G consists of standard feedforward nets with invertible weights and activation functions. The generators are designed with restricted approximability for neural net generators. The neural networks G \u03b8 are parameterized by \u03b8 = (W i , b i ) i\u2208[ ] with activation function \u03c3, satisfying certain conditions. The generators are designed to be invertible, with an inverse also being a feedforward neural net. Assumptions are imposed on the generator networks to prevent them from implementing pseudo-random functions. The function log p \u03b8 can be computed by a neural network with specific parameters and activation functions. The family F of neural networks contains all functions log p \u2212 log q for p, q \u2208 G. The family F of neural networks with at most + 1 layers and O( d 2 ) parameters contains all functions log p \u2212 log q for p, q \u2208 G. The proof involves the change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian can be computed by adding a bias on the final output layer, freeing us from structural assumptions on the weight matrices. The text discusses the representation of a constant bias in the final output layer, freeing from structural assumptions on weight matrices. The proof of Theorem 4.2 involves the discriminator class F and its restricted approximability with respect to invertible-generator distributions. The relationship between KL divergence and IPM is outlined in Lemma 4.3, with a proof sketch provided for Theorem 4.2. The proof of Theorem 4.2 involves the discriminator class F and its restricted approximability with respect to invertible-generator distributions. The proof sketch is outlined below, with details deferred to Appendix D.3. The approach involves lower bounding a quantity by the Wasserstein distance and upper bounding W F (p, q) by the Wasserstein distance. Transportation inequalities by Bobkov-G\u00f6tze and Gozlan are used to establish the lower bound, while two workarounds are proposed for the upper bound in case functions in F are not Lipschitz globally. The upper bound in Theorem D.2 provides two workarounds for functions in F not being Lipschitz globally. By combining restricted approximability and generalization bound, if training succeeds with small expected IPM, the estimated distribution q is close to true distribution p in Wasserstein distance. Corollary 4.4 states that with high probability, if training returns a distribution DISPLAYFORM12, the training error is measured by Eqm [W F (p n ,q m )]. In this section, injective neural network generators are discussed, which generate distributions on a low dimensional manifold. A novel divergence between distributions is designed, sandwiched by Wasserstein distance, to be optimized as IPM. The key idea is to approximate the Wasserstein distance using a variant of IPM. Our key idea is to design a variant of the IPM that approximates the Wasserstein distance for distributions generated by neural nets in G. A smoothed F-IPM is defined between distributions p, q, with an additional variable \u03b2 for optimization. Theorem 4.5 states that for certain discriminator classes, dF approximates the Wasserstein distance. This implies that if d(pn, qn) is small for n poly(d), then W(p, q) is also small, preventing mode collapse in neural network generators. Theorem E.1 states that if d(pn, qn) is small for n poly(d), then W(p, q) is also small, preventing mode collapse in neural network generators. Theoretical results show that mode collapse is avoided when the discriminator family F has restricted approximability with respect to the generator family G. Synthetic experiments confirm that the IPM is correlated with the Wasserstein / KL divergence, suggesting that restricted approximability holds in practice. The IPM is well correlated with the Wasserstein / KL divergence, indicating that GAN training difficulty may stem from optimization challenges rather than statistical inefficiency. Experiments with neural net generators and discriminators show strong correlations between IPM and distance metrics. In synthetic experiments with WGANs, various curves in two dimensions are learned, such as the unit circle and a \"swiss roll\" curve. The Wasserstein distance is used to measure the quality of the learned generator, showing strong correlation with IPM. Standard two-hidden-layer ReLU nets are used for generators and discriminators. In synthetic experiments with WGANs, the generator architecture is 2-50-50-2, and the discriminator architecture is 2-50-50-1. The RMSProp optimizer is used with learning rates of 10^-4 for both the generator and discriminator. Two metrics are compared between the ground truth distribution and the learned distribution: the neural net IPM WF(p, q) and the Wasserstein distance W1(p, q). The empirical Wasserstein distance W1(p, q) is a good proxy for the true Wasserstein distance. The results show that the learned generator is very close to the ground truth distribution. The study demonstrates that the generator in WGANs closely approximates the ground truth distribution in synthetic experiments. The neural net IPM and Wasserstein distance are highly correlated, with the generator approaching the true distribution by iteration 10000. The analysis introduces sample complexity bounds for learning various distributions using GANs, emphasizing convergence guarantees in Wasserstein distance or KL divergence. The approach involves designing discriminators tailored to the generator class to prevent mode collapse and improve generalization. The study focuses on designing discriminators with restricted approximability for GANs to prevent mode collapse and improve generalization. Techniques aim to extend to other distribution families with tighter sample complexity bounds, exploring approximation theory results in the context of GANs. The discriminator family's upper bound is established based on 1-Lipschitz functions, with a lower bound to be determined. The text discusses the establishment of upper and lower bounds for the discriminator family in GANs, focusing on restricted approximability to prevent mode collapse and improve generalization. The lower bound is determined by computing the mean and covariance distances between two Gaussians, utilizing linear and ReLU discriminators. The text also explores the neuron distance and the function R(a) = E[max {W + a, 0}] for W \u223c N(0, 1) to further analyze the discriminator's behavior. The text discusses establishing upper and lower bounds for the discriminator family in GANs, focusing on restricted approximability to prevent mode collapse and improve generalization. By utilizing the W 2 distance between two Gaussians, a lower bound is determined with the constant c = 1/(2 \u221a 2\u03c0). The text also explores the function R(a) = E[max {W + a, 0}] for W \u223c N(0, 1) to further analyze the discriminator's behavior. The text discusses establishing upper and lower bounds for the discriminator family in GANs, focusing on preventing mode collapse and improving generalization. By utilizing the W 2 distance between two Gaussians, a lower bound is determined with the constant c = 1/(2 \u221a 2\u03c0). The text also explores the function R(a) = E[max {W + a, 0}] for W \u223c N(0, 1) to further analyze the discriminator's behavior. The proof involves bounding the growth of \u2207 log p 1 (x) 2 and applying the Rademacher contraction inequality. The text discusses establishing upper and lower bounds for the discriminator family in GANs, focusing on preventing mode collapse and improving generalization. Wasserstein bounds are utilized to show the 1-Lipschitz property of functions. The family F is suitable for learning mixtures of k Gaussians, with restricted approximability and generalization properties. The Gaussian concentration result is used to establish upper and lower bounds for the discriminator family. The text discusses establishing upper and lower bounds for the discriminator family in GANs, focusing on preventing mode collapse and improving generalization. It utilizes Wasserstein bounds to show the 1-Lipschitz property of functions in the family F for learning mixtures of k Gaussians. The Gaussian concentration result is used to establish these bounds. The Rademacher complexity of f \u03b8 for \u03b8 and the Rademacher process show that Y \u03b8 is Lipschitz in \u03b8. By using a one-step discretization bound, we can bound the expected supremum over a covering set. The process Y \u03b8 is the i.i.d. average of random variables, and we can apply sub-Gaussian maxima bounds. Theorem D.2 discusses upper bounding f-contrast by Wasserstein. The text discusses upper bounding f-contrast by Wasserstein. It involves choosing \u03b5 = c/n for small c, bounding f with Wasserstein distance, and using a truncation argument. The proof involves a coupling and Cauchy-Schwarz inequality. The text discusses representing log p \u03b8 (x) by a neural network using a feedforward net with activation \u03c3 \u22121. It involves computing the inverse of x = G \u03b8 (z) and considering the density of Z \u223c N(0, diag(\u03b3 2)). The network implementing G \u22121 \u03b8 has d2 + d parameters in each layer and \u03c3 \u22121 as the activation function. The log density formula for log p \u03b8 (x) can be computed using a neural network with no more than + 1 layers and O( d 2 ) parameters. By adding branches to the network, the log determinant of the Jacobian can also be calculated. This approach involves adding layers with specific activations and inner products, resulting in an efficient computation method. The log density formula for log p \u03b8 (x) can be computed using a neural network with limited layers and parameters. The theorem proves a restricted approximability bound in terms of the W 2 distance. Lemmas are combined to show the existence of a constant for any \u03b8 1 , \u03b8 2 \u2208 \u0398. The Gozlan condition is satisfied for p \u03b8 in \u0398. The network G \u03b8 is L-Lipschitz and satisfies the Gozlan condition with \u03c3 2 = L 2. Theorem D.1(b) is applied to obtain bounds for log p \u03b81 \u2212 log p \u03b82. The log density formula for log p \u03b8 (x) can be computed using a neural network with limited layers and parameters. Theorem D.1(b) is applied to obtain bounds for log p \u03b81 \u2212 log p \u03b82. The W 2 bound is shown by upper bounding the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The W 1 bound is also discussed, with a choice of D = K \u221a d for a sufficiently large constant K. The log density formula for log p \u03b8 (x) can be computed using a neural network with limited layers and parameters. The W 1 bound is discussed, with a choice of D = K \u221a d for a sufficiently large constant K. Reparameterization is done to represent the weights and biases of the inverse network z = G \u22121 \u03b8 (x). The Rademacher complexity of F is at most two times a certain quantity. The log-density network F \u03b8 (x) = log p \u03b8 (x) is re-parametrized with a constant C(\u03b8) representing the normalizing constant for Gaussian density and log det(W i ). A parameter K = K(\u03b8) is introduced, and a metric is defined for any reparametrized \u03b8. The Rademacher process is denoted as Y \u03b8 = 1 n n i=1 \u03b5 i F \u03b8 (X i ). Two lemmas are presented for discretization error and expected max over a finite set. Substituting these lemmas into the bound equation yields a final bound for all \u03b5 \u2264 min {R W , R b } and \u03bb \u2264 \u03bb 0 \u03b4 2 n. The text discusses the re-parametrization of the log-density network F \u03b8 (x) and introduces a parameter K = K(\u03b8). It presents two lemmas for discretization error and expected max over a finite set. Substituting these lemmas into the bound equation yields a final bound for all \u03b5 \u2264 min {R W , R b } and \u03bb \u2264 \u03bb 0 \u03b4 2 n. The text also discusses the Lipschitzness of hidden layers in the network. The text discusses the induction on k to show bounds for equations (26) and (27) in the context of re-parametrization of the log-density network F \u03b8 (x). It also covers Lipschitz properties of hidden layers and the sub-exponential nature of a random variable at a single \u03b8. The mean and sub-Gaussianity parameter of a sub-Gaussian random variable are analyzed, along with the bound for the mean. The text discusses the sub-exponential nature of random variables and their mean bounds, along with the Lipschitz properties of hidden layers. It also covers the sub-Gaussianity parameter analysis and bounds for the mean of sub-Gaussian random variables. The text discusses bounding the expected maximum using a covering argument and Jensen's inequality. It introduces a truncated version of the convolution of a Gaussian distribution and specifies quantities of the generator class relevant for the theorem. The text introduces regularity conditions for the family of generators G, including bounds on partial derivatives and the Lipschitz property of the inverse activation function. It also mentions a theorem stating that for certain F and d, F approximates the Wasserstein distance. The main theorem states that for certain F and d, F approximates the Wasserstein distance. The proof involves a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. Theorem E.2 introduces a family of neural networks F that approximates log p for typical x and is a lower bound of p globally. The proof of Theorem E.1 involves neural networks N1 and N2 from family F approximating log p \u03b2 and log q \u03b2 respectively. This is based on the approximation of log densities for typical x and the lower bound of p globally introduced in Theorem E.2. The proof of Theorem E.1 involves neural networks N1 and N2 from family F approximating log p \u03b2 and log q \u03b2 respectively. By setting f = N1(x) \u2212 N2(x), a lower bound is obtained. For the upper bound, setting \u03b2 = W1/6 suffices. The claim follows by considering the optimal coupling C of p, q and the induced coupling Cz on the latent variable z. The rest of the section is dedicated to the proof of Theorem E.2. The proof of Theorem E.2 involves helper lemmas and induction to establish estimates for a neural network. The Lipschitz constant of the network is determined by the inverse function \u03c3. The section concludes with the completion of the claim. The Lipschitz constant of the neural network is determined by the inverse function \u03c3. The algorithm presented approximates the integral and can be implemented by a small, Lipschitz network. The algorithm involves calculating gradients and Hessians for the discriminator family with restricted approximability for a degenerate manifold. The algorithm involves calculating gradients and Hessians for the discriminator family with restricted approximability for a degenerate manifold. It approximates the integral and can be implemented by a small, Lipschitz network. The algorithm uses an invertor circuit to calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91), and approximate eigenvector/eigenvalue pairs of H + E i. The algorithm involves calculating gradients and Hessians for the discriminator family with restricted approximability for a degenerate manifold. It approximates the integral using a small, Lipschitz network and an invertor circuit to calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91), and approximate eigenvector/eigenvalue pairs of H + E i. Additionally, synthetic WGAN experiments are performed with invertible neural net generators and discriminators designed with restricted approximability to demonstrate the correlation between empirical IPM W F (p, q) and the KL-divergence. The data is generated from a ground-truth invertible neural net generator using Leaky ReLU activation function. The discriminator architecture is chosen based on restricted approximability guarantee. The discriminator architecture is chosen based on restricted approximability guarantee, with constraints on parameters. Training involves generating batches from ground-truth and trained generator, solving min-max problem in Wasserstein GAN formulation. Evaluation metrics include KL divergence between true and learned generator densities. The KL divergence is used as a criterion for distributional closeness between the true and learned generator. Training loss in IPM W F train is the unregularized GAN loss, while the neural net IPM in W F eval involves separately optimizing the WGAN loss to find an approximate maximizer. The discriminator is trained in norm balls without regularization to find f \u2208 F that maximizes contrast. The loss obtained is an approximation of W F. WGAN can learn the true generator in KL divergence, and F-IPM should indicate the KL divergence. In experiments, a two-layer net in d = 10 dimensions is used as G. Results show that WGAN training with a discriminator of restricted approximability can learn the true distribution in KL. Results from experiments show that WGAN training with a discriminator of restricted approximability can effectively learn the true distribution in KL divergence. The KL divergence starts at around 10^-30 and decreases to lower than 1, indicating that GANs are successful in finding the true distribution without mode collapse. Additionally, the W F (eval) and KL divergence are closely correlated, with the addition of a gradient penalty significantly improving optimization. The W F curve can serve as a reliable metric for monitoring convergence, outperforming the training loss curve. Testing with vanilla fully-connected discriminator nets also shows good correlation with IPM results. The experiment tested the necessity of the specific form of the discriminator by using vanilla fully-connected discriminator nets. Results showed that IPM with vanilla discriminators correlated well with KL-divergence. The inferior performance of WGAN-Vanilla in KL divergence was attributed to inferior training performance in terms of IPM convergence. In a synthetic case study, the inferior performance of the WGAN-Vanilla algorithm in KL divergence is not due to the statistical properties of GANs but rather the training performance in terms of IPM convergence. By testing the correlation between perturbations and p, a positive correlation between KL divergence and neural net IPM was observed. The majority of points align with the theory that neural net distance scales linearly in KL divergence. In a study, a positive correlation was found between the KL divergence and neural net IPM. Most points align with the theory that neural net distance scales linearly in KL divergence. Outliers with large KL were attributed to perturbations causing poorly conditioned weight matrices. Experiments with vanilla fully-connected discriminator nets showed weaker correlation in KL divergence compared to restricted approximability, suggesting specific designs may improve generator performance. The study found a positive correlation between KL divergence and neural net IPM. Vanilla discriminator structures may be sufficient for a good generator, but specific designs could enhance the quality of the distance W F. The correlation between KL and neural net IPM is similar for fully-connected discriminators and those with restricted approximability."
}