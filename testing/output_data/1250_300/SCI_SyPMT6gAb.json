{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning is crucial for evaluating and improving policies using historical data from a logging policy. The main challenge is to develop counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging and new policies, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization show significant improvement over baseline algorithms, addressing the challenges of evaluating policies using historical data. Off-policy learning involves utilizing historic data to evaluate policies before deployment, minimizing risks and costs associated with human trials or A/B testing. Various methods like Q learning and doubly robust estimator have been studied in reinforcement learning and contextual bandits. A new direction in off-policy learning involves using logged interaction data with limited feedback, focusing on scalar rewards or losses for actions taken. This approach aims to understand the relationship between policy changes and rewards, optimizing decision-making processes. Off-policy learning utilizes historic data to evaluate policies, minimizing risks and costs. It focuses on understanding the relationship between policy changes and rewards, optimizing decision-making processes. BID34 introduced a new counterfactual risk minimization framework to address distribution mismatch and generalization errors in bandit feedback cases. The framework includes a regularization term using sample variance in empirical risk minimization. However, their linear stochastic model for policy parametrization limits representation power, and computing sample variance regularization requires iterating through all training samples. The paper proposes a new learning principle for off-policy learning with bandit feedback, aiming to minimize generalization error by minimizing distribution divergence between policies. It suggests parametrizing the policy as a neural network and using variational divergence minimization and Gumbel soft-max sampling for end-to-end training. Experiment evaluation on benchmark datasets shows significant results. The policy is parametrized as a neural network and divergence minimization is solved using variational methods and Gumbel soft-max sampling. Experimental results show improved performance over conventional methods, validating theoretical proofs. Stochastic policies map inputs to structured outputs, with actions sampled from the distribution. In online systems, feedback is observed for actions sampled from a distribution h(Y|x). The goal of off-policy learning is to minimize expected risk on test data by finding an improved policy h(Y|x) with lower risks than the logging policy h0(Y|x). The data used includes a loss function \u03b4(x, y; y*) indicating satisfaction with recommended items. In off-line logged learning, the goal is to improve the logging policy h0(Y|x) by finding a new policy h(Y|x) with lower expected risks. Challenges include skewed distribution of logging policy and the need for empirical estimation due to finite samples. A propensity scoring approach using importance sampling can address the distribution mismatch between h and h0. The propensity scoring approach using importance sampling addresses the distribution mismatch between h and h0 by reweighting the expected risk. Regularizing the variance in counterfactual risk minimization involves a regularization term derived from empirical Bernstein bounds to minimize the modified objective function. Stochastic training difficulty due to dataset dependency is overcome by approximating the regularization term via first-order Taylor expansion. The authors derived a stochastic optimization algorithm by approximating the regularization term with a first-order Taylor expansion. Instead of estimating variance empirically, they propose deriving a variance bound directly from the parametrized distribution. The importance sampling approach reweights the expected risk, and a general learning bound exists for importance sampling weights. Lemma 1 states an identity involving importance sampling weights and R\u00e9nyi divergence. Theorem 1 provides an upper bound for the second moment of weighted loss, while Theorem 2 establishes a generalization bound using distribution divergence functions. Theorem 2 derives a generalization bound between expected risk and empirical risk using distribution divergence functions. The proof involves Bernstein inequality and second moment bounds, highlighting bias-variance trade-offs in empirical risk minimization problems. It suggests minimizing variance regularized objectives in bandit learning settings to control the trade-off between empirical risk and model variance. The challenge remains in empirically setting the model hyper-parameter \u03bb. In light of the challenge of setting the model hyper-parameter \u03bb empirically, a new constrained optimization formulation is explored in the next subsection. This formulation, with a pre-determined constant \u03c1 as the regularization hyper-parameter, provides a good surrogate for the true risk R(h) with their difference bounded by \u03c1. The new objective function, objectiveR d(h||h0\u2264\u03c1) (h), serves as a surrogate for the true risk R(h), with their difference bounded by the regularization hyper-parameter \u03c1. This eliminates the need to compute sample variance in existing bounds. However, estimating the divergence function becomes challenging with a parametrized distribution of h(y|x) and finite samples. Recent f-gan networks and Gumbel soft-max sampling can aid in solving this task. The stochasticity of the logging policy is crucial for effective learning, as highlighted by our bounds. A deterministic logging policy with peaked masses and zeros elsewhere makes learning difficult, as unexplored regions hinder progress. The divergence term calculation in counterfactual learning is hindered by unexplored regions, leading to an unbounded generalization bound. The variance regularized objective aims to minimize the square root of a certain term, connecting it to the f-divergence measure. By following the f-GAN method, a lower bound for the objective can be reached. The f-GAN method proposes a lower bound for the divergence term calculation in counterfactual learning, connecting it to the f-divergence measure. By utilizing neural networks as the family of functions, the objective can be theoretically satisfied. The final objective is a saddle point of a function that maps input pairs to a scalar value, with the policy to learn acting as a sampling distribution. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence. The true divergence is denoted as D f, and the empirical estimator is used to approximate it. The estimation error is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation. The universal approximation theorem ensures the second term error is zero. The parametric family of neural networks is used to approximate the true distribution. The error terms are related to the empirical mean estimation and the difference between empirical and population distributions. The strong law of large numbers applies, leading to the error terms approaching zero. Generative-adversarial approaches can be applied, with the T function represented as a discriminator network and the policy as a generator neural network. The T function is represented as a discriminator network, and the policy is a generator neural network. Gumbel soft-max sampling methods are used for differential sampling. The training procedure is outlined for optimizing the generator distribution. The training algorithm involves sampling real and fake data, updating parameters to minimize variance regularization, and optimizing the generator distribution using Gumbel soft-max sampling. The algorithm aims to minimize counterfactual risk from logged data. The algorithm involves two separate training steps: 1) updating policy parameters to minimize reweighed loss, and 2) updating generator and discriminator parameters to regulate variance for improved generalization. Historic data exploitation is crucial in bandit problems, with approaches like doubly robust estimators proposed. Recent theoretical studies have explored minimax risk lower bounds and adaptive learning algorithms. Recent works have extended techniques like doubly robust estimators to reinforcement learning domains. Off-policy learning in RL can utilize methods such as Q function learning and temporal difference learning. Deep RL studies have also addressed off-policy updates through multi-step bootstrapping and off-policy training of Q functions. Learning from log traces involves applying propensity scores to evaluate candidate policies, similar to treatment effect estimation in statistics. In statistics, treatment effect estimation focuses on estimating the effect of an intervention from observational studies. Techniques like unbiased counterfactual estimators and variance regularization are applied in computational advertising and bandit learning. Variance regularization also addresses distribution mismatch in importance sampling problems. Divergence minimization techniques can potentially be applied to supervised learning as well. Regularized empirical risk minimization for supervised learning with a convex objective function has connections to distributionally robust optimization. The divergence minimization technique can be applied to supervised learning and domain adaptation problems to address distribution match issues. Regularization for the objective function is closely related to distributionally robust optimization techniques, where the empirical risk is minimized over an ellipsoid uncertainty set. Wasserstein distance between empirical and test distributions is a well-studied constraint for achieving robust generalization performance. Empirical evaluation of proposed algorithms involves converting supervised learning to bandit feedback methods. Regularized empirical risk minimization for supervised learning with a convex objective function has connections to distributionally robust optimization. The divergence minimization technique can be applied to supervised learning and domain adaptation problems to address distribution match issues. In evaluation, two types of metrics are used for the probabilistic policy h(Y|x), including expected loss and average hamming loss of maximum a posteriori probability prediction. Bandit feedback datasets are created by passing samples through a logging policy and recording actions, loss values, and propensity scores. The text discusses the evaluation of probabilistic policies in supervised learning and domain adaptation. It compares different algorithms such as IPS and POEM using L-BFGS and stochastic optimization solvers. The importance of considering both generalization performance and MAP prediction in model evaluation is highlighted. The study compares neural network policies with and without divergence regularization using L-BFGS and stochastic optimization solvers. Hyperparameters are selected based on validation set performance. Four multi-label classification datasets from the UCI machine learning repo are used, with a three-layer feed-forward neural network for policy distribution and a two or three layer feed-forward neural network for divergence minimization. The networks are trained with Adam optimizer and PyTorch implementation. The study implemented neural network policies with divergence regularization using Adam optimizer and PyTorch. Results show significant improvement in test performance compared to baseline CRF policies. Introduction of variance regularization further enhances testing and MAP prediction loss. No significant difference observed between two Gumbel soft-max sampling schemes. The introduction of additional variance regularization term in neural network policies showed improvement in testing and MAP prediction loss. There was no significant difference between the two Gumbel soft-max sampling schemes. Varying the maximum number of iterations for divergence minimization led to faster decrease in test loss and better generalization to test sets. By increasing the maximum iterations for divergence minimization, the test loss decreases faster and reaches a lower final test loss. This indicates that adding a regularization term helps policies generalize better to test sets. The stronger the regularization imposed, the better the test performance. The regularization also aids in faster convergence of the training algorithm. The generalization performance improves as the number of training samples increases. Varying the number of passes of training data to sample an action shows that both regularized and non-regularized models have increasing test performance with more training samples. Regularized policies consistently outperform models without regularization, aligning with theoretical expectations. Regularized policies show better generalization performance compared to models without regularization. However, MAP prediction performance starts to decrease after training samples exceed 24, indicating potential overfitting. Experimental results compare two training schemes and Gumbel-softmax sampling methods, showing that blending weighted loss and distribution divergence slightly improves performance but makes training more challenging due to gradient balancing difficulties. In this section, the text discusses the impact of logging policies on learning performance. By adjusting the parameter of h0 with a temperature multiplier \u03b1, the algorithm's ability to learn an improved policy is tested. As \u03b1 increases, the logging policy becomes more deterministic. Varying \u03b1 in the range of 2 [-1, 1, ..., 8], the average ratio of performance is reported. The text discusses the impact of adjusting the parameter \u03b1 on the model's ability to learn an improved policy. By varying \u03b1 in the range of 2 [-1, 1, ..., 8], the average ratio of performance is reported. NN policies outperform logging policies when h0's stochasticity is sufficient, but struggle when the temperature parameter exceeds 2/3. Stronger regularization in NN policies shows slightly better performance, indicating the robustness of the learning principle. Regularization improves model performance against weaker models, showcasing robustness. As h0 quality improves, models consistently outperform baselines, but face increased difficulty. The impact of logging policies on learned improved policies is discussed, highlighting the trade-off between policy accuracy and sampling biases. Varying the proportion of training data points for logging policy training affects the performance of improved policies. In this paper, a new training principle is proposed to improve the generalization performance of off-policy learning for logged bandit datasets. The approach involves explicitly regularizing variance to address sampling biases and improve policy accuracy. By combining importance reweighted loss with a regularization term measuring distribution divergence, neural networks are trained using variational divergence minimization and Gumbel soft-max sampling techniques. Theoretical discussions guide the training objective towards achieving better performance against logging policies. The proposed training principle aims to enhance off-policy learning for logged bandit datasets by incorporating importance reweighted loss and a regularization term measuring distribution divergence. Neural network policies are trained using variational divergence minimization and Gumbel soft-max sampling techniques to minimize variance and improve policy accuracy. The effectiveness of the approach was validated on benchmark datasets, with potential extensions suggested for estimating propensity scores to increase algorithm applicability. The proposed algorithm focuses on off-policy learning from logged data, with techniques that can be extended to supervised and reinforcement learning. The proof involves applying Lemma 1 to importance sampling weight function and loss, bounding the variance using Reni divergence, and applying Bernstein's concentration bounds. The algorithm aims to optimize a generator for bandit learning with an approximate minimizer of R(w) using Gumbel soft-max sampling and regularization. The algorithm focuses on off-policy learning from logged data, optimizing a generator for bandit learning with regularization. It involves updating the discriminator and generator iteratively, aiming to minimize variance regularized risk. The statistics of the datasets are reported, showing improvements over the initial policy in expected loss and loss with MAP predictions. The algorithm focuses on off-policy learning from logged data, optimizing a generator for bandit learning with regularization. NN policies show improvement over h0 in expected loss and loss with MAP predictions, but struggle to beat baselines when the logging policy has good MAP prediction performance. This phenomenon requires further investigation."
}