{
    "title": "r1l73iRqKm",
    "content": "In open-domain dialogue, intelligent agents struggle to incorporate knowledge into conversations. To address this, a new dataset grounded in Wikipedia knowledge was created. Dialogue models were developed to retrieve, read, and generate responses based on this knowledge. These models performed well in discussing various topics, marking a significant step towards enabling humans to communicate effectively with machines. The current state-of-the-art in AI aims to enable machines to converse intelligently with humans by mastering skills like language comprehension, memory retention, reasoning, and response generation. Existing approaches like sequence to sequence models lack effective memory and knowledge utilization. To achieve meaningful conversations, direct knowledge memory mechanisms need to be incorporated. In order to enable machines to converse intelligently with humans, direct knowledge memory mechanisms are needed. This involves designing architectures that combine Memory Network and Transformer architectures to retrieve knowledge and generate outputs, termed Transformer Memory Networks. This approach addresses the limitations of standard models in tasks like open-domain dialogue. Transformer Memory Networks combine Memory Network and Transformer architectures to retrieve knowledge and generate outputs for engaging conversations with humans. A supervised dataset of human-human conversations was created using crowd-sourced workers, with topics connected to Wikipedia. The models were evaluated using automatic metrics and human evaluations, showing their ability to utilize existing knowledge effectively. Our Transformer Memory Network architectures, tested in retrieval and generative versions, demonstrate the ability to engage in knowledgeable conversations with humans. A new benchmark in ParlAI aims to drive improvements in this research area, focusing on unstructured knowledge retrieval. Existing dialogue tasks often lack explicit knowledge utilization, unlike our approach which explores accessing diverse knowledge sets for goal-directed dialogue. Our work focuses on investigating unstructured knowledge across a wide range of topics, including all of Wikipedia, for natural human dialogues. Unlike question answering tasks, we analyze dialogues with diverse utterances, not just questions and answers. Our approach differs from existing dialogue tasks by incorporating non-goal directed dialogue with knowledge utilization. Our work investigates unstructured knowledge in natural human dialogues, incorporating diverse utterances and non-goal directed dialogue. Previous works have used Memory Networks for dialogue with structured knowledge, while we use unstructured text like Wikipedia articles. Our approach involves grounding dialogues with known Wikipedia articles and sentences. Our work compares Memory Networks BID19 and Transformers in an open-domain dialogue setting, where participants engage in chitchat with one being a knowledgeable expert (wizard) and the other a curious learner (apprentice). The apprentice freely talks to the wizard, playing the role of a curious learner in a multi-turn dialogue. In an open-domain dialogue setting, participants engage in chitchat with one being a knowledgeable expert (wizard) and the other a curious learner (apprentice. The wizard informs the conversation partner about a chosen topic using an information retrieval system, while the apprentice delves deeply into the topic to keep the conversation engaging and fun. In an open-domain dialogue setting, a knowledgeable expert (wizard) and a curious learner (apprentice) engage in conversation. The wizard uses a system to show relevant paragraphs from Wikipedia to the apprentice, who then responds based on the information provided. The goal is to have engaging and fun conversations, with the wizard instructed to craft relevant replies using the observed knowledge. The study involves conversations between a human wizard and an apprentice on various topics sourced from Wikipedia. The wizard has access to relevant knowledge passages, and the goal is to eventually replace the human wizard with a learned agent. The topics range from commuting to Arnold Schwarzenegger, and the wizard's knowledge retrieval process is fixed using a retriever similar to the Open-SQuAD dataset. The study involves conversations between a human wizard and an apprentice on various topics sourced from Wikipedia. The wizard uses a retriever similar to the Open-SQuAD dataset to collect relevant knowledge passages. During data collection, the wizard selects relevant articles and sentences to respond to the apprentice. The study involves developing dialogue models to replace the wizard in conversations with an apprentice sourced from Wikipedia. The models can retrieve relevant information from a large memory, read and attend to the knowledge, and generate the next dialogue utterance. Two classes of models are developed: retrieval models that select from a set of candidate responses, and generative models that generate responses word-by-word. The study focuses on developing dialogue models that can retrieve information from a large memory and generate dialogue responses. Two classes of models are created: retrieval models that select from candidate responses and generative models that generate responses word-by-word. The input to the models includes the current dialogue context and the goal is to output the next utterance. Knowledge retrieval involves using information retrieval techniques to select a smaller set of candidates from a large knowledge base. The study focuses on developing dialogue models that can retrieve information from a large memory and generate dialogue responses. Information retrieval techniques are used to select a smaller set of candidates for fine-grained selection. The top 7 articles are retrieved for each lookup and flattened into separate sentences. An attention mechanism is used for fine-grained selection of knowledge sentences for the next dialogue turn. The study develops dialogue models for information retrieval and response generation. An attention mechanism selects knowledge sentences for dialogue turns independently. Transformer encoders are used for encoding memory and dialogue context, with dot-product attention between them. The final stage involves predicting the output utterance for the next dialogue turn. Different variants are considered for knowledge attention and utterance prediction in retrieval and generative models. The final input encoding is calculated by performing dot-product attention over enc(m c1 ), enc(m c K ) and adding the resulting weighted sum to enc(x) to get the representation rep LHS (m c1 , m c K , x). Candidate responses are encoded separately with a Transformer to get rep RHS (r i ) for each i. The model is trained to minimize cross-entropy loss, considering Two-stage and End-to-end versions. Both versions find the most relevant knowledge and allow the decoder to attend over both knowledge and dialogue. Beam search of 5 is employed to select the best response. Generative models use BPE encoding for copying rare words from Wikipedia sentences. In the End-to-end version, a shared Transformer encoder is used to encode all candidates m ci. In the End-to-end version, a shared Transformer encoder is used to encode all candidates and the dialogue history. The encoded candidates are flattened into vectors using normalization to produce an attention prediction over the memory. The selected knowledge is concatenated with the dialogue encoding and passed into a Transformer decoder. The model is trained to minimize negative log-likelihood of the response utterance and additional supervision can be added for knowledge selection. In the Two-stage version, two separately trained models are used for knowledge selection and utterance prediction. Knowledge dropout is employed to improve decoder performance by preventing the model from attending to knowledge during training. Experimental setups and results are described, focusing on the ability of models to select knowledge and perform dialogue tasks. The proposed technique, knowledge dropout, enhances generator resilience and training speed. The experimental setups and results of models selecting knowledge and performing dialogue tasks are described. Transformers pretrained on a large dataset like Reddit outperform other baselines. The best performing Transformer model is used in a two-stage generative Memory Network for dialogue generation given knowledge. The study evaluates models for dialogue generation using knowledge in two settings: with human-chosen knowledge or predicted knowledge. Transformer Memory Networks are applied, showing improved performance. Generative experiments compare End-to-end and Two-stage Transformer Memory Network models to baselines. The study evaluates models for dialogue generation using knowledge in two settings: with human-chosen knowledge or predicted knowledge. Transformer Memory Networks show improved performance, with End-to-end and Two-stage models outperforming baselines in response predictions. The Two-stage model excels in using predicted knowledge, while the End-to-end model performs better with gold knowledge. The Two-stage model benefits from strong knowledge selection in gold knowledge experiments, while the End-to-end model is better at employing selected knowledge. Additional knowledge selection supervision in the End-to-end model improves performance on every metric. Knowledge dropout also helps. The conversation topic shifts to E-books preferences, with individuals expressing a preference for physical books over e-books. The conversation shifts to discussing preferences for physical books over e-books. A study compares Two-stage and End-to-end models in terms of knowledge selection and performance metrics. Human evaluation is conducted to assess engagingness and knowledge exhibited by the models. The study compares retrieval and generative models in engagingness and knowledge exhibited. Retrieval models outperform generative models in engagingness. Both retriever models with and without knowledge trend towards using knowledge, with knowledgeable versions obtaining higher Wiki F1 scores. Generative models show improved engagingness with the use of knowledge and convey more knowledge than counterparts without knowledge conditioning. The study compares retrieval and generative models in engagingness and knowledge exhibited. Generative models with knowledge conditioning show significantly improved engagingness ratings and convey more knowledge than counterparts without knowledge conditioning. The gap between retrieval and generative models is larger on unseen data, where retrieval models are limited to producing responses from the training set. Transformer Memory Network models are developed to conduct engaging open-domain conversations using large memory systems containing encyclopedic knowledge. The study focuses on developing Transformer Memory Network models for engaging open-domain conversations using encyclopedic knowledge. They introduce the Wizard of Wikipedia dataset to train and evaluate these models, aiming to bridge the gap between retrieval and generative models. Future work includes exploring the relationship between knowledge-grounded dialogue and existing QA tasks. The study introduces the Wizard of Wikipedia dataset to develop Transformer Memory Network models for engaging open-domain conversations using encyclopedic knowledge. The dataset includes conversations where a wizard has access to an information retrieval system over Wikipedia to ask and answer questions. The aim is to create an engaging conversational agent by combining knowledge retrieval and reasoning in dialogue. The Wizard of Wikipedia dataset includes conversations where the wizard asks questions 39.5% of the time, makes statements 49.3% of the time, and engages in various dialogue acts. The dataset contains \u223c1000 personas with 4-5 sentences describing interests, mapped to relevant Wikipedia pages. This provides 1,431 topics for conversation starters during data collection. The dataset includes 1,431 topics for conversation starters. Additional experiments were conducted on knowledge selection tasks, showing that the retrieval system could be improved with auxiliary loss. Human evaluation experiments analyzed conversations, revealing common errors and behaviors exhibited in different settings. The conversations are re-tokenized and lowercased for analysis in a single-blind setup. Human-human conversations differ from bot conversations, with humans engaging in more small talk. Models attempt to produce factual sentences but can be improved with SQuAD-like training data. The retriever without knowledge is prone to non sequiturs, while the retriever with knowledge sticks to the topic but struggles with subject changes by humans. The retriever without knowledge tends to go off-topic, while the retriever with knowledge sticks to the chosen topic but struggles with subject changes by humans. Additionally, a two-stage retrieval system was tested, showing improved performance in F1 but not Recall@1 compared to the best retrieval method. Human experiments were conducted to calculate the Wiki F1 score for comparison. The retrieval system with knowledge produces similar but factually inaccurate answers, such as listing locations in Greece when asked about parts of Ireland. Despite this, it offers inviting responses for a more natural conversational flow. Conversations with the retriever with knowledge can be seen in FIG5 for both seen and unseen topics. The generator without knowledge exhibits typical behaviors of seq2seq systems, including local repetition. The generator with knowledge, unlike the generator without knowledge, shows fewer issues with repetition and can act as a selfish conversationalist. It sometimes produces inaccurate statements but generally produces accurate ones by copying large fragments from Wikipedia. Despite formulaic responses, it successfully generalizes to unseen topics using Wikipedia knowledge."
}