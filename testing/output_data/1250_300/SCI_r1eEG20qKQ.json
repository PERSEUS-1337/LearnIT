{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization involves adapting regularization hyperparameters for neural networks by fitting compact approximations to the best-response function. This is achieved by modeling the best-response as a single network with gated hidden units, allowing for scalable approximations. The approach does not require differentiating the training loss with respect to the hyperparameters, enabling tuning of discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training without the need for differentiating the training loss. It outperforms other hyperparameter optimization methods on large-scale deep learning problems by discovering hyperparameter schedules that can outperform fixed values. Regularization hyperparameters like weight decay, data augmentation, and dropout are crucial for neural network generalization but challenging to tune. Popular optimization methods like grid search, random search, and Bayesian optimization work well with low-dimensional spaces but ignore structure for faster convergence and require multiple training runs. Hyperparameter optimization can be formulated as a bilevel optimization problem, aiming to find the best-response function w * (\u03bb) to minimize validation loss. Approximating w * with a parametric function offers speed-ups over black-box methods, but computing w * exactly is challenging due to the high-dimensional optimization problem. Proposing a scalable approximation method for high-dimensional optimization problems by approximating the best-response function w * with a parametric function \u0175 \u03c6. This approach involves jointly optimizing \u03c6 and \u03bb, updating \u03c6 to approximate w * in a neighborhood around current hyperparameters, and using \u0175 \u03c6 as a proxy for w * in the optimization process. The proposed Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering a compact approximation by modeling the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of the hyperparameters. The proposed Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering a compact approximation by modeling the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of the hyperparameters. STNs have advantages over other hyperparameter optimization methods, such as easy implementation and outperforming fixed hyperparameter settings empirically. The training algorithm does not require differentiating the training loss with respect to the hyperparameters, allowing tuning of discrete hyperparameters. The proposed Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering a compact approximation by modeling the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of the hyperparameters. STNs have advantages over other hyperparameter optimization methods, such as easy implementation and outperforming fixed hyperparameter settings empirically. Bilevel optimization problems, including hyperparameter optimization, GAN training, meta-learning, and neural architecture search, can be formulated in machine learning. In machine learning, bilevel problems are challenging to solve due to their NP-hard nature. Most work focuses on linear, quadratic, and convex functions, but we aim to find local solutions in the nonconvex, differentiable, and unconstrained setting. We seek to design a gradient-based algorithm for solving these problems, as it offers significant speed-ups over black-box optimization methods. The simplest method is simultaneous gradient descent, updating parameters using gradient information. The algorithm for solving Problem 4 involves using gradient information for speed-ups over black-box optimization methods. Simultaneous gradient descent updates parameters but may give incorrect solutions. A more principled approach is to use the best-response function to convert Problem 4 into a single-level problem. This method requires unique optima for Problem 4b and differentiability of the best-response function. The conditions for differentiability of the best-response function and unique optima for Problem 4b are difficult to verify. Sufficient conditions are given for these to hold in a neighborhood of a point. The gradient of the upper-level objective decomposes into direct and response gradients, with the latter stabilizing optimization by converting the bilevel problem into a single-level one. This conversion ensures gradient stability even when simultaneous gradient descent is possible. The response gradient can stabilize optimization by converting the bilevel problem into a single-level one, ensuring gradient stability. Gradient-based hyperparameter optimization methods struggle with discrete and stochastic hyperparameters, but promising approaches to approximate the best-response function directly have been proposed. Promising approaches to approximate w * directly were proposed by Lorraine & Duvenaud (2018). They introduced global and local approximation algorithms to model w * using differentiable functions. The global approximation involves a hypernetwork \u0175 \u03c6, while the local approximation focuses on modeling w * in a neighborhood around the upper-level parameter \u03bb. The algorithms aim to stabilize optimization and improve gradient-based hyperparameter optimization methods. The approach involves using a factorized Gaussian noise distribution with a fixed scale parameter \u03c3 to minimize an objective function. An alternating gradient descent scheme is employed to update parameters \u03c6 and \u03bb. The method has shown success with L2 regularization on MNIST but its applicability to different regularizers or larger problems is uncertain. Challenges include handling high-dimensional w, setting \u03c3, and adapting to discrete and stochastic hyperparameters. The proposed algorithm constructs a memory-efficient best-response approximation \u0175 \u03c6 for large neural networks, automatically adjusts the scale of the neighborhood for training \u03c6, and easily handles discrete and stochastic hyperparameters. The proposed algorithm introduces Self-Tuning Networks (STNs) that update hyperparameters online during training. It approximates the best-response for weight matrices and biases through an affine transformation of hyperparameters, making it tractable and memory-efficient. This architecture adds a correction to pre-activations to account for hyperparameters, improving performance in neural networks. The best-response architecture for weight matrices and biases introduces a correction for hyperparameters, enabling parallelism and improving sample efficiency. It can be implemented by replacing existing modules in deep learning libraries with \"hyper\" counterparts. The best-response function can be represented exactly using a linear network with Jacobian norm regularization. The best-response function can be represented using a linear network with Jacobian norm regularization. It involves a 2-layer linear network with modulated hidden units, a squared-error loss with L2 penalty on the Jacobian, and a sigmoidal gating of hidden units. This architecture improves sample efficiency and can be implemented by replacing existing modules in deep learning libraries. The best-response function can be approximated using a linear network with sigmoidal gating of hidden units. By replacing the sigmoidal gating with linear gating for a narrow hyperparameter distribution, an affine approximation can be achieved. This approach ensures convergence to a local optimum when minimizing the objective function. The best-response function can be approximated using a linear network with sigmoidal gating of hidden units. Minimizing E \u223cp( |\u03c3) [f (\u03bb + ,\u0175 \u03c6 (\u03bb + ))] yields the correct best-response Jacobian, ensuring convergence to a local optimum. The sampled neighborhood size affects the gradient matching of the approximation. Entries of \u03c3 control the hyperparameter distribution scale for training \u03c6. The flexibility of \u0175 \u03c6 must balance capturing the best-response and local shape. Adjusting \u03c3 during training based on the sensitivity of the upper-level objective to the sampled hyperparameters is proposed to address issues related to the flexibility of \u0175 \u03c6 in capturing the best-response and local shape. The objective function includes an entropy term weighted by \u03c4 \u2208 R +, which enlarges the entries of \u03c3. This approach interpolates between variational optimization and variational inference, aiming for better training and representation learning. The algorithm proposed by Khan et al. (2018) aims to balance the optimization of hyperparameters by minimizing a term that moves probability mass towards an optimum \u03bb * while avoiding a heavy entropy penalty. The algorithm can tune hyperparameters that other gradient-based algorithms cannot, such as discrete or stochastic hyperparameters, using an unconstrained parametrization \u03bb \u2208 R n. The training algorithm involves adjusting \u03c3 based on the sensitivity of the upper-level objective to the sampled hyperparameters, aiming for better training and representation learning. The algorithm proposed by Khan et al. (2018) aims to optimize hyperparameters by minimizing a term that moves probability mass towards an optimum \u03bb * while avoiding a heavy entropy penalty. It can tune discrete or stochastic hyperparameters using an unconstrained parametrization \u03bb \u2208 R n. The training algorithm involves adjusting \u03c3 based on the sensitivity of the upper-level objective to the sampled hyperparameters for better training and representation learning. The algorithm alternates between updating \u03c6 and updating \u03bb and \u03c3 to minimize specific functions. The non-differentiability of r due to discrete hyperparameters is not an issue, and the derivative estimation involves the reparametrization trick. The algorithm by Khan et al. (2018) optimizes hyperparameters by minimizing a term that moves probability mass towards an optimum \u03bb * without heavy entropy penalties. It can tune discrete or stochastic hyperparameters using an unconstrained parametrization \u03bb \u2208 R n. The training algorithm adjusts \u03c3 based on the sensitivity of the upper-level objective to sampled hyperparameters for better training and representation learning. The reparametrization trick is used for derivative estimation. The method was applied to convolutional networks and LSTMs, resulting in self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). The method by Khan et al. optimizes hyperparameters by moving probability mass towards an optimum \u03bb * without heavy entropy penalties. It can tune discrete or stochastic hyperparameters using an unconstrained parametrization \u03bb \u2208 R n. The training algorithm adjusts \u03c3 based on the sensitivity of the upper-level objective to sampled hyperparameters for better training and representation learning. STNs were applied to convolutional networks and LSTMs, resulting in self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). STNs discover schedules for adapting hyperparameters online, outperforming fixed hyperparameter values. ST-LSTM's schedule for output dropout outperforms a fixed rate found by grid search, achieving 82.58 vs 85.83 validation perplexity. The ST-LSTM discovered a schedule for output dropout that outperformed a fixed rate found by grid search, achieving 82.58 vs 85.83 validation perplexity. This improvement was not due to stochasticity introduced by sampling hyperparameters or the limited capacity of the model. STNs also outperformed standard LSTM models when perturbing dropout rates and showed evidence that the schedule itself was beneficial for performance. The ST-LSTM discovered a schedule for output dropout that outperformed a fixed rate found by grid search. The schedule improved performance, showing evidence that the schedule itself was responsible for the improvement. The STN schedule implements a curriculum by using a low dropout rate early in training. The ST-LSTM implemented a curriculum by using a low dropout rate early in training and gradually increasing it later on for better generalization. The model was evaluated on the PTB corpus with a 2-layer LSTM and 650 hidden units per layer. Seven hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. The best results were obtained with a fixed perturbation scale of 1 for the hyperparameters. The ST-LSTM implemented a curriculum with low dropout rates early in training and gradually increasing it later for better generalization. Seven hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. STNs outperformed other methods in achieving lower validation perplexity more quickly. The schedules found for each hyperparameter in STNs were nontrivial, with different forms of dropout used at various stages of training. ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture. The ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture. Fifteen hyperparameters were tuned, including activation dropout, input dropout, and data augmentation parameters. STNs outperformed grid search, random search, and Bayesian optimization in finding better hyperparameter configurations in less time. The experimental setup details are provided in Appendix E, and the hyperparameter schedules found by STN are shown in FIG3. The experimental setup details are provided in Appendix E. STNs find better hyperparameter configurations in less time than other methods. Bilevel Optimization involves replacing the lower-level problem with its KKT conditions as constraints for the upper-level problem. Hypernetworks are functions mapping to the weights of a neural net, used to generate weights for modern CNNs and RNNs. Hypernetworks are functions mapping to the weights of a neural net. Predicting weights in CNNs has been developed in various forms. Gradient-Based Hyperparameter Optimization has two main approaches, one approximating w * (\u03bb 0 ) using w T (\u03bb 0 , w 0 ) after T steps of gradient descent, and the other using the Implicit Function Theorem. The Implicit Function Theorem is used to derive \u2202w * /\u2202\u03bb(\u03bb 0 ) for hyperparameter optimization in neural networks. Different approaches struggle with certain hyperparameters and use Hessian-vector products with conjugate gradient solvers. Bayesian optimization models the conditional probability of performance given hyperparameters and dataset iteratively. Bayesian optimization models the conditional probability of performance given hyperparameters and dataset iteratively, using various methods to balance exploration and exploitation. Model-free approaches like random search, Successive Halving, and Hyperband adaptively allocate resources to promising configurations for hyperparameter optimization. Hyperparameter optimization methods include random search, Successive Halving, and Hyperband. Population Based Training (PBT) considers schedules for hyperparameters by training a population of networks in parallel and periodically evaluating their performance. PBT replaces under-performing networks with better-performing ones and copies their hyperparameters for training new network clones. STNs use gradients to tune hyperparameters during training. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting hidden units. They use gradient-based optimization to tune various regularization hyperparameters, including discrete ones. STNs discover hyperparameter schedules that outperform fixed hyperparameters, achieving better generalization performance in less time on large-scale problems. This approach offers a compelling path towards automated hyperparameter tuning for neural networks. The text discusses the best-response of parameters to hyperparameters in neural networks, supported by various awards and programs. It delves into the mathematical proofs and conditions for optimization, highlighting the Jacobian decomposition and the existence of a unique function. The discussion emphasizes the continuous differentiability and positive definiteness of the Hessian in the context of hyperparameter tuning. The text discusses the best-response of parameters to hyperparameters in neural networks, supported by various awards and programs. It delves into the mathematical proofs and conditions for optimization, highlighting the Jacobian decomposition and the existence of a unique function. The discussion emphasizes the continuous differentiability and positive definiteness of the Hessian in the context of hyperparameter tuning. By the Implicit Function Theorem, a unique continuously differentiable function w * exists such that \u2202f /\u2202w(\u03bb, w * (\u03bb)) = 0 for \u03bb \u2208 V and w * (\u03bb 0 ) = w 0. This leads to the conclusion that w * (\u03bb) is the unique solution to Problem 4b for all \u03bb \u2208 U, following second-order sufficient optimality conditions. The SVD decomposition of the data matrix X is presented, simplifying the function y(x; w) to standard L2-regularized least-squares linear regression. The text discusses the best-response of parameters to hyperparameters in neural networks, supported by various awards and programs. It delves into the mathematical proofs and conditions for optimization, highlighting the Jacobian decomposition and the existence of a unique function. By assumption, f is Q, so that y(x; w) = s Qx = u x. The optimal solution u * (\u03bb) minimizing Equation 19 is given by standard L2-regularized least-squares linear regression. The optimal solution u * to the unregularized version of Problem 19 is given by Q 0 s 0 = u *, where s 0 = D \u22121 U t. There are not unique solutions to Problem 13, so we take any functions Q(\u03bb), s(\u03bb) which satisfy Q(\u03bb) s(\u03bb) = v * (\u03bb) as \"best-response functions\". The text discusses the optimization of a quadratic function in neural networks, with a focus on mathematical proofs and conditions. By assuming f is quadratic, the existence of unique functions and optimal solutions are explored. The text also delves into matrix-derivative equalities and simplifying expressions using linearity of expectation. The text discusses optimizing a quadratic function in neural networks, focusing on mathematical proofs and conditions. It explores matrix-derivative equalities and simplifying expressions using the linearity of expectation. The model parameters were updated, but hyperparameters were not. Training was terminated when the learning rate dropped below 0.0003. Variational dropout was tuned on the input to the LSTM, hidden state between LSTM layers, and output of the LSTM. Embedding dropout was also tuned. The text discusses optimizing a quadratic function in neural networks, focusing on mathematical proofs and conditions. It explores matrix-derivative equalities and simplifying expressions using the linearity of expectation. The model parameters were updated, but hyperparameters were not. Training was terminated when the learning rate dropped below 0.0003. Variational dropout was tuned on the input to the LSTM, hidden state between LSTM layers, and output of the LSTM. Embedding dropout was also tuned. Dropout was applied to the input, hidden state, and output of the LSTM. Embedding dropout removed certain words from all sequences. DropConnect was used to regularize the hidden-to-hidden weight matrix. Activation regularization penalized large activations, while temporal activation regularization was a slowness regularizer. Scaling coefficients were tuned for AR and TAR. Hyperparameter ranges for baselines were specified. The text discusses optimizing a quadratic function in neural networks, focusing on mathematical proofs and conditions. It explores matrix-derivative equalities and simplifying expressions using the linearity of expectation. The model parameters were updated, but hyperparameters were not. Training was terminated when the learning rate dropped below 0.0003. Variational dropout was tuned on the input to the LSTM, hidden state between LSTM layers, and output of the LSTM. Embedding dropout was also tuned. Dropout was applied to the input, hidden state, and output of the LSTM. Embedding dropout removed certain words from all sequences. DropConnect was used to regularize the hidden-to-hidden weight matrix. Activation regularization penalized large activations, while temporal activation regularization was a slowness regularizer. For AR and TAR, scaling coefficients were tuned. Baselines had specific hyperparameter ranges. Additional details on CNN experiments are presented, including training procedures and hyperparameter optimization methods. The ST-CNN's parameters were optimized using SGD with a learning rate of 0.01 and momentum of 0.9 on mini-batches of size 128. Hyperparameters were optimized using Adam with a learning rate of 0.003. Training alternated between best-response approximation and hyperparameters with a schedule of T train = 2 steps and T valid = 1 step. The model had five epochs of warm-up, with an entropy weight of \u03c4 = 0.001. Cutout length was restricted to {0, 24} and cutout holes to {0, 4}. Dropout rates and data augmentation noise parameters were initialized to 0.05. ST-CNN was found to be robust to hyperparameter initialization, with low regularization aiding optimization in the initial epochs. The ST-CNN model showed robustness to hyperparameter initialization, with low regularization aiding optimization in the initial epochs. Hyperparameter schedules were linked to curriculum learning, where increasing dropout over time made the learning problem more challenging. This approach has been suggested to improve optimization and generalization. Hyperparameter schedules implement a form of curriculum learning by increasing dropout over time to make the learning problem more difficult. Grid searches were used to understand the effects of different hyperparameter settings, showing that greedy schedules can outperform fixed values. Validation perplexity was measured for input and output dropout combinations, with larger dropout rates performing better as training progresses. Greedy hyperparameter schedules were shown to have potential benefits in optimization. The benefits of greedy hyperparameter schedules were demonstrated through a fine-grained grid search for dropout values. By adjusting dropout rates over training epochs, better generalization was achieved compared to fixed hyperparameter values. PyTorch code listings for constructing ST-LSTMs and ST-CNNs were provided in this section. In Section 4.1, PyTorch code listings for constructing ST-LSTMs and ST-CNNs using HyperLinear and HyperConv2D classes are provided, along with simplified optimization steps for the training and validation sets."
}