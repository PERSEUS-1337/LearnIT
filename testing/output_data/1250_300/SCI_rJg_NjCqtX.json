{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework using spelling error correction, tokenization, and neural models was proposed to standardize non-systematic names to systematic names, achieving 54.04% accuracy on the test dataset. There are over 100 million named chemical substances worldwide, each assigned systematic names based on their structures. There are over 100 million named chemical substances worldwide, each assigned systematic names based on their structures. Besides systematic names, chemical substances may also have common names or proprietary names for various reasons. Chemical substances in the pharmaceutical industry often have proprietary names to differentiate products. Extracting chemical information from text relies on standard chemical names. Various databases like PubChem store chemical information. Converting systematic names allows for generating other representations like SMILES and BID13. Converting systematic chemical names to other representations like SMILES and InCHI allows for generating structural formulas. Errors in non-systematic names can be categorized into spelling, ordering, common name, and synonym errors. Errors in non-systematic chemical names can include ordering, common name, and synonym errors. These errors can occur simultaneously in a single name, making the conversion to systematic names challenging. A framework has been proposed to automatically correct these errors, including spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. The framework for standardizing chemical names includes spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. Previous work by BID2 used transformation rules and online databases, but our approach utilizes a sequence to sequence model for better application potential. The framework for standardizing chemical names involves a sequence to sequence model, trained end-to-end without external chemical knowledge. The model achieves 54.04% accuracy on test data extracted from Chemical Journals with High Impact Factors. The corpus contains 384,816 data pairs of non-systematic and systematic chemical names, with an overview of Levenshtein distance distribution shown in FIG1. In the experiment, 80%, 19%, and 1% of the data are used as training, test, and development sets respectively to correct spelling errors in chemical substance names. The names are separated into elemental words and vocabularies are created from systematic and non-systematic names to identify and correct errors. The experiment involves using 80% of the data for training, 19% for testing, and 1% for development to correct spelling errors in chemical substance names. Systematic and non-systematic names are split to create vocabularies, with a focus on common names or synonyms. A BK-Tree structure is used for efficient correction searches based on Levenshtein distance. BK-Tree is utilized to correct spelling errors in chemical substance names by efficiently searching for the closest vocabulary item based on Levenshtein distance. It allows for easy insertion of new training data, making it scalable. The process involves separating names into elemental words, inputting them into the BK-Tree, correcting errors, and combining the words to form the full name. This method reduces noise in training sequence to sequence models. BK-Tree is used to correct spelling errors in chemical names by searching for the closest vocabulary item based on Levenshtein distance. To apply the sequence-to-sequence model, chemical names are tokenized using Byte Pair Encoding (BPE) BID11. The symbol set is initialized by splitting names into characters and iteratively counting symbol pairs to merge and create new symbols. The symbol set is initialized with single characters and merged iteratively to create new symbols. The final symbol set size is determined by the initial character size plus the merge operations. Byte Pair Encoding (BPE) is chosen for tokenization to handle out-of-vocabulary issues and separate names into meaningful subwords. BPE examples for chemical names are provided. The tokenized pairs are used to train a sequence-to-sequence model, which consists of two recurrent neural networks (RNN). In this work, a sequence-to-sequence model is trained using split pairs. The model consists of an encoder with a multilayer bidirectional LSTM and a decoder that generates target sequences. The encoder generates a context vector by combining all hidden states, while the decoder calculates the probability of output sequences. In the experiment, various parameters were tested such as the threshold of the BK-Tree and the number of merge operations at the BPE stage. The dimensions of word embeddings and hidden states in the sequence to sequence model were set to 500, with 2 layers in both the encoder and decoder. Training involved spelling error correction for non-systematic names and optimization using stochastic gradient descent with a cross-entropy loss function. During training, parameters of the sequence to sequence model are optimized using stochastic gradient descent with a cross-entropy loss function. The loss is computed over minibatches of size 64 and normalized. We initialize weights with a random uniform distribution, set the initial learning rate to 1.0, and apply decay with a factor of 0.5 every epoch after epoch 8 or when perplexity does not decrease on the validation set. The dropout rate is 0.3, and the model is trained for 15 epochs with a beam size of 5 for decoding. Additionally, an experiment is conducted using a Statistical Machine Translation model with specific training settings. The training sequences are set to 80 and a 3-grams language model is applied using KenLM BID4. Data augmentation is used to handle noisy data, specifically spelling errors. Four types of error insertion methods are applied with equal probability. Accuracy and BLEU score BID10 are used to measure standardization quality. Accuracy is calculated based on successfully standardized non-systematic names. The experiment results for different models on the test dataset are shown in TAB3, with the combination of spelling error correction, BPE tokenization, and sequence to sequence model achieving the best performance. The framework shows significant improvement compared to the SMT model and the ChemHits system. Results for different numbers of BPE merge operations are shown in TAB4, with 5000 being the optimal value. Spelling error correction and data augmentation are beneficial for the framework, with spelling error correction outperforming data augmentation. The experiment results show that spelling error correction and data augmentation are beneficial for the framework. Spelling error correction outperforms data augmentation, with examples demonstrating the sequence to sequence model's ability to fix non-alphabet spelling errors and correct synonym errors. The experiment results demonstrate the effectiveness of spelling error correction and data augmentation in the framework. The sequence to sequence model successfully corrects non-alphabet spelling errors and synonym errors, as shown in the visualization of attentions in FIG2. Examples of standardized non-systematic names are provided, highlighting the model's ability to find relations and correct errors. In this section, the fail standardization attempts of the system are analyzed by randomly selecting 100 samples of failed attempts and labeling their error types. The most confusing error type is synonym error, while the system performs well at spelling error. Common errors are challenging due to the difficulty in finding a rule between an unseen common name and its systematic name. Among the samples, some are nearly correct, some are totally incorrect, and the rest are partially correct. Nearly half of the non-systematic names are not successfully standardized. The accuracy for systematic names of different lengths is shown in Figure 6. The system analyzed 100 samples of failed attempts, with errors ranging from synonym errors to spelling errors. Nearly half of the non-systematic names were not standardized successfully. The framework performed best for systematic names of length between 20 and 40 but poorly for names longer than 60. The model does not consider chemical rules, leading to some names that disobey these rules. The framework proposed in this work aims to automatically convert non-systematic chemical names to systematic names. It includes spelling error correction, tokenization, and a sequence to sequence model, achieving an accuracy of 54.04% on the dataset. This approach outperforms previous rule-based systems and enables practical extraction of chemical information. The framework is end-to-end trained, data-driven, and independent of external chemical knowledge, opening up new research possibilities in chemical information extraction."
}