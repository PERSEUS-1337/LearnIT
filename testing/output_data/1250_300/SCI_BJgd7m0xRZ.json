{
    "title": "BJgd7m0xRZ",
    "content": "Anomaly detection in unlabeled data identifies non-conforming data points, including those from malicious attacks. One-Class Support Vector Machines (OCSVMs) are effective but vulnerable to sophisticated adversaries compromising training data integrity. To enhance security, a defense mechanism based on data contraction is proposed, adding uncertainty to OCSVMs to thwart adversaries. The approach successfully identifies anomalies by contracting data in low dimensional spaces, as shown through theoretical analysis and empirical evidence on benchmark datasets. Anomaly detection involves identifying non-conforming data points, crucial for various applications like network intrusion detection and fraud detection. One-Class Support Vector Machines (OCSVM) are effective in this task but can be vulnerable to adversarial perturbations. A proposed defense mechanism involves contracting data in low dimensional spaces to improve OCSVM performance significantly. Anomaly detection systems, like One-Class Support Vector Machines (OCSVM), are effective in identifying non-conforming data points but can be vulnerable to adversarial attacks that compromise their integrity. Adversaries may manipulate training data to gradually inject malicious data and undermine the decision-making capabilities of learning algorithms. This can lead to decreased performance or avoidance of attack detection. In order to undermine learning algorithms, adversaries may manipulate training data to distort the representation learned by the system. It is crucial to enhance the resistance of OCSVMs against adversarial attacks targeting the integrity of training data through distortions, especially with the rise of automation in various applications. To enhance the resistance of OCSVMs against adversarial attacks, a nonlinear data projection algorithm is utilized to increase attack resistance under realistic assumptions. Nonlinear random projections facilitate large-scale, data-oriented decisions by reducing optimization parameters and variables, improving training and evaluation times without compromising accuracy. This is crucial in securing machine learning systems against adversaries manipulating training data to distort learned representations. Selective nonlinear random projections can increase the attack resistance of OCSVMs under adversarial conditions. By projecting a dataset using a carefully chosen projection matrix with random elements, the original data distribution properties are preserved with minor perturbations. This additional layer of security makes it virtually impossible for adversaries to guess the projection mechanism used by the learner. Selective nonlinear random projections can enhance the security of OCSVMs against attacks by preserving data distribution properties with minor perturbations. This makes it difficult for adversaries to guess the projection mechanism used by the learner. The paper analytically derives an upper bound on the weight vector length of an OCSVM trained on a nonlinearly transformed dataset in a lower dimensional space, showing increased resistance to adversarial distortions. The paper proposes a method to increase the attack resistance of OCSVMs by using selective nonlinear random projections. It aims to add unpredictability through data transformation, enhancing security against adversarial attacks. Additionally, the efficiency of kernel machines is improved by embedding random projections, such as Random Kitchen Sinks, into the kernel formulation. The paper introduces Random Kitchen Sinks (RKS), a data-independent method that approximates kernel functions by mapping data to a low-dimensional randomized feature space. This method improves the efficiency of kernel machines and has been applied to other types of kernel machines. Additionally, BID4 introduced Randomized One-class SVMs (R1SVM), an unsupervised anomaly detection technique using randomized, nonlinear features. Our work focuses on using random projections as a defense mechanism for One-class SVMs under adversarial conditions. Existing research has not yet applied Rahimi and Recht's method to address adversarial learning for anomaly detection with One-class SVMs. This problem has sparked various studies in the machine learning community, such as the introduction of an Adversarial Support Vector Machine (AD-SVM) model. The AD-SVM model introduces additional constraints to thwart adversary attacks, but may lead to unsatisfactory results. Our work focuses on unsupervised learning with OCSVMs, different from previous binary SVM approaches. Recent studies show how adversarial attacks can manipulate data imperceptibly, posing risks to human safety. Our framework combines adversarial learning, anomaly detection with OCSVMs, and random projections for defense. This paper introduces a framework that combines adversarial learning, anomaly detection using OCSVMs, and randomized kernels to address the issue of adversarial attacks on learning systems. The adversary aims to disrupt the learning process by modifying training data, leading to distorted datasets that challenge the decision-making capability of the learner. The learner cannot distinguish between normal data and adversarial perturbations in the training dataset. The adversary can manipulate the perturbations based on its knowledge of the learning system. The learner projects the data to a lower dimensional space to minimize the impact of attacks, using a non-linear transformation with a randomly drawn vector. This transformation helps approximate nonlinear kernels like the Radial Basis Function. In this paper, the anomaly detection problem is addressed using the OCSVM algorithm, which separates training data with a maximal margin in a transformed space. The dual form of the OCSVM algorithm is represented in matrix notation, with key parameters such as Lagrange multipliers and an upper bound on support vectors. The margin of the optimal separating hyperplane is determined by specific calculations involving the solution vector. The OCSVM algorithm uses a parameter \u03bd to define support vectors and outliers. The margin of the separating hyperplane is determined by \u03c1/w^2, with \u03c1 recoverable using support vectors. Integrity attacks aim for false negatives by moving the hyperplane away from normal data, while availability attacks create false positives by shifting the margin towards normal data. This study focuses on integrity attacks, where the adversary minimizes the separation margin by injecting malicious data. Targeted attacks involve smuggling specific anomalies across the margin using a non-zero displacement vector. The adversary in targeted attacks can move anomalies across the margin by adding a displacement vector. The attack model is inspired by a restrained attack model, where perturbed anomalies are injected into the training set to be seen as normal data points by the learning algorithm. The severity of the attack is controlled by a parameter, with anomalies pushed closer to the normal data cloud considered as moderate attacks. The attack severity, controlled by parameter s attack \u2208 [0, 1], is proportional to the distance from the normal data cloud. An anomaly closer to the normal data cloud (small s attack) is a moderate attack, while one farther away (large s attack) is severe. The attacker distorts data points towards a target, requiring computational effort and knowledge of data distribution. The attacker uses the normal data cloud centroid as the target point for distorting anomaly data points. The adversary can distort anomaly data points by adding \u03ba ij to each attribute in the original feature space. Different attack severities (s attack) and percentages of distorted anomaly data points (p attack) can be orchestrated. Increasing s attack moves anomaly data points farther from the normal data cloud, altering the separating hyperplane. Anomaly data points are moved farther away from the normal data cloud, altering the separating hyperplane. Using random projections to reduce dimensionality introduces uncertainty to the adversary-learner problem, increasing security. However, some projections may result in data overlap between classes. Minimizing the impact of adversarial inputs is crucial for enhancing attack resistance in a learning system. To enhance attack resistance in a learning system, a projection that conceals adversarial distortions should be selected. A compactness measure based on Dunn's index BID2 is proposed to identify suitable projection directions in a one-class problem. The learner calculates the compactness of multiple random projections of the training data and selects the one with the highest compactness for increased security. In order to enhance attack resistance in a learning system, a compactness measure based on Dunn's index BID2 is proposed to identify suitable projection directions in a one-class problem. The learner calculates the compactness of multiple random projections of the training data and selects the one with the highest value, which provides the best attack resistance. The objective is to identify the smallest hypersphere that contains the training data set, either in the original dimensional space or in a transformed space, to minimize the effects of attacks. The learner minimizes attack effects by formalizing the approach with random projection parameters. Analyzing perturbations on the margin of separation of the OCSVM, an upper bound on w2 is derived based on assumptions about small, positive distortions made by the adversary in the original feature space. The adversary's distortions are assumed to be small and positive, as they are more likely to go undetected. The defender can minimize attack effects by using random projection parameters and analyzing perturbations on the margin of separation of the OCSVM. The length of the weight vector is bounded above for small, positive distortions, and the strength of the adversary's attacks increases with the strength of the attacks. The defender can tighten the upper bound of the weight vector by reducing dataset dimensionality. Empirical validation is provided in TAB2, and Appendix A offers a proof. Our defense mechanism is effective on MNIST, CIFAR-10, and SVHN datasets against directed attacks. Single-class datasets are generated for evaluation, aiming to classify anomalies as normal data points. The adversary aims to deceive the learner by classifying anomalies as normal data points during evaluation. Two test sets are created for each dataset: a clean test set and a distorted test set. The datasets are normalized, and nonlinear projections are based on the local intrinsic dimensionality. The learner selects the local intrinsic dimensionality (LID) of the dataset and performs nonlinear transformations to achieve the highest compactness. The transformed training set is then used to train an OCSVM with a linear kernel, keeping the \u03bd parameter fixed. The model is evaluated using test sets, and for comparison, an OCSVM is also trained using an undistorted training set. The performance of OCSVMs trained on nonlinearly transformed data is 2-7% higher than OCSVM trained on original feature space. Training time is significantly reduced with nonlinear transformations. F-scores decrease between different training and test sets, indicating OCSVM trained on clean data performs better against adversarial samples. The f-scores decrease between different training and test sets, showing that OCSVM trained on clean data can identify adversarial samples better. Adversaries can manipulate OCSVMs by crafting adversarial data points. Lowering the dimension can increase f-scores, but further reduction decreases them. Detecting adversarial samples is improved by projecting data to a lower dimensional space. The false positive rate of OCSVMs under integrity attacks shows a significant improvement in detecting adversarial samples. The false positive rate of OCSVMs under integrity attacks shows a significant improvement in detecting adversarial samples, with a 23% increase on CIFAR-10 and a 31% increase on MNIST. However, reducing dimensions below a certain threshold leads to a decline in OCSVM performance. The reduction in dimensionality reduces the effects of adversarial data points but also results in a loss of useful information. The effectiveness of the bound derived in Theorem 1 is shown in TAB2, with the upper bound becoming tighter under dimension reduction. The experiments demonstrate that OCSVMs are vulnerable to integrity attacks, but projecting a distorted dataset to a lower dimension can increase model robustness. Performance declines when reducing dimensionality beyond a certain threshold, but projected spaces show comparable performance with less computational burden. The study combines unsupervised anomaly detection with OCSVMs and random projections for dimensionality reduction under adversarial conditions. The study combines unsupervised anomaly detection with OCSVMs and random projections for dimensionality reduction under adversarial conditions. OCSVMs can be significantly affected by an adversary with access to the data they are trained on. Random projections add uncertainty, making it harder for an adversary to guess the learner's details, thus enhancing system security. Our approach enhances system security by reducing the impact of adversarial perturbations and making the search space unbounded. Future work will explore using our approach with other learning algorithms and optimizing the number of dimensions for data transformation. Game-theoretical formulations of anomaly detection and adversarial learning under dimensionality reduction techniques will be studied, along with \"boiling frog\" type attacks. Our approach aims to enhance system security by mitigating adversarial perturbations and expanding the search space. Future work includes exploring the application of our approach with different learning algorithms and optimizing data transformation dimensions. Additionally, we plan to investigate game-theoretical formulations of anomaly detection and adversarial learning problems, including \"boiling frog\" type attacks. The solution for the primal problem in the projected space with adversarial distortions is defined as w*. The optimal solution for the OCSVM without any distortion would give a value less than or equal to the value given by \u03b1. Define w*p as the primal solution optimization in the projected space without adversarial perturbations."
}