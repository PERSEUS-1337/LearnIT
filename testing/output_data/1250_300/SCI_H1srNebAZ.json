{
    "title": "H1srNebAZ",
    "content": "Neural networks trained through stochastic gradient descent (SGD) have been around for over 30 years, but their inner workings remain elusive. This paper takes an experimental approach, focusing on the behavior of single neurons in deep neural networks. The experiments reveal that hidden neurons function as binary classifiers during training and testing, separating inputs into distinct categories. This insight sheds light on the internal mechanics of deep neural networks and has the potential to drive future theoretical and practical advancements. Deep neural networks offer surprises in image classification, with CNNs providing object detectors and universal representations. Networks with binary weights and activations are efficient. However, challenges remain in continuous learning, robustness, and unsupervised learning. The characteristics of SGD trained neural networks that enable these behaviors are still being explored. Deep neural networks present challenges in continuous learning, robustness, and unsupervised learning. Understanding the workings of hidden neurons is crucial for theoretical and practical advancements in neural networks. Experiments offer insights into the mechanisms supporting their success, paving the way for future developments. The study focuses on understanding hidden neurons in deep neural networks, particularly their encoding of information and dynamics during training. The main finding suggests that the way a neuron encodes information about the input remains unknown, and the dynamics of training that lead to this encoding are unexplored. This experimental approach aims to advance the understanding of these aspects of neurons. The encoding and dynamics of neurons during training are explored in this paper. The main finding is that a neuron's behavior can be characterized by a binary classifier. During training, neurons push activation of samples either up or down, creating a binary partition. This behavior is observed across different layers and networks, suggesting a defined behavior in hidden neurons. The behavior of hidden neurons in neural networks trained with stochastic gradient descent has been observed to exhibit a binary classification pattern. This behavior, which emerges across different layers and networks, raises intriguing questions for future investigations. Previous studies have focused on interpreting neuron function in terms of semantically relevant concepts, particularly in image classification tasks using convolutional neural networks. Various visualization methods have been developed to understand how the activation of a single neuron is related to input images. The analysis of neuron activation in convolutional neural networks reveals important parts of input images. Methods have been developed to reconstruct images from network representations. Recent work focuses on quantifying interpretability of extracted signals. Object detection emerges in units with highest activation, suggesting a binary encoding within the network. Further investigation is needed to understand the full information captured by feature maps. The study explores the binary encoding within trained networks and the emergence of concepts in neurons. It also relates to network binarization methods to reduce computational and memory requirements. The experiments validate the encoding of information in neurons, showing that binary values induce minimal loss in accuracy, suggesting that continuous activations may be redundant. Our work challenges the binary nature of individual neurons by showing that a bimodal activation pattern naturally emerges during training. This binary encoding is observed even in deep linear networks, indicating a lack of causal relation with the activation function. The study also highlights consistent patterns in gradients used by the learning algorithm. Our goal is to describe the behavior of neurons in a neural network by analyzing gradients with respect to activations on single samples. Neurons are associated with activation functions, and we define them based on the application of non-linear functions to single values. We consider different pixels of a feature map as different activations from the same neuron when studying statistical distributions. Three different architectures are experimented with, including a 2-layer MLP with 0.5 dropout. The study analyzes the behavior of neurons in different neural network architectures, including a 2-layer MLP with dropout, a 12-layer CNN with batchnorm, and a 50-layer ResNet. Different activation functions are explored, such as ReLU and sigmoid, as well as models without non-linear activation functions. Specific layers of these networks are referenced throughout the paper. The study focuses on analyzing the behavior of neurons in various neural network architectures, including a 2-layer MLP with dropout, a 12-layer CNN with batchnorm, and a 50-layer ResNet. The ResNet50 network is used, and layers are referred to by their stage and block index. Gradients with respect to activations are studied to gain insights into neuron behavior. The experiments were conducted using Keras and Tensorflow libraries. Most works analyzing neural network training dynamics focus on gradients of loss with respect to parameters. However, gradients with respect to activations can provide valuable insights into how a sample's representation is constructed. The study conducted standard training of cifar CNN and MNIST MLP networks, recording gradients of loss with respect to activations regularly. The average sign of partial derivatives with respect to activations was computed for each (input sample, neuron) pair to determine if increased activation benefits or penalizes sample classification. The study focused on analyzing gradients of loss with respect to activations during neural network training. The average sign of partial derivatives was computed to determine if increased activation benefits or penalizes sample classification. The histograms of computed average signs for randomly selected neurons showed consistent derivative signs throughout training, indicating the network's behavior as a binary classifier separating input categories. The study analyzed gradients of loss with respect to activations during neural network training. Neurons aim to partition input distribution into two categories with consistent activation gradients. Regularity in training is observed in activation gradients, with early layers showing more sign changes than later layers. This raises the question of whether regular dynamics are present in early layers but hidden by noise. The study observed that noise in gradients increases with depth in ReLU-networks, affecting the training dynamics. The linear version of the cifar CNN provides a clearer signal than the ReLU version. The presence of noise in gradients raises the question of whether it is an inherent aspect of learning or a result of the architecture and training process. The gradients suggest neurons attempt to separate input categories, which is further explored by categorizing samples based on their average activation partial derivative signs. The study categorizes samples based on their average activation partial derivative signs to observe how neurons attempt to separate input categories during training. The visualization shows a struggle to distinguish between 'low' and 'high' categories, with training stopping before complete separation. The dynamics are not a simple translation, as highlighted by the final highest pre-activations in yellow. This raises the question of what mechanism regulates well-partitioned samples in a neuron. The study examines how neurons categorize samples based on their activation partial derivative signs during training. Visualizations highlight the struggle to separate 'low' and 'high' categories, with distinct patterns observed in different layers. The mechanism behind well-partitioned samples in a neuron remains a question. The study explores how neurons categorize samples based on activation partial derivative signs during training. Categories are determined by the average sign of the loss function derivative, mainly fixed by network parameter initialization. The sign of the derivative signal is influenced by the input class, with output layer neurons dependent on class label. Category definition involves selecting a random subset of classes based on initial parameters. Further exploration of these mechanisms is suggested for future work. The study examines how neurons categorize samples based on activation partial derivative signs during training. Neurons operate as binary classifiers, with categories determined by the average sign of the loss function derivative. The separation of pre-activations into high and low categories is observed during training, indicating a complex encoding process. Further investigation into these mechanisms is recommended for future research. In this Section, the study tests if neurons encode information in a binary partition during testing by modifying activations through quantization and binarization strategies. The experiment aims to determine if neural networks can transmit relevant information with only two distinct values per neuron. The study also includes analysis of ResNet50 to explore the binary aspect of encodings and identify thresholds. The study aims to test if neural networks can transmit relevant information with only two distinct values per neuron through quantization of pre-activations. Percentiles are computed for each neuron to determine thresholds for quantization, with 11 thresholds tested. The experiment shows how accuracy on the test set is affected by quantization on different layers without training to adapt to the new pre-activation distribution. The experiment tested neural networks' robustness to quantization of pre-activations without training to adapt to the new distribution. Results show that most layers are robust to quantization, except for the conv1 layer in ResNet50. The study raises questions about how the signal is encoded and categorized in terms of thresholds and size of categories. The experiment involves a sliding window binarization method to understand how pre-activations are encoded in neural networks. Instead of using a single threshold, two thresholds are used to create a window where activations between them are mapped to 1. This method helps determine the size and categorization of pre-activations. The experiment involves a sliding window binarization method to understand how pre-activations are encoded in neural networks. Activations between specific percentiles are mapped to 1, preserving only if the activation was inside or outside the window. This method helps determine the organization of the representation and allows indirect observation of the binary partition used for encoding information. Test accuracy is monitored after reinitialization and retraining of subsequent layers to measure the usefulness of the transformed pre-activations. Linear classifier probes are used for analyzing ResNet50 layers to verify if the network can effectively utilize the binarized pre-activations. Quantization is performed on ResNet50 layers to analyze the network's ability to utilize binarized pre-activations. The experiment shows robustness to quantization, suggesting neurons transmit binary signals to subsequent layers. Different datasets and activation functions are used in the analysis. The experiment conducted on ResNet50 layers shows robustness to quantization, indicating neurons transmit binary signals. Results indicate a fuzzy partition of two categories with a threshold around percentile rank 50. The performance of the network improves the further away the window center is from rank 50, with a clear signal across all layers and networks. The experiment on ResNet50 layers shows robustness to quantization, indicating neurons transmit binary signals with a fuzzy partition around the 50th percentile rank. The binary behavior observed is not solely due to activation function thresholds, as it also emerges in linear networks without thresholding effects in hidden neurons. This challenges previous studies focusing on binarization at activation function thresholds. In this paper, the authors validate a hypothesis that neurons in a neural network behave like binary classifiers, separating two categories of inputs based on backpropagated gradients. The experiments on networks of different depths and widths confirm this behavior, which has implications for the interpretability of neurons. Previous studies focusing on binarization at activation function thresholds are challenged by these findings. The experiments reveal that neurons in a neural network tend to learn concepts that distinguish half of the observed samples, challenging previous studies on binarization at activation function thresholds. Further investigations are needed to understand the regularity of gradients in deep network layers and the role of backpropagated gradients in supporting the convergence of first layer neurons. This could provide a missing link for a complete characterization of training dynamics in deep networks. Our work offers a new perspective on the role of activation functions in deep networks. We suggest a local and precise role for activation functions in promoting the emergence of a binary encoding in neurons. This could lead to activation functions with well-positioned binarization thresholds for better network design. Additionally, our findings provide a new angle for addressing the generalization gap observed in previous studies. Our work provides a new angle for addressing the generalization gap in deep networks by suggesting a local and precise role for activation functions in promoting the emergence of a binary encoding in neurons. The observations indicate that neurons prioritize samples with common patterns during training, leading to a fuzzy, binary partition of inputs. This encoding pattern is consistent across all layers and networks, revealing important information for classification. The results confirm a binary partition of inputs in neurons across all layers and networks, providing important information for classification. The prioritization of samples with common patterns during training leads to a clear encoding pattern. The sign of the loss function derivative remains constant for neurons throughout training, suggesting a role in generalization abilities. The experimental investigation reveals that neurons aim to partition samples based on positive/negative derivative signs. Binarizing neuron pre-activations preserves task information. The observations raise questions about network learning capabilities and activation function design. The experimental investigation raises questions about network learning capabilities, including convergence in noisy environments and activation function design. Training information includes learning rates, batch sizes, and number of epochs for different layers. Architecture details and data augmentation usage are also provided. Gradients and pre-activations are recorded for various samples in different network stages. Training information for a specific network architecture is not provided. The ResNet50 model used 100,000 training samples from the ImageNet dataset for Figure 4. Test error was computed on the complete ImageNet validation set. A table shows the average and standard deviation of the ReLU threshold percentile rank across neurons, indicating convergence to a precise position in the pre-activation distribution. The ReLU threshold does not seem to cause the binary behavior of neurons. The ReLU threshold position is consistent across neurons, indicating precise convergence in the pre-activation distribution. The binary behavior of neurons is not caused by the ReLU threshold. Neurons receive consistent information on sample activation, allowing them to act as binary classifiers. The neuron-wise histograms in FIG1 show that input samples have negative or positive derivatives, allowing neurons to act as binary classifiers. Layers from the first two rows are trained on MNIST with ReLU and sigmoid activation functions, while the third and fourth rows are trained on CIFAR-10 with ReLU and no activation function. Pre-activation distributions evolve during training, separating into high and low categories based on derivative signs. The final highest pre-activations of the high category are highlighted to show non-simple translation. The final highest pre-activations of the high category are highlighted to show that it is not a simple translation. Neurons act as binary classifiers based on derivative signs, separating into high and low categories during training. Histograms show consistency between sample class and neuron category. The histogram displays the alignment between sample class and neuron category in dense2-relu, with peaks at 0 and 100% indicating high consistency."
}