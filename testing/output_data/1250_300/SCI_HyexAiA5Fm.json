{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models used for high-dimensional continuous measures. This paper introduces a scalable method for unbalanced optimal transport (OT) within the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the transformation of one measure to another. The proposed algorithm utilizes stochastic alternating gradient updates, akin to GANs, and is demonstrated through numerical experiments in population modeling. The focus is on solving the problem of unbalanced optimal transport, which involves finding a cost-optimal way to transform one measure to another by considering mass variation and transport. The cost-optimal transformation of measures involves mass variation and transport, with modern approaches based on the Kantorovich formulation for optimal transport. Regularizing the objective with an entropy term allows for more efficient solutions using the Sinkhorn algorithm. Optimal transport has applications in various fields such as computer graphics and domain adaptation. The Sinkhorn algorithm efficiently solves the dual problem in optimal transport. Stochastic methods based on the dual objective have been proposed for various applications, including computer graphics, domain adaptation, image translation, natural language translation, and biological data integration. Generative models like GANs can be used to learn transport maps in cases where transport costs are not available. The objective is to learn a transport map and scaling factor to push the source to the target, using deterministic or stochastic transport maps. Various methods using GANs have been employed, but they cannot handle mass variation. Formulations for unbalanced optimal transport have been proposed, including scaling algorithms that generalize the Sinkhorn algorithm. These algorithms allow for mass variation in the Kantorovich OT problem. The formulation of unbalanced optimal transport by BID27 relaxes hard marginal constraints using divergences to allow for mass variation. Algorithms have been used in various applications like computer graphics, tumor growth modeling, and computational biology. However, current methods cannot perform unbalanced OT between continuous measures. A novel framework is proposed to directly model mass variation in addition to transport by solving a Monge-like formulation of unbalanced OT. The formulation of unbalanced optimal transport by BID27 relaxes hard marginal constraints using divergences to allow for mass variation. A scalable methodology is developed for solving the relaxed problem, demonstrating practical applications in population modeling using various datasets. Additionally, a new scalable method is proposed in the Appendix for solving the optimal-entropy transport problem in the continuous setting. The algorithm by BID27 extends unbalanced optimal transport to a scalable alternative for large datasets in the continuous setting. Optimal transport addresses cost-optimal transportation between measures, with the Kantorovich OT problem as a convex relaxation. Monge formulated the problem as a search over deterministic transport maps, but it is non-convex and not always feasible. The Monge problem is a non-convex optimization problem for optimal transport maps, while the Kantorovich OT problem is a convex relaxation that uses probabilistic transport plans. Entropic regularization simplifies the dual optimization problem, making it solvable with the Sinkhorn algorithm. Stochastic algorithms have been proposed for computing transport plans for continuous measures. Unbalanced optimal transport formulations have been introduced to handle mass variation. In the context of optimal transport problems, stochastic algorithms have been developed to compute transport plans for continuous measures. Unbalanced optimal transport formulations have been proposed to handle mass variation, with existing numerical methods based on optimal-entropy transport formulations. The state-of-the-art in discrete settings involves iterative scaling algorithms, but practical algorithms for unbalanced optimal transport between continuous measures, especially in high-dimensional spaces, are lacking. This section introduces the first algorithm for unbalanced optimal transport in such scenarios. In this section, a new algorithm is proposed for unbalanced optimal transport between continuous measures in high-dimensional spaces. The algorithm directly models mass variation and aims to learn a stochastic transport map and scaling factor to push a source measure to a target measure in a cost-optimal manner. The formulation involves penalizing the cost of mass transport and variation, with an equality constraint to ensure exact pushing of the source measure to the target measure. The algorithm proposed aims to learn a stochastic transport map and scaling factor to push a source measure to a target measure in a cost-optimal manner. It involves penalizing the cost of mass transport and variation, with an equality constraint to ensure exact pushing of the source measure to the target measure. The model considers stochastic maps for practical problems like cell biology, where one cell can give rise to multiple cells in a target population. The algorithm aims to model the transformation from a source measure to a target measure using a transport map and scaling factor. Different transformation models are optimal based on the costs of mass transport and variation. An unbalanced transport map with a scaling factor can address class imbalances by adjusting sample weights in the source distribution to balance classes with the target distribution. Relaxation techniques are used to handle optimization challenges in satisfying constraints. The algorithm models the transformation from a source measure to a target measure using a transport map and scaling factor. It addresses class imbalances by adjusting sample weights in the source distribution to balance classes with the target distribution. Relaxation techniques are used to handle optimization challenges in satisfying constraints, such as using a divergence penalty in place of an equality constraint. The algorithm uses a transport map and scaling factor to transform a source measure to a target measure, addressing class imbalances by adjusting sample weights. Relaxation techniques, like using a divergence penalty, help handle optimization challenges in satisfying constraints. Theorem 3.4 shows that solutions of the relaxed problem converge to solutions of the original problem under certain conditions. The proof in the Appendix shows that solutions of the relaxed problem converge to solutions of the original problem. The algorithm uses stochastic gradient methods to learn the transport map and scaling factor, optimizing the objective by minimizing a penalty term with an adversary function. The convex conjugate representation allows for optimization using alternating stochastic gradient updates with neural networks. The objective is to minimize the divergence between transported and real samples, similar to GAN training. Cost functions encourage finding a cost-efficient strategy. Practical considerations for implementation and training are discussed in the Appendix. The probabilistic Monge-like formulation involves solving a non-convex optimization problem using neural networks to learn a transport map and scaling factor. The algorithm enables scalable optimization but may not guarantee finding the global optimum due to non-convexity. The algorithm based on a probabilistic Monge-like formulation uses neural networks to learn a transport map and scaling factor, enabling scalable optimization but not guaranteeing the global optimum due to non-convexity. A new stochastic method is proposed in the Appendix to handle transport between continuous measures, overcoming scalability limitations of existing methods. In Section 4, the advantage of directly learning a transport map and scaling factor using Algorithm 1 is demonstrated. The problem of learning a scaling factor for balancing measures in causal inference is discussed, with applications focused on unbalanced optimal transport. Algorithm 1 is used to perform unbalanced optimal transport between two modified MNIST datasets, simulating class imbalance and brightness changes. The scaling factor is evaluated for transporting the source distribution to the target distribution with a high cost of transport. The scaling factor learned by Algorithm 1 reflects class imbalances and can model growth or decline of different classes in a population. Unbalanced optimal transport is applied from the MNIST dataset to the USPS dataset, simulating population evolution. The transport cost is based on Euclidean distance between original and transported images. The reweighting process during unbalanced OT is illustrated in FIG4, while the summary of the transport is visualized in FIG1. The unbalanced optimal transport from MNIST to USPS dataset is visualized in FIG1, showing the scaling factor of the original images. MNIST digits with higher scaling factors were generally brighter and covered more pixels, consistent with USPS digits. The unbalanced optimal transport was applied on the CelebA dataset, modeling the transformation from young to aged faces using a variational autoencoder. The transported faces retained the salient features of the original faces. The unbalanced transport from young to aged faces in the CelebA dataset revealed a gender imbalance, with male faces predicted to become more prominent as the population ages. This phenomenon was confirmed by observing a strong gender disparity between the young and aged populations. The gender imbalance between young and aged populations was confirmed by observing that the young population is predominantly female while the aged population is predominantly male. Using Algorithm 1 on zebrafish embryogenesis data, the scaling factor was assessed by comparing cells from different developmental stages. The scaling factor in zebrafish embryogenesis data was assessed by comparing cells from different developmental stages. Cells with higher scaling factors were found to be enriched for genes associated with mesoderm differentiation and development. This experiment demonstrates the potential for biological discovery through analysis of the scaling factor. Adding a strongly convex regularization term to the primal objective helps make the dual problem unconstrained. This term encourages plans with high entropy, and the relationship between the primal and dual optimizers can be rewritten in terms of expectations. Parameterizing with neural networks and optimizing using stochastic gradient descent is a generalization of classical OT to unbalanced OT. The dual solution learned from Algorithm 2 in unbalanced optimal transport can be used to reconstruct the primal solution, indicating mass transport between points in X and Y. The marginals of the transport map are not necessarily \u00b5 and \u03bd, allowing for implicit mass variation in the problem. Additionally, an \"averaged\" deterministic mapping from X to Y can be learned using the obtained transport map. The marginals of the transport map may not be \u00b5 and \u03bd, allowing for implicit mass variation in the problem. An \"averaged\" deterministic mapping from X to Y can also be learned using the obtained transport map. A stochastic algorithm for learning such a map from the dual solution is proposed in Algorithm 3. The objectives in the formulations are equivalent if one reformulates in terms of \u03b3 instead of (T, \u03be), where \u03b3 is a joint measure. The relation between the formulations is formalized by Lemma 3.3. Lemma 3.3 formalizes the relation between the formulations of the problem. The proof shows the inequality between two functions, and the existence of measurable functions under certain conditions. The proof of Proposition B.1 establishes the equivalence of two functions under certain conditions, leading to the uniqueness of the joint measure specified by any minimizer of L \u03c8 (\u00b5, \u03bd). This result follows from the analysis of optimal entropy-transport by BID27. The uniqueness of the joint measure specified by any minimizer of L \u03c8 (\u00b5, \u03bd) is proven under certain conditions, showing that the product measure generated by the minimizers of L \u03c8 (\u00b5, \u03bd) is unique. This result is derived from the analysis of optimal entropy-transport by BID27. The proof shows that for certain cost functions and divergences, L \u03c8 defines a proper metric between positive measures \u00b5 and \u03bd. By choosing appropriate divergence penalties, solutions of the relaxed problem converge to solutions of the original problem. The uniqueness of the joint measure specified by any minimizer of L \u03c8 (\u00b5, \u03bd) is proven under certain conditions. The text discusses the convergence of minimizers of a cost function under certain conditions. It shows that the sequence of minimizers is bounded and equally tight, leading to weak convergence. The proof establishes the minimizer of the cost function and the product measure induced by minimizers of a related function. In this section, the convex conjugate form of \u03c8-divergence is presented to rewrite the main objective as a min-max problem. Lemma B.2 states that for non-negative finite measures P, Q over T \u2282 R d, a certain equality holds under specific conditions. The proof of this lemma is provided, with a reference to a similar result in the literature used for generative modeling. The proof of Lemma B.2 shows that the optimal cost functions for the problem are well-posed, with specific conditions outlined. It is recommended to choose the cost of transport as a measurement of correspondence between X and Y, while the cost of mass adjustment should be a convex function that prevents extreme values. Various entropy functions can be used for the cost of mass adjustment. The choice of entropy functions for mass adjustment in the problem is crucial. Any \u03c8-divergence can be used to train generative models by matching a generated distribution to a true data distribution. Jensen's inequality ensures that the divergence is minimized when the two distributions are equal, but this may not hold for non-probability measures. The GAN paper illustrates this with an example using the Jensen-Shannon divergence. The \u03c8-divergence corresponds to D \u03c8 (P |Q) with \u03c8(s) = s log s \u2212 (s + 1) log(s + 1) BID33. When P, Q are probability measures, this divergence is equivalent to the Jensen-Shannon divergence and is minimized when P = Q. Additional constraints on \u03c8 are required for divergence minimization to match P to Q when they are not probability measures. The \u03c8-divergence, corresponding to D \u03c8 (P |Q), requires additional constraints for divergence minimization when P, Q are not probability measures. Choice of functions, activation layers, and neural architectures are crucial for implementing \u03c8 in unbalanced OT. The neural network architecture included 3 hidden layers with ReLU activations. The output activation layers used a sigmoid function for T and a softplus function for \u03be to map values to specific ranges."
}