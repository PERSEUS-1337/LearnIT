{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs involves using the Wasserstein-2 metric proximal on the generators. The approach is based on the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experiments show improved speed and stability in training GANs in terms of wall-clock time and FID learning curves. The generator in GANs aims to recreate the density distribution from the real source by minimizing a discrepancy measure. Optimal transport, such as Wasserstein distance, is used to define this measure and has been applied in Wasserstein GANs. The Wasserstein steepest descent flow is derived for deep generative models in GANs using the Wasserstein-2 metric function. The paper introduces the Wasserstein steepest descent flow for deep generative models in GANs, utilizing the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. It proposes using the gradient operator induced by the Wasserstein-2 metric due to challenges with the Fisher-Rao natural gradient in GANs. The proximal operator for generators is computed with a regularization of squared constrained Wasserstein-2 distance, approximated by a neural network. The constrained Wasserstein-2 metric simplifies in implicit generative models, and a relaxed proximal operator for generators is introduced. The paper introduces the Wasserstein steepest descent flow for deep generative models in GANs, utilizing the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. It proposes a relaxed proximal operator for generators, simplifying the computation by only considering the difference of outputs. The method can be easily implemented as a drop-in regularizer for generator updates. The effectiveness of the proposed methods is demonstrated in experiments with various types of GANs. Optimal transportation defines distance functions between probability densities, with Wasserstein-p distance denoted as W p. The Wasserstein-2 distance involves transporting densities with minimal kinetic energy. Extending this theory to parameterized density models, the Wasserstein-2 metric function is constrained to a parameter space. The Wasserstein-2 metric function for parameterized density models is locally injective and constrained to a parameter space. It defines a metric on the parameter space, allowing for steepest descent optimization schemes based on Riemannian structures. The constrained Wasserstein-2 gradient can be obtained from this metric. The constrained Wasserstein-2 metric allows for a Riemannian metric structure, leading to the Wasserstein natural gradient. The gradient descent iteration can be computed using the proximal operator as an alternative to the difficult matrix inversion method. The backward Euler method, also known as the Jordan-Kinderlehrer-Otto (JKO) scheme, provides a numerical scheme for the gradient flow of loss function F : \u0398 \u2192 R. It involves using the proximal operator and a regularization term in the parameter update. The Semi-Backward Euler method is a first-order scheme that is easier to approximate than the forward Euler method, as it does not require computing and inverting G(\u03b8). The Semi-Backward Euler method simplifies parameter updates in implicit generative models by avoiding the need to compute and invert G(\u03b8). It involves a constrained optimization over \u03a6, implemented using a neural network to approximate the variable. This method allows for a simpler formulation of the constrained Wasserstein-2 metric, leading to the definition of the relaxed Wasserstein metric and a straightforward algorithm for the proximal operator on generators. The constrained Wasserstein-2 metric in implicit generative models is simplified by a reformulation, introducing the relaxed Wasserstein metric and a simple algorithm for the proximal operator on generators. The gradient constraint is crucial for 1-dimensional sample spaces but poses computational challenges for higher dimensions. To address this, a relaxed Wasserstein metric on the parameter space is considered, allowing for easier computations of the Wasserstein proximal operator. The gradient constraint in computing the Wasserstein proximal operator is a challenge, especially in high-dimensional sample spaces. To simplify computations, a relaxed Wasserstein metric is used on the parameter space. This approach regularizes the generator by minimizing the squared difference in sample space. An algorithm for the relaxed Wasserstein proximal operator is presented, along with a toy example demonstrating its effectiveness in GANs. The effectiveness of Wasserstein proximal operator in GANs is illustrated using a toy example with a family of distribution. Different statistical distance functions are checked between parameters, with Wasserstein-2 and Euclidean distances being effective. The Euclidean distance is independent of the model structure, while the constrained Wasserstein-2 metric depends on it. The Wasserstein-1 metric is considered as the loss function. The Wasserstein-1 metric is used as the loss function in GAN training. The Relaxed Wasserstein Proximal algorithm improves speed and stability in training GANs by applying regularization on the generator. It outperforms Euclidean proximal in terms of objective function decrease and provides better speed and convergence. The Relaxed Wasserstein Proximal (RWP) algorithm improves GAN training speed and convergence by regularizing the generator. It introduces hyperparameters for the generator update rule and is tested on various GAN types using CIFAR-10 and CelebA datasets. The Relaxed Wasserstein Proximal regularization is tested on different GAN types using CIFAR-10 and CelebA datasets. It improves training speed and convergence by regularizing the generator, with hyperparameters chosen for optimal performance. The results show improved speed and stability of convergence, with comparisons made based on wallclock time. The Relaxed Wasserstein Proximal regularization improves convergence speed and stability in GAN training. Results show lower FID scores and 20% improvement in sample quality for DRAGAN. Similar results are seen in the CelebA dataset. Multiple generator iterations may cause initial learning issues in Standard GANs on CelebA, but can be easily detected and rectified. The effect of multiple generator updates compared to discriminator updates is also examined in RWP. The use of Relaxed Wasserstein Proximal (RWP) regularization improves GAN training speed and stability, leading to lower FID scores and better sample quality. Multiple generator updates in RWP show improved convergence compared to discriminator updates. The regularization also prevents memorization in GAN models. Additionally, RWP enhances speed and achieves lower FID scores, even though multiple generator iterations may initially cause learning issues. The experiment demonstrates the impact of 10 generator iterations per outer-iteration with and without RWP regularization, showing improved convergence and lower FID scores with RWP. The training using Semi-Backward Euler method on CIFAR-10 dataset is comparable to standard WGAN-GP training, with results averaged over 5 runs. The experiment compares the training using Semi-Backward Euler method on CIFAR-10 dataset to standard WGAN-GP training, with results averaged over 5 runs. The Semi-Backward Euler method is shown to be comparable to norm WGAN-GP, leaving room for further investigation in future work. The use of Wasserstein distance as a loss function in optimal transport and GANs is discussed in the literature. The Wasserstein distance is utilized as a loss function in studies for various reasons, including its statistical properties and ability to compare probability distributions. In GANs, the Wasserstein-1 distance function is often chosen as the loss function, requiring the discriminator to satisfy the 1-Lipschitz condition. Additionally, the Wasserstein-2 metric provides a metric tensor structure, forming an infinite dimensional Riemannian manifold known as the density manifold. The Wasserstein-2 metric creates a metric tensor structure in the probability space, forming an infinite dimensional Riemannian manifold called the density manifold. The gradient flow in this manifold is linked to various transport-related partial differential equations, such as the Fokker-Planck equation. Researchers explore leveraging the gradient flow structure in probability space for stochastic gradient descent and study nonparametric models like the Stein gradient descent method. An approximate inference method for computing Wasserstein gradient flow in the full probability set is also considered. The BID24 method is a generalization of Wasserstein gradient flow, with an approximate inference method introduced for computing it in the full probability set. The Wasserstein structure can be constrained on parameter space, with a focus on Gaussian families and elliptical distributions. The approach applies the constrained Wasserstein gradient to implicit generative models, emphasizing regularization of the generator. The method computes the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space for Wasserstein GAN with gradient penalty. The focus shifts from regularizing the discriminator to regularizing the generator in Wasserstein GAN. The proposed method computes the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space, leading to faster convergence speeds and better minimizer in terms of FID. The variational formulation introduces a Riemannian structure in density space, considering smooth and strictly positive probability densities. The text discusses the Wasserstein gradient operator in a Riemannian metric space, focusing on the Wasserstein-2 gradient flow and its application in statistical models defined by parameterization functions. The Wasserstein statistical manifold is introduced, with a Riemannian metric defined on the parameter space. The text introduces the Wasserstein statistical manifold with a Riemannian metric defined on the parameter space, focusing on the constrained Wasserstein gradient operator in parameter space. The proof involves the action function and the gradient operator on a Riemannian manifold. The derivation of the proposed semi-backward method is also presented. The text presents the derivation of the proposed semi-backward method, proving equations and claims related to the gradient operator on a Riemannian manifold. It also introduces an implicit model and discusses the probability density transition equation. The text presents the derivation of the proposed semi-backward method, proving equations and claims related to the gradient operator on a Riemannian manifold. It also introduces an implicit model and discusses the probability density transition equation. Proposition 4 is proven, showing the constrained continuity equation and the push-forward relation. Proposition 5 is also proven, allowing for the explicit computation of the Wasserstein and Euclidean proximal operators. The text presents the derivation of the proposed semi-backward method, proving equations and claims related to the gradient operator on a Riemannian manifold. Proposition 5 allows for the explicit computation of the Wasserstein and Euclidean proximal operators with specific hyperparameter settings for different experiments. The text discusses the use of specific hyperparameters and optimization techniques for training GANs, including the Relaxed Wasserstein Proximal algorithm. It outlines the steps involved in training GANs with this method, emphasizing the differences from standard training approaches. The text discusses the use of specific hyperparameters and optimization techniques for training GANs with the Relaxed Wasserstein Proximal algorithm. It highlights the differences from standard training approaches and presents results from training GANs on different datasets. The FID scores for the generated images are provided, along with observations on latent space walking. The specific hyperparameter settings used for training GANs with RWP regularization on CIFAR-10 include a batch size of 64, DCGAN architecture, Adam optimizer with specific parameters, latent space dimension of 100, and updates to discriminator, generator, and potential in each outer-iteration loop."
}