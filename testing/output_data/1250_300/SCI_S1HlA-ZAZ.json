{
    "title": "S1HlA-ZAZ",
    "content": "We introduce an end-to-end trained memory system inspired by Kanerva's sparse distributed memory. The memory has a robust distributed reading and writing mechanism and is analytically tractable for optimal compression. Formulated as a hierarchical conditional generative model, it combines top-down memory and bottom-up perception to produce observation codes. Empirical results show improved generative models on Omniglot and CIFAR datasets compared to DNC models. Our memory model has greater capacity and is easier to train than DNC variants. Neural networks struggle with efficiently using memory, as seen in models like DNCs and Matching Networks. Different approaches either collapse information into single slots or require memory volume to increase with stored samples. The Neural Statistician offers small summaries but may lose important details. Associative memory architectures provide insights into designing efficient memory structures. The curr_chunk discusses various memory architectures, such as the Hopfield Net, Boltzmann Machine, and Kanerva's sparse distributed memory model. It introduces a conditional generative memory model inspired by Kanerva's model with learnable addresses for efficient memory storage. The paper presents a conditional generative memory model inspired by Kanerva's sparse distributed memory. It introduces learnable addresses and reparametrised latent variables to improve memory writing operations. The model offers a hierarchical generative approach with a memory-dependent prior for quick adaptation to new data. It enriches priors in VAE-like models and provides effective online distributed writing for compression and storage of complex data. The proposal introduces a memory system for online distributed writing, enhancing VAE models with adaptive memory. It utilizes learnable addresses and reparametrised latent variables for efficient compression and storage of complex data. The model optimizes log-likelihood by jointly optimizing generative and inference model parameters. The VAE training objective is to maximize log-likelihood by optimizing \u03b8 and \u03c6 for a variational lower-bound. The model introduces an exchangeable episode concept and factors the joint distribution into marginal distribution and posterior for efficient computation. The joint distribution of the generative model is factorized using the conditional independence of latent variables given the memory M, which is a random matrix with a matrix variate Gaussian distribution. The memory M is a K \u00d7 C random matrix with a matrix variate Gaussian distribution. The distribution is equivalent to the multivariate Gaussian distribution of vectorised M. The addresses A, a K \u00d7 S real-value matrix, are optimised through back-propagation. The addressing variable y t computes weights controlling memory access. The prior p \u03b8 (y t ) is an isotropic Gaussian distribution N (0, 1). The memory matrix M is optimised through back-propagation, with rows normalised to have L2-norms of 1. The addressing variable y t computes weights for memory access. A learned projection transforms y t into a key vector. The weights w t across memory rows are computed using a multi-layer perception (MLP). The code z t generates samples of x t through a conditional distribution, with a memory-dependent prior for z t. The hierarchical model includes a global latent variable M for an episode and local latent variables y t and z t for data x t within the episode. The reading inference model factorizes the posterior distribution using conditional independence, refining the prior distribution with additional evidence from x t. The posterior distribution is parameterized and takes input from x t and the mean of the prior distribution. The parameterized posterior distribution updates memory by balancing old and new information through Bayes' rule. Memory writing is interpreted as inference, computing the posterior distribution of memory. Batch and online inference methods are considered, with the approximated posterior distribution of memory using one sample of y t, x t. The addressing variable and code posterior distributions are parameterized. The posterior distribution of memory is updated by balancing old and new information through Bayes' rule. Memory writing is seen as inference, computing the posterior distribution of memory. The parameters R and U are updated using a linear Gaussian model, with prior parameters trained through back-propagation. The update rule for the posterior distribution of memory in a linear Gaussian model involves balancing old and new information through Bayes' rule. The prior parameters of the model are trained through back-propagation, allowing the prior of memory to capture the general dataset structure while the posterior adapts to specific features in each episode. The main computational cost of the update rule comes from inverting the covariance matrix, with options to reduce costs through online updating or using mini-batches. Storage and multiplication of the memory's row-covariance matrix also contribute to the computational complexity. The update rule for the memory in a linear Gaussian model involves balancing old and new information through Bayes' rule. The computational cost includes inverting the covariance matrix and storing the row-covariance matrix U. Training the model involves optimizing a variational lower-bound of the conditional likelihood. To maximize this lower bound, sampling from the posterior distribution is used for computational efficiency. The mean-field approximation is used for memory, replacing memory samples with the mean R to avoid expensive Cholesky decomposition. The model utilizes distribution-based reading and writing operations. An iterative sampling mechanism is implemented, similar to Kanerva's sparse distributed memory, to decrease errors through iterative reading of the reconstruction. The dynamics of iterative reading improve denoising and sampling by utilizing knowledge about memory and local coupling between variables. The model implements an iterative sampling mechanism that converges to the true posterior distribution efficiently. The model implementation details are described in Appendix C, focusing on evaluating improvements provided by an adaptive memory. The experiments use the Omniglot and CIFAR datasets with variations in convolutional layers, memory size, and code size. The Adam optimizer was used with minimal tuning, and the variational lower bound value is reported for comparison with existing models. Initial testing was done with the Omniglot dataset containing hand-written characters. The study tested the model using the Omniglot dataset with 1623 classes and 20 examples each. A memory size of 64 \u00d7 100 and an address matrix size of 64 \u00d7 50 were used. Images were randomly sampled to form episodes, ignoring class labels. The model was also tested with the CIFAR dataset, using convolutional coders for increased complexity. The study tested the model using the Omniglot dataset with 1623 classes and 20 examples each, and then moved on to test it with the CIFAR dataset using convolutional coders for increased complexity. The CIFAR dataset required a code size of 200, a 128 \u00d7 200 memory, and a 128 \u00d7 50 address matrix. The model's training process was compared with a baseline VAE model, showing a modest increase in parameters for the Kanerva Machine. Learning curves indicated that the model learned to use the memory effectively. The study compared the Kanerva Machine with a VAE model using the Omniglot dataset and CIFAR dataset. The Kanerva Machine showed better performance in terms of reconstruction and KL-divergence, with a sharp dip in KL-divergence indicating effective memory usage. The rich prior from memory led to informative coding at the cost of additional KL-divergence for y t. Similar training curves were observed for CIFAR training. The VAE model reached a negative log-likelihood of \u2264 112.7 at the end of training, worse than state-of-the-art unconditioned generation but comparable to IWAE training results. The Kanerva Machine achieved a conditional NLL of 68.3, showing the power of incorporating adaptive memory into generative models. The VAE model achieved a negative log-likelihood of \u2264 112.7 at the end of training, while the Kanerva Machine demonstrated a conditional NLL of 68.3, highlighting the benefit of incorporating adaptive memory into generative models. The model's weights were well distributed over the memory, showcasing the superimposition of patterns written into the memory. The reconstruction examples at the end of training illustrate the model's ability to denoise through iterative reading, showcasing its capability to generate batches of images across various classes and samples. The VAE and Kanerva Machine models were compared using samples from 2, 4, or 12 classes. The sample quality improved in consecutive iterations, reflecting the conditioning patterns. Conditional samples from CIFAR were also shown. VAEs did not show improvement in sample quality after iterations, unlike the Kanerva Machine. The study compared VAE and Kanerva Machine models using samples from CIFAR dataset. While VAE samples were blurred and lacked structure, Kanerva Machine samples showed clear local structures. The model was tested on recovering corrupted images, showing the ability to recover original images over iterations. Some cases produced incorrect but reasonable patterns due to high ambiguity. The model's structure allows interpretability of internal representations in memory. The model's structure allows interpretability of internal representations in memory. Interpolating between access weights produces meaningful images. DNCs were more sensitive to random initialization and slower compared to Kanerva Machine. Test results show the performance of DNC and Kanerva Machine on different episode sizes and sample classes. The Kanerva Machine outperformed the DNC in terms of robustness to hyper-parameters and achieved lower test loss with varying batch sizes and learning rates. The DNC was sensitive to random initialization and struggled to reach optimal performance levels. The Kanerva Machine showed robustness to hyper-parameters, performing well with batch sizes between 8 and 64 and learning rates between 3 \u00d7 10 \u22125 and 3 \u00d7 10 \u22124. It trained fastest with batch size 16 and learning rate 1 \u00d7 10 \u22124, converging below 70 test loss with all configurations. The model's principled reading and writing operations make it easier to train compared to the DNC. The Kanerva Machine also demonstrated good generalization to larger episodes with varying numbers of classes. The Kanerva Machine, a novel memory model, combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory. It generalizes well to larger episodes with varying numbers of classes and outperforms the DNC in terms of variational lower-bound. By training a generative model that learns the observed data distribution, the Kanerva Machine can retrieve unseen patterns through sampling, departing from the assumption of a uniform data distribution. Previous works have explored probabilistic interpretations of Kanerva's model, extending it to handle discrete data with more than two values. Our model generalizes Kanerva's memory model to continuous, non-uniform data while integrating with deep neural networks. Unlike other models, our memory quickly adapts to new data for episode-based learning. Our model efficiently updates memory by compressing information through statistical regularity in images, learned addresses, and Bayes' rule. Unlike other models, we use an exact Bayes' update-rule without compromising neural network flexibility. This approach combines classical statistical models with neural networks for promising performance and scalability. The model combines classical statistical models with neural networks for efficient memory updates using Bayes' rule. Kanerva's memory model features distributed reading and writing operations with fixed addresses and modifiable memory components. Inputs are compared with addresses through Hamming distance calculation. The fixed addresses A i are randomly sampled from {\u22121, 1} D. Input y is compared with addresses A k through Hamming distance. An address k is selected if distance is smaller than threshold \u03c4. For writing, a pattern x is stored into memory M. For reading, contents of selected addresses are summed. Kanerva showed that with large K and D, sparse and distributed operations are possible. Corrupted queries can still be discovered from memory. Kanerva demonstrated that sparse and distributed operations can be achieved with large K and D, allowing for correct retrieval of stored vectors even with multiple overwrites. However, the application of Kanerva's model is limited by the assumption of uniform and binary data distribution, which is often not true in real-world scenarios. Our model architecture, as shown in FIG4, includes a convolutional encoder for converting input images into 2C embedding vectors. This approach differs from a standard VAE and aims to address the inefficiencies of binary data representation in high-level neural network implementations optimized for floating-point numbers. The model architecture includes a convolutional encoder for converting input images into 2C embedding vectors using 3 consecutive blocks with convolutional layers. Adding noise to the input helps stabilize training, and different likelihood functions are used for different datasets. The model architecture includes a convolutional encoder for converting input images into 2C embedding vectors using 3 consecutive blocks with convolutional layers. To avoid Gaussian likelihood collapsing, uniform noise is added to CIFAR images during training. The differentiable neural computer (DNC) is wrapped with the same interface as the Kanerva memory for fair comparison. DNC receives addressing variable y t and z t during reading and writing stages, using a 2-layer MLP with ReLU nonlinearity as the controller instead of LSTM to avoid interference with DNC's operations. The model architecture includes a convolutional encoder for converting input images into 2C embedding vectors using 3 consecutive blocks with convolutional layers. To avoid interference with DNC's operations, a 2-layer MLP with ReLU nonlinearity is used as the controller instead of LSTM. The DNC is modified to prevent the controller from bypassing memory and ensure it only reads from memory. Additionally, the focus is on memory performance by comparing models using full covariance matrices versus diagonal covariance matrices. The model architecture includes a convolutional encoder for converting input images into 2C embedding vectors using 3 consecutive blocks with convolutional layers. A 2-layer MLP with ReLU nonlinearity is used as the controller to avoid interference with DNC's operations. The focus is on memory performance by comparing models using full covariance matrices versus diagonal covariance matrices. The models using full covariance matrices were slightly slower per-iteration, but the test loss decreased far more quickly. The bottom-up stream in the model compensates for memory by directly sampling z t from p \u03b8 (z t |y t , M ) for the decoder p \u03b8 (x t |z t ). The negative variational lower bound, reconstruction loss, and total KL-divergence during CIFAR training are discussed. The training is ongoing, and the advantage of the Kanerva Machine over the VAE is increasing. Eq. 6 defines a linear Gaussian model for the joint distribution p(vec (Z) , vec(M )). The model architecture includes a convolutional encoder for converting input images into 2C embedding vectors using 3 consecutive blocks with convolutional layers. A 2-layer MLP with ReLU nonlinearity is used as the controller to avoid interference with DNC's operations. The focus is on memory performance by comparing models using full covariance matrices versus diagonal covariance matrices. The models using full covariance matrices were slightly slower per-iteration, but the test loss decreased far more quickly. The bottom-up stream in the model compensates for memory by directly sampling z t from p \u03b8 (z t |y t , M ) for the decoder p \u03b8 (x t |z t ). The negative variational lower bound, reconstruction loss, and total KL-divergence during CIFAR training are discussed. The training is ongoing, and the advantage of the Kanerva Machine over the VAE is increasing. Eq. 6 defines a linear Gaussian model for the joint distribution p(vec (Z) , vec(M )). The model described in this paper utilizes samples from q \u03c6 (z t |x t ) for writing to the memory and mean-field approximation during reading. An alternative approach is presented that fully exploits the analytic tractability of the Gaussian distribution, using parameters \u03c8 = {R, U, V } for the memory."
}