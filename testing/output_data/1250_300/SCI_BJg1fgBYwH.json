{
    "title": "BJg1fgBYwH",
    "content": "The proposed SAFE-DNN enhances classification robustness by incorporating unsupervised learning of low-level features using SNN with STDP. Experimental results show improved noise robustness for various DNN architectures without compromising accuracy on clean images. This is crucial for deploying DNNs in autonomous systems like autonomous vehicles and robotics, where reliable classifications are needed even with noisy data. Pixel level perturbation can lead to incorrect feature maps in kernels of a DNN, affecting classification accuracy. Research has focused on two main approaches to improve robustness: image de-noising networks for pre-processing images before classification, and advanced de-noising networks capable of generalizing to different noise types. However, these networks can add latency to the processing pipeline and may degrade accuracy for clean images. Inference suggests that advanced de-noising networks are effective for different noise types but are not suitable for real-time applications. Developing a classification network inherently robust to input perturbations is an alternative approach, which includes training with noisy data and using pixel level regularization. However, these approaches may degrade classification accuracy for clean images and require noise structure to closely match during training and inference. A new class of DNN architecture is needed for autonomous systems that are inherently resilient to input. The paper proposes a new DNN architecture that integrates features from unsupervised neuro-inspired learning and supervised training to increase robustness to input perturbations. The use of spiking neural networks with spike-timing-dependent plasticity for feature extraction is suggested, as it enhances the DNN's resilience without requiring noisy data for training. The paper introduces a hybrid network architecture called Spike Assisted Feature Extraction based Deep Neural Network (SAFE-DNN) that combines features from supervised training and unsupervised neuro-inspired learning using spiking neural networks with spike-timing-dependent plasticity. This integration enhances the DNN's robustness to input perturbations, allowing for accurate image classification under noisy conditions while maintaining baseline accuracy for clean images. The SAFE-DNN architecture integrates neuro-inspired learning with supervised training, using a spiking convolutional module to extract features robustly. A novel frequency-dependent STDP learning rule enhances local competitive learning of low-level features. The STDP-based spiking convolution is transformed into an equivalent CNN using a special neuron activation unit, creating a single fully-trainable deep network. The SAFE-DNN architecture combines neuro-inspired learning with supervised training, utilizing a spiking convolutional module to extract robust features. A special neuron activation unit enables integration of SNN features within a DNN, creating a single fully-trainable deep network. Implementation on various deep networks demonstrates versatility, with robust classification under different types of input noise. Unlike training-based approaches, SAFE-DNN shows improved accuracy across a wide range of noise structures without prior knowledge of perturbations, maintaining accuracy for clean images. The SAFE-DNN architecture combines neuro-inspired learning with supervised training, utilizing a spiking convolutional module to extract robust features. It does not degrade accuracy for clean images and has negligible computation and memory overhead. Unlike deep SNNs, SAFE-DNN hybridizes STDP and SGD during learning but operates as a DNN during inference. The spiking neural network (SNN) uses biologically plausible neuron and synapse models to exploit temporal relationships between spiking events. The Leaky Integrate Fire (LIF) model is used in this work to capture the firing pattern of real biological neurons. Learning in SNN is achieved through spike-timing-dependent-plasticity (STDP), which includes long-term potentiation (LTP) and long-term depression (LTD) operations. The spiking neural network (SNN) utilizes biologically plausible neuron and synapse models to capture temporal relationships between spiking events. STDP involves long-term potentiation (LTP) triggered by post-synaptic neuron spikes closely after pre-synaptic neuron spikes, and long-term depression (LTD) when the post-synaptic neuron spikes before or without receiving a pre-synaptic spike. The weight update process in a deep neural network (DNN) involves gradient descent to optimize weights based on specific network configurations. The weight optimization in deep neural networks involves updating weights using gradient descent. The gradient of the loss function is calculated with respect to the weight, and back-propagation is used to update weights based on prediction probabilities and ground truth. This process allows for global learning and improves the accuracy of higher-level feature detection. The back-propagation in deep neural networks allows for global learning by considering the impact of all pixels in the input image on weight updates. However, this global learning makes it challenging to enforce local constraints during training, leading to the network being sensitive to noise at the pixel level. To improve robustness to input perturbations, low-level feature extractors should learn to consider local spatial correlation to effectively filter out noisy pixels. The SAFE-DNN approach suggests that low-level feature extractors should learn local spatial correlation to improve robustness to input noise. This local learning helps the network ignore noisy pixels and prevent noise propagation in the DNN pipeline. In SNN, the conductance modulation depends only on input signals within the same receptive field, enabling local learning of features. The value of t spike in a SNN is influenced by the collective sum of input spike trains in one kernel. The modulation of synapse weight depends on input signals within the same receptive field, allowing the network to learn spatial correlation between pixels. SNNs learn to ignore local perturbations, resulting in robust feature extraction. The network architecture of SAFE-DNN includes spiking layers for robust feature extraction. The network architecture of SAFE-DNN includes spiking layers for robust feature extraction. It consists of a spiking convolution module for local and low-level feature extraction, along with auxiliary CNN layers for global learning. The output feature maps from these modules are concatenated and used as input for the main CNN module, responsible for higher-level feature detection and classification. The main CNN module in SAFE-DNN is designed based on existing deep learning models, with features from auxiliary CNN and spiking convolutional modules integrated for global and local learning. The network architecture includes spiking layers for robust feature extraction, with the main CNN module based on MobileNetV2, ResNet101, or DenseNet121. SAFE-DNN implementations do not significantly increase storage or computational complexity compared to baseline networks. Neurons in the SNN system transmit information through spikes, requiring input signal intensity to be converted to spike trains and multiple time steps for neurons to respond, unlike conventional DNNs. The SNN model in SAFE-DNN requires input signal intensity to be converted to spike trains and multiple time steps for neurons to respond, unlike conventional DNNs. To address this, the spiking convolution module is adapted to a single-time-step response system to avoid slowing down training and inference. The training process involves separating STDP-based learning and DNN training into two stages, with the spiking convolution module learning images without supervision in the first stage and then migrating network parameters to the SAFE-DNN module in the second stage. In the second stage, network parameters are migrated to the spiking convolution module of SAFE-DNN. The input signal to spike train conversion process is dropped, and conductance matrix is re-scaled. Batch normalization is added after the convolution layer. A special activation unit (SAU) replaces the basic spiking neuron model to preserve non-linear properties. The entire SAFE-DNN is then fully trained using statistical method, while weights in the spiking convolution module remain fixed. Network inference uses the SAU for modeling neurons instead of the baseline LIF. In the second stage of training, the SAU replaces the baseline LIF for modeling neurons. A frequency-dependent stochastic STDP algorithm is proposed to address associative potentiation issues in STDP. The algorithm dynamically adjusts the probability of LTP/LTD based on input signal frequency, with parameters \u03c4d and \u03c4p determining the time constants. The probability of LTP is higher with smaller \u2206t, indicating a closer timing between pre-synaptic and post-synaptic spikes. The algorithm described adjusts the probability of LTP/LTD based on input signal frequency, with parameters \u03c4d and \u03c4p determining time constants. Smaller \u2206t leads to higher LTP probability, while larger \u2206t results in higher LTD probability. The architecture of the spiking convolutional module is similar to conventional DNN but with differences. The window for inducing LTP is narrower for weak inputs compared to strong inputs. FD stochastic STDP shows better learning capability than conventional STDP. The spiking convolutional module in FD stochastic STDP has a unique architecture that converts input images to spike trains. Neurons in the convolution layer are connected with plastic synapses following STDP rules. Cross-depth inhibition prevents neurons from learning the same feature, leading to robust low-level feature learning. Spiking neurons require multiple spikes to reach a spiking state. The spiking neural network (SNN) uses a layer-by-layer learning procedure to overcome the issue of diminishing spiking frequency in multiple-layer networks. Neurons in each layer are adjusted to increase spiking frequency by lowering the spiking threshold, allowing for the facilitation of learning in subsequent layers. This process is repeated until all layers complete learning. The spike conversion process in SNN involves converting input values to spike frequencies within a specified range. The spike conversion process in SNN involves converting input values to spike frequencies within a specified range. Spike frequency is calculated as F = Clip {(X + \u03be)(f max \u2212 f min )} for the duration of input signal T input. Perturbations within a certain range do not cause extra spikes in receiving neurons, providing robustness to small input changes. Special Activation Unit (SAU) is designed as a step function f(x) = to enhance baseline networks like MobileNetV2, ResNet101, and DenseNet121. The Special Activation Unit (SAU) is designed as a step function to enhance baseline networks like MobileNetV2, ResNet101, and DenseNet121. Two SAFEMobileNetV2 models are trained with FD stochastic STDP and deterministic STDP, tested on noisy input with AWGN noise. The embedding space between the fully connected layers shows improved local feature extraction with FD stochastic STDP, providing better clustering of different classes and higher accuracy. The Special Activation Unit (SAU) enhances baseline networks like MobileNetV2. SAFE-MobileNetV2 with FD stochastic STDP achieves better clustering and accuracy. Comparisons are made with standard MobileNetV2, MobileNetV2-\u00b5, and MobileNetV2-\u03bb. All networks are trained with CIFAR10 dataset. Visualizations in Fig. 7 show embedding space of networks with clean and noisy images. Baseline MobileNetV2 vectors in embedding space are observed with clean input images. Table 2 displays the accuracy of different network variants on the CIFAR-10 dataset. Baseline DNNs experience a decrease in classification accuracy when noise is added to images. Networks trained with noise during training show improved robustness to noise, especially when the inference noise level matches the training noise level. Clean images result in degraded accuracy across all networks. The networks trained with noise (30dB) show higher robustness to noise, especially when inference noise matches training noise. Average filtering improves accuracy in highly noisy conditions but results in performance drop under mild to no noise. SAFE-DNN outperforms original networks in noisy conditions, with significant gains at 20 dB SNR. Clean image accuracy is comparable to baseline networks. In a test scenario with noise levels, SAFE-DNN demonstrates similar performance to networks trained with noise at 30 dB SNR, with increasing advantage at higher noise levels. Clean image accuracy of SAFE-DNN matches baseline networks. Testing on a subset of ImageNet related to traffic, all networks achieve around 70% accuracy on clean images. Noise training improves robustness but affects clean image accuracy. DensNet121 shows more noise robustness than MobileNetV2 and ResNet101, with ResNet101 benefiting the most from average filtering. DensNet121 exhibits more noise robustness than MobileNetV2 and ResNet101 in noise training scenarios. SAFE-DNN implementations of all three networks show improved robustness across all noise levels without affecting clean image accuracy. SAFE-MobileNetV2 outperforms baselines with over 80% accuracy even at 5 dB SNR. Testing on additional noise structures like Wald, Poisson, and salt-and-pepper (SP) further confirms the noise robustness of SAFE-DNN implementations. The SAFE-DNN implementation with Poisson, SP, and Wald distributions shows improved noise robustness compared to baseline and average filtering. Performance drops when noise levels are not aligned with training noise, and mis-aligned noise types result in poor performance. MobileNetV2 networks are tested on ImageNet subset, showing robustness to different noise structures without specific training. Adversarial perturbation testing is also conducted on DNNs trained with conventional methods. SAFE-MobileNetV2 demonstrates robustness to various noise structures without specific training. Adversarial perturbation testing using the fast gradient sign method shows improved robustness for SAFE-DNN. Integration with adversarial training methods for white-box attacks is a potential future direction. The SAFE-DNN architecture combines spiking convolutional networks with STDP based learning for enhanced low-level feature extraction. In this paper, SAFE-DNN is introduced as a deep learning architecture that integrates spiking convolutional network with STDP based learning for robust low level feature extraction. The experimental results demonstrate improved robustness to different input perturbations without prior knowledge of noise during training/inference. SAFE-DNN is compatible with various DNN designs and suitable for real-time autonomous systems in noisy environments."
}