{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. A new framework called EdgeGANRob has been proposed to improve CNN robustness by focusing on shape/structure features and using a generative adversarial network (GAN) to reconstruct images. Additionally, a robust edge detection approach called Robust Canny has been introduced to reduce sensitivity to adversarial perturbations. Comparisons with a simplified procedure called EdgeNetRob show that EdgeGANRob improves clean model accuracy without sacrificing robustness. EdgeNetRob boosts model robustness but reduces clean model accuracy. EdgeGANRob improves clean model accuracy without compromising robustness. Extensive experiments demonstrate EdgeGANRob's resilience in various learning tasks. Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to adversarial examples and data poisoning attacks, leading to reduced generalization accuracy. CNNs tend to learn surface statistical regularities instead of high-level abstractions, impacting their ability to generalize under distribution shifting. Improving the general robustness of Deep Neural Networks (DNNs) remains a challenge, with research focusing on understanding the underlying causes of vulnerability. Recent studies explore the vulnerability of CNNs to adversarial examples and data poisoning attacks, attributing it to their bias towards local features over global object shapes. This bias contributes to their susceptibility to distribution shifting and backdoor attacks. Wang et al., 2019a; Ilyas et al. (2019), Baker et al., 2018; and Geirhos et al., 2019 have all contributed to understanding and addressing this issue. The bias towards local features in CNNs makes them vulnerable to adversarial examples and backdoor attacks. Researchers propose using global shape structures like edges to improve CNN robustness. This paper introduces EdgeGANRob, a new approach to enhance CNNs' robustness by leveraging structural information in images, specifically edges. The EdgeNetRob framework detects edges and trains the classifier based on shape information, reducing texture bias and improving CNNs' robustness. However, challenges remain with vulnerable edge detection algorithms. The EdgeNetRob framework improves CNNs' robustness by detecting edges and training the classifier based on shape information. A new robust edge detection algorithm, Robust Canny, enhances the robustness of EdgeGANRob. EdgeGANRob fills in missing texture/color information before feeding images into the classifier, outperforming adversarial retraining methods. This unified framework improves CNNs' robustness against multiple tasks simultaneously. The EdgeGANRob framework proposes a unified approach to enhance CNNs' robustness by extracting edge/structure information from input images and using GAN to reconstruct the original images. A robust edge detection algorithm, Robust Canny, is introduced to reduce sensitivity to adversarial perturbations. The effectiveness of the framework is demonstrated through evaluations on adversarial attacks, distribution shifting, and backdoor attacks, showing significant improvements in robustness. The EdgeNetRob and EdgeGANRob frameworks show significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Defense methods against adversarial examples are not always robust against adaptive attacks. Gradient obfuscation is a common pitfall in defense methods. CNNs tend to learn superficial statistical cues, but methods like penalizing predictive power can help mitigate this tendency. Recent research has focused on robustifying CNNs against superficial statistical cues and backdoor attacks. Methods include penalizing predictive power of local representations, benchmark datasets for evaluating model robustness, and detecting poisoned training data. Additionally, there is a connection between recognition robustness and robust visual features in image recognition. Recent research has highlighted the connection between recognition robustness and robust visual features in image recognition, with a focus on using neuron pruning to achieve this. A new classification pipeline, EdgeGANRob, is proposed to extract edge/structure features from images and reconstruct them using a generative adversarial network (GAN) to enhance robustness. EdgeGANRob, a method for image reconstruction, extracts edge features and refills texture using a GAN. The process involves EdgeNetRob, Robust Canny, and inpainting GAN. EdgeNetRob focuses on edge maps for classification, reducing sensitivity to textures. EdgeNetRob forces CNN decisions based on edges, reducing sensitivity to textures. Despite simplicity, it degrades CNN performance on clean data. EdgeGANRob fills edge maps with texture/colors for higher accuracy. The robustness of this system depends on the edge detector used. A new robust edge detection algorithm, Robust Canny, is proposed to address vulnerabilities in existing detectors. The text describes the proposal of a new robust edge detection algorithm called Robust Canny. It addresses the vulnerability of existing detectors to adversarial perturbations by improving the robustness of the traditional Canny edge detector. The algorithm includes six stages: noise reduction, gradient computation using the Sobel operator, and noise masking to enhance robustness against perturbations. The Robust Canny algorithm includes stages such as gradient computation using the Sobel operator, noise masking for reducing noise in the presence of perturbations, non-maximum suppression for edge thinning, double thresholding for mapping pixels to different levels, and edge tracking by hysteresis. This algorithm aims to improve the traditional Canny edge detector's robustness against adversarial perturbations. The Robust Canny algorithm enhances the traditional edge detector's robustness by masking perturbations in the gradient computation stage. Parameters like sigma and thresholds impact the detector's robustness and accuracy. Careful parameter selection is crucial for a robust edge detector. Further details on training a Generative Adversarial Network in EdgeGANRob are provided in the experiment section. To achieve a robust edge detector, careful parameter selection is essential. The training process of a Generative Adversarial Network (GAN) in EdgeGANRob involves two stages: first, training a conditional GAN with adversarial and feature matching losses, and second, fine-tuning the GAN along with a classifier to improve accuracy in generating RGB images. The trained GAN in EdgeGANRob, along with the classifier, aims to minimize the classification loss of generated images. This method enhances robustness against adversarial, distribution shifting, and backdoor attacks by focusing on edge features for improved generalization ability. EdgeGANRob focuses on shape structure to improve model generalization and combat distribution shifts during testing. Extracting edges acts as a data sanitization step to thwart backdoor attacks. The method's robustness is evaluated against adversarial attacks, distribution shifts, and backdoor attacks. EdgeNetRob, a variant without inpainting GAN, shows unique advantages in certain scenarios and is considered independently valuable for robust recognition. The study evaluates the robustness of their methods against adversarial attacks, distribution shifts, and backdoor attacks on Fashion MNIST and CelebA datasets. MNIST and CIFAR-10 datasets were not chosen due to their limitations. The evaluation is done using the \u221e adversarial perturbation constraints with a standard perturbation budget. The study evaluates the robustness of their methods against adversarial attacks on Fashion MNIST and CelebA datasets using standard perturbation budgets. They measure the robustness to white-box attacks using the BPDA attack and compare the performance of different edge detection methods. The study compares different edge detection methods for robustness against adversarial attacks on Fashion MNIST and CelebA datasets. Results show that using edges generated by RCF is not robust, while EdgeNetRob and EdgeGANRob achieve higher clean accuracy compared to the baseline model. EdgeGANRob outperforms EdgeNetRob on the CelebA dataset, highlighting the importance of adding GANs for improved accuracy. EdgeNetRob and EdgeGANRob achieve higher clean accuracy compared to the vanilla baseline model. EdgeGANRob outperforms EdgeNetRob on the CelebA dataset, validating the necessity of adding GANs for improved accuracy. Both models show robustness against strong adaptive attacks, with EdgeNetRob having the advantage of time efficiency. Generalization ability is tested under distribution shifting with perturbed Fashion MNIST and CelebA datasets. Our models, EdgeNetRob and EdgeGANRob, are tested on perturbed Fashion MNIST and CelebA datasets with various patterns. Results show significant accuracy improvement on negative color, radial kernel, and random kernel patterns compared to state-of-the-art method PAR. Edge features enhance CNN generalization to shifted data distributions and can serve as a defense against backdoor attacks. Our method embeds invisible watermark patterns into images for Fashion MNIST and CelebA datasets to defend against backdoor attacks. Results show successful poisoning accuracy on both datasets compared to the baseline method. Our method successfully attacks the vanilla Net with high poisoning accuracy on CelebA and Fashion MNIST datasets. Spectral Signature struggles with invisible watermark patterns, while EdgeNetRob and EdgeGANRob consistently have low poisoning accuracy. EdgeGANRob outperforms EdgeNetRob in clean accuracy, showcasing the benefits of using an inpainting GAN. The method combines robust edge features with a generative adversarial network to improve model robustness against backdoor attacks and distribution shifting. Our method combines robust edge features with a generative adversarial network to improve model robustness against backdoor attacks and distribution shifting. Data pre-processing involves resizing images in CelebA to 128 \u00d7 128 using bicubic interpolation and normalizing data. Different attack methods such as Projected Gradient Descent and Carlini & Wagner are used for evaluation. For PGD attacks, evaluations are done with 10 and 40 steps (PGD-10 and PGD-40). Step sizes are set based on distance parameters. CW attacks are evaluated on 1,000 images due to computational complexity. Robust Canny parameters are chosen for Fashion MNIST and CelebA datasets. The last steps in Robust Canny involve non-differentiable transformations, but in white-box attacks, gradients are backpropagated through the edge detection algorithm. The BPDA technique can be used to replace non-differentiable transformations in constructing adversarial samples. The attacker can use the Backward Pass Differentiable Approximation (BPDA) technique to replace non-differentiable transformations in constructing adversarial examples. By finding a differentiable approximation of the Robust Canny algorithm, the attacker can strengthen their attack. This involves breaking the transformation into two stages, with the second stage being a non-differentiable operation. To obtain a differentiable approximation of R-Canny for BPDA, the mask is assumed to be constant. The text discusses using Backward Pass Differentiable Approximation (BPDA) to create adversarial examples by approximating the Robust Canny algorithm. The mask is assumed to be constant for differentiability. Results of test accuracy under radial and random mask transformations are shown, along with visualization results for CelebA and backdoor attacks on Fashion MNIST using EdgeGANRob and EdgeNetRob."
}