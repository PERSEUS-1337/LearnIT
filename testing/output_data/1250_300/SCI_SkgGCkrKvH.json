{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression like Choco-SGD enables data privacy, on-device learning, and efficient scaling to large compute clusters. This approach achieves linear speedup in the number of workers for high compression ratios on non-convex functions and non-IID training data. The practical performance of the algorithm is demonstrated in training deep learning models over decentralized user devices and in datacenters. Distributed machine learning has been successful in research and industry by leveraging computational scalability and data-locality. Key success factors for decentralized training of deep neural networks include computational scalability by leveraging multiple devices and data-locality. Recent theoretical results show decentralized schemes can be as efficient as centralized approaches. Gradient compression techniques have been proposed to reduce data sent over communication links. CHOCO-SGD is a recent algorithm for decentralized training with communication compression, enabling data privacy and efficient scaling. CHOCO-SGD is a new algorithm for decentralized training with communication compression, allowing for high compression ratios and improved generalization performance on machine learning benchmarks. It shows speed-ups over decentralized baselines in both peer-to-peer and datacenter settings, with less communication overhead. CHOCO-SGD demonstrates speed-ups over decentralized baselines in both peer-to-peer and datacenter settings, with reduced communication overhead. It improves time-to-accuracy on large tasks like ImageNet training but faces challenges when scaling to a larger number of nodes. The algorithm converges at a rate of O(1/\u221anT + n/(\u03c1^4\u03b4^2T)) on non-convex smooth functions, highlighting the need for further research on decentralized training schemes that can scale effectively. Further research is needed on decentralized training schemes that scale to a large number of peers. CHOCO-SGD converges at a rate of O(1/\u221anT + n/(\u03c1^4\u03b4^2T)) on non-convex smooth functions, showing a linear speedup in the number of workers. A version with momentum is analyzed for practical performance in on-device training and datacenter settings. Challenges are identified when scaling to a larger number of nodes in decentralized learning approaches. In decentralized learning, various methods like gradient compression and gossip averaging are used to improve resource efficiency and time-to-accuracy. Challenges arise when scaling decentralized schemes to a larger number of nodes. In decentralized learning, methods like gradient compression and gossip averaging are used to improve efficiency. Gossip averaging convergence rate depends on the spectral gap of the mixing matrix. Combining SGD with gossip averaging shows convergence at a certain rate. Communication compression with quantization has been popularized in deep learning. Theoretical guarantees have been established for schemes with unbiased compression. Quantization in decentralized optimization has been studied with various compression schemes. Error correction schemes show practical and theoretical advantages. Gossip averaging can diverge with quantization noise, but adaptive schemes have been proposed for convergence at higher communication costs. In decentralized optimization, various compression schemes have been studied. Adaptive schemes have been proposed for convergence at higher communication costs. CHOCO-SGD algorithm can handle high compression and has been analyzed for non-convex functions. DeepSqueeze is an alternative method that also converges with arbitrary compression ratio. In experiments, CHOCO-SGD achieves higher test accuracy compared to DeepSqueeze. CHOCO-SGD achieves higher test accuracy in decentralized optimization by using a gossip-based stochastic optimization algorithm. The algorithm involves a distributed setup with communication limited to local neighbors defined by a weighted graph. The weights are set based on local node degrees to facilitate message exchange for model updates. CHOCO-SGD is a decentralized optimization algorithm that utilizes a gossip-based stochastic optimization approach. It involves a distributed setup with communication limited to local neighbors on a weighted graph. The weights are determined by local node degrees to facilitate message exchange for model updates. The algorithm aims to transmit compressed messages and is summarized in Algorithm 1, where each worker updates its private variable using a stochastic gradient step. CHOCO-SGD is a decentralized optimization algorithm that involves communication limited to local neighbors on a weighted graph. Each worker updates its private variable using a stochastic gradient step and a modified gossip averaging step to preserve averages of iterates. The nodes communicate with neighbors and update variables using compressed updates, with publicly available copies of private variables. Communication and gradient computation can be executed in parallel, with each node needing to store at most 3 vectors. The communication part and gradient computation in CHOCO-SGD can be executed in parallel as they are independent. Each node only needs to store 3 vectors at most, regardless of the number of neighbors. A momentum-version of CHOCO-SGD is proposed in Algorithm 2 for non-convex problems, with technical assumptions on bounded variance of stochastic gradients. The convergence rate of CHOCO-SGD is shown to be linear compared to SGD on a single node, with compression and graph topology affecting higher order terms. The newly developed momentum version of CHOCO-SGD, Algorithm 2, introduces weight decay and momentum factors for improved convergence in non-convex problems. Experimental comparisons with baselines using various compression operators are conducted, leveraging momentum in all algorithms. The setting for the experiments involves a ring topology with 8 nodes training the ResNet20 architecture on the Cifar10 dataset. For the experiments, a ring topology with 8 nodes is used to train ResNet20 on the Cifar10 dataset. Different compression schemes are applied to each layer separately, and the top-1 test accuracy is evaluated on every node. Momentum factors are utilized in various algorithms, with hyper-parameter tuning and learning rate adjustments during training. Compression schemes are applied to each layer of ResNet20 separately in the experiments. Two unbiased schemes involve quantization and sparsification, while two biased schemes include selecting weights based on magnitude and sign compression. DCD and ECD analysis is limited to unbiased quantization schemes. Hyper-parameter tuning details can be found in Appendix F. Results from the experiments show that unbiased compression schemes like ECD and DCD perform well only at low compression ratios, sometimes diverging at high ratios. On the other hand, biased compression schemes like CHOCO-SGD show better performance across all scenarios with a minimal drop in accuracy. The performance of DCD with biased sparsification is notably better than with unbiased random sparsification, despite lacking theoretical support. In challenging real-world scenarios, decentralized training is essential due to limited communication bandwidth and privacy concerns. Each device has access only to local data, making centralized methods inefficient. This setting is common in sensor networks, mobile devices, and hospitals collaborating on machine learning models. In decentralized training, each device has limited communication bandwidth and privacy concerns, with access only to local data. The global network topology is typically unknown, and the number of connected devices is large. Data is permanently split between nodes, and no prior works have studied this scenario for decentralized deep learning. The centralized baseline involves routing updates to a central coordinator for aggregation. In decentralized training, all-reduce is not efficiently implementable, so comparisons are made to a centralized baseline where updates are routed to a central coordinator. CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression are compared. Training is done on 4, 16, 36, and 64 nodes using ring and torus topologies. Results show that CentralizedSGD performs well for all node numbers, while CHOCO-SGD slows down due to graph topology influence. In decentralized training, comparisons were made between CentralizedSGD and CHOCO-SGD algorithms on different node numbers and topologies. CHOCO-SGD slowed down due to graph topology influence and communication compression. Despite increasing epochs, the gap between centralized and decentralized algorithms could not be fully closed. CHOCO-SGD performed the best in terms of testing accuracy with slight degradation. The focus in real decentralized scenarios is on reducing communication costs rather than minimizing epochs. Experiments were conducted on a real social network graph using decentralized training models on user devices. The Davis Southern women social network with 32 nodes was chosen for training ResNet20 and a three-layer LSTM architecture for image classification and language modeling, respectively. CHOCO-SGD outperformed other algorithms in testing accuracy, with torus topology showing benefits for large node numbers. Decentralized and Centralized SGD required more bits to achieve reasonable accuracy. The study involved training ResNet20 and a three-layer LSTM on the Cifar10 dataset and WikiText-2, respectively. Results showed decentralized algorithm performed best in training accuracy, while centralized scheme had highest test accuracy. CHOCO-SGD outperformed exact decentralized scheme in test accuracy with less transmitted data. CHOCO-SGD outperforms exact decentralized scheme in test accuracy with less transmitted data for language modeling task. When considering perplexity for a fixed data volume, CHOCO-SGD performs best. In large-scale training with Resnet-50 on ImageNet-1k, CHOCO-SGD benefits from scaling to more nodes in a datacenter setting. Decentralized optimization methods address scaling issues even for well-connected devices. Decentralized optimization methods offer a solution for scaling issues in well-connected devices like datacenters with fast connections. Recent studies have shown that decentralized schemes can outperform centralized ones, with impressive speedups for training on multiple GPUs. The experiments were conducted on 8 machines with Tesla P100 GPUs, using Resnet-50 for ImageNet-1k training. In experiments using Resnet-50 on 8 machines with Tesla P100 GPUs, decentralized communication with compressed communication (sign-CHOCO-SGD) in a ring topology was utilized. The mini-batch size on each GPU was 128, following the general SGD training scheme. CHOCO-SGD showed faster training time compared to all-reduce, with a slight 1.5% accuracy loss. Our study demonstrates a 20% time-wise gain over the common all-reduce baseline on commodity hardware clusters. We propose using CHOCO-SGD for decentralized deep learning training in bandwidth-constrained environments, showing theoretical convergence guarantees and linear speedup in node numbers. Empirical performance was tested on image classification tasks and language modeling. Our main contribution is enabling training in strongly communication-constrained settings. The main contribution of this work is enabling training in strongly communication-restricted environments while respecting the constraint of locality of the training data. The authors demonstrate the performance of decentralized schemes for high communication compression, significantly expanding the potential applications of fully decentralized deep learning. The proof of Theorem 4.1 is presented, derived from a more general statement in Theorem A.2, following a structure similar to previous work. The authors demonstrate decentralized schemes for high communication compression in training, expanding applications of decentralized deep learning. Algorithm 1 is a special case of Algorithm 3, which involves stochastic gradient updates and averaging among nodes. Convergence is shown for algorithms with stochastic updates and linear convergence in the averaging scheme. Decentralized SGD with arbitrary averaging is presented in Algorithm 3. The authors present decentralized schemes for high communication compression in training, expanding applications of decentralized deep learning. Algorithm 3 involves stochastic gradient updates and averaging among nodes, showing convergence with linear rate for a parameter 0 < c \u2264 1. Example algorithms like Exact Averaging and CHOCO-SGD are discussed, with a focus on the order of communication and gradient computation parts being exchangeable without affecting convergence rate. The text discusses the proof of Theorem 4.1 and Lemma A.1, showing the convergence of Algorithm 3 with constant stepsize. It highlights the impact of changing initial values on convergence rate and compares the linear speedup of CHOCO-SGD with other algorithms. The dependence on the eigengap of the mixing matrix W is also mentioned. The text discusses the convergence rate of CHOCO-SGD with the CHOCO-GOSSIP averaging scheme, showing a worse dependence on the eigengap of the mixing matrix W. The proof of Theorem A.2 involves L-smoothness and bounding terms to establish guarantees for parameter averaging in a decentralized setting. The text discusses the convergence rate of CHOCO-SGD with the CHOCO-GOSSIP averaging scheme, showing a worse dependence on the eigengap of the mixing matrix W. The convergence guarantees for parameter averaging in a decentralized setting are summarized briefly below. Theorem A.4 establishes convergence under certain assumptions with a constant stepsize \u03b7 and consensus stepsize \u03b3. The convergence rate of Algorithm 3 is determined by i and c, and holds for any T, although the first term is less favorable compared to Theorem A.2. Corollary A.5 discusses the convergence of local weights x (t) i under specific assumptions. The text discusses the convergence of CHOCO-SGD with the CHOCO-GOSSIP averaging scheme, showing a worse dependence on the eigengap of the mixing matrix W. The convergence guarantees for parameter averaging in a decentralized setting are summarized briefly below. Theorem A.4 establishes convergence under certain assumptions with a constant stepsize \u03b7 and consensus stepsize \u03b3. Corollary A.5 discusses the convergence of local weights x (t) i under specific assumptions. Algorithm 4 CHOCO-SGD is presented as an error feedback algorithm with stochastic gradient updates. Algorithm 2 demonstrates how to combine CHOCO-SGD with weight decay and momentum for improved performance. Algorithm 2 combines CHOCO-SGD with weight decay and momentum, adapting Nesterov momentum for a decentralized setting. CHOCO-SGD can be interpreted as an error feedback algorithm, saving quantization errors in internal memory. The procedure of model training and hyper-parameter tuning is detailed, comparing CHOCO-SGD with sign compression to decentralized and centralized SGD without compression. In this section, the comparison includes CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression. Two models are trained: ResNet20 for image classification on Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. The experimental setup follows Merity et al. (2017) with specific configurations for gradient clipping, dropout, and training epochs. The experimental setup includes training ResNet20 for image classification on Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. The BPTT length is set to 30, with gradient clipping at 0.4 and dropout only on the output of LSTM at 0.4. Both models are trained for 300 epochs with a per node mini-batch size of 32. The learning rate of CHOCO-SGD scales linearly with node degree, and momentum is applied only to ResNet20 training. Learning rate warmup and decay strategies are implemented, with optimal learning rate per sample determined by a grid search. The learning rate in CHOCO-SGD is determined by a linear scaling rule based on the node's degree and mini-batch size. The optimal learning rate is found in a predefined grid, ensuring the best performance is in the middle of the grid. The consensus stepsize is also fine-tuned. Tables 4 and 6 show the hyperparameters for training ResNet-20 on Cifar10. In a social network topology, the hyperparameters for training ResNet-20/LSTM are shown in Table 5. The training data is split between nodes with a fixed partition, no shuffling. The per node mini-batch size is 32, and the maximum node degree is 14. Learning curves for the social network are also plotted. The local models in the social network topology reach consensus towards the end of optimization, with their test performances matching that of the averaged model. Prior to decreasing the stepsize at epoch 225, the local models diverge from the averaged model, only converging when the stepsize decreases. This behavior was also observed in a previous study by Assran et al. (2019)."
}