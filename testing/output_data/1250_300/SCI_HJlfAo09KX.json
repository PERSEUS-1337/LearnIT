{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function shows strong convexity and smoothness, allowing gradient descent to converge linearly to a critical point close to the ground truth. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks. Neural networks have gained research interest for their success in practical domains like computer vision and artificial intelligence. Efforts are being made to understand the theoretical underpinnings behind their success, including model recovery to recover the underlying model parameter for better generalization. Efforts are being made to recover the underlying model parameter W in neural networks for better generalization. Previous studies have focused on regression and classification problems, studying single-neuron models, multi-neuron networks, and feedforward networks with ReLU activations. The goal is to recover neural network parameters using gradient methods. The problem of recovering neural network parameters using gradient descent has been studied for Gaussian input x and the kth neuron. Previous research has shown guarantees for convergence to the ground truth using squared loss, with some studies requiring fresh samples at each iteration while others establish strong convexity for linear convergence without per-iteration resampling. In this paper, the authors aim to provide a strong statistical guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function. They show that for a multi-neuron classification problem with sigmoid activations and Gaussian input, the empirical risk function based on the cross entropy loss is uniformly strongly convex in a local neighborhood of the ground truth. The sample size required for this guarantee is O(dK^5 log^2 d), where d is the input dimension and K is the number of neurons. The empirical risk function based on the cross entropy loss is uniformly strongly convex in a local neighborhood of the ground truth for one-hidden-layer neural networks. Gradient descent converges linearly to a critical point with a sample complexity of O(dK^5 log^2 d). The recovery of W is up to certain statistical accuracy, and convergence to W occurs at a rate of O(dK^9/2 log n/n) in the Frobenius norm. The tensor method provides an initialization in the neighborhood of the ground truth. Our proof replaces the homogeneous assumption on activation functions in BID38 with a mild condition on the curvature of activation functions around W, allowing for a larger class of activation functions. Various new techniques are developed to analyze the cross-entropy loss function, including exploiting statistical information of geometric curvatures and developing covering arguments for uniform concentrations. Performance guarantees for classification using the squared loss are also achieved. The focus is on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization. The parameter recovery viewpoint is crucial for non-convex learning in signal processing problems. Studies on one-hidden-layer network models focus on landscape analysis and model recovery. It is known that with a large enough network size compared to data input, there are no spurious local minima in the optimization landscape. In the case of multiple neurons in the under-parameterized setting, the landscape of the population squared loss surface with ReLU activations has been studied. In the under-parameterized setting with multiple neurons, Tian studied the landscape of the population squared loss surface with ReLU activations, revealing spurious bad local minima. Zhong et. al. provided characterizations for the local Hessian in regression with various activation functions. For a single neuron with Gaussian input, BID28 showed linear convergence of gradient descent with ReLU activation and O(d) sample complexity. BID21 demonstrated linear convergence of (projected) gradient descent with bounded derivatives of the activation function. In the under-parameterized setting with multiple neurons, previous studies have analyzed the landscape of the population squared loss surface with ReLU activations and characterized the local Hessian in regression with various activation functions. The current study focuses on the classification problem using the squared loss, showing linear convergence of (projected) gradient descent with bounded derivatives of the activation function. The analysis differs from previous work by studying the cross entropy loss function and the model recovery classification problem under the multi-neuron case, which has not been explored before. The paper introduces a two-layer feedforward network with ReLU activations and identity mapping. Results are not directly comparable to previous studies due to differences in networks and loss functions. The paper is organized into sections discussing problem formulation, local geometry, gradient descent convergence, initialization method, numerical examples, and conclusions. Notation conventions for vectors, matrices, norms, and constants are also provided. The paper describes the generative model for training data and the gradient descent algorithm for learning network weights. It focuses on a one-hidden layer neural network model for classification, aiming to estimate the network weights by minimizing the empirical risk function. The paper presents a one-hidden layer neural network model for classification. The goal is to estimate the network weights by minimizing the empirical risk function using gradient descent algorithm with a well-designed initialization scheme. The algorithm uses the same set of training samples throughout execution, unlike other methods that employ resampling at every iteration. The paper introduces a one-hidden layer neural network model for classification, aiming to estimate network weights by minimizing the empirical risk function using gradient descent. Unlike other methods that use resampling, this algorithm uses the same training samples throughout execution. The main results are based on an important quantity related to the loss function's geometric properties. The paper introduces a neural network model for classification, focusing on minimizing the empirical risk function using gradient descent. The Hessian of the empirical risk function is guaranteed to be positive definite in a local neighborhood of the ground truth W, under certain conditions. The results are based on the loss function's geometric properties. The Hessian of the empirical cross-entropy loss function is positive definite in a neighborhood of the ground truth W under certain conditions. The bounds in Theorem 1 depend on network dimension parameters and activation functions. For orthonormal columns, Theorem 1 guarantees a specific result with a certain sample complexity. In the classification problem, due to quantized labels, W is not a critical point of the loss function. Gradient descent converges to a unique local minimizer close to the ground truth W. Theorem 2 guarantees the existence of a unique critical point W n in a neighborhood of W, which converges to W at a rate of O(K 9/4 d log n/n). Gradient descent converges linearly to W n as long as it is initialized in the basin of attraction. The tensor method proposed in BID38 is used for initialization, ensuring the recovery of W consistently as n approaches infinity. Gradient descent converges linearly to W n at a linear rate when initialized in the basin of attraction. Achieving -accuracy requires a computational complexity of O(ndK^2 log(1/\u03b5)), with the initialization algorithm summarized in Algorithm 2. The tensor method proposed in BID38 is used for initialization to ensure consistent recovery of W as n approaches infinity. The initialization algorithm, summarized in Algorithm 2, involves estimating the direction of each column of W and approximating the magnitude and sign of w i through non-orthogonal tensor decomposition. Technical assumptions are made for the activation function \u03c6(z) in the classification problem, with no requirement for the homogeneous assumption. The activation function \u03c6(z) in the classification model satisfies certain conditions, including a curvature assumption around the ground truth. The performance guarantee for the initialization algorithm is provided in Theorem 3, stating that under specific assumptions, the output W 0 of Algorithm 2 meets certain criteria with high probability. If the sample size is sufficiently large, the output of Algorithm 2 satisfies certain criteria with high probability. The proof involves accurate estimation of the direction and norm of W. Gradient descent is used to verify the strong convexity of the empirical risk function around W. Multiple random initializations are performed to ensure convergence to the same critical point. The variance of the output is calculated to quantify the estimator's standard deviation. The variance of the output of gradient descent is calculated after multiple random initializations. The success rate of gradient descent is shown to be high with large sample complexity. Statistical accuracy of the local minimizer is demonstrated when initialized close to the ground truth. Average estimation error is calculated through Monte Carlo simulations. In this study, the average estimation error is calculated through Monte Carlo simulations with random initializations close to the ground truth. The estimation error decreases as sample size increases, matching theoretical predictions. Gradient descent with cross entropy loss outperforms squared loss in a classification problem, highlighting the preference for cross entropy in such scenarios. The research focuses on model recovery in a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem, characterizing sample complexity for local strong convexity near the ground truth. The study focuses on characterizing sample complexity for local strong convexity near the ground truth in a multi-neuron classification problem using cross entropy loss. The analysis aims to guarantee linear convergence of gradient descent to the ground truth with proper initialization. Future work includes extending the analysis to different activation functions and network structures. The study analyzes sample complexity for local strong convexity near the ground truth in a multi-neuron classification problem using cross entropy loss. It aims to ensure linear convergence of gradient descent with proper initialization. The analysis includes showing smoothness and convexity properties of the Hessian matrix in a neighborhood of the ground truth. The study focuses on sample complexity for local strong convexity near the ground truth in a multi-neuron classification problem using cross entropy loss. It aims to ensure linear convergence of gradient descent with proper initialization by analyzing smoothness and convexity properties of the Hessian matrix in a neighborhood around the ground truth. Lemmas and theorems are presented to demonstrate the relationship between the empirical loss function and the population loss function. The text discusses the existence of a critical point in a neighborhood around the ground truth in a multi-neuron classification problem. It shows that the gradient concentrates around the true gradient, leading to the convergence of gradient descent to the critical point. Lemmas and theorems are used to establish the uniqueness of the critical point in the neighborhood. The text demonstrates the existence of a critical point in a neighborhood around the true solution in a multi-neuron classification problem. Lemmas and theorems are used to prove the uniqueness of this critical point. Gradient descent is shown to converge linearly to this local minimizer. The proof demonstrates the accuracy of estimating the direction of W in gradient descent, showing convergence to the local minimizer Wn. Two parts of the proof are outlined: (a) accuracy of direction estimation similar to BID38, and (b) proof based on a mild condition in Assumption 2. The tensor operation is defined for matrices A, B, C. BID38 shows estimation of direction for regression problems, extending to classification with slight differences in proof. The proof in the curr_chunk focuses on estimating weights for regression and classification problems using Bernstein inequality. It differs from BID38 by not requiring homogeneous conditions on the activation function. The estimation of direction in gradient descent is accurate, converging to the local minimizer Wn. The proof in the curr_chunk provides a different method for estimating weights without requiring homogeneous conditions on the activation function. It involves defining a quantity Q1 and solving an optimization problem to estimate wi. The estimation process involves substituting values and solving equations to obtain accurate estimates of the weights. The curr_chunk discusses the estimation of weights using a specific equation and the sign of \u03b2i. It introduces useful definitions such as sub-gaussian and sub-exponential norms of random variables. The proof involves solving equations to obtain accurate estimates of the weights without requiring homogeneous conditions on the activation function. The curr_chunk introduces the sub-gaussian and sub-exponential norms of random variables, along with calculations of the gradient and Hessian. It discusses the estimation of weights using specific equations and the sign of \u03b2i, aiming to obtain accurate weight estimates without requiring homogeneous conditions on the activation function. The curr_chunk discusses upper bounding E T 2 j,l,k using Cauchy-Schwarz inequality and presents a lemma for sigmoid activation function. It then derives an inequality for a constant C and discusses bounds of the Hessian of the population risk at ground truth. The curr_chunk presents upper and lower bounds of the Hessian of the population risk at ground truth, deriving inequalities and applying Lemma 1 to obtain a uniform bound in the neighborhood of W. It further discusses the proof of Lemma 3 by adapting analysis from a previous setting. The curr_chunk discusses the proof of Lemma 3 by adapting analysis from a previous setting, bounding terms P (A t ), P (B t ), and P (C t ) separately. It also introduces a technical lemma for the proof, stating an upper bound for G i \u03c81."
}