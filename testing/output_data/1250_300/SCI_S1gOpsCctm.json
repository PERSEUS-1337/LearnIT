{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are effective for control policies in reinforcement and imitation learning but are challenging to explain due to continuous-valued memory vectors. A new technique, Quantized Bottleneck Insertion, creates finite representations of these vectors for better analysis. Results on synthetic environments and Atari games show small finite representations with improved interpretability. Deep reinforcement learning and imitation learning have shown impressive performance but lack explainability, limiting their trustworthiness in high-stakes applications. In this paper, the focus is on understanding and explaining RNN policies by creating more compact memory representations. The challenge lies in interpreting the high-dimensional continuous memory vectors used in RNN policies, which are updated through complex gating networks. The goal is to quantize the memory and observation representation to enhance explainability and aid in comprehending RNN policies. Our main contribution is introducing a method to transform RNN policies with continuous memory and observations into a finite-state representation called a Moore Machine. This involves using Quantized Bottleneck Networks (QBNs) to encode memory states and observation vectors encountered during RNN operation. The QBNs are inserted into the RNN policy to replace the wires that transmit memory and observation vectors. This approach aims to capture powerful forms of memory usage in a more understandable and explainable way. The trained RNN is enhanced by QBNs to encode memory states and observation vectors, creating a Moore Machine Network (MMN) that closely resembles the original RNN. The MMN can be used directly or fine-tuned for better accuracy. Training quantized networks with QBNs is shown to be effective, especially with \"straight through\" gradient estimators. Experiments in synthetic domains and Atari games demonstrate the ability to extract near-equivalent MMNs, providing insights into RNN memory use and achieving state-of-the-art performance. Our approach accurately extracts ground-truth MMNs from RNNs, providing insights into memory usage. Experiments on 6 Atari games show near-equivalent MMNs, revealing small memory sizes and different control strategies. Previous work has focused on understanding Recurrent Networks, but our work is unique in learning finite-memory representations of continuous RNN policies. Our work focuses on learning finite-state representations of recurrent neural networks, specifically in extracting Finite State Machines (FSMs) from RNNs trained to recognize languages. Unlike previous approaches that produce an FSM approximation separate from the RNN, our method directly inserts discrete elements into the RNN to preserve its behavior while allowing for a finite state characterization. This approach enables fine-tuning and visualization using standard learning frameworks. Our approach involves inserting discrete elements into RNNs to extract Finite State Machines (FSMs), allowing for fine-tuning and visualization. Unlike prior work on learning fully binary networks, we focus on learning discrete representations of memory and observations while maintaining the network's flexibility. Our approach involves inserting discrete elements into RNNs to extract Finite State Machines (FSMs) for interpretability. RNNs are commonly used in reinforcement learning to represent policies with internal memory, updating a continuous-valued hidden state at each time step. Our approach involves using Moore Machines and their deep network counterparts to extract compact quantized representations of memory and observations in reinforcement learning. Moore Machines are finite state machines labeled by output values, allowing for the investigation of key features of memory and observations. A Moore Machine Network (MMN) is a representation of a Moore Machine using deep networks, with a transition function and policy mapped through deep networks. It also includes a mapping from continuous observations to a finite discrete observation space. The state and observation representations are quantized into discrete vectors. In this work, a Moore Machine Network (MMN) is represented using deep networks with quantized state and observation representations. The MMN is viewed as a traditional RNN with memory composed of k-level activation units and environmental observations transformed to a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging, but a new approach leverages the ability to learn RNNs first. The new approach introduces learning Moore Machine Networks (MMNs) by leveraging the ability to learn RNNs. It involves training quantized bottleneck networks (QBNs) to embed continuous observation features and hidden states into a k-level quantized representation. These QBNs are then inserted into the original recurrent net to create an MMN that consumes quantized features and maintains quantized state. The QBNs aim to discretize a continuous space by quantizing activation units in the encoding layer. A 3-level quantization is used, represented as +1, 0, and -1. To support 3-valued quantization, a specific activation function is utilized to ensure better learning outcomes. The introduction of the quantize function in QBNs makes the output non-differentiable, posing challenges for backpropagation. The activation function used in QBNs is flatter around zero input, with a specific formula \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(\u22123x). The quantize function in QBNs makes b(x) non-differentiable, posing challenges for backpropagation. The straight-through estimator is effective in handling this issue by treating the quantize function as the identity during back-propagation. Training a QBN as an autoencoder involves using the L2 reconstruction error x \u2212 b(x) 2. Recurrent policies can generate training sequences of triples (o t , f t , h t ) for training. The approach involves training two QBNs, b f and b h, on observed features and states respectively. If the QBNs achieve low reconstruction error, their latent \"bottlenecks\" can be used as high-quality encodings. These QBNs are then inserted into the original RNN as \"wires\" to propagate input to output, creating an MMN. QBN is inserted between the output of the recurrent network block and the input to the recurrent block, creating an MMN. Fine-tuning the MMN by training on the original rollout data of the RNN helps match the softmax distribution over actions produced by the RNN. Training in this way is more stable than simply outputting the same action as the RNN. Visualization and analysis tools can be used to investigate the MMN further. The MMN is fine-tuned by training on the original rollout data of the RNN to match the softmax distribution over actions. Visualization and analysis tools can be used to investigate the memory and gain a semantic understanding of its roles. Another approach is to use the MMN to create a Moore Machine over atomic state and observation spaces for further analysis. The Moore Machine is constructed from quantized features and transitions in the data, with a focus on minimizing the number of states. The experiments aim to extract Moore Machine Networks (MMNs) from RNNs without performance loss, especially in complex domains like Atari, to enhance interpretability of recurrent policies. In this section, the study addresses questions about the performance loss, the number of states and observations in minimal machines, and the interpretability of recurrent policies. Two domains are considered: a synthetic environment called Mode Counter and benchmark grammar learning problems. Mode Counter Environments (MCEs) vary in memory requirements and types of memory usage, transitioning between modes based on a distribution. MCEs are a type of Partially Observable Markov Decision Process with rewards based on correct actions. The study explores Mode Counter Environments (MCEs) with varying memory requirements and types of memory usage. MCEs transition between modes based on a distribution and are a type of Partially Observable Markov Decision Process. Three MCE instances are tested, each using memory and observations in different ways: Amnesia, where memory is not needed for optimal actions; Blind, where observations do not inform optimal actions; and a third instance testing memory use for optimal performance. The study explores Mode Counter Environments (MCEs) with varying memory requirements and types of memory usage. Three MCE instances are tested: Amnesia, where memory is not needed for optimal actions; Blind, where observations do not inform optimal actions; and a third instance testing memory use for optimal performance. The MCEs use a recurrent architecture with feed-forward and GRU layers, achieving 100% accuracy on the imitation dataset. The study focuses on training RNNs in Mode Counter Environments (MCEs) using imitation learning. The observation and hidden-state QBNs have varying bottleneck sizes in the experiments. Training QBNs in MCE environments is faster than RNN training as they do not need to learn temporal dependencies. QBNs with bottleneck sizes of 4 and 8 were embedded into the RNN to create a discrete MMN, with performance measured before and after fine-tuning. In the study, QBNs with bottleneck sizes of 4 and 8 were embedded into the RNN to create a discrete MMN. Performance was measured before and after fine-tuning, with most cases not requiring fine-tuning due to low reconstruction error. Fine-tuning resulted in perfect MMN performance, except for one case which yielded 98% accuracy. The combined error accumulation of the two bottlenecks was found to be responsible for reduced performance. The number of states and observations of the MMs extracted from the MMNs before and after minimization were also analyzed. After embedding QBNs into the RNN to create MMNs, the number of states and observations before minimization is typically higher than after. This suggests that MMN learning may not always result in minimal representations, although they accurately describe the RNN. However, after minimization, exact minimal machines were obtained for most MCE domains, showing that MMNs learned via QBN insertions were optimal in most cases. The exception was when MMN did not achieve perfect accuracy, indicating the importance of examining these machines to understand memory use. The machines for Blind and Amnesia demonstrate different memory use. The policies' actions in the Tomita Grammars environments are determined by the current observation. RNNs are trained using imitation learning with a specific architecture and optimizer. The RNNs for each grammar consist of a one-layer GRU with 10 hidden units and a fully connected softmax layer with 2 nodes. Imitation learning is used to train the RNNs with an Adam optimizer and learning rate of 0.001. The training dataset includes accept/reject strings with lengths in the range [1, 50]. Test results show high accuracy, with the exception of grammar #6. MMNs are created without a bottleneck encoder, using a bottleneck for the hidden memory state. Fine-tuning of MMNs generally maintains RNN performance. The MMNs were created without a bottleneck encoder and fine-tuning generally maintained RNN performance. MM extraction and minimization resulted in a reduction of state-space while maintaining performance. The technique was applied to RNNs for six Atari games, showing equivalent results to minimal machines. In this section, the technique is applied to RNNs for six Atari games using OpenAI gym. The input observations for Atari are more complex than previous experiments, making it unclear if similar results can be expected. The recurrent architecture for all Atari agents is the same, with input observations preprocessed and passed through convolutional layers and a GRU layer. No other work aims to extract finite state representations for Atari policies. The network architecture for the RNNs in Atari games includes 4 convolutional layers, a GRU layer, and a fully connected layer. The A3C RL algorithm is used for training with specific parameters. The performance is reported on six games, and the training data is generated using noisy rollouts. The encoder and decoder in the RNN architecture for Atari games have multiple feed-forward layers with varying numbers of nodes. Training data is generated using noisy rollouts to increase diversity. Bottlenecks are trained for different values of B h and B f, with performance reported before and after finetuning. After training the RNN architecture for Atari games with multiple feed-forward layers, the MMNs were fine-tuned to achieve similar performance to the original RNN for games like Pong, Freeway, Bowling, and Boxing. However, for Breakout and Space Invaders, the MMNs achieved lower scores due to poor reconstruction in certain parts of the game, such as failing to press the fire-button in Breakout. The drop in performance in Breakout and Space Invaders was attributed to poor reconstruction in specific game scenarios. For instance, in Breakout, the learned MMN failed to press the fire-button after clearing the first board, resulting in lower scores. Minimizing the MMNs significantly reduced the number of states and observations, making them easier to analyze. Understanding memory use in Atari games revealed the need for more intelligent training approaches to capture critical information in rare but important states. Understanding Memory Use in Atari games revealed different types of memory use in various games. In Pong, the policy only has three states and transitions to the same state regardless of the current state, making it a rule-based mapping of observations to actions. On the other hand, Bowling and Freeway policies ignore input images and are open-loop controllers that depend on time-step rather than observations. Freeway, in particular, always takes the Up action. The MM extraction approach revealed different memory use in Atari games. Freeway has a simple policy of always taking the Up action, while Bowling has a more complex open-loop structure. Breakout, Space Invaders, and Boxing use both memory and observations in their policies. Further analysis of observations and states is needed for a complete understanding of the Atari policies. Our approach extracts finite state Moore Machines from RNN policies by inserting bottlenecks trained with Quantized Bottleneck Networks. This allows for accurate extraction of ground truth machines in known environments and maintains performance in Atari games where ground truth is unknown. Our approach accurately extracts ground truth machines from RNN policies in Atari games, maintaining similar performance. The extracted machines reveal insights into memory usage, showing small memory states and cases where memory was not utilized effectively. Future work includes developing tools for attaching meaning to observations and analyzing finite-state machine structure for further insight. The MCE is parameterized by mode number M, transition function P, lifespan mapping \u2206(m), and count set C. The hidden state is (m, c), where m is the current mode and c is the consecutive time-steps in that mode. Mode changes occur when lifespan is reached, with next mode generated by transition distribution P. Observations o are continuous-valued and determine mode based on current state. The MCE involves mode changes based on lifespan and transition distribution. Observations determine mode based on current state, with memory needed to track mode changes. Experiments include Amnesia, testing reactive policies, and Blind, with deterministic mode transitions. The experiments involve testing different policies in the MCE environment, including Blind and Tracker. Blind uses memory to track deterministic mode sequences, while Tracker requires paying attention to observations and using memory to track mode changes. The environment can pose challenging problems as the number of modes and their life-spans increase."
}