{
    "title": "S1e-0kBYPB",
    "content": "In this work, the authors highlight issues with current explanatory methods for AI systems, particularly in explaining decisions of black-box models like neural networks. They point out that different explanatory perspectives lead to varying instance-wise explanations and that current explainers are mainly validated on simple models, not real-world neural networks. To address this, they introduce a verification framework based on a complex neural network architecture trained on a real-world task, providing guarantees on its inner workings. The authors introduce a framework based on a neural network architecture to evaluate explanatory methods for black-box machine learning models. They aim to provide a publicly available evaluation from a feature-selection perspective, highlighting the limitations of current explainers. In the context of evaluating explanatory methods for black-box machine learning models, different perspectives such as feature-selection and feature-additivity lead to fundamentally different explanations for model predictions. Comparisons between explainers like L2X, LIME, and SHAP may not be coherent due to their different explanation targets. While current methods can identify biases, their reliability in explaining model behavior remains an open question. Current explanatory methods are successful in identifying biases, but their reliability in explaining model behavior is uncertain. Evaluating explainers for complex neural networks often assumes that the target models behave reasonably, but recent works have shown surprising spurious correlations in human-annotated datasets that neural networks rely on heavily. The text discusses the unreliability of penalizing explainers for pointing to insignificant tokens identified by a model. A framework is proposed to evaluate explanatory methods by testing if explainers rank irrelevant tokens higher than relevant ones. The framework is applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis. The framework introduced evaluates explanatory methods by testing if explainers rank irrelevant tokens higher than relevant ones. It is applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis. The test does not rely on speculations on the behavior of the target model and penalizes explainers only when errors are guaranteed. L2X (Chen et al., 2018), a feature-selection explainer, is evaluated under this test. The study evaluates LIME and SHAP explainers against L2X using a feature-selection perspective. LIME and SHAP generally outperform L2X, with errors in predicting relevant tokens highlighted. The test framework will be released for community use in evaluating future explanatory methods. The study compares LIME and SHAP explainers to L2X from a feature-selection perspective. The test framework will be available for community use in evaluating future explanatory methods, which can be applied to various tasks or research areas. Feature-based explainers are categorized into feature-additive and feature-selective types, each explaining predictions in different ways. In this work, the focus is on verifying feature-based explainers, which are the majority of current works. Various types of explanations are discussed, including example-based and human-level explanations. Evaluations commonly performed include testing explainers on interpretable target models like linear regression and decision trees. The evaluation of explainers commonly involves testing them on interpretable target models such as linear regression and decision trees, synthetic setups, and assuming a reasonable behavior in high-performing models. The evaluation of explainers involves assuming a reasonable behavior in high-performing models. Crowd-sourcing evaluation may not be reliable for assessing the faithfulness of the explainer to the target model. Explanations are tested by presenting humans with predictions and explanations to infer the model's outputs. Our evaluation of explainers involves automatic testing on a non-trivial neural network model, ensuring fidelity to the target model. This approach contrasts with human-based evaluations, which can be costly and time-consuming. Our framework is similar to a sanity check introduced by Adebayo et al. (2018) but is more challenging and requires a stronger fidelity of the explainer to the model. Explanatory methods aim to provide accurate explanations for models trained on real data, requiring a strong fidelity to the target model. These methods adhere to the perspective of feature-additivity, where the explanation of a prediction consists of contributions from each feature that approximate the prediction. Various methods, such as LIME and Shapley values from game theory, follow this perspective to explain model predictions. Lundberg & Lee (2017) unified these methods by showing that Shapley values satisfy desired constraints for accurate explanations. The Shapley values from game theory provide contributions that verify three desired constraints for accurate explanations: local accuracy, missingness, and consistency. These values calculate the contribution of each feature in an instance by averaging its contributions over a neighborhood of the instance. The choice of neighborhood is crucial, and it remains an open question which neighborhood is best to use in practice. Another perspective involves feature-selection, where the explanation of a model prediction consists of a small subset of features that alone lead to a similar prediction as the original one. Various studies have explored this perspective, including Chen et al. (2018), Carter et al. (2018), and Ribeiro et al. (2018). The explanation of a model prediction involves a small subset of features that lead to a similar prediction as the original one. Different perspectives on this include maximizing mutual information between features and prediction, but the number of important features per instance is often unknown. The model may not always rely on a small subset of features, but this can be true for tasks like sentiment analysis. The hypothetical sentiment analysis regression model aims to assign scores from 0 (most negative) to 1 (most positive). Real-world neural networks may exhibit biases due to dataset limitations. Specific tokens can heavily influence model predictions, as shown in examples. Feature-additive explanations reveal the importance of certain words in determining sentiment scores. The feature-additive perspective highlights the contribution of words like \"good\" and \"nice\" in determining sentiment scores. The feature-selective perspective focuses on individual instances, showing differences in feature importance rankings. Both perspectives offer insights into model behavior, with potential preference depending on real-world use-cases. In the paper, a verification framework is proposed for the feature-selection perspective of instance-wise explanations using the RCNN model. The framework prunes the dataset to identify irrelevant and relevant features for each datapoint. Metrics are introduced to measure the ranking of tokens by explainers. The RCNN model consists of a generator and an encoder, both utilizing recurrent convolutional neural networks. The study by Lei et al. (2016) introduces a framework with a generator and encoder modules using recurrent convolutional neural networks. The generator selects tokens from input text to pass to the encoder for prediction, with supervision only on the final output. Regularizers encourage the generator to select shorter sub-phrases and fewer tokens. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability during training. The study by Lei et al. (2016) introduced a framework with a generator and encoder modules using recurrent convolutional neural networks. Gradients for the generator were estimated using a REINFORCE-style procedure. The model might have learned an internal communication protocol that encodes information from non-selected tokens, called a handshake. The goal is to eliminate instances that contain handshakes, ensuring non-selected tokens have zero contribution to the prediction. The study introduced a framework with a generator and encoder modules using recurrent convolutional neural networks. The model might have learned an internal communication protocol that encodes information from non-selected tokens, called a handshake. The proof is in Appendix B. The model selects relevant features by pruning instances that contain handshakes, ensuring non-selected tokens have zero contribution to the prediction. The dataset is pruned to retain instances with relevant features, ensuring non-selected tokens have no contribution to the prediction. Further pruning is done to confirm the relevance of selected tokens by checking for a significant change in prediction when removing them. The dataset is pruned to retain instances with relevant features, ensuring non-selected tokens have no contribution to the prediction. Tokens that change the prediction significantly are considered important and are ranked higher. Selected tokens are further evaluated to confirm their relevance for the prediction. The dataset is pruned to retain instances with relevant features, ensuring non-selected tokens have no contribution to the prediction. Evaluation metrics are used to assess explainers that provide a ranking over the features, with error metrics defined based on the importance of tokens in the instance. In this work, evaluation metrics are used to assess explainers that rank features in instances. The study focuses on three metrics: (A) Percentage of instances where the most important token is among non-selected tokens, (B) Percentage of instances with errors in explanation, and (C) Average number of non-selected tokens ranked higher than relevant tokens. The framework is applied to the RCNN model trained on the BeerAdvocate corpus, consisting of human-generated beer reviews. The RCNN model was evaluated on the BeerAdvocate corpus, which includes 100K human-generated beer reviews with aspects like appearance, aroma, and palate. Three separate RCNNs were trained for each aspect, predicting ratings rescaled between 0 and 1. Relevant tokens were identified using a threshold of 0.1, with statistics provided in Appendix A. The study evaluated the RCNN model on the BeerAdvocate corpus, using a threshold of 0.1 to identify relevant tokens. Statistics on the dataset are provided in Appendix A. Three popular explainers, LIME, SHAP, and L2X, were tested with default settings, with some modifications for L2X. In the evaluation of explainers LIME, SHAP, and L2X on the RCNN model using the BeerAdvocate corpus, LIME and SHAP outperformed L2X on most metrics. L2X's limitation lies in requiring the number of important features per instance to be known, which is often not the case in practice. Testing L2X with an assumed average number of important features highlighted by human annotators showed varying results on different aspects. In the evaluation of explainers LIME, SHAP, and L2X on the RCNN model using the BeerAdvocate corpus, LIME and SHAP outperformed L2X on most metrics. L2X's limitation lies in requiring the number of important features per instance to be known, which is often not the case in practice. Testing L2X with an assumed average number of important features highlighted by human annotators showed varying results on different aspects. The average number of tokens highlighted by human annotators was 23, 18, and 13 for the three aspects. Explainers were prone to ranking zero-contribution tokens higher than clearly relevant features, with SHAP showing fewer mistakes compared to L2X. In the evaluation of explainers LIME, SHAP, and L2X on the RCNN model using the BeerAdvocate corpus, LIME and SHAP outperformed L2X on most metrics. L2X's limitation lies in requiring the number of important features per instance to be known, which is often not the case in practice. Testing L2X with an assumed average number of important features highlighted by human annotators showed varying results on different aspects. The average number of tokens highlighted by human annotators was 23, 18, and 13 for the three aspects. Explainers were prone to ranking zero-contribution tokens higher than clearly relevant features, with SHAP showing fewer mistakes compared to L2X. In the qualitative analysis, both LIME and SHAP attributed importance to nonselected tokens, with LIME and SHAP even ranking tokens like \"mouthfeel\" and \"lacing\" as most important. L2X, on the other hand, highlighted \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as the most important tokens. In this work, a distinction between two perspectives of explanations is highlighted. An evaluation test for post-hoc explanatory methods is introduced, offering guarantees on the behavior of neural networks. Error rates on different metrics for explanatory methods are presented to raise awareness of potential failures. The methodology, although applied to natural language processing, is adaptable to other tasks like computer vision. The methodology discussed is generic and can be applied to various tasks, such as computer vision. The evaluation provides insights into the limitations of post-hoc explainers, with statistics on the dataset presented in Table 2. The average numbers of selected tokens, selected tokens with a prediction difference of at least 0.1 when removed individually, and non-selected tokens are denoted as |S x |, |SR x |, and |N x | respectively. The percentage of instances eliminated from the dataset due to a potential handshake is shown in column %(S Sx = S x ). The methodology can be applied to tasks like computer vision, with insights on explainers' limitations presented in Table 2. The methodology involves determining if a handshake occurred by analyzing the prediction consistency of selected tokens. If the prediction remains the same after removing non-selected tokens, it indicates no handshake. This process is applied to tasks like computer vision, highlighting limitations of explainers. The text also briefly describes a beer's appearance, aroma, and taste. The beer poured from a nice brown \"grolsch\" like bottle, with a dark fizzy yellow color and a lot of head that laces well. It smells like fruit, possibly apple and blueberry, with no discernible mouthfeel. The taste is fruity with minimal alcohol, finishing with a slight warming sensation. Overall, better than most American lagers, very smooth, but easy to drink too quickly. The beer, poured from a brown \"grolsch\" bottle, has a dark yellow color with a fruity aroma of apple and blueberry. It is smooth, with minimal alcohol taste and a slight warming sensation. Overall, better than most American lagers."
}