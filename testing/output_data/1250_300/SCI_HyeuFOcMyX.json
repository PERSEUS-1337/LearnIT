{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences in language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance. Evidence from linguists suggests that planning ahead is essential for grammatical correctness in human speech. In contrast, current NMT models lack this planning phase. In neural machine translation, planning ahead for sentence structure is crucial for generating accurate translations. Unlike humans, NMT models lack a planning phase, leading to uncertainty in word prediction. To address this, a proposed framework inserts planner codes at the beginning of output sentences to guide the translation process. This helps the model plan the coarse structure of the sentence before decoding actual words, improving translation performance. In the proposed framework for neural machine translation, planner codes are inserted at the beginning of output sentences to guide the translation process. These codes help in planning the sentence structure before decoding words, improving translation accuracy. The codes are learned through a network that reconstructs the syntactic structure of the translation using both the input sentence and the planner codes. The proposed framework for neural machine translation involves using planner codes to guide the translation process by reconstructing the syntactic structure of the translation. The planner codes are merged with the target sentences in the training data, allowing for improved translation performance and the ability to control the structure of output sentences. The structural annotation is simplified by removing irrelevant POS tags to reduce uncertainty in the decoding phase. The text discusses extracting coarse structural annotations from target sentences by simplifying POS tags and learning planner codes to remove uncertainty in sentence structure during translation. The proposed framework uses planner codes to guide the translation process and improve performance by controlling the output sentence structure. The text describes a code learning model that uses simplified POS tags to compute discrete codes, which are then used to initialize a decoder LSTM for predicting tag sequences in a sequence auto-encoder architecture. The model is optimized with crossentropy loss and can generate planner codes for target sentences in machine translation datasets. The code learning model generates planner codes for target sentences in machine translation datasets. These codes are used to train a regular NMT model, where beam search is used during decoding. Various methods have been proposed to improve the syntactic correctness of translations, such as restricting the search space of the NMT decoder and incorporating target-side syntactic structures explicitly. The BID2 Machine Translation system takes a multi-task approach by incorporating dependency trees into the NMT model. Various methods have been proposed to improve syntactic correctness in translations, including interleaving CCG supertags with output words and training NMT models to generate linearized constituent parse trees. Additionally, some works learn discrete codes for different purposes, such as compressing word embeddings and breaking down dependencies among words with shorter code sequences. The models are evaluated on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks. The decoding process can be accelerated by predicting shorter artificial codes. Models are evaluated on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks. Kytea is used to tokenize Japanese texts and moses toolkit for other languages. The code learning model uses bytepair encoding with 256 hidden units in all layers. Nesterov's accelerated gradient is used for training with a learning rate of 0.25. Different settings of code length N and number of code types K are tested, with the information capacity being N log K bits. A trade-off is observed between accuracy in reconstructing the source sentence and guessing the correct code. The setting of N = 2, K = 4 is found to have a balanced trade-off. The NMT model uses 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with 256 units for IWSLT De-En task and 1000 units for ASPEC Ja-En task. Key-Value Attention is applied in the first decoder layer, and residual connection BID3 is used to combine hidden states in two decoder layers. Dropout with a rate of 0.2 is applied outside of the recurrent function. The NAG optimizer with a learning rate of 0.25 is used for training, and parameters are chosen based on a validation set. Conditioning word prediction on generated planner codes improves translation performance. The NMT model uses key-value attention and residual connections in the decoder layers. Best parameters are chosen based on a validation set. Conditioning word prediction on planner codes improves translation performance, especially with beam search. Manual selection of planner codes can also be beneficial. The NMT model utilizes key-value attention and residual connections in the decoder layers. Best parameters are selected using a validation set. Conditioning word prediction on planner codes enhances translation performance, particularly with beam search. Manual selection of planner codes can also be advantageous. The model can generate translations with diverse structures by manipulating the codes, as shown in Table 3. The proposed method is effective for sampling paraphrased translations with high diversity. The distribution of learned codes for English sentences in the ASPEC Ja-En dataset is illustrated in Figure 3. The distribution of learned codes for English sentences in the ASPEC Ja-En dataset is skewed, indicating room for improvement. Instead of discrete codes, predicting structural annotations directly can degrade performance. Adding a planning phase in neural machine translation improves translation performance by generating planner codes to control sentence structure. The proposed method enhances translation performance and allows for diverse sentence structures. The proposed method in this study involves designing an end-to-end neural network with a discretization bottleneck to predict simplified POS tags of target sentences. Experiments demonstrate improved translation performance and the ability to sample translations with different structures using planner codes. The planning phase aids the decoding algorithm by reducing uncertainty in sentence structure, and the framework can be extended to plan other latent factors like sentiment or topic."
}