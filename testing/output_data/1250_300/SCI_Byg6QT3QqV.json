{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. So, robotic teammates need to be able to provide explanations for their behavior. Previous work focused on explaining the reasoning behind the robot's actions but did not consider the mental workload required to understand these explanations. This means that human teammates must understand the provided explanations, regardless of the amount of information presented, before task execution. In this work, the focus is on generating online explanations during execution to reduce human mental workload. The challenge lies in the interdependence of different parts of an explanation. Three implementations of online explanation generation are presented, based on a model reconciliation setting. Evaluation was done with human subjects in a planning competition domain and simulation with different problems across two domains. As intelligent robots interact more with humans, providing explanations for their decisions is crucial for maintaining trust and shared awareness. Prior work on explanation generation often overlooks the recipient's perspective, but a good explanation should consider the discrepancies between human and robot models. Model reconciliation is essential for generating lucid explanations from the human's viewpoint. In prior work, inconsistencies between human and robot models are encapsulated as model differences. The robot should explain to reconcile these differences, ensuring its behavior makes sense to the human. Model reconciliation is crucial for generating clear explanations. One issue is the lack of consideration for the mental workload required for understanding explanations. Explanations, especially complex ones, should be provided in an online fashion. In this work, the argument is made for providing explanations in an online fashion to reduce mental workload. Online explanations intertwine with plan execution, ensuring smooth communication of information. The challenge lies in ensuring that different parts of an explanation are dependent on each other. The online explanation generation process spreads out information while avoiding cognitive dissonance. An example is given of two friends, Mark and Emma, planning a study session with breaks and activities in between. Mark and Emma plan a study session together. Mark wants to break the session into two parts with lunch in between, while Emma prefers to study in one session and have lunch afterwards. Mark gradually reveals his plan to Emma to ensure they both have energy for studying and lunch. This example highlights the importance of providing explanations in an online fashion to reduce mental workload. The importance of providing online explanations during plan execution is demonstrated in the scenario where Mark and Emma have different values regarding their study session and lunch break. Mark gradually explains his plan to Emma to make it acceptable and understandable, reducing mental workload. This new method of explanation, called online explanation, breaks down information into multiple parts to be communicated at different times during the plan execution. The new form of explanation, known as online explanation, breaks down information into multiple parts to be communicated at different times during plan execution. Three different approaches for online explanation generation are implemented, focusing on matching the plan prefix, making the next action understandable, and aligning the robot's plan with a possible optimal human plan. These approaches aim to reduce the mental workload of the recipient by ensuring that earlier information does not affect later parts of the explanation. The effectiveness of these approaches is evaluated with human subjects and in simulation. Recently, explainable AI has become essential for human-AI collaboration, improving human trust and maintaining shared situation awareness. The effectiveness of explainable agency is assessed based on accurately modeling human perception of the AI agent. This includes modeling the world and other agents' perception of the AI agent to infer their expectations. An explainable AI agent must accurately model human perception by also considering other agents' perception of itself. This model allows the agent to generate understandable motions, plans, and actions. The agent can signal its intentions before execution to provide context for human understanding. Additionally, the agent can explain its behavior by generating tailored explanations based on the recipient's perception model. In order to maintain optimal behavior, research has focused on generating tailored explanations based on the recipient's perception model. It is important to consider the mental workload required for understanding an explanation, especially for complex explanations. Online explanation generation intertwines explanation with plan execution by providing minimal information to explain the current part of the plan. Our problem is closely associated with planning problems, defined as a tuple (F, A, I, G) using PDDL, similar to STRIPS. Actions have preconditions, add and delete effects. The robot's plan to be explained must be optimal according to M R, considering the human's model M H. The cost of the optimal plan based on the initial and goal state pair under M R is explained in a model reconciliation setting, considering the human's model M H. Explanation generation involves updating M H to make the robot's plan fully explainable in the human's model. A mapping function converts a planning problem into a set of features that specify the problem. The explanation generation problem involves converting a planning problem into a set of features to reconcile models and minimize the cost difference between human and robot plans. A complete explanation is one that optimizes the robot's plan in the human's model with minimal changes. Online explanation generation aims to provide minimal information during plan execution to explain the part of the plan that is of interest and not explainable. It involves splitting explanations into sub-explanations made in an online fashion as the plan is executed, ensuring actions in the robot's plan match the human's expectation. Three different approaches are presented based on this concept. Online explanation generation involves splitting explanations into sub-explanations made in real-time as the plan is executed to match the human's expectation. Three approaches are discussed: Plan Prefix matching, Next Action matching, and any prefix matching. The planning process must consider how model changes affect the human's expectations after each sub-explanation. The challenge lies in ensuring model changes do not render a mismatch in previously reconciled plan prefixes. The challenge in online explanation generation is to ensure that model changes do not cause a mismatch in previously reconciled plan prefixes. This is addressed by searching for the largest set of model changes that would not alter the plan prefixes in M H. The process involves recursively checking plan prefixes against the human model M H to find the optimal plan with sub-explanations. Our approach involves searching for sub-explanations by starting from the robot model and stopping when the plan prefixes for the updated human model and robot model match. This process is more computationally expensive but allows us to outperform other approaches in terms of computation. The dotted line represents the border of the maximum state space model modification in the robot model to reconcile the two models up to the current plan execution. The dotted line represents the border of the maximum state space model modification in the robot model to reconcile the two models up to the current plan execution. Maximum updates to the robot model correspond to minimum updates to the human model. The algorithm searches for the largest set of model changes to the human model such that the plan prefix matches with the robot model up to a certain step. The recursive search algorithm for model space OEG is presented for finding model changes. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. We use a recursive model reconciliation procedure on the model space to find the largest set of model changes that can satisfy constraints. The goal is to ensure that the robot and human plan have the same prefix at any step of plan execution. This approach relaxes the plan prefix condition, allowing the robot to match the very next action in the plan regardless of earlier actions. The approach presented involves reconciling between human and robot plans to match the next action, regardless of earlier actions. A recursive model reconciliation procedure is used on the model space, focusing on explaining the immediate next action that differs between the plans. The search is performed from M H \\M H for computational efficiency, and only the next action that does not match is explained. The OEG approach involves reconciling human and robot plans by explaining the immediate next action that differs between them. The search process combines elements from M H and M R for better performance, with the goal of matching the next action without comparing the entire plan prefix. The OEG approach aims to reconcile human and robot plans by matching the next action that differs between them. To achieve this, a compilation approach is implemented to ensure that a plan prefix in the robot's plan is also a prefix in the human's model, reducing the need for costly generation of all human optimal plans. The key in reconciling human and robot plans is to ensure that a plan prefix in the robot's plan matches the human's model. This can be achieved by adding predicates to actions and using a recursive model reconciliation process. The agent checks for a human optimal plan with the same plan prefix as the robot's plan, stopping when no such plan exists. The agent checks for a human optimal plan with the same plan prefix as the robot's plan, stopping when no such plan exists. The process continues until an optimal human plan matches the robot's plan. Evaluation was done with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach. Our approach involves evaluating the differences between M H and M R by randomly removing preconditions from model features in the rover domain. The human subject study aims to confirm the benefits of online explanation generation, hypothesizing that it will reduce mental workload and improve task performance. The evaluation was conducted with human subjects in a modified rover domain scenario on Mars, where the rover explores space, takes rock and soil samples, images, and communicates results to the base station. The rover in the domain scenario can only store one sample at a time and must drop the current sample to take another. In the barman domain, the robot's goal is to serve drinks using specific objects with constraints. Simulation results compare different explanation approaches in the rover and barman domains, showing differences in shared model features and total number of features. The total number of model features in explanations for different approaches (MCE, OEG-PP, OEG-NA, OEG-AP) varies across problems, with OEG focused on minimal information at each time step. OEG-NA and OEG-AP share more information overall due to the dependence between features and planner behavior. There is a remaining distance between robot and human plans in terms of plan action distance. In comparing OEG-NA and OEG-AP approaches with MCE and OEG-PP, there is a distance between the robot's plan and the human's plan in terms of plan action distance. OEG-NA only considers the immediate next action, while OEG-AP may not return the same plan as the robot's. The plan distance gradually decreases during execution in OEG approaches, leading to smoother adjustments for the human. This is expected to reduce the human's mental workload. Model updates are sorted based on feature size, with consistency checks performed as explanations progress. In our implementation, model updates are sorted based on feature size, with consistency checks performed as explanations progress. A human study was conducted to compare three approaches for online explanation generation, including minimally complete explanation (MCE) and randomly breaking MCE during plan execution (MCE-R). The experiment was done using Amazon Mechanical Turk with 3D simulation, where subjects were given a rover domain task to complete within a 30-minute limit. Explanations were provided in plain English language with GIF images depicting rover actions. In a human study, subjects were tasked with acting as a rover commander in a 3D simulated scenario on Mars. They had to determine if the rover's actions were questionable, with explanations provided. Additional spatial puzzles were added to increase cognitive demand. Certain information was deliberately omitted to test the subjects' ability to create correct plans without full knowledge. In a study involving rover commanders on Mars, subjects were tested on their ability to create correct plans without full knowledge. Hidden information introduced differences between models, leading to scenarios where explanations were necessary. The robot shared all information at the beginning in one setting, while in another setting, information was broken up and communicated at different steps. Different approaches to online explanation generation were used, intertwining explanation communication with plan execution. Missing information was provided to subjects at different steps for decision-making. The study involved rover commanders on Mars testing their ability to create correct plans with hidden information. Different approaches to online explanation generation were used, intertwining communication with plan execution. Subjects were provided missing information at different steps for decision-making. NASA Task Load Index was used to evaluate the efficiency of different explanation approaches. The study evaluated mental workload using the NASA Task Load Index, calculating scores based on various sub-scales. Subjects were recruited for an academic survey, with criteria set for valid responses. The experiment focused on understanding how well subjects grasped the robot's plan with different explanations, comparing distances across settings. The study evaluated mental workload using the NASA Task Load Index, comparing distances across settings to understand how well subjects grasped the robot's plan with different explanations. OEG approaches were found to reduce human's mental workload better than MCE approaches, as shown by subjective and objective performance measures. The experiment compared OEG approaches to MCEs in terms of trust towards robots, accuracy of actions, mental workload, and task completion time. OEG approaches had fewer questionable actions, higher accuracy, and lower mental workload compared to MCEs. The overall p-value for mental workload between OEGs and MCEs was 0.0068. Task completion times varied, with OEG-NA being the fastest at 567.44s. In a pairwise comparison, the mental workload between OEGs and MCEs showed a significant difference with an overall p-value of 0.0068. Time analysis revealed varying task completion times, with OEG-NA being the fastest at 567.44s. The accuracy of the secondary task did not differ significantly among the approaches. A novel approach for explanation generation was introduced to reduce mental workload during human-robot interaction by breaking down complex explanations into smaller, easily understandable parts. Our study focused on providing easily understandable explanations through three different approaches, aiming for explainable AI. Evaluation using simulation and human subjects showed improved task performance and reduced mental workload."
}