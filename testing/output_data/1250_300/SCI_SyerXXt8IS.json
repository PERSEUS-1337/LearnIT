{
    "title": "SyerXXt8IS",
    "content": "Auto-generate stronger input features for ML methods with limited training data. Biological neural nets (BNNs) like the insect olfactory network excel at fast learning by utilizing competitive inhibition, sparse connectivity, and Hebbian updates. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, outperforming traditional methods like PCA and NNs. This approach, combining BNN-inspired feature generators with ML methods, significantly improves performance on data sets like MNIST and Omniglot, reducing test set errors by 20% to 55%. The potential value of BNN-inspired feature generators in the ML context is highlighted, aiming to improve ML methods' ability to learn from limited data by automatically generating new class-separating features. The insect olfactory network, a simple BNN, can learn rapidly from just a few samples, suggesting effective ways to separate classes for feature generation. The insect olfactory network, specifically the MothNet model, demonstrates rapid learning of vectorized MNIST digits with superior performance compared to standard ML methods, using competitive inhibition, sparsity in the Mushroom Body, and random synaptic connections. The MothNet model utilizes competitive inhibition and sparsity in the Mushroom Body to generate features for an ML classifier. Weight updates only affect MB\u2192Readout connections, with Hebbian updates based on neural firing rates. The AL-MB acts as a feature generator for the ML module, serving as downstream processing in complex BNNs. The AL-MB network served as an automatic feature generator for the ML module, improving accuracies of NN, SVM, and Nearest Neighbors on a non-spatial dataset. MothNet-generated features outperformed PCA, PLS, NNs, and transfer learning in enhancing ML accuracy, indicating stronger feature generation compared to other methods. The MothNet model outperformed PCA, PLS, NNs, and transfer learning in improving ML accuracy by generating stronger features. The vMNIST dataset was used, with baseline ML methods not achieving full accuracy at low N. The full network architecture details and Matlab code for the experiments can be found in references [11] and [12]. Experiments compared Cyborg vs baseline ML methods on vMNIST, with MothNet trained on a random set of N training samples per class. The experiments compared Cyborg vs baseline ML methods on vMNIST. MothNet was trained using stochastic differential equation simulations and Hebbian updates. The ML methods were then retrained with MothNet outputs as additional features. Trained ML accuracies of baselines and cyborgs were compared to assess gains. MothNet features were compared to PCA and PLS features in vMNIST experiments. PCA and PLS were applied to vMNIST training samples to create new features. PLS, incorporating class information, was expected to outperform PCA. Neural networks were also utilized, with one model pre-trained on vMNIST data and another using transfer learning from an Omniglot dataset. The addition of MothNet features significantly improved the accuracy of machine learning methods, demonstrating the effectiveness of the MothNet architecture in capturing new class-relevant features. MothNet features significantly improved ML accuracy across various tasks, outperforming other feature generators like PCA, PLS, and NN. The gains in accuracy ranged from 10% to 88%, with NN models benefiting the most. The relative reduction in test error ranged from 20% to 55%, with high baseline accuracies seeing the most improvement. MothNet features showed significant improvements in ML accuracy, surpassing other feature generators like PCA, PLS, and NN. The relative reduction in test error ranged from 20% to 55%, with NN models benefiting the most. Gains were observed in almost all cases with N > 3, and MothNet features were found to be more effective than other methods. The MothNet architecture, with its competitive inhibition layer (AL) and high-dimensional sparse layer (MB), showed significant improvements in ML accuracy. Even with a pass-through AL, cyborgs still achieved notable gains in accuracy over baseline ML methods. The AL layer added value by generating strong features, contributing up to 40% of the total gain. NNs benefitted the most from the AL layer, showcasing the importance of the high-dimensional trainable layer (MB). The MothNet architecture, with its competitive inhibition layer, significantly improved ML accuracy by up to 40%. The automated feature generator based on a simple BNN enhanced learning abilities on vMNIST and vOmniglot datasets. MothNet features outperformed standard methods like PCA, PLS, NNs, and pre-training, making class-relevant information more accessible. The competitive inhibition layer created attractor basins for inputs, enhancing classification by increasing the effective distance between samples of different classes. The sparse connectivity from AL to MB has computational benefits, resembling sparse autoencoders but with key differences. The MB requires few samples to improve classification and lacks recurrent connections. Hebbian weight updates in the MB are distinct from backpropagation, operating on a \"use it or lose it\" basis. The Hebbian update mechanism in the MB is different from backpropagation, operating on a \"use it or lose it\" basis without an objective function or output-based loss. The dissimilarity of optimizers (MothNet vs ML) may have increased total encoded information."
}