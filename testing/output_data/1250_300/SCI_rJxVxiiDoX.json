{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. Combining quantization-aware training with weight matrix factorization significantly reduces model size and computation for small-footprint keyword spotting while maintaining performance. Quantization-aware training is used to optimize weights against quantization errors in building a small-footprint low-power keyword spotting system. Dynamic quantization approach is employed for DNN weight matrices, with inputs quantized row-wise on the fly. This method successfully trains 8 bit and 4 bit quantized KWS models. The study utilizes quantization-aware training to optimize weights for a low-power keyword spotting system. Inputs are quantized row-wise during training, with a focus on improving model performance through quantization. The experiments are conducted using a 500-hour far-field speech corpus and evaluate models using DET curves and AUC. Training involves a 3-stage process with a small ASR DNN pre-trained initially. The performance of quantized models is shown to improve with quantization-aware training. In another 20 epochs, the performance of 'naively quantized' models is shown in TAB0. AUC improvement of quantized models' performance using quantization-aware training is demonstrated. DET curves for full-precision, quantized, and quantization-aware trained 50k model are compared. The DET curves for 16 bit and 8 bit quantized-models are not significantly different from the full-precision model."
}