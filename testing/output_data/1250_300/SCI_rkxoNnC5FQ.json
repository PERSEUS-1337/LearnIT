{
    "title": "rkxoNnC5FQ",
    "content": "Deep Learning for Computer Vision relies on supervision sources. A new unsupervised domain adaptation algorithm, SPIGAN, uses Simulator Privileged Information (PI) and Generative Adversarial Networks (GAN). The approach improves semantic segmentation by training on real-world and synthetic data with z-buffer PI. This method outperforms other unsupervised domain adaptation techniques, addressing the challenge of learning with minimal human supervision in Machine Learning and Computer Vision. Training in simulation has shown improvements in various computer vision tasks, but domain gaps between synthetic and real data remain a challenge. Unsupervised domain adaptation algorithms leverage Generative Adversarial Networks for pixel-level adaptation, using simulators as black-box generators. Simulators possess Privileged Information about the world and scene formation, enhancing training with physical properties. In this paper, a novel adversarial learning algorithm called SPIGAN is proposed to leverage simulator Privileged Information (PI) for GAN-based unsupervised learning. The algorithm involves training four networks: a generator G, a discriminator D, a task network T, and a privileged network P. The privileged network P is trained on both synthetic and adapted images to predict their associated privileged information z, acting as an auxiliary task and regularizer. The SPIGAN algorithm leverages privileged information (PI) from a simulator using a privileged network P trained on synthetic and adapted images. This network acts as an auxiliary task and regularizer for the task network T. The approach is evaluated on semantic segmentation in urban scenes using real-world and simulator datasets. SPIGAN successfully learns a semantic segmentation network without real-world labels, bridging the sim-to-real gap and outperforming other unsupervised domain adaptation methods. The paper introduces SPIGAN, an unsupervised domain adaptation algorithm that learns a semantic segmentation network without real-world labels, bridging the sim-to-real gap. It outperforms other related methods and focuses on overcoming the domain gap between source and target distributions in deep learning. The Domain Adversarial Neural Network (DANN) is a popular approach for unsupervised domain adaptation, overcoming differences between source and target distributions. Recent advancements include Curriculum Domain Adaptation for semantic segmentation and adversarial domain adaptation based on GANs, such as CycleGAN for image-to-image translation. CycleGAN is a generative model for image translation between multiple domains, using cycle consistency with forward and backward GANs. Variational Auto-Encoders (VAEs) are an alternative to GANs for image translation. Several works propose GAN-based unsupervised domain adaptation methods to bridge the gap between synthetic and real-world images. SimGAN utilizes simulation to generate annotated datasets and improve the realism of synthetic images. Sadat leverages synthetic data by treating foreground and background differently. The SPIGAN learning algorithm involves training four networks jointly on real-world and synthetic images with labels and privileged information. The SPIGAN learning algorithm involves training four networks jointly on real-world and synthetic images with labels and privileged information. Recent methods focus on simple tasks and visual conditions that are easy to simulate, while BID21 is the first to study semantic segmentation as the task network in adversarial training. The BID studies focus on domain adaptation for semantic segmentation tasks using various approaches such as curriculum learning, leveraging GAN framework, target guided distillation, and adversarial learning. They propose methods to reduce domain gap and improve segmentation performance. Additionally, the use of Privileged Information from a simulator is highlighted as a novel approach to enhance learning objectives in urban scene segmentation. Our approach significantly improves semantic segmentation of urban scenes by augmenting the learning objective with auxiliary privileged tasks, especially in the presence of a large sim-to-real domain gap. Inspired by Learning Using Privileged Information (LUPI), we leverage additional data only available at training time for unsupervised domain adaptation from a simulator. Several works have used privileged information for domain adaptation, such as leveraging RGBD information for object detection and modality distillation for action recognition. Our goal is to design a procedure for unsupervised domain adaptation from a synthetic domain to a real domain by leveraging privileged information from simulators. This involves learning a model for perception tasks using raw sensory data without ground truth data from the target domain. The source domain has labeled synthetic images and Privileged Information (PI) from simulators, while the target domain has unlabeled images. This approach offers full control over the environment for better adaptation in semantic segmentation tasks. The goal is to adapt from a synthetic to a real domain using privileged information from simulators. The simulator provides control over the environment and raw sensory data for training without target supervision. The approach involves leveraging the simulator's privileged information within a GAN framework called SPIGAN. SPIGAN is a GAN framework that leverages privileged information (PI) to adapt from a synthetic to a real domain. The approach involves a generator G transforming images from a source domain to an adapted domain, aiming to maximize the accuracy of the task predictor T during testing. The discriminator D distinguishes between adapted and real images, while the privileged network P predicts the PI z. The target task network T is trained on synthetic images and adapted ones to predict labels, assuming label preservation by the generator. The SPIGAN framework utilizes privileged information (PI) to adapt synthetic images to a real domain. The generator G aims to preserve labels during transformation, while the privileged network P predicts the PI z. The task predictor T is trained to perform a perception task in the target domain, with joint learning objectives and specific loss functions to optimize model training. The SPIGAN framework uses privileged information (PI) to adapt synthetic images to a real domain. The generator aims to preserve labels during transformation, while the privileged network predicts the PI. The task predictor is trained to perform a perception task in the target domain, with specific loss functions for optimization. The minimax objective includes weights for adversarial loss, task prediction loss, PI regularization, and perceptual regularization. The adversarial loss is least-squares based, stabilizing training and improving image results. The task prediction loss is optimized over synthetic images and their adapted versions, assuming label-preservation by the generator. Different tasks may require different loss functions, such as the standard cross-entropy loss for semantic segmentation. The generator in the SPIGAN framework learns through joint estimation of \u03b8 P for scene properties captured by privileged information (PI). Different tasks require different loss functions, such as cross-entropy loss for semantic segmentation. The total combined loss includes PI regularization and perceptual regularization to maintain source image semantics in generated images. Adversarial training is used to optimize the joint learning objective. In the SPIGAN framework, optimization involves adversarial training to jointly learn parameters for the discriminator, generator, privileged network, and task network. The method is evaluated on semantic segmentation using the SYNTHIA dataset for unsupervised domain adaptation in challenging real-world domains. The SYNTHIA-RAND-CITYSCAPES dataset provides pixel-wise segmentation and depth labels for urban scene images under various weather and illumination conditions. It is used for domain adaptation to Cityscapes and Mapillary Vistas datasets, with Cityscapes containing images from European urban streets and Mapillary Vistas offering a wider variety of scenes. Training and evaluation are conducted without using labels from real-world domains, and adaptation from SYNTHIA to Cityscapes is evaluated on 16 classes. In the experiment, adaptation from SYNTHIA to Cityscapes is evaluated on 16 classes using standard evaluation protocols. The model architectures are adapted from CycleGAN and BID23, with a single sim-to-real generator and a PatchGAN discriminator. The task predictor and privileged network use the FCN8s architecture, and the perceptual loss follows the implementation in BID3. The perceptual loss L perc is implemented using a pre-trained VGG19 network with specific layers. Hyper-parameters are set through grid search on a validation set different from the target set. Joint adversarial loss weights are determined for GAN, task, privileged, and perceptual objectives. The most important factors are the GAN and task losses for improving generalization performance across domain gaps. The objective of the study is to improve the generalization performance of the task network by balancing GAN and task losses. Regularization terms stabilize training and constrain adaptation. A critical hyper-parameter for unsupervised learning is the stopping criterion based on discriminator and generator loss comparison. Evaluation is done at two resolutions: 320 \u00d7 640 and 512 \u00d7 1024. Images are resized during training and evaluation. The study aims to enhance the task network's generalization by balancing GAN and task losses. Evaluation is conducted at resolutions of 320 \u00d7 640 and 512 \u00d7 1024, with images resized during training and evaluation. The SPIGAN algorithm is evaluated for adapting a semantic segmentation network from SYNTHIA to Cityscapes. SPIGAN algorithm is evaluated for adapting a semantic segmentation network from SYNTHIA to Cityscapes. Results show SPIGAN achieves state-of-the-art semantic segmentation adaptation in terms of mean IoU on Cityscapes validation set. The use of PI helps estimate layout-related classes like road and sidewalk, and object-related classes like person and rider. SPIGAN achieves state-of-the-art semantic segmentation adaptation with a 3% improvement in mean IoU using PI for layout and object classes. The regularization from P decreases artifacts, confirming the effectiveness of leveraging synthetic data and PI. Further experiments compare SPIGAN with PI, SPIGAN without PI, and SPIGAN-base, showing improved generalization performance across domains. SPIGAN achieves state-of-the-art semantic segmentation adaptation with a 3% improvement in mean IoU using PI for layout and object classes. The regularization from P decreases artifacts, confirming the effectiveness of leveraging synthetic data and PI. Results on the Vistas dataset show SPIGAN with perceptual regularization outperforms SPIGAN-base in both datasets, indicating improved performance across all categories. Perceptual regularization improves performance in all categories, stabilizing adaptation during training. SPIGAN shows significant improvements in various categories, including \"nature\", \"construction\", \"vehicle\", and \"human\". Qualitative results from SYNTHIA to Cityscapes are provided in Figures 5 and 7. On the Vistas dataset, SPIGAN reduces the domain gap by 4.3% mean IoU, with PI playing a crucial role in enhancing generalization performance. Using PI is crucial for improving generalization performance, as SPIGAN-no-PI shows negative transfer with a \u221213% decrease in performance compared to the FCN source. Cityscapes and Vistas datasets differ in visual diversity, with Cityscapes being more visually uniform. This makes Cityscapes more suitable for image translation methods like SPIGAN-no-PI, which mainly adapts at color and texture levels. A larger domain gap increases the risk of negative transfer, as seen in the quantitative measurements in Table 2 and qualitative confirmation in Figure 6. Figure 6 illustrates that adapting SYNTHIA images to Vistas results in more artifacts, leading to a larger domain gap and increased risk of negative transfer. SPIGAN also shows artifacts but to a lesser extent, with improvements seen when depth consistency is maintained. Performance improvements with PI suggest useful constraints for better training and reduced domain shift. Adaptation methods show similar performance trends, with the \"vehicle\" category showing the most improvement across datasets. The \"human\" category did not show the same level of improvement. SPIGAN is a novel method for unsupervised domain adaptation of deep networks using synthetic data and Privileged Information (PI). It addresses large domain gaps between synthetic and real-world data, particularly in tasks like semantic segmentation of urban scenes. Future work includes exploring SPIGAN for additional tasks with different types of PI from simulation."
}