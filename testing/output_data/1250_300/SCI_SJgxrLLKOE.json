{
    "title": "SJgxrLLKOE",
    "content": "Engineered proteins offer the potential to solve problems in biomedicine, energy, and materials science. Creating successful designs is challenging due to the complex relationship between protein sequence and 3D structure, known as the inverse protein folding problem. Generative models for protein sequences conditioned on a graph-structured design target can efficiently capture complex dependencies. This approach improves upon prior models and advances rapid biomolecular design using deep generative models. Computational protein design aims to automate the creation of proteins with specific properties, with significant progress made in the past two decades. The field of protein design has made significant progress in the past two decades, including the design of novel 3D folds, enzymes, and complexes. Current practices often involve multiple rounds of trial-and-error, with challenges stemming from the bottom-up nature of approaches. An alternative top-down framework for protein design involves learning a conditional generative model for protein sequences based on a target structure represented as a graph. This approach combines autoregressive self-attention with graph-based descriptions of 3D structure to capture higher-order dependencies between sequence and structure effectively. Our model utilizes graph-based descriptions of 3D structure to capture higher-order dependencies between sequence and structure effectively. By leveraging sparse and localized graph and self-attention in 3D space, we achieve computational efficiency and linear scaling in sequence length. Additionally, the graph structured inputs offer representational flexibility for both coarse and fine-grained descriptions of structure. Our model utilizes graph-based descriptions of 3D structure to capture higher-order dependencies between sequence and structure effectively. It achieves computational efficiency and linear scaling in sequence length by leveraging sparse and localized graph and self-attention in 3D space. The model offers representational flexibility for both coarse and fine-grained descriptions of structure, demonstrating improved generalization performance over recent deep models of protein sequence given structure. Our model captures the joint distribution of the full protein sequence while grounding dependencies in long-range interactions from the structure. There has been work on deep generative models for protein sequences in individual families, but recent advancements in unconditional protein language models have shown promise in transferring well to supervised tasks. Conditional generative modeling facilitates adaptation to specific parts of structure space. Protein language models trained on evolutionary sequences are limited by the smaller number of 3D folds they design. Evaluating these models with structure-based splitting of sequence data reveals challenges in assigning likelihoods to out-of-training fold sequences. Recent deep models for protein structure can aid in crafting 3D structures for sequence design. For classical protein design approaches, joint modeling of structure and sequence is recommended. Non-parametric methods have also been proposed for decomposing target designs in protein design. In computational protein design, joint modeling of structure and sequence is recommended. A non-parametric approach decomposes target designs into substructural motifs. A model extends the Transformer to capture sparse relational information between sequence elements, avoiding high memory costs. The approach is similar to Graph Attention Networks but includes edge features. Our model utilizes graph-structured self-attention for protein structure representation, incorporating edge features and an autoregressive decoder. The graph representation of protein structure includes node and edge features, accommodating variations in macromolecular design problems. The desired graph representation of coordinates should be invariant to rotations and translations, while also containing locally informative edge features for reconstructing adjacent coordinates. The text discusses the development of invariant and locally informative features for graph neural networks in protein structure representation. It focuses on augmenting points with orientations to define a local coordinate system and deriving spatial edge features for reconstructing adjacent coordinates. The text discusses defining a local coordinate system for protein structures and deriving spatial edge features for reconstructing adjacent coordinates. It involves negative bisectors of angles, unit vectors, rigid body transformations, distance, direction, orientation, and positional embeddings to encode local structure around nodes. Incorporating positional encodings based on the original Transformer model, the text describes obtaining edge encoding vectors by concatenating structural and positional encodings. Node features include dihedral angles of the protein backbone and flexible backbone descriptions based on binary edge features. Only edges in the k-nearest neighbors graph are included, with k = 30 for all experiments. The text introduces a Structured Transformer model that incorporates positional encodings and relational information efficiently by restricting attention to k-nearest neighbors in 3D space. This model combines relative positional encodings with binary edge features like contacts and hydrogen bonds to represent 3D backbone configurations. Unlike the standard Transformer, edge features are included to embed spatial and positional information. Our model extends the Transformer to spatially structured settings by incorporating edge features for positional dependencies. The autoregressive decomposition of the joint distribution is parameterized using an encoder-decoder architecture, where the encoder computes node embeddings from structure-based features and the decoder predicts the next amino acid in the sequence. The encoder utilizes multi-head self-attention components to derive global context estimates for each node. The encoder module in our model utilizes multi-head self-attention components to derive global context estimates for each node in the sequence. The encoder implements a series of self-attention layers and position-wise feedforward layers, stacked on top of each other to refine the embeddings as we traverse the layers. The topmost layer of the encoder produces the final embeddings as its output. The decoder module follows a similar structure to the encoder but with augmented relational information for predicting the next amino acid in the sequence. The encoder module uses self-attention layers to refine embeddings, while the decoder module predicts the next amino acid using relational information. Three layers of self-attention and feedforward modules are stacked with a hidden dimension of 128. The study utilized a dataset based on the CATH hierarchical classification of protein structure to evaluate model generalization. The dataset was split into training, validation, and test sets with a total of 18025 chains in the training set, 1637 chains in the validation set, and 1911 chains in the test set. Models were trained using a specific learning rate schedule, dropout rate, and early stopping based on validation perplexity. The study focused on evaluating model performance using protein sequences. Different perplexities were calculated for protein sequences under various models, with a particular emphasis on likelihood-based evaluations. Protein sequences from the Pfam database were also analyzed for their perplexity per letter. The study evaluated model performance using protein sequences, focusing on likelihood-based evaluations. Protein sequences from the Pfam database had an average perplexity per letter of \u223c11.6 in Pfam 32. There was a significant gap between unconditional language models and models conditioned on structure, with test perplexities of \u223c16-17 for structure-independent models. Protein language models trained on one subset of 3D folds generalized poorly to predict sequences of unseen folds, impacting protein engineering and design. Protein language models trained on specific 3D folds have poor generalization to predict sequences of unseen folds, affecting protein engineering and design. Structured Transformer model achieved a perplexity of \u223c7 on the full test set, with lower perplexities for all structure-based models. Local orientation information in protein structure graph features was found to be important. Comparison with SPIN2, a computationally intensive method, showed that it was trained on complete proteins and evaluated on subsets of the test set. The Structured Transformer model improved perplexities over SPIN2 TAB1 on subsets of the test set by incorporating 3D structural encodings for efficient computation. The model shows promise in designing and engineering protein sequences with deep generative models, emphasizing the importance of modeling sparse long-range dependencies in biological sequences."
}