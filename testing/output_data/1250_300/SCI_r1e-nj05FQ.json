{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, including bacteria, insects, and humans, despite individual incentives conflicting with the common good. This behavior is studied in intertemporal social dilemmas (ISDs) using multi-agent reinforcement learning and evolutionary theory. A modular architecture for deep reinforcement learning agents is introduced to learn cooperation biases in a model-free way, showing promising results in challenging environments. The study relates these findings to cultural and ecological evolution, highlighting the prevalence of cooperation in nature across different scales. In challenging environments, cooperation is prevalent in nature across different scales, despite individual selfish interests. Altruism can be favored by selection when individuals interact with other cooperators. The emergence of cooperation among self-interested agents is a key topic in multi-agent deep reinforcement learning, where social dilemmas involve a trade-off between collective welfare and individual utility. The problem domain is formalized as an intertemporal social dilemma (ISD) by MARL and BID25, extending matrix game social dilemmas to Markov settings. Evolutionary theory predicts that self-interested reinforcement-learning agents tend to converge to defecting strategies instead of achieving the collectively optimal outcome. The goal is to find multi-agent training regimes where cooperation emerges to resolve social dilemmas. Previous solutions fall into three categories: opponent modeling, long-term planning with perfect knowledge, and a specific intrinsic motivation function from behavioral economics. These hand-crafted approaches contrast with end-to-end model-free learning algorithms, which show better generalization abilities. It is proposed that evolution can eliminate the need for hand-crafted intrinsic motivation, similar to other applications in deep learning. Evolution can be applied to remove hand-crafted intrinsic motivation in deep learning algorithms, optimizing hyperparameters, implementing black-box optimization, evolving neuroarchitectures, regularization, loss functions, behavioral diversity, and reward functions. These principles are driven by single-agent search or competitive multi-agent tasks, with no guarantee of success in the ISD setting. Evolutionary simulations of predator-prey dynamics have used subpopulations to evolve populations of neurons for neural networks. The proposed system distinguishes between fast learning and slow evolution time-scales to address ISD challenges. The proposed system addresses the challenges of ISDs by distinguishing between fast learning and slow evolution time-scales. Evolution is used to bridge the gap between these two time-scales through an intrinsic reward function implemented as a neural network. The evolutionary dynamics are structured to achieve the goal of mitigating intertemporal dilemmas. To achieve altruistic behavior, an assortative matchmaking strategy is implemented where agents choose partners based on signals of cooperativeness. However, this approach has limitations and a new modular training scheme called shared reward network evolution is introduced. This scheme involves two neural network modules - a policy network and a reward network - to train agents on fast and slow timescales for reinforcement learning and evolution, respectively. Agents are composed of a policy network and a reward network. The policy network is trained using modified rewards specified by the reward network on a fast timescale. Evolution occurs on a slow timescale, with separate evolution of the policy and reward network modules. Each agent has a distinct policy network but shares the same reward network. The fitness for the policy network is individual reward, while the fitness for the reward network is the collective return for the group. This modular training approach prevents overfitting and suggests a mechanism for the evolutionary origin of social biases. Various parameters were explored, including environments and reward network features. In this study, the researchers explored different combinations of parameters in Markov games within a MARL setting, focusing on intertemporal social dilemmas. These dilemmas involve conflicts between individual selfish actions and group impacts over time, resembling social dilemmas like the Prisoner's Dilemma. Two dilemmas were considered, one involving collecting apples in a Cleanup game. In a partially observable Markov game on a 2D grid, two dilemmas were explored: the Cleanup game where agents collect apples affected by aquifer cleanliness, and the Harvest game where apple spawn rate depends on nearby apples. The dilemmas involve balancing individual rewards with long-term group benefits. In a model exploring dilemmas in a 2D grid game, the dilemma involves balancing individual rewards with long-term group benefits. The reward components for players include total, extrinsic, and intrinsic rewards, with extrinsic rewards obtained from the environment and intrinsic rewards calculated based on social preferences using a neural network. According to the formula, parameters of a 2-layer neural network with 2 hidden nodes are evolved based on fitness. The feature vector for each agent is transformed into intrinsic reward via their reward network, with features being a function of received or expected future rewards. Social preferences in Markov games should not be influenced by the precise temporal alignment of rewards, but rather on comparing temporally averaged reward estimates between players. In Markov games, social preferences should focus on comparing temporally averaged reward estimates between players. Two ways of aggregating rewards are considered: retrospective and prospective methods. The architecture includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The retrospective method judges intrinsic reward based on past rewards, while the prospective method considers future rewards. The study compares retrospective and prospective methods for aggregating rewards in Markov games. The training framework involves distributed asynchronous training with a population of 50 agents. Episode trajectories last 1000 steps and weights are updated using V-Trace. The set of weights evolved includes learning rate, entropy cost weight, and reward network weights. The training framework involves distributed asynchronous training with a population of 50 agents. Episode trajectories lasted 1000 steps and weights were updated using V-Trace. The set of weights evolved included learning rate, entropy cost weight, and reward network weights. Agents observed their last actions, intrinsic rewards, and extrinsic rewards as input to the LSTM in the agent's neural network. The objective function comprised the value function gradient, policy gradient, and entropy regularization, weighted according to hyperparameters baseline cost and entropy cost. Evolution was based on a fitness measure calculated as a moving average of total episode return. The training framework involves distributed asynchronous training with a population of 50 agents. Episode trajectories lasted 1000 steps and weights were updated using V-Trace. The evolution of weights included learning rate, entropy cost weight, and reward network weights. Agents observed their last actions, intrinsic rewards, and extrinsic rewards as input to the LSTM in the agent's neural network. Evolution was based on a fitness measure calculated as a moving average of total episode return, with matchmaking methods for cooperative play. The training framework involves distributed asynchronous training with a population of 50 agents. Cooperative metric-based matchmaking was used for individual reward networks or no reward networks. The reward network was separately evolved within its own population to allow for independent exploration of hyperparameters. Five policy networks were paired with the same reward network in a shared reward network setup. The training framework includes distributed asynchronous training with 50 agents using cooperative metric-based matchmaking. Five policy networks are paired with the same reward network in a shared reward network setup, where reward network parameters are evolved based on total episode return across the group of co-players. This approach focuses on evolving social features for cooperation rather than remapping environmental events, addressing the tension in Intrinsic Social Dilemmas (ISDs) by evolving a form of communication for social cooperation. In a social setting, shared reward networks are crucial for evolving cooperation among multiple agents. Using random matchmaking, individual reward network agents perform poorly compared to those using reward networks with retrospective social features. Assortative matchmaking shows little benefit in adding reward networks if players have separate networks evolved selfishly. Individual reward network agents perform no better than PBT on Cleanup and only moderately better at Harvest. Adding reward networks over social features is not beneficial if players have separate networks evolved selfishly. Shared reward network agents perform as well as assortative matchmaking and handcrafted inequity aversion intrinsic reward, even with random matchmaking. Agents do not necessarily need immediate access to honest signals of other agents' cooperativeness to resolve the dilemma; having the same intrinsic reward function evolved according to collective episode return is sufficient. The retrospective variant of reward network evolution outperforms the prospective variant, as it does not require agents to learn good value estimates before the reward networks become useful. Having no reward network leads to quick apple collection but less sustainable behavior, while reward networks promote more sustainable behavior. Equality is measured by the Gini coefficient over individual returns. The use of reward networks in the game leads to more sustainable behavior compared to having no reward network. The retrospective variant of reward networks results in lower equality, while the prospective variant leads to higher equality. Tagging behavior is more prevalent with prospective or individual reward networks compared to a retrospective shared reward network. The weights of the final retrospective shared reward networks suggest that different social preferences are needed to resolve each game's issues. The final weights in the second layer of the reward network suggest that different social preferences are required to resolve each game. In Cleanup, simpler reward functions sufficed, while Harvest needed a more complex one to prevent over-exploitation. The first layer weights tended to be arbitrary due to random matchmaking. Organisms develop internal drives based on primary or secondary goals, and intrinsic rewards based on other agents were examined. Intrinsic rewards based on features from other agents in the environment were examined to understand cooperation. Natural selection via genetic algorithms did not lead to cooperation, but assortative matchmaking did. A new evolutionary paradigm based on shared reward networks promotes cooperation by improving credit assignment and exposing social signals correlating with selfishness levels. The shared reward network evolution model promotes cooperation by exposing social signals correlating with selfishness levels. This model is inspired by multi-level selection and features modularity seen in nature, such as microorganisms forming multi-cellular structures and prokaryotes incorporating plasmids. Microorganisms form multi-cellular structures to solve adaptive problems, while prokaryotes can incorporate plasmids for cooperation. In humans, a reward network represents a shared cultural norm based on accumulated cultural information. Future research could explore alternative evolutionary mechanisms like kin selection and reciprocity in understanding the emergence of cooperation. The text discusses the evolutionary origins of social biases and suggests studying assortative matchmaking models. It also explores combining an evolutionary approach with multi-agent communication for cooperative behaviors in games with specific actions and rewards. Training involved joint optimization of network parameters and evolution of hyperparameters in a standard setup. The Cleanup game involved joint optimization of network parameters and hyperparameters through evolution in a PBT setup. Gradient updates were applied for every trajectory with a maximum length of 100 steps using RMSProp optimization. PBT utilized genetic algorithms to search over hyperparameters, resulting in an adaptive schedule and joint optimization with network parameters learned through gradient descent. During evolution in a PBT setup for the Cleanup game, network parameters were optimized through gradient descent. A mutation rate of 0.1 was used for evolving hyperparameters, with perturbations for entropy cost, learning rate, and reward network parameters. A burn-in period of 4 \u00d7 10^6 agent steps was implemented to ensure accurate fitness assessment before evolution."
}