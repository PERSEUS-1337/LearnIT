{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks have become the standard in image and object classification, continuously advancing progress in 2D and 3D classification. Network regularization techniques focus on convolutional layer operations, but Wavelet Pooling offers an alternative to traditional pooling methods, reducing feature dimensions and addressing overfitting. Experimental results show Wavelet Pooling outperforms or performs comparably to other pooling methods like max, mean, mixed, and stochastic pooling. Convolutional Neural Networks (CNNs) are the standard in image and object classification, consistently achieving higher accuracy rates than vector-based deep learning techniques. Researchers constantly upgrade CNN components like the convolutional and pooling layers to improve accuracy and efficiency. Pooling, which reduces spatial dimensions of data, originated from predecessors like Neocognitron and Cresceptron. Max pooling is the most popular method, followed by average pooling. Pooling operations in Convolutional Neural Networks aim to reduce spatial dimensions of the data, leading to benefits such as parameter reduction, increased computational efficiency, and regulation of overfitting. Popular methods include max pooling and average pooling, but these deterministic approaches have limitations that hinder optimal network learning. Alternative methods like mixed pooling and stochastic pooling use probabilistic approaches to address these issues. However, all pooling operations utilize a neighborhood approach to subsampling, similar to nearest neighbor interpolation in image processing. To minimize data discontinuities and improve network regularization, a wavelet pooling algorithm is proposed, utilizing second-level wavelet decomposition for feature subsampling. This approach avoids the artifacts introduced by traditional interpolation methods. The proposed wavelet pooling algorithm aims to improve network regularization by using second-level wavelet decomposition for feature subsampling. It avoids artifacts introduced by traditional interpolation methods and is compared to other pooling methods for classification accuracy on various benchmark datasets. The paper is organized into sections discussing background, proposed methods, experimental results, and a summary. The paper discusses the wavelet pooling algorithm for network regularization, comparing it to traditional pooling methods for classification accuracy. Pooling involves condensing the output of convolutional layers by summarizing regions into one neuron value using max or average pooling. Max pooling selects the maximum value of a region, while average pooling calculates the average value. Both methods have their advantages and disadvantages. The paper discusses wavelet pooling as a method for network regularization, comparing it to traditional max and average pooling. Max pooling can erase details from an image, while average pooling can dilute pertinent details. To address these issues, researchers have developed mixed pooling, which combines max and average pooling methods randomly during training. Mixed pooling is a method applied in three different ways: for all features within a layer, mixed between features within a layer, or mixed between regions for different features within a layer. It involves a random value \u03bb (0 or 1) for max or average pooling. Stochastic pooling, a probabilistic method, improves upon max pooling by sampling from neighborhood regions based on activation probabilities. The pooled activation is sampled from a multinomial distribution based on these probabilities. The proposed wavelet pooling method reduces feature map dimensions using wavelets to minimize artifacts and improve image classification by discarding first-order subbands. This approach aims to capture data compression more organically, reducing jagged edges and other artifacts that may hinder correct image classification. The proposed wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT) to capture data compression more organically. This method reconstructs image features using only the 2nd order wavelet subbands, reducing artifacts and improving image classification. The proposed wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT) to capture data compression more organically. This method reconstructs image features using only the 2nd order wavelet subbands, reducing artifacts and improving image classification. The algorithm for forward propagation involves pooling image features by a factor of 2 using the inverse FWT (IFWT) based on the inverse DWT (IDWT). Backpropagation reverses this process by decomposing the image feature with 1st order wavelet decomposition, upsampling the detail coefficient subbands, and reconstructing the image feature for further backpropagation using the IDWT. All CNN experiments use MatConvNet and stochastic gradient descent for training. The proposed method utilizes Haar wavelet basis for 2nd order wavelet decomposition in image feature reconstruction. Experiments are conducted on a 64-bit system with Intel Core i7-6800k CPU and GeForce Titan X Pascal GPUs. Different regularization techniques are tested on CIFAR-10 and SHVN datasets. All pooling methods use a 2x2 window for comparison. The proposed method outperforms all other pooling methods on various datasets, including MNIST. Different pooling methods are compared using a 2x2 window for consistency. The network architecture is based on the MNIST structure with batch normalization. Results show that max pooling starts to overfit, while mixed and stochastic pooling have a stable trajectory. Average and wavelet pooling show smoother learning curves. Two sets of experiments are conducted, one without dropout layers and the other with dropout and batch normalization, with the latter showing improved performance over more epochs. The network structure for CIFAR-10 experiments includes two sets of experiments with pooling methods. The first set has no dropout layers, while the second set includes dropout and batch normalization. Results show that the proposed method ranks second in accuracy, with wavelet pooling resisting overfitting. The method shows a slower learning rate and prevents overfitting compared to max pooling. Mixed and stochastic pooling maintain consistent learning progression. The experiments involved two sets with different pooling methods, one without dropout and the other with dropout. The network structure for the SHVN experiments is shown in FIG0. The method proposed ranks second in accuracy, with wavelet pooling overfitting slightly. Max pooling and the proposed method show better stability. Mixed, stochastic, and average pooling maintain a steady learning progression. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. The dataset had errors which were fixed by mirroring missing or corrupted images and manually cropping them to match specific dimensions. Training and test data were separated with 3,900 images used for training. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. Images were fixed by mirroring and cropping to match specific dimensions. Training and test data were separated, with 3,900 images for training. The images were resized to 128x128 due to constraints. Different pooling methods were evaluated for stability and overfitting, with wavelet pooling showing promise. The proposed wavelet pooling method is presented as a proof-of-concept for potential improvements in computational efficiency. The construction and implementation of wavelet pooling are not efficient, presented as a proof-of-concept for potential improvements in computational efficiency. The accuracy results and novelty serve as a starting point for further enhancements, both from our research and other researchers. Efficiency is calculated based on mathematical operations for different pooling methods like max, average, mixed, and stochastic pooling. Wavelet pooling is the least computationally efficient method among different pooling techniques, using 54 to 213 times more mathematical operations than average pooling. Stochastic pooling is also inefficient, requiring about 3 times more operations than average pooling. Mixed pooling falls in between average and max pooling in terms of computational efficiency. Wavelet pooling is the least computationally efficient method, using 54 to 213 times more mathematical operations than average pooling. Despite this, with improvements in coding practices, GPUs, and an improved FTW algorithm, it can be a viable option. There are enhancements to the FTW algorithm that utilize multidimensional wavelets, lifting, parallelization, and other methods to improve efficiency. Our proposed method outperforms others in the MNIST dataset and performs well in the CIFAR-10 and KDEF datasets. Dropout and batch normalization show the method's response to network regularization. Our proposed wavelet pooling method performs well in CIFAR-10 and KDEF datasets, showing response to network regularization with dropout and batch normalization. Different pooling methods perform better depending on the dataset and network structure. Future work could explore varying wavelet basis and improving feature reduction outside of 2x2 scale. Improvements in FTW algorithm and computational efficiency could enhance performance. Analysis of structural similarity (SSIM) of wavelet pooling is also suggested for further research. Retaining subbands discarded during backpropagation may improve accuracy and reduce errors. Enhancing the FTW method could boost computational efficiency. Comparing the structural similarity (SSIM) of wavelet pooling with other methods can further validate the effectiveness of our approach."
}