{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention for handling scenarios where the target domain may have unseen classes. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC) to address this issue by incorporating domain-specific visual cues from target domain clusters. SE-CC utilizes clustering on unlabeled target samples to capture underlying data space structure, enhancing representation with mutual information maximization. Results on Office and VisDA datasets show superior performance in open-set and closed-set domain adaptation compared to state-of-the-art approaches. The text discusses the challenges of domain shift in machine learning and the use of unsupervised domain adaptation to generalize a target model. Existing models struggle with open-set scenarios where target samples belong to unknown classes. The text discusses the challenges of open-set domain adaptation in machine learning, focusing on distinguishing unknown target samples from known ones and learning a hybrid network for both closed-set and open-set domain adaptation. One approach is to use an additional binary classifier to assign known/unknown labels to target samples, but this may not fully exploit the data structure when target sample distributions are diverse or semantic labels are ambiguous. In open-set domain adaptation, the performance of binary classification may be suboptimal when target sample distributions are diverse or semantic labels are ambiguous. To address this, clustering is used to model the diverse semantics of known and unknown classes in the target domain. By steering domain adaptation with category-agnostic clusters, the representations become domain-invariant for known classes and discriminative for unknown and known classes in the target domain. Additionally, Self-Ensembling is enhanced with a clustering branch to refine representations and preserve the inherent structure of the target domain. In open-set domain adaptation, clustering is used to model diverse semantics of known and unknown classes in the target domain. Self-Ensembling with Category-agnostic Clusters (SE-CC) integrates an additional clustering branch to estimate cluster assignment distribution for each target sample, refining representations to preserve the target domain's inherent structure. The KL-divergence is minimized to enforce learned features to maintain the underlying data structure, while maximizing mutual information among input intermediate features. The SE-CC framework aims to enhance feature representation in unsupervised domain adaptation by minimizing KL-divergence and maximizing mutual information among input features, classification distribution, and cluster assignment distribution. Previous works have utilized methods like Maximum Mean Discrepancy and domain confusion to learn domain-invariant representations. In unsupervised domain adaptation, methods like domain confusion and adversarial training are used to encourage domain confusion and learn domain-invariant representations. Open-set domain adaptation extends traditional adaptation by handling scenarios with new and unknown classes in the target domain. Panareda Busto & Gall (2017) and Saito et al. (2018b) are early attempts at addressing the open-set scenario. In open-set domain adaptation, various methods are utilized to handle scenarios with new and unknown classes in the target domain. Busto et al. exploit target sample assignments as known/unknown classes, Saito et al. use adversarial training for feature separation, and Baktashmotlagh et al. factorize data into shared and private subspaces. Clustering is then applied to decompose unlabeled target samples into category-agnostic clusters for Self-Ensembling in closed-set and open-set scenarios. In open-set domain adaptation, methods like exploiting target sample assignments, adversarial training, and clustering are used to handle scenarios with new and unknown classes in the target domain. SE-CC utilizes clustering to decompose unlabeled target samples into category-agnostic clusters for Self-Ensembling in closed-set and open-set scenarios, enhancing feature representation by maximizing mutual information among feature map, classification, and cluster assignment distributions. SE-CC utilizes category-agnostic clusters for representation learning in open-set domain adaptation, preserving target data structure and aligning sample distributions. It maximizes mutual information among input feature, cluster, and class probability distributions to enhance representation learning. This approach integrates category-agnostic clusters into domain adaptation for both closed-set and open-set scenarios. In this paper, the SE-CC model integrates category-agnostic clusters into domain adaptation for open-set scenarios. The goal is to learn domain-invariant representations for recognizing known classes in the target domain and distinguishing unknown samples. The method builds upon Self-Ensembling and Mean Teacher for semi-supervised learning. The Self-Ensembling method builds upon the Mean Teacher for semi-supervised learning by encouraging consistent classification predictions between teacher and student models under small perturbations of input images. The self-ensembling loss penalizes the difference in classification predictions, with the teacher model's weights updated as an exponential moving average of the student's weights during training. The Self-Ensembling method enhances Mean Teacher for semi-supervised learning by aligning classification predictions between teacher and student models. It incorporates unsupervised conditional entropy loss to drive decision boundaries away from high-density regions in the target domain. The training loss includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data, balanced with tradeoff parameters. Open-set domain adaptation is challenging as it requires classifying outliers into known and unknown classes. In open-set domain adaptation, outliers need to be classified into known and unknown classes. To address the issue of oversimplification in assuming all unknown samples belong to one class, clustering is used to model diverse semantics as category-agnostic clusters. These clusters are integrated into Self-Ensembling to guide domain adaptation by aligning cluster distributions and enforcing domain-invariant feature representations for known classes. In open-set domain adaptation, clustering is utilized to create category-agnostic clusters that reveal the underlying structure of the target domain. K-means is used to decompose unlabeled target samples into clusters, which are then aligned to enforce domain-invariant feature representations for known classes. The clusters are refreshed periodically but do not significantly impact the results. Target samples are represented as output features of pre-trained CNNs for clustering. The clustering branch in the student model predicts the distribution over category-agnostic clusters for cluster assignment of target samples. Each target sample's inherent cluster distribution is measured through cosine similarities with cluster centroids, with the centroid defined as the average of all samples in the cluster. The clustering process aims to reveal the underlying structure of the target domain for domain-invariant feature representations. The clustering branch predicts cluster assignments for target samples based on input features. It uses a modified softmax layer to infer cluster assignment distribution and is trained with KL-divergence loss to preserve data structure and improve discriminative representation for known and unknown classes. The KL-divergence loss is defined to preserve the data structure of the target domain and enhance discriminative representation for known and unknown classes. Inter-cluster relationships are incorporated as a constraint to maintain the similarity between semantically similar clusters. Mutual Information Maximization is utilized to strengthen the learned target feature in an unsupervised manner. To further enhance the learned target feature in an unsupervised way, Mutual Information Maximization (MIM) is used in the student model to maximize mutual information between input features and output distributions. A MIM module is designed to estimate and maximize local and global mutual information among input feature maps, output classification distributions, and cluster assignments. Global Mutual Information is encoded from the output feature map of the student model and concatenated with conditioning classification distribution and cluster assignment. The global Mutual Information discriminator is used to align the global feature vector with classification and cluster assignment distributions. It consists of three fully-connected networks with nonlinear activation. The Mutual Information is estimated using the Jensen-Shannon MI estimator. Additionally, local Mutual Information is exploited among the local input features at each spatial location. The local Mutual Information is utilized to analyze the relationship between local input features and output classification and cluster assignment distributions. This involves replicating and concatenating distributions with input features before feeding them into a discriminator for classification. The discriminator consists of stacked convolutional layers with nonlinear activation, producing a score map indicating the probability of matching input features with the given distributions. The final output score map of the local Mutual Information discriminator indicates the probability of discriminating real input local features with matched distributions. The objective of the MIM module combines local and global Mutual Information estimations with tradeoff parameters. The training objective of SE-CC includes cross-entropy loss on source data, self-ensembling loss, conditional entropy loss, KL-divergence loss, and Mutual Information estimation on target data. Experimental verification was conducted on the Office Saenko et al. VisDA dataset for synthetic-real image transfer. The SE-CC method is empirically verified through experiments on the Office Saenko et al. VisDA dataset, a large-scale dataset for synthetic-real image transfer. The dataset consists of 280k images from three domains: training with synthetic images, validation with real images from COCO, and testing with video frames from YTBB Real. The evaluation is done using source and target domains, with known and unknown classes defined. Three metrics - Knwn, Mean, and Overall - are used for evaluation, with a strict known-to-unknown ratio of 1:10 in the target domain. In the context of evaluating the SE-CC method on the Office dataset for open-set adaptation, different models' performances are compared. AODA and a variant of SE-CC without unknown source samples are included for fair comparison. The SE-CC \u2666 variant learns a classifier that recognizes only the N-1 known classes, labeling target samples as unknown if the predicted probability is below a certain threshold. ResNet152 is used as the backbone for CNNs in both closed-set and open-set scenarios. The SE-CC classifier in open-set domain adaptation recognizes N-1 known classes and labels target samples as unknown if the predicted probability is below a threshold. Results show SE-CC outperforms other models on most transfer directions, especially on harder transfers like D \u2192 A and W \u2192 A. The design exploits target data structure for domain-invariant feature representation. The SE-CC classifier in open-set domain adaptation outperforms other models by recognizing N-1 known classes and labeling target samples as unknown if the predicted probability is below a threshold. By aligning data distributions between source and target domains, RTN and RevGrad show better performance than Source-only. Open-set adaptation techniques (AODA, ATI-\u03bb, and FRODA) outperform RTN and RevGrad by rejecting unknown target samples as outliers and aligning data distributions only for inliers. SE-CC steers domain adaptation by injecting the distribution of category-agnostic clusters as a constraint for feature learning and alignment, showing superior performance in both open-set and closed-set scenarios. In closed-set domain adaptation, SE-CC demonstrates better performance than other state-of-the-art techniques on Office and VisDA datasets. The results highlight the advantage of utilizing category-agnostic clusters to exploit the underlying data structure in the target domain. Ablation study reveals how Conditional Entropy and KL-divergence Loss contribute to improving overall performance by driving classifier decision boundaries and refining features. The SE-CC model utilizes KL-divergence Loss and Mutual Information Maximization to refine features and enhance classifier performance in open-set domain adaptation. Results show a significant 4.2% improvement in Mean accuracy, demonstrating the effectiveness of these design choices. The SE-CC model, consisting of KL and MIM designs, achieves a 4.2% performance gain in Mean metric through exploiting category-agnostic clusters for domain adaptation. This approach separates unknown target samples from known ones and integrates category-agnostic clusters into Self-Ensembling to preserve target data structure. The SE-CC model utilizes category-agnostic clusters for domain adaptation, improving Mean metric performance by 4.2%. It enforces feature learning to preserve data structure in the target domain and enhances feature learning by exploiting mutual information among input features, classification outputs, and clustering branches. Experiments on Office and VisDA show performance improvements compared to state-of-the-art techniques. The implementation is done in PyTorch with SGD optimization, using a learning rate of 0.001 and a mini-batch size of 56. Training iterations are set to 300 and 25 epochs on Office and VisDA, respectively, with global feature dimension set as 128/1,024 in the backbone of AlexNet/ResNet. The SE-CC model utilizes category-agnostic clusters for domain adaptation, improving Mean metric performance by 4.2%. It enforces feature learning to preserve data structure in the target domain and enhances feature learning by exploiting mutual information among input features, classification outputs, and clustering branches. The settings of cluster number K, tradeoff parameters \u03bb 1 , \u03bb 2 , \u03bb 3 , \u03bb 4, and \u03b1 are detailed for open-set and closed-set adaptation tasks. Evaluation of the clustering branch shows that KL-divergence is a better measure than L 1 and L 2 distance. The SE-CC model evaluates the Mutual Information Maximization using different variants of the MIM module. Results show that KL-divergence is a better measure of mismatch than L1 and L2 distance. CLS, CLU, and CLS+CLU estimate mutual information between input features and classification or clustering outputs, with CLS+CLU showing the largest performance improvement by combining outputs from both branches. This demonstrates the benefit of exploiting mutual information among input features and outputs of classification and clustering tasks. The SE-CC model utilizes Mutual Information Maximization with different MIM module variants. It shows that combining outputs from both branches leads to a larger performance boost. By preserving the underlying target data structure, SE-CC separates unknown target samples from known ones, making them distinguishable."
}