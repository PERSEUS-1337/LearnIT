{
    "title": "H1cT3NTBM",
    "content": "In this paper, the use of neural networks for music information retrieval tasks is explored, focusing on improving convolutional neural networks (CNNs) on spectral audio features. Three aspects of CNN design are investigated: network depth, residual blocks with grouped convolution, and global time aggregation. The study, centered on singer classification and singing performance embedding, concludes that global time aggregation significantly enhances CNN performance. Additionally, a singing recording dataset is released for training and evaluation. The research aims to leverage recent advancements in deep learning to enhance the capabilities of neural networks for music analysis. Advancements in deep learning, specifically in convolutional neural networks (CNNs), are explored in this paper to improve the learning capability of deep neural networks when applied on time-frequency representations. ResNet and ResNeXt, two recent convolutional layer variants, enable deeper convolutional layers for audio and music analysis applications. Existing music information retrieval research typically uses vanilla convolutional layers with no more than 5 layers, but this paper investigates the use of ResNet, ResNeXt, and deeper architectures with more than 5 convolution layers. In this paper, deeper convolution layer architectures with more than 5 layers are proposed for music information retrieval using convolutional neural networks. The attention mechanism is introduced as a way to model temporal dependencies and relations in time-frequency representations. The paper explores a novel approach to model temporal dependencies and relations using the attention mechanism as a global aggregation operation along the time axis with learnable parameters. It investigates the effects of global aggregation using average, max, or attention mechanism in singer classification and singing performance embedding tasks. The goal is to predict singer identity and create an embedding space for singers with similar styles to be closer together. This approach can help identify singing style and characteristics through audio recordings analysis. The paper discusses the challenge of isolating the \"singer effect\" from the \"song effect\" in modeling singing styles. It emphasizes the need to learn a representation that highlights singer similarity while reducing the impact of song similarity, similar to face identification in computer vision. The paper discusses the challenge of isolating the \"singer effect\" from the \"song effect\" in modeling singing styles. It emphasizes the need to learn a representation that highlights singer similarity while reducing the impact of song similarity. This is achieved by learning an embedding space for singing voice audio recordings that places recordings of the same identity closer to each other and pushes those with different identities away, similar to face verification in computer vision. The paper discusses learning an embedding space for singing voice audio recordings to identify \"singing style\" or \"singing characteristics\" by clustering recordings of the same identity. It uses CNNs for feature extraction and a siamese architecture for embedding samples into fixed-length vectors. The paper introduces a model for embedding singing recordings into fixed-length vectors for efficient similarity comparison. A new dataset of \"balanced\" singing recordings is released for unbiased evaluation. The neural network construction blocks and experiment details are described in sections 2 and 3, respectively. The paper discusses the neural network construction blocks used in experiments with a 2013 dataset of monophonic vocal music performances. The network architecture involves feeding input time-frequency features as 2-D images into convolutional layers, followed by a global time-wise aggregation layer and dense layers. Different convolution layer variants, including vanilla convolution and residual network design with a bottleneck block, are utilized. The residual network design with the bottleneck block, extended by using the grouped convolutional block, is a variant used in experiments. The vanilla convolutional block, ResNet, and ResNeXt configurations are depicted in FIG0. The ResNeXt bottleneck block involves grouped convolutions, with a max pooling layer placed between convolutional blocks. The ResNet configuration is a special case under the ResNeXt setup. The neural network architecture used in this paper includes convolutional layers with max pooling, batch normalization, and global time-wise aggregation before feeding the output to dense layers. The 3-D feature map is reshaped into 2-D matrices for processing. The 3-D feature map from the convolutional layers is reshaped into 2-D matrices for processing. The feed-forward attention mechanism is used for prediction without sequence-to-sequence learning, allowing weighted access to information from input hidden sequences. The feed-forward attention layer uses non-linear function tanh with learnable parameters w and b for weighted aggregation over the time-axis. This operation is different from standard pooling as it reduces the dimension of the aggregation axis to 1. The network architecture includes convolutional and global aggregation parts for tasks like singer identity classification and singing performance embedding. The network architecture for singer identity classification and singing performance embedding is detailed in Appendix B. Singer classification provides clear evaluation criteria for hyperparameters and network architectures, while the embedding task explores spatial relationships between samples. Evaluation metrics and plots of embedded samples are provided for both tasks using the DAMP dataset with 34620 solo singing recordings by 3462 singers. The DAMP dataset used for singer identity classification contains 34620 solo singing recordings by 3462 singers. To address the dataset imbalance issue, a DAMP-balanced dataset with 24874 recordings by 5429 singers singing 14 songs was created. The last 4 songs serve as the test set, while the first 10 songs can be split into any 6/4 train/validation split where singers in each set sing the same 6/4 song collections. The DAMP dataset includes solo singing recordings by multiple singers. A balanced dataset was created with recordings by more singers and fewer songs. The dataset can be split into train/validation sets for different tasks. Time-frequency representations from raw audio are used as input for neural networks. The Mel-scaled magnitude spectrogram (Mel-spectrogram) is used as input for neural networks, while the constant-Q transformed spectrogram (CQT) is also considered but performs worse due to its inability to preserve harmonic relationships. The Mel-scaled magnitude spectrogram is used as input for neural networks, while the constant-Q transformed spectrogram (CQT) performs worse due to its inability to preserve harmonic relationships. The audio recordings are resampled to 22050Hz and transformed into Mel-spectrograms using FFT with specific parameters. The Mel-spectrograms are then chopped into overlapping matrices for analysis. The Mel-spectrogram of each singing performance audio recording is chopped into overlapping matrices with specific parameters for analysis. Gradient descent is optimized using ADAM with a learning rate of 0.0001 and a batch size of 32. L2 weight regularization is applied to all learnable weights. Hyperparameters are chosen using Bayesian optimization. Early stopping tests are applied every 50 epochs. Non-linear activation functions and specific filter sizes are used in the neural network architecture. The neural network architecture for singer classification uses rectified linear unit activation function, specific filter sizes, and dense layers with 1024 hidden units. A subset of 46 singers from the DAMP dataset is selected for the classification task, with 10-fold cross validation. Different neural network configurations, including vanilla CNN and ResNeXt building blocks, are explored along with various aggregation methods. A baseline SVM classifier is also included in the analysis. The study explores different neural network configurations for singer classification, including vanilla CNN and ResNeXt building blocks, with various aggregation methods. A baseline SVM classifier is also included. Experimental results show that neural network models outperform the baseline method, with global aggregation methods improving performance by 5% to 10%. For singer classification, neural network models with global aggregation methods improved performance by 5% to 10%. A siamese neural network architecture was used to create an embedding space that groups recordings by the same singer together and separates recordings by different singers. The embedding dimension chosen was 16, and pairs of samples from the dataset were labeled based on their identity. The siamese network optimizes the squared euclidean distance between embedded vectors with a contrastive loss function. Training involves randomly sampling pairs of samples from the same or different singers. Results show that feed-forward attention and average aggregation tend to overfit the data. Results from the ResNeXt configurations show that feed-forward attention and average aggregation tend to overfit the data more than max and no aggregation. Shallow architectures perform slightly better than deeper ones with similar parameters. Qualitative characteristics of the embeddings are illustrated in Figure 4, comparing shallow ResNeXt embeddings with handcrafted features for singer classification. The shallow ResNeXt architecture with/without feed-forward attention and handcrafted features were compared for singer classification. The handcrafted features captured the \"song\" effect, while the learned embeddings grouped performances by the same singers. t-SNE projections of 6-second clips before summarizing into songs were shown in FIG4. Leave-one-out k-nearest neighbor classifications using 16-dimensional performance vectors were used for quantitative assessment, with all 224 performances from 56 singers included. The classification accuracies for singer classification using k-nearest neighbor with different network configurations and handcrafted features are shown in Figure 5. The \"song effect\" is evident in the results, but singing performance embedding learning helps dilute this effect and extract more relevant features for characterizing singers. The feed-forward global aggregation enhances \"singer style\" while reducing the \"song effect\" slightly. The balanced nature of the dataset allows for k-nearest neighbor classification on performed songs. In this paper, recent developments in deep learning are explored to solve singer identification and embedding problems. Global aggregation over time significantly improves performance, with feed-forward attention accelerating the learning process. The feed-forward attention layer learns a \"frequency template\" for each convolutional channel, allowing focus on different parts along the frequency axis. In this paper, a \"frequency template\" for each convolutional channel is encoded in w, enabling focus on different parts along the frequency axis. Training deep neural networks with more than 15 convolutional layers on time-frequency input is feasible with global time aggregation. A dataset of over 20000 single singing voice recordings is released. Future work includes experimenting with replacing max-pooling with striding in convolutional layers and improving global-aggregation by considering temporal order. The proposed neural network configurations will be experimented in music information retrieval tasks such as music structure segmentation. The DAMP-balanced dataset, collected from the Sing! Karaoke app, has the same format of metadata as the original DAMP dataset. The querying process differs between the two datasets, with the original DAMP dataset randomly selecting 10 singing performances from 3462 users, while the DAMP-balanced dataset has specific constraints on song collections per user. The DAMP-balanced dataset from the Sing! Karaoke app has specific constraints on song collections per user. Queries retrieve audio recordings and metadata for a group of users who sang specific collections of songs. 14 popular songs are listed, with queries covering different combinations of splitting the songs into 6/4 collections. The dataset was split into training and validation sets based on the songs. The DAMP-balanced dataset from the Sing! Karaoke app has specific constraints on song collections per user. The dataset was split into training and validation sets based on different combinations of splitting songs into 6/4 collections. This specific split resulted in varying numbers of singers and performances for each set, allowing for balanced train/validation rotations within the first 10 songs."
}