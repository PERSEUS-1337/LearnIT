{
    "title": "BJJLHbb0-",
    "content": "Unsupervised anomaly detection on multi- or high-dimensional data is crucial for machine learning research and industrial applications. The Deep Autoencoding Gaussian Mixture Model (DAGMM) combines a deep autoencoder with a Gaussian Mixture Model for anomaly detection. DAGMM optimizes both models simultaneously, improving reconstruction and density estimation in the low-dimensional space. DAGMM optimizes the deep autoencoder and mixture model simultaneously, using a separate estimation network for parameter learning. It balances reconstruction, density estimation, and regularization to escape local optima and reduce errors without pre-training. Experimental results show DAGMM outperforms other anomaly detection techniques by up to 14% in F1 score. Anomaly detection is crucial in various fields, relying on density estimation to identify low probability anomalies in input samples. Conducting robust anomaly detection on high-dimensional data without human supervision remains a challenge. Estimation: Anomalies in high-dimensional data are challenging to detect without human supervision. Two-step approaches involving dimensionality reduction and density estimation are commonly used but may lead to suboptimal performance. Combining dimensionality reduction and density estimation is desirable, but computationally difficult. In this paper, a Deep Autoencoding Gaussian Mixture Model (DAGMM) is proposed as a deep learning framework to address the challenges of anomaly detection in high-dimensional data. The model combines dimensionality reduction and density estimation using deep autoencoders to improve performance. The Deep Autoencoding Gaussian Mixture Model (DAGMM) is a deep learning framework that tackles challenges in unsupervised anomaly detection by preserving key information in a low-dimensional space and utilizing a compression network for dimensionality reduction. Anomalies differ from normal samples in reduced dimensions and are harder to reconstruct, leading to improved performance compared to existing methods. DAGMM uses a compression network for dimensionality reduction and a Gaussian Mixture Model for density estimation in anomaly detection. It addresses challenges in joint optimization by utilizing an estimation network for mixture membership prediction. DAGMM utilizes an estimation network to predict mixture membership for each sample, enabling direct estimation of Gaussian Mixture Model parameters. By minimizing reconstruction error and sample energy simultaneously, DAGMM trains a dimensionality reduction component for density estimation. The model is friendly to end-to-end training, avoiding the need for pre-training and allowing for adjustments to dimensionality reduction behavior. Our empirical study shows that DAGMM is well-learned through end-to-end training, with regularization aiding the autoencoder in escaping local optima. Experiments on benchmark datasets reveal up to a 14% improvement in F1 score for anomaly detection. End-to-end trained DAGMM outperforms methods relying on pre-trained autoencoders, with low reconstruction error and superior performance. Tremendous effort has been devoted to unsupervised anomaly detection methods, including reconstruction based methods like Principal Component Analysis (PCA), kernel PCA, and Robust PCA. These methods analyze the reconstruction error induced by deep autoencoders but are limited by only considering anomaly analysis from a single aspect. DAGMM is a method that considers both compression differences in anomalous samples and the presence of anomalies with normal error levels. It performs density estimation in a reduced low-dimensional space, taking into account reconstruction errors caused by dimensionality reduction. Traditional clustering methods like multivariate Gaussian Models and k-means are limited by the curse of dimensionality in high-dimensional data. Recent works have proposed deep autoencoder based methods to jointly learn dimensionality reduction and clustering components for anomaly detection. However, the performance of these methods is limited by oversimplified clustering models and pre-trained dimensionality reduction components. DAGMM addresses limitations in handling clustering and density estimation tasks for complex data structures. It includes an estimation network to evaluate sample density in a low-dimensional space and predict sample mixture membership for GMM parameter estimation without EM-like procedures. DAGMM allows for end-to-end training to adjust dimensionality reduction components and improve clustering analysis/density estimation quality. Unlike one-class classification approaches, DAGMM estimates data density effectively even with higher dimensions. Unlike one-class SVM and other methods that suffer from the curse of dimensionality, DAGMM estimates data density in a jointly learned low-dimensional space for robust anomaly detection. It focuses on unsupervised settings and uses a deep autoencoder for non-linear dimensionality reduction to extract useful features. Deep Autoencoding Gaussian Mixture Model (DAGMM) focuses on unsupervised anomaly detection by using a deep autoencoder for non-linear dimensionality reduction. It combines induced reconstruction error and learned latent representation to estimate data density in a low-dimensional space. DAGMM consists of a compression network and an estimation network, working together to predict likelihood/energy in the framework of Gaussian. The compression network in DAGMM prepares low-dimensional representations from reduced space and reconstruction error features, feeding them to the estimation network. The estimation network predicts likelihood/energy using a Gaussian Mixture Model. The low-dimensional representations contain features from a deep autoencoder and reconstruction error. The compression network computes the representation of a sample x using parameters \u03b8 e and \u03b8 d, encoding and decoding functions, and reconstruction error features. The compression network in DAGMM generates low-dimensional representations and reconstruction error features for input samples. These representations are then used by the estimation network to perform density estimation with a Gaussian Mixture Model. The estimation network utilizes a multi-layer neural network to predict the mixture membership for each sample based on the low-dimensional representations and the number of mixture components. The estimation network in DAGMM predicts mixture membership for low-dimensional representations using a multi-layer network. Parameters for the Gaussian Mixture Model are estimated based on the membership predictions. Sample energy is inferred using the estimated parameters, and anomalies can be detected using a pre-chosen threshold. The objective function for DAGMM training includes a reconstruction error term from the deep autoencoder. The objective function for DAGMM training involves minimizing reconstruction error in the compression network, maximizing likelihood to observe input samples, and penalizing small values in covariance matrices to avoid singularity issues. The DAGMM model penalizes small values in covariance matrices to prevent singularity issues. Meta parameters \u03bb1 and \u03bb2 are set to 0.1 and 0.005 respectively for desirable results. The estimation network in DAGMM predicts sample membership, similar to latent variable inference in probabilistic graphical models. Neural variational inference can be applied to tackle difficult latent variable inference problems. The contribution of a sample's compressed representation to the energy function can be upper-bounded. The DAGMM model incorporates neural variational inference by using an estimation network to predict sample membership and parametrize a sample-dependent prior distribution. Minimizing the negative evidence lower bound tightens the energy function's bound, making the estimation network approximate the true posterior. Unlike neural variational inference, DAGMM's energy function is tractable and efficient to evaluate. The DAGMM model utilizes a deep estimation network to define a variational posterior distribution and a sample-dependent prior distribution. Unlike existing methods that rely on pre-training, DAGMM employs end-to-end training to improve anomaly detection performance. The compression network and estimation network in DAGMM can boost each other's performance through regularization and mutual learning. The compression network and estimation network in DAGMM mutually boost each other's performance. The regularization introduced by the estimation network helps the deep autoencoder in the compression network reduce reconstruction error. The well-learned low-dimensional representations from the compression network enable the estimation network to make meaningful density estimations. In Section 4.5, the effectiveness of DAGMM in unsupervised anomaly detection is demonstrated using public benchmark datasets like KDDCUP, Thyroid, Arrhythmia, and KDDCUP-Rev. The KDDCUP99 dataset from the UCI repository contains samples with 41 dimensions, where 34 are continuous and 7 are categorical. One-hot representation is used for categorical features, resulting in a dataset of 120 dimensions. In the Thyroid dataset, the hyperfunction class is treated as an anomaly class. For the Arrhythmia dataset, the smallest classes are combined to form the anomaly class. In the KDDCUP-Rev dataset, all samples labeled as \"normal\" are kept, and samples labeled as \"attack\" are randomly selected. The anomaly class in datasets like KDDCUP-Rev is formed by combining specific classes, while the normal class consists of the remaining classes. Various methods, including OC-SVM and DSEBM, are used for anomaly detection, with DSEBM leveraging sample energy to detect anomalies. Energy based model (DSEBM) is a deep learning method for unsupervised anomaly detection. DSEBM-e uses sample energy to detect anomalies, while DSEBM-r uses reconstruction error. Deep clustering network (DCN) is a clustering algorithm that uses k-means to regulate autoencoder performance for anomaly detection. DCN measures the distance between a sample and its cluster center to identify anomalies. GMM-EN is a variant of DAGMM that removes the reconstruction error component, allowing for membership estimation without constraints from the compression network. The DAGMM model includes variants such as PAE, E2E-AE, PAE-GMM-EM, and PAE-GMM, each with different training approaches and criteria for anomaly detection. The DAGMM model has variants like PAE, E2E-AE, PAE-GMM-EM, and PAE-GMM, each with unique training approaches and anomaly detection criteria. The variants use a two-step approach involving pre-training a deep autoencoder for compression and then training the estimation network. Sample energy is used as the criterion for anomaly detection in all variants. The DAGMM model has variants like PAE, E2E-AE, PAE-GMM-EM, and PAE-GMM, each with unique training approaches and anomaly detection criteria. The experiment considers two reconstruction features: relative Euclidean distance and cosine similarity. The network structures of DAGMM used on individual datasets are summarized, including KDDCUP and Thyroid datasets. The compression network for the datasets runs with specific layer configurations and activation functions. Training is done using the Adam algorithm with a set learning rate. Different datasets have varying training epochs and batch sizes. Regularization parameters \u03bb 1 and \u03bb 2 are set for all instances. Baseline methods undergo exhaustive parameter search for optimal performance. For anomaly detection performance evaluation, baseline methods undergo exhaustive parameter search to find optimal meta parameters. Metrics such as average precision, recall, and F1 score are used to compare performance, with DAGMM showing superior results over baselines in terms of F1 score. Table 2 shows the superior performance of DAGMM over baseline methods in terms of F1 score on various datasets, with significant improvements on KDDCUP and KDDCUP-Rev. DAGMM outperforms due to considering both latent representation and reconstruction error in energy modeling, unlike other methods limited by pre-trained deep autoencoders. In density estimation tasks, GMM-EN struggles without reconstruction constraints. PAE may not be sufficient for anomaly detection. E2E-AE performs poorly on KDDCUP and Thyroid due to lost key information during dimensionality reduction. DAGMM and DAGMM-NVI show similar performance. DAGMM outperforms baseline methods in F1 score on various datasets. In the second set of experiments, DAGMM's response to contaminated training data was investigated. Results from 20 runs on the KDDCUP dataset showed that contamination negatively impacted detection accuracy. DAGMM maintained good accuracy with 5% contaminated data, while OC-SVM was more sensitive to contamination ratio. Training with high-quality data is crucial for better detection accuracy. The DAGMM model achieved state-of-the-art accuracy on benchmark datasets, emphasizing the importance of training with clean data for better detection accuracy. A comparison with baselines showed DAGMM's ability to separate anomalous samples from normal ones in low-dimensional space. The DAGMM model outperformed other models in separating anomalous samples from normal ones in low-dimensional space. Pre-trained deep autoencoders tend to be stuck in good local optima for reconstruction, making them suboptimal for density estimation tasks. Reconstruction error in trained DAGMM was as low as in pre-trained deep autoencoders, showing difficulty in reducing error through end-to-end training. The experimental results show that DAGMM combines dimensionality reduction and density estimation effectively through end-to-end training. The compression network and estimation network enhance each other's performance, leading to better compression and robust density estimation. DAGMM offers a promising direction for anomaly detection by leveraging joint training to improve the quality of low-dimensional representations. In this paper, the Deep Autoencoding Gaussian Mixture Model (DAGMM) is proposed for unsupervised anomaly detection. DAGMM consists of a compression network and an estimation network, allowing for end-to-end training. The estimation network predicts sample mixture membership, aiding in Gaussian Mixture Modeling. End-to-end training is more beneficial for density estimation tasks compared to pre-training strategies. The DAGMM model, with a compression network and an estimation network, benefits from end-to-end training for density estimation tasks. It outperforms other techniques in unsupervised anomaly detection, showing up to a 14% improvement in F1 score on benchmark datasets. The optimal parameter \u03bd for OC-SVM is determined through exhaustive search to achieve the highest F1 score. In this study, a reasonable \u03bd is set for anomaly detection in testing phases. The optimal \u03bd values for different datasets are determined through exhaustive search. Reconstruction features are discussed, highlighting the use of deep autoencoders for dimension reduction in a network security dataset. In this study, reconstruction features are discussed, focusing on using deep autoencoders for dimension reduction in a network security dataset. Guidelines for reconstruction feature selection include ensuring the error metric used is continuous and differentiable, and the output falls within a specific range. In practice, reconstruction features are selected based on rules such as the error metric being continuous and differentiable, and the output falling within a small value range for ease of training in DAGMM. Cosine similarity and relative Euclidean distance are chosen as metrics meeting these criteria. The experiment includes a case study on the benefits of joint training in DAGMM over decoupled training. In a case study on joint training in DAGMM versus decoupled training, anomalies with low cosine similarity and high relative Euclidean distance are easily captured by both techniques. However, anomalies with medium relative Euclidean distance and high cosine similarity are more challenging for PAE-GMM to separate from normal samples. DAGMM assigns lower cosine similarity to such anomalies, making it easier to differentiate them from normal samples. The DAGMM model assigns lower cosine similarity to anomalies compared to PAE-GMM, making it easier to differentiate them from normal samples. The objective function of DAGMM includes components from deep autoencoder, estimation network, and penalty function for covariance matrices, with a coefficient ratio of 1:\u03bb1:\u03bb2. The ratio 1:0.1:0.005 consistently delivers expected results across all datasets in experiments. In our exploration, we find that adjusting the ratio of 1:0.1:0.005 consistently delivers expected results in anomaly detection accuracy across datasets. Varying the base from 1 to 9 with step 2 shows that DAGMM performs consistently, with \u03bb1 and \u03bb2 not being sensitive to changes in the base."
}