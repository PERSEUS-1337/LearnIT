{
    "title": "r1gFDS8aHB",
    "content": "Recent advances in deep learning have shown the usefulness of deep neural networks in extracting task-specific features. However, these features are limited in their applicability beyond the initial task. To address this, Variational Auto-Encoders (VAEs) have emerged as effective models for capturing latent variables in a generative sense. By using a modified version of joint-vae, we aim to learn disentangled features by combining continuous and discrete variables in the latent space. Feature learning is crucial in machine learning, with deep learning making significant advancements in this area. Deep neural networks excel at extracting features from raw data, but these features are often task-specific due to the use of specific loss functions. To achieve true artificial intelligence, representations need to be learned in a task-agnostic manner. Disentangled representations, where changes in one unit correspond to changes in a single factor, are being explored using JointVAE. In this work, the focus is on exploring disentangled representations using JointVAE for the dataset Gondal (2019). Various VAE variants have been developed to extract these representations, assuming Gaussian distribution for latent variables. However, in cases of discrete variables, direct representation as multinomial variables is necessary, as done in Joint-VAE Dupont (2018). Interpolation in input feature space and latent variable space is crucial. Joint-VAE Dupont (2018) uses a mix of continuous Gaussian and discrete multinomial latent variables for disentangled features representation. Sampling from continuous variables is done using the normal reparameterization trick, while Gumbel Softmax trick is used for discrete multinomial variables. This approach combines both continuous and discrete variables effectively."
}