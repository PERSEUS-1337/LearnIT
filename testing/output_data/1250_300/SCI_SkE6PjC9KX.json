{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. NPs efficiently fit observed data with linear complexity and can learn a wide family of conditional distributions. However, they suffer from underfitting issues. Attention is incorporated into NPs to improve prediction accuracy, training speed, and expand the range of functions that can be modeled. Regression tasks involve modeling the distribution of output given input using deterministic functions like neural networks. An alternative approach uses training data to compute a distribution over functions mapping inputs to outputs, allowing for reasoning about multiple functions consistent with the data. Non-parametric models like Gaussian Processes and Neural Processes offer efficient methods for modeling a distribution over regression functions. Neural Processes can predict the distribution of an arbitrary target output based on a set of context input-output pairs, with linear prediction complexity. Neural Processes (NPs) offer an efficient method for modeling a distribution over regression functions, with linear prediction complexity. They can predict the distribution of a target output based on a set of context input-output pairs. NPs have the flexibility to model data generated from a stochastic process, but they have a tendency to underfit the context set, leading to inaccurate predictions. The left half of FIG0 shows inaccurate predictive means and overestimated variances in the context set. The right half demonstrates underfitting when predicting the bottom half of a face image from the top half. The encoder aggregates context to a fixed-length latent summary, causing a bottleneck in the mean-aggregation step. Increasing dimensionality does not fully address this issue. Inspired by GPs, we aim to improve regression by considering similarity among data points. In Section 4, it is shown that addressing the issue of underfitting in regression models is not sufficient. Drawing inspiration from Gaussian Processes (GPs), a mechanism is implemented in Neural Processes (NPs) called Attentive Neural Processes (ANPs) using differentiable attention. ANPs improve context reconstruction and training speed compared to NPs, while also demonstrating enhanced expressiveness in modeling a wider range of functions. The Attentive Neural Processes (ANPs) improve context reconstruction and training speed compared to Neural Processes (NPs), demonstrating enhanced expressiveness in modeling a wider range of functions. The NP is a model for regression functions mapping input x to output y, defining conditional distributions for observed contexts and targets with permutation invariance. The likelihood is modelled by a Gaussian factorised across the targets. The Neural Processes (NPs) model regression functions with permutation invariance. Each context (x, y) pair is processed through an MLP to form a representation, aggregated by mean to form r. The likelihood is modelled by a Gaussian factorised across targets. The NP model includes a global latent variable z for uncertainty in predictions, modelled by a factorised Gaussian parametrised by s. The motivation for the global latent is to model different realisations of the data generating process. The Neural Processes (NPs) model regression functions with permutation invariance, using encoder q, r, s. The global latent variable z models different realisations of the data generating process. The model reconstructs targets with a KL term regularisation, ensuring contexts and targets are not too far apart. Parameters are learned via ELBO maximisation for a random subset of contexts and targets. Neural Processes (NPs) model regression functions with permutation invariance, allowing for scalability and flexibility in learning a wide family of conditional distributions. However, they do not satisfy consistency in contexts, as the distribution of targets may vary depending on the order in which they are generated. Neural Processes (NPs) approximate the conditionals of the data-generating process using an attention mechanism that computes weights for key-value pairs. This permutation invariance property of attention is crucial for NPs' application in various areas of Deep Learning. The idea of using a differentiable addressing mechanism in Deep Learning has been successfully applied in handwriting generation, recognition, neural machine translation, natural language processing, and image modeling. Attention mechanisms, such as locality-based and dot-product attention, are used to weight keys according to distance or similarity to queries. This allows for efficient computation of query values with matrix multiplications and softmax. The use of dot-product attention in multihead architecture allows for efficient computation of query values with matrix multiplications and softmax. This architecture enables the query to attend to different keys for each head, resulting in smoother query-values compared to dot-product attention. Self-attention is applied to context points to compute representations of each (x, y) pair, with the target input attending to these context representations for predicting the target output. The model uses self-attention to compute representations of context points before mean-aggregation. This allows each query to focus on relevant context points for prediction, improving the model's ability to capture interactions between context points. Higher order interactions are modeled by stacking self-attention mechanisms. In the deterministic path, cross-attention replaces mean-aggregation, enabling each query to attend closely to context points deemed relevant for prediction. The model uses self-attention to compute query-specific representations of context points, allowing for closer attention to relevant points for prediction. The latent path preserves global dependencies in target predictions, while the deterministic path models local structure. The decoder is updated with query-specific representations. ANP is trained with the same loss function as NP, using Gaussian likelihood and diagonal Gaussian. The NP gains expressivity and accuracy with the added attention mechanism. The ANP is trained using the same loss as the NP, with added attention for improved accuracy. The computational complexity increases due to self-attention, but most computation is done in parallel. ANPs learn faster than NPs in terms of training iterations and wall-clock time. The (A)NP learns a stochastic process and should be trained on multiple functions. The (A)NP learns a stochastic process and should be trained on multiple functions that are realisations of the process. Training involves drawing a batch of realisations, selecting random points as targets and contexts, and optimizing the loss. The decoder architecture is consistent across experiments, with 8 heads for multihead. The model is tested on 1D function regression using synthetic GP data, with fixed or randomly varying kernel hyperparameters. The number of contexts and targets are randomly chosen at each iteration. The (A)NP model is tested on 1D function regression using synthetic GP data with randomly varying kernel hyperparameters. The number of contexts and targets are chosen randomly at each iteration, and the model shows rapid decrease in reconstruction error and lower values at convergence compared to NP, especially for dot product and multihead attention mechanisms. Despite the added computational cost, learning is fast, with Laplace and dot-product ANP having similar computation times to NP, while multihead ANP takes around twice the time. The computation times of Laplace and dot-product ANP are similar to NP for the same value of d, while multihead ANP takes around twice the time. Raising the bottleneck size in NPs can improve reconstructions, but there is a limit to how much they can improve. Using ANPs has significant benefits over simply increasing the bottleneck size in NPs. In a qualitative comparison of attention mechanisms, the predictive mean of the NP underfits the context, while Laplace and dot-product attention show similar behavior. The predictive mean of the NP underfits the context, while Laplace and dot-product attention show similar behavior. Dot-product attention gives predictive means that accurately predict almost all context points, outperforming Laplace attention. Multiple heads in multihead attention help smooth out interpolations and improve reconstruction of contexts and prediction of targets. The multiple heads in multihead attention improve reconstruction of contexts and prediction of targets, showing increased predictive uncertainty away from the contexts. The (A)NP is more expressive than the NP, learning a wider range of functions. Trained (A)NPs are used in a toy Bayesian Optimization problem, demonstrating the ability to sample entire functions and accurate context reconstructions. The (A)NP is trained on image data from MNIST and CelebA datasets using self-attentional layers in the encoder. Three different models are compared: NP, Multihead ANP, and Stacked Multihead ANP. The results show predictions of the full functions. See Appendices C and D for details and analysis. The (A)NP models, including Multihead ANP and Stacked Multihead ANP, are compared using image data from MNIST and CelebA datasets. The Stacked Multihead ANP shows accurate reconstructions of full images with diverse predictions, while the NP model struggles with accuracy. Attention mechanisms improve inpaintings and model less smooth 2D functions effectively. The diversity in generated faces and digits supports the claim that latent variables can capture global image structure. The diversity in faces and digits obtained with different values of z is apparent in different samples, supporting the claim that z can model global image structure. Multihead ANP shows improved context reconstruction error compared to NP, with noticeable gains in crispness and global coherence when using stacked self-attention. Visualizing each head of Multihead ANP for CelebA shows where the attention focuses on different pixels. In FIG7, each head of Multihead ANP for CelebA is visualized, showing different roles and attention focuses. The model can map images from one resolution to another due to the continuous space modeling of pixel locations. The model of (A)NPs trained on images can map images from one resolution to another by predicting pixel intensities in a continuous space. This can be problematic for inaccurate reconstructions, but accurate ANPs can provide reliable mappings between different resolutions. Results show that a Stacked Multihead ANP can map low resolutions to realistic target outputs with diversity. The ANP model can map low resolutions to realistic target outputs with diversity, including higher resolutions like 256x256 images. It learns internal representations of features like filling in missing details in images, such as eyes, with sharper edges compared to baseline methods. The ANP model can map low resolutions to realistic target outputs with diversity, including higher resolutions like 256x256 images. It learns internal representations of features like filling in missing details in images, such as eyes, with sharper edges compared to baseline methods. The iris from the sclera can be extracted using the ANP, highlighting its flexibility in modeling conditional distributions. The use of attention in NPs is motivated by similarities with GP kernels in measuring similarity between points. The use of attention in NPs is related to Deep Kernel Learning with GPs, where learning is still done in a GP framework by maximizing the marginal likelihood. Despite differences in training regimes, GPs have predictive uncertainties dependent on kernel choice, while NPs learn uncertainties directly from data. Comparing the methods directly is challenging due to computational costs and kernel approximations. Variational Implicit Processes (VIP) BID19 are related to NPs, where VIP defines a stochastic process using a decoder setup with a finite dimensional z. The process and its posterior given observed data are both approximated by a GP and learned via a generalisation of the Wake-Sleep algorithm BID13. Meta-Learning (A)NPs focus on few-shot learning, reasoning about new functions by looking at the predictive distribution conditioning on input-output pairs. Few-shot classification and density estimation using attention have been extensively explored in various works. Attention has also been applied in tasks such as Meta-RL for continuous control and visual navigation. The authors of Vfunc have explored regression on a 1D domain without attention mechanisms, while ANPs focus on regression in a less-explored setting. Multitask learning in the GP literature has also been addressed. In a regression setting, the authors explore regression on a toy 1D domain without attention mechanisms. They propose ANPs to resolve underfitting issues, improving prediction accuracy and training speed. ANPs expand the range of functions that can be modeled and offer potential for future work in model architecture. The authors propose Attention Neural Processes (ANPs) to improve prediction accuracy and training speed in a regression setting. Future work includes incorporating cross-attention into the latent path, training ANPs on text data, and exploring connections with the Image Transformer model. Incorporating MLPs in the decoder of ANPs with self-attention allows for a model similar to an Image Transformer on arbitrary pixel orderings. The targets in this setup influence each other's predictions, making target ordering and grouping crucial. The architectural details of NP and Multihead ANP models for 1D and 2D regression experiments are shown in Figure 8. The decoder in the ANP model outputs parameters for the distribution of the target variable. Different forms of multihead cross-attention are used in 1D and 2D regression experiments. Self-attention is utilized with the same architecture as cross-attention but with specific parameters. Stacking multiple layers of self-attention did not significantly improve results. Specific kernel hyperparameters are used in the experiments. In the experiments, different kernel hyperparameters were used for fixed and random cases. A batch size of 16 was employed, and the Adam Optimiser with a fixed learning rate was used. Comparisons were made between trained (A)NP models and oracle GPs, showing that Multihead ANP was closer to the oracle GP but still underestimated predictive variance. Variational inference used in learning the ANP may explain this underestimation. The predictions of Multihead ANP are closer to the oracle GP than the NP but still underestimate predictive variance. Variational inference used in learning the ANP may lead to this underestimation. The conditional distributions for fixed kernel hyperparameters show non-smooth behavior with dot-product attention, which collapses to a local minimum, giving good reconstructions but poor interpolations between context points. Investigating how to address this issue would be interesting. The KL term in the NP loss differs between training on fixed and random kernel hyperparameter GP data. In the random hyperparameter case, the model uses latents to model uncertainty in the stochastic process, allowing for multiple realizations to explain contexts well. ANPs trained on 1D GP data are used to tackle the BO problem, comparing different attention mechanisms to an oracle GP. Using (A)NPs trained on 1D GP data, the BO problem of finding the minimum of test functions drawn from a GP prior is tackled. ANPs consider all previous function evaluations as context points, providing an informed surrogate of the target function. Results show that NP with multihead attention has the smallest simple regret, approaching the oracle GP. The cumulative regret decreases most rapidly for multihead NP, indicating effective use of previous function evaluations for predicting the function minimum. The cumulative regret decreases rapidly for multihead ANP, showing effective use of previous function evaluations for predicting the function minimum. Random pixels of an image are taken as targets during training, with a subset chosen as contexts. The x and y values are rescaled, and a batch size of 16 is used for both MNIST and CelebA datasets. The learning rate and optimizer settings are adjusted accordingly. The self-attention architecture is similar to the Image Transformer BID21, but without Dropout and positional embeddings. The stacked self-attention architecture used in the Image Transformer BID21 does not include Dropout or positional embeddings. Little tuning has been done on the architectural hyperparameters for both Mnist and CelebA datasets. The NP with attention reduces uncertainty significantly as the number of contexts increases. Stacked Multihead ANP improves results over Multihead ANP, providing sharper images with better global coherence. The contexts in the figures contain the target, and relying on the cyan head is sufficient for accurate predictions. The stacked Multihead ANP improves image results significantly, providing sharper images with better global coherence even when the face is not axis-aligned. Different heads play various roles in predicting targets, even when the target is disjoint from the context. Each head attends to different pixels in the NP, with all heads being useful for target prediction."
}