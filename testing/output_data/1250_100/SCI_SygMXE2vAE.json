{
    "title": "SygMXE2vAE",
    "content": "Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art results in various Natural Language Processing tasks. A layer-wise analysis of BERT's hidden states reveals valuable information beyond attention weights, particularly in models fine-tuned for Question Answering (QA). Probing tasks show how QA models transform token vectors to find answers, with hidden state visualizations offering insights into BERT's reasoning process. The transformations within BERT align with traditional pipeline tasks, allowing the system to implicitly incorporate task-specific information into token representations. Transformer models, particularly BERT, can incorporate task-specific information into token representations. Fine-tuning has minimal impact on semantic abilities, and prediction errors can be detected in early layers. These models have gained popularity in Natural Language Processing due to their advancements over RNNs in Machine Translation. The paper discusses the challenges of black box models in deep learning, focusing on the interpretability of Transformer Networks like BERT. While these models have shown significant improvements in various tasks, their lack of transparency and prediction guarantees hinder their real-world application. The paper proposes a new approach to interpreting Transformer Networks. This paper proposes a new approach to interpreting Transformer Networks, focusing on analyzing hidden states between encoder layers rather than attention values. It aims to address questions regarding how Transformers answer questions, the tasks specific layers solve, the influence of fine-tuning on network states, and how network layers can help understand failures in prediction. The study uses fine-tuned models on QA datasets to demonstrate the complexity of tasks like Question Answering and their relation to other NLP tasks. Preliminary tests on the GPT-2 model show similar results to the BERT architecture. The study proposes a layer-wise visualization of token representations in Transformer networks, revealing insights into wrong predictions and context considerations. It applies NLP Probing Tasks, including Question Type Classification and Supporting Fact Extraction, to analyze BERT's abilities and transformations. The analysis shows that BERT's layers encode general language properties in earlier layers, impacting downstream tasks in later layers. The study also includes insights from the GPT-2 model BID28, showing similar phases of transformation even when fine-tuned on different tasks. OpenAI's improved version of GPT-2, while not as high-ranking as BERT, has shown proficiency in language modeling. Other notable Transformer models include Universal Transformer and TransformerXL, which aim to enhance the Transformer architecture. Research on neural model interpretability and probing tasks is a growing field, with recent advancements focusing on creating probing tasks for models like BERT. Recent research has focused on neural model interpretability and probing tasks for models like BERT. Various studies have probed contextualized word embeddings of ELMo, BERT, and GPT-1, analyzing semantic and syntactic information. Different approaches include analyzing attention values in different layers of BERT, exploring phoneme recognition in DNNs, and training diagnostic classifiers to support hypotheses. Some studies also use qualitative visual analysis to study models like CNNs. Li et al. BID17 analyze word vectors and their impact on sequence tagging and classification tasks. Their work is distinct from Liu et al. BID20, who focus on probing pre-trained models without considering fine-tuned models. Inspired by Jain and Wallace BID15, Li et al. propose evaluating hidden states and token representations for explainability in fine-tuned BERT models. The models analyze input tokens using two approaches: qualitative analysis of transforming token vectors and quantitative evaluation of language abilities on QA tasks. The architecture of BERT and Transformer networks allows for tracking token transformations throughout the network. A qualitative analysis involves collecting hidden states from each layer for randomly selected samples to observe token representations. Semantic relations are inferred based on distances between token vectors in the vector space. BERT's pre-trained models have vector dimensions of 1024. Dimensionality reduction techniques such as t-SNE, PCA, and ICA are applied to BERT's pre-trained models with vector dimensions of 1024 and 512. PCA is used to visualize distinct clusters in two-dimensional space, while k-means clustering verifies the distribution in high-dimensional vector space. Semantic probing tasks are conducted to analyze information stored in transformed tokens after each layer. BERT's language information maintenance and task-specific layer allocation are explored through Edge Probing tasks like Named Entity Labeling, Coreference Resolution, and Relation Classification. Additional tasks of Question Type Classification and Supporting Fact Identification are included for Question Answering relevance. Named Entity Labeling involves predicting entity categories from token spans, while Coreference Resolution identifies if two mentions refer to the same entity. These tasks are crucial for language understanding and reasoning. The Coreference task involves predicting if two mentions in a text refer to the same entity, while Relation Classification predicts the relation type between known entities. Question Type Classification identifies the type of question, and Supporting Facts extraction is crucial for Question Answering tasks. BERT's token transformations are examined to understand how distracting from important context parts are distinguished, especially in the multihop case. A probing task is constructed to identify Supporting Facts, where the model predicts if a sentence contains relevant information regarding a specific question. Different probing tasks are created for datasets like HotpotQA and bAbI to test their ability to recognize relevant parts. The input tokens for each probing task sample are embedded with a fine-tuned BERT model. BERT's token transformations are examined to understand how distracting from important context parts are distinguished, especially in the multihop case. A probing task is constructed to identify Supporting Facts, where the model predicts if a sentence contains relevant information regarding a specific question. Different probing tasks are created for datasets like HotpotQA and bAbI to test their ability to recognize relevant parts. The output embeddings from different layers of BERT models are used for classification tasks, including Question Answering datasets like SQUAD, bAbI, and HotpotQA. The common punishment in schools in the UK, Ireland, and other countries is detention, where students have to stay in school after hours or on non-school days. HotpotQA is a dataset with multihop questions that require combining information from different parts of a context. The distracting facts in HotpotQA are reduced to fit the input size of the pre-trained BERT model. The QA bAbI tasks are artificial toy tasks designed to test neural models' abilities in reasoning over multiple sentences. These tasks differ from other QA tasks in their simplicity and artificial nature. The analysis is based on models like BERT and GPT-2, which are Transformers that build upon previous ideas. The decoder half of BERT is used in our probing setup. We fine-tune pre-trained BERT models on different datasets, adjusting hyperparameters through grid search. Models are trained for 5 epochs with evaluations every 1000 iterations. Input length varies for different tasks. For bAbI tasks, we evaluate single-task and multitask models. Answer Questions? CIKM '19, November 3rd-7th, 2019, Beijing, China. Span prediction and Sequence Classification settings for bAbI tasks. HotpotQA tasks include Support Only (SP) and Distractor tasks. Training results show high accuracy on SQuAD task but more challenging tasks from HotpotQA. BERT and GPT-2 perform differently on various tasks. While GPT-2 excels in bAbI tasks, BERT struggles with tasks requiring positional reasoning. Qualitative analysis reveals recurring patterns in vector transformations. Results from probing tasks are compared across different models in FIG1. The PCA representations of tokens in different layers of BERT models suggest multiple phases in answering questions. These phases are observed in various QA tasks and supported by probing task results. The four phases include Semantic Clustering in early layers grouping tokens into clusters, and Connecting Entities with Mentions and Attributes in middle layers. Connecting Entities with Mentions and Attributes in neural networks involves task-specific clusters that filter question-relevant entities. Clusters show relationships within input contexts, aiding in answering questions. The model recognizes entities, identifies mentions, and handles coreferences effectively. The ability of neural networks to recognize entities, identify mentions, and find relations improves with higher network layers. Named Entity Labeling is learned first, while coreference resolution and relation recognition require input from additional layers. BERT models match question tokens with relevant context tokens to improve Question Answering and Information Retrieval. The models transform token representations to align question and supporting facts in the vector space, showing strong ability in probing tasks. The models show strong ability to distinguish relevant information in higher layers, as seen in FIG1 for SQuAD and bAbI. However, the fine-tuned HotpotQA model in FIG2 does not perform well without fine-tuning, especially in identifying Supporting Facts. The vector representations help identify important facts matched with the question, aiding in retracing decisions. In the last network layers, the model separates correct answer tokens from others, forming homogeneous clusters. This task-specific vector representation is learned during fine-tuning, leading to a performance drop in general NLP probing tasks. The fine-tuned BERT model on HotpotQA shows a performance drop in general NLP tasks, especially in last-layer representations. The model loses the ability to perform well on tasks like NEL or COREF after fine-tuning. The phases of answering questions can be compared to human reasoning, with BERT able to run multiple processes concurrently depending on the task at hand. In comparing BERT and GPT-2 models, a major difference is that GPT-2 focuses on the first token of a sequence, leading to a separation of clusters in all layers except for the Embedding Layer and the first and last Transformer blocks. This issue is addressed by masking the first token during dimensionality reduction. GPT-2, like BERT, separates relevant information in the vector space but also extracts additional sentences with similar meaning. The GPT-2 model, unlike BERT, does not separate the correct answer \"cats\" but leaves it as part of the sentence. This suggests that findings in GPT-2 may extend beyond the BERT architecture. Future work will involve more probing tasks to confirm this observation. Visualizations in explainable Neural Networks can show failure states and the difficulty of a task. For wrong predictions, early layers can provide insights into why the wrong candidate was chosen. The wrong candidate was chosen due to factors like selecting the wrong Supporting Fact or misresolution of coreferences. Low network confidence can lead to incorrect answers, with little transformation in later layers. Fine-tuning has minimal impact on the model's core NLP abilities, as the pretrained model already contains sufficient information for various tasks. The positional embedding in Transformer networks plays a crucial role in maintaining sequential information, even in late layers. Fine-tuning on the SQuAD dataset improves the model's ability to resolve question types, leading to better performance from layer 5 onwards. The model fine-tuned on bAbI tasks loses the ability to distinguish question types, likely due to the static structure of bAbI samples. Surprisingly, the model fine-tuned on HotpotQA does not outperform the model without fine-tuning. Transformer networks store interpretable information in hidden states, which can help identify misclassified examples and model weaknesses. The curr_chunk discusses the importance of understanding which parts of the context are crucial for model decision-making, the transferability of lower layers in Transformer networks for different tasks, and the modularity of specific layers solving different problems. The authors suggest further research to explore methods for processing this information and optimizing layer depth selection based on the task at hand. Further research should focus on understanding state-of-the-art Transformer-based models and how they tackle downstream tasks to enhance their performance."
}