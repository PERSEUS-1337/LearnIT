{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map observed input-output pairs to a distribution over regression functions. NPs efficiently fit data with linear complexity and can model a wide range of conditional distributions. However, they suffer from underfitting issues. Attention mechanisms are incorporated into NPs to improve prediction accuracy, training speed, and expand the range of functions that can be modeled. In regression, Neural Processes (NPs) model a distribution over functions mapping inputs to outputs, offering efficient prediction complexity linear to the context set size. NPs can predict the distribution of a target output based on context input-output pairs of any size, allowing for modeling data generated from a stochastic process. Unlike Gaussian Processes (GPs), NPs have different training regimes. Neural Processes (NPs) are trained on multiple realisations of a stochastic process, while Gaussian Processes (GPs) are trained on observations from one realisation. NPs tend to underfit the context set, leading to inaccurate predictions and overestimated variances. The encoder in NPs aggregates the context set to a fixed-length latent summary, which may act as a bottleneck causing underfitting behavior. The encoder in Neural Processes (NPs) acts as a bottleneck due to the mean-aggregation step, making it difficult for the decoder to learn relevant context points for target predictions. To address this, inspiration is drawn from Gaussian Processes (GPs) to implement Attentive Neural Processes (ANPs) using differentiable attention. This mechanism learns to attend to relevant contexts for a given target while maintaining permutation invariance. The ANPs are evaluated on 1D function regression and 2D image regression tasks. Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) in 1D and 2D regression tasks by enhancing reconstruction of contexts, training speed, and expressiveness. ANPs model a wider range of functions by conditioning on observed contexts and targets with permutation invariance. The model uses a deterministic function to aggregate context pairs into a finite representation, allowing for better performance in practice. The NP model includes a global latent variable z to capture uncertainty in predictions of y for observed (x, y) pairs. This latent variable is modelled by a factorised Gaussian parametrised by s, with q(z|s) as the prior. The model's likelihood is the decoder, while q, r, and s make up the encoder. The global latent z allows for modeling different realisations of the data generating process. The NP model uses both deterministic and latent paths to define the model. Parameters are learned by maximizing ELBO for a subset of contexts and targets. The NP reconstructs targets with a KL term to keep context and target summaries close. Randomly chosen contexts and targets allow the NP to learn various conditional distributions efficiently. The NP model offers scalability, flexibility, and permutation invariance in predicting targets based on contexts. However, it lacks consistency in contexts. Maximum-likelihood learning minimizes the KL divergence between the conditional distributions of the data-generating process and the NP. An attention mechanism computes weights for key-value pairs to form the value. The attention mechanism computes weights for key-value pairs to form the value corresponding to the query. This permutation invariance property of attention is crucial in its application to NPs. Differentiable addressing mechanisms have been successfully applied in various areas of Deep Learning. The (normalised) Laplace kernel and (scaled) dot-product attention are used to compute query values. Multihead attention allows for different keys to be attended to for each head, resulting in smoother query-values. Self-attention is applied to context points to compute representations, which are then used for cross-attention to predict the target output. The self-attention mechanism is used to model interactions between context points, allowing queries to focus on relevant points. In the deterministic path, mean-aggregation is replaced by cross-attention, where each query attends to context points for prediction. This mechanism is not used in the latent path to preserve global latent information. The latent path in the model preserves global latent information to induce dependencies between target predictions. The decoder is modified to use query-specific representation. Attention mechanism ensures permutation invariance in contexts. The computational complexity increases due to self-attention across contexts. The (A)NP learns a stochastic process and should be trained on multiple functions that are realisations of the process. Training involves drawing a batch of realisations from the data generating process and selecting random points as targets and contexts to optimize the loss. The decoder architecture is consistent across experiments, with 8 heads for multihead. The (A)NPs are explored on data generated from a Gaussian Process with a squared-exponential kernel and small likelihood noise. The study explores the use of attention mechanisms in training Neural Processes (NPs) and Attentive Neural Processes (ANPs) on data generated from a Gaussian Process. ANPs show faster learning and lower reconstruction error compared to NPs, especially with dot product and multihead attention. The experiments involve randomly varying hyperparameters of the kernel and selecting random contexts and targets for optimization. The study compares the computational times of Laplace and dot-product ANP with NP, showing multihead ANP takes twice the time. Increasing the bottleneck size in NPs improves reconstructions but slows learning. ANPs offer significant benefits over raising the bottleneck size in NPs. Visualizing the learned conditional distribution shows NP underfitting the context. The study compares Laplace and dot-product attention in ANP and NP models. Dot-product attention provides accurate predictive means for context points, outperforming Laplace attention. Multihead attention helps smooth out interpolations and improves reconstruction of contexts and prediction of targets. Dot-product attention displays non-smooth predictions with increased predictive uncertainty away from contexts. The ANP is more expressive than the NP, as shown in a toy Bayesian Optimization problem. Image data regression can be seen as a stochastic process, mapping pixel locations to intensities on a fixed grid. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Three different models are compared: NP, Multihead ANP, and Stacked Multihead ANP. Results show that Stacked Multihead ANP provides more accurate reconstructions of the whole image compared to NP. The Stacked Multihead ANP provides accurate reconstructions of images compared to the NP, with crisper inpaintings and improved context reconstruction errors. The model can generalize well even with limited training data, showing gains in target point predictions with multihead crossattention. The Stacked Multihead ANP shows small gains in crispness and global coherence compared to NP. Each head of Multihead ANP has different roles in attending to pixels, with consistent behavior observed for other target pixels. The model can map images from one resolution to another, leveraging the real-valued pixel locations. The Stacked Multihead ANP can map images from one resolution to another, predicting pixel intensities in a continuous space. While NPs may have inaccurate reconstructions, ANPs show accurate mappings between different resolutions. The model can map low resolutions to realistic target outputs and even higher resolutions. The Stacked Multihead ANP can accurately map images from one resolution to another, producing realistic high-resolution outputs with sharper edges. The model learns internal representations of features like facial appearance, filling in details like eyes even in coarse original images. The ANP is not meant to replace state-of-the-art algorithms for image inpainting or super-resolution. The use of attention in Neural Processes (NPs) is related to Gaussian Processes (GPs) and Deep Kernel Learning, highlighting the flexibility of ANPs in modeling conditional distributions. The training regimes of GPs and NPs differ, making direct comparison challenging, but learning GPs via the NP training regime is a possibility. Gaussian Processes (GPs) update kernel hyperparameters via gradient steps on mini-batches, but have high computational costs. Predictive uncertainties in GPs depend on kernel choice, while Neural Processes (NPs) learn uncertainties from data. GPs have consistent stochastic processes and exact covariance expressions, unlike NPs. Variational Implicit Processes (VIP) approximate processes with GPs. Meta-Learning NPs focus on few-shot learning. Few-shot classification and density estimation using attention have been extensively explored in various works. Attention has been used in tasks such as Meta-RL, continuous control, and visual navigation. The authors of Vfunc also explore regression on a toy 1D domain without attention mechanisms. Multitask learning in Gaussian Processes (GPs) has been addressed, while Neural Processes (NPs) focus on learning uncertainties from data. Variational Implicit Processes (VIP) approximate processes with GPs. In the GP literature, multitask learning has been explored without attention mechanisms. Generative Query Networks (GQN) are models for spatial prediction that use viewpoints to render scenes. ANPs augment NPs with attention to address underfitting issues, improving prediction accuracy and training speed. Future work for ANPs includes incorporating cross-attention and global latents. Incorporating global latents and self-attention in the decoder of ANPs can extend their expressiveness, resembling an Image Transformer defined on arbitrary pixel orderings. This approach may improve prediction accuracy and training speed, with potential applications in text data as well. The architectural details of the NP and Multihead ANP models for regression experiments are shown. The models use relu non-linearities and parameterize distributions using sigmoid and softplus functions. Different forms of multihead cross-attention are used for 1D and 2D regression experiments. Self-attention is also employed, with the option to stack multiple layers for increased expressiveness. In the 2D Image regression experiments, 2 layers of self-attention are stacked for Stacked Multihead ANP. Stacking more layers did not show significant improvements. Different hyperparameters are used for fixed and random kernel experiments. A batch size of 16 is used, along with Adam Optimiser BID14. One sample of q(z|s C ) is used for MC estimate of the loss. Trained (A)NP models are compared against the oracle GP, showing Multihead ANP closer to the oracle GP. The Multihead ANP is closer to the oracle GP than the NP but underestimates predictive variance due to variational inference. The dot-product attention behavior shows non-smoothness, collapsing to a local minimum, giving good reconstructions but poor interpolations. The KL term in NP loss differs between fixed and random kernel hyperparameters in training. In the fixed kernel hyperparameter GP data, the KL for multihead ANP quickly goes to 0, indicating the model deems the deterministic path sufficient for accurate predictions. However, in the random hyperparameter case, there is added variation in the data, leading to a non-zero KL as the model uses latents to model uncertainty in the stochastic process. ANPs trained on 1D GP data are used for Bayesian optimization by considering all previous function evaluations as context points. Thompson sampling is used for drawing. Thompson sampling is used with multihead attention neural processes (NPs) to minimize simple regret in function predictions. Results show that multihead NPs outperform other NPs in terms of cumulative regret slope, utilizing previous function evaluations effectively. The lower initial cumulative regret compared to the oracle GP is due to under-exploration. Random pixels in images are used as targets and contexts for training, with rescaled x and y values. A batch size of 16 is used for MNIST and CelebA datasets. Thompson sampling with multihead attention neural processes (NPs) is used to minimize simple regret in function predictions. A batch size of 16 is used for both MNIST and CelebA datasets, with rescaled x and y values. The stacked self-attention architecture is similar to the Image Transformer BID21, but without Dropout and positional embeddings. Little tuning has been done regarding the architectural hyperparameters. The NP overestimates predictive variance, but with attention, uncertainty is reduced as the number of contexts increases. Stacked Multihead ANP improves image results significantly over Multihead ANP, providing sharper images with better global coherence even when the face isn't axis-aligned. Different heads in the model play various roles, making all heads useful for target prediction even when the context is disjoint from the target. Visualizations in FIG0 show pixels attended by each head of multihead attention in the NP, with different colors representing each head."
}