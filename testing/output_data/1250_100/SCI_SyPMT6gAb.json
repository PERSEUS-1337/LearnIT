{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning involves evaluating and improving policies using historical data from a logging policy. One major challenge is deriving counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization show significant improvement over conventional baseline algorithms. Off-policy learning is crucial for evaluating deterministic policies using historical data, avoiding costly and risky on-policy evaluations in real-world scenarios like clinical trials and online advertising A/B testing. Off-policy learning in the context of reinforcement learning and contextual bandits involves utilizing historic data for safe exploration of policies before deployment. Various methods like Q learning, doubly robust estimator, and self-normalized approaches have been studied. A new direction includes using logged interaction data with limited feedback, such as scalar rewards, which restricts the information available about alternative actions and potential rewards. This is particularly relevant in scenarios like online recommendation systems where understanding the relationship between policy changes and rewards is crucial. The paper discusses the challenges of off-policy learning in bandit feedback cases, focusing on the distribution mismatch between logging and new policies. A new framework is proposed to minimize counterfactual risk by adding sample variance as a regularization term. However, the linear stochastic model used for policy parametrization limits representation power, and efficient training algorithms remain a challenge. The paper introduces a new learning principle for off-policy learning with bandit feedback, drawing a connection to the generalization error bound of importance sampling. The paper introduces a new learning principle for off-policy learning with bandit feedback, proposing to regularize the generalization error of the new policy by minimizing the distribution divergence with the logging policy. The policy is parametrized as a neural network for end-to-end training, utilizing variational divergence minimization and Gumbel soft-max sampling. Experimental results show significant performance improvement over conventional baselines. The text discusses the use of stochastic policies in online systems, where actions are taken based on sampled distributions. Feedback is observed to evaluate the policy's performance, with the goal of minimizing expected risk. In off-policy learning, data from a logging policy is used to find a policy with minimum risk on test data. In off-policy learning, the goal is to improve a logging policy h 0 by finding a new policy h that reduces expected risks. Challenges include skewed data distribution and the need for empirical estimation due to finite samples, leading to generalization error. The propensity scoring approach using importance sampling can address these issues by reweighting the expected risk. With a historic dataset D, empirical risk R D (h) can be estimated. Counterfactual risk minimization aims to address flaws in the vanilla approach by introducing a regularization term for sample variance derived from empirical Bernstein bounds. This modification leads to a new objective function to minimize, which is challenging to optimize due to the variance term being dependent on the entire dataset. To overcome this, the authors approximated the regularization term using a first-order Taylor expansion, resulting in a stochastic optimization algorithm. However, this first-order approximation neglects non-linear terms and introduces errors in reducing sample variance. Instead of empirically estimating variance from samples, the authors suggest deriving it from a parametrized version of the policy to enable direct stochastic training. The parametrized policy h(Y|x) allows for deriving a variance bound directly from the distribution. An identity for importance sampling weights is established, leading to an upper bound for the second moment of the weighted loss. This approach enables direct stochastic training by deriving variance from the policy. The conditional divergence between two sampling distributions of y, h(y|x) and h 0 (y|x), is defined as d 2 (h(y|x)||h 0 (y|x); P(x)). A generalization bound between expected risk and empirical risk is derived using distribution divergence. The proof involves Bernstein inequality and second moment bound, highlighting bias-variance trade-offs in empirical risk minimization problems. This motivates optimizing reweighed loss in bandit learning settings to reduce variance. In bandit learning settings, minimizing variance by optimizing reweighed loss is crucial. To address the challenge of setting the regularization hyper-parameter \u03bb empirically, a new constrained optimization formulation is explored. This formulation, with a pre-determined constant \u03c1 as the regularization hyper-parameter, provides a good surrogate for the true risk, with the difference bounded by \u03c1 and approaching 0 as the sample size N increases. The new objective function in bandit learning settings is bounded by the regularization hyper-parameter \u03c1 and approaches 0 as sample size N increases. Recent f-gan networks and Gumbel soft-max sampling techniques are discussed for variational divergence minimization. The importance of stochasticity in the logging policy is highlighted, as a deterministic policy may hinder exploration in certain regions. The derived variance regularized objective requires minimizing the square root of the conditional expectation. By connecting the divergence to the f-divergence measure, a lower bound of the objective can be reached using the f-GAN for variational divergence minimization method. The bound is obtained through Fenchel convex duality, allowing for the swapping of expectation and supreme operators. The universal approximation theorem of neural networks states that they can approximate continuous functions on a compact set with any desired precision. By choosing the family of neural networks as the function T, the saddle point objective can be theoretically satisfied. The objective is a consistent estimator of the true divergence when trained with mini-batch estimation. The true divergence is denoted as Df and the empirical estimator is used to approximate it. The estimation error is decomposed into two terms: one from restricting the parametric family to neural networks and the other from the approximation error of empirical mean estimation. The strong law of large numbers applies, leading to the first term approaching zero. The second term can be minimized using Theorem 5 and a generative-adversarial approach. The text discusses using a generative-adversarial approach to optimize a generator distribution for structured output problems with discrete values. The approach involves representing functions as neural networks, using Gumbel soft-max sampling for differential sampling, and updating parameters iteratively to minimize divergence. The text presents an algorithm for end-to-end learning to minimize variance regularization in counterfactual risk from logged data. It involves updating parameters to minimize divergence and includes training for the original ERM formulation. The algorithm minimizes variance regularized risk through separate training steps to update policy parameters and improve generalization performance. It addresses exploiting historic data in multi-armed bandit problems and extends techniques to reinforcement learning domains. Techniques such as doubly robust estimators and Q function learning are utilized for off-policy learning in reinforcement learning. Off-policy learning in reinforcement learning (RL) involves methods like multi-step bootstrapping and off-policy training of Q functions. Learning from log traces uses propensity scores to evaluate candidate policies, similar to treatment effect estimation in statistics. Techniques like unbiased counterfactual estimators and variance regularization aim to improve generalization performance in bandit feedback problems. The focus is on estimating the effect of interventions from observational studies with different interventions, addressing distribution mismatch between training and testing data. The original purpose was to address distribution mismatch in supervised learning. Variance regularized empirical risk minimization for supervised learning with a convex objective function was discussed. Divergence minimization technique can be applied to supervised learning and domain adaptation problems. Regularization for the objective function has connections to distributionally robust optimization techniques. Wasserstein distance between empirical distribution and test distribution is a well-studied constraint for achieving robust generalization performance. Empirical evaluation of proposed algorithms follows the conversion from supervised learning to bandit feedback method. The text discusses the conversion from supervised learning to bandit feedback method BID0. A logging policy is constructed for each sample in a dataset, and bandit feedback datasets are created. Evaluation metrics for the probabilistic policy are used to measure generalization performance. The text discusses the comparison of different algorithms for generating predictions in bandit feedback methods. The focus is on the trade-off between MAP and EXP performance, highlighting the importance of diverse predictions for generalization. Various algorithms like IPS and POEM are compared using different optimization solvers. Additionally, neural network policies without divergence regularization are used as baselines to evaluate the effectiveness of variance regularization. Regularization techniques were applied to four multi-label classification datasets using neural networks for policy distribution and divergence minimization. Training was done with Adam optimizer and PyTorch on Nvidia K80 GPU cards. Results were obtained from 10 experiment runs and reported with two evaluation metrics. Gumbel-softmax sampling schemes were used for regularized neural network policies. By introducing neural network parametrization of policies, test performance significantly improved compared to baseline CRF policies. Additional variance regularization further enhanced testing and MAP prediction loss. No significant difference was observed between the two Gumbel soft-max sampling schemes. Variance regularization effectiveness was studied quantitatively by varying the maximum number of iterations in each divergence minimization sub loop. Results were plotted for expected loss in test sets against epochs average over 10 runs using the yeast dataset. By adding regularization to the models, test performance improved, convergence rate was faster, and generalization to test sets was better. Increasing the number of training samples led to improved test performance for both regularized and non-regularized models. Regularized policies showed better performance overall. Regularized policies have better generalization performance compared to models without regularization. However, after the replay of training samples exceeds 24, MAP prediction performance starts to decrease, indicating potential overfitting. Experimental results show that blending weighted loss and distribution divergence slightly outperforms models without regularization, but balancing the gradient of the objective function is challenging. In this section, the effect of logging policies on learning performance is discussed. The algorithm's ability to improve policy depends on the stochasticity of the logging policy. By adjusting a parameter with a temperature multiplier, the policy can become more deterministic. Experimental results show that neural network policies outperform logging policies when the stochasticity is reduced. The study compares the performance of neural network policies with logging policies based on the stochasticity of h0. It is observed that NN policies perform better when h0 is sufficiently stochastic, but become harder to improve when the temperature parameter exceeds 2/3. Stronger regularization in NN policies shows slightly better performance, indicating robustness in learning. The decreasing stochasticity of h0 makes it harder to learn improved NN policies, while regularization helps in achieving better generalization. As h0 quality improves, models consistently outperform baselines, but the difficulty increases. The impact of logging policies on learned improved policies is also discussed. In this study, the trade-off between policy accuracy and sampling biases in bandit datasets is explored by varying the proportion of training data points used for the logging policy. Improved policies outperform the logging policy, addressing sampling biases. Regularizing variance helps in generalization performance for off-policy learning in logged bandit datasets. Theoretical discussion led to a training objective combining importance reweighted loss and distribution divergence regularization. Variational divergence minimization and Gumbel soft-max sampling techniques were applied to train neural network policies effectively. Evaluations on benchmark datasets validated the learning principle and training algorithm, with potential for improvement by estimating propensity scores. The work focuses on extending the proposed algorithm to general supervised learning and reinforcement learning. The study applies various theorems and techniques to optimize the generator for importance sampling in bandit learning. The approach involves updating the discriminator by sampling 'fake' and 'real' data batches to minimize the reweighted loss and distribution divergence. The study extends the algorithm to supervised and reinforcement learning, optimizing the generator for importance sampling in bandit learning. The approach involves updating the discriminator with 'fake' and 'real' data batches to minimize reweighted loss and distribution divergence. The statistics of the datasets are reported, showing the effect of stochasticity of h0 on test loss with MAP predictions. The quality of the logging policy is compared to expected test loss with MAP predictions, highlighting the difficulty in beating baselines with neural network policies. Further investigation is warranted. The study compares the quality of the logging policy to expected test loss with MAP predictions. Neural network policies can still improve over h0 in expected loss, but beating baselines with MAP predictions is challenging if the logging policy has full training data exposure."
}