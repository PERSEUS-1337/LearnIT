{
    "title": "H1eRI04KPB",
    "content": "Deep generative modeling using flows has gained popularity due to tractable exact log-likelihood estimation and efficient training. A solution to the challenge of high dimensional latent space is a multi-scale architecture proposed by Dinh et al. (2016). Our novel approach involves data-dependent factorization based on the contribution of each dimension to the total log-likelihood, enabling versatile implementation for generic flow models. Deep Generative Modeling aims to learn embedded distributions in input data without human labeling effort. Representations learnt can be used in downstream tasks like semi-supervised learning, synthetic data augmentation, and text analysis. The repository includes Likelihood based models such as autoregressive models, latent variable models, flow based models, and implicit models like GANs. Deep Generative Modeling involves learning embedded distributions in input data without human labeling. Autoregressive models achieve high log-likelihood scores but have slow sampling processes. Latent variable models capture global features but lack exact density estimates. GANs synthesize realistic data but lack a suitable latent space for downstream tasks. Flow based generative models offer exact density estimation with fast inference and sampling, providing an information-rich latent space for various applications. Multi-scale architecture introduced by Dinh et al. (2016) addresses the bottleneck of flow models scaling with increasing input dimensions. It performs iterative early gaussianization of dimensions at regular intervals, making the model computationally efficient. Our proposed multi-scale architecture uses data-dependent factorization to determine which dimensions pass through more flow layers, based on the total log-likelihood contribution of each dimension. This approach improves training by distributing the loss function throughout the network. The research introduces a log-determinant based heuristic to determine the contribution of each dimension to the total log-likelihood in a multi-scale architecture. It also presents a data-dependent splitting of dimensions in the architecture, implemented for various flow models, leading to quantitative and qualitative improvements. Ablation studies confirm the novelty of the method. The research proposes a data-dependent splitting of dimensions in a multi-scale architecture for flow based generative models. It introduces an invertible transformation mapping observed data to latent variables, allowing for composition of compatible dimension flows. The research introduces a multi-scale architecture for flow-based generative models, proposing a data-dependent splitting of dimensions. This allows for the composition of compatible dimension flows, optimizing the network's loss function distribution. Different types of flows can be constructed, including inverse flows, confirming the properties of a flow. Our research work builds on a multi-scale architecture for flow-based generative models, optimizing the network's loss function distribution by factoring dimensions at each flow layer efficiently. We introduce a heuristic to estimate the contribution of each dimension towards the total log-likelihood, enabling preferential splitting in the multiscale architecture. The multi-scale architecture factors dimensions at each flow layer to capture local variance and maximize log-likelihood. Careful design of flow layers can lead to maximizing log-determinant, contributing to the total log-likelihood. The dimension of the input space is s \u00d7 s \u00d7 c, and a flow with K component flows is applied to intermediate variables denoted by y K. The log-det term in a multi-scale architecture with K component flows decomposes contributions by variables towards total log-likelihood. Variables with higher log-det values are more valuable for flow formulation, guiding early gaussianization decisions. Selectively exposing variables with more log-det to additional flow layers optimizes the architecture. In a multi-scale architecture, variables with higher log-det values are more valuable for flow formulation, guiding early gaussianization decisions. Selectively exposing these variables to additional flow layers optimizes the architecture for enhanced density estimation performance and qualitative reconstruction. Variants of hybrid factorization techniques can be implemented to improve density estimation and qualitative performance in different types of flow models. The Likelihood Contribution based Multi-scale Architecture (LCMA) improves density estimation and qualitative performance by decomposing the log-det of the jacobian. In the dimension factorization phase, dimensions with more/less log-det are grouped and split to preserve local spatial variation. The decision of which dimensions to factor at each flow layer is fixed before training, allowing for the use of non-invertible operations for efficient factorization with log-det heuristic. Individual contributions of dimensions are computed in the dimension factorization phase. LCMA improves density estimation by decomposing the log-det of the jacobian. In the dimension factorization phase, dimensions are grouped based on log-det values and split to preserve spatial variation. The computation of individual dimension contributions towards total log-likelihood is crucial for flow models. Dimensions with higher log-det are forwarded to more flow layers, while those with lower log-det are early gaussianized. LCMA improves density estimation by decomposing the log-det of the jacobian. In the training phase, dimensions are grouped based on log-det values and split to preserve spatial variation. Flow models like RealNVP and Glow offer methods to obtain individual likelihood contributions of dimensions, involving a multiscale architecture. For RealNVP, the logarithm of individual diagonal elements of the jacobian provides per-dimensional likelihood contributions, while Glow expresses log-det terms for each dimension as the log of corresponding diagonal elements of the jacobian. LCMA improves density estimation by decomposing the log-det of the jacobian. Glow utilizes 1 \u00d7 1 convolution blocks with non-diagonal log-det terms for channel dimensions. Singular values of weight matrix W determine individual log-det contributions for each channel. i-ResNet is a flow model with invertibility and efficient jacobian computation properties. The log-likelihood expression involves the trace of the power series of the log-det of the jacobian. Behrmann et al. (2018) introduced a method for per-dimensional likelihood contribution in i-ResNet. Multi-scale architecture and variants, including LCMA, have been successful in deep generative modeling. They utilize a power series for the jacobian and have been applied in invertible neural networks for dimensionality reduction and enhanced training. Denton et al. (2015) and Reed et al. (2017) have used multiscale variants in GAN models and auto-regressive models, respectively. Our proposed method for generative flow models involves factorization of dimensions based on their likelihood contribution, determining important dimensions for density estimation and reconstruction. Prior works have explored multi-scaling and permutation among dimensions. Kingma & Dhariwal (2018) introduced a 1 \u00d7 1 convolution layer to redistribute dimension contributions, while we focus on evenly splitting tensors along the channel dimension in our implementation of multi-scale architecture. Our method focuses on the individuality of dimensions and their importance in log-likelihood contribution, providing more versatility in multi-scale architectures compared to previous works. Other flow models have implemented multi-scale architectures but without considering the importance of individual dimensions or permutations. Our proposed factorization method focuses on preserving the spatiality of images in flow models with a multi-scale architecture, improving density estimation and qualitative performance. The approach considers the individual contribution of dimensions towards the total log-likelihood, contrasting with previous works that overlook this aspect. The method is applied to the RealNVP flow model and compared quantitatively with Glow and i-ResNet. LCMA is compared with conventional RealNVP using the same experimental settings, except for dimension factorization. Experiments are conducted on CIFAR-10, Imagenet, and CelebA datasets, with scaling performed accordingly. LCMA is also compared with Glow and i-ResNet on CIFAR-10, showing improvements in bits/dim. Ablation studies are done to compare LCMA with other dimension splitting options, with results presented in Table 1. The density estimation results using LCMA show significant improvement compared to the baseline RealNVP. The improvement is particularly high for CelebA due to the high redundancy in facial features. LCMA exposes dimensions to more flow layers, making them more expressive. The improvement in bits/dim is less for natural image datasets due to high variance among features. LCMA implementation for RealNVP, Glow, and i-ResNet with CIFAR-10 dataset is summarized in Table 2. LCMA implementation for RealNVP, Glow, and i-ResNet with CIFAR-10 dataset is summarized in Table 2. The density estimation scores for flows with LCMA outperform the same flow with conventional multi-scale architectures. A dimension factorization method capturing local variance over flow layers aids in qualitative sampling. LCMA introduces local max and min pooling operations on log-det heuristic to decide early gaussianization of dimensions. Ablation studies compare LCMA with other dimension factorization methods in a multi-scale architecture, including fixed random permutation, early gaussianization of high log-det dimensions, checker-board and channel splitting, and early gaussianization of low log-det dimensions proposed by LCMA. LCMA outperforms other factorization methods in multi-scale architectures, improving density estimation and sample quality. Gaussianizing high log-det variables early provides the worst density estimation, while LCMA exposes variables carrying more information to more flow layers, resulting in the best sample quality. Fixed random permutation has better scores than early gaussianization methods, with RealNVP showing improvement in sample quality. LCMA implementation with CIFAR-10 dataset shows superior performance in generating qualitative samples. The proposed multi-scale architecture for generative flows, LCMA, improves log-likelihood scores and sample quality compared to other methods like RealNVP and Glow. Empirical studies on benchmark image datasets validate the strength of LCMA, with ablation study results confirming its superiority in dimension factorization. Experimental settings and comparisons with existing methods are detailed for CIFAR-10 and other image datasets. The study introduces the LCMA architecture for generative flows, utilizing datasets like CIFAR-10, Imagenet, and CelebFaces Attributes. Pre-processing involves resizing images to 64x64, with a flow model architecture using affine coupling layers. Different numbers of coupling layers and residual blocks are used based on image resolutions. More detailed architectures are available in the source code release."
}