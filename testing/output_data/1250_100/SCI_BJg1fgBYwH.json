{
    "title": "BJg1fgBYwH",
    "content": "The proposed SAFE-DNN improves classification robustness under stochastic input perturbation by incorporating unsupervised learning of low-level features using SNN with STDP. Experimental results show enhanced noise robustness for various DNN architectures on CIFAR-10 and ImageNet subset without compromising accuracy on clean images. This is crucial for deploying DNNs in autonomous systems like autonomous vehicles and robotics, where reliable classifications are needed even with noisy data. Approaches to improve DNN robustness to pixel perturbation can be categorized into image de-noising networks and classification networks. De-noising networks enhance accuracy under noise but may reduce accuracy for clean images and struggle with different noise structures. Advanced de-noising networks can generalize to multiple noise levels but are complex and not suitable for real-time or lightweight platforms. Alternatively, developing a robust classification network is another approach. The paper proposes a new class of DNN architecture that integrates neuro-inspired learning and supervised training to improve robustness to input perturbations without requiring noisy data. The paper introduces a hybrid DNN architecture that combines neuro-inspired learning with supervised training to enhance robustness to input perturbations. By integrating features from both global (supervised training) and local (STDP) learning, the hybrid network can effectively filter out locally uncorrelated perturbations in pixels while extracting accurate feature representations from images, leading to improved image classification under noisy conditions. The paper presents a hybrid network architecture called SAFE-DNN, combining neuro-inspired learning with supervised training for robust image classification. It integrates a spiking convolutional module with a DNN pipeline, utilizing a frequency-dependent stochastic STDP learning rule for local competitive learning of features. Additionally, a methodology is developed to transform the spiking convolution to an equivalent CNN using a special neuron activation unit (SAU). The SAFE-DNN architecture combines neuro-inspired learning with supervised training for robust image classification. It integrates a spiking convolutional module with a DNN pipeline using a special neuron activation unit (SAU) to transform the spiking convolution to an equivalent CNN. The network shows improved accuracy under various types of input noise without prior knowledge of perturbations, complementing de-noising networks for input pre-processing. The SAFE-DNN architecture integrates neuro-inspired learning with supervised training for image classification. It combines a spiking convolutional module with a DNN pipeline using a special neuron activation unit (SAU) to enhance accuracy under input noise. Unlike de-noising networks, SAFE-DNN has minimal computation and memory overhead, making it suitable for resource-constrained autonomous platforms. It hybridizes STDP and SGD during learning, creating a single hybrid network operating as a DNN during inference. The work described parameters controlling neuron dynamics and synapse conductance in SNN. Learning is achieved through spike-timing-dependent-plasticity (STDP) with long-term potentiation (LTP) and long-term depression (LTD) operations. Causality between connected neurons is extracted based on temporal relationships of their spikes. LTP is triggered by post-synaptic neuron spiking after pre-synaptic neuron, while LTD occurs when the order is reversed. Magnitude of modulation is determined by \u2206G p. The gradient descent weight update process in a DNN computes new weights based on specific network configurations. Parameters like \u03b1, \u03b2, G max, and G min are tuned for LTP and LTD actions. Weight optimization is done using cross entropy loss and back-propagation from the output layer to the target layer. The back-propagation in deep networks affects weight updates by considering global impact of each pixel for accurate classifications. This global learning makes it challenging to impose local constraints during training, leading to the network not ignoring local perturbations during low-level feature extraction. To improve robustness to input noise, low level feature extractors should learn to consider local spatial correlation. The SAFE-DNN approach aims to enhance low-level feature extraction by enabling local learning in neural networks. Unlike conventional DNNs, SNN conductance is not updated globally through backpropagation, but rather through local modulation based on spike timing differences and neighboring spike patterns. This allows the network to effectively ignore noisy pixels and inhibit input noise propagation. The spike patterns of neighboring pre-synaptic neurons control the post-synaptic spike in a Spiking Neural Network (SNN). Spike Timing-Dependent Plasticity (STDP) helps the network learn spatial correlations between pixels. During inference, the network can ignore noise in individual pixels due to learned local correlations. The SNN extracts robust features by focusing on local correlations rather than individual pixels, reducing interference from local perturbations. The network architecture of SAFE-DNN includes spiking convolution modules for robust extraction of low-level features, along with conventional CNN layers for global learning through backpropagation. The SAFE-DNN network architecture combines spiking convolution modules with conventional CNN layers for feature extraction and global learning. The auxiliary CNN module runs in parallel with the spiking convolution module, and their output feature maps are concatenated for input to the main CNN module. The main CNN module, based on existing deep learning models like MobileNetV2, ResNet101, or DenseNet121, handles higher-level feature detection and classification. The network configurations show that SAFE-DNN implementations do not significantly increase storage or computational complexity. In the SAFE-DNN network architecture, spiking convolution modules are combined with conventional CNN layers for feature extraction and global learning. Neurons in the SNN transmit information through spikes, requiring input signal intensity to be converted to spike trains. To adapt the spiking convolution module to a single-time-step response system, the training process involves separating STDP-based learning and DNN training into two stages. The learning algorithm follows a novel frequency-dependent STDP method. In the second stage of training in the SAFE-DNN architecture, network parameters are migrated to the spiking convolution module. The input signal to spike train conversion process is eliminated, and a special activation unit (SAU) is designed to replace the basic spiking neuron model. Batch normalization is added after the convolution layer. The entire SAFE-DNN is then fully trained using statistical methods, while weights in the spiking convolution module remain fixed. Network inference is performed using the SAU for modeling neurons instead of the baseline LIF. The proposed frequency-dependent stochastic STDP algorithm dynamically adjusts the probability of LTP/LTD based on input signal frequency. It addresses the issue of associativity in STDP by controlling the conductance modulation process based on the timing of pre- and post-synaptic spikes. The algorithm utilizes time constant parameters and probabilities of LTP and LTD to determine the strength of the causal relationship between inputs.\u03b3 p and \u03b3 d control the peak values of probabilities in the algorithm. The proposed frequency-dependent stochastic STDP algorithm adjusts LTP/LTD probabilities based on input signal frequency. It addresses associativity in STDP by controlling conductance modulation based on pre- and post-synaptic spike timing. The architecture of the spiking convolutional module resembles conventional DNN but with differences in input processing and connections. FD stochastic STDP shows better learning capability than conventional STDP. The spiking convolution layer utilizes plastic synapses with STDP learning rule. Cross-depth inhibition prevents neurons at the same location from learning the same feature. A layer-by-layer learning procedure is used in multiple-layer SNN to address diminishing spiking frequency. Adjusting neuron thresholds helps increase spiking frequency in the first layer. The spiking convolution layer uses plastic synapses with STDP learning rule and cross-depth inhibition to prevent redundant learning. Neuron thresholds are adjusted to increase spiking frequency in the first layer, facilitating learning behavior in subsequent layers. Spike conversion process in SNN involves converting input values to spike frequency, providing robustness to small input perturbations. The Special Activation Unit (SAU) is designed as a step function to enhance network performance. The Special Activation Unit (SAU) is designed as a step function to enhance network performance by improving local feature extraction. Two SAFEMobileNetV2 models are trained with different synaptic plasticity rules and tested on noisy input, showing better clustering and accuracy with FD stochastic STDP. The comparison between SAFE-DNN architecture and alternative designs like MobileNetV2-\u00b5 and MobileNetV2-\u03bb shows the benefits of SAFE-MobileNetV2 in maintaining good separation in embedding space with clean and noisy images. The networks are trained on CIFAR10 dataset, and results indicate that SAFE-MobileNetV2 outperforms baseline MobileNetV2 in classification accuracy under noisy conditions. Table 2 displays the accuracy of different network variants for CIFAR-10. Noise significantly degrades classification accuracy in baseline DNNs, but training with noise improves robustness. Average filtering boosts accuracy in highly noisy conditions but causes a performance drop in mild to no noise scenarios due to loss of feature details. For SAFE-DNN implemented with various architectures, performance in noisy conditions improves significantly compared to the original network. At 20 dB SNR, SAFEMobileNetV2 shows a 50% gain in accuracy while the original network drops below 40%. Similar trends are seen at other noise levels, with SAFE-DNN performing similarly to networks trained with noise at around 30 dB SNR and outperforming them at higher noise levels. In a test on a subset of ImageNet related to traffic, all networks achieve around 70% accuracy on clean images. Noise training improves robustness but affects clean image accuracy. In noise testing, DensNet121 shows more robustness than MobileNetV2 and ResNet101. SAFE-DNN implementations of all three networks exhibit improved robustness over all noise levels without affecting clean image accuracy. SAFE-MobileNetV2 maintains above 80% accuracy even at 5 dB SNR, outperforming baselines. Random perturbation tests with noise structures like Wald, Poisson, and salt-and-pepper show improved performance of SAFE-DNN. The SAFE-DNN implementation with Wald I, Poisson I, and SP I distributions shows improved noise robustness compared to baseline and average filtering. Performance drops when noise levels are not aligned with training noise, and mis-aligned noise types result in poor performance. Networks based on MobileNetV2 are tested with different noise structures, showing that SAFE-MobileNetV2 is robust without specific noise training. Adversarial perturbation tests are also conducted using DNNs trained with conventional methods as target networks. The study introduces SAFE-DNN, a deep learning architecture that enhances robustness to input perturbations without prior noise knowledge. It integrates spiking convolutional networks with STDP-based learning into conventional DNNs. Results show improved robustness to adversarial perturbations, but not to white-box attacks. Future work includes integrating SAFE-DNN with adversarial training methods. SAFE-DNN enhances robustness to input perturbations and is suitable for real-time autonomous systems in noisy environments."
}