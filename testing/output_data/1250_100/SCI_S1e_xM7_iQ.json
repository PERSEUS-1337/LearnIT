{
    "title": "S1e_xM7_iQ",
    "content": "Parameter pruning is a method for compressing and accelerating CNNs by removing redundant model parameters with tolerable performance loss. A new regularization-based pruning method called IncReg assigns different regularization factors to weight groups based on their importance, proving effective on popular CNNs compared to existing methods. This approach addresses the fragility of CNN expressiveness during pruning. Structured pruning is proposed for CNN acceleration to eliminate redundant model parameters with tolerable performance loss. There are two types of structured sparsity: row sparsity and column sparsity. Two categories of structured pruning include importance-based methods and regularization-based methods. Existing group regularization approaches focus on the regularization form but overlook the impact of the regularization factor, leading to issues with a constant factor for all weight groups in the network. In this paper, a new regularization-based method named IncReg is proposed to incrementally learn structured sparsity in CNNs. The method addresses the issue of weights with larger magnitudes being more important than those with smaller magnitudes during pruning. The objective function for regularization is formulated based on the dimensions of weight tensors in the network. The IncReg method introduces a structured sparsity regularization approach for CNNs, focusing on the importance of weight groups. It utilizes weight decay for regularization and adjusts regularization factors \u03bbg for different weight groups and iterations. The method prunes conv layers simultaneously and independently, with \u03bbg values initialized to zero and increased iteratively. A piece-wise linear punishment function is proposed to determine the regularization increment, rewarding unimportant weights more. The IncReg method introduces structured sparsity regularization for CNNs, focusing on weight group importance. It adjusts regularization factors for different groups and iterations, pruning conv layers simultaneously. Regularization is increased gradually to push weights towards zero, leading to increased structured sparsity when weights fall below a threshold. The IncReg method introduces structured sparsity regularization for CNNs, focusing on weight group importance. It adjusts regularization factors for different groups and iterations, pruning conv layers simultaneously. When the sparsity of a layer reaches its pre-assigned pruning ratio, structured regularization stops. IncReg consistently achieves higher speedups and accuracies compared to other regularization schemes like SSL and AFP on CIFAR-10. The incremental way of regularization allows the network more time to adapt during pruning, especially with large pruning ratios. The method introduces structured sparsity regularization for CNNs, focusing on weight group importance and adjusting regularization factors for different groups and iterations. It achieves higher speedups and accuracies compared to other methods on CIFAR-10 and ImageNet datasets, especially with large pruning ratios and compact networks."
}