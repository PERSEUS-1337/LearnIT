{
    "title": "H1e0-30qKm",
    "content": "In this paper, two novel disentangling methods are introduced: Unlabeled Disentangling GAN (UD-GAN, unsupervised) and UD-GAN-G (weakly supervised). UD-GAN decomposes latent noise by generating similar/dissimilar image pairs and learns a distance metric with siamese networks. UD-GAN-G modifies UD-GAN with user-defined guidance functions to focus on desired semantic variations. Both methods outperform existing unsupervised approaches in measuring semantic accuracy of learned representations. Simple guidance functions in UD-GAN-G help capture desired data variations. In this paper, two novel disentangling methods are introduced: Unlabeled Disentangling GAN (UD-GAN, unsupervised) and UD-GAN-G (weakly supervised). UD-GAN generates image pairs, embeds them with Siamese Networks, and learns disentangled representations without using labeled data. The Unlabeled Disentangling GAN (UD-GAN) generates image pairs and embeds them with Siamese Networks to learn a distance metric on a disentangled representation space. Another method, UD-GAN-G, uses guidance functions to capture desired semantic variations in the input to Siamese networks. Various studies have explored disentangled representations in generative models, with different approaches based on the level of supervision required. In BID20 and BID11, techniques are used to disentangle pose, light, and shape of objects by manipulating hidden units and synthesized images. InfoGAN, \u03b2-VAE, and DIP-VAE are unsupervised methods that discover categorical and continuous factors in image generation by maximizing mutual information and encouraging disentanglement in latent representations. Our method builds on existing approaches by operating on pairs of similar/dissimilar image pairs and computing image embeddings using separate networks. We use a GAN framework where the generator maps a latent variable to images and the discriminator distinguishes real from generated images. Training is done as a minimax game, and we modify the generator loss for stability. The paper introduces a method that slices the latent variable into multiple vectors to control different semantic variations in image generation. The network architecture involves using latent vectors to represent various attributes for disentanglement. Training the model involves sampling different vectors for each attribute and ensuring that similar image pairs have the same attribute using Siamese Networks. The paper utilizes Siamese Networks to map image pairs into a representation space and learns a distance metric using Contrastive Loss. A guidance function can restrict information going into the network for a desired representation space. Contrastive Loss is used to pull similar image pairs together and push dissimilar pairs apart, with an adaptive margin for semantic meaning. The discriminator network remains unchanged to separate real and generated image distributions. Our method utilizes latent variables to create similar and dissimilar image pairs without the need for labels. The final loss function includes a GAN loss, embedding loss weight, and guidance function to capture specific variations in the input. For example, by guiding the network to focus on \"Hair Color\" attributes in the CelebA dataset, we can control the information flow into the siamese network and capture only desired variations in the embedding space. The knob q top changes hair color globally due to the interplay of adversarial and contrastive losses. Different guidance functions capture variations in the CelebA dataset. A probabilistic interpretation is illustrated with colored polygons and independent factors of shape and color controlled by knobs q i and q j. The analysis involves decomposing attribute probabilities into discrete mixture distributions for shape and color. The contrastive loss aims to minimize the spread of each mixture component to achieve disentanglement in the representation space. Training involves minimizing the spread of embedding spaces to achieve disentanglement. The text discusses minimizing the spread of embedding spaces to achieve disentanglement in representation space. It focuses on separating different shapes in the embedding space using an adaptive margin, and maximizing divergence between probability distributions over image embeddings. The energy distance is used as a measure, resembling the Contrastive Loss. The text discusses minimizing embedding spread for disentanglement, using an adaptive margin to separate shapes. It maximizes divergence between image embedding distributions, similar to Contrastive Loss. Experiments are conducted on CelebA and 2D Shapes datasets, with details on hardware, architecture, and optimization parameters provided. The text discusses using a 32 and 10-dimensional latent variables for CelebA and 2D Shapes datasets, respectively. Two versions of the algorithm are compared: UD-GAN without guidance and UD-GAN-G with guided training. Comparison is made against \u03b2-VAE, DIP-VAE, and InfoGAN. Guidance is provided for the first 28 latent knobs in the CelebA dataset. The text discusses using 32 and 10-dimensional latent variables for CelebA and 2D Shapes datasets, respectively. The algorithm compares UD-GAN without guidance and UD-GAN-G with guided training against \u03b2-VAE, DIP-VAE, and InfoGAN. Guidance is provided for the first 28 latent knobs in the CelebA dataset. The disentanglement metric scores for different methods are illustrated in TAB0, showing that both UD-GAN versions outperform the baseline on the CelebA dataset. Our guided network (UD-GAN-G) outperforms the baseline on the CelebA dataset by relating similarities/differences of latent variable pairs to image pairs, providing a discriminative image embedding. The guidance used in our method prevents cluttering with irrelevant attributes, resulting in better disentanglement and improved scores. The disentanglement scores for the 2D shapes dataset are high due to simple synthetic images and highly correlated guidances chosen. Our guided approach outperforms baseline methods on CelebA attribute classification accuracy by isolating spatially related attributes through cropping. The method captures attributes like hair and mouth better, but struggles with attributes like \"Bangs\" due to heuristic cropping. Images generated by different methods on CelebA dataset illustrate changes in semantic properties like smile, azimuth, and hair color. The DIP-VAE method generates blurrier images compared to adversarial methods like InfoGAN and UD-GAN-G, due to the data likelihood term in VAE-based approaches. The guided approach provides better control over captured attributes and allows for direct manipulation of attributes like smile, azimuth, and hair color. The guided approach in FIG1 allows direct manipulation of attributes using knobs q bot, q mil, and q top. Table 3 and Table 4 show results for varying latent dimensions in the 2D Shapes dataset. UD-GAN-G captures desired features with weak supervision, avoiding the need for multiple model trainings. In the aligned CelebA dataset, faces are centered around the nose, simplifying guidance design. A pre-trained object detection method like YOLO can be used for complex scenarios. Backpropagating gradients into an image may lead to adversarial samples, but a discriminator can help. Differentiable relaxations can guide the network, such as superpixel segmentation for low-level image segmentation. In this paper, UD-GAN and UD-GAN-G are introduced as novel GAN formulations using Siamese networks with contrastive losses to disentangle image segmentation. The experiments show that the method outperforms current state-of-the-art methods on CelebA and 2D Shapes datasets, allowing for semantically meaningful image manipulation. Future work will focus on investigating more powerful embedders. In future work, more powerful embedders will be explored for semantic segmentation and landmark detection. The neural network layers used in the generator for different datasets are shown in TAB2. Siamese Networks aim to map images into embedding spaces for grouping within distinct semantic contexts. Disentangling shape and color may not be directly achievable in an unsupervised setting, but small assumptions and domain knowledge can still benefit from the disentangling capability without labeled data. The text discusses the challenge of disentangling shape and color attributes in images. By modifying the network architecture, the shape and color variations can be separated. This is achieved by feeding the average color of randomly sampled pixels to one network, allowing it to focus solely on color. After training, one network captures the shape of a digit while the other captures color variations. This separation is visualized using t-SNE to show embedding spaces for shape and color. The text discusses using t-SNE to visualize embedding spaces for shape and color in images. Three experiments are conducted on the MS-Celeb dataset using guided siamese networks with different proxies. The first experiment shows that the guided network captures the outline of the face, while the unguided network modifies the image with minimal changes. The second experiment guides the second knob with the average color, resulting in disentangled image manipulation. The third experiment employs cropped guidance networks for further exploration. In the third experiment, cropped guidance networks are used with two knobs adjusting the top and bottom parts of the image. The knobs control specific facial features like hair, eyes, chin, and mouth shape. Siamese networks for the 2D shapes dataset are guided by estimating the center of mass and object size. These guidances are highly correlated with ground truth attributes. Additional semantic properties captured by UD-GAN-G are illustrated in FIG5. Classification performance is compared to InfoGAN on all attributes in the CelebA dataset in TAB5. In the CelebA dataset, correlation between embedding dimensions and attributes is compared. DIP-VAE encodes uncorrelated representation but doesn't always lead to disentangled semantic representation."
}