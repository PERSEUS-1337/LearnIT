{
    "title": "SJg9z6VFDr",
    "content": "Recently, a new model called Graph Ordinary Differential Equation (GODE) has been proposed for graph data, inspired by neural ordinary differential equations (NODE) for Euclidean data. GODE uses continuous-depth models and two efficient training methods. It outperforms existing graph networks and can be easily adapted to different graph neural networks, improving accuracy in various tasks. The GODE model is efficient for graph data tasks, offering accurate gradient estimation and generalizability. Unlike CNNs limited to grid data like images and text, graphs represent objects as nodes and relations as edges, making them suitable for irregularly structured datasets like social networks and knowledge graphs. Traditional methods for graph modeling have lower expressive capacity compared to GODE. Graph neural networks (GNN) have been proposed as a new class of models to improve the performance of graphs. There are two main types of methods for convolution on a graph: spectral and non-spectral. Spectral methods compute the graph Laplacian for filtering, while non-spectral methods directly perform convolution in the graph domain. Existing GNN models have discrete layers, making it challenging to model continuous diffusion processes in graphs. In this work, the authors propose graph ordinary differential equations (GODE) to model message propagation on graphs as an ODE. They address the limitations of NODEs in image classification tasks by improving gradient estimation during training. Their memory-efficient framework for accurate gradient estimation generalizes to various model structures and achieves high performance. The framework proposed for free-form ODEs generalizes to various model structures and achieves high accuracy for both NODE and GODE in benchmark tasks. Key contributions include accurate gradient estimation for deep-learning models, memory-efficient implementation for free-form ODEs, generalization to graph data with GODE models, and improved performance on different graph models and datasets. Previous works have explored neural networks as differential equations, with NODE treating neural networks as continuous ODEs. The curr_chunk discusses the training of NODE and GNNs, highlighting the inaccurate gradient estimation issue in NODE. It also introduces spectral and non-spectral GNNs, explaining their differences in filtering and computation requirements. The curr_chunk discusses various approaches to graph convolution, including spectral and non-spectral methods such as Chebyshev expansion, localized filters, MoNet, GraphSAGE, and graph attention networks. These methods aim to improve the efficiency and performance of graph convolution on graph data. Invertible blocks are utilized in various neural network models, such as normalizing flow and invertible networks. They enable accurate reconstruction of inputs from outputs, allowing for memory-efficient structures and back propagation without storing activations. This approach has been applied in discrete-layer models with residual connections. Neural ordinary differential equations (NODE) represent hidden states using z(t) in the continuous case and x_k in the discrete case. The forward pass of a model with discrete layers involves applying an output layer after K layers. In contrast, a NODE's forward pass integrates states z over time T and applies an output layer on z(T). Integration in the forward pass can be done using any ODE solver. The adjoint method is commonly used in optimal process control and functional analysis. Model parameters are denoted as \u03b8, which are time-independent. The adjoint is defined and compared with direct back-propagation through ODE solver in Figure 1. The reverse-time solution in adjoint method can be numerically unstable, causing errors in gradient calculation. During the backward pass, the computation graph is rebuilt by evaluating at the same time points, accurately reconstructing the hidden state for precise gradient evaluation. Gradient descent is then performed to optimize \u03b8 and minimize the loss function. The reverse-time integration in Eq. 6 can be solved with any ODE solver, but storing z(t) during the forward pass requires significant memory consumption. In summary, the forward pass solves Eq. 2 forward in time, while the backward pass solves Eq. 2 and 6 in reverse, with initial conditions determined from Eq. 5 at time T. The reverse-time ODE solver can lead to inaccurate gradients in adjoint methods. The reverse-time ODE solver can lead to inaccurate gradients in adjoint methods due to instability, which affects the accuracy of the computed gradient. To address this, it is proposed to directly back-propagate through the ODE solver. To address inaccuracies in gradient computation, direct back-propagation through the ODE solver is proposed. This involves accurate reconstruction of hidden states using two methods: saving activation values or rebuilding the computation graph at evaluated time points. The adjoint for discrete forward-time ODE solution is defined, ensuring accurate back-propagation regardless of stability. Detailed derivations and an algorithm for gradient estimation in ODE solver for free-form functions are provided. The ODE solver for free-form functions involves forward and backward passes with adaptive step sizes and error estimation. During the forward pass, numerical integration is performed, and time points are saved. In the backward pass, the computation graph is rebuilt at saved time points for accurate back-propagation. The algorithm for ODE solver involves rebuilding the computation graph without adaptive searching, performing reverse-time integration during the backward pass, and supporting free-form continuous dynamics. It offers a memory-efficient method with reduced memory consumption by deleting middle activations during the forward pass and not needing to search for an optimal step size in the backward pass. Additionally, using invertible blocks further reduces memory consumption. The algorithm for ODE solver offers a memory-efficient method by using invertible blocks, reducing memory consumption by not storing activations and allowing for O(N f ) memory consumption. The forward and inverse of a bijective block can be denoted as (y 1 , y 2 ) with the same size as (x 1 , x 2 ), where F and G are differentiable neural networks.\u03c8(\u03b1, \u03b2) is a differentiable bijective function w.r.t \u03b1 when \u03b2 is given, making the block defined by Eq. 8 a bijective mapping. This approach eliminates the need to store activations, making it memory-efficient. Graph neural networks are introduced with discrete and continuous layers, extending to graph ordinary differential equations (GODE). Nodes and edges represent a graph, with nodes colored for visualization. GNNs use a message passing scheme, with functions parameterized by neural networks. A GNN model consists of message passing, message aggregation, and node update stages. The aggregation function in graph neural networks is typically permutation invariant, using operations like mean and sum. The states of a node are updated based on its original states and message aggregation. Graph ordinary differential equations (GODE) can convert discrete-time GNNs to continuous-time GNNs, capturing highly non-linear functions. GODE's asymptotic stability is related to over-smoothing phenomena. Graph convolution is a special case of Laplacian smoothing, with a modified equation for input and output layers. The continuous smoothing process in graph neural networks is achieved through graph ordinary differential equations (GODE). The GODE model ensures asymptotic stability, leading to over-smoothing phenomena. Experimental results show that with a large enough integration time, nodes from different classes will have similar features, causing a drop in classification accuracy. The method was evaluated on various benchmark datasets including image classification tasks and graph datasets. GODE is applied to various graph neural networks for graph classification tasks without pre-processing the raw dataset. It can be easily generalized to existing structures by replacing functions in the model. The study compared different graph neural network models (GCN, GAT, ChebNet, GIN) with varying depths of layers for graph and node classification tasks. Hyper-parameters like channel numbers and number of hops were set consistently for fair comparison. The performance of each model was evaluated based on accuracy over 10 runs, and the comparison between adjoint method and direct back-propagation was demonstrated. The study compared different graph neural network models for graph and node classification tasks. Direct back-propagation outperformed the adjoint method on image classification tasks and benchmark graph datasets. NODE18 showed better performance than ResNet18 and ResNet101 with the same number of parameters. The method also demonstrated robustness to ODE solvers of different orders. During inference, using different solvers is equivalent to changing model depth without re-training the network. Our method supports NODE and GODE models with free-form functions, demonstrating the effectiveness of GODE models over their discrete-layer counterparts.\u03c8 functions can be easily generalized, with most GODE models outperforming their corresponding discrete-layer models significantly. The effectiveness of GODE models over discrete-layer models is validated, with continuous-time models being more important than coupling function \u03c8. Different structures like GCN, ChebNet, and GIN were experimented with, showing GODE models performing significantly better. Integration time in GODE models affects information gathering and over-smoothing issues, leading to accuracy drops. We propose GODE to model continuous diffusion process on graphs, with a memory-efficient back-propagation method for gradient estimation. Our paper addresses the gradient estimation problem for NODE, improving accuracy on benchmark tasks. Experiments are conducted on various datasets, including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained with modifications for bijective mapping. The study by Gomez et al. (2017) is generalized to include a family of bijective blocks with different functions \u03c8. A parameter state checkpoint method is proposed to enable multiple calls of bijective blocks while maintaining accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch. Memory consumption is reduced by keeping only necessary outputs in the forward function. The efficiency of the bijective block is demonstrated through training a GODE model and comparing memory consumption with a memory-inefficient method. The study introduces a memory-efficient method for bijective blocks in training GODE models. By storing only necessary outputs in cache, memory consumption is significantly reduced compared to conventional methods. The method is demonstrated to be efficient through experiments on the MUTAG dataset, showing minimal increase in memory usage with increasing depth of ODE blocks. The study presents a memory-efficient approach for bijective blocks in training GODE models, reducing memory consumption by storing only necessary outputs. Experimental results on the MUTAG dataset show minimal memory usage increase with deeper ODE blocks. Theorem 1 states that a bijective block with forward and reverse mappings is bijective. The proof involves showing injectivity and surjectivity of the mappings. The text discusses the bijective property of mappings in GODE models, demonstrating injectivity and surjectivity. It includes a computation graph for gradient derivation and extends from continuous to discrete cases. Notations and problem setup are defined for neural-ODE models. The text discusses the optimization problem setup for neural-ODE models, using the Lagrangian Multiplier Method to solve it. The Karush-Kuhn-Tucker (KKT) conditions are derived as necessary for optimality. The text discusses deriving the derivative using calculus of variation for a function of t. The conditions for Leibniz integral rule are checked, leading to the optimal condition dz(t)/dt - f(z(t), t, \u03b8) = 0. In discrete cases, the ODE condition is transformed into a finite sum equation."
}