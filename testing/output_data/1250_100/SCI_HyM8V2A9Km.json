{
    "title": "HyM8V2A9Km",
    "content": "Sparse reward is a challenging problem in reinforcement learning. Hindsight Experience Replay (HER) addresses this by converting failure experiences into successful ones. However, HER has limited applicability due to a lack of a universal goal representation. Augmenting experienCe via TeacheR's adviCE (ACTRCE) extends HER using natural language as the goal representation. ACTRCE efficiently solves difficult reinforcement learning problems in 3D navigation tasks, where HER fails. Language goal representations allow the agent to generalize to unseen instructions and lexicons. Hindsight advice is crucial for solving challenging tasks, but a small amount is sufficient for learning to progress practically. The practical aspect of deep reinforcement learning applications relies on carefully-crafted reward functions. Designing a good reward function is challenging and can lead to biased learning. Sparse reward functions make learning difficult, but Hindsight Experience Replay (HER) addresses this by converting failed experiences into successful ones. HER assumes a goal exists for every state in the environment. The assumption in HER is that there is a goal for every state in the environment. Representing goals using the state space is inefficient and redundant. Language representation of goals is expressive, flexible, and compact, allowing for the compression of redundant information in states. In this paper, the HER framework is combined with natural language goal representation to propose the ACTRCE technique for reinforcement learning. A teacher provides advice in natural language to the agent after each episode, helping to alleviate the sparse reward problem. Language goal representation enhances the agent's ability to solve reinforcement learning problems in challenging environments and generalize to unseen instructions and lexicons. The use of hindsight advice is crucial for success. Our work combines reinforcement learning with rich language advice to efficiently tackle language grounding problems. By using hindsight advice, even a small amount can significantly aid learning. This approach is crucial for achieving goals described in natural language, a key aspect of language grounding. The text discusses reinforcement learning with a focus on the Bellman equation and Q-learning algorithm. It explains the components of the algorithm, such as state space, action space, discount factor, transition model, and reward function. The goal is to maximize the expected cumulative return by choosing actions based on a policy. The optimal policy is denoted as \u03c0* and the optimal Q-function satisfies the Bellman equation. Q-learning is an off-policy, model-free algorithm based on the Bellman equation and uses semi-gradient descent. The algorithm discussed is Deep Q-Network, which uses a neural network to approximate Q * and minimizes the squared Temporal Difference error. It incorporates a replay buffer and target network for training stability. The reinforcement learning framework includes a goal space G, where a goal g is chosen for each episode, inducing a reward function r g. The agent's objective is to maximize the expected discounted cumulative return given the goal. The algorithm discussed is Deep Q-Network, which uses a neural network to approximate Q * and minimizes the squared Temporal Difference error. It incorporates a replay buffer and target network for training stability. The reinforcement learning framework includes a goal space G, where a goal g is chosen for each episode, inducing a reward function r g. The agent's objective is to maximize the expected discounted cumulative return given the goal. In many scenarios, the reward function is very sparse, making it difficult for an agent to learn. HER proposes a solution to this problem by allowing the agent to learn from goal-transformed experiences. The algorithm Deep Q-Network uses a neural network to approximate Q * and minimize the squared Temporal Difference error. It includes a replay buffer and target network for training stability. The reinforcement learning framework involves a goal space G, where a goal g is chosen for each episode, inducing a reward function r g. The objective is to maximize the expected discounted cumulative return given the goal. HER proposes a solution to sparse reward functions by allowing the agent to learn from goal-transformed experiences. In goal-oriented MDPs, the goal representation mapping T : S \u2192 G describes what goal is satisfied at each state. The text discusses using natural language to represent the goal space in reinforcement learning. A teacher provides advice in the form of a goal description for each state, helping to convert failure trajectories into successful ones. This approach reduces redundancy in goal representation and allows for positive experiences to be gained from both positive and negative feedback. In reinforcement learning, natural language is used to represent the goal space. Teachers provide advice with goal descriptions for each state, converting failure trajectories into successful ones. This approach reduces redundancy in goal representation and allows for positive and negative feedback to be beneficial. In reinforcement learning, natural language is used to represent the goal space. Teachers provide advice with goal descriptions for each state, converting failure trajectories into successful ones. This approach involves a group of teachers giving different advice, relabeling the original goal with each advice and corresponding reward, and augmenting the replay buffer with these trajectories. The approach explores converting a language goal into a continuous vector representation for neural network training using one-hot vectors and recurrent neural networks. In reinforcement learning, natural language is utilized to represent the goal space. Teachers provide advice with goal descriptions for each state, converting failure trajectories into successful ones. The approach involves converting a language goal into a continuous vector representation for neural network training using recurrent neural networks. To integrate language representation of goals into the model, the architecture consists of 3 modules: a language component, an observation processing component using convolution neural networks, and gated attention to fuse goal information and observation. The proposed method integrates language goal representations into neural network training using recurrent neural networks. It compares different goal representations and demonstrates the effectiveness of pre-trained word embeddings in generalizing to out-of-training vocabulary words. The proposed method integrates language goal representations into neural network training using recurrent neural networks. Significant improvement in sample efficiency is shown by using advice from teachers, even with limited amount of advice. Tested in 2D and 3D environments, the method demonstrates low burden in practice. The state is a raw pixel image from first person perspective of a room in the game Doom with various objects. The goal is to follow natural language instructions like \"Go to the green torch\". Different tasks are composed of singleton tasks using \"and\" and \"or\" combinations. DQN algorithm is used for reinforcement learning in both environments. In Appendix C, details of experiments are provided. Multi-environment training involved sampling 16 environments, collecting data, updating agents with average gradient, and resampling environments. Appendix D contains further training details. The study compares language-based goal representations with non-language ones, showing that language representations are more effective as task difficulty increases. Language goal representations also enable generalization to unseen goals in training, showcasing robustness with pre-trained sentence embeddings. Three goal representations are described, including Language Sentence Representation with GRUs. We use one hot vectors to represent words and embed them into a GRU for instruction representation. The GRU has a hidden size of 256 and can be pre-trained using InferLite BID16. InferLite is a sentence encoder trained for natural language inference without RNNs. The original sentence embedding is 4096 dimensions, but we project it down to 256. The non-language baseline uses one-hot vectors for instructions. All three goal representations are used for learning singleton and compositional tasks with hindsight advice. The study compares different goal representations in learning tasks in ViZDoom environment. One-hot representations perform well in singleton tasks but struggle in compositional tasks. Language representations show high success rates in compositional tasks and better generalization ability. GRU language goal representation achieves 83% success in zero-shot learning. The visualization analysis compared the learned embeddings of goals in the ViZDoom environment. Different goal representations showed varying correlations, with GRU and InferLite embeddings having similar structures while one-hot goal embeddings had no correlation. t-SNE embeddings revealed meaningful clustering for language goals and sporadic embeddings for one-hot goals. Pre-trained embeddings were used to generalize to unseen lexicons at test time. The use of pre-trained embeddings allows the model to generalize to unseen lexicons at test time. By replacing words with synonyms, the agent achieves tasks above 66% of the time. Understanding synonyms improves learning robustness in noisy settings. Hindsight advice is crucial in learning, as demonstrated in the effectiveness of language goal representation. Our method \"ACTRCE\" compared to DQN algorithm without hindsight advice. DQN struggled with challenging tasks, but even 1% advice improved learning significantly. Recurrent neural networks used for language representation. Successful results in different grid environments and ViZDoom configurations. In challenging environments, the ACTRCE agent outperformed the DQN baseline in learning tasks with 7 objects. Multitask and Zero-Shot generalization performance was summarized in Table 1. In KGW grid experiments, ACTRCE showed efficient learning compared to DQN. ViZDoom compositional tasks with 5 objects also demonstrated ACTRCE's superior performance. The ACTRCE agent outperformed the baseline DQN in learning tasks with 7 objects, showing efficient learning in challenging environments. The agent was able to learn well with hindsight advice, even with minimal (1%) advice given, demonstrating robustness in practical settings. Previous approaches have utilized natural language advice to improve RL agent performance. BID17 and BID21 used natural language feedback to enhance RL agents in soccer and image captioning tasks. BID13 developed an agent using English instructions for self-monitoring in Atari games. BID0 proposed language as a latent space for few-shot learning and policy search. Task-oriented language grounding was explored by Misra et al. and Yu et al. in manipulating blocks and maze navigation tasks, respectively. Our work extends previous research on using natural language feedback to enhance RL agents by proposing the ACTRCE method, which utilizes natural language as a goal representation for hindsight. This builds on techniques such as experience replay and gated-attention architectures for executing written instructions in a 3D environment. The ACTRCE method utilizes natural language as a goal representation for hindsight advice in reinforcement learning. It efficiently solves challenging 3D navigation tasks and can generalize to unseen instructions with a pre-trained language component. The algorithm relies on hindsight advice, but only a small amount is needed for learning to be effective. The KrazyGrid World environment used in experiments includes tiles with different functionalities and colors, along with four discrete actions for the agent. In the KrazyGrid World environment, the goal is to reach different colored goals using global grid state observations. The grid includes tiles with functionality and color attributes represented in one hot vectors. The environment consists of a 9x9 grid with 3 distinct colored goals and various lava obstacles. Episodes end when the agent reaches a goal or lava, or after a set number of time steps. The goal space is expanded to include compositions of goals, leading to modifications in the environment and episode termination conditions. The ViZDoom environment is a 3D learning environment based on the game Doom. The agent navigates using turn left, turn right, and move forward actions to reach a specified object goal within 30 time steps. An additional \"flag\" action allows the agent to end the episode early if it believes the goal is achieved. In the ViZDoom environment, the agent navigates to reach a specified object goal within 30 time steps. The environment has easy and hard modes, with different object and agent distributions. Compositional instructions consist of two single object instructions joined by \"and\", ensuring unambiguous instructions. The ViZDoom environment requires the agent to navigate to specific objects within a limited time frame. Compositional instructions are designed to be unambiguous by combining two single object instructions. The HUD displays thumbnail images of reached objects, with the episode ending once two objects are reached. Synonym instructions are generated by replacing words with synonyms. Positive and negative feedback is provided based on the agent's interactions with objects. In the ViZDoom environment, agents navigate to specific objects within a time limit. Compositional instructions combine single object instructions. Positive and negative feedback is given based on the agent's interactions with objects, including randomly selecting unreached objects for negative feedback. Different strategies are used based on the number of objects reached during the episode. In the ViZDoom environment, agents navigate to specific objects within a time limit. Compositional instructions combine single object instructions. Positive and negative feedback is given based on the agent's interactions with objects, including randomly selecting unreached objects for negative feedback. Different strategies are used based on the number of objects reached during the episode. Instructions are processed using convolution layers and LSTM for language input. In the ViZDoom environment, language sentences are processed using convolution layers and LSTM. The observation is preprocessed with ReLU activation functions and augmented with a history vector. Two more convolution layers with ReLU activation functions are applied before passing through fully connected layers to predict action values. The architecture is similar to BID4, with a linear output layer for action values and no dueling architecture used. The state input is an RGB image. The network architecture for processing language input in the ViZDoom environment includes convolution layers, GRU, attention mechanism, and LSTM. Hyperparameters for KrazyGrid World experiments are tuned for learning rate, replay buffer size, and training frequency. The ViZDoom environment utilizes Double DQN with a replay buffer size of 5000 or 10000. Training the network occurs every 1, 2, or 4 frames. Episodes are generated using an \u03b5-greedy policy starting at 1.0 and decaying linearly to 0.01 by 10000 frames. The network uses Huber loss for stable gradients. The cyclic buffer replay buffer contains the last 10000 and 20000 most recent transitions for easy and hard modes, respectively. Training begins after the first 1000 frames have been collected. The training in ViZDoom starts after collecting 1000 frames. Double DQN is used to reduce Q-value overestimation with Huber loss for stable gradients. The Adam optimizer with a learning rate of 0.0001 is utilized. The network is updated every 4 frames on easy mode and 16 frames on difficult mode. Sampling from the replay buffer involves selecting 32 consecutive frames from a random episode. Sequential mini-batches allow for n-step Q learning. 16 parallel threads are used to alleviate sample correlation. The target network is synchronized with the current model every 500 time steps. In this section, the details of teacher types are described. A subset of desired goals is denoted as Gd, and a goal g is sampled from this set for the agent to explore. Advice is obtained from a teacher T based on the terminal state. Three types of teachers are considered: Optimistic, Knowledgeable, and another type that gives no advice. The Optimistic teacher provides advice only when a desirable goal is achieved, while the Knowledgeable teacher describes the agent's achievements in all scenarios. The teacher types are described, with a focus on how they provide advice to the agent based on its achievements. Different methods of giving advice are compared, including the ACTRCE method and the DQN algorithm. Results from testing on KrazyGrid World with different environments show that ACTRCE quickly learns and achieves good results, outperforming the baseline DQN. After achieving good results on KrazyGrid World, ACTRCE struggled with increased lava obstacles, dropping from 83% to 63% performance. However, with knowledgeable teachers providing language advice, ACTRCE maintained a similar learning rate in more difficult settings, outperforming ACTRCE \u2212 by 17%. The study explores whether learning easier tasks can signal success in more challenging tasks in the KGW setting. In a transfer learning experiment, agents were pretrained with a pessimistic teacher in the KGW setting. Despite different goals in pretraining, the agents learned faster than unpretrained ones. Pretrained agents performed as well as ACTRCE in environments with 6 lavas, showing that learning easier goals can aid in mastering more difficult tasks. In experiments on ViZDoom, DQN and ACTRCE were compared on easy and hard tasks with 5 objects. DQN struggled with consistency on the hard task, while ACTRCE showed low variance. A3C had lower sample efficiency than DQN/ACTRCE on the easy task and failed to reproduce results on the hard task. The average episode length was reported for each task. During training in ViZDoom scenarios, the average episode length decreased with ACTRCE but remained flat with baseline DQN for harder tasks. A cumulative success rate curve showed increasing success with episode length, indicating model performance. The Multi-task cumulative success rate for 3 ViZDoom tasks using GRU hidden state language encoding is shown in FIG0. In the 5 objects hard mode, ACTRCE outperformed baseline DQN after episode length 20. In the 7 objects hard mode, ACTRCE maintained performance while DQN only succeeded on short episodes. For the 5 objects composition task, ACTRCE had two groups of trajectories based on proximity of target objects. For the 5 objects composition task, ACTRCE had two groups of trajectories based on proximity of target objects, requiring careful navigation to reach the second target."
}