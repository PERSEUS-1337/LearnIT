{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. A new method using \\textit{M-of-N} rules was proposed to map the complexity/accuracy landscape of rules describing hidden features in a Convolutional Neural Network (CNN). Experiments show an optimal trade-off between comprehensibility and accuracy, with rules in the first and final layers being highly explainable. The results highlight the feasibility of rule extraction from deep networks, emphasizing the value of decompositional knowledge extraction for explainability in Artificial Intelligence. Recent interest in explainable AI contrasts with the lack of explainability in neural network models, particularly deep networks, which rely on distributed representations. Unlike symbolic AI or symbolic Machine Learning, distributed representations may not correlate with easily identifiable features, making knowledge extraction challenging. Knowledge extraction aims to enhance the explainability of neural networks by revealing the implicit knowledge learned in their weights. Techniques involve translating networks into symbolic rules or decision trees, with decompositional and pedagogical approaches used. The challenge lies in the complexity of the extracted rules, which may not always be more understandable than the original network. The distributed nature of neural networks makes knowledge extraction challenging, as important concepts are represented by patterns of activity over many neurons. Some argue that explaining latent features using symbolic knowledge extraction is a dead end, and suggest adopting methods like distillation instead. Distillation is also proposed for improving robustness, but its efficacy is debated. Other practical approaches focus on guarantees of network behavior or visualizations for explaining individual classifications. In this paper, a method is developed to examine the explainability of latent variables in neural networks by using rule extraction to describe the variables. The rule extraction landscape shows the trade-off between rule complexity and accuracy in capturing network behavior. When applied to a CNN trained on fashion MNIST, some layers have accurate rules while others do not, indicating an ideal rule for each latent variable. The explainability trends vary between layers and architectures. The study explores rule extraction in neural networks, showing a trade-off between rule complexity and accuracy. Different layers in a CNN trained on fashion MNIST exhibit varying levels of accuracy in rule extraction. Previous algorithms for knowledge extraction are briefly discussed before presenting experimental results on the accuracy/complexity landscape. The study discusses rule extraction in neural networks, focusing on algorithms that extract M-of-N rules from activated neurons. These algorithms generate binary trees representing rules, with recent methods selecting rules based on maximum information gain. Some methods treat the model as a black box for rule extraction, while others use alternative visually oriented techniques. Various methods have been developed to address the 'black-box' problem of neural networks. Methods developed to address the 'black-box' issue in neural networks can be found in BID8. While most rule extraction techniques have focused on shallow networks and input/output relationships, some have attempted to explain the latent features of deep networks. However, explaining arbitrary hidden features in deep networks with multiple layers can lead to complex rule hierarchies that are difficult for humans to understand. Despite this challenge, experiments show that certain layers in deep networks may have explainable rules that can clarify the network's behavior in terms of specific features. This suggests that rule extraction could be used for modular explanations in networks. The possibility of rule extraction as a tool for explaining network models and understanding latent features by comparing optimal extracted rules is defined formally. Logical rules in programming are explained as implications A \u2190 B, with A as the head and B as the body. Neurons' states are represented by literals, with binary values as True or False, and continuous values with a threshold. Rule extraction can help clarify neural network behavior in terms of specific features. In neural networks, a latent variable is described by M-of-N rules, which soften the conjunctive constraint on logical rules. M-of-N rules offer a compact representation reflecting input/output dependencies and are a subset of all propositional formulas. They provide a more general representation than a conjunction and are useful for rule extraction in neural networks. The paper discusses M-of-N rules as a way to explain neural networks, comparing them to weightless perceptrons. These rules have been used for knowledge extraction in the past but are now being revisited for explainability. When dealing with continuous activation values, splitting values are chosen for each neuron to define literals for rule extraction. The paper discusses using M-of-N rules to explain neural networks by generating literals for target neurons based on information gain. Input literals are then generated by choosing splits that maximize information gain with respect to the target literal. This process results in a single rule for each feature map in a layer, rather than a collection of rules. The paper discusses rule extraction in neural networks, focusing on accuracy and comprehensibility. Accuracy is measured by the difference between rule predictions and network outputs, while comprehensibility is subjective. Rules are used to predict network outputs, and discrepancies are measured over a test set. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF). For an M-of-N rule, complexity is calculated as M N M. Complexity is normalized relative to a maximum complexity, which is determined by the number of possible input variables. The normalized complexity measure is obtained by taking the logarithm. An example is given with a perceptron and a rule h = 1 \u21d0\u21d2 1-of{x 1 = 1, \u00ac(x 2 = 1)}, showing an error of 0.25. A 1-of-2 rule is the most complex for 2 variables. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF). A 1-of-2 rule is the most complex for 2 variables, with a weighted sum loss function defined by a parameter \u03b2. By using a brute force search procedure with various values of \u03b2, the relationship between rule complexity and accuracy is explicitly determined. Neurons and splits are generated to create a set of literals, and M-of-N rules are searched through to find the minimum loss rule. The complexity of rules is determined by the length of the body in disjunctive normal form (DNF). A search procedure is used to find M-of-N rules with variables in the body and head that minimize the loss function. Neurons and splits are generated to create literals, and rules are reordered based on the weight magnitude. The search procedure relies on the ordering of variables and the assumption that accurate rules use literals corresponding to neurons with the strongest weights. The search procedure for extracting rules from neural networks relies on ordering literals by weight magnitude to find accurate rules. The algorithm was implemented in Spark on IBM cloud services to handle a large number of test examples and input neurons efficiently. The accuracy of the extracted rules was evaluated using examples from the training set to measure their performance with respect to the network's output. By running the search in parallel, we can map the accuracy/complexity graph for about 50 hidden neurons in the second and third layer in several hours. To demonstrate the procedure, we examine the extraction process for the first hidden feature in the CNN trained on the fashion MNIST dataset. Selecting 1000 random input examples from the training set, we compute the activations of each neuron in the CNN and the predicted labels of the network. Each neuron corresponds to a different 5x5 patch of the input, with 784 neurons per feature in the first layer. The optimal splitting value of each neuron is found by computing the information gain with respect to the network's predicted labels. The neuron with the maximum information gain for the first feature is neuron 96, with an information gain of 0.015 when split on the value 0.0004. This neuron corresponds to the image patch centered at (3, 12). Using this split, the variable H is defined as 1 if h 96 \u2265 0.0004. Input splits are defined by choosing values that result in maximum information gain with respect to H. The search through M-of-N rules determines an optimal rule explaining H for different error/complexity tradeoffs, resulting in three different rules visualized in Figure 1. The input feature is not included in the M-of-N rule, with white indicating a positive literal and black indicating a negative literal. The most complex rule is a 5-of-13 rule with a 0.025 error, agreeing with the network 97.5% of the time. Adding penalties to complexity results in simpler rules with higher errors. Applying the technique to the DNA promoter dataset shows an exponential relationship between complexity and error in the first layer. In the output layer, the rule 1-of-{H39, H80} gives 100% fidelity to the network. The rules in the network are defined by information gain, with M-of-N rules extracted from the input layer. Errors propagate through layers when rules don't perfectly approximate each layer. To replace a network with hierarchical rules, a single set of splits for each layer must be chosen based on information gain. The experiments involve conducting layerwise rule extraction on a CNN trained on fashion MNIST in tensorflow. The CNN has a standard architecture with convolutional and max pooling layers followed by a fully connected layer. The goal is to provide a baseline for the best achievable rules when doing rule extraction with M-of-N rules and to evaluate other extraction algorithms. In the experiments, random inputs from the fashion MNIST training data were used to test extracted rules against the network. Different layers underwent rule searching procedures with varying complexities and errors. The extracted rules showed a trade-off between complexity and error, with accurate rules extracted from the first and final layers. The experiments involved testing extracted rules against the network using random inputs from the fashion MNIST training data. Different layers showed varying complexities and errors in rule extraction. The first and final layers produced accurate rules, while the second and third layers had a trade-off between accuracy and complexity. The optimal accuracy/complexity trade-off was not solely based on the number of input nodes, as seen in the comparison between the second and third layers. There was a critical point where error rapidly increased with increasing complexity, indicating a natural set of rules for explaining the latent features. The paper introduces a novel approach to rule extraction algorithms, considering rule complexity as a key factor. Current algorithms do not account for complexity optimization, leading to challenges in rule simplification. Empirical evaluation of extraction algorithms is crucial for validation, revealing limitations and potential. Some layers in the CNN exhibit complex features with high error rates, highlighting the difficulty in explaining certain features. The paper discusses the challenges of rule extraction in neural networks, particularly in complex layers with high error rates. It suggests that selective use of decompositional algorithms based on layer complexity may improve explainability. The black box problem of neural networks hinders their societal deployment, emphasizing the need for explainability. Knowledge extraction success in neural networks remains mixed, with critics highlighting the distributed nature of networks as a barrier to interpretation. The distributed nature of neural networks poses challenges for rule extraction, with critics questioning the feasibility of decomposing latent features. A novel search method for M-of-N rules was applied to explain CNN latent features, revealing an optimal rule balancing error and complexity. While rule extraction may not adequately describe all latent variables, simplifying explanations without sacrificing accuracy can be beneficial for easily understandable features. Further research is needed to explore the impact of different transfer functions, datasets, architectures, and regularization methods on the accuracy and interpretability of decompositional rule extraction in understanding network behavior."
}