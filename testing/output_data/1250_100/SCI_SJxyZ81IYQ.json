{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively in a bottom-up manner. This approach preserves semantic content better and follows a recursive structure that aligns with human language properties. Additionally, the proposed procedure requires less. The current text chunk discusses the limitations of sequential models in image captioning, which fail to capture hierarchical structures in natural languages. These models rely too heavily on n-gram statistics and lack the ability to represent hierarchical dependencies among words in a caption. The text chunk discusses the limitations of sequential models in image captioning, which fail to capture hierarchical structures in natural languages. It proposes a new paradigm where semantics and syntax are separated into two stages for constructing captions. This approach involves deriving explicit semantic representations from images and recursively composing captions based on noun-phrases. The proposed paradigm for image captioning separates semantics and syntax into two stages, using parametric modular nets for phrase composition. This approach preserves semantic content, allows for hierarchical dependencies among words, and is more interpretable and controllable than conventional models. Recent works in image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. The proposed paradigm aims to increase caption diversity while maintaining semantic correctness and generalizing well to new data, even with limited training data. The early approaches in image captioning involved bottom-up and detection-based methods, where visual concepts were extracted from images and used to generate captions. Recent works in image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. The proposed paradigm aims to increase caption diversity while maintaining semantic correctness and generalizing well to new data, even with limited training data. Xu et al BID3 extended their work by representing the input image with feature vectors and applying an attention mechanism to extract relevant image information. Lu et al BID0 adjusted the attention computation to also attend to the generated text. Anderson et al BID1 added an additional LSTM for better attention control. Dai et al BID18 reformulated latent states as 2D maps to capture semantic information. Recent approaches extract phrases or semantic words directly from the input image. Yao et al BID19 predicted frequent training word occurrences, feeding the prediction into the LSTM as an additional feature vector. Tan et al BID20 treated noun-phrases as hyper-words, enabling the decoder to produce full noun-phrases in one step. In BID21, a hierarchical approach was proposed where one LSTM decides on phrases to produce, while a second-level LSTM generates words for each phrase. The proposed paradigm in image captioning aims to increase caption diversity by representing the input image with noun-phrases and constructing captions through a recursive composition procedure. This approach preserves semantics effectively, requires less data to learn, and generates more diverse captions compared to sequential generation models. Unlike other approaches, the recursive composition procedure allows for versatile caption generation containing multiple objects. The proposed compositional paradigm in image captioning involves extracting noun-phrases from the input image to generate captions through a recursive composition procedure. This two-stage framework aims to produce captions following a hierarchical structure, utilizing neural networks to learn plausible compositions. CompCap is a captioning model that considers nonsequential dependencies among words and phrases in a sentence. It represents image semantics explicitly using noun-phrases like \"a black cat\" and \"two boys\". The model extracts these noun-phrases from input images, which is essential for visual understanding tasks. The number of distinct noun-phrases in a dataset is smaller than the number of images, making it a valuable approach for image captioning. The task of noun-phrase extraction is formalized as a multi-label classification problem. Distinct noun-phrases are derived from training captions, treated as classes, and binary classification is performed for each noun-phrase based on visual features extracted from images. The input image is represented using top scoring noun-phrases, pruned through Semantic Non-Maximum Suppression for semantically similar concepts. The process of constructing captions involves a recursive procedure called CompCap, which uses a Connecting Module to generate longer phrases by connecting pairs of noun-phrases. An Evaluation Module is then used to determine if the new phrase is a complete caption. If not, the process is repeated with updated phrase pool. The Connecting Module (C-Module) selects connecting phrases and evaluates their scores based on left and right phrases. An alternative strategy treats phrase generation as a classification problem due to the larger space of incomplete captions. The proposed paradigm limits the number of distinct connecting phrases, with over 1 million samples collected but only about 1,000 distinct connecting phrases in MS-COCO BID4. Connecting sequences are mined from training captions and treated as different classes. A connecting module acts as a classifier, using a two-level LSTM model to encode phrases and interact with visual features for attention and evolution of the encoded state. The proposed paradigm limits the number of distinct connecting phrases in MS-COCO. Connecting sequences are mined from training captions and treated as different classes. A connecting module acts as a classifier, using a two-level LSTM model to encode phrases and interact with visual features for attention and evolution of the encoded state. The encoders for P (l) and P (r) share the same structure but have different parameters. Their encodings go through fully-connected layers followed by a softmax layer. The Evaluation Module (E-Module) is used to determine whether a phrase is a complete caption by encoding it into a vector. The E-Module encodes input phrases into vectors using a two-level LSTM model and evaluates the probability of completeness. It can also check for other properties like caption quality using a caption evaluator. The framework can be extended for generating diverse captions through beam search or probabilistic sampling, allowing for multiple beams to avoid local minima. Additionally, user preferences or conditions can be incorporated into the framework. The E-Module encodes input phrases into vectors using a two-level LSTM model and evaluates the probability of completeness. It can also check for caption quality. Experiments are conducted on MS-COCO and Flickr30k datasets with specific vocabulary and training data requirements. CompCap is a modularized recursive compositional procedure for image captioning that generalizes better and requires 2 or 3 steps to obtain a complete caption. It is compared with baselines such as NIC, AdapAtt, TopDown, and LSTM-A5, which encode images as semantical feature vectors. Predictions of noun-phrase classifiers are used as additional features for LSTM-A5. All methods have been re-implemented and trained for a fair comparison. CompCap is compared with baselines like NIC, AdapAtt, TopDown, and LSTM-A5 in image captioning. ResNet-152 is used to extract image features, with fixed parameters and a learning rate of 0.0001 for all methods. Best performance parameters are selected for caption generation. CompCap selects 7 noun-phrases for input image representation. Quality comparison is done on MS-COCO and Flickr30k test sets using SPICE, CIDEr, BLEU-4, ROUGE, and METEOR metrics. CompCap outperforms baselines like NIC, AdapAtt, TopDown, and LSTM-A5 in image captioning. It achieves the best results in SPICE metric with predicted noun-phrases, showing higher correlation with human judgements. However, it falls short in CIDEr, BLEU-4, ROUGE, and METEOR metrics. The sequential generation procedure favors frequent training n-grams, while the compositional generation procedure preserves semantic content more effectively. An ablation study on CompCap components shows a significant boost in all metrics when using groundtruth noun-phrases from associated captions. This indicates that CompCap effectively preserves semantic content. CompCap generates better captions by preserving semantic content and integrating noun-phrases from ground-truth captions. It excels in handling out-of-domain semantic content and requires less data to learn. Experimental studies show improved performance in SPICE and CIDEr metrics, indicating CompCap's effectiveness in composing syntactically correct captions. In the second study, CompCap trained on in-domain and out-of-domain data shows competitive results in SPICE and CIDEr metrics compared to baselines. The ability of CompCap to generate diverse captions is analyzed using five metrics, including the ratio of novel and unique captions. CompCap obtained the best results in all metrics, indicating diverse and novel captions. Qualitative samples in FIG2 show the generated captions. The proposed method for image captioning generates captions in a compositional manner by factorizing the procedure into two stages: extracting an explicit representation of the input image in the first stage and generating captions in the second stage. The errors in captions generated by CompCap mainly stem from misunderstanding the visual content, which can be improved with more sophisticated techniques in noun-phrase extraction. The proposed method for image captioning involves a two-stage process: extracting a representation of the input image using noun-phrases and generating captions hierarchically. The compositional procedure aims to preserve semantics effectively, require less training data, generalize better across datasets, and produce diverse captions by finding semantically similar noun-phrases through comparing central nouns. This is achieved by using encoders in the C-Module to obtain encodings for each noun-phrase, allowing for the suppression of semantically similar phrases. The C-Module uses encoders to compare semantically similar noun-phrases by computing normalized euclidean distances for z (l) and z (r) of each phrase. The sum of distances determines similarity, with independent parameters for P (l) and P (r) to differentiate encoding based on position in the ordered pair. Results show better performance with independent parameters. Adjusting hyperparameters for CompCap can lead to better performance. Additional hyperparameters include the size of beam search for pair selection and connecting phrase selection. Experimentally set at 10 and 1 respectively, these hyperparameters have minor influence on CompCap's performance, as shown in FIG6."
}