{
    "title": "rJeWvcsPiQ",
    "content": "Deep neural networks like DenseNet have high computational costs due to dense connections. A reinforcement learning framework with layer-wise pruning is designed to search for efficient DenseNet architectures while maintaining advantages like feature reuse. This approach helps DenseNet achieve a better balance between accuracy and computational efficiency. Experiments show that DenseNet with layer-wise pruning is more compact and efficient compared to other alternatives. Neural architecture search has been successful in designing model architectures, but current methods are not efficient for DenseNet due to dense connectivity. A layer-wise pruning method based on reinforcement learning is proposed to search for an adaptive neural network structure for DenseNet, balancing computational budget and accuracy. The agent learns to prune weights and connections while maintaining good accuracy, generating a curriculum for effective network pruning. The study proposes a layer-wise pruning method using reinforcement learning for DenseNet architecture search. The controller makes decisions on connections between layers, with a reward function considering both prediction accuracy and computational efficiency. The study introduces a layer-wise pruning method using reinforcement learning for DenseNet architecture search, focusing on maximizing prediction accuracy while minimizing computation. The training procedure involves curriculum learning, joint training, and training from scratch, with results reported on CIFAR datasets. The controller selects the child network with the highest reward for further training. The study introduces a layer-wise pruning method using reinforcement learning for DenseNet architecture search. The training procedure involves curriculum learning, joint training, and training from scratch, with the controller selecting the child network with the highest reward for further training. The study proposes methods to learn compact neural network architectures by analyzing input channels in DenseNet layers and connection dependencies between convolution layers and their preceding layers. The connection dependency is visualized in FIG3, showing that the child network learned from vanilla DenseNet is efficient and compact."
}