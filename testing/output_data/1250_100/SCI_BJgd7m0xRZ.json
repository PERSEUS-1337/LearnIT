{
    "title": "BJgd7m0xRZ",
    "content": "Anomaly detection in unlabeled data identifies non-conforming data points, often targeted by malicious attacks. One-Class Support Vector Machines (OCSVMs) are effective but vulnerable to sophisticated adversaries compromising training data integrity. To enhance security, a defense mechanism based on data contraction is proposed, adding uncertainty to OCSVMs to thwart adversaries. The approach successfully identifies anomalies by contracting data in low dimensional spaces, as shown through theoretical analysis and empirical evidence on benchmark datasets. Anomaly detection involves identifying non-conforming data points, crucial for various applications like network intrusion detection and fraud detection. One-Class Support Vector Machines (OCSVM) are effective but can be compromised by adversaries. A proposed defense mechanism involves contracting data in low dimensional spaces to improve OCSVM performance in identifying adversarial samples. Anomaly detection systems in evolving data environments require periodic retraining to prevent adversaries from injecting malicious data and compromising learning algorithms. Adversaries aim to avoid detection or decrease system performance by manipulating training data to distort the learning algorithm's representation. It is challenging to provide a comprehensive analysis of attacks across different machine learning algorithms due to the diverse ways adversaries can undermine learning systems. In this work, the goal is to enhance the attack resistance of OCSVMs against adversarial attacks targeting training data integrity. Adversaries can manipulate data to force the learning algorithm to favor them, posing a threat to machine learning systems, especially in image recognition where imperceptible perturbations can lead to misclassification. The aim is to use a nonlinear data projection algorithm to bolster OCSVMs' defense against adversaries in realistic scenarios. The theory of nonlinear random projections is used to enhance the attack resistance of OCSVMs against adversarial attacks by reducing optimization parameters. Nonlinear random projections preserve data distribution properties with minor perturbations, providing an additional layer of security to the learner. The theory of nonlinear random projections enhances the attack resistance of OCSVMs against adversarial attacks by reducing optimization parameters. Nonlinear random projections provide an additional layer of security to the learner by making it virtually impossible for the adversary to guess the projection mechanism used. The main contribution of this work is deriving an upper bound on the weight vector length of an OCSVM trained on an undistorted dataset that has been nonlinearly transformed to a lower dimensional space. The proposed approach aims to increase the attack resistance of OCSVMs through nonlinear data transformations against adversarial opponents. This method adds unpredictability through randomized kernels, such as Random Kitchen Sinks, to improve efficiency and security in anomaly detection. The kernel value of two data points is approximated by the dot product in a low-dimensional space z, making it computationally efficient for training a linear SVM. Randomized feature maps improve efficiency in evaluating new data points. BID11 introduced a transformation method with lower time and space complexities than RKS, while BID13's method has been applied to other kernel machines. BID4 introduced R1SVM for unsupervised anomaly detection, reducing training and evaluation times without compromising accuracy. This work focuses on using random projections as a defense mechanism for OCSVMs under adversarial conditions. No existing work adopts Rahimi and Recht's method for adversarial learning in anomaly detection with OCSVMs. Adversarial learning has inspired research in the machine learning community, with AD-SVM introducing additional constraints to thwart attacks. While DNNs are robust to noise, they struggle with adversarial data points. Our work focuses on unsupervised learning with OCSVMs and kernels, different from previous works using binary SVMs. Recent work by BID5 demonstrated how an attacker could manipulate a road sign to be misclassified by a vehicle's learning system, potentially leading to dangerous consequences. This paper introduces a novel framework combining adversarial learning, anomaly detection with OCSVMs, and randomized kernels. The focus is on adversarial learning for anomaly detection in the presence of a malicious adversary who alters training data to disrupt the learning process. The goal is to compromise the integrity of input data to hinder the decision-making capability of the learning system. The learner cannot distinguish between D and (X + D) to prevent removal of adversarial distortions during training. The adversary determines D based on its knowledge, with limited magnitude to avoid detection. Data is projected to a lower dimensional space to minimize attacks, using a nonlinear transformation with a vector b. This transformation helps approximate nonlinear kernels like the Radial Basis Function, reducing computational overheads in kernel-based learning algorithms. The OCSVM algorithm is used for anomaly detection by separating training data with a maximal margin in a transformed space. The dual form of the algorithm is represented in matrix notation, with parameters like Lagrange multipliers and a margin for the separating hyperplane. Adversaries can manipulate the hyperplane using D to shift it towards anomalies, aiming for false negatives in integrity attacks. This work focuses on integrity attacks in anomaly detection using the OCSVM algorithm. The adversary aims to minimize the margin of separation by injecting perturbed data into the training set, targeting a specific class of anomalies. The adversary can manipulate data points by adding a displacement vector, without knowledge of the learner's projection mechanisms. The attack model involves pushing anomalies towards the normal data cloud and injecting them into the training set. The OCSVM algorithm considers all data points in the training set as normal, making distorted anomalies appear as normal data points. The severity of the attack is controlled by the parameter s attack \u2208 [0, 1], with closer anomalies considered moderate attacks and farther anomalies severe attacks. As attack severity increases, distorted digits resemble their original form. The attacker optimally chooses target points for distortion, requiring computational effort and data distribution knowledge. The adversary uses the centroid of the normal data cloud in the training set as the target point for distorting anomaly data points. Different attacks can be orchestrated by adjusting the percentage of distorted anomaly data points and the severity of the distortion. Increasing the severity of the attack moves anomaly data points farther away from the normal data cloud, altering the position of the separating hyperplane. The OCSVM is trained using the entire dataset as normal, but precautions can be taken to minimize the effects of distortions by contracting the data to a lower dimensional space using random projections. This introduces uncertainty to the adversary-learner problem, increasing security. However, the unpredictability of random projections can lead to overlapping data from different classes. To increase attack resistance, a projection that conceals potential distortions of an adversary is proposed. The proposed method aims to increase attack resistance by selecting a projection that conceals potential distortions of an adversary in a one-class problem. A compactness measure is used to identify suitable projection directions by calculating the compactness of projected data for multiple random projections of the training data. The projection with the highest compactness value is chosen as the best defense against attacks. The anomaly detection algorithm aims to identify the smallest hypersphere containing the training data set. The learner's objective is to minimize the radius of the hypersphere. The effects of the adversary's perturbations on the margin of separation of the OCSVM are analyzed. The distance between the hyperplane and the origin is crucial for determining the margin of separation. An upper bound on the vector of weights is derived analytically to resist attacks in a one-class problem. An upper bound on the vector of weights is derived analytically to resist attacks in a one-class problem, considering small, positive distortions made by the adversary. The primal solution in the presence of a malicious adversary is compared to the solution without adversarial distortions. The defender can tighten the upper bound of the weight vector by reducing the dataset dimensionality. Experimental evaluation shows the effectiveness of the proposed defense mechanism against directed attacks on benchmark datasets like MNIST, CIFAR-10, and SVHN. Single-class datasets are generated for evaluation, with the adversary aiming to misclassify anomalies as normal data points. The evaluation involves creating single-class and unlabeled training sets, with two test sets for each dataset - a clean set and a distorted set. The datasets are normalized, and nonlinear projections are chosen based on local intrinsic dimensionality. The learner performs transformations to select the projection with the highest compactness, which is then applied to the test sets. Equation 5 describes the process of training an OCSVM with a linear kernel using transformed training sets and evaluating the model on test sets. The \u03bd parameter is kept fixed to analyze the impact of adversarial distortions on OCSVM performance. The classification performance of OCSVMs trained on nonlinearly transformed data is 2-7% higher than those trained on the original feature space. The OCSVM trained on transformed data shows improved performance compared to the original feature space. The f-scores decrease across dimensions, indicating better identification of adversarial samples with clean data. OCSVMs are vulnerable to integrity attacks, as adversaries can manipulate models by crafting adversarial data points. Reducing dimensions initially increases f-scores, but further reduction leads to a decrease, confirming the impact of data projection on model performance. The study demonstrates that projecting data to a lower dimensional space aids in identifying adversarial samples not detectable in the original feature space. Graphs show improved detection rates under the proposed approach, with a notable increase in identifying adversarial samples. However, reducing dimensions beyond a certain threshold leads to a decline in performance due to the loss of useful information. The experiments show that OCSVMs are vulnerable to integrity attacks, but projecting a distorted dataset to a lower dimension can increase model robustness. Performance declines when reducing dimensions beyond a threshold, but in projected spaces without attacks, performance is comparable with less computational burden. The study combines unsupervised anomaly detection with OCSVMs and random projections for dimensionality reduction under adversarial conditions. The study explores the impact of nonlinear random projections on the robustness of OCSVMs against adversarial perturbations. Results show that OCSVMs can be significantly affected under adversarial conditions, with projections making it harder for adversaries to guess the learner's details. The approach aims to enhance system security by contracting the normal data cloud and adding a layer of randomness to the search space, making it more challenging for adversaries. The study investigates the impact of nonlinear random projections on OCSVMs' robustness against adversarial perturbations. It aims to enhance system security by adding randomness to the search space, making it challenging for adversaries. Future work includes exploring game-theoretical formulations of anomaly detection and adversarial learning problems under dimensionality reduction techniques. The study explores the effect of nonlinear random projections on OCSVMs' resilience to adversarial perturbations. It seeks to bolster system security by introducing randomness to the search space, making it harder for attackers. Future research may delve into game-theoretical approaches to anomaly detection and adversarial learning amidst dimensionality reduction methods."
}