{
    "title": "rkgoyn09KQ",
    "content": "In this work, a neural autoregressive topic model is combined with a LSTM-based language model to address challenges in probabilistic topic modeling. The approach aims to incorporate language structure and semantics while learning the thematic structure in a collection of documents. The unified probabilistic framework, named ctx-DocNADE, combines the strengths of both models to better estimate word probabilities in a given context. Incorporating external knowledge into neural autoregressive topic models via a language modeling approach improves word-topic mapping on smaller or short-text corpora. The proposed extension, ctx-DocNADEe, addresses challenges in settings with limited context or smaller training corpora. The novel neural autoregressive topic model variants with neural language models consistently outperform state-of-the-art generative topic models in generalization, interpretability, and applicability over long-text. Probabilistic topic models like LDA, RSM, and DocNADE are commonly used for topic extraction from text collections. These models learn latent document representations for NLP tasks but ignore word order and semantic information. Extending these models to incorporate word order and language structure is important for better analysis. Recent advancements in language models have shown that deep contextualized LSTM-based models can capture different language concepts in a layer-wise fashion. However, these models do not capture semantics at a document level. This is important for better analysis in NLP tasks, as traditional topic models like LDA and DocNADE ignore word order and semantic information. Recent studies have integrated latent topics with neural language models to improve semantics at a document level. While some models capture word order in short contexts, they struggle with long-term dependencies and language concepts. In contrast, DocNADE variants focus on word occurrences across documents but ignore language structure. By introducing language structure into neural autoregressive topic models via LSTM-LM, word ordering, language concepts, and long-range dependencies are accounted for. The proposed neural topic model, ctx-DocNADE, combines word and latent topic learning in a unified framework to predict words accurately based on global and local contexts. It addresses challenges in learning from contextual information in short texts and few documents due to limited word co-occurrences and training corpus size. Distributional word representations like word embeddings have shown to capture semantic and syntactic relatedness in words, outperforming traditional topic models in NLP tasks. Previous work has integrated word embeddings into topic models like LDA and DMM to improve information extraction from short texts. However, some approaches overlook the underlying language structure. Incorporating distributed compositional priors in DocNADE with pre-trained word embeddings via LSTM-LM enhances topic and textual representations, improving generalizability, interpretability, and applicability. This approach, named ctx-DocNADEe, combines complementary learning and external knowledge to model short and long text documents effectively. Our modeling approaches consistently outperform state-of-the-art generative topic models on various datasets, showing improvements in topic coherence, precision, and text classification. The proposed textTOvec generates contextualized topic vectors for short-text and long-text documents. The code is available at https://github.com/pgcool/textTOvec. NADE BID13 decomposes the joint distribution of binary observations into autoregressive conditional distributions using feed-forward networks, allowing for tractable gradients of data negative log-likelihood. DocNADE BID12 models collections of documents as bags of words, focusing on learning word representations reflecting document topics only. Each word observation is computed using preceding observations in a feed-forward neural network. The log-likelihood of a document is computed using word embeddings and hidden units. Two extensions of the DocNADE model are proposed: ctx-DocNADE introduces language structure via LSTM-LM, while ctx-DocNADEe incorporates external knowledge through pre-trained word embeddings. These models account for word ordering, syntactical and semantic structures, long and short term dependencies, and external knowledge, overcoming limitations of BoW-based representations. Similar to DocNADE, ctx-DocNADE models each document as a sequence of words represented by embedding vectors. The conditional probability of a word in ctx-DocNADE is determined by two hidden vectors, incorporating topic information and word embeddings. The LSTM component in ctx-DocNADE introduces word ordering and language concepts, extending DocNADE's capabilities. The ctx-DocNADE model extends DocNADE by incorporating word ordering and language concepts through an LSTM component. The embedding layer in ctx-DocNADEe is initialized with a pre-trained embedding matrix E and a weight matrix W. The computational complexity in ctx-DocNADE or ctx-DocNADEe is O(HD + N), where N is the total number of edges in the LSTM network. The textTOvec representation can be extended to a deep, multiple hidden layer architecture for improved performance. The conditional probability is computed using the last layer, and the modeling approaches are applied to various datasets for evaluation. In evaluating topic models, four quantitative measures are presented: generalization (perplexity), topic coherence, text retrieval, and categorization. Data statistics are shown in TAB1, with 20NS representing 20NewsGroups and R21578 representing Reuters21578. Performance of proposed models ctx-DocNADE and ctx-DocNADEe is compared with various baselines, including word and document representations, LDA-based BoW TMs, neural BoW TMs, and jointly trained topic and language models. DocNADE is often trained on a reduced vocabulary (RV). Experimental Setup: DocNADE is trained on full text/vocabulary (FV) and document representations are computed for evaluation tasks. Models were run in the FV setting over 200 topics with glove embeddings of 200 dimensions. Pre-training for 10 epochs with \u03bb set to 0 was done for ctx-DocNADEs. Short-text datasets were evaluated with 200 topics and hidden size, using the same pre-trained word embeddings as ctx-DocNADEe. See appendices for experimental setup and hyperparameters. The generative performance of topic models like ctx-DocNADEe was evaluated by estimating log-probabilities for test documents and computing average held-out perplexity per word. The optimal \u03bb value was determined based on the validation set, with ctx-DocNADE achieving lower perplexity scores compared to baseline DocNADE on short and long texts. Topic coherence was assessed using a measure that identifies context features for each topic word, with higher scores indicating more coherence. The coherence of topics generated by ctx-DocNADEe was evaluated using a sliding window approach over a reference corpus. Higher coherence scores were observed, indicating that contextual information and language structure contribute to generating more coherent topics. The introduction of embeddings in ctx-DocNADEe further improved topic coherence, outperforming baseline methods like glove-DMM and glove-LDA. Additionally, comparisons were made with other models combining topic and language models, showing the effectiveness of the proposed approach. Our work focuses on improving topic models for textual representations by incorporating language concepts and external knowledge via neural language models. We compare the performance of our models with TCNLM on topic coherence using the BNC dataset. Different hyper-parameters such as sliding window size and mixture weight are explored to enhance topic modeling. The top words of learned topics from our models and TCNLM are compared, showing the relevance of our proposed models. Our proposed models, including ctx-DocNADEe, show improved topic coherence compared to the baseline DocNADE. However, ctx-DocNADEe does not exhibit the same improvements in topic coherence over ctx-DocNADE due to the nature of the BNC dataset. Additionally, a qualitative comparison of topic expressions between TCNLM and our models reveals differences in capturing specific types of words. In terms of model performance, we focus on topic coherence as the primary evaluation metric. The text discusses the retrieval precision scores for short-text and long-text datasets at a retrieval fraction of 0.02. It highlights the improved performance of introducing pre-trained embeddings and language/contextual information, particularly for short texts. The study also explores topic modeling without pre-processing and filtering certain words, showing that using DocNADE(FV) or glove(FV) enhances IR precision. Overall, ctx-DocNADEe reports a gain in performance across various datasets. On average, ctx-DocNADEe shows a 7.1% gain in IR precision compared to DocNADE(RV) across multiple datasets. The deep variant ctx-DeepDNEe also performs well on TREC6 and Subjectivity datasets. Additionally, the proposed models outperform TDLM and ProdLDA by noticeable margins. In text categorization, logistic regression classifiers with L2 regularization are used with textTovec representations. Glove embeddings outperform DocNADE in classification performance for short texts. The ctx-DocNADEe outperforms DocNADE in classification performance for short texts, showing a gain of 4.8% and 3.6% in F1 scores. Overall, a 4.4% improvement is observed. In terms of classification accuracy on the 20NS dataset, ctx-DocNADE and ctx-DocNADEe outperform NTM and SCHOLAR. Meaningful topics are captured, with ctx-DocNADEe extracting more coherent topics due to embedding priors. The contribution of word embeddings and textTOvec representations in topic models is analyzed using DocNADE and ctx-DocNADEe models. The top 3 texts for an input query from the TMNtitle dataset are retrieved, with ctx-DocNADEe showing no unigram overlap with the query. Examples of query retrievals from the 20NS dataset are provided, showcasing the effectiveness of the models in retrieving relevant texts. The quality of representations learned at different fractions of the training set from TMNtitle data is quantified in this work. The proposed models, ctx-DocNADE and ctx-DocNADEe, show improvements over DocNADE, especially in sparse data settings. The findings demonstrate the importance of incorporating language concepts in neural autoregressive topic models. Incorporating language concepts in neural autoregressive topic models improves word probability estimation. A combined DocNADE and LSTM-LM model learns latent representations from documents, enhancing topic model performance. External word embeddings further enhance model accuracy, outperforming state-of-the-art generative topic models on various metrics. Training instructors must have tertiary education, equipment operation experience, and English proficiency. The Contractor must provide proficient English-speaking staff for 24/7 maintenance and on-call services for the Signalling System, cables, and equipment installation. They must also coordinate with the Engineer for labeling format and content. The Contractor must provide English-speaking staff for 24/7 maintenance of the Signalling System. Stations with multiple platforms can be set for service reversal. Document retrieval was done using gensim for training Doc2Vec models. For training Doc2Vec models, a logistic regression classifier was used on document vectors to predict class labels. Multilabel datasets were handled with a one-vs-all approach. Models were trained with L2 regularization and evaluated for accuracy and F1 score. Glove-DMM and glove-LDA models were trained using LFTM. Classification tasks used relative topic proportions as input. Topic coherence was higher for SCHOLAR compared to DocNADE. The experimental results show that DocNADE outperforms SCHOLAR in generating representations for tasks like information retrieval and classification, but lags behind in interpretability. This opens up new avenues for future research."
}