{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The teachers and student can have different structures and be trained on different tasks. The student becomes independent of the teachers after training, and this approach outperforms fine-tuning and other knowledge exchange methods in various learning tasks. Research communities have developed various deep net architectures for different tasks, with some architectures trained from scratch and others fine-tuned using structurally similar deep nets. In reinforcement learning, different approaches like progressive neural nets, PathNet, 'Growing a Brain', Actor-mimic, and Knowledge distillation have been explored to transfer knowledge from multiple teachers to a new student model. Knowledge flow addresses limitations of existing techniques by transferring knowledge from multiple teachers to a student model, ensuring independence of the student at the final training stage. It allows flexibility in choosing teacher models and maintains a constant size for the resulting student net. This approach is applicable to various tasks from reinforcement learning to fully-supervised learning. Our approach is applicable to various tasks from reinforcement learning to fully-supervised training. We evaluate knowledge flow and compare using the same discount factor. The goal of reinforcement learning is to find a policy that maximizes the expected future reward from each state. In this paper, we follow the asynchronous advantage actor-critic (A3C) formulation. The policy mapping and value function are approximated by deep nets with parameters to optimize the policy. In reinforcement learning, a common loss function involves a negative log-likelihood and negative entropy regularizer. The goal is to optimize the policy and value function to maximize expected return. A proposed framework called knowledge flow transfers knowledge from 'teachers' to a 'student' deep net during training. The proposed framework of knowledge flow involves transferring knowledge from pre-trained 'teachers' to a 'student' deep net during training. The student net's parameters are randomly initialized, and knowledge from multiple teachers is added to the student net by transforming and scaling intermediate representations. This process allows the student to gradually become independent as training progresses. The proposed framework involves transferring knowledge from pre-trained 'teachers' to a 'student' deep net during training. A trainable matrix Q is scaled via a weight p w normalized to sum to one for each student layer. The normalized weights determine which representation to trust at each layer. As training progresses, the student is encouraged to become more independent and eventually master the task on its own. The proposed framework involves transferring knowledge from pre-trained 'teachers' to a 'student' deep net during training. Two additional loss functions are introduced to encourage this transfer: dependency loss captures how much a student relies on teachers, and another loss ensures the student's behavior doesn't change rapidly when teachers' influence decreases. Parameters from current and previous iterations are controlled by \u03bb1 and \u03bb2 to decrease the teacher's influence gradually. The proposed framework involves transferring knowledge from pre-trained 'teachers' to a 'student' deep net during training. Parameters \u03bb1 and \u03bb2 control the influence of teachers gradually, with \u03bb1 allowing the student to rely on teachers initially. The method decreases the weight for teacher layers to prevent negative transfer, while still benefiting from low-level knowledge transfer. Candidate sets are defined for each layer in the student model, combining layers from teachers to modify the deep nets and decrease their influence successively. The candidate set for each layer in the student model is determined to decide which teachers' or student's representation to trust. Normalized weights are introduced for each layer in the student deep net. The combined intermediate representation of a layer in the student model is obtained using a specific method. It is recommended to link one teacher layer to one or two student layers for optimal results. In the final stage of training, the student becomes independent and no longer relies on additional parameters introduced earlier. The influence of teachers is gradually decreased by encouraging the student to rely more on its own layers. This is achieved by minimizing a dependence cost function, leading to the student becoming more independent as training progresses. In the final stage of training, the student becomes independent from teachers' influence, with a gradual decrease in teacher influence to prevent performance degradation. A Kullback-Leibler regularizer is used to control the rate of change in the student's output distribution. Knowledge flow is evaluated in reinforcement and supervised learning tasks, with results reported using only the student model to avoid teacher influence. The agent learns to predict actions based on rewards and input images from the environment. It chooses an action every four frames, with the last action repeated on skipped frames. The model architecture includes three hidden layers with specific filter sizes and units. Different hyper-parameter settings are used compared to previous work, with a focus on the learning rate and optimization algorithm. The text discusses the selection of \u03bb 1 and \u03bb 2 in a framework, the evaluation metrics used, and the comparison of the framework with other state-of-the-art transfer reinforcement learning frameworks. The results are reported based on multiple experiments and evaluation procedures. Our transfer framework outperforms PathNet and PNN in experiments, showing effective knowledge transfer from teachers to the student. Increasing the number of teachers improves student performance significantly. Different environment/teacher settings are explored, not used by PathNet or PNN, with results summarized in TAB2. In experiments, knowledge flow with expert teachers outperforms baseline A3C implementation. Knowledge flow allows learning from multiple teachers, avoiding negative impact from insufficiently pretrained teachers. Training curves are shown in FIG5. The student benefits from knowledge flow with expert teachers, even if input, output, and objectives differ. Teachers like Chopper Command and Space Invaders help the student model achieve scores ten times larger than learning without a teacher. Various image classification benchmarks are used for supervised learning, with evaluation metrics reported for each dataset. The training and test sets for CIFAR-10 and CIFAR-100 contain 50,000 and 10,000 images respectively. Experiments are conducted using Densenet as a baseline model with standard data augmentation. Teachers are trained on different datasets and the student model is trained using a combination of teachers. Fine-tuning from a CIFAR-100 expert improves performance by 4% over the baseline on the CIFAR-10 task, while fine-tuning from an SVHN expert performs worse. Knowledge flow from both good and inadequate teachers improves performance by 13% over the baseline. The results on the CIFAR-100 dataset are similar to those on CIFAR-10. Additional results demonstrating knowledge flow properties can be found in the appendix. Various techniques for knowledge transfer have been considered, with a focus on avoiding catastrophic forgetting. Our method ensures independence of the student during training, addressing limitations in previous approaches. Distral, a combination of 'distill & transfer learning,' involves joint training of multiple tasks sharing a common policy. A shared policy in multi-task learning encourages consistency between tasks. Knowledge flow leverages information from multiple teachers to help a student learn a new task. Various related works include actor-mimic, learning without forgetting, and policy distillation. The general knowledge flow approach allows training a deep net from multiple teachers, showing improvements in reinforcement and supervised learning. In the context of multi-task learning, knowledge flow involves distilling knowledge from larger teacher models to smaller student models, resulting in improved performance in reinforcement and supervised learning tasks. Experiments conducted on various datasets such as MNIST, CIFAR-100, and ImageNet show that the student models benefit from the distilled knowledge, outperforming traditional knowledge distillation methods. The student model in our framework shows better performance than KD by benefiting from the teacher's output layer behavior and intermediate layer representations. The 'EMNIST Letters' dataset includes 26 balanced classes of handwritten letters in 28x28 pixel images, while the 'EMNIST Digits' dataset has 10 balanced classes of handwritten digits. Training and test sets for each dataset are specified. Teachers were trained on different EMNIST datasets, with the target task being EMNIST Letters. Results compared to baseline models and state-of-the-art are presented in Table 5. In our experiment, we trained teachers on CIFAR-10 and CIFAR-100 and compared our results to fine-tuning and the baseline model on the STL-10 dataset. The student model in our framework outperformed KD by utilizing the teacher's output layer behavior and intermediate layer representations. Fine-tuning a model with weights pretrained on CIFAR-10 and CIFAR-100 reduced test errors by more than 10%. In our framework, student model training reduces test errors by 3% compared to fine-tuning. Results are obtained using fewer data and may not be directly comparable to other approaches. We compare to Distral BID26, a multi-task reinforcement learning framework, on Atari games with three tasks. Our model is trained for 40M steps, while Distral is trained for 120M steps. The results are summarized in TAB5, showing that Distral is suboptimal for learning a multi-task agent. Our framework improves test errors by 3% compared to fine-tuning, using less data. Compared to Distral, a multi-task reinforcement learning framework, our model trained for 40M steps outperforms Distral trained for 120M steps on Atari games with three tasks. Distral is suboptimal for learning a multi-task agent due to its inability to decrease a teacher's influence when the target task differs significantly from the source tasks. Our framework successfully reduces negative transfer by decreasing the teacher's influence, as demonstrated in the C10 experiment with C100 and SVHN teachers. The normalized weights of the teachers show that the C100 teacher, more relevant to C10, has a higher value than the SVHN teacher. Ablation studies confirm that learning with knowledgeable teachers yields better performance than with untrained teachers. Our experiments show that learning with knowledgeable teachers achieves higher rewards compared to untrained teachers in various environments and settings. The KL term prevents drastic changes in the student's output distribution when the teachers' influence decreases. Ablation studies demonstrate that without the KL term, rewards drop significantly as a teacher's influence decreases, while with the KL term, performance remains stable. Learning with the KL term results in an average reward of 2907, whereas learning without it yields an average reward of 1215. In additional experiments, architectures for the teacher and student models are used, with the teacher model based on BID16 and the student model based on BID17. The models have different convolutional layers and fully connected layers, with specific connections between corresponding layers. The target task in the experiment is KungFu Master. In the experiment, teachers with different architectures achieve similar performance as teachers with the same architecture when teaching the student for the KungFu Master task. Using an average network for the old parameters is explored, updating it with an exponential running average of the model weights. The experiment explores updating the old parameters with an exponential running average of the model weights, achieving similar performance as using a single model. Various techniques for knowledge transfer have been considered, such as fine-tuning, PathNet, and actor-mimic, contrasting them with the approach discussed. Progressive Net introduces lateral connections to learned features, while our method adds scaling with normalized weights for student independence. Distral combines distillation and transfer learning for joint training of multiple tasks, promoting consistency. Knowledge flow focuses on single-task transfer of information, unlike multi-task learning frameworks. Our proposed technique enables knowledge transfer between different domains, leveraging teachers' representations at the beginning of training. It differs from existing methods like Knowledge distillation and Actor-mimic by allowing for transfer between source and target domains and preventing forgetting of original capabilities when adding new tasks to a deep net. The proposed technique allows for knowledge transfer between different domains by leveraging teachers' representations at the start of training. It prevents forgetting of original capabilities when adding new tasks to a deep net, using only data from the new task while retaining old capabilities. This approach contrasts with existing methods like Knowledge distillation and Actor-mimic."
}