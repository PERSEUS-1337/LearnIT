{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization in deep neural networks. This paper discusses novel observations about dropout in DNNs with ReLU activations, leading to the proposed method \"Jumpout\" which samples the dropout rate using a decreasing distribution for better training of local linear models. Jumpout is a new technique that adaptively normalizes the dropout rate at each layer and training sample, improving performance on various datasets. It addresses the overfitting problem in deep neural networks by randomly setting hidden neuron activations to 0. Dropout is a technique that sets hidden neuron activations to 0 to reduce co-adaptation in deep neural networks. However, it requires tuning dropout rates for optimal performance, which can slow convergence if too high or yield no improvements if too low. In practice, a single dropout rate is often used for all layers, limiting the model's ability to generalize to noisy samples. Dropout is a technique in deep neural networks that sets hidden neuron activations to 0 to reduce co-adaptation. However, using a constant dropout rate for all layers can limit generalization to noisy samples and cause incompatibility with batch normalization. This has led to dropout becoming less popular compared to batch normalization in modern DNN architectures. The proposed \"jumpout\" is an improved version of dropout, addressing limitations such as incompatibility with batch normalization. It modifies dropout by randomly changing ReLU activation patterns, improving generalization by training linear models for data points in nearby polyhedra. In \"jumpout,\" the dropout rate is a random variable sampled from a decreasing distribution, ensuring a higher probability of smaller dropout rates. This leads to smoother transitions between data points in nearby polyhedra, improving generalization performance compared to fixed dropout rates. In \"jumpout,\" the dropout rate is adaptively normalized for each layer and training sample, ensuring consistent neural deactivation rates. This addresses the incompatibility between dropout and BN by rescaling outputs to maintain variance. Jumpout randomly generates a mask over hidden neurons, similar to dropout, without requiring extra training. Jumpout is a novel approach that adaptively adjusts the dropout rate for each layer and training sample, ensuring consistent neural deactivation rates. It outperforms traditional dropout on various tasks across different benchmark datasets. Unlike other methods that rely on additional trained models, jumpout does not require this and can be easily incorporated into existing architectures with minor modifications. Jumpout is a dropout method that adjusts the dropout rate based on ReLU activation patterns without the need for additional trained models. It introduces minimal computation and memory overhead and can be easily integrated into existing model architectures. Other recent dropout variants include Gaussian dropout, variational dropout, Swapout, and Fraternal Dropout. Jumpout is a dropout method that adjusts the dropout rate based on ReLU activation patterns without extra training costs or additional parameters. It can be integrated with existing model architectures and targets different dropout problems. The method can be applied alongside other dropout variants and is compatible with various deep neural network architectures. The DNNs with bias terms at each layer can be represented as a fully-connected network of m layers. The convolution operator is essentially a matrix multiplication, resulting in a sparse weight matrix with tied parameters. Average-pooling and max-pooling can be represented as matrix multiplications, and residual network blocks can be represented by appending an identity matrix. Piecewise linear activation functions like ReLU can be written as a piecewise linear function in the DNN. The DNN with bias terms is represented as a fully-connected network of m layers. ReLU activation functions can be expressed as a piecewise linear function in the DNN, where the model becomes a linear function around a data point x. The gradient \u2202x is the weight vector of the linear model associated with activation patterns on all layers, defining a convex polyhedron. Dropout improves the generalization performance of DNNs with ReLU activations by promoting independence among neurons and training multiple smaller networks. This is achieved by preventing co-adaptation of neurons and randomly dropping a portion of them during training. During training, dropout promotes independence among neurons in DNNs with ReLU activations by training multiple smaller networks. This leads to the network prediction being treated as an ensemble during test/inference, reducing variance. Dropout also smooths local linear models within convex polyhedra, ensuring each training data point has its own distinct model. The proposed method suggests sampling a dropout rate from a truncated half-normal distribution to address the fragility and lack of smoothness in deep neural networks. This approach aims to improve generalization ability by ensuring each training data point has its own distinct model within convex polyhedra. The proposed method suggests sampling a dropout rate from a truncated half-normal distribution to improve generalization ability in deep neural networks. The dropout rate is controlled by a hyper-parameter, the standard deviation \u03c3, to ensure smoothness and effectiveness of each local linear model within convex polyhedra. This Gaussian-based dropout rate distribution encourages smooth generalization performance, with smaller dropout rates sampled more frequently for closer polyhedra. The dropout rate for each layer is a hyper-parameter that controls smoothness among nearby local linear models. Tuning dropout rates separately for different layers is ideal for network performance, but setting a global dropout rate is common due to computational constraints. However, using a single global dropout rate can be suboptimal as the proportion of active neurons varies across layers. The effective dropout rate for each layer is influenced by the fraction of active neurons, leading to different behaviors of dropout across layers. To better control dropout behavior across layers and training stages, the dropout rate is normalized by q + j and set as p j = p j /q + j. This ensures consistent activation patterns and allows for precise tuning of dropout as a single hyper-parameter. The compatibility issue between dropout and batch normalization arises from the variance difference between training and test phases. Combining dropout with batch normalization layers requires careful consideration to avoid unpredictable DNN behavior. Dropout layers with BN layers involve a linear computational layer followed by a BN layer, ReLU activation layer, and dropout layer. Neuron values are treated as random variables, affected by dropout rate. Dropout changes mean and variance scales during training, impacting BN layer parameters. Dropout rate is normalized for consistent activation patterns and precise tuning. Compatibility issues between dropout and batch normalization require careful consideration to avoid unpredictable DNN behavior. During testing, batch normalization (BN) is not consistent with dropout as dropout is not applied. To address this inconsistency, the output y j should be rescaled to counteract dropout's effects on mean and variance scales. Rescaling dropped neurons by (1 \u2212 p j ) \u22121 recovers the original mean scale, while rescaling by (1 \u2212 p j ) \u22120.5 is needed for the variance scale. However, scaling un-dropped neurons by (1 \u2212 p j ) \u22121 for mean consistency is recommended. Simple scaling methods cannot fully address the shift in both mean and variance, requiring careful consideration of dropout rescaling factors. During testing, batch normalization (BN) is not consistent with dropout as dropout is not applied. To address this, the output y j should be rescaled to counteract dropout's effects on mean and variance scales. Rescaling dropped neurons by (1 \u2212 p) \u22121 recovers the original mean scale, while rescaling by (1 \u2212 p) \u22120.5 is needed for the variance scale. In practice, a trade-off point of (1 \u2212 p) \u22120.75 is proposed to make both the mean and variance consistent for cases using dropout and not using it. The performance of using dropout with batch normalization (BN) in convolutional networks is compared in FIG3. The original dropout with BN shows a decrease in accuracy with dropout rates over 0.15, while dropout with a rescaling factor (1\u2212p)^-0.75 continues to improve performance until a dropout rate of 0.25. This approach outperforms the other configurations. The proposed improved dropout method, \"Jumpout,\" overcomes drawbacks of the original dropout by sampling from a decreasing distribution for random dropout rates. It adapts the dropout rate based on active neurons, enforces consistent regularization, and scales outputs differently during training. Jumpout requires a main hyper-parameter \u03c3 and two auxiliary truncation hyperparameters (p min , p max ). Jumpout has three hyperparameters: \u03c3, p min, and p max. The values of p min and p max are set to 0.01 and 0.6, respectively, which have shown consistent performance across various datasets and models. The input h j is considered to be the features of layer j for one data point. When dealing with a mini-batch of data points, the average q + j is used as the estimate for the mini-batch to reduce computation and memory usage. Jumpout has a similar memory cost to original dropout, with minimal additional computation required for counting active neurons and sampling from the distribution during DNN training. In this section, dropout and jumpout are applied to various DNN architectures and compared on six benchmark datasets. The architectures include small CNN, WideResNet, ResNet, and others applied to different datasets like CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet. Experiments follow standard settings and data preprocessing/augmentation. On ImageNet, pre-trained ResNet18 models are used for training with dropout and jumpout. The experimental results show that jumpout consistently outperforms dropout on all datasets and DNNs tested. Jumpout brings improvements even on datasets with high test accuracy. Additionally, jumpout achieves significant improvements on CIFAR100 and ImageNet without the need to increase model size. A thorough ablation study confirms the effectiveness of each proposed modification in enhancing vanilla dropout. The study demonstrates that jumpout outperforms dropout on various datasets and DNNs. Jumpout shows improvements without increasing model size, especially on CIFAR100 and ImageNet. The modifications in jumpout enhance vanilla dropout effectively."
}