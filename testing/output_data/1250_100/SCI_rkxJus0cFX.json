{
    "title": "rkxJus0cFX",
    "content": "Data parallelism is a popular method for scaling Deep Neural Network (DNN) training across multiple nodes. Compressing communication traffic to alleviate synchronization bottlenecks in large-scale distributed training has gained significant attention. Residual Gradient Compression (RGC) is a successful approach for compressing transmitting messages in deep networks. A new RGC method called RedSync improves training time in multi-GPU systems by optimizing communication bandwidth. RedSync shows performance improvement for DNNs with high communication to computation ratio. Data parallelism is popular for deep neural networks (DNNs) on multiple computing nodes, but communication bandwidth has become a bottleneck. DNN models are growing larger, increasing the challenge of communicating model updates. The bottleneck of training has shifted towards communication across models, with synchronization overhead becoming a bottleneck for data parallelism on distributed systems. Recent studies focus on reducing communication costs by quantizing gradients to low-precision values. Recent studies aim to reduce communication costs in deep neural networks by quantizing gradients to low-precision values. Another approach is to sparsify communication gradients and restrict weight-updates to a small subset of parameters. Residual Gradient Compression (RGC) method is a promising pruning technique that transmits only a small subset of gradients while maintaining the rest locally as residuals for the next iteration. Different implementations of RGC have been proposed to improve compression ratio and training accuracy, with the latest variants addressing accuracy loss in specific network structures. The latest RGC variants, such as BID14, BID4, and BID9, achieve a 0.1% compression ratio on local gradients without loss of model accuracy. However, integrating RGC methods into real distributed training systems, especially multi-GPU systems, faces challenges due to inefficient compression algorithms and synchronization issues with sparse data structures. RedSync is a highly-efficient RGC implementation for multi-GPU systems, combining pruning and quantization techniques to compress transmitting gradients. It uses allgather operation with MPI for sparse synchronization and ensures minimal accuracy loss when training DNNs. RedSync shows significant performance improvements for communication-intensive networks like VGG, AlexNet, and LSTMs. RedSync is a highly-efficient RGC implementation for multi-GPU systems, utilizing pruning and quantization techniques to compress transmitting gradients. It employs synchronous SGD with allreduce operations for sparse synchronization, ensuring minimal accuracy loss in training DNNs. Each worker in the system computes gradients using local data and maintains a residual for accumulating untransmitted gradients from previous iterations. Selected elements are synchronized among all nodes using allreduce operations. The communication-set selection method in RedSync utilizes allreduce operations for synchronization among nodes, optimizing performance on HPC systems. This method, widely adopted in large-scale CNN training tasks, assigns remaining elements as new residuals for the next iteration. The algorithm's workflow is similar to the Deep Gradient Compression Method. The efficiency of communication-set selection is crucial for overall system performance, with a focus on implementing select, allreduce, and decompress operations for practical efficiency. The communication-set selection method in RedSync optimizes performance on HPC systems by utilizing allreduce operations for synchronization among nodes. Recent work suggests selecting top 0.1% elements from residuals of each layer, but this is challenging to implement on GPU. To address this, two more efficient communication-set selection algorithms are proposed: trimmed top-k selection and threshold binary search selection. These methods limit the radixSelect operation on a smaller subset, improving efficiency on GPUs. The algorithm calculates mean and maximum of residuals' absolute values in each layer to determine a threshold for selecting top elements. If the number of elements above the threshold is smaller than k, the threshold is dynamically decreased until the desired number is reached. Elements below the threshold are trimmed, and a top-k selection operation is performed using radixSelect. This approach improves efficiency by limiting radixSelect on a smaller subset of elements. The algorithm proposes a method to select the top 0.1% elements as a communication-set by finding a threshold using a binary search algorithm. This approach aims to avoid using radixSelect operation on GPU and includes at least 0.1% of the largest elements in the communication-set. The algorithm suggests a method for efficiently selecting the top 0.1% elements using a binary search approach to find a threshold. This method aims to reduce the time taken for selection operations on large parameter lists, such as in VGG16 and LSTM models. By reusing the threshold element and introducing only minimal overhead, the algorithm significantly improves efficiency compared to traditional selection methods like radixSelect. In practice, compression strategies are dynamically chosen based on parameter sizes. Trimmed top-k selection is suitable for middle size layers like convolutional layers, ensuring a compression ratio of 0.1%. Threshold binary search selection is ideal for large layers like hidden and softmax layers in LSTMs, optimizing compression costs. Compressed residuals may be quantized to reduce communication bandwidth requirements. In order to facilitate quantization compression, the select method is modified to ensure elements in the communication-set have the same sign. The largest and smallest k elements are chosen alternately as the communication-set. Threshold binary search selection cannot be used with quantization. Synchronization of dense gradient structures in traditional distributed DNN systems is achieved with an allreduce operation. Sparse allreduce in a distributed setting is complex due to different non-zero indices contributed by each worker. After modifying the select method for quantization compression, sparse allreduce is implemented using allgather operation to gather compressed residuals from each node. The message includes indices and values of elements in the communication-set, packaged together to reduce latency. Compressed residuals are added to local model weights after scaling. RedSync implements algorithm improvement techniques for sparse synchronization, including momentum correction and local gradient clipping. Performance gain is analyzed using a communication cost model based on latency and bandwidth. The network interface is assumed to be single ported. The number of elements in residuals is denoted as M, with a compression ratio of D. The compression ratio D is crucial for reduction operations, with different ratios for each node in binary search. Recursive doubling and Rabenseifners algorithm are used for communication. Sparse synchronization's bandwidth is proportional to the number of nodes, not the compression rate. The overhead of reduction can become a bottleneck. The reduction overhead in scaling RedSync may become a bottleneck, with linear increase in overhead with the number of nodes. Performance was tested on two multi-GPU systems, including a server with eight GPUs and a GPU supercomputer with 5320 nodes connected by Aries interconnect. Pytorch v4.0 and horovod MPI wrapper were used for DNN training operations and collective communication. For deep learning applications, performance was tested on Image Classification tasks using various CNN models like ResNet-44, VGG16, AlexNet, and ResNet-50. Nesterov's momentum SGD was used as an optimizer. Warm-up technique was applied to the first 5 epochs of ResNet50 and VGG16. For Language Modeling tasks, evaluation was done on the Penn Treebank corpus and WikiText language modeling dataset using a 2-layer LSTM model with 1500 hidden units per layer. Learning rate decays when no improvement has been made in validation loss. RedSync convergence was examined on the datasets. RedSync convergence was tested on various datasets using different deep learning models such as ResNet44, VGG16, AlexNet, and ResNet50. The performance and scalability of RedSync were evaluated on different numbers of GPUs, showing no loss of accuracy when increasing batch size to 2048. Figures and tables illustrate the results of the tests conducted. The implementation provided by horovod involved averaging training time in 1000 iterations. The use of trimmed top-k algorithm for layer compression in CNNs larger than 128KB and threshold binary search algorithm for hidden and softmax layers in LSTM was highlighted. RedSync was shown to be effective for accelerating data parallel training on DNNs with high communication to computation ratio, achieving significant speedup with more than 2 GPUs for VGG16, AlexNet, and LSTM models. However, no performance gain was observed for ResNet50 on Piz Daint and Muradin. The ratio of computation to communication for ResNet50 is the highest among the DNNs investigated. RedSync training on ResNet50 wastes time on decompression, negating the benefit of communication bandwidth reduction. RedSync scalability on Piz Daint shows a concave shape, with better speedup on 32 GPUs than 128 GPUs for AlexNet due to linear growth in communication bandwidth requirement and decompression overhead. Quantized-RedSync outperforms RedSync for CNNs, but performs worse for LSTM training on a small scale. CNN uses trimmed top-k for communication-set selection, with quantized version having similar computation cost. The reducing of communication cost by quantization improves system performance in CNN training. RedSync outperforms Quantized-RedSync on small-scale due to faster selection. RedSync accelerates DNN training using Residual Gradient Compression. Performance was tested on two GPU platforms with significant speedup observed for AlexNet, VGG16, and LSTM. The left part of FIG3 illustrates how sparse allgather works by recursive doubling method for large-scale DNN training. Communication-set selection involves exchanging compressed residuals between nodes at varying distances, leading to a reduction in message transfer time. The Rabenseifners algorithm is used for allreduce operation on messages, involving reduce-scatter followed by allgather. It uses a recursive halving algorithm for data exchange among nodes, reducing communication at each step for a total of lgp steps. The Rabenseifners algorithm involves reduce-scatter and allgather phases with bandwidth and latency requirements. Time is the sum of reduce-scatter, allgather, and reduction operations. Gradient clipping is used to avoid explosion by rescaling gradients exceeding a threshold. RGC method performs local clipping before communication, unlike traditional methods. The RGC method involves back-propagation for gradient computation, clipping, and compression for communication. Local clipping synchronizes computing and communication, eliminating Communication hiding. Gradient clipping is abandoned for CNNs to explore potential overlapping, while RNNs use Back Propagation Through Time for gradient calculation. RedSync integrates momentum masking and correction schemes for momentum SGD optimizers. The Momentum SGD version of RGC method adopted by RedSync involves warm-up training to accelerate convergence in the first few iterations. Instead of using high-compression-ratio RGC method, original SGD optimizer synchronized by allreduce is used in the first few epochs if necessary."
}