{
    "title": "rygVV205KQ",
    "content": "In this work, imitation learning is used to tackle challenges in high-dimensional sparse reward tasks for reinforcement learning agents. Adversarial imitation is shown to be effective in learning a representation of the world from pixels and exploring efficiently with rare reward signals. The proposed agent can solve a robot manipulation task of block stacking using only video demonstrations and sparse reward, outperforming non-imitating agents and competing approaches. The adversary acting as the reward function is small and easily trained, removing limitations of contemporary imitation approaches. In this work, the development of a new adversarial goal recognizer enables the agent to learn stacking without task rewards, purely through imitation. GAIL can handle high-dimensional pixel observations with a single-layer discriminator network, improving efficiency with a D4PG agent. Various types of features can be successfully used with a small adversary, where a deep adversary on pixels would fail. The proposed approach utilizes coding, random projections, and value network features to solve a robotic block stacking task using only demonstrations and sparse rewards. It outperforms previous imitation methods by reducing the need for hand-crafted rewards and achieving faster stacking. Key contributions include a Jaco robot arm agent achieving a 94% success rate and an early termination method for actor processes to improve task performance. An adversary-based early termination method for actor processes improves task performance and learning speed by creating a natural curriculum for the agent. The agent learns without task rewards using an auxiliary goal recognizer adversary, achieving 55% stacking success from video imitation only. Ablation experiments on Jaco stacking and a 2D planar walker benchmark BID33 show that random projections with a linear discriminator work well in some cases, and using value network features is even better. The goal of training an agent is to find a policy that maximizes the expected sum of discounted rewards. DDPG BID23 is an actor-critic method where neural networks represent the actor (policy) and critic (action-value function). New transitions are added to a replay buffer for better exploration. The action-value function is trained to match 1-step returns, and target networks are updated every K learning steps for stability. The policy network is trained via gradient descent to maximize the action-value function. Building on DDPG, D4PG introduces improvements. GAIL learns a reward function by training a discriminator network to distinguish between agent and expert transitions. The GAIL objective involves a discriminator network to differentiate between agent and expert transitions. It is related to MaxEnt inverse reinforcement learning. A D4PG agent is used for off-policy training with experience replay. The reward function is jointly trained with actor and critic updates. The discriminator distinguishes expert transitions from those of previous agents without using actions. The reward function combines imitation reward and sparse task reward. The GAIL objective involves a discriminator network to differentiate between agent and expert transitions. The reward function combines imitation reward and sparse task reward. The actor process includes early termination of episodes based on the discriminator score. Multiple CPU actor processes run in parallel with a single GPU learner process. The discriminator network's output is used as the reward function. The discriminator network used in GAIL is crucial for distinguishing between agent and expert transitions. It is important to find the right balance in the network's capacity to effectively teach the agent how to solve the task. Expert demonstrations provide valuable data for feature learning, covering essential regions of the state space. Access to expert actions is not assumed, ruling out behavior cloning for feature learning. Learning features by predicting in pixel space is not preferred due to high-resolution images and the need to capture long-term data structure. Contrastive predictive coding (CPC) is a representation learning technique that encourages long-term structure learning in data. It uses a probabilistic contrastive loss with negative sampling, allowing joint training of the encoder and autoregressive model without the need for a decoder. Swapping sparse rewards with a neural network goal recognizer trained on expert trajectories can remove task rewards, but the network may be exploited by the agent to receive imitation rewards without solving the task. To address the issue of agents exploiting the goal recognizer for imitation rewards, a secondary goal discriminator network can be used to detect goal states in expert demonstrations. By training this network to recognize goal states, agents can surpass the demonstrator by learning to reach the goal faster. This approach allows for improved performance beyond traditional imitation learning methods like GAIL. The first environment involves a Kinova Jaco arm with 9 degrees of freedom, controlled by policies through joint velocity commands. Observations are 128x128 RGB images, and hand-crafted reward functions are used. Demonstrations were collected using a SpaceNavigator 3D motion controller. The second environment features a 2D walker from DeepMind control. Our method using a tiny adversary outperforms a comparable D4PG agent on dense and sparse reward tasks, as well as GAIL agents with discriminator networks. The Conditional Predictive Coding (CPC) model accurately predicts future observations for expert sequences but not for non-expert ones. Conditioning on k-step predictions improves performance on stacking tasks when the discriminator also uses CPC embeddings. D4PG with sparse rewards struggles due to exploration complexity in this task. Our imitation methods outperform D4PG with sparse rewards and GAIL agents on tasks. The value network features lead to quicker learning compared to CPC features. GAIL with tiny adversaries on random projections has limited success. Norm clipping in the critic optimizer may explain why GAIL value features work while pixel features do not. However, applying norm clipping to pixel features does not improve performance. In addition to using CPC features as input to the discriminator, one can query CPC about future expert states. Ablation experiments on Jaco stacking show that adding layers to the discriminator network does not improve performance. The optimal choice for imitation learning is explored, with results showing that a tiny discriminator may be sufficient. The effect of the number of layers on the discriminator's performance is explored, with findings indicating that a small discriminator is advantageous for meaningful representation. An early termination criterion is introduced to improve learning speed, and the impact on episode length is analyzed. Data efficiency of the method is evaluated through ablation experiments, showing that even with 60 demonstrations, good performance can be achieved. The study explores the impact of the number of demonstrations on performance, finding that even with 60 demonstrations, good results can be obtained. Results on the planar walker show that the proposed method using value network features and random projections outperform conventional GAIL. Agents trained without rewards achieved a success rate of 55%, with efficient stacking and exploitation demonstrated. The human teleoperator takes up to 30s to complete a task. Leveraging expert demonstrations to improve agent performance has a long history in robotics. Recent work shows that priming a Q-function on expert demonstrations can improve agent performance. However, in our task, we only have access to pixel observations, not states and actions. Imitation learning is attractive for deep learning as it resembles classification and regression problems. Supervised imitation is a simple yet effective approach. In robotics, supervised imitation learning is effective but limited by the need for many demonstrations and access to demonstrator actions. One-shot imitation learning aims to replicate behaviors from single demonstrations using encoder and decoder networks. Different approaches exist, such as using attention mechanisms or gradient-based meta learning. Our approach focuses on agent interaction with the environment rather than supervised learning, aiming to overcome limitations of behavioral cloning. Inverse reinforcement learning (IRL) is proposed as an alternative to behavior cloning in robotics, where a reward function is learned from demonstrations and then optimized using reinforcement learning. Various methods like deep Q-Learning from demonstration (DQfD) and deterministic policy gradients from demonstration (DPGfD) have been developed to train agents using expert trajectories. Generative Adversarial Imitation Learning (GAIL) applies adversarial learning to imitation problems, introducing many variants. Our contribution involves using minimal adversaries on learned features to solve sparse reward tasks with high-dimensional input spaces. Unlike other approaches that focus on tracking a single expert trajectory, we aim to generalize all possible initializations of a hard exploration task by utilizing static self-supervised features and dynamic value network features. The curr_chunk discusses the use of predictive coding and dynamic value network features to train block stacking agents from sparse rewards on pixels. It also mentions supplementary videos of learned agents and the architecture of the behavior cloning model. Additionally, it includes a visualization of CPC on video data, explaining the encoder and autoregressive model components. The curr_chunk discusses optimizing loss through contrastive predictive coding (CPC) to maximize mutual information between context and target variables. This approach helps extract slow features by linearly embedding common variables into compact representations. The model is trained using CPC, and the agent utilizes future predictions without the need to predict in pixel space. Reward functions are modified for evaluation, with dense staged rewards defined for different stages of the task. The curr_chunk discusses the implementation details of the actor and critic networks for a task involving stacking colored blocks. The actor and critic share a residual network with convolutional layers and fully connected layers. Distributional Q functions are used instead of a scalar state-action value function. The paper adopts a categorical representation of random variable Z for Q functions. The z i 's are fixed atoms bounded between V min and V max. The implementation details involve computing a bootstrap target with N-step returns using a categorical projection. The loss function for training distributional value functions is based on cross entropy. Distributed prioritized experience replay is utilized for stability and learning efficiency."
}