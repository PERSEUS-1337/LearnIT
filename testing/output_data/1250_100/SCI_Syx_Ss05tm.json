{
    "title": "Syx_Ss05tm",
    "content": "Deep neural networks are vulnerable to adversarial attacks, where perturbations to images can cause the model to perform a task chosen by the attacker. This reprogramming attack can repurpose models to perform tasks they were not trained for, such as counting or classification tasks on different datasets. The study of adversarial examples focuses on attackers causing model prediction errors with small input changes. Various methods have been proposed to construct and defend against adversarial attacks, including untargeted attacks to degrade model performance and targeted attacks to produce specific outputs. In this work, the focus is on reprogramming a model to perform a task chosen by the attacker without needing to compute the specific desired output. The adversary achieves this by learning reprogramming functions that map between the original task and the adversarial task. The parameters of the adversarial program are adjusted to accomplish this goal. In this work, the focus is on adversarial reprogramming, where a model is repurposed to perform a new task chosen by the attacker. The reprogramming functions map between the original task and the adversarial task, with parameters adjusted to achieve the goal. The attack does not need to be imperceptible to humans to be successful, and potential consequences include theft of computational resources and repurposing AI-driven assistants for malicious purposes. Adversarial reprogramming involves repurposing AI models for malicious tasks, such as turning AI-driven assistants into spies or spam bots. The flexibility of neural networks allows for changes in inputs to lead to significant changes in output patterns, even with restrictions on parameter updates. This type of attack poses risks discussed in more detail in Section 5.2. This paper introduces adversarial reprogramming, presenting instances of crafting adversarial programs to make neural networks perform new tasks. Experimental demonstrations show how adversarial programs can alter network functions, such as counting squares in images or classifying different datasets. The susceptibility of trained and untrained networks to adversarial reprogramming is examined, revealing the ability to conceal adversarial programs and data. Transfer learning does not fully explain the results of adversarial reprogramming. Adversarial examples are inputs intentionally designed to cause machine learning models to make mistakes. These attacks can be untargeted or targeted, affecting various domains like malware detection and generative models. Reprogramming methods aim to produce specific functionality rather than a specific output. Developing reprogramming methods to create specific functionality rather than hardcoded output, such as using adversarial examples like an \"adversarial patch\" to manipulate model predictions. Parasitic computing involves making a system perform tasks it wasn't designed for by exploiting network protocols, while weird machines use crafted inputs to run arbitrary code on a computer. Adversarial reprogramming is a form of parasitic computing but focuses on manipulating model processing rather than communication protocols. Transfer learning and adversarial reprogramming aim to repurpose neural networks for new tasks. Neural networks can develop useful properties for various tasks, such as image feature recognition. Transfer learning involves using knowledge from one task to learn another, while adversarial reprogramming manipulates model processing. It is possible to train a neural network for one task and use a linear SVM classifier to adapt it for other tasks. Transfer learning allows model parameters to be changed for new tasks, unlike adversarial reprogramming where an attacker manipulates the input without altering the model. Adversarial reprogramming across tasks with different datasets is more challenging. The adversary aims to reprogram a neural network by crafting an adversarial program to be included in the input. The program is an additive contribution to network input and is not specific to a single image. Transfer learning allows model parameters to be changed for new tasks, unlike adversarial reprogramming where an attacker manipulates the input without altering the model. Adversarial reprogramming involves crafting an adversarial program to be included in the input, which is an additive contribution to network input and is not specific to a single image. The adversarial program parameters are defined with a masking matrix to apply to images, and the perturbation is bounded within a specific range. The probability of ImageNet labels is mapped using a hard-coded function for adversarial tasks. Adversarial reprogramming involves crafting a program to manipulate input for a specific task, maximizing the probability of assigned labels. Optimization includes a weight norm penalty to prevent overfitting, with hyperparameters specified. The adversary's computational cost is minimized post-optimization, requiring minimal computation during inference. Nonlinear behavior of the target model is exploited, unlike traditional adversarial examples based on linear approximations. Adversarial reprogramming involves manipulating input for specific tasks using linear approximations of deep neural networks. Experiments on six architectures trained on ImageNet showed reprogramming for tasks like counting squares, MNIST, and CIFAR-10 classification. The study also explored resistance to reprogramming through adversarial training and the possibility of concealing the adversarial program and data. The study involved generating images with white squares for a counting task. An adversarial program was created by embedding these images, with ImageNet labels representing the number of squares. Despite the mismatch between ImageNet and adversarial labels, accuracy was evaluated by comparing network predictions to the actual number of squares in the images. The study demonstrated the vulnerability of neural networks to reprogramming using an adversarial program. By embedding MNIST digits within a frame representing the adversarial program and training it with ImageNet labels, the networks were successfully reprogrammed to classify MNIST digits. The adversarial program generalized well from training to test set, indicating it doesn't rely solely on memorization. The study showed that neural networks can be reprogrammed using adversarial programs, as demonstrated by successfully repurposing ImageNet models to classify CIFAR-10 images. The adversarial program increased accuracy on CIFAR-10 with minimal computation cost, showing visual similarities with other adversarial programs. The degree of susceptibility to adversarial reprogramming depends on the model being attacked, as seen in the examination of attack success on an Inception V3 model trained on ImageNet. The study demonstrated that neural networks can be reprogrammed using adversarial programs, as shown by successfully repurposing an Inception V3 model trained on ImageNet to classify MNIST digits. Results indicate that the model, trained with adversarial training, remains vulnerable to reprogramming with only a slight decrease in attack success. This suggests that standard adversarial defense methods are not effective against adversarial reprogramming due to differences in goals, magnitude of programs, and generalization to new data. The study showed that neural networks can be reprogrammed with adversarial programs, even with models having random weights. The MNIST task was challenging for randomly initialized networks compared to ImageNet-trained models. Adversarial programs differed qualitatively between the two, indicating the importance of the original task in adversarial reprogramming. Random networks, despite their rich structure, did not perform as well as expected in generating adversarial programs. The study demonstrated that neural networks can be reprogrammed to classify shuffled MNIST and CIFAR-10 images, even with randomly initialized models. Despite the lack of spatial structure in the shuffled images, the reprogrammed networks achieved comparable accuracy to standard datasets. This suggests that the network may rely on similarities between original and adversarial data for successful reprogramming. The study showed that neural networks can be reprogrammed to classify shuffled CIFAR-10 images, with accuracy comparable to fully connected networks. This suggests the possibility of reprogramming across tasks and domains, even with limitations on program size and scale. The experiment used an Inception V3 model pretrained on ImageNet to classify MNIST digits adversarially, achieving successful reprogramming with lower accuracy. Results show that adversarial reprogramming remains successful, even with a small imperceptible program. The study tested concealing adversarial tasks within normal images from ImageNet by shuffling pixels and limiting perturbation scale. The reprogramming method was extended to include adversarial data combined with the program, shuffled sequences, and random ImageNet images. The study demonstrated the successful reprogramming of neural networks to classify MNIST digits using adversarial images hidden within normal images from ImageNet. The reprogramming method involved shuffling pixels and limiting perturbation scale, showing the flexibility of repurposing trained weights for new tasks. The results suggest that trained neural networks are more susceptible to adversarial reprogramming than random networks, even when the data structure is different. Our results show the practicality of reusing neural circuits in modern artificial neural networks for easier repurposing, increased flexibility, and efficiency. Recent work has focused on building dynamically connected networks with reusable components. It remains unclear if reduced performance in targeting random networks or reprogramming for CIFAR-10 classification is due to limitations in expressivity or optimization difficulty. Future research could explore adversarial reprogramming in other domains like audio, video, or text. The ability to reprogram trained networks to classify shuffled images without original spatial structure suggests potential for reprogramming across different tasks. Reprogramming neural networks, especially RNNs with attention or memory, can lead to potential adversarial attacks. By finding inputs that induce simple operations, like incrementing counters or changing attention locations, an attacker could reprogram the RNN to perform any task. This could result in theft of computational resources, such as using a computer vision classifier to solve captchas for spam accounts. The flexibility of reprogramming RNNs poses a significant danger beyond just computational theft. The text discusses the potential dangers of reprogramming neural networks, specifically RNNs, to perform tasks that violate ethical principles. Adversarial attacks can lead to the misuse of computational resources and the reprogramming of neural networks to perform novel tasks. The results show the vulnerability of deep neural networks to such attacks, highlighting the need for further research on mitigating and defending against adversarial reprogramming. The shuffled image is combined with an adversarial program to reprogram Inception V3 model, successfully classifying shuffled digits despite being unrelated to the original data."
}