{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as a source of domain knowledge representation. However, ontology information is rarely considered in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, showing improved performance by incorporating ontological information. Ontologies provide a formal representation of domain knowledge through categories and relationships, aiding in structuring training data and neural network architecture design. Sound event classification often overlooks the valuable information provided by ontologies, despite neural networks being the state of the art for this task. Ontologies provide structure to training data and neural network architecture for sound event classification. They are based on abstraction hierarchies defined by linguistics, such as super categories representing subcategories. Taxonomies can be defined by nouns or verbs, and examples include datasets like ESC-50, UrbanSounds, DCASE, and AudioSet. Hierarchical relations in classifiers allow for back-off to more general categories, disambiguation of acoustically similar classes, and penalization of misclassifications differently. Ontologies provide structure to training data and neural network architecture for sound event classification, based on abstraction hierarchies. They can be used to model neural networks and improve performance. Ontology-based network architectures have shown benefits in performance and reduced overfitting. Different approaches, such as deep restricted Boltzmann machines and perceptrons, have been used to classify sound events. The flexibility of adapting structures in deep learning models allows for the proposal of ontology-based networks for sound event classification. Our proposed ontology-based networks aim to utilize ontological information in deep learning architectures for sound event classification. The framework includes assumptions about the type of ontologies used and presents a Feed-forward model with ontological constraints. Additionally, ontology-based embeddings are computed using Siamese Neural Networks to maintain consistency with the ontological structure. The framework is designed for ontologies with two levels but can be generalized to more levels. Training data consists of audio representations associated with labels from the ontology, allowing for hierarchical classification. The proposed ontology-based networks integrate ontological information into deep learning architectures for sound event classification. The framework includes a Feed-forward model with ontological constraints and utilizes Siamese Neural Networks for ontology-based embeddings. It is designed for ontologies with two levels but can be extended to more levels, with training data consisting of audio representations linked to ontology labels for hierarchical classification. The proposed framework integrates ontological information into deep learning architectures for sound event classification. It includes a Feed-forward Network with an ontological layer that utilizes ontology structure. The network generates two outputs, p(y 1 |x) and p(y 2 |x), for hierarchical classification based on audio features. The proposed framework integrates ontological information into deep learning for sound event classification. It includes a Feed-forward Network with an ontological layer that generates two outputs for hierarchical classification based on audio features. The ontological layer reflects the relation between super classes and sub classes given by the ontology, and is used to predict classes in C1 and C2 for any input x. The model is trained using a gradient-based method to minimize the loss function, which is a combination of two categorical cross-entropy functions. The text describes the use of a Siamese neural network to learn ontology-based embeddings for sound event classification. The network enforces samples of the same class to be closer while separating samples of different classes, preserving the ontological structure. The architecture includes a Feed-forward Network with an ontological layer, where samples from different subclasses but the same superclass are closer than samples from different superclasses. The weights are learned simultaneously at every parameter update. The text discusses the use of a Siamese neural network to learn ontology-based embeddings for sound event classification. The network aims to differentiate between samples of the same class and those of different classes based on their ontological structure. The architecture includes a Feed-forward Network with an ontological layer, where samples from different subclasses but the same superclass are closer than samples from different superclasses. The weights are updated simultaneously during training. The dataset for the Making Sense of Sounds Challenge 2 (MSoS) consists of 1500 audio files divided into five categories, with 300 files each. The evaluation dataset has 500 audio files, with 100 files per category. All files are in a single-channel 44.1 kHz, 16-bit .wav format and are 5 seconds long. The Urban Sounds -US8K dataset is designed for urban sound classification and has a taxonomy with more nodes than the annotated number of classes. The dataset for the Making Sense of Sounds Challenge 2 (MSoS) consists of 1500 audio files divided into five categories. The Urban Sounds -US8K dataset has a taxonomy with more nodes than the annotated number of classes. The adjusted taxonomy has two levels, with 10 classes at level 1 and 4 classes at level 2. The dataset contains 8,732 audio files in a single-channel 44.1 kHz, 16-bit .wav format. Audio recordings were represented using state-of-the-art Walnet features BID1 and transformed via a convolutional neural network (CNN). The base network architecture consists of 4 layers: an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256. The baseline models for the MSoS and US8K datasets consist of dense layers with Batch Normalization, dropout rate of 0.5, and ReLU activation function. The models do not include ontological information and have an output layer for either level 1 or level 2. The baseline performance of the models is shown in Table 1. The study evaluated the impact of an ontological layer on the performance of models trained on the MSoS and US8K datasets. Different values of \u03bb were tested, with the best results achieved using \u03bb = 0.8 for MSoS and \u03bb = 0.7 for US8K. The ontological layer led to an absolute improvement of 5.4% and 6% in accuracy for MSoS, and 2.5% and 0.2% for US8K, compared to baseline models. The study evaluated the impact of an ontological layer on model performance in the US8K dataset. The best result was achieved with \u03bb = 0.7. The ontology-based embeddings led to tighter and better-defined clusters in t-SNE plots. The Siamese neural network was trained for 50 epochs using the Adam algorithm to produce the embeddings. Hyperparameters were tuned for good performance with input features. The study evaluated the impact of an ontological layer on model performance in the US8K dataset. The Siamese neural network was trained with different numbers of pairs for input training data, with 100,000 pairs yielding the best performance. The loss function used values from previous experiments, with different lambdas affecting overall performance. Results showed accuracy performance for MSoS and US8K, with conclusions drawn on the architecture's performance compared to baseline. Ontology-based embeddings provided better grouping, as seen in t-SNE plots. The study evaluated the impact of an ontological layer on model performance in the US8K dataset, showing that ontology-based embeddings provided better grouping. The Feed-forward Network with Ontological Layer achieved 0.88 accuracy, while using ontological embeddings achieved 0.89, outperforming significantly the baseline in sound event classification. Our study utilized a Feed-forward Network with an ontological layer to improve predictions across different hierarchy levels. We also introduced a Siamese neural Network to generate ontology-based embeddings, resulting in clustered super classes with various sub classes. The results from our experiments on datasets and the MSoS challenge surpassed baseline performance, indicating the potential for further exploration of ontologies in sound event classification."
}