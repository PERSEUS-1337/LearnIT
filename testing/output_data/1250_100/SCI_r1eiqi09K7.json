{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. The generalization is shown to be effective, with faster convergence and lower train loss values compared to standard algorithms in experiments involving embedding the WordNet taxonomy in the Poincare ball. Developing stochastic gradient-based optimization algorithms is crucial for various applications, especially when dealing with a large number of parameters. First order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD are commonly used for computational efficiency. While these algorithms are designed for parameters in a Euclidean space, recent work focuses on optimizing parameters on a Riemannian manifold, allowing for non-Euclidean geometries. This approach has been successful in various applications, such as solving Lyapunov equations, matrix factorization, and dictionary learning. In this work, the focus is on generalizing adaptive optimization algorithms to a Riemannian setting, specifically in the case of a product of manifolds. The challenge lies in adapting these algorithms to a Riemannian manifold where intrinsic coordinate systems are not readily available, making concepts like sparsity and coordinate-wise updates irrelevant. The study proposes new algorithms and analyzes their convergence in the context of a product of manifolds, where each manifold represents a \"coordinate\" of the adaptive scheme. The study focuses on generalizing adaptive optimization algorithms to a Riemannian setting, specifically in the case of a product of manifolds. They propose new algorithms and analyze their convergence in this context, supporting their claims with empirical evidence on hyperbolic taxonomy embedding. The absence of Riemannian adaptive algorithms could hinder the development of competitive optimization-based Riemannian embedding methods, particularly in the recent rise of embedding methods in hyperbolic spaces. A manifold M of dimension n is a space that can locally be approximated by a Euclidean space R n. It can be understood as a generalization to higher dimensions of the notion of surface. A Riemannian metric \u03c1 is a collection of inner-products varying smoothly with x, defining the geometry locally on M. A Riemannian manifold is a pair (M, \u03c1) where a choice of metric induces a global distance function on M. In a Riemannian manifold, the length of a path is determined by integrating the size of its speed vector. Riemannian SGD updates involve the gradient of the objective function and a step-size. The exponential map is used to update along the shortest path in the relevant direction while staying in the manifold. When the exponential map is not known, a retraction map is commonly used as a first-order approximation. Notable algorithms include ADAGRAD, which rescales updates based on past gradients. ADAM, proposed by BID9, combines a momentum term and an adaptivity term in its update rule. It is similar to RMSPROP but uses an exponential moving average instead of a sum for past gradients. A mistake in the convergence proof of ADAM led to the proposal of AMSGRAD by BID18, which modifies the algorithm or uses a time-dependent schedule for a parameter. The ADAMNC algorithm introduces a time-dependent schedule for the \u03b22 parameter, with updates defined intrinsically on a Riemannian manifold. The formalism involves working with local coordinate systems called charts, ensuring that quantities defined are intrinsic to the manifold. The RSGD update is intrinsic as it only involves exp and grad functions. The RSGD update of Eq. FORMULA1 is intrinsic, involving exp and grad functions on the Riemannian manifold. It is uncertain if Eqs. (3,4,5) can be expressed intrinsically. One approach is to use a canonical coordinate system in the tangent space and parallel-transport it along the optimization trajectory. However, in a Riemannian manifold, curvature introduces a rotational component to parallel transport, potentially disrupting the sparsity of gradients and adaptivity. The interpretation of adaptivity in optimizing different features at different speeds is lost due to the coordinate system dependency on the optimization path. The proposed approach involves viewing each component as a coordinate in a Riemannian manifold, leading to a simple adaptation of the update equation. The difficulty in designing adaptive schemes in a general Riemannian manifold is highlighted, as intrinsic coordinates are absent. In the context of optimizing features at different speeds, the adaptivity term in the update equation involves rescaling the gradient using Riemannian norms. Different optimization algorithms like ADAGRAD, ADAM, AMSGRAD, and ADAMNC with varying momentum parameters are discussed. The assumptions and notations involve geodesically complete Riemannian manifolds with sectional curvature constraints. Riemannian AMSGRAD is presented as an optimization algorithm for geodesically convex functions on product manifolds. The algorithm is compared to standard AMSGRAD and variations like RADAM and ADAM. The convergence guarantee for RAMSGRAD is discussed. The convergence guarantees for RAMSGRAD and RADAMNC are presented in Theorems 1 and 2 respectively. When the curvature is small but non-zero, the regret bound worsens by a multiplicative factor of approximately 1 + D \u221e |\u03ba|/6. The update rule for RADAGRAD is defined in Eq. (8) with \u03b2 1 := 0 in Theorem 2. The convergence of RADAMNC is discussed in Theorem 2, where sequences (x_t) and (v_t) are obtained with specific parameters. The role of convexity in regret bounds for convex objectives is explained, with a focus on bounding terms using the cosine law in the Riemannian case. The text discusses the use of adaptive algorithms in optimization, specifically focusing on the benefits of adaptivity in improving convergence rates. The algorithms RADAM, RAMSGRAD, and RADAGRAD are compared to the non-adaptive RSGD method. Theoretical results are presented, including the use of momentum/acceleration in convergence theorems. Empirical assessments are conducted to evaluate the proposed algorithms. The text discusses embedding the WordNet noun hierarchy in the Poincar\u00e9 model of hyperbolic geometry for optimization. The choice of the Poincar\u00e9 model is justified by the closed form expressions used in the algorithm. The dataset used is the transitive closure of the WordNet taxonomy graph. The transitive closure of the WordNet taxonomy graph contains 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in a way that minimizes distance between connected words. The loss function used is similar to log-likelihood, with negative word pair sampling. Metrics include loss value and mean average precision for link prediction. Training involves a \"burn-in phase\" for 20 epochs. Training details include a \"burn-in phase\" for 20 epochs with a fixed learning rate of 0.03 using RSGD with retraction. Negative word sampling is based on graph degree raised at power 0.75. Optimization methods favor RADAM over RAMS-GRAD. Convergence to lower loss values was observed when using the first-order approximation of the exponential map. Retraction-based methods are reported separately from fully Riemannian analogues. Results from the experiments show that RADAM consistently achieves the lowest training loss compared to other methods. It also outperforms all other methods on the MAP metric for both reconstruction and link prediction in the full Riemannian setting. However, in the \"retraction\" setting, RADAM reaches the lowest training loss value and performs similarly to RSGD on the MAP evaluation. RAMSGRAD shows faster convergence in terms of MAP for the link prediction task, indicating better generalization capability. After the introduction of Riemannian SGD by BID1, various other first-order Riemannian methods emerged, including Riemannian SVRG, Riemannian Stein variational gradient descent, Riemannian accelerated gradient descent, and averaged RSGD. New methods were also developed for convergence analysis in the geodesically convex case. Stochastic gradient Langevin dynamics was extended to optimize on the probability simplex. Additionally, Riemannian counterparts of SGD with momentum and RMSprop were proposed by BID20, suggesting the transportation of the momentum term using parallel translation. However, these algorithms lack convergence guarantees and compromise convergence possibilities due to coordinate-wise adaptive operations. Another version of Riemannian ADAM for the Grassmann manifold G(1, n) was introduced by BID3, also utilizing parallel translation for the momentum term. The algorithm proposed generalizes adaptive optimization tools to Cartesian products of Riemannian manifolds, providing convergence rates similar to Euclidean models. Experimental results show superiority over non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. The discussion highlights the lack of adaptivity across manifolds and absence of convergence analysis in existing algorithms. Lemma 3 states an inequality for convergence in optimization algorithms in Alexandrov spaces. Lemma 6 discusses the cosine inequality in Alexandrov spaces, while Lemma 7 presents an analogue of Cauchy-Schwarz inequality. Lemma 8, known as BID0, is used in the convergence proof for ADAMNC. Lemma 8 (BID0) is used in the convergence proof for ADAMNC, stating an inequality for non-negative real numbers y1, ..., yt."
}