{
    "title": "B1eHgu-Fim",
    "content": "Modern deep neural networks often have a large number of weights, making them challenging to deploy on devices with limited computational resources like mobile phones. One common method to reduce model size and computational cost is low-rank factorization, but using a small rank can impact model expressiveness and performance. To address this, a mixture of multiple low-rank factorizations is proposed in this work, with dynamically computed coefficients based on input. This approach has shown improved computation efficiency and maintained accuracy in language modeling and image classification tasks compared to full-rank counterparts. In recent years, low-rank factorization has been extensively explored as a way to reduce matrix size. A large weight matrix is factorized into two smaller matrices without requiring sparsity support from specialized hardware. However, using a small rank in the factorization can limit model expressiveness and lead to worse performance. To address this dilemma, the proposal is to increase expressiveness by learning an adaptive, input-dependent factorization instead of a fixed factorization of a weight matrix. The proposal suggests using a mixture of multiple low-rank factorizations with adaptive mixing weights based on the input to improve performance at a small additional cost. The input-dependent factorization creates an adaptive linear projection from high-dimensional to low-dimensional space, enhancing model expressiveness compared to conventional low-rank factorization methods. The proposed method suggests using adaptive mixing weights to enhance the expressiveness of the projected low-dimensional space. By incorporating pooling before projection and utilizing non-linear transformations, the model aims to reduce parameters and computation in the mixing weights. By applying pooling before projection and using random matrices for linear projection, the method aims to reduce parameters and computation in the mixing weights, enhancing expressiveness in the projected low-dimensional space. Our method dynamically adjusts mixing weights for the linear bottleneck in recurrent neural networks for language modeling. We use LSTM models and test three variants of the proposed model against regular low-rank factorization. These variants compute mixing weights in different ways, with varying amounts of extra parameters. The study compares the performance of LSTM models with adaptive mixtures against regular low-rank factorization. Results show a 40% reduction in FLOPs and a decrease in perplexity by 1.7 points. Additionally, using adaptive mixtures improves performance compared to non-adaptive low-rank factorization. Pooling before projection is recommended for computing mixing weights, reducing computation and parameter size while capturing global information for better accuracy. The effectiveness of the approach is further demonstrated on compressing CNN models on ImageNet. The study compares the performance of LSTM models with adaptive mixtures against regular low-rank factorization. Results show a 40% reduction in FLOPs and a decrease in perplexity by 1.7 points. Compared to MobileNet V2, separable convolutions in MobileNet achieve significantly better results with negligible extra FLOPs."
}