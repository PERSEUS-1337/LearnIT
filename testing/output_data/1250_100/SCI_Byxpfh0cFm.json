{
    "title": "Byxpfh0cFm",
    "content": "Data augmentation is a common method used in machine learning to enhance learning by expanding the training set with transformed examples. However, this process can be inefficient and lead to storage and training costs. A new approach using subsampling policies based on model influence and loss can reduce the augmentation set size by 90% while maintaining accuracy gains. This technique allows for the same benefits of data augmentation with fewer data points. Data augmentation is a key component in achieving state-of-the-art results in image recognition. However, it can be expensive due to the increase in dataset size. Applying a few sets of augmentations can significantly grow the dataset, leading to higher storage costs and training time. Selecting the optimal transformations for each data point is crucial but challenging. In this work, the aim is to make data augmentation more efficient by identifying subsamples of the dataset that are good candidates for augmentation. Drawing inspiration from the VSV method, which focuses on augmenting samples close to the margin, the goal is to create a more robust decision surface. The method aims to apply class-preserving data augmentations to improve the dataset. The method involves applying class-preserving data augmentations to support vectors in the training set, resulting in a decision surface with transformation invariance. The goal is to develop policies that reduce the augmentation set size for a broader range of models by ranking data points based on metrics like loss and influence. The paper discusses the use of robust statistics in data augmentation for high accuracy in image classification. It demonstrates that augmenting only a subset of the dataset can maintain high accuracy levels. The proposed policies for selecting points to augment based on training loss or model influence outperform random sampling. Additional modifications like sample reweighting and online learning can further enhance performance. The experiments are conducted on benchmark datasets like MNIST, CIFAR10, and NORB. In image classification, data augmentation is widely used to improve performance by applying various transformations to training data. Different studies have explored augmentation techniques for audio and text modalities as well. The selection of augmentation strategies can significantly impact performance and requires careful tuning. Various approaches have been proposed to select and tune the best transformations, such as adaptive augmentation, learning composed transformations, and reinforcement learning. The focus is on selecting which data points to augment while maintaining high accuracy levels. Our aim is to select data points for augmentation while keeping transformations fixed, which complements existing approaches like reinforcement learning. Recent works have used adversarial training methods for augmentation, generating artificial points from a target distribution. Our work is related to the Virtual Support Vector (VSV) method, which limits transformations to support vectors for augmentation in SVMs. In the context of SVMs, the idea of limiting transformations to support vectors for augmentation is straightforward. However, extending this concept to methods beyond SVMs has not been explored. Inspired by the VSV method, we aim to downsample candidate points for augmentation using metrics beyond support vectors. This includes measuring loss at each training point and exploring model influence to determine impactful data points. This work is closely related to subsampling methods for augmentation. The work discussed in Section 4 is related to subsampling methods for dataset reduction. Unlike traditional subsampling, the goal here is to increase the dataset through augmentation. The motivation is that the augmented dataset may become too large, but the original training set is manageable. The approach involves fitting a model to the original data to obtain necessary information for augmentation. In this work, the focus is on making data augmentation more efficient by providing effective policies for subsampling the original training dataset. The impact of performing translation augmentations on final test accuracy for datasets like MNIST, CIFAR10, and NORB is discussed. Test accuracies from augmenting 5, 10, and 25 percent of the data are compared, highlighting the importance of subsampling strategies. The study focuses on efficient data augmentation by proposing optimal subsampling policies for training datasets. Results show that augmenting just 25% of the dataset selected at random can yield over half of the total accuracy gain. Optimal policies can achieve similar results to full augmentation when augmenting only 10% of the data. Details on augmentation policies and experimental setup are provided in Sections 4 and 5 of the paper. The study proposes optimal subsampling policies for efficient data augmentation, aiming to minimize the subset size while maintaining performance similar to augmenting the entire dataset. Two metrics, loss and model influence, are used to generate augmentation scores, with policies including deterministic and random subset selection based on these scores. The focus is on test accuracy as the performance measure in experiments. In data augmentation, augmentation scores are used to select a subset of training points either deterministically based on top scores or probabilistically by sampling. The scores can be updated iteratively to adjust for model shifts. Different policies for selection function and score update are compared, with metrics including training loss and model influence. Sampling data points at random is used as a baseline. The augmentation scores for training points are obtained through training loss and model influence. Training loss is calculated directly from the training set, allowing for extension beyond SVMs and inclusion of data points beyond support vectors. Model influence is measured using Leave-One-Out (LOO) influence, which evaluates the impact of removing a training point on its own loss. The LOO influence is calculated based on the minimizer of the loss function and the Hessian matrix. The magnitude of the LOO influence is important for augmentation scores. The potential of using training loss and model influence for scoring is demonstrated through a histogram of model influence across CIFAR10 and NORB datasets. Most of the mass is centered around 0, allowing for ranking points by preference. The values are correlated before and after augmentation, indicating a reliable measure of future impact. Reweighting individual samples is motivated by augmentation policies that duplicate selected samples, resulting in a net effect of reweighting samples with twice the original weight. One approach to reweighting individual samples involves post-processing to correct class imbalanced sampling. By dividing weights of original and augmented samples by the set size, the original weight assigned to a point is conserved. More advanced reweighting policies, such as considering the trustworthiness of samples, are areas for future investigation. While reweighting can have a negative impact in some cases, it may be more beneficial for addressing class imbalance by altering the distribution over classes. Updating scores for data augmentation can be done by either using the same influence information or updating it to account for model behavior changes. Avoiding repeated influence calculations can reduce computation while maintaining similar performance. This technique may not justify the extra cost, as it does not significantly affect the policy's expected performance. The benefit is that it may only require computing selection metadata once throughout the augmentation process. Detailed results on proposed policies for data subsampling are provided in this section. In this section, detailed results are provided on the performance of proposed policies for data subsampling using Convolutional Neural Networks (CNN) and linear logistic regression models. The experiments explore augmentation policies on three datasets: MNIST, CIFAR10, and NORB. Different architectures are used for each dataset, and various augmentations such as translation, rotation, and crop are considered. The study also examines the impact of re-generating features for MNIST while training both features and models from scratch. The study explores augmentation policies on MNIST, CIFAR10, and NORB datasets using CNN and logistic regression models. Augmentations like translation, rotation, and crop are applied exhaustively to selected samples, with augmented points added back to the training set. Regularization's impact was found to be negligible. Augmented test points are added to the test set to make the augmentation effects more apparent. Imgaug is used for augmentations, and the code is written in Python using Keras CNN implementations. In the study, augmentation policies on MNIST, CIFAR10, and NORB datasets using CNN and logistic regression models were explored. Different policies for data augmentation were compared, with those based on loss and influence consistently outperforming random selection. The policies were evaluated by comparing test accuracy before and after augmentation, showing improvement with targeted augmentation strategies. The code for the study is publicly available online. The study compared augmentation policies based on loss and influence, showing that they outperformed random selection. These policies achieved full augmentation accuracy with only 5-10% of the data, with influence slightly outperforming loss. It was also noted that higher accuracy than full augmentation could be achieved with a reduced set of points, possibly due to a bias towards harder examples in the dataset. Additionally, the use of support vectors for augmentation was explored. The study explored augmentation policies based on loss and influence, showing superior performance compared to random selection. Support vectors were used for augmentation, resulting in strong performance on certain tests but not as reliable for finding the optimal subset of points for transformation. The approach has a fixed augmentation set size based on the number of support vectors, limiting its flexibility. The study examined augmentation policies based on reweighting samples and updating scores during augmentation. Results showed that while reweighting had a positive effect on MNIST, it could hurt performance in CIFAR10 and NORB. Updating scores had a slight positive impact but did not significantly improve performance compared to the original policy. The extra expense of model updating led to the conclusion that simpler policies may be more efficient. In conclusion, simpler policies are preferred due to the extra expense of model updating. Examining the top vs. bottom points in MNIST, CIFAR10, and NORB datasets shows the benefits of downsampling for promoting diversity and removing redundancy in learning invariances through augmentation efficiently. The proposed policies based on training loss and model influence can select the most useful subset of points for augmentation in machine learning models. The potential improvements in augmented training can scale superlinearly with respect to the original dataset size. Subset selection policies that consider the entire subset and encourage diversity may further improve performance. Most points have low loss and influence, indicating they can be augmented with low probability. The implementation details of the experiments include key architectural ideas such as data loading, augmentations, selection policy, featurization preprocessing, and logistic regression model in Python. The dataset is processed through a CNN model to obtain a feature vector, which is then used to train and test the logistic regression model. Loss and influence are measured for each training point, and augmentations are applied to the test set to close the performance gap. The implementation involves augmenting the training set in rounds using a policy that selects points to augment based on scores. Score updates can be enabled, and models may need to be retrained for testing. Experiments are conducted in Python using various libraries like Keras, Tensorflow, Scikit-Learn, AutoGrad, and Imgaug. CNNs are wrapped in Scikit-Learn transformers, and new classes are created for classifiers and influence functions. The implementation involves augmenting the training set using Imgaug for additional tuning. CNNs are used to create bottleneck features for input into a linear logistic regression model. Different models like LeNet and ResNet50v2 were trained for various datasets with different levels of success. For CIFAR10 tests, good performance was achieved without using augmentations in training. Pretrained ImageNet ResNet50 model resulted in poor performance. NORB showed good performance on the translate task without augmentations, but other augmentations degraded predictions. ResNet model was retrained with random rotations, shifts, and flips. Datasets were converted into binary classification tasks with specific class splits. Augmentations included translate, rotate, and crop applied to generate multiple augmented images. Augmentations are applied with various parameters to create multiple augmented images. Translate is applied for different pixel distances depending on the dataset, rotate is applied for specific rotations, and crop and zoom are used to adjust image dimensions. While augmentations are typically designed to preserve labels, it is possible to create augmentations that utilize label information or induce a change in labels. These specialized augmentations can be costly and require domain knowledge. The experiments conducted involved augmentations such as translate, rotate, and crop, with results presented in tables using abbreviations for different policies. The VSV method was used, along with SVM margin scoring to determine the importance of points in logistic regression models. The experiments involved augmentation with cluster-based stratified sampling using k-means clustering. Two policies were explored: Baseline Clustered and Random Proportional Influence Clustered. The performance of CIFAR10 k-DPP policies was evaluated using bottleneck features or a combination of different scoring methods. The performance of CIFAR10 k-DPP policies was evaluated using bottleneck features or a combination of influence and bottleneck features in the kernel. Only 250 augmented points were used due to computational constraints. The DPP kernel construction involved using influence for quality and bottleneck features for diversity. Results showed that using both influence and bottleneck features in the kernel outperformed using only bottleneck features. The influence weighted DPP performance is competitive with the influence driven approach. Using solely bottleneck features for L resulted in poor performance. Sampling a DPP takes O(N k^3), which can be limiting for larger k. Training a ResNet50v2 network on CIFAR10 dataset showed linear scaling performance. In experiments with MNIST data, training time decreased linearly when subsampling. Using a fraction of training data for CIFAR10 tasks also showed reduced training time without significant loss in model performance. Augmenting a subset of the training set has the potential to decrease training time."
}