{
    "title": "rygFWAEFwS",
    "content": "We propose SWAP, an algorithm to accelerate DNN training using large mini-batches and weight averaging. The resulting models generalize well and are produced in a shorter time. SGD is commonly used for DNN training, with larger mini-batches allowing for higher learning rates and faster loss reduction. In a distributed setting, multiple nodes can be utilized. In a distributed setting, multiple nodes can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates, with one synchronization event per iteration. Training with larger mini-batches requires fewer updates, thus fewer synchronization events, yielding good overall scaling behavior. However, there is a maximum batch size after which the resulting model tends to have worse generalization performance. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging the weights of a set of models sampled from the final stages of a training run. By generating multiple independent SGD sequences and averaging models from each, the resulting model achieves similar generalization performance. Stochastic Weight Averaging in Parallel (SWAP) accelerates DNN training by utilizing compute resources efficiently. SWAP achieves comparable generalization performance to models trained with small-batches in a time similar to large-batch training. It reduces training times for efficient models and is simple to implement with good results. The mechanism by which the training batch size affects generalization performance is still unknown. Larger mini-batches may lead to models getting stuck in sharper global minima. Various studies have shown conflicting results on the impact of batch size on model behavior and performance. In (Hoffer et al., 2017), the authors argue that using a larger batch size implies fewer model updates, impacting generalization performance. Training with large batches for longer times can improve generalization performance, but takes more time than the small-batch alternative. The batch size also affects the optimization process, as shown in (Ma et al., 2017) for convex functions in the over-parameterized setting. In the over-parameterized setting, convex functions show that a critical batch size exists where an iteration with batch size M is equivalent to M iterations with batch size one. Larger batch sizes do not improve convergence rate. Various methods with adaptive batch sizes exist but may require specific datasets or extensive hyper-parameter tuning. Local SGD and Post-local SGD are distributed optimization algorithms that trade off gradient precision with communication costs, resulting in better generalization and significant speedups. Stochastic weight averaging (SWA) is a method that improves model generalization by averaging models from later stages of training. SWAP, a variant of SWA, averages models after tens of thousands of updates, unlike Post-local SGD which does so after at most 32 iterations. This difference suggests that SWAP and Post-local SGD operate on different optimization mechanisms in deep neural networks. SWAP is an algorithm with three phases: workers train a single model together in the first phase, refine individual models independently in the second phase, and average the resulting models in the last phase. Phase 1 stops before reaching 100% accuracy to prevent optimization issues. Phase 2 involves reduced batch sizes for independent training. In SWAP, workers train together in a large-batch phase with synchronized models, then independently in a small-batch phase with diverging models. The averaged model outperforms individual models in testing accuracy. The accuracy of the averaged weight model is determined by averaging independent models and calculating the test loss. To visualize the SWAP mechanism, error is plotted on a plane with outputs from different algorithm phases. Orthogonal vectors are chosen to span the plane containing model parameters. Training and testing errors for the CIFAR10 dataset are plotted, showing the progression of the model through different phases. During phase 2, the model moved to a different side of the basin, with the final model 'SWAP' closer to the center. The variations in basin topology caused 'LB' and 'SGD' points to have higher errors, while 'SWAP' was less affected. Worker points 'SGD1', 'SGD2', 'SGD3' were at different sides of the basin, with 'SWAP' closer to the center. The change in topology led to higher testing errors for workers compared to 'SWAP'. The authors suggest that in later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process. The authors argue that SGD weight iterates behave like an Ornstein Uhlenbeck process, converging to a high-dimensional Gaussian distribution centered at the local minimum. The distribution's covariance grows with the learning rate and is inversely proportional to the batch size. Sampling weights from different SGD runs can generate independent samples from the stationary distribution, as long as all runs start in the same basin of attraction. In this section, the authors evaluate the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. They found the best hyper-parameters using grid searches and trained using mini-batch SGD with Nesterov momentum and weight decay. Data augmentation was done using cutout, and a custom ResNet 9 model was used for training. The authors evaluated the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. They used a custom ResNet 9 model and experimented with different batch sizes and training epochs. The results were compared in terms of test accuracies and training times. The study evaluated SWAP for image classification on CIFAR10, CIFAR100, and ImageNet datasets using custom ResNet 9 models. Significant test accuracy improvement was observed after averaging models. Training with small-batches achieved higher accuracy but took longer, while SWAP achieved comparable accuracy in less time. SWAP accelerated training of CIFAR10 to 94% accuracy in 27 seconds using 8 Tesla V100 GPUs. The default settings for this code modify the learning rates and batch sizes throughout the optimization. Small-batch experiments train ImageNet for 28 epochs on 8 Tesla V100 GPUs, while large-batch experiments double the batch size and learning rates on 16 Tesla V100 GPUs. SWAP phase 1 uses large-batch settings for 22 epochs, and phase 2 runs two workers with 8 GPUs each using small-batch settings for 6 epochs. Doubling the batch size reduces test accuracies, but SWAP recovers generalization performance with reduced training times. Results are compiled in Table 3, showing accelerations achieved with no tuning other than adjusting learning rates and reverting to the original schedule when transitioning between phases. When transitioning between phases, adjustments are made to batch size, number of GPUs, and learning rate. SWAP is compared with SWA using the CIFAR100 dataset. SWA samples models with 10 epochs in-between, while SWAP uses 8 independent workers for 10 epochs each. Both aim to recover test accuracy of small-batch training on a large-batch run. Initial training cycle with cyclic learning rates is followed to sample 8 models. The large-batch training run achieves lower accuracy, but SWA does not improve it. SWA with small-batches takes longer to compute the model but reaches test accuracy. SWA and SWAP start from the best model found by small-batch training, with peak learning rate selected through grid-search. Phase two starts from the model generated in phase one. SWAP achieves better accuracy than SWA with longer training time. SWAP speeds up over SWA by relaxing constraints and increasing the phase two schedule. SWAP algorithm uses large mini-batches for quick approximate solution and refines it by averaging weights from multiple models trained with small-batches. The final model has good generalization performance and is trained in a shorter time. The variant of SWA uses large-batches initially but achieves good generalization performance by refining models sequentially or in parallel. This method is validated on CIFAR10, CIFAR100, and ImageNet datasets. Visualizations show that averaged weights are closer to the center of the training loss basin. The large mini-batch run converges to the same basin as the refined models, suggesting a connection between regions of good and bad generalization performance. A hyperparameter for the transition point between large and small-batches is required for this method. For future work, the focus will be on choosing the transition point between large and small-batches in SWAP. Additionally, exploring SWAP's behavior with other optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad is planned. Parameters used in experiments were obtained through independent grid searches, with momentum and weight decay constants set at 0.9 and 5 \u00d7 10 \u22124 for CIFAR experiments. Tables 5 and 6 list the remaining hyperparameters, with a stopping accuracy of 100% indicating the maximum number of epochs used."
}