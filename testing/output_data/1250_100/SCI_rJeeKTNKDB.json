{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive and outperforms previous baselines in multiple molecular optimization tasks. Molecular optimization aims to improve biochemical properties by modifying compounds through graph-to-graph translation. The task of generating molecular graphs from input molecular structures is challenging due to the vast space of potential candidates and complex dependencies. Prior work utilized a junction tree encoder-decoder approach but had limitations in the separate encoding of trees and graphs. The decoding process was strictly sequential, impacting subsequent substructure choices. Our proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation addresses the limitations of prior work by interleaving the prediction of substructure components with their attachments in a non-autoregressive manner. This allows for modeling strong dependencies between successive attachments and substructure choices. The encoder represents molecules at different resolutions, while the decoder predicts triplet sequences to unravel the target graph, enabling accurate prediction of attachments and substructures. Our decoding process efficiently decomposes generation steps into smaller hierarchies to avoid combinatorial explosion. We handle conditional translation by inputting desired criteria, enabling different combinations at test time. Interleaving tree and graph decoding steps prevents generation of invalid junction trees. Our autoregressive decoder predicts substructures with attachments, addressing inconsistencies in local substructure predictions. The model is evaluated on various molecular optimization tasks, outperforming previous state-of-the-art methods. Our model significantly outperforms previous methods in discovering molecules with desired properties, showing improvements on QED and DRD2 optimization tasks. It runs 6.3 times faster during decoding and utilizes hierarchical decoding and multi-resolution encoding. Previous work on molecular graph generation includes methods that generate molecules based on SMILES strings or output adjacency matrices and node labels of graphs. Graph generation models have been proposed by various researchers, including those decoding molecules node by node and using hypergraph grammar. Our method, closely related to Jin et al. (2018), predicts substructures and their attachments jointly with an autoregressive decoder, unlike previous methods that introduce local independence assumptions and apply decoding stage-wise. Our method predicts substructures and their attachments jointly with an autoregressive decoder, representing molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees. It is related to graph encoders for molecules and learns to represent graphs in a hierarchical manner. Our method focuses on graph generation by encoding molecules into multiple sets of vectors at different resolutions. The encoder-decoder model with neural attention is used to learn a function that maps a molecule into another with improved chemical properties. The decoder adds new substructures and determines how they should be attached to the current graph, with attachment predictions made in two steps. The proposed method involves encoding molecules into a hierarchical graph with substructure, attachment, and atom layers. The encoder generates substructure, attachment, and atom vectors for decoding predictions. The decoder utilizes attention mechanisms and multi-layer neural networks for prediction steps. Substructures are defined as subgraphs of the molecule induced by specific vertices and edges. The substructures in the molecule are defined as subgraphs induced by specific vertices and edges. A substructure tree is constructed to show how substructures are connected. The graph decoder generates a molecule by expanding its substructure tree in a depth-first order. The model predicts new substructures based on the encoding of input. The model predicts new substructures in a molecule by creating a substructure tree and making topological, substructure, and attachment predictions. It uses a MLP with attention over substructure vectors to predict new substructures and their attachments in a depth-first order. The model predicts new substructures in a molecule by creating a substructure tree and making topological, substructure, and attachment predictions. It uses a MLP with attention over substructure vectors to predict new substructures and their attachments in a depth-first order. The prediction process involves enumerating possible configurations of attaching atoms, finding corresponding atoms in the substructure, and computing the probability of candidate attachments based on learned atom representations. The predictions are made in an autoregressive manner, where each step depends on the outcome of the previous step. During training, teacher forcing is applied to determine the generation order based on a depth-first traversal over the ground truth substructure tree. The attachment enumeration in the model is tractable due to small substructures. The encoder represents a molecule with a hierarchical graph, consisting of atom and attachment layers. The attachment layer provides necessary information for attachment prediction. The encoder represents a molecule with a hierarchical graph, consisting of atom and attachment layers. The attachment layer provides essential information for attachment prediction. Figure 4 illustrates the structure of A i and the vocabulary A(S i ). The substructure layer provides crucial information for substructure prediction in the decoding process. Edges connect atoms and substructures between layers to propagate information. The hierarchical graph H X for molecule X is encoded by a hierarchical message passing network (MPN) with three MPNs encoding each layer. The MPN encoding process, denoted as MPN \u03c8 (\u00b7) with parameter \u03c8, consists of three layers: Atom Layer MPN, Attachment Layer MPN, and Substructure Layer MPN. Each layer involves message passing between atoms, attachments, and substructures for T iterations to compute representations. The hierarchical graph of a molecule is encoded by these layers to predict attachments and substructures in the decoding process. The hierarchical encoder generates substructure representations through message passing over the substructure layer. The hierarchical MPN architecture is used during decoding to encode the hierarchical graph at each step, ensuring prediction depends on previously generated outputs. The model is trained on molecular pairs to generate diverse outputs using a variational translation model with an additional input z indicating the intended mode of translation. During testing, the model samples z from a Gaussian prior and uses variational inference for training. The posterior Q(z|X, Y) is computed by encoding X and Y into representations and summarizing structural changes with vector \u03b4 X,Y. The latent code z is used to reconstruct output Y in the decoder. Conditional translation is introduced to handle desired criteria input for multi-property optimization. During variational inference, \u00b5 X,Y and \u03c3 X,Y are computed with translation criteria g X,Y. The latent code is augmented as [z, g X,Y] for decoding. Users can specify criteria in g X,Y for control over outcomes. A novel conditional optimization task is constructed with input criteria for molecular similarity maintenance. Single-property optimization tasks are evaluated based on provided training and test data. In different tasks, the model is trained and evaluated on provided training and test sets without g X,Y as input. The LogP Optimization task involves translating input X into output Y where logP(Y) > logP(X). Two similarity thresholds are experimented with, and criteria for improving drug-likeness and DRD2 activity are encoded as vector g for conditional translation setup. Evaluation metrics include translation accuracy and diversity. The HierG2G method is compared against baselines like GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. Translation success rate and diversity are evaluated by measuring property improvement and Tanimoto distance between translated compounds. Our model, HierG2G, outperforms baselines like JTNN and CG-VAE in translation accuracy and output diversity on four tasks. It uses an atom-based translation model (AtomG2G) for direct comparison, achieving state-of-the-art results. Our model, HierG2G, outperforms AtomG2G on three datasets, showing the advantage of our hierarchical model in terms of translation accuracy and output diversity. Training our model on a specific criteria resulted in a low success rate, but transferring knowledge from other pairs improved performance significantly. Ablation studies were conducted on the QED and DRD2 tasks to analyze the impact of different architecture choices. Replacing the hierarchical decoder with an atom-based decoder resulted in a decrease in model performance. Reducing the number of hierarchies in the encoder and decoder also led to a drop in translation accuracy, highlighting the importance of substructure information in molecule analysis. In this paper, a hierarchical graph-to-graph translation model is developed for generating molecular graphs using chemical substructures. The model is fully autoregressive and learns coherent multi-resolution representations, outperforming previous models in various settings. The LSTM MPN architecture is used for both HierG2G and AtomG2G baseline, with a decrease in translation performance but still outperforming JTNN. The attention layer is a bilinear function, and the AtomG2G decoding process is illustrated in Figure 7. AtomG2G decoding process involves predicting new atoms attached to frontier nodes in the queue Q. The model predicts atom type of new node u t and bond type between u t and other nodes in Q for |Q| steps. AtomG2G is a direct comparison to HierG2G, representing molecules as molecular graphs. Training set sizes and substructure vocabulary are listed in Table 3. Multi-property optimization combines QED and DRD2 datasets. Hyperparameters for HierG2G are also discussed. The hyperparameters for HierG2G and AtomG2G models are set, including hidden layer dimensions, embedding layer dimensions, latent code dimension, KL regularization weight, and number of message passing iterations. CG-VAE models are used for molecule generation and property prediction tasks, with three models for logP, QED, and DRD2 optimization. At test time, compounds are translated into latent representations and gradient ascent is performed to maximize predicted property scores. The study explores the importance of keeping the KL regularization weight low for meaningful results in molecule generation. Ablation studies were conducted, including experiments with different decoder architectures and varying hierarchies in the encoder and decoder MPN. The results show the impact of these modifications on molecule representation and prediction. The study focuses on adjusting the hidden layer dimension to match the original model size in order to make topological and substructure predictions based on atom vectors."
}