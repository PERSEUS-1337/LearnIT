{
    "title": "rkZzY-lCb",
    "content": "Methods for calculating dense vector representations for features in unstructured data have been successful for knowledge representation. Feat2Vec is a method that calculates embeddings for data with multiple feature types, ensuring they exist in a common space. In supervised learning, it offers advantages over other methods by enabling higher prediction accuracy and avoiding the cold-start problem. Feat2Vec outperforms existing algorithms in unsupervised learning by leveraging the structure of multiple feature types to create dense embeddings. These embeddings have advantages such as enabling more efficient training and unsupervised learning. Word2Vec algorithms provide embeddings useful for various prediction tasks, with the continuous bag of words (CBOW) algorithm tuned to predict the next word in a sequence. Feat2Vec is a novel method that allows calculating embeddings of arbitrary feature types from both supervised and unsupervised data, outperforming existing algorithms in unsupervised learning by leveraging the structure of multiple feature types to create dense embeddings. Feat2Vec is the first algorithm to calculate general-purpose embeddings for arbitrary feature types, including documents. It is flexible, allowing for embeddings of unseen items and outperforms existing methods in unsupervised learning. Factorization Machine is a model that replaces individual pairwise parameters with a vector of parameters for each feature, reducing the number of parameters to estimate. It uses latent factors to encode interactions between features, making the model less expressive but exploiting shared latent structure. Feat2Vec extends the Factorization Machine model by allowing grouping of features and enabling arbitrary feature extraction functions. It introduces structure into feature interactions by defining feature groups where interactions only occur between different feature groups. Feat2Vec extends the Factorization Machine model by allowing grouping of features and enabling arbitrary feature extraction functions. It introduces structure into feature interactions by defining feature groups where interactions only occur between different feature groups. The addition of deep extraction methods in Feat2Vec results in a statistical model where sub-features are grouped together, allowing for interactions at a higher level of abstraction. This grouping enables feature embeddings to interact with those of other groups, enhancing the overall understanding of the data. An example application includes grouping individual words in a document into a \"document\" feature group, which can then interact with other document metadata embeddings. Feat2Vec extends the Factorization Machine model by allowing grouping of features and enabling arbitrary feature extraction functions. It introduces structure into feature interactions by defining feature groups where interactions only occur between different feature groups. The addition of deep extraction methods in Feat2Vec results in a statistical model where sub-features are grouped together, allowing for interactions at a higher level of abstraction. This grouping enables feature embeddings to interact with those of other groups, enhancing the overall understanding of the data. Feat2Vec can be used to address the cold-start problem by utilizing alternative descriptions of items, such as images or text passages. Feat2Vec extends the Factorization Machine model by introducing structure into feature interactions through feature groups. It allows for arbitrary feature extraction functions and deep extraction methods, enhancing the overall understanding of data. Feat2Vec can address the cold-start problem by utilizing alternative item descriptions like images or text passages. In contrast, the approach in FIG1 focuses on treating words as indexed features within structured feature groups, improving predictive accuracy by using neural networks within factorization machines. In this work, a deep factorization model is proposed to learn parameters using training data by minimizing a loss function. For labeling and classification tasks, binary cross-entropy is optimized, while mean squared error is optimized for regression tasks. Neural models are learned using mini-batch updates with the Keras programming toolkit. The embeddings can no longer be interpreted as containing latent factors related to the target task when using a neural function instead of the dot product. The approach can be combined with Feat2Vec but is not explored in this work. In this paper, models are built using the Keras programming toolkit BID3, now part of Tensorflow BID0, for multiclass or binary classification. ADAM BID12 optimization algorithm is used, and for multi-label tasks, Feat2Vec with binary output is employed. Negative labels are sampled for each training example, and different sampling strategies are used based on validation error. Feat2Vec can be used to learn embeddings in an unsupervised setting with no explicit target for prediction. The training dataset consists of observed data, such as documents. Positive and negative examples are required for Feat2Vec, with unobserved data serving as negative examples. Implicit sampling is used to select a fixed number of negative labels for each positively labeled record, particularly in scenarios with high-dimensional feature groups. Feat2Vec uses implicit sampling to select negative labels for each positively labeled record, allowing for unsupervised learning of embeddings for structured feature sets. Unlike Word2Vec, Feat2Vec does not constrain feature types to be words, enabling the model to reason on more abstract entities in the data by grouping subfeatures. For example, in experiments on a movie dataset, Feat2Vec groups non-mutually exclusive indicators for movie genres like comedy, action, and drama films. Feat2Vec uses implicit sampling to select negative labels for each positively labeled record in unsupervised learning. It iterates over observations in the training dataset, randomly selecting a feature group from a noise distribution to create negative observations. This method allows for reasoning on abstract entities in the data by grouping subfeatures. The noise distributions used in Feat2Vec are described, with sampling of feature groups based on complexity. The hyper-parameter \u03b1 1 adjusts the sampling probabilities to flatten the distribution. The feature sampling rate varies with complexity, using a strategy similar to Word2Vec. Negative samples are handled through Noise Contrastive Estimation (NCE) in unsupervised learning of embeddings. The statistical model \u0177 = p(y = 1| x, \u03c6, \u03b8) in Equation 3 accounts for random negative labels that may look like positive ones. Parameters \u03b8 are learned during training. For unsupervised learning, Equation 3 with a logistic link function \u03c9 is used. NCE requires calculating a partition function Zx for each unique record type x, but setting Zx = 1 does not significantly affect model performance, as shown by BID18. This simplifies computation and model complexity. The new structural probability model for Feat2Vec optimizes parameters \u03b8 and feature extraction functions \u03c6, while considering the probability of negative samples. It has theoretical properties similar to Factorization Machines, allowing for multi-label classification. Feat2Vec is equivalent to optimizing a convex combination of loss functions from n individual Factorization Machines, resulting in n multi-label classifiers. Experiments involve a development set and a 10% test set, with results reported on a single training-test split. For multi-label classification, a probability is predicted for each document-label pair using AUC-ROC evaluation metric. Negative labels are sampled based on label frequency to prevent bias towards popular labels. The evaluation strategy for Feat2Vec involves using AUC as a metric for classification and MSE for regression tasks. Regularization was found to slow down convergence without improving prediction accuracy, so early stopping is used to prevent overfitting. The feature extraction function for text utilizes a Convolutional Neural Network (CNN), and hyperparameters are chosen based on previously published guidelines. The code for the experiments is shared online for reproducibility. Feat2Vec is compared with Collaborative Topic Regression (CTR) on the CiteULike dataset, consisting of scientific articles and users. CTR's performance degrades significantly for unseen documents, while Feat2Vec achieves better AUC and faster training with GPU acceleration. Feat2Vec is compared with DeepCoNN, a method for predicting customer ratings using textual reviews. Feat2Vec shows significant performance improvement on the Yelp dataset by utilizing a feature extraction function for item identifiers, users, and review text. Comparisons with DeepCoNN's results are made due to the lack of a public implementation. Feat2Vec provides a large performance increase over Matrix Factorization in terms of mean squared error. It is claimed to be more general and efficient, especially for datasets with a large number of reviews per user and per item. The performance of the unsupervised Feat2Vec algorithm is evaluated through a ranking task using unseen records. The algorithm is compared to Word2Vec's CBOW algorithm for learning embeddings based on cosine similarity. In the movie dataset, cosine similarity is used to compare movie directors to actors in the same film. For the educational dataset, rankings of textbooks are evaluated based on the similarity of textbook and user embeddings. The evaluation is done using mean percentile rank (MPR), where a lower value indicates better performance. Further details on the experimental setup can be found in the appendix. The Internet Movie Database (IMDB) dataset contains information on 465,136 movies, including details on writers, directors, and cast members. A proprietary educational dataset from a leading technology company includes 57 million observations with 9 categorical feature types. In the movie dataset, cast member embeddings are used to predict the actual director of a film by ranking directors based on cosine similarity to the summed cast member vector. Feat2Vec outperforms CBOW in predicting the actual director, with a Top-1 Precision metric of 2.43% compared to CBOW's 1.26%. Feat2Vec's advantage lies in its ability to handle real-valued features with complex extraction functions, such as predicting movie ratings based on IMDB rating embeddings. The experiment involved predicting movie ratings in the test dataset using IMDB rating embeddings similar to the director's embedding. Varying the flattening hyperparameter \u03b11 affected the performance, with Feat2Vec outperforming Word2Vec's CBOW algorithm in all settings. The results showed that Feat2Vec consistently performed better than random prediction and CBOW in the prediction task. The algorithm's performance is not very sensitive to hyperparameter choice and has been outperformed by Feat2Vec across all settings. Field-Aware Factorization Machine BID10 allows different weights for feature interactions but lacks feature groups or extraction functions like Feat2Vec. Various algorithms have been proposed for generating embeddings of entities other than words, such as biological sequences BID1 and network graphs BID19. Generative Adversarial Networks (GANs) have been used for unsupervised image and language embeddings but not for jointly embedding multiple feature types. Adversarial training could be an alternative to NCE for unsupervised learning. A promising algorithm called StarSpace BID29 with similar goals is still in development. Feat2Vec can embed all types of features, unlike StarSpace which is limited to bag of words. Feat2Vec's embeddings are generalizable and outperform other algorithms. It decouples feature extraction from prediction, making it a versatile and easily interpretable method. Feat2Vec's embeddings outperform algorithms designed for text, even with the same feature extraction CNN. In the unsupervised setting, Feat2Vec captures relationships across features better than Word2Vec's CBOW algorithm. It exploits dataset structure for more sensible embeddings and is the first method to calculate continuous representations of data with arbitrary feature types. Future work could focus on reducing human knowledge requirements and automating feature grouping and extraction functions. Overall, supervised and unsupervised Feat2Vec are evaluated on two datasets each. In the evaluation, supervised and unsupervised Feat2Vec are tested on two datasets each. The testing sets are carefully selected to ensure the models learn about specific characteristics. Cross-validation is performed to determine the number of epochs for training. In the evaluation, supervised and unsupervised Feat2Vec are tested on two datasets each. Cross-validation is performed to determine the number of epochs for training. Regularization of embeddings during training did not significantly impact results. Left-out entity pairs in the test dataset are ranked based on cosine similarity of target and input embeddings. Different settings are used for the IMDB and educational datasets, with specific parameters for training Feat2Vec. CBOW is implemented for unsupervised experiments, with specific extraction functions used for the IMDB dataset. In the datasets, documents are created with tokenized information for Feat2Vec. Features are prepended with their names and truncated to 10 levels. Sequences are padded with \"null\" to maintain fixed length. CBOW Word2Vec algorithm is used with a context window covering all tokens. Unique embeddings are learned for categorical variables without one-hot encodings. The Feat2Vec algorithm allows for multiple categories to be active, resulting in a single embedding for the group. Text is preprocessed by removing non alpha-numeric characters, stopwords, and stemming words. Real-valued features are passed through a 3-layer feedforward neural network to output a vector of dimension r. This highlights one advantage of Feat2Vec: using numeric values as embeddings. Feat2Vec algorithm allows for multiple categories to be active, resulting in a single embedding for the group. It can learn a highly nonlinear relation mapping a real number to a high-dimensional embedding space. Feat2Vec outperforms CBOW in extracting information into embeddings, especially for entities that appear sparsely in the data. The Feat2Vec algorithm excels at extracting information into embeddings, particularly for entities that are sparse in the data. The gradient for learning embeddings with Feat2Vec is a convex combination of gradients from targeted Factorization Machines for each feature in the dataset. The loss function in the algorithm is a convex combination of targeted classifiers' loss functions for each feature, leading to a gradient that is also a convex combination. The algorithm learns a convex combination of gradients for targeted classifiers on each feature, weighted by feature group sampling probabilities. The feature extraction network for labeling tasks uses 1000 convolutional filters with a width of 3 words. The network converts input words into a sequence of one-hot encodings and pads or truncates the text as needed. The input text is processed through layers including an embedding layer and convolutional filters to extract features. Filters are applied to groups of adjacent word embeddings, resulting in vectors of varying lengths. Zero-padding is used for shorter texts to ensure learning from text features. To enforce learning from the features of the text, 1-max pooling is applied to the output of convolutional filters. A fully connected layer with ReLU activation is used to learn higher-level features. Dropout regularization is applied during training to prevent overfitting. The final embedding for x j is computed by a dense layer with an activation function. The maximum vocabulary size is set to 100,000 words with an input embedding size of 50. The input word embeddings and label embeddings are initialized using Word2Vec. Multiple architectures or hyperparameter settings have not been evaluated, but good results are obtained on diverse datasets. The CNN architecture used for DeepCoNN consists of a word embedding lookup table, convolutional layer, 1-max pooling, and a fully connected layer. Hyperparameters include 100 convolution filters, 50 units for the fully connected layer, word embedding size of 100, vocabulary size of 100,000, and maximum document length of 250. PReLU activations are used for the final layer in the CTR dataset to address the issue of ReLU units 'dying' during training. Comparisons are made with Collaborative Topic Regression using Feat2Vec. To compare Feat2Vec with Collaborative Topic Regression, different embedding sizes were tested (r \u2208 {5, 10, 15) to find the best performance. Results are shown in TAB2.2."
}