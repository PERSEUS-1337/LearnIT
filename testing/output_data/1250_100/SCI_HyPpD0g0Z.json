{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"conditionally invariant\" features X^ci and \"orthogonal\" features X^orth. The former features do not change substantially across domains, while the latter can vary significantly. To prevent adversarial domain shifts, it is ideal to use the \"conditionally invariant\" features for classification. The distributional change of features across different domains is not directly observable due to the latent nature of the domain variable. In data augmentation, we can generate multiple images from one original image using an ID variable. This method only requires a small fraction of images to have an ID. In data augmentation, multiple images can be generated from one original image using an ID variable. This method only requires a small fraction of images to have an ID. A causal framework is provided by adding the ID variable to the model of Gong et al. (2016). The domain is treated as a latent variable in settings where it cannot be observed directly. Samples with the same class and identifier are treated as counterfactuals under different style interventions. Regularizing the network using a grouping-by-ID approach improves performance in settings with changing domains. Deep neural networks have achieved outstanding performance on prediction tasks like visual object and speech recognition, but issues can arise due to domain shifts caused by changing conditions. Domain shifts in machine learning systems can lead to degraded predictive performance, as seen in the \"Russian tank legend\" example where sampling biases affected training data. Hidden confounding factors, like image quality, can create indirect associations. Deep learning requires large sample sizes to mitigate these effects. Large sample size is crucial for achieving invariance to known factors like translation, point of view, and rotation in data augmentation. Adversarial examples, imperceptibly perturbed inputs misclassified by ML models, do not fool humans. Mimicking human ability to learn invariances from few instances and aligning DNN features with human cognition is a goal. Controlling input data characteristics to avoid biases in learned representations is important for fairness and discrimination considerations. Existing biases in datasets used for training ML algorithms can be replicated in estimated models. For example, Google's photo app once tagged two non-white people as \"gorillas\" due to biased training data. To address this issue, counterfactual regularization (CORE) is proposed to control latent features extracted by the estimator. CORE focuses on 'conditionally invariant' (core) features related to the target of interest, making the estimator robust to adversarial domain shifts. CORE relies on observing \"counterfactuals\" in certain datasets, where the same object is seen under different conditions. It exploits grouping knowledge to improve predictive performance. The manuscript includes motivating examples, related work review, and introduces counterfactual regularization. The CelebA dataset is used to classify whether a person wears glasses, utilizing grouping information to ensure consistent predictions for images of the same person. The study utilizes counterfactual observations to improve predictive performance by exploiting grouping information. By grouping instances of the same identity, the test error is reduced significantly, showcasing the effectiveness of leveraging the group structure in the training process. The study demonstrates that using CORE reduces test error significantly by leveraging grouping information, making data augmentation more efficient. By enforcing invariance with respect to style features, CORE improves estimator performance compared to standard methods. The degree of rotations is randomly sampled from [35, 70]. Using CORE reduces test error from 32.86% to 16.33%. Similar works include BID14 and Domain-Adversarial Neural Networks (DANN). BID13 focuses on learning a representation without discriminative information, while BID14's data generating process is similar to our model. The approach in BID14 identifies conditionally independent features by adjusting variables to minimize MMD distance between distributions in different domains. A key difference is the use of a different data basis and the explicit observability of the domain identifier in BID14. Causal modeling, like in BID17 and BID1, aims to guard against adversarial domain shifts and interventions on predictor variables. Transferring these results to adversarial domain changes in image classification faces challenges due to the anti-causal nature of the classification task. The challenge in anti-causal prediction lies in guarding against domain shifts in style features, which standard causal inference struggles to address. Recent approaches leverage causal motivations in deep learning or use deep learning for causal inference, but they focus on cause-effect inference rather than anti-causal prediction and non-ancestral interventions on style variables. BID27 proposes the Neural Causation Coefficient (NCC) to estimate causal relations between image features, distinguishing between object features and context features. The text discusses the use of generative neural networks for cause-effect inference and causal generative models. Various approaches are proposed, including fitting CGANs in different directions, using deep latent variable models, and devising regularizers to estimate causal probabilities. The focus is on identifying causal relationships between features and utilizing causal graph structures in the models. The text discusses the use of deep latent variable models and proxy variables to estimate individual treatment effects, as well as exploiting causal reasoning for fairness considerations in machine learning. Algorithms avoiding proxy discrimination require classifiers to be constant as a function of the proxy variables in the causal graph, showing structural similarity to disentangling factors of variation in generative modeling. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce the dependence of latent representation on specified transforms, such as location as an orthogonal style feature. The goal is to learn a latent representation that excludes certain features, such as location, image quality, posture, brightness, background, and contextual information. The approach involves a confounding situation where style features differ based on class, and grouped observations are exploited to separate style and content in a variational autoencoder framework. The focus is on solving a classification task directly without explicitly estimating latent factors in a generative framework. The text discusses classification tasks and parameter estimation in machine learning, focusing on minimizing expected loss through empirical risk minimization. It also introduces the standard notation for classification and regression, emphasizing the use of a predictor X and a target Y. The goal is to develop a causal graph to compare adversarial domain shifts with transfer learning, domain adaptation, and adversarial examples. The text discusses parameter estimation in machine learning, emphasizing minimizing loss through empirical risk minimization. It introduces a structural model for variables, including latent variables like the ID variable. The prediction is anti-causal, with the class label causing changes in the image. Causal effects are mediated through core and style features, with interventions possible on style features but not on core features. The text discusses interventions on style features in machine learning, where the distribution of style features can change across domains. Core features are conditionally invariant, while style features like point of view, image quality, and color changes are context-dependent. The style intervention variable influences both latent style and the image prediction. In this work, the focus is on guarding against adversarial domain shifts using causal graphs. The goal is to minimize adversarial loss by devising a classification that considers interventions on style features, leading to potential misclassifications due to imperceptible changes in input dimensions. The aim is to estimate the conditional distribution of Y given the image under chosen interventions, addressing adversarial domain shifts with strong interventions on style features. The text discusses guarding against adversarial domain shifts using causal graphs and minimizing adversarial loss through interventions on style features. It aims to estimate the conditional distribution of Y given the image under chosen interventions to address shifts in test data distributions. The problem of causal inference is highlighted, emphasizing the challenge of observing counterfactuals in certain scenarios. The text discusses the challenge of observing counterfactuals in causal inference, focusing on interventions on style features to address adversarial domain shifts. It compares the concept of counterfactuals in medical examples to image analysis, where observing different conditions ('treatments') is feasible. In image analysis, different images of the same person are examined for style interventions to rule out parts of the feature space for classification. The focus is on penalizing changes in classification under different style interventions while keeping class and identity constant. The standard approach pools observations without considering grouping information. The pooled estimator in image analysis uses a ridge penalty and cross-validated choice of penalty parameter. The adversarial loss may be infinite, but the estimator performs well in terms of adversarial loss. Conditions are outlined for the estimator to work effectively, including the absence of certain edges in the model. To minimize adversarial loss, the function must remain constant across different inputs. The text discusses the importance of ensuring a constant function across different inputs to minimize adversarial loss. It introduces the concept of an invariant parameter space and the use of empirical risk minimization to approximate the optimal invariant parameter vector. The unknown invariant parameter space is approximated by an empirically invariant space using a regularization constant. The text discusses the concept of invariant parameter space and the use of empirical risk minimization to approximate it. It mentions the use of a penalty parameter \u03bb in constrained optimization and the graph Laplacian regularization to penalize variances. The graph is formed in the sample space based on the identifier variable ID. The text discusses the importance of defining the graph based on the identifier variable ID for effective regularization against adversarial domain shifts. It also highlights the differences in performance between various regularizations. Additionally, it explores the adversarial loss in logistic regression for binary classification, showing that the CORE estimator outperforms the pooled estimator in handling confounded training data and changing style features in test distributions. The text discusses the CORE estimator's performance in handling confounded training data and changing style features in test distributions. It includes experiments on classifying elephants and horses, gender and wearing glasses, and brightness. Additional experimental results and implementation details are provided. The text also addresses the sensitivity of the tuning parameter \u03bb and the causal relationship between height and age in stickmen images. The text discusses the dependence between age and movement in training data, illustrated in FIG0.9, due to a hidden common cause. It mentions how a model relying on this dependence may fail when presented with new data. Test sets 2 and 3 intervene on X \u22a5, removing the dependence between Y and X \u22a5. Large movements are associated with both children and adults in these test sets. FIG0.10 shows examples from all test sets. Misclassification rates for CORE and the pooled estimator are shown for c = 50 with a total sample size of m = 20000. CORE achieves good predictive performance with as few as 50 counterfactual observations. CORE achieves good predictive performance on test sets 2 and 3 with as few as 50 counterfactual observations, outperforming the pooled estimator. The learned representation of the pooled estimator relies on movement as a predictor for age, while CORE does not use this feature due to counterfactual regularization. Including more counterfactual examples would not improve the pooled estimator's performance. The text uses the CelebA dataset to classify whether a person in an image is wearing eyeglasses, with X\u22a5 representing image quality that differs based on Y. The strength of the image quality intervention is controlled by sampling from a Gaussian distribution. In the study, counterfactual observations are used to evaluate the impact of image quality on classification performance. Different test sets with varying image quality interventions are analyzed, showing that the pooled estimator outperforms CORE on test set 1. The pooled estimator can leverage predictive information from image quality, while CORE is restricted from doing so. The study evaluates the impact of image quality on classification performance using counterfactual observations. The pooled estimator outperforms CORE on test sets 2-4 by using image quality as a predictor, while CORE's performance is not affected by changing image quality distributions. The experiment aims to achieve invariance with respect to color in the learned representation, using the \"Animals with attributes 2\" dataset. The data generating process includes counterfactual examples with grayscale images for elephants. Test sets 2 and 3 have modified color distributions, affecting the pooled estimator but not CORE's performance. The experiment aims for color invariance using the \"Animals with attributes 2\" dataset. The CORE estimator aims for color invariance by learning from grayscale images of elephants. It distinguishes core and style features in images and uses counterfactual regularization to achieve robustness against interventions on style features. If \"color\" is a protected attribute, CORE ensures fairness by not including it in its learned representation. The CORE estimator aims to achieve invariance of classification performance with respect to adversarial interventions on style features such as image quality, fashion type, color, or body posture. It can work despite sampling biases in the data and has applications in known and unknown style features. Regularization of CORE penalizes features that vary strongly between different instances of the same object in the training data, avoiding the usage of unknown style features. Larger models like Inception or large ResNet architectures can be used with CORE for better results. The CORE estimator aims to achieve invariance of classification performance with respect to adversarial interventions on style features. It can work despite sampling biases in the data and has applications in known and unknown style features. The approach involves using logistic regression to predict class labels while considering interventions on style features and their linear impact on images. The potential future direction includes using video data for grouping and counterfactual regularization, which could also help debias word embeddings. The CORE estimator focuses on achieving classification performance invariance to adversarial interventions on style features using logistic regression. It considers interventions on style features and their impact on images, with potential applications in addressing sampling biases and known/unknown style features. The approach may extend to using video data for grouping and counterfactual regularization to debias word embeddings. The formulation of Theorem 1 relies on certain assumptions: \u2206 is sampled from a distribution in Rq with positive density, matrix W has full rank q, and c \u2265 q. The sampling process involves collecting independent samples and selecting counterfactual examples. Theorem 1 states that under these assumptions, the pooled estimator has infinite potential. The pooled estimator has infinite adversarial loss with probability 1 in the training data. The CORE estimator also converges to infinity for misclassification loss as n approaches infinity. The proof involves showing that W t\u03b8pool = 0 with probability 1, leading to the conclusion that the pooled estimator has infinite potential. The counterfactual training data x i,j (0) is unaffected by interventions (\u2206 i,j = 0), leading to the oracle estimator \u03b8 * being the same for both true and counterfactual data. The derivative g(\u03b4) in FORMULA24 can be expressed as the difference between FORMULA24 and FORMULA25. Interventions \u2206 i,j can be written as W \u2206 i,j, where \u03b4 = W u. The estimator \u03b8 * is independent of interventions \u2206 i,j, as it remains identical whether trained on original or counterfactual data. The interventions are drawn from a continuous distribution, leading to the left hand side of the equation having a continuous distribution. The probability of the left hand side not being 0 is 1, proven by contradiction. With probability 1, the core parameter equals the true parameter. The estimator remains unchanged when using data without interventions for training. The population-optimal vector can be expressed in different forms. In the CelebA dataset, a confounding problem is created by including mostly images of men wearing glasses and women without glasses. Counterfactuals are used to address this issue, with images of the same person without glasses for males and with glasses for females. This is referred to as \"CF setting 2\". Test set 1 and test set 2 have different distributions regarding gender and glasses associations. In the CelebA dataset, a confounded setting is analyzed where the association between gender and glasses is reversed. The study compares training a four-layer CNN end-to-end with using Inception V3 features and retraining the softmax layer. Results show similar trends with increasing c values, indicating smaller performance differences between estimators. The pooled estimator performs worse on test set 2 as m increases, suggesting a larger exploitation of X \u22a5. The study focuses on classifying whether a person in an image is wearing eyeglasses in a confounded scenario involving a hidden common cause related to indoor/outdoor image settings. The study analyzes a confounded setting in the CelebA dataset where the association between gender and glasses is reversed. The pooled estimator performs better than CORE on test set 1 by exploiting predictive information in image brightness. However, it does not perform well on test sets 2 and 4 as it uses brightness as a predictor for the response, which fails in those scenarios. The study compares the performance of different methods on the CelebA dataset. The pooled estimator outperforms CORE on test set 1 by using image brightness as a predictor, but fails on test sets 2 and 4 due to differences in brightness distribution. Counterfactual settings are explored, with setting 1 showing the best results in isolating brightness as the invariant factor. The study explores the impact of tuning parameters on predictive performance, showing that CORE helps mitigate confounders in small sample sizes. Varying the number of identities in the training dataset affects misclassification rates, with CORE outperforming the pooled estimator, particularly with small sample sizes. The study demonstrates that as sample sizes increase, the performance of CORE and the pooled estimator become comparable, with fewer confounding factors present. Results show that CORE has lower misclassification rates on rotated digits, making data augmentation more efficient. The CORE estimator's performance is not sensitive to the number of counterfactual examples. The CORE estimator shows consistent performance regardless of the number of counterfactual examples. The pooled estimator struggles with predictive performance on test sets 2 and 3, using \"movement\" as a predictor for \"age\". Experiments were conducted for counterfactual settings 1-3 with c = 5000. Counterfactual setting 1 yielded the best results, while settings 2 and 3 had similar predictive performance. A notable performance difference was observed between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator, possibly due to image quality not being predictive enough. The models were implemented in TensorFlow, with rotations applied to all images in test set 3. The model architectures used in the experiments are detailed in TAB1. The CORE and pooled estimators have the same network architecture and training procedure, with only the loss function differing. Experimental results are based on training each model five times to assess variance. Training data is shuffled in each epoch, with mini batches containing counterfactual observations. The mini batch size is set to 120, making optimization more challenging for small c values."
}