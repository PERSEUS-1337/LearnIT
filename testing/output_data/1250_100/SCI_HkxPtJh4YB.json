{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for computing matrix expectations in an exponential family defined over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, offering a solution to the intractability of computing the permanent of a matrix. Our approach is demonstrated to be effective in probabilistic neuron identification in C.elegans. Sinkhorn variational marginal inference is introduced as a scalable method for computing matrix expectations in an exponential family over permutation matrices. The Sinkhorn approximation of the permanent is used to address the challenge of computing the permanent of a matrix. This approach is shown to be effective in probabilistic neuron identification in C.elegans. The Sinkhorn variational marginal inference method introduces a scalable approach for computing matrix expectations in an exponential family over permutation matrices. It addresses the challenge of computing the permanent of a matrix by using the Sinkhorn approximation. This method has been effective in probabilistic neuron identification in C.elegans. The approximation is based on replacing the intractable dual function with a component-wise entropy, resulting in the Sinkhorn permanent. Bounds for this approximation are provided, and it has been independently proposed recently. The Sinkhorn approximation, proposed independently, lacks a theoretical framework. The Bethe variational inference method offers a general rationale for variational approximations in graphical models. It has been successfully applied to permutations, providing better theoretical guarantees than the Sinkhorn approximation. Computational differences exist between the two methods. The Sinkhorn and Bethe algorithms offer different approaches to approximating marginals in graphical models. While Bethe iterations involve complex message computations, Sinkhorn iterations tend to produce qualitatively better marginals in some cases. Sinkhorn iterations also scale better for moderate values of n, as observed in experiments with the C.elegans dataset. In the C.elegans dataset, approximations were made using Sinkhorn and Bethe algorithms for marginal inference. Sampling-based methods were also considered, but an elementary MCMC sampler failed to produce sensible inferences. The C.elegans species has a stereotypical nervous system with a consistent number of neurons and connections. Recent advances in neurotechnology have enabled whole brain imaging in C.elegans, allowing for the study of how brain activity relates to behavior. A technical problem of identifying worm neurons using probabilistic neural identification was addressed using NeuroPAL. This involved estimating probabilities for neuron identification, providing uncertainty estimates for model predictions. NeuroPAL uses a gaussian model for canonical neurons with parameters inferred from annotated worms. A permutation is used to determine the likelihood of observing data. A downstream task involves computing probabilistic neural identifies, where humans label uncertain neurons to improve model accuracy. NeuroPAL uses a gaussian model for canonical neurons with parameters inferred from annotated worms. A downstream task involves computing probabilistic neural identifies, where humans label uncertain neurons to improve model accuracy. Results show that Sinkhorn and Bethe approximations outperform other baselines, with Sinkhorn slightly better due to more accurate estimates of low probability marginals. MCMC does not provide better results than the naive baseline, indicating a lack of convergence for chain. The Sinkhorn approximation for marginal inference is a sensible alternative to sampling, providing faster and more accurate approximate marginals than the Bethe approximation. The relation between quality of permanent approximation and corresponding marginals needs further analysis. The (log) Sinkhorn approximation of the permanent of L is obtained by evaluating S(L) in the problem it solves. The dataset used consists of ten NeuroPAL worm heads with available human labels. The log-likelihood matrix L was computed for ten NeuroPAL worm heads with neuron numbers ranging from 180 to 195. The Sinkhorn and Bethe approximations were used with 200 iterations each. Results showed convergence with these values. The MCMC sampler method by Diaconis (2009) was used with 100 chains of length 1000. Results were obtained on a desktop computer with an Intel Xeon W-2125 processor. The Bethe approximation algorithm was implemented for efficient log-space computation. In a study on C.elegans matrices, 1000 submatrices of size n were randomly drawn from ten available log likelihood matrices. Error bars were too small to be displayed."
}