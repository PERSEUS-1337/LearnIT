{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models. This vulnerability allows attackers to exploit black-box systems. In this work, the adversarial perturbation is decomposed into model-specific and data-dependent components, with the latter contributing mainly to transferability. Adversarial examples are crafted using noise reduced gradient (NRG) to approximate the data-dependent component, enhancing transferability significantly across various ImageNet models. Low-capacity models show stronger attack capability compared to high-capacity models with similar test performance. These findings offer a principled approach to creating successful adversarial examples and insights for developing defense strategies against black-box attacks in the era of large neural network models used in real-world applications. The works show that adversaries can manipulate inputs to produce incorrect outputs, creating adversarial examples. Understanding this phenomenon and defending against it effectively are open questions. Adversarial examples can transfer across different models, allowing attacks on black-box systems. Adversarial vulnerability is attributed to the linear nature and high dimensionality of deep neural networks. The FGSM and DeepFool methods were proposed for crafting adversarial examples. Different approaches were explored for white-box and black-box attacks, with a focus on transferability. Defense mechanisms like defensive distillation, adversarial training, and image transformation were also studied. Some works aimed at detecting adversarial examples, but they were found to be easily broken. In this work, the transferability of adversarial examples is explained, leading to enhanced black-box attacks. Adversarial perturbations consist of model-specific and data-dependent components, with the latter contributing to transferability across models. A noise-reduced gradient (NRG) method is proposed to construct adversarial examples, showing significant improvements on the ImageNet validation set. The success rate of black-box attacks can be dramatically increased by using noise reduced gradient in conjunction with other methods. Model-specific factors like capacity and accuracy influence the success rate, with higher accuracy and lower capacity models being more effective. This phenomenon is explained by transferability and provides guidance for attacking unseen models. In this work, adversarial examples are studied in the context of deep neural networks. Non-targeted attacks aim to misclassify x by the model, while targeted attacks aim to produce a specific wrong label. In a black-box attack, the adversary has no knowledge of the target model but can construct adversarial examples on a local model to fool the target model. Crafting adversarial perturbation involves optimizing a loss function J to measure the prediction-ground truth discrepancy and perturbation magnitude. The distortion is typically measured using \u221e and 2 norms. Ensemble-based approaches, like using a large ensemble of source models, can improve adversarial example strength. Ensemble-based approaches improve adversarial example strength by averaging predicted probabilities of each model. Different optimizers can be used, with the paper mainly using a normalized-gradient based optimizer. Two methods, Fast Gradient Based Method and Iterative Gradient Method, are discussed for solving non-targeted and targeted attacks. The normalized gradient is used in fast gradient-based methods for attacks, with different choices for different attack types. Understanding transferability of adversarial examples between models is crucial for black-box attacks and defenses. Previous works suggest transferability comes from similarity in decision boundaries and span a contiguous subspace. Investigating what enables transferability between models with high performance on the same dataset. The transferability of adversarial examples between models is influenced by factors such as model architectures and random initializations. Perturbations can be decomposed into data-dependent and model-specific components, with the former contributing more to transferability due to shared data-dependent information. Model-specific components have varying behaviors off the data manifold. This is illustrated in FIG0, where the decision boundaries of two models are similar in the inter-class area, leading to misguidance for both models by the adversarial perturbation crafted from model A. The transferability of adversarial examples between models is influenced by factors such as model architectures and random initializations. Perturbations can be decomposed into data-dependent and model-specific components. The model-specific component inherits from random initialization and is noisy, while the data-dependent component encodes knowledge learned from training data. To increase success rates of black-box adversarial attacks, enhancing the data-dependent component is crucial. The NRG method reduces model-specific noise to achieve this. The NRG method reduces model-specific noise to enhance the data-dependent component for black-box adversarial attacks. It visualizes NRG for different sample sizes, showing smoother gradients for larger samples. Attacks using NRG drive the optimizer towards more data-dependent solutions, improving generalization. The noise-reduced iterative sign gradient method (nr-IGSM) and noise-reduced fast gradient sign method (nr-FGSM) are variations of this approach. To analyze the effectiveness of NRG for enhancing transferability, start-of-the-art classification models trained on ImageNet dataset are used. The ImageNet ILSVRC2012 validation set with 50,000 samples is utilized, selecting 5,000 images for each attack experiment. Pre-trained models provided by PyTorch including resnet, vgg, densenet, alexnet, and squeezenet are used. Top-1 and Top-5 accuracies can be found on a website for reference. All models are employed for reliability, but only selected for specific experiments to save computational resources. In this section, the effectiveness of noise-reduced gradient technique is demonstrated by combining it with commonly-used methods like fast gradient based methods. The success rates of FGSM and nr-FGSM are compared, with the results showing promising outcomes. The effectiveness of noise-reduced gradient (nr-FGSM) is demonstrated in comparison to original FGSM in various blackbox and white-box attacks. Results show nr-FGSM consistently outperforms FGSM, even with increased computational cost. Additionally, nr-IGSM generates more easily transferable adversarial examples compared to IGSM, indicating the noise-reduced gradient guides the optimizer towards data-dependent solutions. Large models like resnet152 are found to be more robust to adversarial transfer. The transferability of adversarial examples across models with similar architectures is influenced by model-specific components. IGSM generally generates stronger adversarial examples than FGSM, except in attacks against alexnet. Higher confidence adversarial examples are more likely to transfer to the target model. The inappropriate choice of hyperparameters can lead to underfitting. Alexnet's differences in architecture and accuracy result in IGSM overfitting more than FGSM, leading to a lower fooling rate. Our noise reduced gradient technique improves cross-model generalization by removing model-specific information from gradients. We apply this method to ensemble-based approaches with a reduced evaluation set of 1,000 images. Results show improved success rates for IGSM attacks, with Top-5 rates reported for clarity. FGSM and nr-FGSM attacks are also tested. Generating targeted adversarial examples is challenging, especially for unseen target models. Generating targeted adversarial examples, predicted by unseen target models, is more difficult than non-targeted examples. Single-model approaches are ineffective for targeted attacks. Targeted examples are sensitive to step size \u03b1, requiring a larger step size for success. NRG methods outperform normal methods significantly in both targeted and non-targeted attacks, as shown in Table 3. In this section, the sensitivity of hyper parameters m and \u03c3 in NRG methods for black-box attacks is explored using the nr-FGSM approach. Larger m leads to higher fooling rates due to better gradient direction estimation. An optimal value of \u03c3 is crucial for best performance, varying for different source models. In the experiment, \u03c3 is around 15 for resnet18 and 20 for densenet161. In this section, the robustness of adversarial perturbations to image transformations is explored. The destruction rate is used to quantify the influence of transformations on adversarial examples. Densenet121 and resnet34 are chosen as source and target models, with rotations, Gaussian noise, Gaussian blur, and JPEG compression considered as image transformations. Results show that adversarial examples generated by NRG methods are more robust than those generated by vanilla methods. In this section, decision boundaries of different models are studied to understand why NRG-based methods perform better. Resnet34 is the source model, and nine target models are considered. The \u2207f is estimated with specific parameters, and the decision boundaries are shown in FIG8. The sensitivity of models along sign \u2207f and sign (\u2207f \u22a5 ) is analyzed, with resnet34 being sensitive in both directions, while other target models are more sensitive along sign \u2207f. The study explores how penalizing the optimizer along model-specific directions can prevent overfitting and improve transferability between models. It is observed that larger models have greater distances for producing adversarial transfers compared to smaller models. Different models exhibit varying performances in attacking target models, with densenet121 consistently performing well. This highlights the importance of understanding the principles behind model robustness and transferability in adversarial attacks. The study investigates the relationship between model size, test error, and attack capability. Smaller models with lower test error and capacity exhibit stronger attack capability. This phenomenon is explained by the model's bias for approximating ground truth and the complexity of the model-specific component. The study explores the impact of model size and test error on attack capability. Smaller models with lower capacity show stronger attack capability due to their bias for approximating ground truth. Adversarial perturbations consist of model-specific and data-dependent components, with the latter contributing more to transferability. Noise-reduced gradient (NRG) based methods are proposed for crafting more effective adversarial examples. Models with lower capacity and higher accuracy are better equipped for black-box attacks. Future work will focus on combining NRG-based methods with adversarial training for defense against black-box attacks. The study focuses on defending against black-box attacks using adversarial training. It highlights the data-dependent component contributing to transferability and the difficulty in defending against white-box attacks. Future research may involve incorporating NRG strategy for stable feature learning. The impact of hyperparameters on the quality of adversarial examples generated using IGSM is explored, with resnet152 and vgg16 bn as target models. In an experiment, a large step size of \u03b1 = 15 compared to distortion \u03b5 = 20 can harm attack performance. A small step size of \u03b1 = 5 with more iterations can lead to worse performance due to overfitting. Model redundancy influences attack capability, with low-capacity models showing stronger attacks. This aligns with previous observations. In an experiment, a large step size of \u03b1 = 15 compared to distortion \u03b5 = 20 can harm attack performance. A small step size of \u03b1 = 5 with more iterations can lead to worse performance due to overfitting. Model redundancy influences attack capability, with low-capacity models showing stronger attacks. This is consistent with our observation in Section 6.4. Top-1 Error (%) Figure 9: Top-1 success rates of FGSM and IGSM attacks against resnet152 for various models. The annotated value is the percentage of adversarial examples that can transfer to the resnet152. The distortion is chosen as \u03b5 = 15."
}