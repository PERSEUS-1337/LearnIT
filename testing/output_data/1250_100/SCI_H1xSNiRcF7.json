{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. A novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes, improving optimization robustness. The text discusses a novel hierarchical embedding model that uses Gaussian convolutions over boxes to improve optimization robustness in geometric representations. This approach shows improved performance on various tasks, especially in cases of sparse data. Embeddings have been a key technique in machine learning, with recent interest in structured or geometric representations. The text discusses geometric structures like density functions, convex cones, and hyperrectangles in the probabilistic Box Lattice model. Box embeddings replace vector lattice ordering with overlapping boxes, but their \"hard edges\" pose challenges for optimization. The text discusses challenges with gradient-based optimization in models with disjoint boxes, especially for sparse data. A new approach is proposed to smooth the hard edges of boxes into density functions, showing superior results in modeling transitive relations and market basket datasets. The text discusses various models for structured embeddings, including order embeddings, box embeddings, and hyperrectangle-based embeddings. These models differ in their probabilistic or deterministic nature and their approach to representing relationships between entities. The text discusses models for structured embeddings, focusing on optimizing energy functions in nonprobabilistic models. Hyperbolic space's negative curvature favors tree structures, but is less suitable for non-treelike DAGs. Smoothing the energy landscape with Gaussian convolution is common in optimization methods. The approach aims to map concepts to event space subsets, emphasizing transitive relations and fuzzy concepts of inclusion and entailment. The text discusses order theory definitions and introduces vector and box lattices as geometric representations. Posets generalize totally ordered sets to allow incomparable elements, suitable for acyclic directed graph data. Lattices have unique least upper and greatest lower bounds, with bounded lattices containing additional top and bottom elements. Bounded lattices have join and meet operations, satisfying specific properties. The extended real numbers form a bounded lattice under min and max operations. Sets partially ordered by inclusion also form a lattice. The dual lattice can be obtained by swapping meet and join operations. A vector lattice, or Riesz space, is a vector space with a lattice structure. The Order Embeddings of BID20 embed partial orders as vectors using the reverse product order, corresponding to the dual lattice. Vilnis et al. introduced a box lattice for knowledge graphs, where each concept is associated with two vectors representing the minimum and maximum coordinates of an axis-aligned hyperrectangle. The box lattice for knowledge graphs associates concepts with vectors representing the minimum and maximum coordinates of an axis-aligned hyperrectangle. The lattice structure involves least upper bounds and greatest lower bounds, with max and min denoting scalar coordinates. Marginal probabilities of events are determined by the volume of boxes, their complements, and intersections under a suitable probability measure. Gradient-based optimization can be used to learn box embeddings, but there may be issues when two concepts are incorrectly identified. The original work identifies a problem when two concepts are incorrectly labeled as disjoint, leading to no gradient signal flow. A surrogate function is proposed to optimize this issue, but a more principled framework is suggested to develop alternate measures. This aims to avoid optimization problems and improve the final model quality. An example is shown with two disjoint intervals before and after applying a smoothing kernel, demonstrating the importance of overlap. The curr_chunk discusses the use of kernel smoothing to address gradient sparsity in box embeddings. By replacing indicator functions with functions of infinite support, specifically convolution with a normalized Gaussian kernel, the approach aims to improve optimization and maintain desirable properties of the base lattice model. The joint probability between intervals is rewritten as an integral of the product of indicators, with the solution involving functions with infinite support. The approach involves using kernel smoothing to address gradient sparsity in box embeddings by replacing indicator functions with functions of infinite support. The solution to the joint probability between intervals is expressed as an integral, with the formula derived using a logistic sigmoid approximation. The convolution with a zero-bandwidth kernel results in the original equation, but Gaussian-smoothed indicators do not meet the idempotency requirement. A modification of equation 3 allows for a function that retains smooth optimization properties. The hinge function satisfies a specific identity, unlike the softplus function. In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for intervals x = y = (a, b) = (c, d), while equation 3 is not. This leads to defining probabilities using a normalized version of equation 7. Softplus function can output values greater than 1 and requires normalization in experiments with a small number of entities. The softplus function allows boxes to learn unconstrained, normalizing probabilities by dividing dimensions and ensuring p(x) = p(x). A comparison with the Gaussian model shows better behavior for disjoint boxes, maintaining the meet property. The Gaussian model must lower its temperature to achieve high overlap, causing vanishing gradients. Experiments on WordNet hypernym prediction show the performance of these improvements. The WordNet hierarchy has 837,888 edges. Positive examples are randomly chosen, negative examples are generated by swapping terms. The smoothed box model performs nearly as well as the original. Further experiments are done to explore performance in a sparse regime. In further experiments on the WordNet mammal subset, different numbers of positive and negative examples were used to compare the box lattice, a smoothed approach, and order embeddings (OE) as a baseline. The models were tested on balanced and imbalanced data, showing that the smoothed box model outperformed OE and the original box model in all settings, especially on imbalanced data scenarios. In experiments on the WordNet mammal subset, the smoothed box model outperformed order embeddings and the original box model in all settings, especially on imbalanced data scenarios. The experiments were conducted on the Flickr entailment dataset, showing a slight performance gain compared to the original model, with improvements most concentrated on unseen captions. Additionally, the method was applied to a market-basket task using the MovieLens dataset to predict users' preference for movies. The study evaluates a smoothed box embedding method on the MovieLens dataset, comparing it with various baselines. The method outperforms all other baselines, especially in Spearman correlation, a key metric for recommendation tasks. Additional experiments on the robustness of the smoothed model to initialization are also conducted. The study presented a method for smoothing the energy and optimization landscape of probabilistic box embeddings, showing improved performance on various datasets, especially with sparse data and poor initialization. The research aims to address learning challenges posed by complex embedding structures, such as unions of boxes, through continued exploration of function lattices and constraint-based learning approaches. The research presented a method for smoothing the energy and optimization landscape of probabilistic box embeddings, showing improved performance on various datasets, especially with sparse data and poor initialization. The Gaussian kernel is normalized to have total integral equal to 1, preserving the overall areas of the boxes. The antiderivative of \u03c6 is the normal CDF, allowing for the evaluation of the integral of interest. Fubini's theorem is applied to evaluate the equation, leading to the robustness of the smoothed box model to initialization. The MovieLens dataset, with a large proportion of small probabilities, is suitable for optimization by the smoothed model. The research presented a method for smoothing the energy and optimization landscape of probabilistic box embeddings, showing improved performance on various datasets, especially with sparse data and poor initialization. Parametrizing the initial distribution of boxes with a minimum coordinate and a positive width, the study adjusted the width parameter to analyze the robustness of models to disjoint boxes. Results showed that the smoothed model performed well even with disjoint initialization, while the original box model degraded significantly. Detailed methodology and hyperparameter selection methods for each experiment are provided, with code available for reproduction on GitHub. For the WordNet experiments, the model is evaluated every epoch on the development set for a large fixed number of epochs, and the best development model is used to score the test set. Baseline models are trained using the parameters of BID22, with the smoothed model using hyperparameters determined on the development set. Negative examples are generated randomly based on the ratio for each batch of positive examples. The experimental setup uses the same architecture as BID22 and BID9, a single-layer LSTM that reads captions and produces a box embedding parameterized by min and delta. The model is trained for a large fixed number of epochs and tested on the development data at each epoch. Hyperparameters were determined on the development set. For all MovieLens experiments, the model is evaluated every 50 steps on the development set. The model is evaluated every 50 steps on the development set, and optimization is stopped if the best development set score fails to improve after 200 steps. The best development model is used to score the test set."
}