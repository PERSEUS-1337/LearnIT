{
    "title": "BJxOHs0cKm",
    "content": "While empirical evidence suggests that model generalization is related to local properties of the optima described via the Hessian, the PAC-Bayes paradigm connects model generalization with the local solution property. The generalization ability is linked to the Hessian, Lipschitz constant, and parameter scales. A metric is proposed to score generalization capability and an algorithm optimizes the perturbed model accordingly. Deep models excel in computer vision, speech recognition, and natural language processing despite having more parameters than training samples. Classical learning theory suggests generalization capability is related to the complexity of the hypothesis space. The generalization ability of models is related to the local properties of the optima, such as the spectrum of the Hessian matrix. Metrics have been introduced to measure the \"sharpness\" of the solution and its connection to generalization. Hessian-based sharpness measures may not always explain generalization due to re-parameterization effects. The Taylor expansion method introduced by Mackay in 1995 uses the Hessian of the loss function to evaluate model simplicity. Recent studies have connected the PAC-Bayes bound with Bayesian analysis to analyze generalization behavior in deep models. This approach considers the sharpness of minima and the complexity of predicted labels to determine optimal batch sizes and classification boundaries. In this paper, the authors explore the relationship between model generalization and the local \"smoothness\" of a solution from a PAC-Bayes perspective. They establish that the generalization error is linked to the Hessian of the loss function, its Lipschitz constant, parameter scales, and the number of training samples. A new metric for generalization is introduced, allowing for the selection of an optimal perturbation level related to the Hessian. An algorithm leveraging Hessian estimation is proposed to enhance model generalization in supervised learning scenarios. The PAC-Bayes paradigm involves minimizing expected loss by drawing functions from posterior distribution. The gap between expected and empirical loss is bounded by KL divergence. With perturbations around parameters, a PAC-Bayes bound can be obtained. By optimizing parameters, a scalable bound can be achieved. One may optimize \u03b7 to scale as BID33. The perturbation bound FORMULA4 connects generalization with local properties around the solution w through perturbation u. Searching for an optimal perturbation level minimizes the bound. The Hessian matrix \u22072L(w) and model generalization connection is not rigorously explored. The local smoothness assumption and main theorem are introduced in this section, focusing on small local neighborhoods. In this paper, the focus is on defining a neighborhood set around a reference point and discussing the Hessian Lipschitz condition for smoothness. Theorem 2 states that with a bounded model weight, there is a probability bound over n samples for any assumption to hold. The perturbation level can be optimized to connect generalization with local properties around the solution. Theorem 2 discusses controlling the expected loss of a perturbed model by choosing perturbation levels carefully. The bound is related to the Hessian diagonal element, Lipschitz constant, neighborhood scales, number of parameters, and samples. Perturbation levels are inversely related to flat coordinates. Truncated Gaussian perturbation is also considered. If the empirical loss function satisfies the local Hessian Lipschitz condition, perturbations can be bounded up to the third order. Linear and second-order terms simplify for perturbations with zero expectation. The posterior distribution of model parameters is uniform, with support varying for different parameters. Perturbed parameters are bounded, and the third-order term is also bounded. The over-parameterization phenomenon is not explained by the bound in Theorem 2. Lemma 3 provides conditions for controlling the expected loss of a perturbed model. The text discusses re-parameterization of RELU-MLP models and the impact on generalization power. The spectrum of \u2207 2L alone is insufficient to determine generalization power. By re-parameterizing the model and scaling the Hessian spectrum, the model's prediction and generalization remain unaffected. The optimal perturbation levels scale inversely with parameter scaling, resulting in a logarithmic change in the bound speed. The bound's stability during re-parameterization varies, but the general trend is a logarithmic change. The text discusses heuristic-based approximations and empirical observations related to the generalization power of RELU-MLP models. A new generalization metric called pacGen is introduced, which is based on PAC-Bayes theory and involves estimating the diagonal elements of the Hessian and the Lipschitz constant. The metric is calculated on real-world data using efficient methods inspired by Adam optimization. The text discusses estimating the Lipschitz constant \u03c1 of the Hessian using Adam optimization. Experiments vary batch size and learning rate, showing larger gaps between test and training loss as batch size grows and as learning rate decreases. Proposed metric \u03a8 \u03ba (L, w * ) follows similar trends. The text introduces a method to perturb model weights based on the PAC-Bayes bound to improve generalization power. It suggests optimizing the perturbed empirical loss instead of just minimizing the empirical loss for better results. The algorithm presented includes using exponential smoothing to estimate the Hessian and treating \u03b7 as a hyper-parameter. The text also discusses the implications of the gradient \u2207L \u00b7 u in practical applications. The text introduces a method to perturb model weights based on the PAC-Bayes bound to improve generalization power. It suggests optimizing the perturbed empirical loss instead of just minimizing the empirical loss for better results. The algorithm presented includes using exponential smoothing to estimate the Hessian and treating \u03b7 as a hyper-parameter. The text also discusses the implications of the gradient \u2207L \u00b7 u in practical applications. If the gradient \u2207L is close to zero, then the first order term can be ignored. In Algorithm 1, parameters with small gradients below \u03b2 2 are perturbed efficiently using a per-parameter \u03c1 i capturing the variation of the diagonal element of Hessian. The perturbation level decreases with a log factor as the epoch increases. Results of the perturbed algorithm are compared against the original optimization method on CIFAR-10, CIFAR-100 BID17, and Tiny ImageNet 8 using Wide-ResNet BID36 as the prediction model. The text introduces a method to perturb model weights based on the PAC-Bayes bound to improve generalization power. It suggests optimizing the perturbed empirical loss instead of just minimizing the empirical loss for better results. The algorithm presented includes using exponential smoothing to estimate the Hessian and treating \u03b7 as a hyper-parameter. The perturbation method shows improved performance compared to traditional dropout techniques. The text discusses the relationship between model generalization power and the Hessian, smoothness of the solution, parameter scales, and training sample size. It introduces a new metric and perturbation algorithm based on the Hessian to improve model generalization. Empirical results show the algorithm's effectiveness in enhancing performance on unseen data. The text discusses a model using a 5-layer MLP with sigmoid activation and cross entropy loss. The model has two free parameters, w1 and w2, and is trained using 100 samples. The loss function is plotted with respect to the model variables, showing multiple local optima. The colors on the loss surface represent generalization metric scores, with a smaller score indicating better generalization power. The figure displays a sharp and flat local optimum, with the global optimum potentially having poor generalization capability. The text discusses the model's local optima, comparing a sharp minimum with a flat minimum in terms of predicted labels. The Gaussian distribution is truncated to meet bounded perturbation requirements. The event P(E) is analyzed using the union bound, with coefficients bounded by \u03c4. The text discusses bounding coefficients with \u03c4, choosing prior \u03c0 as N(0, \u03c4I), and approximating the bound with \u03b7 = 39. Lemma 4 addresses loss function l(f, x, y) \u2208 [0, 1] with bounded model weights. The algorithm treats \u03b7 as a hyper-parameter for optimization. The text discusses optimizing \u03c3 to minimize a formula, combining inequalities and equations for proof, and building a grid to optimize \u03b7. It also mentions a lemma about eigenvalues of the Hessian and generalization, and another lemma about correlated perturbations in the loss function. The section discusses comparing dropout with a proposed perturbation algorithm, showing results on wide resnet architectures with varying dropout rates on CIFAR-10 and CIFAR-100 datasets. The study compares dropout with a perturbation algorithm on wide resnet models with different dropout rates on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. Results show improved accuracy with added dropout, with specific dropout rates working best for each dataset. The perturbed algorithm outperforms dropout in experiments on CIFAR-10 due to its ability to apply varying levels of perturbation based on local smoothness structures, while dropout uses a single rate for all parameters."
}