{
    "title": "B1xwcyHFDr",
    "content": "The information bottleneck method is used for representation learning by training an encoder to retain relevant information for predicting the label while minimizing superfluous information. This method is extended to the multi-view unsupervised setting, where two views of the same entity are provided without labels. A new multi-view model is defined, showing state-of-the-art results on datasets like Sketchy and MIR-Flickr. The theory is also applied to the single-view setting, demonstrating better generalization capabilities compared to traditional unsupervised approaches. The information bottleneck principle guides representation learning by discarding irrelevant input information to enhance robustness for downstream tasks. In supervised learning, it minimizes mutual information between the input and representation while maximizing it between the representation and label. Unsupervised learning focuses on generating representations that efficiently support supervised tasks with fewer labels. In unsupervised learning, the InfoMax objective maximizes mutual information between x and z, extending the information bottleneck method to the multi-view setting. The goal is to improve generalization by discarding non-shared information between views, resulting in a representation containing only shared information. This approach eliminates independent factors of variations and enhances robustness for downstream tasks. The text discusses extending the information bottleneck principle to unsupervised multi-view settings, introducing a new model that achieves state-of-the-art results on two datasets, and demonstrating the robustness of the model's representations in single-view settings. The goal is to find a distribution that maps data observations into a code space, with a focus on sufficiency of the representation for predicting labels. The text discusses sufficiency in representation for predicting labels, emphasizing the importance of information preservation in encoding procedures. Sufficient representations should lead to better generalization for unlabeled data instances. The information bottleneck principle is extended to unsupervised multi-view settings, aiming to map data observations into a code space efficiently. The text discusses minimizing superfluous information in representations for predicting labels, emphasizing the importance of sufficiency in encoding. It introduces a strategy to reduce information content safely, even without observing the label, by utilizing redundant information from multiple views of the data. In the multi-view setting, ensuring that shared information between different data views is maintained in the representation is crucial for predicting labels accurately. Redundancy between views can be defined by the absence of conditional information, highlighting the importance of minimizing irrelevant information for efficient prediction. In the multi-view setting, maintaining shared information in the representation is crucial for accurate label prediction. Mutual redundancy between views implies that a representation containing all shared information is as predictive as observing both views together. By minimizing unique information in the representation, irrelevant data can be discarded for efficient prediction. In the multi-view setting, maintaining shared information in the representation is crucial for accurate label prediction. The less two views have in common, the more robust the resulting representation. If v1 and v2 share only label information, z1 is minimal for y. An objective function for z1 of v1 should discard as much information as possible without losing any label information. By ensuring I(v1; v2|z1) = 0 and decreasing I(z1; v1|v2), the robustness of the representation is increased by discarding irrelevant information. In a multi-view setting, maintaining shared information in the representation is crucial for accurate label prediction. By combining terms using a relaxed Lagrangian objective, a Multi-View Information Bottleneck model is defined to optimize parameters for robust representation. The model includes Lagrangian multipliers to balance sufficiency and robustness, with a trade-off coefficient \u03b2 as a hyper-parameter. The objective is to ensure the sufficiency of representations z1 and z2 for predicting y, with a focus on discarding irrelevant information while preserving label information. The MultiView Information Bottleneck (MIB) model is defined with a focus on optimizing parameters for robust representation. It involves balancing sufficiency and robustness through Lagrangian multipliers and a trade-off coefficient beta. The model aims to discard irrelevant information while preserving label information, utilizing symmetrized KL divergence and mutual information between representations. The methodology introduces mutually redundant views by exploiting known symmetries of the task. The MultiView Information Bottleneck (MIB) model focuses on optimizing parameters for robust representation by balancing sufficiency and robustness. It aims to discard irrelevant information while preserving label information through mutually redundant views. This is achieved by artificially building views that satisfy mutual redundancy for the target variable y. The model utilizes symmetrized KL divergence and mutual information between representations to achieve its goal. The two generated views in the MultiView Information Bottleneck (MIB) model share parameters and use one encoder when the views have similar marginal distributions. Representations in the Information Plane are characterized by information about the raw observation and accessible predictive information. Recent progress in mutual information estimation has brought attention to the InfoMax principle for unsupervised learning. The InfoMax principle for unsupervised representation learning aims to preserve all information from raw observations. However, the effectiveness of InfoMax models is influenced by inductive biases in architecture and estimators. In contrast, Variational Autoencoders balance compression and reconstruction error through a hyper-parameter \u03b2, leading to lossless or compressed representations with increased generalization and disentanglement. Variational Autoencoders (VAEs) aim to balance compression and reconstruction error using a hyper-parameter \u03b2, leading to lossless or compressed representations with increased generalization and disentanglement. However, as \u03b2 approaches infinity, I(z; x) goes to zero, and there are no guarantees that VAEs will retain label information. The transition between low and high \u03b2 regimes depends on how well label information aligns with the inductive bias introduced by encoder, prior, and decoder architectures. Concurrent work applies the InfoMax principle in Multi-View settings to maximize mutual information between representations of different data-views. The target representation for MultiView InfoMax (MV-InfoMax) models should contain predictive information for the second data-view, aiming for I(z; x) \u2265 I(x; v2) on the Information Plane. The text discusses the optimal representation for multi-view information maximization, comparing different models and introducing a new approach that explicitly discards irrelevant information. This approach is based on the InfoMax principle and aims to improve generalization capabilities in unsupervised settings. The text discusses the effectiveness of a new model in removing superfluous information compared to existing models in both multi-view and single-view settings. The model aims to improve generalization capabilities by explicitly discarding irrelevant information, based on the InfoMax principle. The comparison between MIB and other InfoMax-based models shows better performance. MIB is evaluated on sketch-based image retrieval and Flickr multiclass image classification tasks. The Sketchy dataset consists of images and handdrawn sketches of objects. A total of 6,250 sketches are randomly selected for testing, leaving 69,221 sketches for training. The sketch-based image retrieval task involves ranking natural images according to a query sketch. The model achieves strong performance in sketch-based image retrieval and image classification tasks. The representation captures common class information between pictures and sketches, with regularization enhancing alignment of representations for retrieval tasks. The MIR-Flickr dataset contains 1M images with hand-crafted features (v1) and multihot encodings (v2). It is divided into labeled and unlabeled sets, with training images having at least two tags. The labeled set includes 38 topic classes and is split into train, validation, and test sets. The model is trained on unlabeled pairs and a logistic classifier is trained on labeled images for macro-categories. The quality of the processed dataset and splits will be publicly released upon paper acceptance. The Multi-View InfoMax objective does not consistently produce representations for two views, making it challenging to use for ranking. The encoder consists of a multi-layer perceptron with ReLU activations learning 1024-dimensional representations for images and tags. The MIB model is compared with other multi-view learning models, showing improved performance with fewer labels and higher \u03b2 values. In this section, different unsupervised learning models are compared based on their data efficiency and representation on the Information Plane. The effectiveness of MIB against other baselines may be due to using mutual information estimators that do not require reconstruction, unlike Multi-View VAE and Deep Variational CCA. The focus is on small experiments to uncover differences in popular approaches for representation learning, using a dataset generated from MNIST with two views created through data augmentation. The effectiveness of MIB in unsupervised learning models is compared based on data efficiency and representation on the Information Plane. Two views, v1 and v2, are created through data augmentation without using label information. Encoders are trained on the multi-view dataset, and a logistic regression model is trained on the resulting representations. Mutual information estimation networks are used to estimate I(x; z) and I(y; z) on the final representations. The encoder architecture consists of 2 layers of 1024 hidden units with ReLU activations, resulting in 64-dimensional representations. The Multi-View Information Bottleneck method introduces a novel approach using multiple data-views to create robust representations for downstream tasks. By retaining less information about the data while maintaining predictive information, better classification performance is achieved at low-label regimes. The method focuses on discarding irrelevant information to improve robustness and data efficiency, showing that models with minimal information loss perform well even with limited labels. The Multi-View Information Bottleneck method focuses on creating robust representations for downstream tasks by retaining predictive information while discarding irrelevant data. It shows strong performance in various tasks and can be applied even when mutual redundancy is approximate. Future work includes exploring more than two views and the role of data augmentation in bridging the gap with the Information Bottleneck principle. The text discusses the properties of mutual information and theorems related to random variables. It highlights the ability to find a label y for which a representation z is not predictive for y while z is, based on certain conditions. The text discusses the properties of mutual information and theorems related to random variables, highlighting the ability to find a label y for which a representation z is not predictive for y while z is, based on certain conditions. The theorems presented show the existence of such y in various scenarios, proving the relationships between different variables. The text discusses the properties of mutual information and theorems related to random variables, highlighting the ability to find a label y for which a representation z is not predictive for y while z is, based on certain conditions. The theorems presented show the existence of such y in various scenarios, proving the relationships between different variables. Theorems and corollaries are discussed, considering the independence assumption derived from a graphical model. Proposition B.4 states that when I(t1(x); y) = I(t2(x); y) = I(x; y), the views t1(x) and t2(x) are mutually redundant for y. The text discusses mutual information properties and theorems related to random variables, proving relationships between different variables. Theorems show the existence of a label y where a representation z is not predictive for y while z is, based on certain conditions. Theorems and corollaries consider independence assumptions from a graphical model. Proposition B.4 states that when I(t1(x); y) = I(t2(x); y) = I(x; y), views t1(x) and t2(x) are mutually redundant for y. The text discusses the properties of mutual information and theorems related to random variables. It proves relationships between different variables, showing conditions where a representation z is not predictive for label y. Proposition B.4 states that views t1(x) and t2(x) are mutually redundant for y under certain conditions. Theorems and corollaries consider independence assumptions from a graphical model. The mutual redundancy condition between multiple views and a label is not transitive due to higher order interactions. An example with three views and a task is provided to illustrate this non-trivial relationship. The theory becomes complex when extending beyond two views due to counter-intuitive interactions. The theory of Multi-View Information Bottleneck becomes non-trivial when extending beyond two views, requiring an extension to ensure sufficiency for the label. Equivalence between supervised Information Bottleneck and Multi-View Information Bottleneck is shown when redundant views share only label information. This equivalence is proven by demonstrating that a representation sufficient and minimal for one view is also sufficient and minimal for the label. The Multi-View Information Bottleneck theory extends beyond two views, ensuring sufficiency for the label. It is proven that a representation minimal for one view is also minimal for the label, with InfoMax being the key concept. The losses L1 and L2 aim to create minimal representations z1 and z2, with a tight bound when z1 and z2 are consistent. The experimental procedure involves modeling stochastic encoders with Normal distributions parametrized by neural networks. The symmetrized KL-divergence can be directly computed, while mutual information estimator is needed for I \u03b8\u03c8 (z 1 ; z 2 ). The hyper-parameter \u03b2 is gradually increased during training to optimize the mutual information estimator I \u03b8\u03c8 (z 1 ; z 2 ). Starting with a small value, it is slowly raised to prevent the encoder from collapsing into a fixed representation. The update policy for \u03b2 does not significantly impact the representation once the mutual information estimator network is fully trained. Input for sketch-based classification includes 4096 dimensional features from VGG-16 models pre-trained on images and sketches. Feature extractors are frozen during training, with batches of size 128 used for each iteration. The training procedure involves using batches of size 128 for both sketch and image encoders, with a gradual increase in \u03b2 value from 10^-4 to 1.0 over 250,000 iterations. Evaluation includes comparing the 64-dimensional outputs using Euclidean distance and Hamming distance for binary hashing methods. The training procedure uses batches of size 128 for sketch and image encoders, with a gradual increase in \u03b2 value from 10^-4 to 1.0 over 250,000 iterations. Evaluation involves comparing the 64-dimensional outputs using Euclidean distance and Hamming distance for binary hashing methods. The encoders consist of a multi-layer perceptron with 4 hidden ReLU units of size 1,024, and the output defines mean and variance of the factorized Gaussian posterior. The training procedure involves using batches of size 128 for sketch and image encoders, with a gradual increase in \u03b2 value from 10^-4 to 1.0 over 250,000 iterations. Evaluation includes computing the representation of 25,000 labeled images and training a multi-label logistic regression classifier. Encoders for the MNIST experiments consist of neural networks with two hidden layers of 1,024 units and ReLU activations, producing a 2x64-dimensional parameter vector for Gaussian posteriors. The decoders and critic architecture used in the VAE experiments have specific network sizes and layer configurations. The \u03b2 update policy involves increasing the value exponentially until a certain iteration, then keeping it constant. Evaluation of the trained representations follows a protocol to maximize mutual information lower bound and compute numerical estimations. The final values for mutual information are computed by averaging estimations after removing the lowest and highest 5% to reduce variance. In this section, additional quantitative results and visualizations are presented for the singleview MNIST experiments. A comparison of input information, label information, and accuracy of a linear classifier trained with different amounts of labeled examples is provided. Results from Jensen-Shannon and InfoNCE estimators are reported. The linear projection of the embedding obtained using the MIB model shows ten distinct clusters corresponding to different digits. With 10 labeled examples, the centroid coordinates align with the digit labels effectively. The effect of different ranges of corruption probability as a data augmentation strategy is visualized in Figure 8. The MV-InfoMax Model does not benefit from increasing corruption, while models trained with the MIB objective can utilize augmentation to improve representation. However, as corruption approaches 100%, MIB performance deteriorates. The MIB models lose label information with low corruption probability but regain it as corruption increases, possibly due to optimization issues. The optimization procedure may cause bias in the Monte-Carlo estimation for the symmetrized Kullback-Leibler divergence. Increasing examples of views from the same data-point within a batch can help. The hyper-parameter \u03b2 in the Multi-View Information Bottleneck model balances sufficiency and minimality of representation. Different values of \u03b2 affect the trade-off between information retention and removal. The model shows a better trade-off between different information measures compared to \u03b2-VAE. The Multi-View Information Bottleneck model's effectiveness is supported by predictive accuracy values. Presented at ICLR 2020."
}