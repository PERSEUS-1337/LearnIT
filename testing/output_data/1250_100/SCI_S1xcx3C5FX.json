{
    "title": "S1xcx3C5FX",
    "content": "The new approach presented assesses the robustness of neural networks by estimating the proportion of inputs where a property is violated. It differs from formal verification by providing an informative measure of network robustness and can scale to larger networks. The approach uses multi-level splitting, a Monte Carlo method, to estimate the probability of rare events. It can emulate formal verification procedures on benchmark problems while offering a statistical estimate of unsatisfiability when no violation is found. The study of neural network verification aims to ensure network robustness in critical applications by assessing the violation probability of important properties. Traditional verification methods have limitations in determining network robustness and scalability to larger networks. The traditional verification methods for neural networks have limitations in determining network robustness and scalability to larger networks. A new measure of intrinsic robustness based on the probability of property violation under an input distribution model is developed to address these shortcomings. This measure provides a more practical approach to assessing network robustness, considering the challenges of formal verification in many applications. The approach focuses on calculating the probability of property violation under an input distribution model to determine network robustness. It provides a practical measure of robustness, even for large networks where formal verification is challenging. The method addresses the limitations of traditional verification methods and offers a more feasible way to assess network robustness in practical applications. The adaptation of the adaptive multi-level splitting (AMLS) algorithm to network verification allows for reliable estimation of rare event probabilities, providing a more informative robustness metric for non-verifiable networks. The framework is easy to implement, scales linearly with neural network operation costs, and is agnostic to network architecture and input model assumptions. It also generates diverse samples of property violations. The literature on neural network robustness focuses on two main threads: the classical approach to verification, which seeks to formally prove properties of neural networks but struggles to scale to larger networks, and the deep learning community's research on constructing and defending against adversarial attacks. The research on neural network robustness includes constructing defenses against adversarial attacks and estimating the robustness of networks. However, current methods have drawbacks such as incorrect estimates of adversarial distortion, inability to handle non-Lipschitz continuous networks, and lack of producing adversarial examples. Researchers have also explored relaxing the satisfiability problem of classical verification to produce certificates-of-robustness for some samples. The research explores neural network robustness by producing certificates-of-robustness for some samples, focusing on reinforcement learning agents' robustness to failure. The ACASXU dataset is used to train a neural network for collision avoidance in unmanned aircraft, with interpretable properties encoded in functions to detect violations. The research focuses on neural network robustness, using interpretable properties encoded in functions to detect violations. The formal verification problem determines if there exists an input violating a property, making it satisfiable (SAT) or unsatisfiable (UNSAT). Adversarial properties in deep learning, like perturbations in image classification, are subsumed in the framework. The approach requires a neural network, a property function, and an input. The method for neural network robustness involves using interpretable properties to detect violations, with a focus on formal verification problems. It requires a neural network, a property function, and an input model to define an integration problem. The main challenge lies in estimating the integral, making the method applicable to any neural network as long as the property function can be evaluated. The property function reflects how the network is performing with respect to a specific property, with the goal of predicting rare events where the property is violated. The input model is a distribution over the input domain considered for counterexamples. The input model specifies the domain for counterexamples, with the property function determining the probability of failure. Estimating the integral is crucial for measuring network robustness, but generating example inputs that violate the property is challenging due to the rarity of such events. Monte Carlo estimation is typically not feasible for real problems due to the large number of samples required for accuracy. Adaptive multi-level splitting (AMLS) is a highly effective method for estimating the probability of rare events with high accuracy, even when the event is very rare. It can be adapted to address computational challenges in neural networks, such as scaling well in dimensionality. AMLS allows the use of MCMC transitions, making it effective in high-dimensional problems where other methods may struggle. Adaptive multi-level splitting (AMLS) is effective in estimating rare event probabilities and can scale well in high-dimensional spaces. It produces property-violating examples and samples from a specific distribution, which can be used for robust learning. Multi-level splitting divides the problem into simpler ones by constructing intermediate targets to bridge the gap between input and target models. Consecutive levels can be estimated reliably by using samples from one level to initialize the next. The approach starts by drawing samples from the initial distribution and estimating subsequent levels using simple Monte Carlo. Adaptive multi-level splitting (AMLS) is a reliable method for estimating rare event probabilities by dividing the problem into simpler levels. It involves drawing samples from an initial distribution and using them to estimate subsequent levels using simple Monte Carlo. The method ensures accurate estimates by setting appropriate thresholds for property violations. To avoid sample shrinking between levels in Adaptive multi-level splitting (AMLS), a rejuvenation step is necessary. This involves resampling with replacement to generate a new set of samples, followed by applying Metropolis-Hastings transitions to produce fresh samples. Setting appropriate thresholds for property violations and running more MH transitions improves estimator performance. The levels Lk must be sufficiently close for reliable estimation, and the number of levels K can be chosen freely. In Adaptive multi-level splitting (AMLS), a rejuvenation step is necessary to avoid sample shrinking between levels. AMLS BID9 controls the trade-off between rarity of events and computational costs by adaptively selecting levels. The approach terminates when the level reaches zero, with K being a dynamic parameter implicitly chosen. Choosing the \u03c1th quantile of property values allows explicit control of event rarity. This approach gives P k = \u03c1 if all sample property values are distinct. The application of AMLS to the verification problem introduces a complication where the true probability of a rare event may be zero. To address this, a termination criterion based on a threshold probability, P min, is introduced. If the estimate for I falls below P min, the algorithm terminates and returns I = 0. This ensures that even if a finite estimate for I is eventually generated, it will be less than P min. The complete method is outlined in Algorithm 1, with low-level implementation details in Appendix B. In the first experiment, the robustness estimation framework is tested to emulate formal verification approaches and provide additional robustness information for SAT properties. The method identifies properties as UNSAT (I = 0) or SAT (I > 0) and provides a measure of robustness for SAT properties through the estimate for I. The COLLISIONDETECTION dataset is used, consisting of a neural network with specific properties for verification. The dataset used for verification consists of 172 SAT and 328 UNSAT properties. The approach was tested on all 500 properties using specific hyperparameters. Our method correctly identified all UNSAT properties and found counterexamples for all SAT properties, unlike the naive Monte Carlo estimate. The variances in our estimates were low. Our approach showed low variances in estimates and matched unbiased MC baseline estimates for commonly violated properties. It was significantly faster than naive MC, with a speed up of several orders of magnitude. AMLS is unbiased under certain assumptions, and our method demonstrated low bias compared to naive MC estimation. The bias in estimates decreases with lower values of \u03c1 and rareness of the event. Larger values of M and N improve sampling, while setting \u03c1 too high introduces biases. Empirically, \u03c1 = 0.1 provides a good trade-off between bias and variance. AMLS was run on SAT properties of COLLISIONDETECTION with varying parameters, showing faster run times compared to large values of \u03c1. The study compared different values of N and M for estimating rare events, finding that N did not significantly impact results. The setting of \u03c1 had a noticeable effect on estimates, with larger M values giving better results. The algorithm was validated on MNIST and CIFAR-10 datasets using a dense ReLU network. After training classifiers, multilevel splitting was performed on test set samples with varying values of M for MNIST and CIFAR-10 datasets. Naive MC estimates were also evaluated, taking longer than AMLS estimates. Results showed that larger M values were needed for CIFAR-10 due to its higher input dimension. Adversarial examples for CIFAR-10 were found to be more perceptually similar to the datapoint compared to MNIST. Estimates for adversarial properties of single datapoints with varying values of M for MNIST/CIFAR-10/CIFAR-100 datasets were conducted. The error bars from multiple runs showed low variance and bias, with estimates converging as M increases. Adversarial properties were also tested on a larger DenseNet architecture for CIFAR-100 dataset. The study compared the computational efficiency of naive Monte Carlo estimates with AMLS estimates, showing agreement in results. The robustness metric for a ReLU network trained against norm bounded perturbations was examined, with the method of BID27 producing certificates-of-robustness for certain values. Memory constraints limited the calculation of certificates for certain values before epoch 32 using BID27, but the proposed metric did not face the same issues. The study investigates the robustness of a CNN model trained on MNIST using a robust loss function. The model is trained for 100 epochs with cross-entropy loss, then for another 100 epochs with the robust loss. The architecture includes convolutional and fully connected layers with ReLU activations. Robust training is done in an l \u221e -ball around the inputs. Robustness metrics are calculated at various epochs, showing results in FIG2 and per-sample results in Appendix C.2. Our approach captures variations in network robustness, producing certificates-of-robustness for some datapoints. If the result is less than 0, no adversarial examples exist in an l \u221e ball around the datapoint. Comparing our method to a classical approach, we provide richer information for SAT properties and establish UNSAT properties. The fraction of robust samples verified by our method is shown in FIG2. Our method provides an upper bound on the fraction of robust samples, which can be tightened by reducing P min \u2192 0. In contrast, BID27 offers a lower bound dependent on the convex outer bound's tightness. Our bound signifies the proportion of samples with violations below a specified threshold, unlike BID27. The memory usage of BID27 is high due to ReLU activations crossing thresholds, making it infeasible to calculate maximum values on the convex outer bound. Our approach introduces a new measure for the intrinsic robustness of neural networks, validated on various datasets. It emulates formal verification for satisfiable properties and provides accurate predictions for unsatisfiable properties. The key advantages are an explicit measure of robustness and improved scaling for identifying unsatisfiable properties. However, it may not be suitable when facing explicit adversaries. Our approach introduces a new measure for the intrinsic robustness of neural networks, validated on various datasets. It emulates formal verification for satisfiable properties and provides accurate predictions for unsatisfiable properties. The key advantages are an explicit measure of robustness and improved scaling for identifying unsatisfiable properties. However, in practical scenarios, counterexamples close to the input may exist, making formal verification unrealistic. Our approach offers significant advantages in such cases. Further efficiency improvements could be made by using a more efficient base MCMC kernel in our AMLS estimator. Using advanced inference approaches like Langevin Monte Carlo (LMC) and Hamiltonian Monte Carlo (Neal, 2011) can speed up Markov chains mixing, reducing MCMC transitions. Acknowledgments were made to Sebastian Nowozin for suggesting multilevel splitting, Rudy Bunel for help with the COLLISIONDETECTION dataset, and Leonard Berrada for a pretrained DenseNet model. Metropolis-Hastings (MH) allows sampling from unnormalized target distributions. Each MH transition proposes a new sample based on the current state and unnormalized density. Metropolis-Hastings (MH) algorithm involves proposing new samples based on the current state, calculating acceptance probabilities, and generating samples that converge to the target distribution. The algorithm has a computational cost of O(N M K) and can be parallelized over N. MH updates are performed on all chains in the algorithm. Metropolis-Hastings (MH) updates on all chains in the algorithm to reduce correlations and improve performance. An adaptive scheme for proposal distribution is used to maintain an acceptance ratio of 0.234. The value of M in the scheme impacts the quality of results for larger values of \u03c1. Per-sample robustness measure during training shows varying levels of improvement for different datapoints. During training, the Metropolis-Hastings algorithm uses an adaptive proposal distribution to reduce correlations and improve performance. The robustness measure varies for different datapoints, with some points not robust at the end of training for a specific perturbation size. The mean AMLS estimate is compared to the naive MC estimate for different properties, showing accurate estimation for certain properties with a naive MC estimate greater than -6.5."
}