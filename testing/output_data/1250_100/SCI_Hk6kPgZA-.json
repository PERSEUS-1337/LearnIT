{
    "title": "Hk6kPgZA-",
    "content": "Neural networks are vulnerable to adversarial examples, and researchers have proposed various attack and defense mechanisms. A principled approach using distributionally robust optimization guarantees performance under adversarial input perturbations. By utilizing a Lagrangian penalty formulation within a Wasserstein ball, a training procedure is provided that incorporates worst-case perturbations of training data. This method achieves moderate levels of robustness with minimal computational or statistical cost compared to empirical risk minimization, and can efficiently certify robustness for the population loss. Additionally, for imperceptible perturbations, this approach matches or surpasses heuristic methods. In supervised learning, minimizing expected loss over a parameter is crucial for robustness to changes in data distribution. Deep networks in critical systems like self-driving cars are vulnerable to adversarial attacks, leading to misbehavior. Researchers have proposed attack and defense mechanisms, but identifying defendable attack classes remains a challenge. Our work focuses on developing efficient procedures with rigorous guarantees for small to moderate amounts of robustness in deep networks. We use distributionally robust optimization and provide an adversarial training procedure with provable guarantees on its computational and statistical performance. By considering a class of distributions around the data-generating distribution, we aim to minimize the impact of adversarial attacks on the network. Our method provides an adversarial training procedure with convergence guarantees similar to non-robust approaches. It certifies performance even for the worst-case population loss and generalizes to prevent attacks on the test dataset. The formulation of the problem is intractable for deep networks, so we consider a Lagrangian relaxation for a fixed penalty parameter. The formulation of the problem involves a fixed penalty parameter \u03b3 \u2265 0, where the goal is to minimize a robust surrogate function that allows for adversarial perturbations of the data. By solving the penalty problem with the empirical distribution, moderate levels of robustness can be achieved at no significant computational or statistical cost for smooth losses. Stochastic gradient methods applied to this problem have similar convergence guarantees as non-robust methods. In Section 3, a certificate of robustness is provided for any \u03c1, with an efficiently computable upper bound on worst-case loss. Results suggest advantages of networks with smooth activations over ReLU's. Experimental verification in Section 4 shows state-of-the-art performance on various adversarial attacks. Robust optimization minimizes losses under uncertainty sets, underlying recent advances in adversarial training. The text discusses the challenges of perturbing data during stochastic optimization and the use of locally linearized loss functions in adversarial training. It highlights the difficulty of finding worst-case perturbations with deep networks using ReLU activations and suggests that smoothness in architectures like ELU's can help in finding worst-case perturbations with low computational cost. The text also mentions distributionally robust optimization and the challenges in optimizing the objective function. The text discusses robust optimization and the choice of uncertainty sets in the optimization problem. Previous approaches have used finite-dimensional parametrizations and non-parametric distances for probability measures. Tractable classes of uncertainty sets and losses have been studied, with some using convex optimization approaches for f-divergence balls. The text also mentions converting the saddle-point problem to a regularized ERM problem for worst-case regions formed by Wasserstein balls. In this work, a larger class of losses and costs are considered, with direct solution methods provided for a Lagrangian relaxation of the saddle-point problem. The approach focuses on defending against adversarial perturbations and efficient optimization procedures using a distributionally robust approach. The optimization problem becomes strongly-concave when the loss is smooth in z and the penalty is sufficiently large. The text discusses the computationally efficient and principled approach for robust optimization problems using a duality result for the minimax problem and its Lagrangian relaxation for Wasserstein-based uncertainty sets. Stochastic gradient descent methods are used to find minimizers in the convex case or approximate stationary points in the non-convex case for relaxed robust problems. Wasserstein distances define a notion of closeness between distributions. The text discusses the duality result for robust optimization problems and its Lagrangian relaxation using Wasserstein-based uncertainty sets. Stochastic gradient-type methods are developed for the relaxed robust problem, emphasizing computational benefits over strict robustness requirements. The text discusses the computational benefits of relaxing strict robustness requirements in robust optimization problems. Assumptions about the continuity and convexity of functions are made to ensure tractable computation of the robust surrogate. Smoothness conditions are also required for efficient computability. Lemma 1 shows that under certain conditions, the robust surrogate remains smooth. The text discusses Algorithm 1, a stochastic-gradient approach for the penalty problem in robust optimization. It highlights the benefits of Lagrangian relaxation for efficient computation and convergence properties of the algorithm. The text discusses the convergence guarantee of the stochastic gradient method for nonconvex optimization problems with smooth losses. The method achieves rates of convergence comparable to standard smooth non-convex optimization. The smoothness of the loss function is crucial for the method's convergence, as it allows for concavity in the penalized version of the objective function. In deep learning, replacing ReLU's with sigmoids or ELU's allows for tractable distributionally robust optimization. Adversarial perturbations in supervised-learning scenarios focus on feature vectors, not labels. The Wasserstein cost function is defined for feature vectors. The framework can handle perturbations to only a subset of coordinates in Z, such as a small fixed region in an image. The general formulation covers variants by modifying the cost function to prevent attacks on the test set. Results hold uniformly over parameter space, providing a data-dependent upper bound on robustness. Adversarial perturbations on the training set generalize, guaranteeing a similar level of robustness as solving the population counterpart. Our main result in this section provides a data-dependent upper bound for the worst-case population objective with high probability. The empirical maximizers of the Lagrangian formulation are efficiently computable, giving a certificate of robustness to Wasserstein perturbations. The bound relies on covering numbers for the model class, ensuring uniform convergence guarantees typical of empirical risk minimization. The main result of this section provides a data-dependent upper bound for the worst-case population objective with high probability, relying on covering numbers for the model class to ensure robustness to Wasserstein perturbations. The bound is efficiently computable and guarantees uniform convergence typical of empirical risk minimization. The main result of this section provides a data-dependent upper bound for the worst-case population objective with high probability, ensuring robustness to Wasserstein perturbations. The certificate for robustness is easy to compute and the output of Algorithm 1 is expected to be close to the minimizer of the surrogate loss, providing the best guarantees. The bounds may be too large for practical use in security-critical applications, but the strong duality result still applies to any distribution. The strong duality result, Proposition 1, applies to any distribution. Given a collection of test examples Z test i, we can compute the Monge-map and test loss to guarantee bounds on parameter sensitivity. The level of robustness on the training set generalizes, with Lemma 1 showing smoothness under Assumptions A and B. The level of robustness achieved for the empirical problem concentrates uniformly around its population counterpart with high probability. The text discusses distributionally robust optimization with adversarial training, extending beyond supervised learning. Empirical evaluations on supervised and reinforcement learning tasks are presented, comparing performance with empirical risk minimization (ERM) and various training methods. The technique involves augmenting stochastic gradient steps with projected gradient ascent, iterating over data points. The adversarial training literature is referenced for further information. The text discusses defending against weaker adversaries by training against L2-norm attacks. The overall cost is defined as the covariate-shift adversary for WRM. Different methods are used for training and testing against adversarial perturbations. Visualizations show the benefits of certified robustness. Experiments include supervised learning on MNIST and reinforcement learning tasks. In training against L2-norm attacks, the process differs from testing. WRM shows theoretical guarantees for large \u03b3 but becomes heuristic for small \u03b3. Comparisons with other methods on attacks with large adversarial budgets are made. WRM matches or outperforms other heuristics against imperceptible attacks but underperforms for attacks with large budgets. Synthetic data is generated for experiments, training small neural networks with different activations and comparing with ERM and FGM. The classification boundaries for training procedures using ReLU and ELU activations are illustrated in Figure 0. WRM with ELU provides a certified level of robustness, yielding an axisymmetric classification boundary that hedges against adversarial perturbations in all directions. The lack of robustness guarantees in WRM with ReLU results in sensitivities, while WRM pushes the boundaries further outwards compared to ERM and FGM. In Figure 2 (a), a certificate is plotted against the worst-case performance for WRM with ELU's. The Lagrangian relaxation is used to evaluate the worst-case loss for different values of \u03b3 adv. Adversarial perturbations are considered for the test dataset, and the certificate provides a performance guarantee for various levels of robustness. A neural network classifier is trained on the MNIST dataset using WRM with specific parameters. In Figure 2 (b), the certificate of robustness for WRM is illustrated, showing performance guarantees for different levels of robustness. Adversarial training techniques are compared, with all methods achieving high test-set accuracy. WRM demonstrates more robustness against PGM attacks compared to other methods. Training with Euclidean cost also provides defense against fast gradient attacks. Stability of the loss surface to input perturbations is further examined. The text discusses the effectiveness of adversarial-training methods in defending against gradient-exploiting attacks by reducing gradient magnitudes near the nominal input. The models differ in stability, with WRM being the most stable. Adversarial examples are used to show misclassifications, with WRM consistently making reasonable predictions. WRM defends against exploits by learning a representation that redirects gradients towards inputs of other classes. The text discusses defense mechanisms against gradient-based attacks by reducing gradient magnitudes and improving interpretability. It explores distributional robustness in Q-learning, focusing on maximizing cumulative rewards in robust Markov decision processes with state-action transition probabilities. Incorporating distributional robustness in Q-learning involves modifying the state-transition update with an adversarial perturbation to provide robustness to uncertainties in state-action transitions. This approach can be efficiently solved using gradient descent, especially for tabular Q-learning. Testing this adversarial training procedure in the cart-pole environment shows promising results in balancing a pole on a cart by moving it left or right. Incorporating distributional robustness in Q-learning involves modifying state-transition updates with adversarial perturbations for uncertainties. The cart-pole environment caps episode lengths at 400 steps and ends prematurely if the pole falls too far from vertical or the cart translates too far. Reward function is based on the angle of the pole from vertical. Q is represented in a tabular form with discretized states for pole angle and its time-derivative. Action space is binary - push cart left or right with fixed force. Testing models with perturbations to physical parameters shows varying levels of instability. Performance comparison shows robust model outperforms in harder environments. The robust model outperforms in harder environments and learns more efficiently than the nominal model. A method for guaranteeing distributional robustness with adversarial data perturbation is provided, showing strong statistical guarantees and fast optimization rates. Empirical evaluations demonstrate robustness to data perturbations and outperform less-principled adversarial training techniques. Smooth networks may be preferable for certifying robustness in deep learning. Our approach offers simplicity and wide applicability across various machine-learning scenarios, with potential for future investigation. However, limitations exist for small values of robustness and certain Wasserstein costs. Statistical guarantees may be limited by model complexity, especially for deep networks. While our guarantees are satisfactory in a learning-theoretic context, they may fall short in security-essential situations. Future work could enhance theoretical guarantees by incorporating more intricate notions. The work focuses on small-perturbation attacks and building models to guard against them efficiently. In the large-perturbation regime, training certifiably secure systems remains a challenge. Conventional defense heuristics may not be effective in the face of large-perturbation attacks. Moving beyond current attack and defense models may be necessary for advancements in security research in deep learning. The text discusses the behavior of gradient-based perturbations on images and the consistency of misclassifications in different models. It also explores the impact of penalty parameters on model robustness and the efficiency of computing for small penalties. The study visualizes the smallest perturbation needed to cause misclassification in a model. The text discusses training methods for defending against adversarial attacks using different cost functions and norms. The inner supremum is no longer strongly concave for over 10% of the data, affecting performance guarantees. The method becomes heuristic for large adversaries, similar to other approaches. Comparisons are made with standard adversarial training methods using different norms. The text discusses training methods for defending against adversarial attacks using different cost functions and norms. It compares the performance of WRM with other techniques under different training adversarial budgets. WRM outperforms other heuristics against imperceptible attacks for both Euclidean and \u221e norms, but its performance is worse than other methods for attacks with large adversarial budgets. The text discusses the challenges of optimizing the Lagrangian formulation with \u221e-norm adversaries and proposes a heuristic algorithm based on proximal algorithms. Results comparing the proposed method with other adversarial training procedures are shown in figures. It is shown that computing worst-case perturbations for feedforward neural networks with ReLU activations is NP-hard. The text discusses the challenges of optimizing the Lagrangian formulation with \u221e-norm adversaries and proposes a heuristic algorithm based on proximal algorithms. It is shown that worst-case perturbation problem for feedforward neural networks with ReLU activations is NPO and NP-hard. The decision problem can be verified in polynomial time and is NP-complete. It Turing-reduces to the optimization problem, which is NP-hard. An alternative proof using convex analysis is provided, with a focus on normal integrands and a specific theorem. Theorem 5 states that for functions f and c, where g(x, z) = \u03b3c(x, z) - f(x) is a normal integrand for any \u03b3 \u2265 0. The proof involves convexity of the mapping P \u2192 Wc(P, Q) in the space of probability measures and the use of standard duality results. The goal is to show equality by considering regular conditional probabilities from Z to X. The distribution P(\u00b7|z) is regular if it is a distribution for each z, and for each measurable A, the function z \u2192 P(A|z) is measurable. Using measurability results, it is shown that the conditional distribution P(\u00b7|z) supported on x(z) is measurable. The uniqueness and Lipschitzness of z(\u03b8) are discussed, with strong convexity and concavity playing key roles in the argument. The text discusses the differentiability of a function f with respect to \u03b8, showing that f is inf-compact and directionally differentiable. The uniqueness of S(\u03b8) is proven through strong convexity, leading to the conclusion that f is differentiable. The proof in DISPLAYFORM8 uses inequality FORMULA7 and is based on BID20's proof. By performing gradient steps with DISPLAYFORM0 and a constant stepsize \u03b1 = DISPLAYFORM1, we show the bound (11) and concentration of E Pn [\u03c6 \u03b3 (\u03b8; Z)]. The functional \u03b8 \u2192 F n (\u03b8) satisfies bounded differences, leading to the result for the second bound (12). The proof in DISPLAYFORM8 uses inequality FORMULA7 and is based on BID20's proof. By performing gradient steps with DISPLAYFORM0 and a constant stepsize \u03b1 = DISPLAYFORM1, we show the bound (11) and concentration of E Pn [\u03c6 \u03b3 (\u03b8; Z)]. The functional \u03b8 \u2192 F n (\u03b8) satisfies bounded differences, leading to the result for the second bound (12). Since we have DISPLAYFORM3 from the strong duality in Proposition 1, our second result follows. The result is essentially standard BID52, which we now give for completeness. Note that for DISPLAYFORM0, we show that P * (\u03b8) and P * n (\u03b8) are attained for all \u03b8 \u2208 \u0398. The transportation mapping T (\u03b8, z) is well-defined and unique under the assumption of strong concavity. By using Kantorovich duality, it is shown that c is Lipschitz in both \u03b8 and z. Lemma 4 states conditions for \u03b8 1 , \u03b8 2 \u2208 \u0398, and provides proofs for cases where c is Lipschitz in its arguments. The transportation mapping T(\u03b8, z) is unique under strong concavity. Lemma 4 provides conditions for \u03b81, \u03b82 \u2208 \u0398 and proves cases where c is Lipschitz in its arguments. The transport map for covariate shift is defined, with a decreasing function \u03b2 \u2192 i:vi>\u03b2(vi - \u03b2) - \u03b1\u03bb\u03b2 = h(\u03b2)."
}