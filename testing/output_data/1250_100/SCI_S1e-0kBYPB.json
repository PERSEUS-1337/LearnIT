{
    "title": "S1e-0kBYPB",
    "content": "In this work, the authors highlight issues with current explanatory methods for AI systems, particularly in explaining black-box models like neural networks. They point out that different explanation perspectives lead to varying instance-wise explanations and that current explainers are mainly validated on simple models, not real-world neural networks. To address this, they introduce a verification framework based on a complex neural network architecture trained on a real-world task to provide guarantees on its inner workings. The authors introduce a verification framework based on a complex neural network architecture to provide guarantees on its inner workings and evaluate current explainers for black-box models. They highlight the different perspectives on explanations and how they lead to varying instance-wise explanations. The authors discuss how different explanatory methods lead to varying explanations and compare feature-selection and feature-additivity explainers. They also question the reliability of current explainers when the target model has less dramatic biases, highlighting the challenge of evaluating explainers for complex neural networks. The authors question the reliability of current explainers when the target model has less dramatic biases, highlighting the challenge of evaluating explainers for complex neural networks. They propose a framework for generating evaluation tests for explanatory methods under the feature-selection perspective, identifying subsets of tokens that have zero contribution to the model's prediction. The authors introduce a framework for evaluating explainers in the context of multi-aspect sentiment analysis. They test if explainers rank zero-contribution tokens higher than relevant tokens and provide guarantees on the behavior of the target model for critical failures. This framework generates necessary evaluation tests without relying on speculations on the model's behavior, offering a non-trivial evaluation method for explainers. The authors introduce a framework for evaluating explainers in multi-aspect sentiment analysis, focusing on feature-selection explanatory methods like LIME and SHAP. They find that LIME and SHAP generally outperform L2X in their test, highlighting potential failures in explainers predicting relevant tokens. They plan to release their test for community use in evaluating future explanatory methods. The methodology for evaluating feature-selection explanatory methods is discussed, including feature-based explainers and different types of explanations such as feature-additive and feature-selective. Other types of explanations like example-based and human-level explanations are also mentioned. In this work, the focus is on verifying feature-based explainers, with evaluations commonly performed on interpretable target models like linear regression and decision trees. Synthetic setups are also used for evaluation, creating controlled tasks to assess the faithfulness of explainers to the target model. The focus is on verifying feature-based explainers, with evaluations commonly performed on interpretable target models like linear regression and decision trees. Synthetic setups are used for evaluation, creating controlled tasks to assess the faithfulness of explainers to the target model. Tasks include 2-dim XOR, orange skin, nonlinear additive model, and switch feature. Evaluations involve identifying intuitive heuristics that high-performing models are assumed to follow, but neural networks may discover surprising artifacts. Explanations are evaluated based on their ability to help humans predict the model's behavior. The evaluation of explainers involves humans predicting model outputs based on explanations. While previous methods require human effort, a new automatic framework evaluates a neural network model with guaranteed inner-workings. This approach is more challenging and ensures explainers provide different explanations for real data models compared to randomized ones. The evaluation of explainers involves humans predicting model outputs based on explanations. A new automatic framework evaluates a neural network model with guaranteed inner-workings, making the test more challenging. Current explanatory methods adhere to two major perspectives of explanations, with many methods following the feature-additivity perspective. Shapley values from game theory are shown to be the only set of feature-additive contributions that verify desired constraints. The contribution of each feature in an instance is averaged over a neighborhood of the instance, with the choice of neighborhood being critical. Different perspectives on explanations include the feature-selection approach, where a subset of features is identified that leads to a similar prediction as the original model. Various methods adhere to this perspective, such as L2X, which learns the subset by maximizing mutual information with the prediction. The number of important features per instance is often unknown in practice. In practice, the number of important features per instance is often unknown. A hypothetical sentiment analysis regression model is used to illustrate the differences between relying on a subset of features and using all features. Real-world neural networks may heavily rely on specific tokens in the input, which may not always be accurate indicators for the target class. The feature-additive and feature-selective perspectives provide different explanations for the model's behavior on specific instances. The feature-additive perspective gives an average explanation for a neighborhood of the instance, while the feature-selective perspective focuses on the pointwise features used by the model on the instance in isolation. The differences between the two perspectives are highlighted by the varying rankings of features on different instances. In the paper, a verification framework is proposed for the feature-selection perspective of instance-wise explanations using the RCNN model. The framework prunes the dataset to identify irrelevant and relevant features for each datapoint. Metrics are introduced to measure the ranking of tokens by explainers. The RCNN model consists of a generator and an encoder, both utilizing recurrent convolutional neural networks. The RCNN model includes a bidirectional network that selects a subset of tokens from input text x for prediction. The generator and encoder are trained jointly with supervision only on the final prediction. Regularizers encourage the selection of a short sub-phrase and fewer tokens. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability. The model may have learned an internal communication protocol for encoding predictions. The RCNN model has learned an internal communication protocol that encodes information from non-selected tokens via a handshake. The goal is to eliminate handshakes by ensuring that non-selected tokens have zero contribution to the model's prediction. The presence of a handshake can be captured by Equation 7, where non-selected tokens are considered irrelevant or zero-contribution. The model's selection of tokens does not necessarily imply the presence of a handshake. The dataset is pruned to retain instances where selected tokens are relevant for the prediction, ensuring non-selected tokens have zero contribution. Noise tokens like \"the\" or \"a\" may be selected for continuity but are not penalized. Selected tokens must be clearly relevant for the prediction. The dataset is pruned to retain instances where selected tokens are relevant for the prediction, ensuring non-selected tokens have zero contribution. Tokens must be clearly relevant for the prediction, with a significant change in prediction when removed. Selected tokens are further partitioned into clearly relevant tokens and tokens for which relevance is unknown. The dataset is pruned to retain instances where selected tokens are relevant for the prediction, ensuring non-selected tokens have zero contribution. Evaluation metrics are defined based on the ranking provided by explainers on the features in the instance x. Error metrics are calculated to assess the performance of explainers in identifying the most important tokens. In this work, the framework is instantiated on the RCNN model trained on the BeerAdvocate corpus, consisting of human-generated beer reviews with three aspects. The RCNN aims to predict ratings rescaled between 0 and 1 for appearance, aroma, and palate independently. Three separate RCNNs are trained for each aspect. Three separate RCNNs are trained for each aspect independently with the same default settings. Datasets are gathered for each aspect, with a threshold of 0.1 used to determine clearly relevant tokens. Statistics on dataset lengths and token relevance are provided in the appendix. The threshold of 0.1 is considered strict, with 1 or 2 clearly relevant tokens per datapoint. The evaluation ensures the no-handshake condition. In evaluating explainers for feature selection, LIME and SHAP outperformed L2X, despite L2X being a feature-selection explainer. A major limitation of L2X is the need to know the number of important features per instance, which is often unknown in practice. In real-world testing, L2X used the average number of tokens highlighted by human annotators as K, with average values of 23, 18, and 13 for different aspects. The evaluation revealed that explainers like LIME and L2X sometimes rank zero-contribution tokens higher than relevant features, with L2X showing more errors in ranking. In the evaluation dataset, the heatmap shows rankings of tokens by explainers for the palate aspect. LIME and SHAP rank nonselected tokens as important, while L2X highlights \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as crucial, omitting \"gorgeous\". In this work, an evaluation test for post-hoc explanatory methods was introduced, highlighting the distinction between two perspectives of explanations. Error rates on different metrics for three popular explanatory methods were presented, showing potential failures in predictions. The methodology is adaptable to various tasks, such as computer vision, where neural networks can make predictions based on selected features. The evaluation test for post-hoc explanatory methods introduced in this work highlights the limitations of current explainers. Statistics of the dataset are provided, including the number of instances retained, average review length, and average numbers of selected and non-selected tokens. The percentage of instances eliminated due to potential handshakes and the absence of selected tokens with significant effects on predictions are also discussed. The text discusses the impact of non-selected tokens on model predictions, suggesting that if these tokens do not influence the final prediction, there is no handshake effect. The proof involves equations and the concept of no handshake in the instance x. The text also briefly mentions a bottle of \"grolsch\" beer. The curr_chunk describes a dark yellow beer with fruity aromas, minimal mouthfeel, and a smooth taste. The beer is better than most American lagers and comes in a reusable \"grolsch\" bottle. The dark yellow beer has fruity aromas, minimal mouthfeel, and a smooth taste. It is better than most American lagers and comes in a reusable \"grolsch\" bottle."
}