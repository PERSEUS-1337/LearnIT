{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is popular for training deep neural networks. Momentum adjustment reduces convergence time in non-stochastic convex optimization. Empirical evidence shows that stochastic momentum improves convergence time in deep network training. Theoretical justification for stochastic momentum lies in its ability to help escape saddle points faster and find a second order stationary point. SGD with stochastic momentum is widely used in nonconvex optimization and deep learning. Empirical and experimental findings support the use of momentum parameter $\\beta \\in [0,1)$ close to 1. Various applications such as computer vision, speech recognition, natural language processing, and reinforcement learning benefit from SGD with stochastic momentum for faster convergence compared to standard SGD. SGD with stochastic momentum, a widely used tool in nonconvex optimization and deep learning, has been shown to achieve faster convergence compared to standard SGD. The success of momentum has made it essential for designing new optimization algorithms. Despite the lack of clear empirical evidence, large momentum values (e.g. \u03b2 = 0.9) are commonly used in practice. Theoretical analysis for SGD with momentum is provided in this paper, outlining the required step size parameter \u03b7 and momentum parameter \u03b2. In this paper, the focus is on SGD with stochastic heavy ball momentum for finding second-order stationary points in non-convex optimization. The algorithm maintains a weighted average of stochastic gradients and updates the current update in the direction of momentum. The goal is to escape saddle points faster than standard SGD, with guarantees under mild conditions. The objective is to find a (\u03b5, \u03b4)-second-order stationary point that satisfies specific conditions. The paper focuses on using stochastic heavy ball momentum in SGD to find second-order stationary points in non-convex optimization. It aims to show the benefits of momentum in reaching an approximate second-order stationary point, introducing a condition for updates to correlate with negative curvature directions. The recursive dynamics of SGD with heavy ball momentum amplify the escape signal \u03b3, enabling faster escape from saddle points. If SGD with momentum satisfies certain properties, it can reach a second-order stationary point in a certain number of iterations. A larger momentum parameter helps in escaping saddle points faster, which is beneficial in optimization and deep learning. Stochastic momentum helps avoid saddle points by moving iterates in the direction of the smallest eigenvector of the Hessian, enabling faster escape from regions with negative eigenvalues. This approach provides a fast escape from saddle points, which is crucial for optimization and deep learning. Stochastic momentum accelerates escape from saddle points by moving in the direction of the smallest eigenvector of the Hessian, aiding optimization and deep learning. Daneshmand et al. (2018) studied non-momentum SGD and found that updates escape saddle points due to strong non-orthogonality between the update direction and the direction of large negative curvature. This effect is amplified in successive iterations with momentum updates when \u03b2 is close to 1. Stochastic momentum can accelerate escape from saddle points in optimization tasks, showing a performance improvement by a factor of 1 \u2212 \u03b2. However, the choice of \u03b2 is constrained and cannot be arbitrarily close to 1. Empirical evidence demonstrates the benefits of stochastic momentum in solving non-convex optimization challenges with saddle points. In optimization tasks, stochastic momentum can accelerate escape from saddle points, with empirical evidence showing significant acceleration in convergence for both objectives. The algorithms are initialized at the origin and use the same step size, with convergence plotted in function value and relative distance to the true model. Phase retrieval problem with real applications in physical sciences involves finding an unknown vector with limited samples. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases. However, stochastic momentum has been shown to significantly accelerate convergence in optimization tasks, particularly in escaping saddle points. This is the first reported empirical finding of the speedup in phase retrieval for finding optimal solutions. Our work belongs to the category of specialized algorithms designed to exploit negative curvature and escape saddle points faster. Previous works by Ge et al. (2015) and Jin et al. (2017) have shown methods to guarantee gradient descent escapes saddle points and reaches a second order stationary point. The assumption of Correlated Negative Curvature for stochastic gradient allows the algorithm to avoid perturbing updates. Our work assumes Correlated Negative Curvature (CNC) for stochastic momentum instead of perturbing updates with isotropic noise. We assume the gradient is L-Lipschitz and the Hessian is \u03c1-Lipschitz, ensuring certain properties. The stochastic gradient has bounded noise and momentum, and our analysis relies on three key properties of the stochastic momentum dynamic. Aligned with Gradient (APAG) 4 states that SGD with stochastic momentum satisfies Almost Positively Correlated with Gradient (APCG) with parameter \u03c4 if certain conditions are met. The momentum term must not be significantly misaligned with the gradient, ensuring progress in the algorithm. APCG requires that the momentum term is almost positively correlated with the gradient, measured in the Mahalanobis norm induced by M t. The PSD matrix M t measures the local curvature of the function with respect to the trajectory of SGD with momentum. Empirical evidence shows that SGD with momentum exhibits APAG and APCG properties. The expected values for the phase retrieval problem may be nonnegative. The analysis focuses on negative curvature and alignment between stochastic momentum and gradient in optimization algorithms like GrACE. Results are presented in Figures 3 and 4, showing quantities related to APAG, APCG, and gradient norm when solving problems with SGD momentum. The analysis follows a similar template to previous studies. The analysis follows a similar template to previous studies (Jin et al. (2017); Daneshmand et al. (2018); Staib et al. (2019)). The proof is structured into three cases: (a) \u2207f (w) \u2265 , (b) \u2207f (w) \u2264 and \u03bb min (\u2207 2 f (w)) \u2264 \u2212 , or (c) \u2207f (w) \u2264 and \u03bb min (\u2207 2 f (w)) \u2265 \u2212 , leading to a second-order stationary region. Algorithm 2 is analyzed, with a larger step size r. The algorithm shows progress in cases (a) and (b), and weakly hurts progress in case (c). Ultimately, a second-order stationary point is reached with high probability. Theorem 1 states that stochastic momentum in SGD leads to reaching a second order stationary point faster. Higher \u03b2 values enable faster escape from saddle points. Constraints on \u03b2 are necessary to prevent it from being too close to 1. The proof details can be found in Subsection 3.2.1 and in Appendix G. The text discusses constraints on \u03b2 to prevent it from being too close to 1, showing that higher momentum values help escape saddle points faster. It is proven that stochastic momentum in SGD leads to reaching a second order stationary point quicker. The analysis demonstrates the effectiveness of Algorithm 2 over CNC-SGD in finding second order stationary points in the high momentum regime. The text discusses proving by contradiction that the function value must decrease by at least F thred in T thred iterations on expectation. It leverages negative curvature to show that the lower bound is larger than the upper bound, leading to the conclusion. The dependency on \u03b2 suggests that larger values can lead to faster escape from saddle points. Lemma 1 shows that C upper,t is monotone increasing with t, defining C upper as C upper,T thred. Lemma 2 provides a lower bound for E t0 [ w t0+T thred \u2212 w t0 2 ] using recursive dynamics of SGD with momentum. Lemma 3 further analyzes the lower bound, emphasizing the critical component for ensuring it surpasses the upper bound. The text leverages negative curvature to demonstrate the relationship between the lower and upper bounds. In this paper, three properties are identified that guarantee SGD with momentum reaches a second-order stationary point faster with a higher momentum parameter. The practice of using a large momentum parameter is justified as it helps in escaping strict saddle points faster. However, ensuring that SGD with momentum possesses these three properties is not clear. The heavy ball method, proposed by Polyak in 1964, does not provide a speedup over standard gradient descent in most cases. Recent research has analyzed the method for non-quadratic functions but found that it does not outperform standard SGD. Other works have also examined stochastic heavy ball momentum and Nesterov's momentum for non-convex functions, showing a convergence rate of O(1/ \u221a t). The heavy ball method, proposed by Polyak in 1964, does not provide a speedup over standard gradient descent in most cases. Recent research has analyzed the method for non-quadratic functions but found that it does not outperform standard SGD. Other works have also examined stochastic heavy ball momentum and Nesterov's momentum for non-convex functions, showing a convergence rate of O(1/ \u221a t). Additionally, there are specialized algorithms and simple GD/SGD variants aimed at reaching a second order stationary point, with some showing faster escape from saddle points under certain conditions. The heavy ball method, proposed by Polyak in 1964, does not provide a speedup over standard gradient descent in most cases. Recent research has analyzed the method for non-quadratic functions but found that it does not outperform standard SGD. Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates, assuming an inherent property of stochastic gradients to escape saddle points faster. They compare iteration complexity results with other works and focus on the effectiveness of stochastic heavy ball momentum. Their analysis framework is based on previous work by Daneshmand et al. (2018). Lemma 6, 7, and 8 discuss the properties of SGD with momentum. Lemma 7 shows that under the APAG property, SGD with momentum decreases the function value by a constant, while Lemma 8 bounds the increase of function value of the next iterate. If SGD with momentum has the APAG property and the step size is limited, progress is made. If SGD with momentum has the GrACE property, progress is also made. Lemma 1 discusses the update process in stochastic momentum, showing the bound on the expectation of the update. The proof involves bounding the expectation using the Lipschitzness of the Hessian and the triangle inequality. The update equation is analyzed to show progress in decreasing the function value. Lemma 5 discusses the complexity of F thred in stochastic momentum. The proof involves bounding the expectation using a specific inequality. Lemma 5 discusses the complexity of F thred in stochastic momentum. The proof involves bounding the expectation using a specific inequality. The lemma establishes constraints on parameter \u03b2 to ensure certain conditions are met for the algorithm to have the APCG property. These constraints limit the value of \u03b2 to prevent it from being too close to 1. The analysis shows that the dependencies on L, \u03c3, and c are artificial and can be adjusted without loss of generality. The proof of Lemma 5 requires a series of lemmas with specific parameter choices. Lemma 5 establishes constraints on parameter choices for stochastic momentum in the context of F thred complexity. The proof involves bounding expectations using specific inequalities and ensuring conditions are met for the algorithm to have the APCG property. The analysis shows dependencies on L, \u03c3, and c can be adjusted without loss of generality. The lemma requires a series of lemmas with specific parameter choices, leading to upper bounds on various terms. The text discusses bounding expectations using specific inequalities and parameter choices for stochastic momentum in the context of F thred complexity. Lemmas 11, 12, and 13 provide lower bounds on different terms by utilizing specific conditions and proofs. These lemmas involve defining matrices, ensuring symmetry, and utilizing the zero mean assumption of certain variables. The matrix B is symmetric positive semidefinite, and the text discusses lower bounding expectations using specific inequalities and parameter choices for stochastic momentum in the context of F thred complexity. Lemmas 11, 12, and 13 provide lower bounds on different terms by utilizing specific conditions and proofs related to matrices and symmetry. The text discusses proving a contradiction by showing that the lower bound is larger than the upper bound, leading to the conclusion that the function value must decrease by at least F thred in T thred iterations. By utilizing specific inequalities and parameter choices, it is shown that the inequality holds by choosing T thred large enough. The text discusses proving a contradiction by showing that the lower bound is larger than the upper bound, leading to the conclusion that the function value must decrease by at least F thred in T thred iterations. By utilizing specific inequalities and parameter choices, it is shown that the inequality holds by choosing T thred large enough. With probability at least 1 \u2212 \u03b4, choosing a w k where \u03a5 k did not occur is guaranteed. If specific properties are met, SGD with momentum reaches a second order stationary point in T iterations with high probability. The text discusses proving a contradiction by showing that the lower bound is larger than the upper bound, leading to the conclusion that the function value must decrease by at least F thred in T thred iterations. By utilizing specific inequalities and parameter choices, it is shown that the inequality holds by choosing T thred large enough. With probability at least 1 \u2212 \u03b4, choosing a w k where \u03a5 k did not occur is guaranteed. If specific properties are met, SGD with momentum reaches a second order stationary point in T iterations with high probability. In the proof based on Lemma 15, conditions are satisfied to apply the lemma and finish the theorem. Based on the context provided, the condensed text chunk is:\n\nBy utilizing Lemma 15 and specific parameter choices from Table 3, setting T = 2T thred f (w 0 ) \u2212 min w f (w) /(\u03b4F thred ) = O((1 \u2212 \u03b2) log( Lcm\u03c3 2 \u03c1c c h (1\u2212\u03b2)\u03b4\u03b3 ) \u221210 ) returns a second order stationary point. Algorithm 2 outperforms previous methods by not depending on the variance of stochastic gradient, showing that higher momentum can lead to faster convergence to a second order stationary point."
}