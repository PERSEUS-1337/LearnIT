{
    "title": "HylvlaVtwr",
    "content": "Recent progress in physics-based character animation has focused on human motion synthesis using deep reinforcement learning. A new hierarchical reinforcement learning framework is proposed for interactive tasks like sitting on a chair, which outperforms traditional methods. The approach also shows promise for motion prediction from image inputs. The capability of synthesizing realistic human-scene interactions is crucial for simulating human living space and training robots to collaborate with humans. Motion capture data and deep learning algorithms have advanced motion synthesis, but the lack of physical interpretability in synthesized motion remains a major limitation, especially in interactions involving human-object or human-human interactions. This limits the practical use of these approaches. Recent progress in physics-based character animation has shown impressive results in synthesizing realistic motions through deep reinforcement learning. These approaches can adapt to different physical contexts and perform well in interaction-based motions, such as walking on uneven terrain or performing stunts under obstacle disturbance. However, a drawback is that a single model is trained for a specific task with a distinct motion pattern, limiting their generalization to higher-level interactive tasks that require flexible motion patterns. In this paper, the focus is on high-level interactive tasks like sitting onto a chair, which require different sequences of actions based on human-chair configurations. A hierarchical reinforcement learning method is proposed to address the challenge of generalization by decomposing the main task into subtasks and training a meta controller to execute them properly. This approach leverages the observation that humans have a repertoire of motion skills for different tasks. The paper extends physics-based motion imitation to higher-level interactive tasks using a hierarchical approach. It demonstrates the strength of this approach over single level and hierarchical baselines and shows its application in motion synthesis in human living space with 3D scene reconstruction. Kinematic modeling of human motions has a substantial literature in both vision and graphics domains, with recent progress in deep learning enabling more efficient algorithms for modeling human motions from large-scale mocap data. The focus in the vision community is often on motion prediction, where a sequence of mocap poses is used for historical observation. Recent work in the graphics community focuses on motion synthesis from mocap examples, aiming for realistic motions but facing challenges in generalization. Physics-based models for character animation have a long history in computer graphics, with recent work using deep reinforcement learning to imitate mocap data and produce robust and realistic motions. Recent work in the graphics community focuses on motion synthesis from mocap examples, aiming for realistic motions but facing challenges in generalization. Our model, inspired by hierarchical reinforcement learning, addresses a more complex task of executing diverse subtasks like walking, turning, and sitting onto a chair. Unlike previous works, our meta controller is trained to select any subtask at any time point, allowing for more flexibility in task completion. Our model, inspired by hierarchical reinforcement learning, aims to generate realistic motions by decomposing tasks into simpler subtasks. It is connected to learning object affordances in the vision domain, focusing on motion synthesis in a physics simulated environment using limited mocap examples and reinforcement learning. The main task is to generate motion given a chair and a skeletal pose of a human in 3D space. The system aims to generate realistic motions by decomposing tasks into subtasks, using a physics simulated environment and limited mocap data. It involves controlling a humanoid to sit on a chair through a hierarchical architecture with subtask controllers and a meta controller. Subtask policies are trained on mocap data to imitate human motions, with control input generated at different timescales. The system uses a hierarchical architecture with subtask controllers and a meta controller to generate realistic motions in a physics simulated environment. Subtasks are formulated as independent reinforcement learning problems, with each subtask having its own policy network that maps state vectors to actions. The network architecture is consistent across subtasks, using a multi-layer perceptron with two hidden layers of size 64. The output of the network parameterizes the probability distribution of actions, modeled by a Gaussian distribution with a fixed diagonal covariance matrix. The system uses a hierarchical architecture with subtask controllers and a meta controller to generate realistic motions in a physics simulated environment. Each subtask is treated as an independent reinforcement learning problem, with its own policy network mapping state vectors to actions. The reward function is crucial in shaping the humanoid's motion style, with a focus on imitating mocap references and achieving task goals. The system utilizes a hierarchical architecture with subtask controllers and a meta controller to create lifelike movements in a physics simulated setting. Each subtask is viewed as a separate reinforcement learning challenge, with its own policy network linking state vectors to actions. The reward function plays a vital role in shaping the humanoid's motion style, emphasizing mimicking mocap references and reaching task objectives. The approach encourages similarity in local joint angles and velocities between the humanoid and the reference motion, with specified weights for different factors. The state representation includes proprioceptive features and a goal feature for target-directed locomotion. The training strategy involves two stages with distinct task objectives for target-directed walking policy. The first stage focuses on steering patterns similar to reference motion, while the second stage rewards motion towards the target. Additionally, the sit subtask involves lowering the body to be seated, with a state capturing pose information of the chair. The sit subtask involves capturing the pose information of the chair, with the state consisting of proprioceptive features and chair state in humanoid-centric coordinates. The task objective rewards moving the pelvis towards the center of the seat surface. The meta controller encodes chair pose information and controls the execution of subtasks. The switch and target components are part of a meta task in an independent RL problem. The policy network parameterizes the probability distributions for switch and target, with switch being a categorical distribution and target being a Gaussian distribution. The reward function encourages the pelvis to move towards and be in contact with the seat surface. The meta task involves training subtasks and a meta controller independently using standard RL algorithms. Subtasks are trained first, followed by training the meta controller. Controllers are trained using the proximal policy optimization (PPO) algorithm in an actor-critic framework. Subtask training includes initializing the humanoid pose and using an early termination strategy based on height and motion alignment. Target-directed walking involves sampling new targets in front of the humanoid periodically. In the second stage, controllers are fine-tuned by setting the initial pose to a sampled ending pose of another subtask. The meta controller's task is to have the humanoid sit down regardless of its starting position, which can vary in difficulty. Training from a challenging initial state requires the humanoid to navigate by chance. To aid training a humanoid to sit down, a multi-stage strategy inspired by curriculum learning is proposed. Starting from easier states and progressively increasing difficulty, the humanoid is trained to sit from different positions. Motion data is collected from CMU Graphics Lab Motion Capture Database, retargeted to a humanoid model, and implemented in the simulation environment based on OpenAI Roboschool using the Bullet physics engine. The simulation environment is based on OpenAI Roboschool using the Bullet physics engine. A randomly selected chair model from ShapeNet is used. The PPO algorithm for training is implemented from OpenAI Baselines. Qualitative results of subtask controllers are shown, including walking, following a target, turning, and sitting on a chair. Two metrics are used to evaluate the main task: success rate and minimum distance. Success is defined as continuous contact with the seat surface for 3.0 seconds. Success rate is reported over 10,000 trials, while minimum distance is also computed. In the simulation environment, the humanoid's performance is analyzed by computing the per-trial minimum distance between the pelvis and the seat surface. Two initialization settings, Easy and Hard, are considered for the task. In the Easy setting, the humanoid simply walks forward, turns around, and sits down, while in the Hard setting, it needs to walk around the chair to sit down successfully. The approach is benchmarked against various baselines in the Easy setting, including a kinematics-based method using a mocap clip with a motion sequence. The humanoid's performance is analyzed by computing the minimum distance between the pelvis and the seat surface. Two initialization settings, Easy and Hard, are considered for the task. The kinematics baseline method aligns frames to the humanoid's initial pose, while the physics-based approach trains a controller to imitate the motion. Results show that both methods do not achieve success in physical interactions with the chair. The humanoid's performance in approaching a chair is analyzed using kinematics and physics-based baselines. The baselines fail to imitate realistic sitting motions, leading to zero success rates. Hierarchical approaches outperform single-level methods, supporting the hypothesis that hierarchical control is more effective in this task. Hierarchical models outperform single-level approaches in approaching a chair, showing better generalization by breaking tasks into reusable subtasks. The success rate remains low due to failures in subtask execution, transitions, and an insufficient subtask repertoire. Individual subtask success rates vary, highlighting challenges in achieving realistic sitting motions. The success rate in approaching a chair can be improved by fine-tuning transitions between subtasks. Analyzing the meta controller's behavior reveals favored transitions based on starting positions. Increasing task difficulty by initializing the humanoid in different zones shows the impact of curriculum learning strategy on success rates. The success rate in approaching a chair improves with a tailored curriculum, especially in Zones 2 and 3. Without curriculum learning, the humanoid struggles to turn and sit. Different behaviors are observed when starting from the rear versus the front side of the chair. Vision-based application synthesizes sitting motions from a single RGB image of a human living space. The study focuses on synthesizing sitting motions in a simulated environment using detected chairs and their 3D positions. While the synthesized humanoid motion appears physically plausible, it may not always be accurate due to the lack of modeling other objects in the scene. Future work could involve learning motion in cluttered scenes or synthesizing motions based on observed humans using 3D human pose extraction from a single image."
}