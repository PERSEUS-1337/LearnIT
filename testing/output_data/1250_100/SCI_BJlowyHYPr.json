{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model designed for forecasting data streams from geospatial point-cloud sources. It utilizes a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. The D-Conv operator can be integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. CloudLSTM is applied to mobile service traffic and air quality forecasting using point-cloud streams. Results show accurate long-term predictions, outperforming other neural network models. Point-cloud stream forecasting deals with irregular, unordered data points and complex spatial correlations. The PointCNN model leverages spatial-local correlations of point clouds, regardless of input order, to forecast over point-cloud data streams directly. It differs from ConvLSTM and PredRNN++ which are limited to grid-structured data, making it suitable for handling scattered point-clouds. The CloudLSTM architecture is designed to forecast over point cloud-streams by utilizing the DConv operator. It combines CloudLSTM with Seq2seq learning and attention mechanisms to achieve precise forecasting. The point-cloud is defined as a set of N points, each containing value features and coordinates. Different channels of the point cloud can be obtained at each time step. The model aims to embrace five key aspects for ideal point-cloud stream forecasting. The ideal point-cloud stream forecasting model should have five key properties: order invariance, information intactness, interaction among points, robustness to transformations, and location variance. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module for this purpose. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM, satisfying key properties for point-cloud stream forecasting models. DConv generalizes convolution on grids by operating on point-clouds, ensuring information intactness. It takes U in channels of a point-cloud S and outputs U out channels with the same number of elements as the input. The Dynamic Point Cloud Convolution (DConv) operator in CloudLSTM operates on point-clouds, preserving information integrity. It utilizes learnable weights to aggregate features and points, generating output channels with the same number of elements as the input. The DConv operator in CloudLSTM uses biases for output maps, sigmoid function for coordinate prediction, and normalizes raw point-cloud coordinates. The K nearest points vary per channel due to different types of measurements in the dataset, reflecting spatial correlations influenced by human mobility. The DConv operator in CloudLSTM utilizes biases for output maps, sigmoid function for coordinate prediction, and normalizes raw point-cloud coordinates. It aims to learn spatial correlations influenced by human mobility and improve forecasting performance by allowing each channel to find the best neighbor set. DConv is a symmetric function that operates on neighboring point sets to capture local dependencies and enhance robustness to global transformations. The DConv operator in CloudLSTM captures local dependencies, improves robustness to global transformations, and learns the layout of the cloud-point for dynamic positioning. It can be efficiently implemented using 2D convolution and is related to PointCNN and Deformable Convolution. The DConv operator in CloudLSTM builds upon PointCNN and DefCNN on grids, introducing variations tailored to pointcloud structural data. It ensures order invariance without extra complexity or information loss by aligning weights based on distance rankings. DConv can be seen as DefCNN over point-clouds, deforming input maps and selecting neighboring points for operations. Both models offer transformation modeling flexibility. The CloudLSTM model, incorporating the DConv operator, allows for adaptive receptive fields on convolution and learning spatial and temporal correlations over point-clouds. It combines CloudLSTM with Seq2seq learning and the soft attention mechanism for forecasting. The Seq2seq CloudLSTM model incorporates an encoder and decoder with CloudLSTMs for forecasting. Data is processed by CloudCNN layers before feeding it to the model. Increasing the number of stacks and channels does not significantly improve performance. The CloudRNN and CloudGRU models, derived from CloudLSTM, lack the attention mechanism but share a similar Seq2seq architecture. Performance evaluation is conducted using traffic and air quality datasets, comparing with 12 baseline deep learning models. All models are implemented in TensorFlow and TensorLayer, trained on a computing cluster with NVIDIA Tesla K40M GPUs. In this study, models are optimized using the Adam optimizer and trained on a computing cluster with NVIDIA Tesla K40M GPUs. The experiments focus on spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments. Large-scale multi-service datasets are used for mobile traffic forecasting in European metropolitan areas. The data includes traffic volume from devices associated with antennas in the target cities. Coordinate features are omitted in the final output, but may be included in different use cases like crowd mobility forecasting. The study focuses on spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments using large-scale multi-service datasets for mobile traffic forecasting in European metropolitan areas. The dataset includes traffic volume from devices associated with antennas in two target cities, aggregated over 5-minute intervals for 38 different mobile services. Air quality forecasting performance is also investigated using a public dataset from China, comprising six air quality indicators collected by 437 monitoring stations. Before feeding to the models, measurements for mobile services and air quality indicators are transformed into input channels of the point-cloud S. Coordinate features are normalized to the (0, 1) range. Point-clouds are transformed into grids for baseline models using the Hungarian algorithm. The performance of CloudLSTM is compared with PointCNN, CloudCNN, and PointLSTM for feature extraction from point-clouds. The CloudLSTM model is compared with other Seq2seq architectures and its variations, CloudRNN and CloudGRU. Various baseline models like MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++ are also discussed. The accuracy of CloudLSTM is evaluated using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are used to measure the fidelity of forecasts. Different neural networks are employed to predict city-scale mobile traffic consumption over a time horizon of 30 minutes based on past measurements. In the air quality forecasting use case, RNN-based models like LSTM, ConvLSTM, PredRNN++, CloudLSTM, CloudRNN, and CloudGRU are evaluated for long-term performance. Models receive 12 measurements as input and forecast indicators for the following 12 hours. Performance metrics are reported for 6-step forecasting across the test set. CloudLSTM and its variants outperform other architectures, showing superior performance in urban scenarios. The study evaluates RNN-based models like CloudLSTM, CloudRNN, and CloudGRU for long-term air quality forecasting. CloudLSTM performs better than the other architectures in urban scenarios, showing reliable long-term forecasting performance up to 3 hours. The attention mechanism improves forecasting by capturing dependencies between input sequences and vectors in decoders. The study evaluates CloudLSTM models for long-term air quality forecasting, showing reliable performance up to 72 future time steps. Results demonstrate CloudLSTMs outperform state-of-the-art methods by up to 12.2% and 8.8% in terms of MAE and RMSE. CloudCNNs are superior to PointCNNs as feature extractors for point-cloud data. Overall, CloudLSTM models are effective for spatiotemporal data modeling across various tasks. The study introduces CloudLSTM models for long-term air quality forecasting, demonstrating superior performance up to 72 future time steps. Results show CloudLSTMs outperform state-of-the-art methods by up to 12.2% and 8.8% in MAE and RMSE. CloudCNNs are better than PointCNNs for feature extraction in point-cloud data. CloudLSTM, a dedicated neural model for spatiotemporal forecasting, utilizes the DConv operator for spatial feature learning while maintaining permutation invariance. The DConv operator enables spatial feature learning in CloudLSTM models for long-term air quality forecasting. It predicts values and coordinates of each point, adapting to changing spatial correlations. DConv can be combined with RNN models, Seq2seq learning, and attention mechanisms efficiently using a standard 2D convolution operator. The DConv operator reshapes the output map and applies the sigmoid function to enable translation into a standard convolution operation. The complexity of DConv is analyzed by separating the operation into finding neighboring sets and performing weighting computations. The complexity of each step is discussed separately, assuming both input and output channels are 1. The overall complexity is equivalent to a vanilla convolution operator. The DConv operator introduces extra complexity by searching for K nearest neighbors for each point, but this complexity does not increase much with higher dimensional point clouds. Normalizing the coordinates features enables transformation invariance with shifting and scaling. The proposed CloudLSTM is combined with an attention mechanism. The model is compared against baseline models like MLP and CNN. In this study, various baseline models such as MLP, CNN, 3D-CNN, LSTM, ConvLSTM, PredRNN++, CloudRNN, and CloudGRU were compared against the proposed DConv operator. The CloudLSTM with an attention mechanism was introduced and detailed configurations for each model were provided in Table 3. In this study, different models were compared against the proposed DConv operator. The CloudLSTM with an attention mechanism was introduced, and various architectures were optimized using the MSE loss function. Evaluation metrics such as MAE, RMSE, PSNR, and SSIM were used to assess model performance. The performance of models is evaluated using metrics like PSNR, VAR, and COV. Anonymized locations of antenna sets in cities are shown in Figure 5. Data collection involves traditional flow-level deep packet inspection at the packet gateway. Confidentiality constraints prevent disclosure of operator names, target regions, and detailed classifier operations. The dataset used for the study is fully anonymized and complies with privacy regulations. The set of services analyzed includes 38 different services, with streaming being the dominant type of traffic, accounting for almost half of the total traffic. The raw data cannot be made public due to a confidentiality agreement with the data owner. The air quality dataset from 43 cities in China, collected by Microsoft Research, contains 2,891,393 records from 437 monitoring stations over a year. Stations are divided into two clusters based on location. Missing data was filled through interpolation. Evaluation of forecasting accuracy for mobile services using Attention CloudLSTMs is presented in Fig. 8. The Attention CloudLSTMs show similar performance in predicting mobile service traffic in different cities. Services with higher traffic volume have higher prediction errors due to frequent fluctuations. MAE increases over time for all models in air quality forecasting. Larger K values in CloudLSTM improve robustness and slow down MAE growth. Hidden features of CloudLSTM are visualized for mobile traffic forecasting evaluation. The hidden features of CloudLSTM are visualized for mobile traffic forecasting, showing scatter distributions of the hidden state in H t for both CloudLSTM and Attention CloudLSTM. Examples of NO 2 forecasting in City Cluster A are generated by RNN-based models, offering a performance comparison from a visual perspective. The proposed architectures, including Attention CloudLSTMs, show better prediction and capture trends in point-cloud streams for high visual fidelity over time. The DConv method uses Sigmoid functions to regulate coordinate features, allowing points to adjust positions for accurate forecasting, even with outlier points. The model's effectiveness is demonstrated using density-based spatial clustering with noise (DBSCAN). In the air quality dataset, DBSCAN is used to identify outliers in city clusters. CloudLSTM shows the lowest prediction error compared to other models. CloudCNN with DConv operator performs best among CNN-based models. Experiments on outlier robustness involve creating a toy dataset with randomly placed outliers. In the air quality dataset, DBSCAN is used to identify outliers in city clusters. CloudLSTM shows low prediction error compared to other models, including CloudCNN with DConv operator. Experiments on outlier robustness involve creating a toy dataset with randomly placed outliers. The positions of weather stations are normalized, and CloudLSTM performs well forecasting over inliers and outliers, outperforming PointLSTM significantly. Comparisons with simple baselines using MLPs and LSTMs with different input forms are also conducted. Performing prediction using K nearest neighbors' data with varying values of K, our CloudLSTM outperforms MLPs and LSTMs. The number of neighbors affects the model's receptive field, with small K focusing on local dependencies and large K considering global correlations. Results show K does not significantly impact baseline performance, while our CloudLSTM, utilizing DConv kernels for local dependencies and time steps for global dependencies, excels. Seasonal information in mobile traffic data can enhance forecasting, but directly inputting data spanning multiple days is impractical. To efficiently capture seasonal information in mobile traffic data, the model concatenates 30-minute sequences with a sub-sampled 7-day window. This creates an input with a manageable length of 90 data points. Experimental results on a subset of mobile traffic data show that incorporating seasonal information improves forecasting performance by learning periodic patterns. However, this approach increases model complexity. Future work will focus on addressing this issue. Future work will focus on a more efficient way to fuse seasonal information, with a marginal increase in complexity."
}