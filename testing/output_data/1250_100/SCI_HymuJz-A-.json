{
    "title": "HymuJz-A-",
    "content": "The text discusses the limitations of modern machine vision algorithms in learning visual relations, as demonstrated through experiments that strain convolutional neural networks (CNNs). It also highlights the challenges faced by relational networks (RNs) in solving visual question answering problems. The importance of feedback mechanisms like working memory and attention in abstract visual reasoning is emphasized. The text discusses the limitations of modern machine vision algorithms in learning visual relations, as demonstrated through experiments that strain convolutional neural networks (CNNs). It highlights the challenges faced by relational networks (RNs) in solving visual question answering problems. The CNN struggles to recognize simple relations, even after training on millions of images. Modern computer vision algorithms, including CNNs and relational networks (RNs), struggle to learn the concept of \"sameness\" in visual tasks. Despite recent successes in visual question answering benchmarks, both CNNs and RNs face limitations in tasks like recognizing visual relations. This failure is evident even though animals across different species can easily recognize visual relations. Existing machine learning models struggle with visual reasoning tasks, as shown by previous studies on synthetic visual reasoning tests. Despite extensive training data, black-box classifiers and CNN architectures have failed to solve most problems. The inability of feedforward neural networks to solve visual-relation problems raises questions about the choice of hyperparameters or a systematic failure of these models. The limitations of CNNs and other visual reasoning networks on visual-relation tasks are highlighted in a systematic performance analysis. It is suggested that feedback mechanisms like working memory and attention are necessary for computer vision models to efficiently solve complex visual reasoning tasks. The study reveals a dichotomy between hard same-different problems and easy spatial-relation problems, showing that CNNs solve same-different tasks through rote memorization. The SVRT challenge consists of twenty-three binary classification problems where stimuli obey abstract rules, showcasing limitations of CNNs in solving same-different tasks through rote memorization. The study suggests looking to neuroscience and cognitive science for inspiration in designing visual reasoning architectures. High-throughput screening of CNNs with different hyper-parameters on twenty-three SVRT problems showed lower accuracies on same-different tasks compared to spatial-relation tasks. The CNNs tested had varying depths and receptive field sizes, with all networks using pooling kernels of size 3\u00d73 and ReLu activations. Training was done on 2 million examples split evenly into training and test sets for each problem, resulting in a total of n = 207 conditions. The study trained nine networks on twenty-three SVRT problems using an ADAM optimizer. The problems were sorted by accuracy, with red bars representing Same-Different (SD) problems and blue bars representing Spatial-Relation (SR) problems. CNNs performed much worse on SD problems, indicating that they struggle with tasks involving transformations. The study found that CNNs struggle with Same-Different (SD) tasks, while performing equally well on Spatial-Relation (SR) problems. Larger networks showed higher accuracy on SD tasks, and feedforward models performed poorly on visual-relation problems. The SVRT challenge has limitations in its sample selection. The SVRT challenge includes twenty-three problems that represent various visual relations, making direct comparisons difficult due to unique image structures and generation methods. For example, Problem 2 requires specific object configurations that conflict with other problems. The SVRT challenge includes twenty-three problems representing various visual relations, making direct comparisons difficult due to unique image structures and generation methods. The necessary object configuration conflicts with other problems, requiring different numbers of objects in a single image. Using simple closed curves as items in SVRT images makes it challenging to quantify and control image variability. The PSVRT challenge addresses these issues by constructing a new dataset with two idealized problems. In response to the challenges of the SVRT dataset, a new dataset called PSVRT was created with two simplified problems: Spatial Relations (SR) and Same-Different (SD). The image generator used square binary bit patterns to create gray-scale images, with parameters controlling item size, image size, and number of items. This allowed for the same image dataset to be used in both SR and SD problems by labeling images differently based on specific rules. The PSVRT dataset was created to address challenges in the SVRT dataset by simplifying problems into Spatial Relations (SR) and Same-Different (SD). Parameters control item size, image size, and number of items, allowing for the same dataset to be used in both SR and SD tasks. Images are generated with square binary bit patterns, and the number of possible images is quantified using specific rules. The PSVRT dataset simplifies problems into Spatial Relations (SR) and Same-Different (SD) tasks by controlling item size, image size, and number of items. Images are generated with unique items randomly placed in an n \u00d7 n image, ensuring 1 background pixel spacing between items. The difficulty of learning PSVRT problems was examined by training a baseline architecture for different parameter configurations and measuring the number of training examples required to reach 95% accuracy (TTA). The study focused on measuring problem difficulty using TTA. Different image parameters were varied to assess their impact on learnability. The baseline CNN was trained in various conditions with 20 million images, reporting the best-case result for each experimental condition. The study examined problem difficulty using TTA and varied image parameters to assess their impact on learnability. The baseline CNN was trained in different conditions with 20 million images, showing a strong dichotomy in learning curves with a sudden rise in accuracy from chance-level to 100%. The study analyzed problem difficulty using TTA and different image parameters to evaluate their effect on learnability. The training runs that showed a learning event almost always reached 95% accuracy within 20 million images, while those that did not reach 95% accuracy never exhibited a learning event. The final accuracy across experimental conditions displayed a strong bi-modality, with either chance-level or close to 100%. In SR, no straining effect was observed across image parameters. However, in SD, increasing image size and number of items led to a significant straining effect, making the learning event less likely. The network never learned the problem when there were 3 or more items in an image, even with a relaxation of the same-different rule. Increasing image size and number of items led to a significant straining effect on CNNs, reflecting how these parameters contribute to image variability. Increasing the number of items in an image leads to a quadratic-rate increase in image variability, while increasing the image size results in an exponential-rate increase in image variability. The straining effect on CNNs is equally strong regardless of network width, with a constant rightward shift in the TTA curve over image sizes. Increasing item size does not have a visible straining effect on CNNs. Learnability is preserved and stable over different item sizes. CNNs learning a PSVRT condition are building a feature set tailored for a specific dataset rather than learning general features. The Relational Network (RN) is an architecture designed to detect visual relations and is tested on various VQA tasks. It sits on top of a CNN, learning a map from pairs of high-level CNN feature vectors to answers for relational questions. This network is different from CNNs, which build a feature set specific to a dataset rather than learning general features. The Relational Network (RN) is trained to answer relational and non-relational questions on visual reasoning tasks, outperforming CNNs on tasks like \"sort-of-CLEVR\". The tasks involve scenes with simple 2D items, where the RN learns to compare attributes but lacks the concept of sameness and has low item variability, leading to rote memorization of item configurations. The Relational Network (RN) was trained on a two-item sort-of-CLEVR same-different task and PSVRT stimuli to measure its ability to transfer concepts. The model architecture included a convolutional network with four layers and a relational network with MLP layers. The system used ReLu activations and dropout in the penultimate layer. The CNN+RN architecture used ReLu activations, 50% dropout in the penultimate layer, softmax function in the final layer, and trained with cross-entropy loss using ADAM optimizer. The model was initialized with Xavier initialization and reproduced results from BID22 on the sort-of-CLEVR task. Twelve versions of the dataset were created, each missing one color+shape combination. The CNN+RN was trained to detect sameness of two scene items and did not generalize well to left-out color+shape combinations. The CNN+RN model does not generalize well to left-out color+shape combinations on the sort-of-CLEVR task. Despite rapid training accuracy improvement, the validation accuracy remains at chance, indicating a lack of transfer of same-different ability to the left-out condition. The model learns orders of magnitude faster than CNNs on PSVRT stimuli due to the limited number of color+shape combinations in each setup. The CNN+RN model behaves like a vanilla CNN in Experiment 2 of the sort-of-CLEVR task. Training on 20M images, the system achieves over 95% accuracy for image sizes of 120 or below, but fails to learn for sizes of 150 and 180. The representational capacity of the RN architecture seems to be the limiting factor, as visual-relation problems can exceed CNNs' capacity. Learning templates for object arrangements becomes intractable due to the combinatorial explosion in the number of templates needed. The limitations of feedforward networks in representing stimuli with a combinatorial structure have been acknowledged by cognitive scientists. Humans excel at detecting relations in visual systems, as shown by their ability to learn and generalize complicated visual rules with few training examples. In contrast, computer vision systems struggle with complex problems, such as problem 20 involving shapes and reflections. Visual reasoning abilities are not exclusive to humans, as birds and primates also exhibit similar capabilities. Ducklings demonstrated rapid learning of abstract concepts of same and different from a single example, contrasting with computer vision systems' struggles in transferring such concepts to novel objects. This highlights the visual reasoning abilities shared by humans, birds, and primates. The neural substrate of visual-relation detection may depend on reentrant/feedback signals beyond feedforward processes. Object localization in clutter requires attention, as spatial relations between objects in a scene also require attention, even when objects can be detected pre-attentively. The computational role of attention and working memory in detecting visual relations involves flexible representations constructed dynamically through attention shifts, rather than storing templates statically. This mechanism helps prevent capacity overload and combinatorial explosion, allowing humans to easily detect same objects or spatial relations. Humans have a superior ability to detect visual relations compared to modern computers. Exploring attentional and mnemonic mechanisms is crucial for computational understanding of visual reasoning."
}