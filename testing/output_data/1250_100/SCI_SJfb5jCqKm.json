{
    "title": "SJfb5jCqKm",
    "content": "We address uncertainty estimation in deep neural classification, highlighting biases in current methods due to training dynamics. Our algorithm selectively estimates uncertainty for confident points using earlier model snapshots, outperforming existing methods in experiments. This is crucial for applications like autonomous driving and medical diagnosis. The Bayesian framework offers a principled approach to infer uncertainties from a model, but there are computational hurdles in implementing it for deep neural networks. Presently, uncertainty estimation methods for deep learning rely on signals from standard networks trained in a standard manner, such as raw softmax response, entropy, signals from embedding layers, and the MC-dropout method. A recent study suggests that an ensemble of softmax response values from multiple networks performs better than other approaches. This paper introduces a method of confidence estimation that can enhance existing methods, including the ensemble approach. Our algorithm improves confidence score functions extracted from deep classifiers by addressing erroneous estimates caused by the training process. It focuses on ordinal ranking according to uncertainty and probability calibration, decoupling uncertainty estimation into two separate tasks. Our algorithm enhances confidence scoring functions from deep classifiers by improving uncertainty estimation through a selection mechanism that assigns early stopped models to boost confidence scoring functions for deep neural networks. This method requires an auxiliary training set and aims to address erroneous estimates caused by the training process. The proposed method improves uncertainty estimation for deep neural networks by training a selection mechanism. Extensive experiments show consistent improvements over baseline methods on four image datasets. Uncertainty is considered as negative confidence, and results are validated using calibrated uncertainty estimates. The text discusses the training process of a deep neural classification model using a softmax layer for multi-class classification. It introduces a confidence score function to quantify confidence in predicting class labels based on signals extracted from the model. In the domain of uncertainty estimation, there is no consensus on performance measurement for ordinal estimators. Different studies have used various metrics such as Brier score, negative-log-likelihood, and area under the ROC curve. A new unitless performance measure for confidence score functions is proposed in this section, drawing from existing approaches. The performance of \u03ba functions in selective classification is measured by the area under the risk-coverage curve (AURC), with a normalization called \"excess AURC\" (E-AURC) for meaningful comparisons. A selective classifier consists of a classifier (f) and a selection function (g). A selective classifier is a pair (f, g), where f is a classifier and g is a selection function that serves as a binary qualifier for f. The performance of a selective classifier is quantified using coverage and risk, which can be empirically evaluated over any finite labeled set. The overall performance profile of a family of selective classifiers can be measured using the risk-coverage curve (RC-curve) defined as the selective risk as a function of coverage. The performance of a selective classifier is defined by the area under the RC-curve (AURC) based on the threshold over \u03ba values. A better \u03ba leads to a smaller AURC, indicating a more effective classifier. The RC-curve is illustrated using a DNN trained on the CIFAR-100 dataset, with the softmax response confidence score \u03ba(x) = max i f (x) i. The curve shows the empirical selective risk of the classifier, increasing monotonically with coverage. The risk associated with different coverage levels for a selective classifier is shown in Figure 1 for the CIFAR100 dataset. The optimal in hindsight confidence score function, denoted by \u03ba*, yields the optimal risk coverage curve, with selective risk reaching zero at coverage rates below 1 \u2212 r(f|Vn). The AURC of a selective classifier is defined by the area under the RC-curve based on \u03ba values, indicating the effectiveness of the classifier. The AURC of a selective classifier is normalized by AURC(\u03ba*) to obtain a unitless performance measure. The Excess-AURC (E-AURC) is defined as E-AURC(\u03ba, f|Vn) = AURC(\u03ba, f|Vn) - AURC(\u03ba*, f|Vn), with the optimal \u03ba having E-AURC = 0. The focus is on non-Bayesian methods in deep neural classification, with the Monte-Carlo dropout (MC-dropout) technique proposed for uncertainty estimation in DNNs. The concept of uncertainty in DNNs is addressed through various methods such as MC-dropout and classification margin measurement. Recent advancements include using K-nearest-neighbors algorithm in the embedding space and ensemble-based uncertainty scoring for DNNs. These approaches aim to improve predictive performance by estimating neural network uncertainties using activations from non-final layers. Ensemble methods proposed by BID17 improve predictive performance by averaging softmax responses of trained DNN models. While state-of-the-art, it requires large computing resources. Other methods like BID14 and BID15 use snapshots during training or average weights across SGD iterations, but produce different results from the proposed technique. Ensemble methods like BID17 improve predictive performance by averaging softmax responses of trained DNN models. However, our method utilizes \"premature\" ensemble members, which differ in classification performance. We present an example with a deep classification model trained over epochs and monitored on a validation set. By analyzing softmax response values, we can make meaningful statements about confidence estimation for unseen test points. This example considers two groups of instances based on confidence assessment, highlighting the importance of softmax response evaluation. The softmax response values of green points stabilize early in training, indicating they can be predicted well by an intermediate model. This suggests that an intermediate model like f 130 can estimate the confidence of green points accurately. The E-AURC measure is used to evaluate the quality of confidence functions. The confidence estimation for green points improves and then degrades monotonically with intermediate classifiers, with f 130 being the best estimator. Surprisingly, the final model is one of the worst estimators for green points. In contrast, confidence estimates for red points improve monotonically with training, with the final model being the best estimator. This behavior is consistent across all datasets. The learning of uncertainty estimators for easy instances resembles overfitting, where higher confidence points degrade as training continues. An algorithm using early stopping is proposed to address this issue. The section presents two algorithms for uncertainty estimation in a pointwise fashion. The first algorithm improves a scoring function for a neural classifier, while the second algorithm is an approximation that does not require additional training examples. The Pointwise Early Stopping (PES) algorithm is introduced for confidence scores. The Pointwise Early Stopping (PES) algorithm operates by extracting the most uncertain points from a set V, determining the best model in F, and finding the best performing confidence score over a set S. This algorithm produces a partition of X based on confidence levels, iterating through layers until V is empty. The Pointwise Early Stopping (PES) algorithm determines the best confidence score for a given point x at test time by searching for the minimal i that satisfies a condition. The Averaged Early Stopping (AES) is a simplified version of PES that leverages the observation that \"easy\" points are learned earlier during training. It approximates the area under the learning curve by averaging evenly spaced points on the curve. The AES algorithm is used in experiments with confidence scores on image datasets like CIFAR-10 and CIFAR-100. Results are presented in Table 4, showing performance compared to baseline methods. The AES algorithm is applied to confidence scores on image datasets like CIFAR-10 and CIFAR-100, with results presented in Table 4. E-AURC values reflect the difficulty level of learning problems, with CIFAR-10 and SVHN considered relatively easy, CIFAR-100 harder, and Imagenet the hardest. The E-AURC measure proves useful in quantifying problem difficulty. The AES algorithm improves baseline methods in 39 out of 42 experiments, consistently reducing E-AURC values. The ensemble estimation approach of BID17 is currently the best among baselines, showing that AES enhances state-of-the-art performance. While computationally intensive, AES improves single-classifier methods like softmax response in CIFAR-10 by 6%. NN-distance in CIFAR-10 also benefits significantly from AES, aligning its performance with top methods. The AES algorithm improves baseline methods in 39 out of 42 experiments, consistently reducing E-AURC values. It enhances state-of-the-art performance, particularly in NN-distance in CIFAR-100, improving by 22% using AES. Probability scaling results align with raw uncertainty estimates, showing AES improves calibrated probabilities. The PES algorithm was implemented over the softmax response method for various datasets, with E-AURC values multiplied by 10^3 for clarity. The time complexity of applying PES over NN-distance and MC-dropout is discussed, with recommendations for parameters. The PES algorithm significantly reduced E-AURC on all datasets, with the best improvement seen on CIFAR-100. The AES version is simpler and scalable, generating confidence scores that outperform existing techniques. Further research is needed to enhance the algorithm's efficiency. The PES and AES algorithms improve estimation techniques on various datasets by utilizing snapshot models during training to overcome confidence score deformations. Future research could focus on developing a loss function to prevent confidence deformations while maintaining high classification performance. Additionally, incorporating distillation methods and early stopping criteria could reduce inference time and computational effort. The softmax response method and NN-distance method were implemented, but proposed extensions were not pursued to avoid degrading performance for uncertainty estimation. The MC-dropout method with p = 0.5 dropout rate and 100 feed-forward iterations is used for uncertainty estimation. Ensemble method averages softmax values from 5 DNNs. Platt scaling is applied using logistic regression on confidence measures for calibration. Performance is evaluated using negative log likelihood and Brier score. The table presents experiments of AES for softmax response and NN-distance with standard errors. E-AURC values on CIFAR-10, CIFAR-100, SVHN, and ImageNET for various k values are compared to the baseline method. The method divides the domain into \"easy points\" and \"hard points\" to recover uncertainty estimates. Overfitting is observed in all cases, motivating the strategy to extract information from early training stages. The experiments show overfitting in all cases, with MC-dropout showing less overfitting compared to NN-distance. The proposed correction strategy is potentially useful for all three cases. Figures 3 and 4 display E-AURC values for MC-dropout and NN-distance on CIFAR-100 for points with highest and lowest confidence."
}