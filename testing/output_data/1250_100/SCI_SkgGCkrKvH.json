{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, such as Choco-SGD, enables data privacy, on-device learning, and efficient scaling to large compute clusters. This approach achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. The algorithm's practical performance is demonstrated in training scenarios over decentralized user devices and in datacenters, showcasing computational scalability and data-locality benefits of distributed machine learning. Recent theoretical results show that decentralized training schemes can be as efficient as centralized approaches in terms of convergence of training loss. Gradient compression techniques have been proposed to reduce data sent over communication links. CHOCO-SGD, introduced for convex problems, overcomes restrictions on compression operators for decentralized training of deep neural networks. The focus is on generalization performance evaluation on the test-set. The algorithm focuses on generalization performance on test-set in machine learning benchmarks, departing from previous work. It studies training in peer-to-peer and datacenter settings, showing speed-ups and improved scalability with CHOCO-SGD. Decentralized algorithms face challenges in scaling to larger nodes. Decentralized training schemes encounter difficulties in reaching the same performance as centralized schemes. Reporting these results aims to spur further research on scalable decentralized training schemes. CHOCO-SGD converges at a rate matching centralized baselines on non-convex smooth functions, showing linear speedup with the number of workers. A version with momentum is analyzed for on-device training and datacenter scenarios, focusing on reducing bandwidth requirements and improving practical performance. Decentralized training schemes face challenges in achieving performance comparable to centralized approaches. Various methods, such as decentralized schemes, gradient compression, asynchronous methods, and multiple local SGD steps, have been proposed for training in communication-restricted settings. Combining decentralized SGD schemes is advocated for improved efficiency in deep learning model training. In this paper, decentralized SGD schemes are combined with gradient compression, focusing on gossip averaging approaches. The convergence rate depends on the spectral gap of the mixing matrix. Recent studies have shown convergence rates for related schemes, with quantization being popular in communication compression for deep learning. Theoretical guarantees have been established for schemes with unbiased and biased compression, including error correction methods. Recent studies have explored decentralized optimization with quantization, noting challenges with gossip averaging in the presence of quantization noise. Adaptive schemes have been proposed to balance compression accuracy and communication cost. For deep learning applications, algorithms like DCD and ECD have been introduced to converge at a similar rate as centralized approaches. The CHOCO-SGD algorithm is also discussed in this context. The CHOCO-SGD algorithm introduced by Koloskova et al. in 2019 can handle high compression ratios and has been analyzed for both convex and non-convex functions. It achieves higher test accuracy compared to DeepSqueeze in experiments. The algorithm is a decentralized optimization method for distributed optimization problems across multiple nodes with different local data distributions. Communication in a decentralized optimization setting is limited to local neighbors defined by a weighted graph. The weights are based on local node degrees to ensure positive connectivity. Messages are compressed using compression operators, allowing for a larger class of compression methods compared to traditional quantization operators. Algorithm CHOCO-SGD is a decentralized optimization method where each worker updates its private variable using stochastic gradient and gossip averaging steps. Communication is limited to local neighbors with compressed updates, preserving averages even with quantization noise. Communication and gradient computation can be parallelized, and publicly available copies of private variables are shared among neighbors. The decentralized optimization method CHOCO-SGD allows parallel execution of communication and gradient computation. It extends the analysis to non-convex problems and converges asymptotically with bounded stochastic gradients. The algorithm shows a linear speed-up compared to SGD on a single node, with compression and graph topology affecting higher order terms. Further details and experimental comparisons are provided in the text. The decentralized optimization method CHOCO-SGD enables parallel communication and gradient computation, showing linear speed-up compared to SGD on a single node. Experimental comparisons with compression operators and graph topologies are conducted, including momentum implementation in algorithms like DCD, ECD, and DeepSqueeze. The study compares CHOCO-SGD with momentum and standard mini-batch SGD with momentum, using compression schemes like quantization and sparsification. Hyper-parameters are fine-tuned, and compression is applied to every layer of ResNet20 separately. Top-1 test accuracy is evaluated on every node over the dataset, with average performance reported. The study compares CHOCO-SGD with momentum and standard mini-batch SGD with momentum, using compression schemes like quantization and sparsification. Results show that ECD and DCD only perform well with small compression ratios for unbiased schemes, sometimes diverging with high ratios. Performance of DCD with biased top a sparsification is also observed. In challenging real-world scenarios, decentralized training is essential due to limited communication bandwidth and unknown network topology. This setting is common in sensor networks, mobile devices, and hospitals where each device has access to local data. Fully decentralized methods are motivated by the inefficiency of centralized approaches in these scenarios. In challenging real-world scenarios, decentralized training is essential due to limited communication bandwidth and unknown network topology. This fully decentralized setting is strongly motivated by privacy aspects, enabling training data to remain private on each device. The training data is permanently split between nodes, with each node having a distinct part of the dataset. The study compares CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression for decentralized deep learning. The centralized baseline involves all nodes routing their updates to a central coordinator for aggregation. In studying the scaling properties of CHOCO-SGD, training was conducted on 4, 16, 36, and 64 nodes using decentralized algorithms on different topologies. Results showed that CHOCO-SGD's performance slowed down due to graph topology and communication compression, impacting training uniformly for all topologies. The degradation in performance was attributed to slower convergence rather than a generalization issue. In a real decentralized scenario, the focus is on reducing communication costs for users' mobile data. CHOCO-SGD outperforms other algorithms, with slight degradation as the number of nodes increases. Torus topology is beneficial for large networks due to good mixing properties, while smaller networks show little difference between topologies. Both Decentralized and Centralized SGD require a significantly larger number of bits to achieve reasonable accuracy. Training models on user devices in a Real Social Network Graph setting was simulated. Training models on user devices in a Real Social Network Graph setting was simulated using ResNet20 for image classification and a three-layer LSTM architecture for language modeling on WikiText-2. Results show that the decentralized algorithm performs best in training accuracy, followed by centralized, while the test accuracy is highest for the centralized scheme. CHOCO-SGD significantly outperforms the exact decentralized algorithm in terms of test accuracy for the same transmitted data. CHOCO-SGD outperforms exact decentralized scheme in test accuracy for the same transmitted data. It shows a slight accuracy drop compared to baselines but performs best in test perplexity for a fixed data volume. In large-scale training with Resnet-50 on ImageNet-1k, CHOCO-SGD benefits from scaling to more nodes in a datacenter setting. Decentralized optimization methods address scaling issues even with fast connections like InfiniBand or Ethernet. In large-scale training with Resnet-50 on ImageNet-1k, decentralized optimization methods like CHOCO-SGD benefit from scaling to more nodes in a datacenter setting. They address scaling issues even with fast connections like InfiniBand or Ethernet (10Gbps). Lian et al. (2017) and Assran et al. (2019) have shown that decentralized schemes can outperform centralized ones, with impressive speedups for training on multiple GPUs. Their algorithm includes asynchronous gossip updates, time-varying communication topology, and exact communication, making it not directly comparable to CHOCO-SGD. The decentralized optimization method CHOCO-SGD with a size of 128 on each GPU follows the general SGD training scheme. Results show that CHOCO-SGD outperforms the all-reduce method in terms of time and achieves a slightly lower accuracy. Despite hardware differences, CHOCO-SGD demonstrates a 20% time-wise gain over the common all-reduce baseline. The use of CHOCO-SGD is proposed for decentralized deep learning training in bandwidth-constrained environments, with theoretical convergence guarantees provided for the non-convex setting. The decentralized optimization method CHOCO-SGD with 128 nodes on each GPU shows improved performance over the all-reduce method in terms of time, achieving slightly lower accuracy. It offers a 20% time-wise gain in bandwidth-constrained environments, with theoretical convergence guarantees for non-convex settings. The algorithm demonstrates a linear speedup in the number of nodes and expands the reach of decentralized deep learning applications. The proof of CHOCO-SGD for arbitrary stepsizes \u03b7 is derived as a special case of a more general class of algorithms. Algorithm 1 consists of stochastic gradient updates and averaging among nodes, showing convergence as long as the averaging scheme exhibits linear convergence. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, with assumptions for the averaging scheme's convergence rate. The text discusses the convergence of algorithms for decentralized stochastic gradient descent. It mentions the exact averaging algorithm, D-PSGD algorithm, and CHOCO-SGD algorithm. The order of communication and gradient computation parts can be exchanged without affecting convergence rate. The proof of CHOCO-SGD for arbitrary stepsizes is derived from a more general class of algorithms. The Algorithm 3 iterates with constant stepsize \u03b7 satisfy Lemma A.1. The recursion verifies that rt \u2264 \u03b7^2 4Ac^2, completing the proof of Theorem A.2. Under Assumptions 1-3 with stepsize \u03b7 = nT+1 for T \u2265 64nL^2, Algorithm 3 converges. The convergence rate affects the second-order term, with a rate of O(1/\u221anT + n/(T\u03c1^2)). CHOCO-SGD with CHOCO-GOSSIP averaging converges at rate 2. The dependence on \u03c1 is worse than in the exact case, possibly due to high compression support. The proof of Theorem A.2 involves estimating terms using L-smoothness. Theorem A.4 provides guarantees for averaged parameter vectors in a decentralized setting, where averaging all parameters across machines is costly. Similar guarantees can be achieved on individual iterates. Corollary A.3 shows convergence of local weights under certain conditions, with a proof based on L-smoothness. Algorithm 3 converges with a constant stepsize and consensus stepsize, with a specific convergence speed. Theorem A.4 provides convergence guarantees for decentralized averaging of parameter vectors. Algorithm 3 converges at a specific speed, with a worse first term compared to Theorem A.2. Corollary A.5 shows convergence of local weights under certain conditions. Lemma B.1 and B.3 provide inequalities for vectors and matrices. Algorithm 4, CHOCO-SGD, involves stochastic gradient updates and error feedback. Algorithm 4, CHOCO-SGD, combines weight decay and momentum in a decentralized setting. It uses error feedback to correct quantization errors and update internal memory for model training and hyper-parameter tuning. The method achieves a balance between accuracy and compression level. The CHOCO-SGD algorithm balances accuracy and compression level by combining weight decay and momentum in a decentralized setting. It uses error feedback to correct quantization errors and update internal memory for model training and hyper-parameter tuning. The method is tested on ResNet20 and LSTM models for image classification and language modeling tasks, respectively. The CHOCO-SGD algorithm utilizes a linear scaling rule for node degree, applies momentum only to ResNet20 training, and gradually warms up the learning rate. The optimal learning rate per sample is determined by a linear scaling rule, and hyperparameters are fine-tuned for training ResNet-20 on Cifar10. The fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20/LSTM on a social network topology with 32 nodes are presented in Table 5. The training data is split between nodes with a fixed partition, no shuffling. The per node mini-batch size is 32, and the maximum node degree is 14. Learning curves and accuracy plots for the social network topology are shown in Figures 1, 6, 8, and 9. The local models in the optimization process initially diverge from the averaged model but reach consensus towards the end, with their test performances aligning. This behavior is consistent with findings from a previous study by Assran et al. (2019)."
}