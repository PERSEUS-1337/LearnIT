{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function shows strong convexity and smoothness near the ground truth, allowing gradient descent to converge linearly to a critical point close to the ground truth. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, with near-optimal sample and computational complexity. The success of deep neural networks in practical domains like computer vision and artificial intelligence is well-known, but the theoretical reasons behind this success are still not fully understood. Research has focused on understanding the classes of functions that can be represented by deep neural networks, the effectiveness of gradient descent for optimizing non-convex loss functions, and why these networks generalize well. One important area of research is model recovery, where the goal is to recover the underlying model parameter W based on training samples generated from a neural network model. This is crucial for the network to generalize effectively. Studies in this area have focused on two types of data generations, including regression problems where each sample is generated based on a weight vector and input data. The curr_chunk discusses the study of regression and classification problems in neural networks with Gaussian inputs. Previous research has focused on single-neuron models, multi-neuron networks, and two-layer feedforward networks with ReLU activations. The recovery of neural network parameters using gradient descent over squared loss has been explored, with statistical guarantees provided for model recovery using the squared loss. The curr_chunk discusses developing a strong statistical guarantee for the loss function in eq. (2) for neural networks with sigmoid activations. This study provides the first performance guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function. The curr_chunk provides a statistical guarantee for the loss function in eq. (2) for neural networks with sigmoid activations. It shows that the empirical risk function is uniformly strongly convex in a local neighborhood of the ground truth, leading to linear convergence of gradient descent with near-optimal sample complexity. The convergence rate to the ground truth is quantified in the Frobenius norm, and a tensor method is used for initialization. The curr_chunk discusses the proof for providing an initialization near the ground truth in neural networks with sigmoid activations. It introduces a new approach that considers the curvature of activation functions, including sigmoid and tanh, to analyze the cross-entropy loss function. The technique also extends to classification problems using squared loss. The focus is on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization, with relevance to parameter recovery in signal processing problems. The curr_chunk discusses various optimization problems in neural networks, such as matrix completion, phase retrieval, blind deconvolution, and tensor decomposition. It focuses on the statistical model for data generation, average-case performance, and the landscape analysis of one-hidden-layer network models. The work of Tian studied the landscape of the population squared loss surface with ReLU activations, revealing the presence of spurious bad local minima. Zhong et. al. provided characterizations for the local Hessian in regression problems with different activation functions. The curr_chunk discusses optimization problems in neural networks, focusing on the local Hessian for regression with various activation functions. It highlights findings on gradient descent convergence with ReLU activation and bounded derivatives. The study differs from previous work by analyzing the cross entropy loss function and exploring model recovery in multi-neuron cases. The paper discusses optimization problems in neural networks, focusing on local geometry and linear convergence of gradient descent. It introduces new findings on the Hessian for regression with different activation functions and cross entropy loss function in multi-neuron cases. The study is distinct from previous work and includes numerical examples and conclusions in subsequent sections. The paper introduces the generative model for training data and describes the gradient descent algorithm for learning network weights in neural networks. It focuses on the classification setting with a sigmoid activation function and one-hidden layer neural network model. The goal is to estimate W by minimizing the empirical risk. The paper introduces a generative model for training data and describes the gradient descent algorithm for learning network weights in neural networks. The goal is to estimate W by minimizing the empirical risk function using cross entropy loss. The algorithm uses a well-designed initialization scheme to avoid getting stuck at local minima. The update rule is given with a step size \u03b7. The algorithm uses the same set of training samples throughout, unlike other methods that resample at each iteration. The paper introduces an important quantity \u03c6(z) capturing geometric properties of the loss function. It characterizes local strong convexity of f n (W) near the ground truth W. The Hessian of f n (W) is guaranteed to be positive definite with high probability for the classification model with sigmoid activation function. The paper introduces \u03c6(z) to characterize local strong convexity of the loss function near the ground truth W. The Hessian of the empirical cross-entropy loss function is guaranteed to be positive definite in a neighborhood of W for the classification model with sigmoid activation function. Theorem 1 ensures this property as long as certain conditions are met, with sample complexity being near-optimal in d up to polynomial factors of K and log d. The paper introduces \u03c6(z) to characterize local strong convexity of the loss function near the ground truth W. The Hessian of the empirical cross-entropy loss function is guaranteed to be positive definite in a neighborhood of W for the classification model with sigmoid activation function. Theorem 2 guarantees the existence of a unique critical point W n in B(W , r) with linear convergence of gradient descent. The paper introduces \u03c6(z) to characterize local strong convexity of the loss function near the ground truth W. The Hessian of the empirical cross-entropy loss function is guaranteed to be positive definite in a neighborhood of W for the classification model with sigmoid activation function. Theorem 2 guarantees the existence of a unique critical point W n in B(W , r) with linear convergence of gradient descent. The initialization algorithm based on the tensor method from BID38 is briefly described, providing performance guarantees for recovering W consistently as n approaches infinity. The paper introduces \u03c6(z) to characterize local strong convexity of the loss function near the ground truth W. The initialization algorithm based on the tensor method involves estimating the direction of each column of W and approximating the magnitude and sign of w i. Assumptions are made regarding the activation function \u03c6(z) and the curvature around the ground truth for a larger class of activation functions. The paper introduces \u03c6(z) to characterize local strong convexity of the loss function near the ground truth W. Assumptions are made regarding the activation function \u03c6(z) and the curvature around the ground truth for a larger class of activation functions. The performance guarantee for the initialization algorithm is presented in Theorem 3, showing accurate estimation of the direction of W and the approximation of its norm. More details can be found in the supplementary materials. The paper introduces \u03c6(z) to characterize local strong convexity of the loss function near the ground truth W. It verifies the empirical risk function's strong convexity using gradient descent in a local region around W. Multiple random initializations show convergence to the same critical point Wn with the same training samples. Successful experiments have SDn \u2264 10\u22122. Gradient descent converges to the same local minima with high probability when sample complexity is sufficient. The statistical accuracy of the local minimizer for gradient descent is shown when initialized close to the ground truth. Average estimation error decreases with sample size in Monte Carlo simulations. Cross entropy loss outperforms squared loss in a classification problem for a one-hidden-layer neural network. The sample complexity for local strong convexity in a multi-neuron classification problem using cross entropy loss is characterized. Gradient descent converges linearly to the ground truth with high probability if initialized properly. Future work aims to extend the analysis to ReLU-like activations and convolutional neural networks. The population loss function is denoted as DISPLAYFORM0, and the proof of Theorem 1 involves showing smoothness and local strong convexity of the Hessian \u2207 2 f (W) in a neighborhood of W. The Hessian of the empirical loss function is shown to be close to the population loss function in a uniform sense, establishing local strong convexity and smoothness in a chosen radius around the ground truth for sigmoid activations. The proof of Theorem 1 combines Lemma 3 and Lemma 1 to show the strong convexity of the function in a chosen radius. The proof of Theorem 2 involves showing the gradient concentration and convergence to a critical point in the same radius. Lemma 4 establishes that the gradient uniformly concentrates around a critical point. For sigmoid activation function with certain conditions, there exists a unique critical point in a specific radius. This critical point is shown to be close to the estimate at each iteration, leading to local linear convergence of gradient descent. Gradient descent converges linearly to the local minimizer Wn, with the proof divided into two parts. Part (a) focuses on the accuracy of estimating the direction of W, while part (b) is based on a mild condition in Assumption 2 rather than the homogeneous condition for the activation function. The proof involves defining a tensor operation and utilizing a regression problem approach. For the regression problem, if the sample size n \u2265 dpoly (K, \u03ba, t, log d), then DISPLAYFORM1 holds with high probability. The proof involves bounding the estimation error of P 2 and R 3 using Bernstein inequality. In the classification problem, Bernstein inequality is applied to all neurons together, with differences in bounding the label y i and output y i. A different proof from BID38 is provided to estimate w i without the homogeneous condition on the activation function, based on a more relaxed condition in Assumption 2. The text discusses the estimation of parameters in a regression problem using a more relaxed condition in Assumption 2. It involves defining a quantity Q1, estimating w_i through an optimization problem, and obtaining an estimate of w_i via a specific equation. The text also mentions the correct estimation of s_i based on the sign of \u03b2_i and the existence of a constant \u03b4 for the inverse function of m3,1. The text introduces definitions and results related to the norms of random variables, specifically sub-gaussian and sub-exponential norms. It also discusses the calculations of the gradient and Hessian of a specific equation in the context of parameter estimation in regression problems. The text discusses the calculation of the Hessian block in parameter estimation, using the mean value theorem to derive equations and upper bounds for certain variables. It also presents a lemma related to the sigmoid activation function and provides inequalities for further analysis. The text presents upper and lower bounds for the Hessian of the population risk at ground truth, along with applying Lemma 1 to obtain a uniform bound in the neighborhood of W. It also discusses the proof of Lemma 3 by adapting analysis from a previous setting. The text introduces the covering number N of the Euclidean ball B(W, r) and a cover set W = {W1, ..., WN}. It discusses the minimization of distances between points in the cover set and a technical lemma for bounding terms in the proof."
}