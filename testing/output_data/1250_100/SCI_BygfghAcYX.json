{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is proposed for two layer ReLU networks, offering a tighter generalization bound. The capacity bound correlates with test error behavior with increasing network sizes and partly explains the improvement in generalization with over-parametrization. Additionally, a matching lower bound for Rademacher complexity is presented, surpassing previous capacity lower bounds for neural networks. Deep neural networks have been successful in various tasks, with over-parametrized networks being common in practice. Increasing the capacity of neural networks has been traditionally thought to lead to overfitting, but recent studies have shown that larger models can actually improve generalization error. This phenomenon has been observed across various architectures and hyper-parameter choices, with training on models with more hidden units resulting in decreased test error for image classification tasks. Complexity measures based on network parameters like VC bounds do not explain the generalization improvement seen with over-parametrization in neural networks. Different measures like norm, margin, and sharpness have been proposed to capture network capacity. Even when a network can perfectly fit training data, test error continues to decrease with larger networks. Unit capacity and unit impact are key factors in determining network complexity and output influence. The impact of hidden units on network output is crucial for network capacity. Empirical observations show that unit capacity and unit impact decrease faster than 1/ \u221a h with the number of hidden units. Existing complexity measures fail to explain why over-parametrization helps and increase with network size. In this study, the focus is on simplifying network architectures while preserving key properties. Two layer ReLU networks are chosen for analysis, showing similar behavior to more complex architectures. A tighter generalization bound is proven for these networks, with capacity decreasing as the number of hidden units increases. The complexity is characterized at a unit level, with measures shrinking faster than 1/ \u221a h as network size grows. The generalization bound depends on layer norms, which decrease with network size. The study focuses on simplifying network architectures while maintaining key properties. For two-layer ReLU networks, capacity decreases as the number of hidden units increases. The closeness of learned weights to initialization in over-parametrized settings suggests that optimization algorithms require less tuning as network size grows. Studies have evaluated PAC-Bayes measures and observed smaller Euclidean distances to initialization. In this paper, the authors empirically investigate the impact of over-parametrization on the generalization of neural networks. They show that existing complexity measures increase with the number of hidden units, but do not explain generalization behavior with over-parametrization. The authors also prove tighter generalization bounds for two-layer ReLU networks and propose a complexity measure that decreases with the increasing number of hidden units, potentially explaining the effect of over-parametrization on generalization. The authors provide a lower bound for the Rademacher complexity of two-layer ReLU networks with a scalar output, improving upon previous bounds. They consider fully connected ReLU networks for c-class classification tasks, defining the margin operator to measure prediction accuracy. The ramp loss is defined as the difference between the score of the correct label and the maximum score among other labels. It is bounded between 0 and 1, with the expected margin loss of a predictor being defined for any distribution and margin \u03b3 > 0. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels, increasing with the complexity of the class. The Rademacher complexity of neural networks is bounded to estimate generalization error, depending on the function class chosen. Choosing the right function class is crucial to capture trained networks accurately. Experiments on CIFAR-10 dataset show that while spectral and Frobenius norms initially decrease, they eventually increase with the number of hidden units. The Frobenius norm increases faster, but the distance Frobenius norm decreases compared to initialization. The Frobenius norm of weights in larger networks increases due to random initialization. The distance to initialization per unit decreases with increasing network size. The distribution of angles between learned weights and initial weights shifts from orthogonal to aligned in large networks. Unit capacity is defined as the per unit distance to initialization. In the second layer, both Frobenius norm and distance to initialization decrease with increasing network size. The Frobenius norm and distance to initialization decrease with network size, suggesting limited role of initialization. Outgoing weights from hidden units decrease faster than 1/ \u221a h. Unit impact, defined as \u03b1 i = v i 2, plays an important role in neural networks. Networks from real data have bounded unit capacity. The generalization behavior of networks learned from real data is influenced by bounded unit capacity and unit impact. Studying the generalization properties of the function class can provide a better understanding of these networks. A generalization bound for two-layer ReLU networks is proven by bounding the Rademacher complexity of the class F W in terms of the sum over hidden units' unit capacity and unit impact. This bound is derived by combining the Rademacher complexity with a specific equation. The proof involves decomposing the network complexity into that of hidden units, a novel technique different from previous works that decompose complexity based on layers and use the network's Lipschitz property to bound generalization error. The generalization behavior of two-layer ReLU networks is influenced by bounded unit capacity and unit impact. A tighter bound on the Rademacher complexity is derived by decomposing the complexity across hidden units. The generalization error is bounded for any function in the class defined by specific values of \u03b1 and \u03b2. The generalization bound holds for all networks by covering possible values of \u03b1 and \u03b2. The generalization error decreases with increasing width for networks learned in practice. An explicit lower bound for the Rademacher complexity is also provided. The Rademacher complexity is tightly bounded by decomposing it across hidden units, showing a decrease in capacity with over-parametrization. The generalization bound is extended to p norms, revealing a tradeoff between terms. Comparison with Golowich et al. (2018) shows similarities in the first term but differences in the key complexity term. The capacity of networks increases with h, as shown in experimental comparisons on CIFAR-10 and SVHN datasets. Larger networks exhibit better generalization even without regularization. Unit capacity and unit impact decrease with increasing h. The number of epochs needed to reach 0.01 cross-entropy loss decreases for larger networks. Generalization bounds scale with effective capacity C. The effective capacity of function classes typically scales as C/m, where C is the capacity. Our bound decreases with h and is consistently lower than other norm-based bounds. It even outperforms VC-dimension for networks larger than 1024. The numerical values may be loose, but they help understand generalization behavior relative to complexity measures. Our capacity bound is the only one that decreases with network size, even for networks with 100 million parameters. Other norm-based bounds initially decrease for smaller networks but then increase significantly for larger networks. Our capacity bound could indicate the properties that enable better generalization. Our capacity bound decreases with network size and may indicate properties that enable better generalization. The complexity measure correlates well with generalization behavior, as shown by comparing networks trained on real and random labels. A lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound. This lower bound extends to a bigger function class with an additional constraint on spectral norm. The complexity lower bound for neural networks matches the upper bound terms, indicating a tight upper bound in Theorem 1. This lower bound shows a gap between the Lipschitz constant of the network and its capacity, even with additional constraints on spectral norm. The lower bound for neural networks highlights a capacity gap between networks with ReLU activations and linear models. By setting weight matrices to the Identity matrix, a stronger lower bound is achieved. This result improves upon previous bounds and shows a factor of \u221ah improvement. Additionally, a lower bound for the composition of 1-Lipschitz loss function and neural networks is also provided. In this paper, a new capacity bound for neural networks is presented, which decreases with the increasing number of hidden units. The focus is on understanding the role of width in the generalization behavior of two-layer networks. The interplay between depth and width in controlling network capacity is also explored. Matching lower bounds for capacity are provided, improving on current lower bounds. The question of whether optimization algorithms converge to low complexity networks in the function class considered is not addressed. In this experiment, a pre-activation ResNet18 architecture was trained on the CIFAR-10 dataset with specific settings. The architecture includes a convolution layer, 8 residual blocks, and a linear layer. Different hyperparameter choices were not explored, leaving it for future work. In this experiment, fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets using specific settings. Data augmentation techniques were applied, and various architectures were tested with different sizes. Training was done using SGD with specific parameters, and evaluations were conducted to calculate generalization bounds. In this section, the generalization bounds for fully connected feedforward networks trained on CIFAR-10, SVHN, and MNIST datasets are discussed. The margin is set to the 5th percentile of the data points, and bounds from previous studies are adjusted for multi-class classification. Distributions are estimated using Gaussian kernel density estimation, and the behavior of different measures on networks of varying sizes is analyzed. The over-parametrization phenomenon in the MNIST dataset is observed, and a new theorem is generalized to p norm with the introduction of Lemma 11 for constructing a cover for the p ball with entry-wise dominance. Theorem 5 provides a bound on generalization error for fully connected feedforward networks trained on various datasets. It introduces a new theorem for constructing a cover for the p ball with entry-wise dominance, improving on previous bounds. Corollary 6 further bounds the generalization error for any function f(x) in the settings of Theorem 5. Lemma 7 relates the norm of a vector to the expected magnitude of its inner product with Rademacher random variables. The Rademacher complexity of fully connected feedforward networks can be decomposed to that of hidden units. The complexity is bounded by a certain inequality, with the ramp loss being Lipschitz with respect to each dimension. The Rademacher complexity of fully connected feedforward networks can be decomposed to that of hidden units, bounded by a certain inequality. Lemma 10 introduces the Ledoux-Talagrand contraction, used in the proof of Theorem 1. Lemma 11 presents a covering lemma for a p ball with dominating elements entry-wise. Lemma 14 provides specific results for the case p = 2 in relation to the generalization error of fully connected feedforward networks. It considers various parameters such as h, c, d, \u03b3, and \u03b4, along with the matrix U 0. The lemma establishes bounds on the generalization error based on these parameters. Lemma 14 provides bounds on the generalization error for the case p = 2 in fully connected feedforward networks. The proof involves upper bounding the generalization bound for p = 2 and a specific choice of \u00b5. The proof of Theorem 2 follows from Lemma 14. Additionally, Lemma 15 provides a generalization bound for any p \u2265 2, with extra constants and logarithmic factors compared to Lemma 14. The generalization error is bounded for fully connected feedforward networks. The proof involves upper bounding the generalization bound for a specific choice of \u00b5 and p. Theorems 3 and 5 follow from Lemmas 15 and 14, respectively. The dataset is divided into groups with n copies of different elements in a standard orthonormal basis. The orthonormal basis F(\u03be) is chosen to construct U(\u03be) as Diag(\u03b2) \u00d7 F(\u03be), ensuring that U(\u03be) 2 \u2264 max i \u03b2 i."
}