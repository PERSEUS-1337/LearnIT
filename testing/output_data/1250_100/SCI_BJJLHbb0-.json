{
    "title": "BJJLHbb0-",
    "content": "Unsupervised anomaly detection on multi- or high-dimensional data is crucial for machine learning research and industrial applications. The Deep Autoencoding Gaussian Mixture Model (DAGMM) combines a deep autoencoder with a Gaussian Mixture Model for anomaly detection. DAGMM optimizes both models simultaneously, improving reconstruction and density estimation in a low-dimensional space. Joint optimization in DAGMM balances autoencoding reconstruction, density estimation of latent representation, and regularization to escape local optima and reduce reconstruction errors without pre-training. Experimental results show DAGMM outperforms state-of-the-art anomaly detection techniques, achieving up to 14% improvement in F1 score. Anomaly detection is crucial in various fields like cybersecurity, complex system management, and medical care, with density estimation being at its core. Conducting robust anomaly detection on high-dimensional data remains challenging, especially without human supervision. Density estimation in the original feature space can be challenging due to the curse of dimensionality. Two-step approaches involving dimensionality reduction followed by density estimation are commonly used but may lead to suboptimal performance. Recent works have explored combining dimensionality reduction and density estimation using deep networks, but face limitations in preserving essential information or having sufficient modeling capacity. Deep Autoencoding Gaussian Mixture Model (DAGMM) is a deep learning framework proposed in this paper for unsupervised anomaly detection. It addresses challenges such as model capacity and training strategy issues. DAGMM preserves key information in a low-dimensional space, capturing anomalies that deviate in reduced dimensions and are harder to reconstruct compared to normal samples. DAGMM utilizes a compression network for dimensionality reduction and a Gaussian Mixture Model for density estimation in anomaly detection. It addresses challenges in model learning by combining these two aspects in a joint optimization approach. DAGMM utilizes an estimation network to predict mixture membership for each sample, enabling direct estimation of GMM parameters for evaluating sample energy/likelihood. By jointly minimizing reconstruction error and sample energy, DAGMM trains a dimensionality reduction component for density estimation. Unlike pre-training, DAGMM allows end-to-end training, benefiting from regularization to escape local optima in the compression network. DAGMM, with end-to-end training and regularization, outperforms baseline methods for anomaly detection. It shows superior performance with up to 14% improvement in F1 score. Existing methods for unsupervised anomaly detection can be categorized into three groups, including reconstruction-based methods like PCA and Robust PCA. DAGMM outperforms baseline methods for anomaly detection by considering both reconstruction error and density estimation in a low-dimensional space. This approach addresses the limitations of existing methods that only analyze anomalies based on reconstruction error. Recent works propose deep autoencoder based methods to jointly learn dimensionality reduction and clustering components for density estimation and anomaly detection. Traditional techniques use a two-step approach, where dimensionality reduction is conducted first, followed by clustering analysis, leading to potential loss of key information. The performance of state-of-the-art methods is limited by oversimplification. DAGMM addresses limitations of current clustering models by incorporating an estimation network to evaluate sample density in low-dimensional space. It allows for end-to-end training to adjust dimensionality reduction components and improve clustering analysis. One-class classification approaches, like one-class SVM, are also used for anomaly detection by learning a discriminative boundary around normal instances. DAGMM offers a novel approach to anomaly detection by estimating data density in a low-dimensional space through joint learning. Unlike traditional methods, DAGMM utilizes deep autoencoders for non-linear dimensionality reduction and GMM for density estimation. This method is focused on unsupervised settings and aims to extract useful features for anomaly detection. Deep Autoencoding Gaussian Mixture Model (DAGMM) combines a deep autoencoder with a Gaussian Mixture Model (GMM) for unsupervised anomaly detection. It consists of a compression network for dimensionality reduction and an estimation network for predicting likelihood/energy in the GMM framework. The compression network prepares low-dimensional representations from input samples and reconstruction error features, which are then fed to the estimation network for anomaly detection. The compression network in DAGMM utilizes a deep autoencoder to generate low-dimensional representations and reconstruction error features. These features are then used in the estimation network for anomaly detection using a Gaussian Mixture Model (GMM). The estimation network estimates GMM parameters and evaluates likelihood/energy for samples without alternating procedures. The estimation network in DAGMM utilizes a multi-layer neural network to predict mixture membership for samples and estimate parameters in GMM. Sample energy can be inferred using the estimated parameters, and anomalies can be detected by comparing sample energy to a pre-chosen threshold during testing. The objective function in DAGMM training includes components for reconstruction error, sample energy modeling, and avoiding singularity issues in covariance matrices. The compression network aims to minimize reconstruction error, while the estimation network maximizes the likelihood of observing input samples. To prevent trivial solutions, small values on the diagonal entries are penalized. In DAGMM, the estimation network predicts sample membership using deep neural networks for latent variable inference. The contribution of a sample's compressed representation to the energy function is bounded by the estimation network. This approach can be adapted into neural variational inference framework. In DAGMM, the estimation network predicts sample membership using deep neural networks for latent variable inference. By minimizing the negative evidence lower bound, the estimation network approximates the true posterior and tightens the bound of the energy function. Unlike neural variational inference, DAGMM explicitly employs the deep estimation network to parametrize a sample-dependent prior distribution, making it a powerful deep unsupervised version of adaptive mixture of experts combined with a deep autoencoder. DAGMM utilizes end-to-end training instead of pre-training for anomaly detection. The compression and estimation networks enhance each other's performance, with the deep autoencoder reducing reconstruction error and the estimation network making meaningful density estimations. Public benchmark datasets are used to compare pre-training and end-to-end training in DAGMM. In this section, DAGMM is demonstrated to be effective in unsupervised anomaly detection using benchmark datasets like KDDCUP, Thyroid, and Arrhythmia. Different datasets are treated with specific anomaly classes, such as \"attack\" in KDDCUP, hyperfunction in Thyroid, and combined smallest classes in Arrhythmia as anomalies. The anomaly class in datasets like KDDCUP is formed by specific classes, while the normal class includes the rest. Different methods like OC-SVM and DSEBM are used for anomaly detection, with DSEBM leveraging sample energy to detect anomalies. The DSEBM-r, DSEBM-e, and DSEBM-r BID29 share the same core technique for anomaly detection using reconstruction error. The DCN (Deep Clustering Network) BID26 is a clustering algorithm that uses k-means to regulate autoencoder performance. An adaptation of this technique for anomaly detection involves using the distance between a sample and its cluster center as the criterion. Variants of DAGMM, such as GMM-EN and PAE, demonstrate the importance of individual components in anomaly detection. The DAGMM variant is equivalent to a deep autoencoder and uses sample reconstruction error for anomaly detection. Other variants like E2E-AE, PAE-GMM-EM, PAE-GMM, and DAGMM-p adopt different approaches for training the compression network and criteria for anomaly detection. The DAGMM variant, equivalent to a deep autoencoder, uses sample reconstruction error for anomaly detection. DAGMM-p is a compromise between DAGMM and PAE-GMM, with pre-training the compression network and fine-tuning DAGMM. DAGMM-NVI adopts neural variational inference framework and uses relative Euclidean distance and cosine similarity for reconstruction features. The estimation network in DAGMM uses a GMM with 4 mixture components for best performance. The compression network for the dataset provides 3-dimensional input to the estimation network, with fully-connected layers and dropout. Training is done using TensorFlow and Adam algorithm with specific learning rates and epochs for different datasets. Lambda values are set to impact DAGMM. In DAGMM, lambda values impact the model's performance. Baseline methods are optimized for best performance through exhaustive search. Anomaly detection performance is evaluated using average precision, recall, and F1 score. DAGMM outperforms baseline methods in terms of F1 score on all datasets. DAGMM shows significant improvement in F1 score compared to existing methods on datasets like KDDCUP and KDDCUP-Rev. The curse of dimensionality limits OC-SVM's performance, while DAGMM outperforms DSEBM by jointly considering latent representation and reconstruction error in energy modeling. Pre-trained deep autoencoders may restrict the performance of DCN, PAE-GMM, and DAGMM-p. E2E-AE struggles with reducing reconstruction error on certain datasets due to potential loss of key information during dimensionality reduction. DAGMM exhibits improved performance on KDDCUP and Thyroid datasets compared to E2E-AE. The results of DAGMM and DAGMM-NVI are similar, with no significant enhancement from neural variational inference. An analysis of the energy function learned by DAGMM is provided in Appendix B. The model's response to contaminated training data is also explored, showing a decrease in detection accuracy as contamination ratio increases. In summary, DAGMM maintains good detection accuracy even with 5% contaminated data, while OC-SVM is more sensitive to contamination ratio. Training a model with high-quality data is crucial for better detection accuracy. DAGMM achieved state-of-the-art accuracy on benchmark datasets and offers a promising alternative for unsupervised anomaly detection. Figure 3 displays the low-dimensional representation learned by DAGMM, PAE, DAGMM-p, and DCN. Figure 3 illustrates the low-dimensional representation learned by DAGMM, PAE, DAGMM-p, and DCN on the KDDCUP dataset. DAGMM effectively separates anomalous samples from normal samples in the low-dimensional space, while anomalies overlap more with normal samples in the spaces learned by PAE, DAGMM-p, and DCN. Despite efforts to fine-tune pre-trained deep autoencoders, anomalies still mix with normal samples in the representations. Pre-trained deep autoencoders may be suboptimal for density estimation tasks. DAGMM achieves low reconstruction error similar to pre-trained deep autoencoders, making it challenging to reduce the error for deep models. The Deep Autoencoding Gaussian Mixture Model (DAGMM) shows promising results for density estimation and anomaly detection by combining dimensionality reduction and density estimation through end-to-end training. The compression and estimation networks enhance each other's performance, leading to better compression and robust density estimation. DAGMM effectively separates anomalous samples from normal ones in the low-dimensional space, unlike other models like PAE, DAGMM-p, and DCN. The Deep Autoencoding Gaussian Mixture Model (DAGMM) is proposed for unsupervised anomaly detection, consisting of compression and estimation networks for projecting samples into a low-dimensional space and evaluating sample energy under Gaussian Mixture Modeling. DAGMM allows end-to-end training, predicting sample mixture membership and escaping local optima for low reconstruction error. Compared to pre-training, end-to-end training benefits density estimation tasks, showing superior performance on benchmark datasets with up to 14% improvement. The study discusses state-of-the-art techniques for unsupervised anomaly detection, focusing on the use of OC-SVM and DSEBM models. OC-SVM requires setting parameter \u03bd during training, while DSEBM follows the network structure of DAGMM for encoding. The optimal \u03bd values are determined through exhaustive search, with specific values set for different datasets. The importance of reconstruction features is highlighted, especially in a private network security dataset. In a private network security dataset, deep autoencoders are used to reduce dimensions from 20 to 1 for analyzing network flows. Some anomalies have similar representations to normal samples in the reduced space, but their L2 reconstruction error helps in separating them. This observation motivates the inclusion of reconstruction features into DAGMM for anomaly detection. In a private network security dataset, deep autoencoders reduce dimensions for analyzing network flows. Anomalies with similar representations to normal samples are separated by their L2 reconstruction error. Guidelines for reconstruction feature selection include using error metrics that are continuous, differentiable, and output small values. Cosine similarity and relative Euclidean distance are selected based on these rules. As long as an error metric meets the criteria, it can serve as a candidate metric for DAGMM's reconstruction features. In a case study on the KDDCUP dataset, anomalies with low cosine similarity and high relative Euclidean distance are easily captured by both joint training in DAGMM and decoupled training in PAE-GMM. However, anomalies with medium relative Euclidean distance and high cosine similarity are more challenging for PAE-GMM to separate from normal samples. DAGMM tends to assign lower cosine similarity to such anomalies, making it easier to differentiate them from normal samples. The objective function of DAGMM includes components from deep autoencoder, estimation network, and penalty function. The coefficient ratio among the components in DAGMM can be characterized as 1:\u03bb1:\u03bb2. A large \u03bb1 value diminishes the role of the deep autoencoder in optimization, while a small value affects the estimation network. A large \u03bb2 value leads to GMM with large covariance, while a small value may not counter the singularity effect effectively. A ratio of 1:0.1:0.005 consistently delivers expected results in anomaly detection. Varying the base of the ratio affects anomaly detection accuracy. For example, setting the base to 2 adjusts \u03bb1 and \u03bb2 to 0.2 and 0.01 respectively. DAGMM's performance on the KDDCUP dataset remains consistent as the base varies from 1 to 9 with step 2. \u03bb1 and \u03bb2 are not sensitive to changes in the base."
}