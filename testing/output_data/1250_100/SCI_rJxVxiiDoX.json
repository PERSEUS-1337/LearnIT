{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. Additionally, combining quantization-aware training with weight matrix factorization significantly reduces model size and computation for small-footprint keyword spotting while maintaining performance. The training method involves optimizing weights against quantization errors using quantization-aware training. Dynamic quantization is used for DNN weight matrices, with inputs quantized row-wise on the fly. The keyword 'Alexa' is chosen for experiments using a 500 hrs far-field speech dataset. The training method involves optimizing weights against quantization errors using quantization-aware training. Evaluation of 70 models using DET curves and AUC. Training with GPU-based distributed DNN training method in 3 stages. Performance improvement observed with quantization-aware training. DET curves for different quantized models compared to full-precision model."
}