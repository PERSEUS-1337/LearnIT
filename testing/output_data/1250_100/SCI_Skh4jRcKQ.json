{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified by showing that the expected coarse gradient correlates positively with the population gradient, making its negation a descent direction for minimizing the population loss. Deep neural networks (DNN) have achieved success in various machine learning applications. Efforts have been made to train coarsely quantized DNN for memory savings and energy efficiency. Training fully quantized DNN involves minimizing population loss using a gradient descent algorithm. Poor choice of STE can lead to instability near local minima, as shown in CIFAR-10 experiments. Training fully quantized DNN involves minimizing a nonconvex empirical risk function subject to weight quantization constraints. The gradient in training activation quantized DNN is mostly zero, requiring a modified chain rule with the straight-through estimator (STE) for back-propagation. Another approach involves stochastic neurons for training. Bengio et al. (2013) proposed a stochastic neuron approach, while Friesen & Domingos (2017) introduced the target propagation algorithm for learning hard-threshold networks. The perceptron algorithm from the 1950s inspired the idea of STE for backpropagation in DNNs with binary activations. Hinton (2012) extended this concept to train multi-layer networks with binary neurons. Hubara et al. (2016) trained DNNs with weights and activations constrained to \u00b11. In training DNNs with binary activations, Hubara et al. (2016) used the saturated Straight-Through Estimator (STE) in the backward pass. This idea was later applied to DNNs with quantized ReLU activations by various researchers. However, there is limited theoretical understanding of using STE with stair-case activations. Goel et al. (2018) showed convergence using the Convertron algorithm with leaky ReLU activation. Wang et al. (2018) proposed an implicit weighted nonlocal Laplacian layer for improving DNN generalization accuracy. In the context of training DNNs with binary activations, the Straight-Through Estimator (STE) was used in the backward pass to improve generalization accuracy. Different approaches like the Convertron algorithm and nonlocal Laplacian layer have been proposed for this purpose. The backward pass differentiable approximation introduced by Athalye et al. successfully broke defenses relying on obfuscated gradients. The choice of STE is non-unique, and understanding its optimization perspective is crucial for training quantized ReLU nets. The derivatives of various activation functions and the Straight-Through Estimator (STE) are analyzed for training algorithms in DNNs with binary activations. Proper choices of STE lead to descent directions for minimizing population loss, with clipped ReLU STE showing promising results on deeper networks like VGG-11 and ResNet-20. Empirical performance on MNIST and CIFAR-10 datasets supports these findings. In CIFAR experiments, it was found that using identity or ReLU STE for training can lead to instability and lower generalization accuracy. The convergence guarantees of perceptron and Convertron algorithms were proven for identity STE, but these results do not apply to networks with two trainable layers. The choice of STE is crucial, as different STEs can affect gradient descent differently. Monotonicity of quantized activation functions plays a role in coarse gradient descent, but there are other factors at play as well. In CIFAR experiments, identity or ReLU STE can cause instability and reduced generalization accuracy. The choice of STE is critical as it impacts gradient descent. Different STEs affect training algorithms differently, with clipped ReLU matching quantized ReLU at extrema to avoid instability. The energy landscape of a two-linear-layer network with binary activation and Gaussian data is studied, with empirical performance comparisons of STEs in 2-bit and 4-bit activation quantization. The model described in the current chunk utilizes trainable weights in linear layers to make predictions based on input data. The activation function used is a binary function instead of ReLU. The learning task is framed as a population loss minimization problem. The input data is assumed to be sampled from a Gaussian distribution. The model uses trainable weights in linear layers with binary activation to minimize population loss. The Gaussian assumption allows for analytic expressions of the objective function and its gradient. The idea of STE is to replace zero components with a non-trivial function for training. Using STE \u00b5 in training a two-linear-layer CNN with binary activation leads to coarse gradient descent. The coarse gradient descent algorithm for learning a two-linear-layer CNN with binary activation is described. The population loss function is analyzed, showing that the only possible global minimizers are non-differentiable points. Stationary points, if they exist, are shown to be saddle points and potential spurious local minimizers. The coarse gradient descent algorithm for learning a two-linear-layer CNN with binary activation is discussed. The model has no saddle points or spurious local minimizers except for DISPLAYFORM4 and DISPLAYFORM5. The population gradient is proven to be Lipschitz continuous. The main focus is on the complex case where both saddle points and spurious local minimizers are present. The algorithm using the derivative of vanilla or clipped ReLU converges to a critical point, while the one with the identity function does not. The coarse gradient descent algorithm for learning a two-linear-layer CNN with binary activation is discussed. The algorithm converges to a critical point using the derivative of ReLU or clipped ReLU, but not with the identity function. When there are infinite training samples, the convergence guarantee is established. With more data, the empirical loss gains monotonicity and smoothness, explaining the success of STE in deep learning with large datasets. The mathematical analysis for the main results of the coarse gradient descent algorithm for learning a two-linear-layer CNN with binary activation is outlined. The key observation is that the coarse partial gradient has non-negative correlation with the population partial gradient, forming a descent direction for minimizing the population loss. If certain conditions are met, the inner product between the expected coarse and population gradients is discussed. The coarse gradient descent algorithm for learning a two-linear-layer CNN with binary activation is discussed. The significance of the estimate in guaranteeing the descent property of the algorithm is highlighted. When the algorithm converges, the energy decreases monotonically until convergence, and it can only converge to a critical point of the population loss function. The results for the algorithm using clipped ReLU are similar to those using ReLU. The coarse gradient using clipped ReLU generally correlates positively with the true gradient of the population loss. It vanishes only at critical points and converges when Algorithm 1 converges. However, the coarse gradient from the identity function does not vanish at local minima, potentially preventing Algorithm 1 from converging there. The coarse gradient using clipped ReLU correlates positively with the true gradient of the population loss, vanishing only at critical points and converging when Algorithm 1 converges. In contrast, the identity function's coarse gradient does not vanish at local minima, potentially hindering convergence. The empirical performances of vanilla and clipped ReLUs differ on deeper nets, as shown in comparisons on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations. In experiments, the 2-bit quantized ReLU and its clipped version are compared for performance. The training algorithm's instability with improper STE is discussed. The resolution \u03b1 for quantized ReLU is crucial and determined using a modified batch normalization layer. The \u03b1 is pre-computed using Lloyd's algorithm applied to simulated data and fixed during training. Batch normalization is added to each activation layer in LeNet-5. The quantization approach used is not claimed to be superior. The quantization approach used in the experiments is based on HWGQ, with uniform quantization. Stochastic gradient descent with momentum is used for optimization. Training is done for 50 epochs on LeNet-5 and 200 epochs on VGG-11 and ResNet-20. Parameters are initialized from pre-trained full-precision models. Results show that the derivative of clipped ReLU performs the best, followed by vanilla ReLU and then the identity function. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU is comparable on LeNet-5. The experiment used a two-linear-layer CNN and reported instability issues with the identity STE on ResNet-20. Training with the identity STE led to significantly worse results compared to using vanilla or clipped ReLUs. The algorithm was found to be unstable even with tiny learning rates, resulting in a much worse minimum. The coarse gradient with identity STE does not vanish at good minima, leading to poor performance on 2-bit activated ResNet-20. The instability of the training algorithm at good minima is illustrated in Figure 4. When initialized with weights from vanilla and clipped ReLUs on ResNet-20 with 4-bit activations, coarse gradient descent using identity STE is repelled. The first theoretical justification for STE is provided, showing that vanilla and clipped ReLUs generate descent directions for minimizing population loss, while identity STE does not. The instability of the training algorithm with identity STE was confirmed in CIFAR experiments. Future work aims to understand coarse gradient descent for large-scale optimization problems. In Lemma 11, Gaussian random vectors and angles are discussed, with proofs provided for various identities. Lemma 12 discusses Gaussian random vectors and angles, proving various identities. The angle between vectors w and w is denoted by \u03b8, with corresponding equations provided. Lemma 12 discusses Gaussian random vectors and angles, proving various identities related to vector angles and partial gradients. The text also touches on the local optimality of stationary points in the context of the objective function. The objective function is rewritten with scaling and constant terms, leading to saddle points at stationary points. Perturbed objective values are analyzed, showing that small non-zero changes result in increased objective values. Lemmas are used to prove the local optimality of stationary points. Combining inequalities validates the claim. Lemma 4 states the expected partial gradient w.r.t. v and w. Proof of Lemma 4 involves invoking Lemmas 11 and 5. Lemma 5 discusses the inner product between expected coarse and true gradients. The proof involves Lemmas 2 and 4, showing the relationship between w and w*. Lemma 6 shows the conditions for saddle points according to Proposition 1. Lemma 7 provides further conditions for saddle points and the inner product between expected gradients. The proof involves calculations based on previous Lemmas and inequalities. Lemma 8 is proven similarly to Lemma 6, with q(\u03b8, w) being non-negative and equaling 0 only at \u03b8 = 0, \u03c0, and p(0, w) \u2265 p(\u03b8, w) \u2265 p(\u03c0, w) = 0. Lemma 9 states that the expected coarse partial gradient w.r.t. w is \u00b5(x) = x. Lemma 10 shows that if w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0), then the inner product between the expected coarse and true gradients w.r.t. w is calculated."
}