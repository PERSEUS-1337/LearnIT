{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters and improved performance. This branched architecture, referred to as \"coupled ensembles\", involves tighter coupling of branches by averaging their log-probabilities. The approach is applicable to various neural network architectures and shows promising results on tasks like CIFAR-10, CIFAR-100, and SVHN. The design of early convolutional architectures involved choices of hyper-parameters like filter size and number of filters. State-of-the-art models like ResNet and DenseNet follow a template with fixed filter size and feature maps. Our work introduces \"coupled ensembling\" where the network is divided into branches, achieving improved performance. The proposed template introduces a branch-based CNN architecture with improved performance and reduced parameter count. It suggests splitting parameters among branches, combining activations with arithmetic mean, and ensembling for further enhancement. The paper discusses related work, introduces coupled ensembles, evaluates the approach, and concludes with future work. The proposed network architecture is similar to Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network. However, it differs in that it trains a single model composed of branches, has a fixed parameter budget, combines branch activations by their log-probabilities, and uses the same input for all branches. Multi-branch architectures have been successful in vision applications, with recent modifications proposed for further improvement. Our proposed modification of CNN architecture involves using a template \"element block\" replicated as parallel branches to form the final model. Unlike other modifications, we do not require additional epochs for convergence or depend on batch size. Our method involves rearranging a given architecture's parameters without introducing additional choices, leading to efficient parameter usage. Unlike ensembles of independent networks, our model consists of parallel branches trained jointly for improved performance. The proposed model consists of parallel branches trained jointly, similar to ResNet and ResNeXt. Arranging parameters into branches improves performance, and ensembling can further enhance results. Snapshot ensembles use checkpoints during training for efficiency, but increase model size and prediction time. The approach aims to maintain model size while improving performance or achieving the same performance with a smaller model. The proposed model comprises multiple branches, each using a different model architecture like DenseNet-BC and ResNet. These branches are combined using a fuse layer, with different operations explored in Section 4.4. The model outputs a score vector for classification tasks with a linear layer for the number of target classes. The fully connected layer, followed by a SoftMax layer, produces a probability distribution over target classes. Different network architectures for image classification have variations before the last FC layer. Fusion in ensemble models involves averaging individual predictions from separate instances. A \"super-network\" with parallel branches and an averaging layer is functionally equivalent to this ensemble approach. In our setup, the model consists of parallel branches producing score vectors for target categories. These vectors are fused through a \"fuse layer\" during training to make a single prediction, known as coupled ensembles. Three options are explored for combining score vectors during training and inference: FC average, LSM average, and LL average. This transformation involves multiple branches and combining their scores. The proposed architecture involves multiple branches producing score vectors for target categories, which are fused through a \"fuse layer\" to make a single prediction. This composite branched model shows improved performance with a lower parameter count in experiments on CIFAR-10, CIFAR-100, and SVHN datasets. The parameter vector W is the concatenation of element block parameter vectors, with all parameters contained in the element blocks. The input images for these datasets are of size 32x32 pixels, and hyperparameters are set according to the original descriptions of the element block used. The input images for CIFAR-10, CIFAR-100, and SVHN datasets are normalized by subtracting the mean image and dividing by the standard deviation. Data augmentation is used during training on CIFAR datasets, while no data augmentation is used for SVHN. Testing is done after normalizing the input in the same way as during training. Error rates are given in percentages and correspond to an average of the last 10 epochs. DenseNet-BC with a dropout ratio of 0.2 is used for training on SVHN. Execution times were measured using a single NVIDIA 1080Ti GPU with optimal micro-batch 2. Experiments on CIFAR-100 dataset are done with DenseNet-BC, depth L = 100, growth rate k = 12. The proposed branched architecture is compared with a single branch configuration as the baseline reference point. The proposed branched architecture outperforms an ensemble of independent models and a single branch model with similar parameters. The multi-branch configuration shows lower test error rates, indicating its effectiveness as the number of parameters increase. The proposed branched architecture is more efficient in terms of parameters compared to a large single branch or multiple independent models. The performance of the branched model with different \"fuse layer\" combinations is evaluated, showing lower error rates on the CIFAR-100 test set. The performance of models with different \"fuse layer\" operations for inference is evaluated, showing that the branched model with LSM fusion has comparable performance to a DenseNet-BC model with significantly more parameters. Coupled ensembles with LSM fusion result in lower error rates compared to training individual instances separately. The coupling of ensembles in training forces them to learn complementary features and better representations individually. Averaging log probabilities updates all branches consistently, providing a stronger gradient signal. Ensemble combinations outperform single branch networks, with a significant reduction in error rate when using multiple branches. However, training with Avg. FC does not perform well, as the FC average may be reached with unrelated instances. The study explores the impact of non-linearity in the SM layer on FC average distortion. Avg. FC training with prediction works well, outperforming Avg. SM prediction. The optimal number of branches for a given model parameter budget is investigated using DenseNet-BC on CIFAR-100. Results show varying performance based on configurations of branches, depth, and growth rate. In the study, the impact of non-linearity in the SM layer on FC average distortion is explored. DenseNet-BC on CIFAR-100 is used to investigate the optimal number of branches for a given model parameter budget. Results show that configurations of branches, depth, and growth rate significantly affect model performance. The optimal configuration is found to be e = 3, L = 70, k = 9, which reduces error rates compared to single branch models. The study explores the impact of non-linearity in the SM layer on FC average distortion using DenseNet-BC on CIFAR-100. Results show that configurations of branches, depth, and growth rate significantly affect model performance. The optimal configuration is found to be e = 3, L = 70, k = 9, reducing error rates compared to single branch models. Coupled ensembles are evaluated against existing models, showing robust performance with slight variations in parameters. The gain in performance comes with increased training and prediction times due to smaller values of k. Ensembles are coupled as a single global model, with further ensembling involving multiple models. Coupled ensembles with ResNet pre-act as element block and e = 2, 4 show better performance than single branch models. DenseNet-BC architecture is tested with 6 different network sizes. The trade-off between depth L and growth rate k is not critical for a given parameter budget. Experimentation with single-branch and multi-branch versions of the model is done, with varying numbers of branches. The coupled ensemble approach with DenseNet-BC models outperforms single branch models and achieves better performance than reported state-of-the-art implementations. The larger models of coupled DenseNet-BCs show lower error rates on CIFAR 10, CIFAR 100, and SVHN datasets. Comparisons with meta-learning scenarios are also presented in the supplementary material. The coupled ensemble approach with DenseNet-BC models outperforms single branch models and achieves better performance than reported state-of-the-art implementations. The hardware limitations restricted the network size to 25M parameters, leading to the use of classical ensembling with significant performance gains. Ensembling four large coupled ensemble models resulted in a significant improvement, surpassing all other ensemble-based implementations. The proposed approach involves replacing a single deep convolutional network with multiple \"element blocks\" connected via a \"fuse layer\" to improve performance. This method outperforms single branch models, achieving the best performance for a given parameter budget. The approach results in a small increase in training and prediction times but leads to significant performance improvements. The proposed approach replaces a single deep convolutional network with multiple \"element blocks\" connected via a \"fuse layer\" to improve performance. It achieves the best performance for a given parameter budget, with better individual \"element block\" performance. Training and prediction times increase due to sequential processing of branches, but this can be addressed by extending data parallelism to branches or spreading branches over multiple GPUs. Coupled ensembles show lower error rates on ImageNet compared to single branch models. The proposed approach involves using multiple \"element blocks\" connected via a \"fuse layer\" to enhance performance. The structure of the test and train versions of networks is illustrated in Figures 3 and 4, showcasing the placement of the averaging layer. Element blocks are reused from other groups for efficiency and meaningful comparisons. Each branch is defined by a parameter vector, with the global network defined by a concatenation of all parameter vectors. Training is done in a coupled mode for improved prediction accuracy. The overall network architecture involves using multiple \"element blocks\" connected via a \"fuse layer\" for enhanced performance. A dedicated script is used to split the parameter vector W into W e ones for training and prediction. The global hyper-parameters determine the train versus test mode, number of branches, and placement of the AVG layer. Larger models may require splitting data batches into \"micro-batches\" for training. The text discusses the use of \"micro-batches\" in training neural networks to optimize memory usage and processing speed. By accumulating gradients over micro-batches and averaging them, an almost equivalent gradient is obtained compared to processing data as a single batch. This approach allows for parameter updates using batch gradients while performing forward passes with micro-batches for optimal throughput. The memory requirement depends on network depth and mini-batch size, with the micro-batch \"trick\" used to adjust memory usage while maintaining a default mini-batch size. The multi-branch version does not require more memory if the branches' width is kept constant. Hyper-parameter search experiments indicated that reducing both width and depth was the best option. For \"full-size\" experiments, micro-batch sizes of 16 for single-branch and 8 for multi-branch versions were used within 11GB memory. Splitting the network over two GPU boards does not significantly increase speed or improve performance. The experiments conducted on branched coupled ensembles with different combinations of depth and growth rate showed stable performance. The best combination predicted for the test set was (L = 82, k = 8, e = 3), while (L = 70, k = 9, e = 3) also performed well. Comparison with model architectures recovered using meta learning techniques was done in TAB9. When comparing model architectures recovered through meta learning techniques, the reproducibility of experiments and statistical significance of performance differences are key issues. Variations in performance measures can stem from factors like framework used (Torch7 or PyTorch), random seed for network initialization, CuDNN non-determinism during training, fluctuations in batch normalization, and the choice of model instance from training epochs. The choice of model instance from training epochs can be the model obtained after the last epoch or the best performing model, determined by looking at test data. Despite factors like numerical determinism and Batch Norm moving average, there is still a dispersion in evaluation measures due to random initialization. This dispersion, though small, complicates method comparisons as differences below the standard deviation may not be significant. In experiments with different seeds or the same seed, variations in model performance were observed. The study focused on a DenseNet-BC model with specific parameters on CIFAR 100 dataset. Different configurations were tested using Torch7 and PyTorch, with varying seeds. Results were compared based on error rates at the last epoch or average of error rates over multiple epochs. The study aimed to quantify the impact of different factors on model performance. The study compared Torch7 and PyTorch implementations of a DenseNet-BC model on CIFAR 100 dataset with different seeds. Results showed no significant difference between implementations or seed usage. Standard deviation was slightly smaller when computed over the last 10 epochs compared to a single epoch. Reproducing exact results was not possible due to observed dispersion. The study compared Torch7 and PyTorch implementations of a DenseNet-BC model on CIFAR 100 dataset with different seeds. Results showed no significant difference between implementations or seed usage. Standard deviation was slightly smaller when computed over the last 10 epochs compared to a single epoch. Reproducing exact results was not possible due to observed dispersion. The mean of the measures computed on the 10 runs is significantly lower when the measure is taken at the best epoch than when they are computed either on the single last epoch or on the last 10 epochs. This is expected since the minimum is always below the average. In system comparison, bias is introduced for absolute performance estimation. Error rate at the last 10 iterations shows smaller standard deviation, making it preferred for single experiments. Using 10 or 25 last epochs does not impact learning. CIFAR experiments used average error rate from last 10 epochs for robustness. SVHN experiments used last 4 iterations due to smaller epochs. Results suggest using average error rate from last epochs for more robust and conservative outcomes. In this study, comparisons between single-branch and multi-branch architectures were made at a constant parameter budget, showing an advantage for multi-branch networks. However, multi-branch networks have longer training times. The study investigates if multi-branch architectures can still improve over single-branch ones at a constant training time budget. Different ways to reduce training time are explored, with results shown for three options on CIFAR 10 and 100 datasets. The study compares single-branch and multi-branch architectures at a constant parameter budget, showing an advantage for multi-branch networks. Different options are explored to reduce training time while maintaining performance levels. Results show that even with reduced parameter counts and training times, the multi-branch networks still outperform the single-branch baseline. In this section, single branch models and coupled ensembles are compared in a low training data scenario. Results show that coupled ensembles outperform single branch models for a fixed parameter budget. Preliminary experiments on ILSVRC2012 also support this finding. The study compares single branch models and coupled ensembles in a low training data scenario. Results indicate that the coupled ensemble approach with two branches significantly outperforms the baseline, even with a constant training time budget. Further experiments with full-sized images and increased data augmentation are planned for future updates."
}