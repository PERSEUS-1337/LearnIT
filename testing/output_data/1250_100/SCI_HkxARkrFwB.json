{
    "title": "HkxARkrFwB",
    "content": "Deep learning NLP models use word embeddings like word2vec or GloVe to represent words as continuous vectors, enabling easier integration with neural layers. Storing and accessing embedding vectors for all words requires significant space, leading to issues with limited GPU memory. To address this, word2ket and word2ketXS methods inspired by quantum computing efficiently store word embedding matrices, reducing space requirements by a hundred-fold or more without sacrificing accuracy in NLP tasks. Word embedding approaches like word2vec and GloVe use vectors of smaller dimensionality to represent words, allowing for efficient training on large text corpora. The embeddings are stored in a d x p matrix in GPU memory for access during training and inference. Vocabulary sizes can reach up to 10^6, with embedding dimensionality ranging from 300 to 1024 in current systems. The embedding matrix in word2vec and GloVe models ranges from p = 300 to p = 1024, becoming a significant part of the model's parameter space. Quantum bits, or qubits, are described by complex unit-norm vectors in a quantum register, allowing for entanglement and exponential state space dimensionality. Entanglement is a unique quantum phenomenon not seen in classical bits, where individual bit states can be listed separately. In this paper, the authors propose two methods, word2ket and word2ketXS, inspired by quantum computing, for efficiently storing word embedding matrices during NLP training and inference. These methods offer high space saving rates with minimal impact on the accuracy of NLP models. The tensor product space V \u2297 W is a separable Hilbert space constructed using ordered pairs v \u2297 w, where v \u2208 V and w \u2208 W. The inner product between v \u2297 w and v \u2297 w is defined as a product of individual inner products v \u2297 w, v \u2297 w = v, v w, w. The tensor product of two unit-norm vectors from V and W is a unit norm vector in V \u2297 W. The space is a collection of equivalence classes of pairs v \u2297 w, with vectors often referred to as tensors. The orthonormal basis sets {\u03c8 j } and {\u03c6 k } in V and W respectively form an orthonormal basis {\u03c8 j \u2297 \u03c6 k } jk in V \u2297 W. The tensor product space V \u2297 W is formed by pairs of vectors with coefficients equal to the products of coefficients in V and W. The dimensionality of V \u2297 W is the product of the dimensionalities of V and W. Tensor product spaces can be created by multiple applications of tensor product, with arbitrary bracketing. In Dirac notation, a vector u \u2208 C 2n is written as |u and called a ket. The tensor product space H = V \u2297 W exhibits properties of linearity and bilinearity. Expressing v \u2297 w + v \u2297 w as \u03c6 \u2297 \u03c8 for some \u03c6 \u2208 V, \u03c8 \u2208 W is not always possible. The tensor product space contains vectors of the form v \u2297 w and their linear combinations, some of which cannot be expressed as \u03c6 \u2297 \u03c8. Tensors with rank greater than one are called entangled. A word embedding model maps word identifiers into a p-dimensional real Hilbert space to capture semantic information from language corpus. In word2ket, word embeddings are represented as entangled tensors with a tensor of rank r and order n. The inner product calculation between word embeddings takes O(rq log q log p) time and O(1) additional space. The proposed word2ket embedding representation involves entangled tensors with a tensor of rank r and order n. It utilizes a balanced tensor product tree for parallel processing, reducing sequential processing time to O(log2n). The representation allows for gradients with respect to individual elements and can be seen as a sequence of O(log2n) linear layers. The word2ket representation involves a tree structure with linear layers and LayerNorm to address gradient issues. Linear operators A and B are defined, and their tensor product A \u2297 B is explained. In the finite-dimensional case, A \u2297 B is represented as a matrix composed of blocks. The text discusses representing word embeddings as linear operators using tensor product-based exponential compression, resulting in space efficiency. The word embedding matrix is transformed into a matrix representation of the linear operator, utilizing a balanced binary tree structure similar to word2ket. In order to achieve space efficiency, the text discusses using lazy tensors to avoid reconstructing the full embedding matrix each time a small number of rows is needed for downstream neural NLP tasks. The proposed space-efficient word embeddings were evaluated in text summarization, language translation, and question answering tasks, comparing their accuracy with regular embeddings. In text summarization experiments, the GIGAWORD dataset was used with an encoder-decoder sequence-to-sequence architecture. Different dimensionality values were explored, with word2ket achieving a 16-fold reduction in trainable parameters. Word2ketXS was even more space-efficient, offering a 34,000 fold reduction in parameters while maintaining similar scores. In German-English machine translation using the IWSLT2014 dataset, a drop of about 1 point on the BLEU scale was observed for a 100-fold reduction in parameter space. The Stanford Question Answering Dataset (SQuAD) was used with a 3-layer bidirectional LSTM model, achieving a test set F1 score. DrQA conducted experiments for 40 epochs using embeddings with a vocabulary size of 118,655 and dimensionality of 300. By increasing the tensor order in word2ketXS to four, they achieved a 1000-fold saving in parameter space with only a 0.5 point drop in F1 score. The computational overhead for word2ketXS embeddings resulted in increased training times, with the model using tensors of order 4 taking 9 hours to train. The experiments were conducted on a machine with a single NVIDIA Tesla V100 GPU card and 2 Intel Xeon Gold 6146 CPUs. The experiments showed significant reductions in memory usage for word embeddings in sequence-to-sequence models. Transformer models like BERT, GPT-2, RoBERTa, and Sparse Transformers require millions of parameters. During training, memory is also needed for storing activations in all layers. Various approaches have been proposed to reduce the memory requirements for word embeddings in models like BERT and GPT-2. These include dictionary learning, word embedding clustering, bit encoding, and quantization methods. Additionally, techniques such as pruning, sparsity, and low numerical precision have been used to compress models for low-memory inference and training. Fourier-based approximation methods have also been explored for matrix approximation. Word2ketXS achieves superior space saving rates compared to other methods like bit encoding and parameter sharing. While some methods offer higher saving rates, they are limited by vocabulary size and embedding dimensionality. Tensor product spaces have also been used for document embeddings in related work."
}