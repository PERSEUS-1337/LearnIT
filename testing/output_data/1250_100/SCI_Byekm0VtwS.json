{
    "title": "Byekm0VtwS",
    "content": "The uncertainty in intelligence is crucial for flexibility and creativity. Neuromorphic computing chips with uncertainty can mimic the brain, but current neural networks do not consider this, leading to lower performance on these chips. A proposed uncertainty adaptation training scheme improves neural network performance on uncertain chips, highlighting the importance of uncertainty in human thinking. The fuzziness and stochasticity in human thinking help the brain efficiently process information and make creative decisions. These characteristics are lacking in current artificial intelligence systems, like deep neural networks, which rely on fixed precision numbers for weights and activations. Researchers have found that using 8-bit integers can be sufficient for many applications. Methods like network quantization and Bayesian networks are being explored to address these limitations. Additionally, neuromorphic computing chips offer a hardware solution to incorporate uncertainty in AI systems. Neuromorphic computing chips, utilizing nanotechnology and crossbar structures, offer a hardware solution to address uncertainty in AI systems. The crossbar structure efficiently performs vector-matrix multiplication (VMM) using Ohm's law and Kirchhoff's law, with nanoscale nonvolatile memory (NVM) providing additional storage capability. This computing in memory (CIM) architecture alleviates memory bottlenecks in traditional von Neumann architecture, making neuromorphic computing chips more energy and area efficient. The chips are promising for AI applications due to their VMM capabilities and memory requirements, while also incorporating uncertainty as an intrinsic feature. The uncertainty in neuromorphic computing chips arises from fuzziness caused by analog to digital converters (ADCs) and stochasticity induced by NVM devices. The stochasticity of NVM devices is due to random particle movement, leading to varied conductance and different output currents even with the same voltage applied. This stochasticity is typically simulated as a non-ideal factor affecting chip performance. In this work, a training scheme utilizing the stochasticity of neuromorphic computing chips is proposed to improve performance. Different types of NVM devices exhibit varying levels of stochasticity due to intrinsic physical mechanisms. The Gaussian distribution is used to model device stochasticity, with the mean representing the conductance value of the stable state. The variance of the distribution corresponds to the mean according to experimental results. The conductance of devices in neuromorphic computing chips is modeled using a Gaussian distribution, with the mean representing the stable state conductance. The standard deviation is linearly correlated to the mean, and conductances below a minimum value are cut off. Device fuzziness is a result of the writing process, essential for AI applications. Target conductance for each device must be determined before writing. The mapping process determines target conductance for each device based on neural network weights. To achieve higher energy efficiency, lower conductances are preferred. However, accuracy is hindered by device stochasticity and circuit fuzziness. Conductance values are affected by device variability and measurement precision, requiring a model to describe the process. The fuzziness model using Gaussian distribution is applied to describe device uncertainty in the mapping process. Uncertainty can impact DNN performance, but can be mitigated with uncertainty adaptation training scheme (UATS). Stochasticity model is introduced in every feed forward process to guide neural networks in handling uncertainty. The stochasticity model is used in feed forward processes to handle uncertainty by replacing weights with random variables. The fuzziness model, applied during training, replaces weights with random variables to improve network performance under uncertainty. Additionally, the loss function calculation is adjusted to evaluate network performance in uncertain conditions. The loss function is calculated by averaging the output of multiple FF processes with the same input batch. The effect of uncertainty on MNIST dataset was evaluated using different models. Training and testing were done with varying levels of uncertainty, and the average test error and standard deviation were reported. Without using the UATS, uncertainty increases test errors for MLP and CNN models. CNN model (LeNet-5) performs best without uncertainty but is most affected by it. UATS significantly improves accuracies in fine-tuning and retraining experiments. Retraining results are mostly better than fine-tuning using UATS, achieving comparable results to the ideal case with low uncertainty levels. UATS also shows power on CIFAR-10 dataset. The study validated the effectiveness of UATS on CIFAR-10 dataset with a ResNet-44 DNN model. Results showed that UATS can achieve lower error rates than ideal cases with proper hyper-parameters. It acts as a regularization method, particularly beneficial for DNNs with more layers. The uncertainty in intelligent systems is crucial, and while Bayesian networks are useful, controlling weight distribution is challenging. UATS offers a convenient alternative without the need for additional circuits. Various distributions were explored to model device stochasticity. The study explored various distributions to model device stochasticity, including Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli distributions. Despite different behaviors, network performance using these distributions with the same mean and variance is similar due to the VMM transforming individual distributions into random parameters. Methods to reduce computation intensity of UATS include sampling weights for inputs or batches instead of every VMM, and using uncertainty model results instead of weights for faster simulation speed with similar outcomes."
}