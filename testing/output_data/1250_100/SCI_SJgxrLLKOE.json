{
    "title": "SJgxrLLKOE",
    "content": "Engineered proteins offer the potential to solve problems in biomedicine, energy, and materials science. Creating successful designs is challenging due to the complex coupling between protein sequence and 3D structure. Generative models for protein sequences conditioned on a graph-structured design target can efficiently capture complex dependencies. This approach improves upon prior models and advances rapid biomolecular design with deep generative models. The goal of computational protein design is to automate the invention of proteins with specific structural and functional properties, reducing the need for trial-and-error. The current practice of protein design often involves multiple trial-and-error rounds, with initial designs frequently failing. An alternative top-down framework for protein design involves learning a generative model for protein sequences based on a target structure represented as a graph. This model combines autoregressive self-attention with graph-based 3D structure descriptions to capture higher-order dependencies between sequence and structure efficiently. The benefits of leveraging sparse and localized graph and self-attention in 3D space for computational efficiency, inductive bias, and representational flexibility in protein science. The approach shows improved generalization performance in structural generalization for protein sequences outside the training set. Generative models for protein engineering have been explored using neural networks. Our model captures the joint distribution of the full protein sequence by considering long-range interactions arising from the structure. Previous works have focused on predicting amino acids independently or using deep neural networks to model conditional distributions. Other approaches include generative models conditioned on fold topology and deep generative models for specific protein families. Unconditional protein language models have also shown promising results. Several groups have achieved promising results using unconditional protein language models to learn protein sequence representations that can transfer well to supervised tasks. Conditional generative modeling can facilitate adaptation to specific parts of structure space. Evaluating protein language models with structure-based splitting of sequence data may reveal challenges for unconditional language models in assigning high likelihoods to sequences from out-of-training folds. Deep models of protein structure have been proposed for crafting 3D structures for input to sequence design. For classical approaches to computational protein design, which joint model structure and sequence, refer to a review. Our model extends the Transformer to capture sparse, pairwise relational information between sequence elements, avoiding high memory and computational costs. It incorporates graph-structured self-attention and can be likened to graph neural networks, with edge features and an autoregressive decoder. The protein structure is represented as an attributed graph with node and edge features. The graph representation of coordinates should be invariant to rotations and translations while containing sufficient information to reconstruct adjacent coordinates. This approach addresses limitations of current graph neural networks. The protein structure is represented as an attributed graph with invariant and locally informative features derived from augmenting points with 'orientations'. Spatial edge features are derived from rigid body transformations for distance, direction, and orientation. The text discusses how positional embeddings are obtained in a graph representation of protein structure. It mentions using a sinusoidal function to encode the positioning of neighbors relative to a node. Additionally, it describes obtaining an aggregate edge encoding vector by concatenating structural and positional encodings. The model only includes edges in the k-nearest neighbors graph with k=30. The Structured Transformer model incorporates positional encodings and binary edge features to represent protein structure in a graph. It uses k-nearest neighbors in 3D space to reduce memory and computation costs. The Structured Transformer model extends the standard Transformer by incorporating edge features to capture spatial and positional dependencies in attention. It utilizes an autoregressive decomposition to predict amino acid sequences based on input structure and preceding amino acids. The encoder module implements multi-head self-attention to refine node embeddings from structure-based features. The Structured Transformer model extends the standard Transformer by incorporating edge features to capture spatial and positional dependencies in attention. It utilizes an autoregressive decomposition to predict amino acid sequences based on input structure and preceding amino acids. The encoder module implements multi-head self-attention to refine node embeddings from structure-based features. The decoder module has the same structure as the encoder but with augmented relational information allowing access to preceding sequence elements in a causally consistent manner. The Structured Transformer model incorporates edge features to capture spatial and positional dependencies in attention. It uses autoregressive decomposition to predict amino acid sequences based on input structure and preceding amino acids. The model includes three layers of self-attention and position-wise feedforward modules for the encoder and decoder with a hidden dimension of 128. The dataset used for evaluation is based on the CATH hierarchical classification of protein structure, with chains split into training, validation, and test sets at an 80/10/10 split. The dataset was split into training, validation, and test sets with 18025 chains in training, 1637 in validation, and 1911 in test. Models were trained with specific parameters and evaluated based on perplexity per letter of test protein folds. Protein sequences were evaluated for likelihood-based performance. The average perplexity per letter of protein sequences in the Pfam database was found to be around 11.6, indicating potential usefulness for functional protein sequences. However, there was a significant gap between unconditional language models and models conditioned on structure, with test perplexities barely better than null letter frequencies. Protein language models trained on one subset of 3D folds generalized poorly to predict sequences of unseen folds. The Structured Transformer model achieved a perplexity of \u223c7 on the full test set by considering local orientation information in protein structure. Comparisons with the SPIN2 method showed that the Structured Transformer model significantly improved perplexities. The model was evaluated on subsets of the test set, showing better performance than SPIN2, a computationally intensive method. Our new deep generative model enhances traditional Transformers with 3D structural encodings for designing protein sequences efficiently. It outperforms state-of-the-art models on unseen data, suggesting the potential for structurally-guided protein sequence engineering. The importance of modeling sparse long-range dependencies in biological sequences is highlighted."
}