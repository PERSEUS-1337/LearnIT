{
    "title": "BkiIkBJ0b",
    "content": "Deep reinforcement learning (DRL) algorithms have shown progress in learning to navigate challenging environments, but do not memorize maps at the testing stage. The NavA3C-D1-D2-L algorithm can choose shorter paths when trained and tested on the same maps. The algorithm can choose shorter paths on familiar maps but uses a wall-following strategy on unseen maps. Navigation in unstructured environments remains a challenge, with recent advancements in end-to-end navigation methods gaining traction. Recent advancements in end-to-end navigation methods, like BID10, BID6, BID7, and BID12, simplify decision-making in mapping steps. These methods show promise in efficiently finding goals in complex environments using monocular first-person views. However, the black-box nature of DRL-based navigation poses challenges in understanding their limitations. It is crucial to analyze these methods across various experiments to comprehend their strengths and weaknesses. In this work, the authors thoroughly explore and analyze state-of-the-art BID7 methods in end-to-end navigation. They set up an environment with a randomly generated map, an agent, and a goal. The agent is provided with only a first-person view and must find the goal as many times as possible within a fixed time, respawning each time it reaches the goal. The algorithms are trained and evaluated with increasing difficulty levels, starting with a static goal, spawn, and map setup. The authors experiment with BID7 methods in end-to-end navigation, starting with a static setup and gradually increasing difficulty by randomizing spawn and goal locations. They question the repeatability of results on different maps and evaluate DRL-based algorithms on unseen map structures. The experiments show no evidence of mapping in cases where algorithms are tested. The experiments show no evidence of mapping in cases where algorithms are evaluated on unseen maps and no evidence of optimal path planning, even when the map is constant and only the goal is randomized. Models discard most of the image information, focusing attention on a small band in the middle of the image except around junctions. In the field of robotics, there has been a significant advancement in mapping algorithms over the past three decades, with various approaches such as topological maps and occupancy grid maps. End-to-end navigation algorithms are gaining attention for their ability to optimize map storage based on the navigation task. Deep reinforcement learning (DRL) has been successfully applied to train agents for tasks like Atari games and end-to-end navigation. Agents are commonly trained and tested on the same maps with variations in spawn points and goal locations. In robotics, mapping algorithms have advanced significantly in the past three decades. Deep reinforcement learning (DRL) has been used to train agents for tasks like Atari games and navigation. Agents are typically trained and tested on maps with variations in spawn points and goal locations. Different studies have tested algorithms on random unseen maps, with agents trained to choose between potential goal locations based on past observations. Some agents are texture invariant but train and test on maps with the same geometric structure. The question of whether DRL-based agents remember enough information to obviate mapping algorithms is being addressed in a comprehensive set of experiments. The problem of navigation in robotics is formulated as an interaction between an agent and an environment, modeled as a Partially Observable Markov Decision Process (POMDP). The POMDP is defined by observation space, state space, action space, transition function, and reward function. The observation space consists of an encoded feature vector generated from input image, previous action, and reward. The action space includes four actions: rotate left, rotate right, move forward, and move backward. The reward function is defined for each experiment. The DRL algorithms model the state space as a vector of floats and use a combined transition function to estimate the next state. Policy-based DRL involves modeling a policy function and a value function, sharing parameters to maximize expected future reward. The objective is to estimate unknown weights that maximize future reward, expressed as a formula. Asynchronous Advantage Actor-Critic is a policy-based method used in this work. In this work, the Asynchronous Advantage Actor-Critic (A3C) method is utilized, allowing weight updates to occur asynchronously in a multi-threaded environment. The architecture includes inputs of the current image, previous action, and previous reward, with auxiliary outputs of loop-closure signal and predicted depth. Gradients are accumulated in each thread and applied to a shared copy of target weights periodically. The weight update is based on the product of advantage and characteristic eligibility. The NavA3C+D 1 D 2 L architecture, proposed by BID7, enhances the A3C algorithm by incorporating two LSTMs and auxiliary outputs for depth and loop-closure predictions. It uses inputs of current image, previous action, and reward, optimizing predictions for the auxiliary outputs. The algorithm is evaluated in a simulated environment using Deepmind Lab game engine. The game setup involves an agent navigating a maze to reach a goal, respawning in the same maze upon success. Smaller apple rewards are scattered in the maze for exploration, with the goal having a higher reward. A wall penalty discourages the agent from hugging walls, and a 4-action space is used for faster training. Ten random mazes are chosen for evaluation out of 1100 generated maps. The study involves generating 1100 random maps for maze navigation experiments. Ten maps are selected for static-map experiments, while agents are trained on subsets of maps and tested on unseen ones. The NavA3C+D 1 D 2 L algorithm is evaluated on maps with 5 difficulty stages, showing better performance on easier stages but not on the hardest one. The experiments propose a 5-stage benchmark for navigation algorithms. In maze navigation experiments, different setups are used such as random goal with static spawn and map, random goal with random spawn and static map. BID7 addresses the challenge of localizing the agent within the map and exploiting map information with limited success. In maze navigation experiments, different setups are used such as random goal with static spawn and map, random goal with random spawn and static map. The current study evaluates algorithms on unseen maps, training agents on multiple maps and testing them on 100 unseen maps. The evaluation is based on rewards, Latency 1 :> 1, and Distance-inefficiency metrics. The NavA3C+D1D2L Mirowski et al. FORMULA0 algorithm is tested on ten randomly chosen maps of increasing difficulty. The study evaluates algorithms on unseen maps by training agents on multiple maps and testing them on 100 unseen maps. Results show that with static goals, rewards are higher and agents find the shortest path efficiently. However, with random goals, agents struggle to find the shortest path and fail to generalize to new maps. The Distance-inefficiency metric measures the ratio of total distance traveled by the agent versus the approximate shortest path. The Distance-inefficiency metric measures the length of the path with respect to the shortest possible path. Results from experiments on ten randomly chosen maps show that for static goals, the reward is consistently high and the path chosen is the shortest available. The Distance-inefficiency metric measures path length compared to the shortest path. Experiments show that for random goals, the agent's performance is inconsistent, with paths not always being the shortest. The Latency 1 :> 1 on 10 chosen maps suggests map-exploitation. Large Distance-inefficiency numbers confirm this. Results show agents randomly explore mazes instead of using shortest paths. Training on 10 to 100 maps improves reward and goal hits significantly, but no further increase with 500 to 1000 maps. Wall-following strategy learned with variation in 100 maps. Evaluation on maps with apples and texture shows varying effects. The algorithm is trained on random textures and apple placements, showing robustness. Evaluation on simple maps reveals the agent often moves towards initialization direction. The agent only takes the shortest path 50.4% of the time, similar to random behavior. Evaluation on a Wrench map aims to eliminate orientation dependency. The algorithm is trained on random textures and apple placements, showing robustness. Evaluation on simple maps reveals the agent often moves towards initialization direction. On the Wrench map, the shortest path is independent of spawn orientation, with the agent taking the shortest path only 32.9% of the time. The goal map penalizes wrong decisions more than the Wrench map, with the agent choosing the correct path 42.6% of the time. Distance-inefficiency experiments show that the effect of apples or textures can be ignored, with a clear trend in favor of random textures without apples in exploiting goal location. The NavA3C+D 1 D 2 L algorithm, trained on 1000 maps, struggles to generalize to simple maps, often opting for a wall-following strategy instead of planning based on goal location. Evaluation examples on Square, Wrench, and Goal maps show varying success rates in choosing the shortest path, with percentages ranging from 32.9% to 50.4% over 100 episodes. The attention in the image is visualized using the normalized sum of absolute gradient of the loss with respect to the input image. It is observed to be uniformly distributed when the agent spawns, narrows down in the corridor, and spreads around turns and junctions. The attention also focuses on important objects like the goal and apples. Visualizing attention for two sequences, one trained and evaluated on the same map, and another trained on 1000 maps and evaluated on one map. The algorithm narrows down in the center while navigating through the corridor and spreads around turns and junctions. It pays attention to important objects like the goal and apples. The study evaluates DRL-based navigation algorithms through experiments on randomly chosen maps, showing that they struggle with path-planning and mapping in unseen environments. The results suggest that these algorithms do not truly \"learn to navigate\" but rather memorize specific environments. The study evaluates DRL-based navigation algorithms through experiments on randomly chosen maps in the Deepmind Lab environment. Apple rewards and goals provide +1 and +10 rewards, respectively, with a wall penalty of up to -.2 per frame. Episodes last 40 seconds with 30 frames per second interaction. The mazes are 900units\u00d7900units, and A3C implementation uses RGB images of 84 \u00d7 84 \u00d7 3 dimensions with 16 threaded agents. Learning rate is 10 \u22124 with AdamOptimizer for training. The study evaluates DRL-based navigation algorithms in the Deepmind Lab environment using a learning rate of 10^-4 with the AdamOptimizer. Trained models will be released online for comprehensive experimental evaluations, including metric scores on various environments. The work aims to facilitate the development of generalized DRL navigation methods without the need for extensive infrastructure engineering. All work will be available on github post blind-review process."
}