{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are popular for their ease of training, scalability, and lack of a lexicon requirement. Contextual acoustic word embeddings are constructed from a supervised sequence-to-sequence model, showing competitive performance on standard sentence evaluation tasks and spoken language understanding. Learning fixed-size representations for variable length data is a key focus in text and speech-based applications. Learning word representations from variable length acoustic frames involves challenges due to the variability in speakers, acoustics, and microphones. Prior techniques either align speech and text by providing word boundaries or segment input speech into fixed-length segments. However, these methods do not capture the specific audio context associated with each word, leading to a lack of contextual dependencies in resulting word embeddings. Our work focuses on constructing individual acoustic word embeddings from utterance-level acoustics using an attention-based sequence-to-sequence model for direct Acoustic-to-Word (A2W) speech recognition. By automatically segmenting and classifying input speech into individual words, we eliminate the need for pre-defined word boundaries. Our approach allows us to learn acoustic word embeddings in the proper context of their containing sentence, which proves useful in non-transcription downstream tasks. Our work focuses on constructing individual acoustic word embeddings from utterance-level acoustics using an attention-based sequence-to-sequence model for direct Acoustic-to-Word (A2W) speech recognition. The methods to construct word representations (CAWE) directly from a speech recognition model are competitive with text-based word2vec embeddings. The utility of CAWE is demonstrated on a speech-based downstream task of Spoken Language Understanding, showing the potential for transfer learning. A2W modeling has been pursued using CTC and S2S models, with progress showing the possibility of training these models with smaller amounts of data. The solutions for generating out-of-vocabulary words involve using smaller units like characters or sub-words. A S2S model for large vocabulary A2W recognition was developed using a 300-hour Switchboard corpus with a vocabulary of 30,000 words. The model was further improved for large vocabulary tasks, showing the ability to learn word boundaries without supervision. Various methods were explored to learn acoustic word embeddings, with most using unsupervised learning approaches except for one that used a supervised Convolutional Neural Network model. The BID4 model proposes an unsupervised method to learn speech embeddings using a fixed context of words in the past and future. It has drawbacks such as the need for forced alignment between speech and words for training. Our work ties A2W speech recognition model with learning contextual word embeddings from speech, similar to the Listen, Attend and Spell model. The encoder network consists of a pyramidal multi-layer bi-directional LSTM network that maps input acoustic features to higher-level features. The decoder network, an LSTM network, models the output distribution over the next target based on previous predictions. It uses an attention mechanism to generate targets from the encoder's features. The model follows the experimental setup of word-based models with the difference of learning 300 dimensional acoustic feature vectors instead of 320 dimensional. The method described involves obtaining acoustic word embeddings from an end-to-end trained speech recognition system. It utilizes hidden representations from the encoder and attention weights from the decoder to construct \"contextual\" acoustic word embeddings. The approach addresses the challenge of aligning input speech with output words using a location-aware attention mechanism. This mechanism helps segment continuous speech into words, which are then used to derive word embeddings. The method involves constructing contextual acoustic word embeddings using attention weights on acoustic frames' hidden representations. These weights correspond to the importance of frames in classifying a word, allowing for the creation of word representations. The model maps words to acoustic frames based on attention weights, and provides three methods for obtaining acoustic word embeddings for a word. The method creates Contextual Acoustic Word Embeddings (CAWE) by using attention weights on acoustic frames' hidden representations. Three methods are used to obtain acoustic word embeddings for a word: unweighted Average, Attention weighted Average, and maximum attention. The datasets used are the 300 hour Switchboard corpus and a subset of the How2 dataset. The A2W model generates word embeddings with a word error rate of 22.2% on Switchboard and 36.6% on CallHome. The embeddings are evaluated in various tasks such as Semantic Textual Similarity, classification, entailment, and semantic relatedness using datasets like STS and SICK. The SentEval toolkit is used for evaluation. The SentEval toolkit is used to evaluate word embeddings in various tasks like classification. CAWE-M outperforms other models on STS tasks but may perform worse on classification tasks due to noisy estimation of word embeddings. The CAWE model outperforms U-AVG on STS and SICK-R tasks due to its construction process. The A2W speech recognition model has a limited vocabulary, but CAWE still competes well with word2vec CBOW. The CAWE model competes well with word2vec CBOW, outperforming it on 10 out of 16 tasks when acoustic embeddings are concatenated with text embeddings. Evaluations on Switchboard and ATIS datasets show the effectiveness of CAWE in speech-based tasks. The model architecture for the downstream evaluation task includes an embedding layer, a single layer RNN-variant, and a dense layer with softmax. Training involves 10 epochs with RMSProp and different seed values. Speech-based word embeddings show comparable performance to text-based embeddings, highlighting their utility. Acoustic word embeddings are learned from a speech recognition model for improved performance. The curr_chunk discusses an acoustic-to-word speech recognition model that generates contextual acoustic word embeddings, outperforming traditional methods by up to 34% in semantic tasks. These embeddings match text-based embeddings in spoken language understanding, indicating potential for pre-training in speech-based tasks. Future work includes scaling the model and comparing with non-contextual methods."
}