{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is a key aspect of the natural world, where organisms collaborate despite individual incentives. The study focuses on intertemporal social dilemmas (ISDs) in multi-agent problems, using MARL and natural selection to promote cooperation. A modular architecture for deep reinforcement learning agents is introduced to support multi-level selection, showing promising results in challenging environments. The research relates these findings to cultural and ecological evolution, highlighting cooperation across various scales in nature. Altruism and cooperation in species-wide societies, influenced by natural selection and various mechanisms like kin selection and reciprocity. Evolutionary theory predicts self-interested agents in multi-agent reinforcement learning tend to defect rather than cooperate, highlighting the challenge of achieving collectively optimal outcomes. The study explores intertemporal social dilemmas and the emergence of cooperation among self-interested agents in deep reinforcement learning. The study aims to find training regimes for individuals to resolve social dilemmas and promote cooperation. Previous approaches include opponent modeling, long-term planning, and intrinsic motivation functions. Evolution can be used to remove hand-crafted intrinsic motivation, similar to its applications in deep learning for optimizing hyperparameters, implementing black-box optimization, and evolving neuroarchitectures and reward functions. The proposed system distinguishes between fast learning and slow evolution processes in addressing challenges of intertemporal social dilemmas. Evolution is used to bridge the two time-scales through an intrinsic reward function implemented as a neural network. Evolutionary theory predicts that evolving intrinsic reward weights across a population does not lead to altruistic behavior. To address this, a \"Greenbeard\" strategy is implemented where agents choose partners based on signals of cooperativeness. An alternative modular training scheme called shared reward network evolution is introduced to overcome limitations of assortative matchmaking. The policy network and reward network modules evolve separately in an evolutionary paradigm to prevent overfitting. The policy networks are lower level units while the reward networks are higher level units. This approach resolves difficult issues without handcrafting and suggests a potential mechanism for the evolutionary origin of social inductive biases. Various parameters were explored, including environments, reward network features, and matchmaking strategies. In this paper, the authors discuss Markov games in a MARL setting, focusing on intertemporal social dilemmas where selfish actions benefit individuals in the short term but harm the group in the long run. Two dilemmas are considered, one involving collecting apples in a polluted environment. The policy and reward networks evolve separately to prevent overfitting, with various parameters explored. In the Harvest game, agents face a dilemma between harvesting all apples quickly for short-term gain or preserving apples for long-term group yield. Rewards for players consist of total, extrinsic, and intrinsic components, influencing their decision-making process. The extrinsic reward rEi(s, a) is the environment reward obtained by player i when taking action a from state s, while the intrinsic reward u(f) is an aggregate social preference calculated using a neural network with evolved parameters. Each agent has access to the same set of features, with their own feature being specially demarcated, which are transformed into intrinsic reward via their reward network. The text discusses the aggregation of rewards in Markov games for social preferences. Two methods of aggregating rewards are considered: retrospective and prospective. The architecture includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The retrospective method derives intrinsic reward from past rewards, while the prospective variant derives intrinsic reward from future expected rewards. The text discusses the training framework for multi-agent environments using retrospective and prospective reward variants. A population of 50 agents with policies {\u03c0 i } is trained using distributed asynchronous training and population-based training (PBT). Episode trajectories last 1000 steps and weights are updated using V-Trace. The set of weights evolved includes learning rate, entropy cost weight, and reward network weights \u03b8 4. The training framework for multi-agent environments involves evolving parameters of the policy network and reward network. Agents observe their last actions and rewards as input to the LSTM in their neural network. Evolution is based on a fitness measure calculated as a moving average of total episode return. Matches are determined through random or assortative matchmaking methods. In the training framework for multi-agent environments, agents are either randomly selected or matched based on recent cooperativeness metrics. Cooperative matchmaking groups agents of similar rank together, ensuring that highly cooperative agents play with each other. The cooperativeness metric is calculated differently for Cleanup and Harvest tasks. Cooperative metric-based matchmaking is used with individual reward networks or no reward networks, but not with the multi-level selection model. The reward network is separately evolved within its own population to allow different modules of the agent to compete. The reward network is separately evolved within its own population, allowing for independent exploration of hyperparameters and generalization to a wide range of policies. Fitness is based on individual agent return, while the reward network parameters evolve based on total episode return across the group of co-players. This approach differs from previous work by focusing on social features rather than environmental events and addressing the tension in ISDs. The reward network evolution in a social setting is crucial for dealing with the tension in Intrinsic Safety Defaults (ISDs). Shared reward networks blend group fitness with individual rewards, contrasting with hand-crafted aggregation methods. Performance comparison shows the importance of using reward networks over random matchmaking in improving agent performance in different games. The study found that using individual reward networks led to high performance in resolving the dilemma of selfishly evolved players. Shared reward network agents performed as well as assortative matchmaking, indicating that having the same intrinsic reward function was sufficient to address the dilemma. The retrospective and prospective variants of reward network evolution were compared, with the prospective variant showing better results when using a shared reward network. The prospective variant of reward networks generally results in worse performance and instability compared to the retrospective variant. Sustainability, equality, and tagging metrics are used to analyze agent behavior, showing differences in behavior based on the type of reward network used. The study compares the performance of different reward networks in agent behavior. The final weights of the retrospective shared reward networks varied based on the game, suggesting different social preferences were needed. Cleanup required a simpler reward network, while Harvest needed a more complex one to prevent over-exploitation. Random matchmaking led to arbitrary values in the first layer weights. In a study comparing reward networks in agent behavior, random matchmaking led to arbitrary values in weights. Evolutionary theory did not lead to cooperation, but assortative matchmaking did with honest signals. A new evolutionary paradigm based on shared reward networks promotes cooperation by improving credit assignment between selfish acts and collective fitness. The shared reward network evolution model promotes cooperation by improving credit assignment between selfish acts and negative group outcomes. It also mitigates social dilemmas by exposing social signals correlated with selfishness levels, leading to mechanisms for mutual cooperation. This model, inspired by multi-level selection, features lower level units constantly swapping with higher level units, similar to modularity seen in nature with microorganisms forming structures for adaptive problems. The curr_chunk discusses the spread of cultural norms and the emergence of cooperation through alternative evolutionary mechanisms like kin selection and reciprocity. It also suggests exploring how an evolutionary approach can be combined with multi-agent communication to understand cooperative behaviors better. The curr_chunk discusses the details of the game setup for Cleanup and Harvest, including the playable area size, observation window, action space, and training methods. It mentions joint optimization of network parameters, gradient updates, optimization via RMSProp, and evolving entropy cost during training. The learning rates evolved using PBT with genetic algorithms, starting at 4 \u00d7 10 \u22124. Hyperparameters were mutated with perturbations for entropy cost and learning rate, and a burn-in period of 4 \u00d7 10 6 agent steps was implemented for accurate fitness assessment before evolution."
}