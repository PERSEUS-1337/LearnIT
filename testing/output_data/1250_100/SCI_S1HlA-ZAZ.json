{
    "title": "S1HlA-ZAZ",
    "content": "We introduce an end-to-end trained memory system inspired by Kanerva's sparse distributed memory. The memory has a robust distributed reading and writing mechanism and is analytically tractable for optimal compression. Formulated as a hierarchical conditional generative model, the memory improves generative models on Omniglot and CIFAR datasets. Compared to Differentiable Neural Computers (DNC), our memory model has greater capacity and is easier to train. Neural Computers (DNCs BID10) collapse reading and writing into single slots, limiting information sharing and requiring additional slots for new inputs. Matching Networks BID25 BID4 and Neural Episodic Controller BID21 store embeddings directly, increasing memory volume with samples. In contrast, Neural Statistician BID7 summarizes datasets by averaging embeddings, potentially losing information. Associative memory architectures like the Hopfield Net BID14 store data in low-energy states, offering robustness with limited capacity. The Boltzmann Machine BID1 introduces latent variables to lift constraints on recurrent connections in dynamic systems, but requires slow reading and writing mechanisms. Kanerva's sparse distributed memory model BID15 resolves this issue by allowing fast reads and writes with independent memory size from input dimensionality. A conditional generative memory model inspired by Kanerva's model is presented in this paper, with learnable addresses and reparametrised latent variables. The model features a Bayesian memory update rule for effective memory writing, resulting in a hierarchical generative model with a memory-dependent prior that quickly adapts to new information. The hierarchical generative model proposed in this paper features a memory-dependent prior that adapts quickly to new data, enriching priors in VAE-like models through an adaptive memory system. This memory architecture extends the variational autoencoder (VAE) by deriving the prior from an adaptive memory store, with parameterized distributions implemented as multivariate Gaussian distributions. The VAE model uses neural networks to output means and variances for Gaussian distributions with diagonal covariance matrices. Training aims to maximize log-likelihood by optimizing parameters for a variational lower-bound. The model introduces the concept of an exchangeable episode and factors the joint distribution of data and memory. The joint distribution of data and memory is factorized into marginal distribution and posterior, allowing for the interpretation of writing data into memory. This approach is proposed as a way to formulate memory-based generative models, maximizing mutual information between memory and data. The generative model's joint distribution is factorized using conditional independence, with memory represented as a random matrix with a matrix variate Gaussian distribution. The distribution of data and memory is factorized into marginal distribution and posterior, enabling the writing of data into memory. Memory is represented as a random matrix with a matrix variate Gaussian distribution. The covariance between rows of M and V is a C \u00d7 C matrix providing covariances between columns of M. Independence is assumed between columns but not rows of M, with V fixed as the identity matrix I C. The addressing variable y t computes weights controlling memory access, with a learned projection transforming y t into a key vector. The weights across the rows of M are computed using a multi-layer perception (MLP) projection. The projection f is implemented as a multi-layer perception (MLP) to transform the distribution of y t and w t. The code z t generates samples of x t through a conditional distribution. The prior for z t is memory-dependent, resulting in a richer marginal distribution. In the hierarchical model, M captures global statistics of an episode, while local variables y t and z t capture local statistics for data x t. The approximated posterior distribution is factorized for reading inference. The approximated posterior distribution is factorised using conditional independence, with a parameterised posterior refining the prior distribution with additional evidence. Balancing old and new information is a challenge in memory updating, which can be optimized through Bayes' rule. Memory writing is interpreted as inference, with batch and online inference methods considered. The approximated posterior distribution of memory is discussed. The posterior distribution of memory is factorized using conditional independence, with a parameterized posterior refining the prior distribution with additional evidence. Memory updating involves balancing old and new information, optimized through Bayes' rule. The posterior of the addressing variable and code is parameterized, and the posterior of memory in a linear Gaussian model is analytically tractable with update rules for parameters R and U. The prior parameters of p(M), R0, and U0 are trained through back-propagation in the linear Gaussian model. Updating using one sample at a time can reduce the per-step cost, and performing intermediate updates with mini-batches is also possible. The storage and multiplication of the memory's row-covariance matrix U is a major cost in the update rule, but experiments suggest that this covariance is useful for memory accessing coordination. The cost of O(K 2 ) in coordinating memory accessing is usually small, dominated by the encoder and decoder parameters. Future work includes investigating low-rank approximation of U for better cost-performance balance. To train the model, a variational lower-bound of the conditional likelihood J is optimized. Sampling from q \u03c6 (y t , z t |x t , M ) approximates the inner expectation, with mean-field approximation for memory efficiency. The model penalizes complex addresses and deviation of code z t from memory-based prior for useful learning. The iterative reading mechanism in Kanerva's sparse distributed memory decreases errors when initial error is within a generous range, converging to a stored memory. This iterative process is also present in our model, improving denoising and sampling. Knowledge about memory is helpful in reading, suggesting the use of q \u03c6 (y t |x t , M ) for addressing. Training a parameterised model with the whole matrix M as input can be costly, but iterative reading reliably improves performance. In the coding literature, intractable posteriors in non-tree graphs can be efficiently approximated by loopy belief-propagation. Iterative reading in the model is effective due to the local coupling between variables. Model implementation details are provided in Appendix C, using encoder and decoder models to evaluate adaptive memory improvements. The experiments involve Omniglot and CIFAR datasets with variations in convolutional layers, memory size, and code size. The Adam optimizer was used with minimal tuning, and the variational lower bound was reported in all experiments. We tested our model on the Omniglot dataset with 1623 classes and 20 examples per class, using a 64 \u00d7 100 memory M and a 64 \u00d7 50 address matrix A. We randomly sampled 32 images to form an \"episode\" without class labels. For optimization, we used Adam with a learning rate of 1 \u00d7 10 \u22124. Additionally, our model was tested on the CIFAR dataset, where we ignored label information and used convolutional layers to handle the increased complexity of the dataset. In the unsupervised setting, convolutional coders with 32 features are used for CIFAR, with a code size of 200 and a 128 \u00d7 200 memory. The training process of the model is compared with a baseline VAE model, showing lower negative variational lower-bound and stable training. The model has learned to use memory, as indicated by the dip in KL-divergence. The Kanerva Machine achieved better reconstruction and KL-divergence compared to the VAE model. The model learned to use memory to induce a more informative prior, leading to a rich prior at the cost of additional KL-divergence. The reduction in KL-divergence was crucial for improving sample quality, as observed in experiments with Omniglot and CIFAR datasets. At the end of training, the VAE reached a negative log-likelihood of \u2264 112.7, indicating its performance. The Kanerva Machine achieved a conditional negative log-likelihood (NLL) of 68.3, demonstrating the power of incorporating adaptive memory into generative models. The weights were well distributed over the memory, showing patterns written into the memory were superimposed on others. The Kanerva Machine achieved a conditional negative log-likelihood (NLL) of 68.3, demonstrating the power of incorporating adaptive memory into generative models. The corrupted pattern is shown in the second column, with reconstruction after 1, 2, and 3 iterations. The model generates samples from a batch of images with many classes and samples, improving in quality with each iteration. Conditional samples from CIFAR are also discussed, highlighting the limitations of VAEs in this context. The study compares samples from CIFAR using the Kanerva Machine and VAEs. The Kanerva Machine produces clear local structures, while VAE samples are blurred. The model can recover corrupted images through iterative reading, showcasing interpretability of internal representations in memory. The study compares samples from CIFAR using the Kanerva Machine and VAEs. The Kanerva Machine produces clear local structures, while VAE samples are blurred. Interpolating between access weights in memory produces meaningful and smoothly changing images. DNCs were more sensitive to random initialization, slower, and plateaued with larger error compared to Kanerva Machine. Test variational lower-bounds of DNC and Kanerva Machine were compared for different episode sizes and sample classes. The study compared the Kanerva Machine and DNC models in a storage and retrieval task with Omniglot data. The DNC was sensitive to hyper-parameters and random initialization, while the Kanerva Machine was robust and easier to train. The Kanerva Machine performed well with various batch sizes and learning rates, converging below 70 test loss with all tested configurations. This makes it significantly easier to train compared to the DNC. The Kanerva Machine outperformed the DNC in a storage and retrieval task with Omniglot data. It showed robustness and ease of training with various batch sizes and learning rates, converging below 70 test loss with all configurations tested. The Kanerva Machine combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory, allowing for retrieval of unseen patterns through sampling. The Kanerva Machine combines slow-learning neural networks with a fast-adapting linear Gaussian model as memory, enabling retrieval of unseen patterns through sampling. This model generalizes Kanerva's memory to continuous, non-uniform data while integrating with deep neural networks for modern machine learning applications. Other models have combined memory mechanisms with neural networks, but lack the adaptability to new data seen in this model. Our model learns to store information in a compressed form by leveraging statistical regularity in images through the encoder, learned addresses, and Bayes' rule for memory updates. It employs an exact Bayes' update-rule without compromising neural network flexibility, showing promising performance and scalability in combining classical statistical models with neural networks for novel memory models in machine learning. Kanerva's sparse distributed memory model utilizes fixed addresses A pointing to a modifiable memory M with distributed reading and writing operations. Inputs are compared to addresses using Hamming distance, with selection based on a threshold \u03c4. During writing, patterns are stored into memory M. Kanerva's sparse distributed memory model involves storing patterns into memory M by adding a pattern x to M using a binary weight vector. Reading involves summing the memory contents pointed to by selected addresses. Kanerva showed that even with large K and D, a small portion of addresses will always be selected, allowing for correct retrieval of stored vectors. However, the model's application is limited by the assumption of a uniform and binary data distribution, which is rarely true in practice. The model architecture includes a convolutional encoder and decoder with 3 blocks each, using 16 or 32 filters. Adding noise to the input helps stabilize training. The model architecture includes a convolutional encoder and decoder with 3 blocks each, using 16 or 32 filters. Adding noise to the input helps stabilize training. The added noise, Gaussian with zero mean and standard deviation of 0.2, is used for all experiments. Bernoulli likelihood function is used for Omniglot dataset, and Gaussian likelihood function for CIFAR. To prevent Gaussian likelihood collapsing, uniform noise U(0, 1/256) is added to CIFAR images during training. The differentiable neural computer (DNC) is wrapped with the same interface as the Kanerva memory for fair comparison. The DNC receives addressing variable y_t and z_t during reading and writing stages, with a 2-layer MLP controller with 200 hidden neurons and ReLU nonlinearity. During writing, the read-out from the DNC is discarded, keeping only its state as memory; during reading, the state at each step is discarded to prevent storing new information. The model uses a 2-layer MLP with 200 hidden neurons and ReLU nonlinearity as the controller instead of LSTM to avoid interference with DNC's external memory. To prevent controllers from bypassing memory, the output is removed, ensuring DNC only reads from memory. The focus is on memory performance, comparing models with full covariance matrices to diagonal ones. The bottom-up stream in the model compensates for this. The model utilizes a 2-layer MLP with 200 hidden neurons and ReLU nonlinearity as the controller to avoid interference with DNC's external memory. The bottom-up stream compensates for memory, sampling z t from p \u03b8 (z t |y t , M ) for the decoder p \u03b8 (x t |z t ). The training shows a decrease in test loss, with a focus on memory performance and comparing models with full covariance matrices to diagonal ones. The advantage of the Kanerva Machine over the VAE is increasing during training. The model described in the paper utilizes a 2-layer MLP with 200 hidden neurons and ReLU nonlinearity as the controller to avoid interference with DNC's external memory. It uses samples from q \u03c6 (z t |x t ) for writing to the memory and mean-field approximation during reading. An alternative approach fully exploits the analytic tractability of the Gaussian distribution, simplifying notation with parameters \u03c8 = {R, U, V}."
}