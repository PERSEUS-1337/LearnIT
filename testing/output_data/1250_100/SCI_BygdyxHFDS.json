{
    "title": "BygdyxHFDS",
    "content": "Exploration is crucial in reinforcement learning, but optimal approaches are computationally challenging. Researchers have focused on designing mechanisms for exploration bonuses and intrinsic rewards inspired by natural systems. A strategy is proposed to encode curiosity algorithms as programs in a domain-specific language, enabling RL agents to perform well in new domains. The rich language of programs includes neural networks and other building blocks, allowing for generalizable programs across various domains. Pruning techniques are developed to make the approach feasible, including predicting a program's success based on its syntax. Empirical results show the effectiveness of the approach, with both familiar and novel curiosity strategies performing well and generalizing effectively. Our RL agent is augmented with a curiosity module, obtained through meta-learning over a complex space of programs, to compute a pseudo-reward at every time step. Effective exploration strategies are crucial for RL systems in complex environments, with researchers focusing on constructing reward signals that induce agents to explore their domain for longer-term learning and behavior. The researchers aim to address the challenge of hand-designing exploration strategies for RL agents by proposing a meta-learning approach inspired by curiosity in young humans and animals. They suggest using an outer loop to search for algorithms that dynamically adapt the agent's reward signal for meaningful exploration, while the inner loop performs standard reinforcement learning. This process is illustrated in Figure 1, with the aggregate agent having the interface of an RL agent. The inner RL algorithm adapts to input states and rewards to optimize rewards. The outer \"evolutionary\" search aims to find a curiosity module for the agent's lifetime return or global objectives. The architecture relies on foundational RL methods but may require adjustments for nonstationary rewards. The goal in this meta-learning setting is to find an effective curiosity module. In a meta-learning setting, the objective is to discover curiosity mechanisms that can generalize across a broad distribution of environments, including different state and action spaces. This involves exploring a rich, open-ended space of programs to find effective curiosity strategies. Previous research in Meta-RL has focused on reducing experience needed and efficient exploration in low-diversity environment distributions. In a novel approach, meta-learning is performed in a rich, combinatorial space of programs for meta-reinforcement-learning. The focus is on disparate environments and long agent lifetimes, requiring unique representation and search techniques. Curiosity strategies are represented in a domain-specific language with neural networks, gradient-descent mechanisms, and other components. This approach aims to discover general exploration methods through a computationally expensive meta-learning process. The approach involves meta-learning in a combinatorial space of programs for meta-reinforcement-learning, focusing on diverse environments and long agent lifetimes. By including different difficulty levels and predicting algorithm performance, the search for effective solutions is made feasible with modest computation. This method saves time by evaluating candidate programs in simpler domains first and monitoring agent learning curves to stop unpromising programs early. The effectiveness of the approach is demonstrated empirically by finding curiosity strategies similar to those in existing literature and discovering novel ones. The approach involves meta-learning in a combinatorial space of programs for meta-reinforcement-learning, focusing on diverse environments and long agent lifetimes. It includes different difficulty levels and predicts algorithm performance to search for effective solutions with modest computation. This method saves time by evaluating candidate programs in simpler domains first and monitoring agent learning curves to stop unpromising programs early. The effectiveness is demonstrated by finding curiosity strategies similar to those in existing literature and discovering novel ones. The agent is equipped with an RL algorithm and a curiosity module that modifies rewards to add exploration to the policy. The goal is to design a curiosity module that encourages the agent to maximize total rewards over time steps or achieve a global goal. This module induces state transitions in the environment and learning updates for the agent. The objective is to find a curiosity module that maximizes the expected original reward obtained by the system in the environment. The mathematical language in science and computing plays a crucial role in this process. In science and computing, mathematical language is successful in describing phenomena and algorithms with short descriptions. Valiant emphasizes the generality and predictive power of mathematics and algorithms. To create curiosity modules that generalize over tasks and provide exploration guidance, they are described as general programs in a domain-specific language. Algorithms in this language map history into a proxy reward. The curiosity module is decomposed into two components: one outputs an intrinsic reward value based on experienced transitions, and the other processes the current time-step. The curiosity module consists of two components: one generates intrinsic reward values based on experienced transitions, and the other processes the current time-step by combining actual and intrinsic rewards to calculate a proxy reward. The programs are drawn from a basic class with a directed acyclic graph structure of modules. Input modules have no inputs and output types corresponding to states, actions, or rewards. The program consists of pink arcs representing paths for gradient flow, buffer and parameter modules, functional modules for computing output values, and update modules for adding variables or real-valued outputs to a global loss. The output node in the DAG is designated as the program's overall output, with input and parameter values propagated through functional modules on each call. The program utilizes neural network weight updates via gradient descent as a form of memory. Multiple copies of the same agent run simultaneously with shared policy and curiosity modules. Parameters are updated using gradient descent with the Adam optimizer. Non-differentiable operations like buffers and \"Detach\" are used to control gradient flow. The program learns to make predictions online and adjusts proxy rewards based on prediction quality. The program utilizes neural network weight updates via gradient descent as a form of memory. It uses multiple agents with shared policy and curiosity modules, updating parameters with the Adam optimizer. Non-differentiable operations like buffers and \"Detach\" control gradient flow. Predictions are made online, and proxy rewards are adjusted based on prediction quality. Various curiosity modules are represented, including inverse features, random network distillation, and ensemble predictive variance. Polymorphic data types are key to the program search, with inputs and outputs typed based on the environment. The program utilizes neural network weight updates via gradient descent as a form of memory. It uses multiple agents with shared policy and curiosity modules, updating parameters with the Adam optimizer. Non-differentiable operations like buffers and \"Detach\" control gradient flow. Predictions are made online, and proxy rewards are adjusted based on prediction quality. Various curiosity modules are represented, including inverse features, random network distillation, and ensemble predictive variance. Polymorphic data types are key to the program search, with inputs and outputs typed based on the environment. The RND program abstracts neural network modules based on input types, enabling generalization across different input and output spaces. The program utilizes neural network weight updates via gradient descent as a form of memory. It uses multiple agents with shared policy and curiosity modules, updating parameters with the Adam optimizer. Non-differentiable operations like buffers and \"Detach\" control gradient flow. Predictions are made online, and proxy rewards are adjusted based on prediction quality. Various curiosity modules are represented, including inverse features, random network distillation, and ensemble predictive variance. Polymorphic data types are key to the program search, with inputs and outputs typed based on the environment. The RND program abstracts neural network modules based on input types, enabling generalization across different input and output spaces. The dimensions of the neural networks depend on the type of input, with CNNs used for images and fully connected networks for other inputs. The output intrinsic reward is based on the difference between the outputs of two neural networks, driving the agent to explore new states. The program limits the total number of modules in the computation graph to 7 to prioritize short, meaningful programs and manage the large search space. In the next section, strategies for speeding up the search over tens of thousands of programs are explored. To find effective curiosity programs in various environments, multiple pruning strategies are designed to discard less promising programs quickly. These efforts are divided into three categories: simple tests independent of running the program, filtering based on poor performance in simple environments, and meta-meta-RL learning to predict program performance. Heuristics are developed to immediately prune obviously bad curiosity programs. Two heuristics are used to quickly prune bad curiosity programs: checking for duplicates by seeding and comparing outputs, and ensuring loss functions cannot be minimized independently of input data. This helps to discard less promising programs efficiently. Our goal is to find algorithms that perform well on various environments. We observe that there are few programs that excel in all environments, while many perform poorly. To efficiently search for promising candidates, we test multiple programs in simpler environments and only a few in more complex ones. This approach is inspired by sequential halving and helps us focus on general and robust programs. Our search process predicts program performance from structure, bootstrapping an initial training set and using a k-nearest neighbor regressor. We explore the search space efficiently, discovering top programs by searching only half of it. Results are detailed in appendix C, and we can prune algorithms during the training process. During the training process of the RL agent, the top K current best programs are used as benchmarks for all T timesteps. A new candidate program's performance is compared with the top K programs at each timestep, and if significantly lower, the run is stopped. The RL agent uses PPO based on the implementation in PyTorch. The code is designed to work with any OpenAI gym environment with a specified exploration horizon T. Multiple trials are conducted for each curiosity algorithm with initialized weights and data-structures. Multiple rollouts are run for evaluation. During training, the RL agent uses PPO in PyTorch and benchmarks the top K programs at each timestep. The search for a good intrinsic curiosity program is done in an exploratory environment with image-based grid worlds. Programs are optimized for the total number of distinct pixels visited, limited to 7 operations. Around 52,000 programs are split across 4 machines with Nvidia Tesla K80 GPUs for 10 hours to find the highest-scoring 625 programs. After training with PPO in PyTorch, the RL agent uses Nvidia Tesla K80 GPUs for 10 hours to search for the highest-scoring 625 programs. Programs are pruned based on statistical significance, using a 10-nearest-neighbor regressor to predict performance and selecting programs with an \u03b5-greedy strategy. This approach focuses on evaluating the most promising programs early in the search process. After training with PPO in PyTorch on Nvidia Tesla K80 GPUs for 10 hours to search for the highest-scoring programs, the study found that most programs performed poorly, with only 0.5% showing statistically significant improvement. The top program, named Top, was surprisingly simple with only 5 operations and used a neural network to predict actions and generate high intrinsic rewards based on prediction differences. This approach allowed for early pruning of less promising programs, saving computation time. The study found that the algorithm in the program uses NLL loss or appends zeros to actions for dimension matching, applying MSE loss. The network predicts actions to imitate the policy learned by the internal RL agent. Performance in gridworld correlates with lunar lander and acrobot environments. Most intrinsic curiosity programs that perform well in gridworld also excel in the other two environments. The study discusses methods for improving gridworld performance, including disagreement, inverse features, and random distillation. The top-performing programs are variations of a program called Top, with some incorporating state-transition prediction. A reward combiner was developed based on the best program from a set of 16,000, resembling Random Network Distillation. The best reward combiner formula involves a combination of intrinsic and extrinsic rewards, with a linear interpolation. In future work, the study aims to co-adapt the search for intrinsic reward programs and combiners, evaluating them on lunar lander and acrobot environments with longer horizons and vector-based inputs. Results show good correlation between performance on grid world and new environments, with significant performances starting at an intrinsic reward above 370. The top 16 programs from grid world were also evaluated on MuJoCo environments with longer exploration horizons. The study evaluates meta-learned algorithms on environments with longer exploration horizons and continuous action-spaces. Results show that the top programs perform significantly better than constant rewards and are comparable to published algorithms. Performance is measured by mean episode reward, with confidence intervals calculated across trials and algorithms. The top programs outperform weak baselines and are statistically equivalent to published work, confirming their effectiveness. The study demonstrates that meta-learned curiosity programs outperform weak baselines and are comparable to published algorithms. Meta-training on a single environment led to good generalization to different environments, showcasing the broad applicability of meta-learning program representations. This approach differs from neural architecture search and hyperparameter optimization by aiming to generalize across multiple environments and searching over programs that include non-neural operations and data structures. The work involves searching over programs, including non-neural operations and data structures, to determine loss functions for training. It draws inspiration from AutoML and meta-learning with genetic programming, focusing on optimizing neural network weights and using neural networks as basic operations within larger algorithms. The curr_chunk discusses the use of neural networks in algorithms, specifically focusing on modular metalearning and designing curiosity algorithms. It mentions using neural network training as implicit memory and generating meaningful algorithms similar to novelty search and EX 2. However, it acknowledges that there are exploration algorithm classes not covered, such as those focusing on generating goals and learning progress. The curr_chunk discusses research on meta-learning exploration policies, including LSTM exploration in environments, improving exploration efficiency, and combining gradient-based meta-learning with structured noise for exploration. It also addresses challenges in generalizing bonus-based curiosity algorithms to new environments. In contrast to existing methods that parametrize intrinsic reward functions or modify reward functions over an agent's lifetime, the proposed approach involves searching over algorithms to optimize exploration over a larger number of time-steps. Evolved policy gradients meta-learn a neural network to compute a loss function for optimizing new policies efficiently. The proposed approach involves meta-training a robot to generalize between different environments efficiently by meta-learning programs. These programs leverage polymorphic data types to adapt neural networks to the environment they are running in, resulting in algorithms with broad generalization for reliable reinforcement learning settings. The algorithm search code will be open-sourced for further research on exploration algorithms. Meta-learning programs, instead of network weights, may have applications beyond curiosity algorithms. Types include real numbers, positive numbers, state space, and action space. The algorithm search code will be open-sourced for further research on exploration algorithms. Meta-learning programs, instead of network weights, may have applications beyond curiosity algorithms. The action space A is adapted to each environment, and feature-space F is a space useful for neural network embeddings. Lists support extra operations like average or variance. RunningNorm normalizes input by variance. Most work on meta-RL focuses on transferable feature representations or parameter values for adapting to new tasks or improving performance on a single task. The curr_chunk discusses the challenges of generalization in reinforcement learning, highlighting the limitations of current methods in transferring knowledge between different environments. It mentions various studies that have attempted to address this issue by proposing benchmarks and techniques for improving transfer learning. The focus is on achieving generalization between completely different environments, even those with different state and action spaces. The curr_chunk discusses predicting algorithm performance to find the best programs faster. It shows that optimized search finds 88% of the best programs after evaluating only 50% of them. The mean performance across 26,000 programs evaluated forms a gaussian distribution with a small long tail of programs with statistically significant performance. Top variant in preliminary search on grid world and a good algorithm found by their search are also highlighted. The curr_chunk discusses using trained networks in an algorithm found through search. It utilizes random network distillation and compares predictions based on state transitions. The same network is used for mapping and weight sharing between predictions."
}