{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. A new framework called EdgeGANRob has been proposed to improve CNN robustness by focusing on shape/structure features and using a generative adversarial network (GAN) to reconstruct images. Additionally, a robust edge detection approach called Robust Canny has been introduced to reduce sensitivity to adversarial perturbations. Comparisons with a simplified procedure called EdgeNetRob show that EdgeGANRob improves clean model accuracy without sacrificing robustness. EdgeGANRob enhances clean model accuracy while maintaining the robustness benefits of EdgeNetRob. Extensive experiments demonstrate EdgeGANRob's resilience across various learning tasks and settings. Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to adversarial examples, where imperceptible perturbations can manipulate predictions. Data poisoning or backdoor attacks manipulate training data to reduce model generalization accuracy. CNNs tend to learn surface statistical regularities instead of high-level abstractions, leading to a lack of generalization. Improving the robustness of CNNs remains a challenge, with research exploring underlying vulnerabilities. Recent studies suggest that CNNs are vulnerable to adversarial examples due to their reliance on non-robust features. Human recognition focuses on global object shapes, while CNNs are biased towards local patterns. This bias potentially contributes to vulnerabilities in CNNs, such as distribution shifting and backdoor attacks. The paper proposes EdgeGANRob, a method to improve CNN robustness by focusing on global shape structure using edge information. EdgeGANRob aims to enhance CNNs' resistance to adversarial attacks, distribution shifting, and backdoor attacks by leveraging structural information in images. The approach involves a two-stage procedure called EdgeNetRob, as shown in Figure 1. EdgeNetRob is a simplified version of EdgeGANRob that focuses on extracting structural information by detecting edges and training the classifier based on shape information. A new robust edge detection algorithm, Robust Canny, significantly improves the robustness of EdgeGANRob against attacks. However, EdgeNetRob decreases clean accuracy due to missing texture/color information, leading to the development of EdgeGANRob, which refills texture/colors based on edge images before classification. The main contributions of this paper include proposing a unified framework, EdgeGANRob, to improve CNNs' robustness against multiple tasks by extracting edge/structure information and refilling textural information with GAN. A robust edge detection approach, Robust Canny, is introduced to reduce sensitivity to adversarial perturbations. Evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. More visualization results can be found on the website provided. Defense methods against adversarial attacks have shown significant improvements, but many are not robust against adaptive attacks. Adversarial training is the state-of-the-art method, but gradient obfuscation is a common pitfall. Evaluation against customized white-box attacks is recommended. Distribution shifting is more common in real-world applications, with CNNs tending to learn superficial statistical cues. A method to robustify CNNs by penalizing predictive power and mitigating fitting superficial cues has been proposed. Recent research has focused on evaluating model robustness against common perturbations and backdoor attacks, which involve injecting specific patterns into training data to manipulate model predictions. Methods to detect poisoned training data and protect models from backdoor attacks have been proposed. Additionally, studies have highlighted the importance of robust visual features in improving recognition robustness, with CNNs relying more on textures and humans relying more on shape structure. Adversarially robust models tend to capture global structure of objects, emphasizing the need for non-robustness awareness. In this work, the authors propose using edge features as robust features for image classification. They introduce a new classification pipeline called EdgeGANRob, which extracts edge features from images, reconstructs them using a GAN, and then feeds them into a classifier. The method includes EdgeNetRob, Robust Canny, and inpainting GAN, and evaluates robustness under three settings. EdgeNetRob is a classification pipeline that trains an image classifier on extracted edge maps to make decisions solely based on edges, reducing sensitivity to local textures. However, this approach degrades CNN performance on clean test data due to missing texture/color information. To address this, EdgeGANRob fills edge maps with texture/colors using a generative model, improving clean accuracy. The robustness of this system depends on the edge detector used, as some detectors are vulnerable to attacks, leading to lower recognition accuracy. The text discusses the vulnerability of neural network-based edge detectors to attacks, leading to low accuracy in recognition tasks. To address this, a robust edge detection algorithm named Robust Canny is proposed, which improves upon traditional methods like Canny by truncating noisy pixels in its intermediate stages. The algorithm consists of 6 stages including noise reduction and gradient computation. The Robust Canny algorithm enhances traditional edge detection methods by incorporating noise reduction, gradient computation, noise masking, non-maximum suppression, double thresholding, and edge tracking by hysteresis. This algorithm aims to mitigate perturbation noise and improve accuracy in recognition tasks. The Robust Canny algorithm improves edge detection by reducing adversarial noise on the gradient map without compromising final edge map quality. Parameters like \u03c3 and thresholds \u03b8 l , \u03b8 h impact robustness, with larger values enhancing smoothing and pruning effects but potentially leading to blurrier images and loss of useful information. Careful parameter selection is crucial for a robust edge detector. Training a Generative Adversarial Network (GAN) in EdgeGANRob involves two steps, following the image-to-image translation framework (pix2pix). In EdgeGANRob, a conditional GAN is trained with an objective function including adversarial and feature matching losses. The classifier is fine-tuned with the GAN in the second stage to improve accuracy on generated RGB images. The method enhances robustness against adversarial, distribution shifting, and backdoor attacks by focusing on edge pixels. EdgeGANRob focuses on shape structure to improve model generalization and robustness against distribution shifting. Extracting edges acts as a data sanitization step to prevent backdoor attacks. EdgeNetRob, a backbone of EdgeGANRob, shows unique advantages in certain settings and is listed as an independent method for comparison. Our method is evaluated for robustness against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. We use the same network architecture for classification and evaluate using \u221e adversarial perturbation constraints. We evaluate our method's robustness against white-box attacks using standard perturbation budgets on Fashion MNIST and CelebA datasets. The need for a robust edge detector is illustrated by comparing different edge detection methods and their vulnerability to strong adaptive attacks. The study compares EdgeNetRob and EdgeGANRob with adversarial training for robustness against strong adaptive attacks. Results show that both EdgeNetRob and EdgeGANRob achieve higher clean accuracy compared to the baseline model. EdgeGANRob performs better on the CelebA dataset, highlighting the importance of using GANs for complex datasets. Both EdgeNetRob and EdgeGANRob demonstrate robustness comparable to adversarial training under strong adaptive attacks. EdgeNetRob does not use adversarial training, showing time efficiency. Tests include generalization ability under distribution shifting using perturbed Fashion MNIST and CelebA with various patterns. Comparison with state-of-the-art method PAR shows EdgeNetRob and EdgeGANRob improve accuracy on different patterns, outperforming PAR. Our method, EdgeNetRob, outperforms PAR in accuracy on greyscale images. Edge features aid CNN generalization to test data under distribution shifting and can be used as a defense against backdoor attacks. We embed invisible watermark patterns in Fashion MNIST and CelebA, with qualitative results shown in figures. Comparisons with baseline method Spectral Signature in Tran et al. (2018) are presented in tables, demonstrating successful attacks using our embedding pattern. Our method utilizes robust edge features to improve model robustness, achieving competitive results in adversarial robustness and generalization under distribution shifting. Additionally, it can enhance defense against backdoor attacks. The embedding pattern successfully attacks the vanilla Net with high poisoning accuracy on CelebA and Fashion MNIST. EdgeGANRob outperforms EdgeNetRob in clean accuracy, showcasing the benefits of inserting an inpainting GAN. The invisible watermark pattern's effect can be removed by the edge detector, highlighting the importance of using shape in improving model robustness. In improving model robustness, shape information is crucial. Data pre-processing involves resizing images to 128x128 and normalizing data. Different models like LeNet-style CNN and ResNet are used for different datasets. Various attacks like PGD and CW are evaluated with specific parameters. Robust Canny is used for evaluating adversarial robustness with hyper-parameters chosen using a validation set. In Robust Canny, hyper-parameters are chosen using a validation set to balance robustness and accuracy. The algorithm involves non-differentiable transformations, but in a white-box attack scenario, gradients need to be backpropagated through the edge detection algorithm. To strengthen attacks, a differentiable approximation of the Robust Canny algorithm is found by breaking the transformation into two stages. The Robust Canny algorithm involves non-differentiable operations, with the output being a masked version of the input. To make it differentiable for backpropagation, a constant mask assumption is made. Test accuracy changes are shown under radial and random mask transformations. Visualization results for CelebA and backdoor attacks on Fashion MNIST are also presented. The poisoning pattern can be slightly removed by EdgeNetRob, and the generated images do not share similar patterns."
}