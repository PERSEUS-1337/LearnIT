{
    "title": "BJInEZsTb",
    "content": "In this paper, a deep autoencoder network is introduced for studying representation learning and generative modeling of geometric data represented as point clouds. The learned representations outperform the state of the art in 3D recognition tasks and enable shape editing applications. Different generative models, including GANs and Gaussian mixture models, are also studied, with GMMs trained in the latent space of the autoencoders producing samples of the best fidelity and diversity. The paper introduces a deep autoencoder network for studying representation learning and generative modeling of geometric data represented as point clouds. The proposed measures of fidelity and diversity are based on optimally matching point clouds, which are essential for various applications like vision, robotics, and augmented reality. Different encodings like view-based projections and volumetric grids complement traditional shape representations, but may lack semantics. Editing and designing new objects with these representations often require complex parametric models, posing challenges in generative model design. Recent advances in deep learning have eliminated the need for hand-crafting features and models in domains with abundant data. Deep learning architectures like autoencoders and Generative Adversarial Networks are successful at learning complex data representations. Point clouds, a relatively unexplored 3D modality, provide a compact representation of surface geometry, making them attractive for learning purposes. Recent advances in deep learning have focused on using deep architectures for 3D point clouds, with a particular emphasis on generative models. Generative models, such as GANs, have gained attention in the deep learning community, but training them can be challenging and unstable. Evaluating generative models involves assessing fidelity and coverage, with coverage being especially important to avoid mode collapse. The text discusses the challenges of training and evaluating generative models for 3D point clouds, focusing on avoiding mode collapse. It introduces a new AE architecture for learning compact representations and proposes a workflow that involves training an AE first before a plain GAN in a fixed latent representation. This approach aims to improve classification, interpolation, and semantic operations while ensuring good coverage of the training and test dataset. The text discusses the benefits of using latent GANs for training generative models, highlighting their ease of training and superior reconstruction capabilities. It also compares the performance of GMMs trained in the latent space of fixed AEs. Various metrics are evaluated for learning good representations and evaluating generated samples, including proposing fidelity and coverage metrics. The paper is organized with a background in Section 2 and an introduction to the evaluation process. Section 2 provides background information and introduces evaluation metrics. Section 3 presents models for latent representations and point cloud generation. Section 4 evaluates the models quantitatively and qualitatively, with additional results in the appendix. The code for all models is publicly available. Autoencoders (AE) aim to reproduce input data through a bottleneck layer, while Generative Adversarial Networks (GANs) generate samples indistinguishable from real data. The text discusses the use of Generative Adversarial Networks (GANs) to generate samples that resemble real data. It also mentions the challenges specific to point cloud geometry, such as the lack of grid-like structures for convolution operations. The improved Wasserstein GAN is highlighted for its stability during training. Point clouds pose challenges due to their unordered nature, requiring permutation-invariant metrics for comparison. Earth Mover's distance (EMD) transforms one set to another, while Chamfer distance (CD) measures squared distances between points and their nearest neighbors. EMD is differentiable almost everywhere, while CD is more computationally efficient. These metrics are essential for evaluating representations and generative models. In the paper, comparisons between point clouds are essential for evaluating representations and generative models. Metrics like Coverage, COV-CD, COV-EMD, and Minimum Matching Distance (MMD) are used to assess the quality, faithfulness, and diversity of models. Coverage measures how well a point-cloud distribution matches a ground truth distribution, while MMD captures fidelity by matching every point cloud to the closest one in the other distribution. The paper discusses metrics like Coverage, COV-CD, COV-EMD, and Minimum Matching Distance (MMD) for evaluating representations and generative models of point clouds. MMD measures distances in pairwise matchings to assess the realism of elements in point clouds. Jensen-Shannon Divergence (JSD) is used to compare marginal distributions in 3D space. The architectures of representation and generative models for point clouds, including autoencoder design and GAN tailored to point-cloud data, are described. A more efficient pipeline is introduced, involving learning an AE and training a smaller GAN in the latent space, as well as a simpler generative model based on Gaussian Mixtures. The encoder architecture of the AE network for point clouds includes 5 1-D conv layers followed by ReLU and batch-norm layers. The output is a k-dimensional vector for the latent space. Two distinct AE models, AE-EMD and AE-CD, are explored using EMD-distance approximation and Chamfer-Distance for structural losses. The decoder consists of 3 fully connected layers to produce a 2048 \u00d7 3 output. The study constructed 8 AEs with different bottleneck sizes and trained them on point clouds of a single object class. The best generalization error was found with a bottleneck size of k = 128. The raw point cloud GAN operates directly on the input, while the latent-space GAN passes data through a pre-trained autoencoder. The study trained separate autoencoders for each object class with EMD loss function. The GAN operates on the 128-dimensional bottleneck variable of the AE. The architecture for the l-GAN is simpler than the r-GAN, with shallow designs for both generator and discriminator. Gaussian Mixture Models were also trained on the latent spaces learned by AEs. GMMs can be turned into point-cloud generators by sampling latent-space from the distribution and using the AE's decoder. Reconstructions of unseen shapes from the test split of input data were shown in Figure 1. The study trained separate autoencoders for each object class with EMD loss function and used a bigger bottleneck of 512 for an experiment on 57,000 models from ShapeNet. The performance of the latent features computed by the AE was evaluated by applying them as feature extractors on supervised datasets. The study utilized a 512-dimensional bottleneck layer vector extracted from a network to process features for input 3D shapes. A linear classification SVM trained on ModelNet BID32 was used to evaluate the features, showing superior results compared to previous methods. The decoupling of latent representation from generation allowed for flexibility in choosing the AE loss, impacting the learned feature. Different loss functions performed equivalently on ModelNet10 but CD loss showed better results with increased variation within the collection. The experiment also demonstrated the domain-robustness of the learned features. Our Autoencoders (AEs) demonstrate the ability to reconstruct unseen shapes and enable shape editing applications such as interpolations, part editing, and analogies. The learned latent representation shows generalization ability and comparable reconstruction quality on training and test splits. Five generative models were trained on chair point-cloud data distribution, with two AEs established with a 128-dimensional bottleneck layer. In the study, various generative models were trained on chair point-cloud data distribution, including Autoencoders (AEs) with a 128-dimensional bottleneck layer. Different GANs were trained in the latent spaces of the AEs, with model selection based on synthetic results matching the ground-truth distribution using JSD or MMD-CD metrics. The training epochs were limited to 2000, and the final models were selected based on the similarity of synthetic point clouds to the validation set. To compare synthetic datasets to validation datasets, JSD or MMD-CD metrics are used. Model selection is done every 100 epochs (50 for r-GAN) to reduce computational cost. Models are selected based on JSD criterion, with 32 Gaussian components for GMM. Full covariance matrices perform better than diagonal ones for GMMs. MMD-CD criterion also yields similar quality models with 40 Gaussians. Evaluation of 5 generators on chair dataset shows 84.7% classification score for ground-truth point clouds. Models are compared based on their ability to generate synthetic data after selection. Upon model selection, we compare their ability to generate synthetic samples resembling the train and test splits of the ground truth distribution. Results are reported in Table 2 for the train split, and average classification probability for chair recognition using PointNet classifier BID17 is shown. A similar experiment is conducted for the test split, with results averaged over three pseudo-random repetitions and various comparison metrics reported in TAB10. Training a Gaussian mixture model in the latent space of the EMD-based AE yields the best results in terms of fidelity and coverage. The achieved fidelity and coverage are comparable to the reconstruction baseline, with the MMD-EMD values being 0.05 for AE-EMD and 0.06 for GMMs. The generalization ability of the models is demonstrated through comparable performance on training vs. testing splits. Synthetic datasets are generated for experiments, with the size being equal to the train dataset for train split and three times bigger for test split and validation split comparisons. When selecting models, synthetic datasets three times larger than the ground truth are used to reduce sampling bias, especially for measuring MMD or Coverage statistics. The MMD-CD distance to the test set may appear small for r-GANs, but qualitative inspection reveals limitations in distinguishing pathological cases. Examples show r-GANs and l-GANs generating point clouds, with the inadequacy of chamfer distance highlighted. The r-GAN results are compared to the nearest neighbor in the l-GAN set using CD and EMD distances. CD values do not capture the lower quality of r-GAN results due to point concentration in likely occupied areas. EMD promotes one-to-one mapping, penalizing r-GAN results in terms of MMD and coverage. Training trends were monitored extensively to understand model behavior. Extensive measurements were conducted during model training to analyze behavior. Results showed that r-GAN struggled to provide good coverage of the test set, while l-GAN (AE-CD) performed better in terms of fidelity with fewer epochs. However, CD distance favored r-GAN results due to high-density areas. Switching to an EMD-based AE (l-GAN, AE-EMD) led to a significant improvement in coverage and fidelity. Switching to a latent WGAN largely eliminates mode collapse in GANs on point-cloud data, improving coverage and fidelity. Comparisons to voxel-based methods show favorable results in terms of JSD on the training set of the chair category. The l-GANs outperform BID31 in terms of diversity and classification, with shorter training time and better results. The 32-component GMM also shows high-quality results, demonstrating the strength of the learned representation. The l-GAN produces clearer results compared to the r-GAN, showing the advantage of using structural loss on a pre-trained AE. Synthetic point clouds were generated using l-GAN and a 32-component GMM trained on the latent space of an AE with EMD loss. Experiments were conducted with an AE-EMD trained on a mixed set of point clouds from 5 categories, with comparisons made against class-specific AEs. The study evaluates the fidelity of l-WGANs trained on multi-class AE compared to class-specific ones. Results show similar performance, with minor sacrifices in visual quality. Limitations include decoding failures for chairs with rare geometries and missing high-frequency details, impacting the style of input shapes. The r-GAN struggles to create realistic shapes. The r-GAN faces challenges in generating realistic shapes, especially for certain classes like cars. Improving raw-GANs for point clouds is a promising area for future research. Training Gaussian mixture models in the latent space of an autoencoder, related to VAEs, can help address issues like over-regularization. Fixing the AE before training generative models yields positive results, with novel architectures for 3D point-cloud representation learning and generation showing good generalization and meaningful semantics in the generated samples. Our generative models can produce accurate samples without memorizing specific examples. The best-performing model in our experiments is a GMM trained in the fixed latent space of an AE. Simple latent GMMs can be as powerful as adversarially trained models, suggesting classic tools should not be overlooked. The AE used for experiments had specific encoder and decoder configurations, with batch normalization and data augmentation techniques applied during training. The decoder in the AE model consisted of 3 FC-ReLU layers with varying numbers of neurons. Different setups like denoising or regularized did not provide any advantage over the basic architecture. The discriminator had 1D-convolutions with specific filters and leaky-ReLU layers. The generator in the r-GAN model had 5 FC-ReLU layers with different numbers of neurons. Training was done with Adam optimizer and a noise vector drawn from a Gaussian distribution. The discriminator and generator architectures for the model were specified, including the number of neurons in each layer. The training parameters for the l-Wasserstein-GAN and r-GAN models were detailed, along with the classification experiments using a linear SVM classifier. Optimization parameters for the SVMs used in each dataset with structural loss of the AE were also provided in Table 5. The text discusses the reconstruction quality of autoencoders (AEs) for shape editing applications, showcasing their ability to generalize across different object classes. It also demonstrates editing parts in point clouds using vector arithmetic on the AE latent space, such as tuning car appearance, adding armrests to chairs, and removing handles from mugs. The AE-EMD embedding trained across all 55 object classes enables interesting applications involving different shapes. The text discusses using latent representations in autoencoders for shape editing applications, showcasing the ability to model structural differences between object sub-categories. By transforming latent representations, properties of objects can be changed, allowing for interpolation between different shapes. The latent representation supports removing and merging shapes, enabling a \"morph-like\" sequence between shapes. The text demonstrates the power of latent representations in autoencoders for shape editing, allowing for morphing between shapes and finding shape analogies through linear manipulations and nearest-neighbor searching in the latent space. Preliminary results of point-cloud generators working with voxel-based AEs are also included. The text highlights the use of latent representations in autoencoders for shape editing, showing improvements over raw GAN architectures. The study compares AE-based GMM models with voxel-based GANs, demonstrating superior performance in shape analogies and coverage. The study compares the performance of voxel-based GMM models with point-cloud-based models, showing that point-cloud-based models outperform voxel-based models in fidelity to ground truth. Voxel-based models may artificially increase coverage due to matching all generated shapes against ground truth, even if they have missing components. The study compares voxel-based and point-cloud-based GMM models, finding that point-cloud models outperform voxel-based models in fidelity to ground truth. Voxel-based models may artificially increase coverage by matching poor quality partial shapes. The study compares voxel-based and point-cloud-based GMM models, finding that point-cloud models outperform voxel-based models in fidelity to ground truth. The voxel-based model architecture includes consecutive layers with specific parameters, trained for 100 epochs with Adam optimizer. Reconstruction quality is evaluated using intersection-over-union metric on ShapeNetCars dataset, comparing against a state-of-the-art method. The study compares the performance of GMM-generator models using voxel-based and point-cloud-based approaches. The point-cloud models show better fidelity to ground truth compared to voxel-based models. The generative models are evaluated by comparing memorized training data sets with test point-clouds, demonstrating good coverage and fidelity. The advantage of using a learned representation is highlighted, showing the ability to compactly represent data and generate novel shapes. Despite some mode collapse, the generative models achieve excellent fidelity, as indicated by the almost identical MMD to the memorization case. In Tables 10, 11, 12, comparisons are provided with BID32 for major ShapeNet classes. JSD-based comparisons for models are shown in Table 10, while MMD/Coverage comparisons are in TAB12. Generalization error of various GAN models is depicted in FIG0, showing measurements using JSD and MMD-CD metrics. GMM model selection is illustrated in Figure 17, with models achieving smaller JSD with a full covariance matrix. In Table 10, models with full covariance matrices achieve smaller JSD compared to diagonal covariance models. Having 30 or more clusters is sufficient for minimal JSD. Evaluation of five generators on chair data using minimal MMD-CD for model selection. GMM-40-F represents a GMM with 40 Gaussian components with full covariances. Synthetic point-clouds are sampled for each model to measure MMD-CD match with ground truth data."
}