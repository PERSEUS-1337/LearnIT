{
    "title": "SylCrnCcFX",
    "content": "In this paper, a new learning problem is proposed to encourage deep networks to have stable derivatives over larger regions, focusing on networks with piecewise linear activation functions. The algorithm consists of an inference step to identify stable linear approximation regions around points and an optimization step to expand them. A novel relaxation is proposed to scale the algorithm to realistic models, illustrated with residual and recurrent networks on image and sequence datasets. The text discusses the challenges of unstable derivatives in deep learning models, leading to unreliable first-order approximations for model explanations. It emphasizes the instability of derivatives with respect to input coordinates, caused by over-parametrization in state-of-the-art models. Gradient stability is highlighted as different from adversarial examples, with the focus on maintaining stability within local regions. In this paper, the focus is on deep networks with piecewise linear activations to address the challenges of unstable derivatives in deep learning models. The goal is to ensure gradient stability within local regions by inferring lower bounds on the maximum radius of p-norm balls around a point where derivatives are provably stable. The special structure of these networks allows for the formulation of a regularization problem to maximize the lower bound, particularly focusing on the case of p = 2 for which an analytical solution exists. The paper focuses on deep networks with piecewise linear activations to ensure gradient stability within local regions. They propose a regularization problem to maximize the lower bound of stable derivatives, resembling support vector machines. A novel perturbation algorithm is introduced for piecewise linear networks to collect exact gradients efficiently. Empirical experiments are conducted on various network architectures with image and time-series datasets. Key contributions include inference algorithms for identifying stable input regions and a novel perturbation algorithm for efficient gradient evaluation. The paper focuses on neural networks with piecewise linear activations for stability. They introduce a novel learning criterion and perturbation algorithms for high-dimensional data. The focus is on networks with piecewise linear activation functions like ReLU and its variants. The approach is based on a mixed integer linear representation of piecewise linear networks. The activation patterns in neural networks with piecewise linear activations form linear regions in the input space. These regions have stable derivatives and can be visualized, used for reachability analysis, and are connected to vector quantization. The distinction between locally linear regions and decision regions is important for understanding the network's behavior. In contrast to previous work on quantifying complexity, this study focuses on expanding local linear regions through learning. The stability concept considered here differs from adversarial examples, with methods that are more scalable and efficient for certifying margins around points in high-dimensional settings. The proposed learning algorithm aims to maximize the margin of linear regions around each data point in an unsupervised manner, similar to transductive SVM. It introduces a smooth relaxation of the margin and perturbation algorithms for gradient stability in complex models. The curr_chunk discusses the use of gradient-based explanation methods for deep models, focusing on establishing robust derivatives in neural networks with ReLU activations. It introduces notation, inference, and learning algorithms for FC networks. The curr_chunk introduces the computation process in neural networks with ReLU activations, focusing on the transformation matrix and bias. It discusses the linear output transformation and the piecewise linear property of neural networks. The activation pattern used in the paper is defined as a set of indicators for neurons that specify functional constraints. The activation pattern in neural networks with ReLU activations is defined by a set of indicators for neurons that specify functional constraints. Each linear region of the network is characterized as a convex polyhedron with linear constraints. The feasible set of the activation pattern is equivalent to a convex polyhedron, and the p margin of x subject to its activation pattern is defined. The feasibility of directional perturbations in neural networks with ReLU activations can be verified by checking if a point x + \u00af\u2206x is within the feasible set S(x). This can be done by ensuring that x + \u00af\u2206x satisfies the activation pattern O in S(x). Binary searches can then be used to find certificates for directional perturbations and 1-balls, with the feasibility of 1-balls being more tractable compared to \u221e-balls in high dimensions. The feasibility of directional perturbations in neural networks with ReLU activations can be verified by checking if a point x + \u00af\u2206x is within the feasible set S(x). Certificates for directional perturbations and 1-balls can be found using binary searches, with the feasibility of 1-balls being more tractable. The certification process is efficient due to the convexity of S(x) and can be done analytically by exploiting the polyhedron structure of S(x). The minimum 2 distance between a point x and the union of hyperplanes can be computed efficiently, and the number of linear regions in the neural network is related to the overall number of linear regions in the input space. Counting the number of linear regions in the whole space is intractable due to the combinatorial nature of activation patterns, so certifying the number of complete linear regions among the data points is proposed instead. In neural networks with ReLU activations, the number of complete linear regions of f \u03b8 among data points D x can be efficiently certified. The objective is to maximize the 2 margin\u02c6 x,2 through a regularization problem, but the rigid loss surface may hinder optimization. To address this, a hinge-based relaxation to the distance function similar to SVM is proposed. If a certain condition is not met, a valid upper bound is still provided. The text discusses deriving a relaxation for a smoother problem in neural networks with ReLU activations. It introduces a regularization problem to maximize the margin in a linear model scenario and compares different loss functions in a binary classification dataset. The text introduces a relaxed regularization approach for neural networks with ReLU activations, aiming to maximize margin in a linear model scenario. It compares different loss functions in a binary classification dataset and discusses the resulting piecewise linear regions and prediction heatmaps. The relaxed regularization enlarges linear regions around training points and generalizes properties to the whole space, resulting in a smoother prediction boundary. The final objective is to learn Robust Local Linearity (ROLL) by considering a set of neurons with high losses to a given point. The text discusses a simple additive structure in Eq. FORMULA17 when \u03b3 = 100, stabilizing the training process without a nonlinear sorting step. It introduces a parallel algorithm for computing gradient norms without back-propagation, utilizing the linear function of hidden neurons and constructing a linear network g \u03b8. The proposed approach involves computing gradients of neurons using a perturbation algorithm, which is more efficient than back-propagation. Despite the parallelizable computation, it is challenging to compute loss for large networks in high dimensions. An unbiased estimator for the loss is proposed, which can be computed in a single forward pass. The sum of gradient norms is decoupled for efficient computation. The proposed approach involves efficiently computing gradients of neurons using a perturbation algorithm. It can be applied to deep learning models with affine transformations and piecewise linear activation functions. Comparisons with a baseline model ('vanilla') are made in various scenarios. The proposed approach efficiently computes neuron gradients using a perturbation algorithm for deep learning models. Comparisons with a baseline model are made in different scenarios, except for regularization. Evaluation measures include accuracy, number of linear regions, and margins of linear regions. Experiments are conducted on a single GPU with 12G memory using a 4-layer FC model with ReLU activations on the MNIST dataset. Tuned models with specific parameters are compared to the baseline model, showing significant improvement in performance. The ROLL loss achieves significantly larger margins compared to the vanilla loss, with a tradeoff of 1% accuracy. The Spearman's rank correlation between testing data is consistently high. Parameter analysis shows that increased C and \u03bb result in decreased accuracy but increased margin. Higher \u03b3 values indicate less sensitivity to hyper-parameters. The proposed method shows efficiency in terms of running time for gradient descent steps. The ROLL loss achieves larger margins compared to the vanilla loss with a 1% accuracy tradeoff. Results show that the perturbation algorithm is only twice slower than the vanilla loss and achieves a 12 times speed-up compared to back-propagation. The computational overhead of the method is minimal, and the approximate loss is about 9 times faster than the full loss. The dataset from UCI repository BID11 has variable sequence length, 12 channels, and 9 classes. The network implemented uses the state-of-the-art scoRNN with LeakyReLU activation. Results show that the approach leads to larger margins on testing data compared to the vanilla loss. Sensitivity analysis on derivatives was conducted to identify stability bounds. Visualization using the ROLL model with 98% accuracy is provided in FIG4. The ROLL model with 98% accuracy shows larger stability bounds compared to the vanilla model. Experiments are conducted on Caltech-256 dataset with downsized images and a 18-layer ResNet. The ROLL loss is used with 120 random samples per channel. Evaluation measures are challenging due to high input dimensionality, so a sample-based approach is used to assess gradient stability. The ROLL model, with 98% accuracy, demonstrates larger stability bounds compared to the vanilla model on the Caltech-256 dataset. Evaluation of gradient stability is done using the ROLL loss with 120 random samples per channel. The gradient distortion is evaluated in terms of expected 1 distortion and maximum 1 distortion within an intersection of an \u221e-ball and image domain. Adversarial gradient is found using a genetic algorithm for black-box optimization due to computational limits. The study uses 8000 samples to approximate expected distortion, evaluating 1024 random images for gradient distortions. Results show ROLL loss has stable gradients and slightly better precision than vanilla loss. Only 40 and 42 images change labels in ROLL and vanilla models, respectively. The goal is to create locally transparent neural networks for stable applications. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. Feasible activation patterns are equivalent to satisfying linear constraints in the first layer. The proof of directional feasibility and 1-ball feasibility is provided. The text discusses the convexity of a set S(x) and the 2-ball Certificate for a point x. It also mentions constructing a neural network with linear activation functions for parallel computation of gradients. The text discusses constructing a neural network with linear activation functions for parallel computation of gradients. It explains the procedure to collect partial derivatives with respect to an input axis and analyzes the complexity of the proposed approach. The text discusses constructing a neural network with linear activation functions for parallel computation of gradients. It explains the procedure to collect partial derivatives with respect to an input axis and analyzes the complexity of the proposed approach. In this setting, a forward pass up to the last hidden layer takes M operations. The perturbation algorithm computes gradients for a batch of inputs by taking two forward passes, totaling 2M operations. Back-propagation, on the other hand, requires sequential computation of gradients for each neuron, taking M i=1 2iN i operations in total. Dynamic programming using the chain-rule of Jacobian is used to compute all gradients efficiently. The text discusses constructing a neural network with linear activation functions for parallel computation of gradients. It explains the procedure to collect partial derivatives with respect to an input axis and analyzes the complexity of the proposed approach. The dynamic programming approach efficiently computes all gradients using the chain-rule of Jacobian. In the context of convolutional layers, explicitly representing the convolutional operation as a linear transformation is costly. An introductory guide to derivations for maxout/max-pooling nonlinearity is provided, highlighting the feasibility of deriving inference and learning methods for piecewise linear networks with max-pooling nonlinearity. It is suggested to use convolution with large strides or average-pooling instead of max-pooling neurons to avoid new linear constraints. The text discusses constructing a neural network with linear activation functions for parallel computation of gradients. It explains the procedure to collect partial derivatives with respect to an input axis and analyzes the complexity of the proposed approach. The network degenerates to a linear model once an activation pattern is fixed, inducing a feasible set in the input space. The feasible set forms a convex polyhedron with linear constraints, allowing for the application of inference and learning algorithms. The model consists of fully-connected hidden layers with specific dimensions and loss function. Training is done for 5000 epochs with Adam optimizer. The model is trained for 5000 epochs with Adam optimizer. The regularization parameters are tuned, data is normalized, and margin calculations are reported. The fully-connected model has 4 hidden layers with ReLU activation. Training involves stochastic gradient descent with Nesterov momentum. Tuning is done through grid search. The model is trained with a learning rate of 0.01 and momentum of 0.5. Grid search is done on \u03bb, C, \u03b3 parameters. The data is not normalized, and the representation is learned using a single layer scoRNN. LeakyReLU activation functions are used with hidden neurons set to 512. AMSGrad optimizer is used with a learning rate of 0.001. Another grid search is done on \u03bb, C parameters with \u03b3 set to 100. Testing accuracy is compared to the baseline model. The model is trained with a learning rate of 0.01 and momentum of 0.5. Grid search is done on \u03bb, C, \u03b3 parameters. The data is not normalized, and the representation is learned using a single layer scoRNN. LeakyReLU activation functions are used with hidden neurons set to 512. AMSGrad optimizer is used with a learning rate of 0.001. Another grid search is done on \u03bb, C parameters with \u03b3 set to 100. Testing accuracy is compared to the baseline model. The largest testing accuracy of 2.50% less compared to the baseline model is reported, with models trained on normalized images and a bijective mapping established between normalized and original distance. The pre-trained ResNet-18 is modified by replacing max-pooling with average-pooling and enlarging the receptive field of the last pooling layer. The model is trained with stochastic gradient descent with Nesterov momentum for 20 epochs and an initial learning rate of 0.005. The model is trained with gradient descent and Nesterov momentum for 20 epochs, with an initial learning rate of 0.005 adjusted to 0.0005 after 10 epochs. The batch size is 32, and the best validation loss model is selected. Tuning involves fixing C = 8, using 18 samples for learning, tuning \u03bb, and training with 360 random samples. A genetic algorithm (GA) BID33 with 4800 populations and 30 epochs is implemented for further evaluation. Genetic algorithm (GA) is used with 4800 populations and 30 epochs for further evaluation. Samples are sorted based on distance, with the top 25% kept. Crossover involves replacing the remaining samples with a random linear combination from the top samples. Projection ensures feasibility. Mutation is not implemented due to computational reasons. Crossover operator is analogous to a gradient step. Various images are visualized including original, gradient, adversarial gradient, and integrated gradient attributions. The text discusses visualizing gradients and integrated gradients using a common implementation procedure. The examples from the Caltech-256 dataset are shown to highlight the differences in gradient distortions on the ROLL model. The visualization focuses on integrated gradients to showcase variations in different settings. The captions in Figures 5 and 6 show the maximum 1 gradient distortions for images in the Caltech-256 dataset on the ROLL model. The values differ slightly from Table 4 due to interpolation. The maximum distortions for the vanilla model and ROLL model are highlighted for different images in the dataset."
}