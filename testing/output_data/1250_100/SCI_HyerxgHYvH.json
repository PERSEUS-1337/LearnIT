{
    "title": "HyerxgHYvH",
    "content": "We propose a Lego bricks style architecture for evaluating mathematical expressions, using small independent neural networks for specific operations. By reusing these networks, we can solve more complex tasks like n-digit multiplication and division. This approach allows for generalization up to 7 digit numbers and introduces reusability in computations. Our solution for evaluating mathematical expressions uses a Lego bricks style architecture with small independent neural networks for specific operations, allowing for generalization up to 7 digit numbers. While Artificial Neural Networks lack generalization and understanding of inherent rules, other living species possess fundamental capabilities in numerical extrapolation and quantitative reasoning. Living species, including humans, demonstrate intelligence through numerical extrapolation and quantitative reasoning. By understanding and reusing memorized examples, complex operations can be developed. Inspired by human learning methods, fundamental arithmetic operations are identified and learned using neural networks for generalization. Fundamental arithmetic operations are learned using neural networks to develop larger and more complex networks for solving various problems like n-digit multiplication and division. This is the first work to propose a generalized solution for these arithmetic operations, working for both positive and negative numbers. Neural networks are known for approximating mathematical functions, and recent works have explored different network architectures for improving performance. Recent works have explored different network architectures, such as gating units and densely connected layers, to train networks that can generalize over minimal training data. EqnMaster uses generative recurrent networks to approximate arithmetic functions, but struggles to generalize beyond 3-digit numbers. The Neural Arithmetic Logic Unit (NALU) uses linear activations and gate operations to predict arithmetic function outputs, highlighting extrapolation issues in end-to-end learning tasks. Additionally, a simple Feed Forward Network can solve arithmetic expressions like multiplication, but may not be the most efficient for more complex tasks. Optimal Depth Networks have also been proposed for solving arithmetic expressions efficiently. Our work builds on the premise of Binary Multiplier Neural Networks, proposing a network that can predict the output of basic arithmetic functions for both positive and negative decimal integers. Unlike existing models that only work on limited digits, we train several smaller networks to perform different subtasks needed to complete complex arithmetic problems. Our work proposes using smaller networks for different subtasks in complex arithmetic operations like signed multiplication, division, and cross product. We also suggest a loop unrolling strategy to generalize solutions from 1-digit to n-digit arithmetic. Digital circuits using shift and accumulator methods are known for accurate arithmetic operations and can be easily scaled. Initial work shows neural networks can simulate digital circuits, inspiring our analysis of n-digit multiplication involving 1-digit operations. The neural networks designed for arithmetic operations include addition, subtraction, multiplication, place value shifting, and sign calculation. Complex functions like multiplication and division can be achieved using these fundamental blocks. Neurons in the network perform summation by multiplying inputs with weights and passing them through an activation function. The addition module uses a single neuron with two inputs and weights set to {+1, +1}. The subtraction module follows a similar structure. The subtraction module consists of a single neuron with two inputs and weights set to {+1, \u22121}. It facilitates shift-and-add multiplication by multiplying digits of the multiplier and multiplicand one at a time. The output is combined to create a single number, placed using a place value shifter, and added to obtain the final output. The module can be unrolled for n inputs, each taking a 1-digit number. The network uses fixed weights in the power of 10 for each preceding digit. In 1-digit integer multiplication, there are 82 possible outcomes, leading to a feed forward network with two input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. The model for single digit multiplication consists of a neural network with 2 hidden layers and 82 neurons. It computes the absolute value of a single number by performing operations like addition, subtraction, and maxpooling. Input and output sign calculators are used to determine the sign of numbers in the multiplication or division process. The final layer in the neural network performs a subtraction of 1 from the previous layer's output. The network takes sign and magnitude as input, passes it to a hidden layer of 10 neurons, and uses a soft-sign activation in the output layer to predict the sign multiplication result. The module is designed for assigning a sign to the output of complex operations like multiplication and division. Signed multiplication involves converting numbers to positive integers, extracting input and output signs, and using a multiply sub module for the actual multiplication process. The multiplication model based on digital circuitry involves multiplying tokens of multiplicand with tokens of multiplier, adding results with carry forwards, and assigning a sign using 1-digit sign multiply. The division model separates sign and magnitude, with the architecture inspired by long division where n-digit divisor controls output computation. The division model in the architecture involves selecting the smallest non-negative integer using additional layers. The selected node represents the remainder and quotient result of division for the n-digit dividend and divisor. The architecture of the multiplication network is shown in Figure 2(b,d). Comparison with other models like Neural Arithmetic and Logic Unit (NALU) is done for signed arithmetic operations. Our model demonstrates superior performance in arithmetic operations on integers compared to recurrent and discriminative networks. It achieves 100% accuracy even with negative integers and exclusive signed multiplication. Comparison with the Neural Arithmetic and Logic Unit (NALU) model shows our model's effectiveness, especially in division operations. The NALU network's performance is limited to a specific range, while our model shows consistent results across different digit lengths. In this paper, the approach involves dividing complex tasks into smaller sub-tasks that share similarities. Instead of training a single end-to-end neural network, multiple smaller networks are trained independently to perform specific operations. Fundamental arithmetic operations are identified and learned using simple feed forward neural networks, which are then reused to build larger networks for tasks like n-digit multiplication and division. One limitation is the use of float operations in the tokenizer, restricting end-to-end training of complex networks. The current work involves using pre-trained smaller networks for fundamental operations without hindering the current work. A cross product network has been designed and its accuracy is being tested. Future work includes developing a point cloud segmentation algorithm using a larger number of identical smaller networks."
}