{
    "title": "ByBAl2eAZ",
    "content": "Deep reinforcement learning methods use noise injection for exploratory behavior. Adding noise to agent's parameters can lead to more consistent exploration. Combining parameter noise with traditional RL methods benefits both off- and on-policy methods. Exploration is a key challenge in deep RL to prevent premature convergence to local optima. Many methods have been proposed to address this challenge in high-dimensional and continuous-action environments. In high-dimensional and continuous-action MDPs, exploratory behavior in deep reinforcement learning can be enhanced by adding temporally-correlated noise to algorithms like DQN, DDPG, and TRPO. This approach improves exploration by introducing parameter space noise, leading to a wider variety of behaviors. Parameter noise, such as BID18 and TRPO, enhances exploratory behavior in deep reinforcement learning. It outperforms traditional action space noise in tasks with sparse reward signals. The standard RL framework involves an agent interacting with a fully observable environment modeled as a Markov decision process. The goal is to maximize the expected discounted return by optimizing the policy parameterized by \u03b8. Off-policy RL methods like Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG) allow learning from data collected by different policies. DQN uses a deep neural network to estimate the optimal Q-value function, while DDPG is an actor-critic algorithm suitable for continuous action spaces. Both algorithms update their models using off-policy data to improve performance. Trust Region Policy Optimization (TRPO) is an extension of traditional policy gradient methods that ensures a small change in the policy distribution by computing an ascent direction. It solves a constrained optimization problem using discounted state-visitation frequencies and the advantage function. This work focuses on policies represented as parameterized functions, denoted as \u03c0 \u03b8, with \u03b8 as the parameter vector. Policies are sampled by adding Gaussian noise to the parameter vector, and the perturbed policy is kept fixed for the entire rollout. State-dependent exploration is discussed, highlighting the difference between action space noise and parameter space noise. Policies are represented as parameterized functions, denoted as \u03c0 \u03b8, with Gaussian noise added to the parameter vector for sampling. Perturbing deep neural networks using spherical Gaussian noise is achieved through a reparameterization technique, ensuring consistency in actions and introducing a dependence between state and exploratory action. Adaptive noise scaling in parameter space involves adjusting noise scale over time based on action space variance, using a distance measure to determine when to increase or decrease noise. This method addresses limitations in selecting noise scale for deep neural networks. Realization of distance measures for various RL algorithms like DQN, DDPG, and TRPO is discussed. Parameter space noise can be applied in both off-policy and on-policy methods, with adaptations for each. In on-policy settings, parameter noise is incorporated using an adapted policy gradient. The expected return is expanded using likelihood ratios and the re-parametrization trick for N samples. The value of \u03a3 is fixed and scaled adaptively. This section addresses the benefits of incorporating parameter space in state-of-the-art RL algorithms. Parameter space noise can benefit state-of-the-art RL algorithms by aiding in exploring sparse reward environments and improving sample efficiency compared to evolution strategies. Reference implementations of DQN and DDPG with adaptive parameter space noise are available online for both discrete-action environments and continuous control tasks. In discrete-action environments, comparisons are made using DQN with parameter noise against a baseline DQN agent with -greedy action noise. By using parameter perturbation, the network is reparametrized to represent the greedy policy \u03c0 implied by the Q-values. A fully connected layer is added after the convolutional part of the network to predict a discrete probability distribution over actions. Perturbing \u03c0 instead of Q results in more meaningful changes, as it defines an explicit behavioral policy. The policy is trained to output the greedy action based on the current Q-network, enhancing exploration in sparse reward environments. The policy is trained to exhibit the same behavior as running greedy DQN. Parameter space noise approach is compared against regular DQN and two-headed DQN with -greedy exploration. Actions are randomly sampled for the first 50 thousand timesteps to fill the replay buffer before training. Parameter space noise performs better when combined with a bit of action space noise. Experimental details are provided in Section A.1. 21 games of varying complexity are chosen for training, with learning curves shown in FIG2. Each agent is trained for 40 M frames, and performance is evaluated with three random seeds. The study compares parameter space noise with action space noise in training policies for various games. Results show parameter space noise outperforms action space noise in consistency-demanding games but struggles in extremely challenging games like Montezuma's Revenge. Further exploration methods like BID4 may be needed for such games. The study also confirms that the architecture change to a double-headed DQN with -greedy exploration does not account for the improved exploration seen with parameter space noise. The study compares parameter space noise with action space noise in training policies for various games, showing that parameter space noise outperforms action space noise in consistency-demanding games. Proposed improvements to DQN like double DQN, prioritized experience replay, and dueling networks are mentioned as potential enhancements. In evaluating performance on continuous control tasks, parameter space noise outperforms other exploration schemes, achieving higher returns and breaking out of sub-optimal behaviors. This is particularly evident in the HalfCheetah environment, where parameter space noise significantly outperforms correlated action space noise. Parameter space noise performs well on most environments, even without noise present, indicating well-shaped reward functions. In Walker2D, adding parameter noise reduces performance variance between seeds, aiding in escaping local optima. Parameter noise enables RL algorithms to learn on environments with sparse rewards, where uncorrelated action noise fails. The agent navigates a chain of states with varying rewards. Different DQN algorithms are compared on this task with varying chain lengths. The goal is to achieve the optimal return within a certain number of episodes. Results are plotted to show the median number of episodes before the problem is considered solved. Green indicates success, while blue indicates failure within 2 thousand episodes. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN in a simple environment where the optimal strategy is always to go right. However, in more complex environments where the optimal action depends on the state, parameter space noise may not work as well. The results highlight the exploration behavior difference compared to action space noise in this specific case, but parameter space noise does not guarantee optimal exploration in general. In rllab environments, SparseCartpoleSwingup, SparseDoublePendulum, SparseHalfCheetah, SparseMountainCar, and SwimmerGather are considered. DDPG and TRPO are used to solve these tasks with a time horizon of T = 500 steps. SparseDoublePendulum is relatively easy for DDPG, while SparseCartpoleSwingup and SparseMountainCar results are more intriguing. Parameter space noise can improve exploration behavior in off-the-shelf algorithms, as shown in experiments with SparseCartpoleSwingup, SparseMountainCar, and SparseHalfCheetah tasks. Evolution strategies also use noise in parameter space for exploration but lack temporal information. Combining parameter space noise with traditional RL algorithms can enhance exploration while incorporating temporal information. Parameter space noise can enhance exploration behavior in off-the-shelf algorithms by incorporating temporal information. Comparing ES and traditional RL with parameter space noise directly, DQN with parameter space noise outperforms ES on 15 out of 21 Atari games, demonstrating a combination of exploration properties and sample efficiency. In the context of deep reinforcement learning, various techniques have been proposed to improve exploration, but they are often computationally expensive. Perturbing policy parameters has been suggested as a way to enhance exploration, showing better performance than random exploration in some cases. However, previous studies have limitations in terms of low-dimensional policies and state spaces. Our method is applied and evaluated for both on and off-policy settings, using high-dimensional policies and environments with large state spaces. It is closely related to evolution strategies and aims to improve exploration by perturbing network parameters directly, leading to more efficient exploration compared to traditional methods like Bootstrapped DQN. Parameter space noise is proposed as a replacement for traditional action space noise in deep RL algorithms like DQN, DDPG, and TRPO. It has shown improved performance in solving environments with sparse rewards compared to action noise. This approach perturbs network parameters directly for more efficient exploration. The network architecture includes ReLUs in each layer, layer normalization in the fully connected part, and a policy network with a softmax output layer. Target networks are updated every 10K timesteps, trained using Adam optimizer with a learning rate of 10^-4 and a batch size of 32. The replay buffer can hold 1M state transitions. Parameter space noise is scaled for a similar effect in action space, ensuring a softly enforced maximum KL divergence between perturbed and non-perturbed policies. The policy is perturbed at the start of each episode, with the standard deviation adapted every 50 timesteps. The network architecture includes ReLUs in each layer, layer normalization in the fully connected part, and a policy network with a softmax output layer. Target networks are updated every 10K timesteps, trained using Adam optimizer with a learning rate of 10^-4 and a batch size of 32. The replay buffer can hold 1M state transitions. Parameter space noise is scaled for a similar effect in action space, ensuring a softly enforced maximum KL divergence between perturbed and non-perturbed policies. The policy is perturbed at the start of each episode, with the standard deviation adapted every 50 timesteps. To avoid getting stuck, -greedy action selection with = 0.01 is used, along with 50 K random actions to collect initial data for the replay buffer before training starts. Observations are down-sampled to 84 \u00d7 84 pixels and converted to grayscale, with a concatenation of 4 subsequent frames sent to the network. For DDPG, a similar network architecture as described by BID18 is used, with 2 hidden layers of 64 ReLU units each for both the actor and critic. Layer normalization (BID2) is applied to all layers, and target networks are soft-updated with \u03c4 = 0.001. The critic is trained with a learning rate of 10^-3 while the actor uses a learning rate of 10^-4. The critic and actor in the network architecture are updated using Adam optimizer with different learning rates. The critic is regularized with an L2 penalty, and the replay buffer holds 100K state transitions. Parameter space noise is scaled for action space, with different noise levels for dense and sparse environments. TRPO uses a step size of \u03b4 KL = 0.01 and specific network architectures for different tasks. The baseline is a learned linear transformation of observations, and various environments from OpenAI Gym are utilized. OpenAI Gym environments like Swimmer and SparseDoublePendulum are used for training agents with DQN. The observation encoding follows a proposed method, and a simple network with layer normalization is used. Agents are trained for up to 2K episodes with varying chain lengths and seeds for evaluation. Performance is assessed by sampling trajectories with noise after each episode. The current policy performance is evaluated by sampling trajectories with noise disabled. The problem is solved if one hundred subsequent trajectories achieve the optimal return. Different DQN methods are compared, including adaptive parameter space noise DQN and bootstrapped DQN with 20 heads. The network is trained using the Adam optimizer with specific parameters. The expected return in parameter space can be expanded using likelihood ratios and the reparametrization trick for N samples. To address the issue of picking a suitable scale for parameter space noise, a proposed adaption method is used to re-scale as needed. This method involves adapting the scale of parameter space noise over time using a time-varying scale \u03c3 k, which is updated every K timesteps. The scale of parameter space noise is adapted over time using a time-varying scale \u03c3 k, updated every K timesteps. A heuristic based on the Levenberg-Marquardt method is used to update \u03c3 k, with a distance measure d(\u00b7, \u00b7) between non-perturbed and perturbed policies. The choice of \u03b4 and \u03b1 = 1.01 depends on the policy representation, with different measures for methods like DDPG, TRPO, and DQN. In DQN, the policy is implicitly defined by the Q-value function, requiring a probabilistic formulation to account for changes in Q-values. The probabilistic formulation of policies uses the softmax function over predicted Q values to measure the distance in action space. This approach normalizes Q-values and avoids the need for an additional hyperparameter \u03b4. The KL divergence between greedy and \u03b5-greedy policies is used to compare the two approaches. The KL divergence is used to relate action space noise and parameter space noise by adaptively scaling \u03c3. For DDPG, noise induced by parameter space perturbations is related to additive Gaussian noise. The distance measure between non-perturbed and perturbed policy is used to set an adaptive parameter space threshold. TRPO scales noise by computing a trust region around the noise direction to keep the perturbed policy close to the non-perturbed version. The conjugate gradient algorithm is used with a line search along the noise direction to ensure constraint conformation in the context of adaptive parameter space noise. Learning curves for 21 Atari games are provided, comparing the final performance of ES and DQN. Results show that adaptive parameter space noise achieves stable performance on InvertedDoublePendulum, with overall performance comparable to other exploration approaches. The performance of TRPO with noise scaled according to the parameter curvature is shown in FIG10. Results indicate that adding parameter space noise aids in learning more consistently on challenging sparse environments."
}