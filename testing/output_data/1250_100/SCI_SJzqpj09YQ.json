{
    "title": "SJzqpj09YQ",
    "content": "Spectral Inference Networks is a framework for learning eigenfunctions of linear operators through stochastic optimization. It generalizes Slow Feature Analysis to various operators and is related to Variational Monte Carlo methods. This tool can be used for unsupervised representation learning from video or graph data. Training Spectral Inference Networks is formulated as a bilevel optimization problem, enabling online learning of multiple eigenfunctions. Results show accurate recovery of eigenfunctions and interpretable representations from video in an unsupervised manner. Spectral algorithms are essential in machine learning and scientific computing. In applications, solving for the eigenfunction of a linear operator is crucial for studying PDEs and understanding classical and quantum systems. Full eigendecomposition can be achieved efficiently for reasonably-sized matrices, while iterative algorithms are used for larger matrices. In cases where eigenvectors cannot be explicitly stored in memory, approximations are made using a fixed number of points and the Nystr\u00f6m method. This method involves evaluating a kernel between new and training set points. In this paper, the authors propose Spectral Inference Networks (SpIN) to approximate eigenfunctions of linear operators in high-dimensional function spaces using neural networks. They demonstrate training these networks through bilevel stochastic optimization, showing success in solving problems in quantum physics and extracting interpretable representations from video data. The method significantly advances unsupervised learning without a generative model and has potential for various applications of spectral methods. The paper includes a review of related work on spectral learning and stochastic optimization, as well as defining the objective function for Spectral Inference Networks. The paper introduces Spectral Inference Networks (SpIN) for approximating eigenfunctions of linear operators using neural networks. It discusses the algorithm for training SpIN through bilevel optimization and presents experiments and future directions. Spectral methods are widely used in various machine learning applications, such as spectral clustering and Laplacian eigenmaps. Spectral algorithms can also be applied for estimating parametric models like hidden Markov models. Markov models and latent Dirichlet allocation are used for computing the SVD of moment statistics. Spectral decomposition of predictive state representations is proposed for learning a coordinate system of environments in reinforcement learning. Proto-value functions (PVFs) are eigenfunctions of symmetric transition functions and have been proposed for discovering subgoals in reinforcement learning. Stochastic PCA has a long history, with Oja's rule being one of the earliest works in this area. The text discusses the importance of learning eigenfunctions over a large space efficiently, contrasting with traditional fixed-size eigenvector approaches. It mentions Slow Feature Analysis (SFA) as a related work, highlighting its application in training shallow or linear models. Unlike SFA, SpIN allows for simultaneous learning of all eigenfunctions, which is beneficial in an online setting. The integration of spectral methods and deep learning is also briefly touched upon. The text discusses the integration of spectral methods and deep learning, highlighting the use of spectral networks in neural network architectures. It also mentions the application of spectral decompositions in training implicit generative models like GANs. Additionally, it touches on Variational Quantum Monte Carlo methods for approximating eigenfunctions in computational physics. Neural networks have been used for calculating ground and excited states in computational physics. Stochastic optimization for Variational Quantum Monte Carlo (VMC) methods dates back to Harju et al. (1997). Eigenvectors of a matrix A can be defined as vectors u such that Au = \u03bbu, with the largest eigenvector being the solution of the Rayleigh quotient. Minimizing the Rayleigh quotient can find the lowest eigenvector of A. The Rayleigh quotient is used to find eigenvectors of a matrix A. Algorithms like power iteration can converge to the global solution despite it being a nonconvex problem. By solving maximization problems, we can compute the top N eigenvectors. If A and u are too large, a symmetric kernel k(x, x ) can be used instead. The inner product on \u2126 is defined with respect to a probability distribution with density p(x). To compute a function spanning the top N eigenfunctions, we solve an optimization problem over functions u : \u2126 \u2192 R N. The objective is Tr(\u03a3 \u22121 \u03a0), where \u03a3 and \u03a0 are empirical estimates of covariance quantities. The kernel k simplifies to Eq. 6, penalizing differences between neighbors. For adjacent video frames, this is Slow Feature Analysis (SFA), a special case of SpIN. The algorithm allows for online learning of SFA with arbitrary function approximators. The Laplace-Beltrami operator generalizes to generic manifolds, replacing the double expectation over x and x with a single expectation. In optimizing the quotient in Eq. 6, the approach focuses on directly optimizing the function that spans the top N eigenfunctions of K, rather than using constrained optimization methods or constructing an orthonormal function basis. This approach is chosen due to the unknown distribution p(x) and the need for methods that work for large discrete spaces. The approach focuses on optimizing the function spanning the top N eigenfunctions of K by masking the flow of information from the gradient, allowing for simultaneous learning of all eigenfunctions in order. This method avoids the need for constrained optimization or orthonormal function basis due to the unknown distribution p(x) and the requirement for methods applicable to large discrete spaces. The approach optimizes the top N eigenfunctions of K by masking the gradient flow, enabling simultaneous learning of all eigenfunctions. This method eliminates the need for constrained optimization or orthonormal function basis, suitable for large discrete spaces with unknown distributions. The closed form expression for the masked gradient is provided, along with a TensorFlow implementation in the supplementary material. Learning Spectral Inference Networks is challenging due to biased gradient estimates, requiring a bilevel optimization approach. Bilevel stochastic optimization involves solving two coupled minimization problems simultaneously in machine learning. This approach optimizes functions on two timescales to converge to local minima. It is commonly used in actor-critic methods, generative adversarial networks, and imitation learning. Spectral Inference Networks involve solving a bilevel problem by using moving averages to estimate random variables. This approach converts the problem into a two-timescale update by updating moving averages and parameters alternately. This method combines various elements to define what a Spectral Inference Network is in machine learning. Spectral Inference Networks involve minimizing an objective function through stochastic optimization over deep neural networks. The algorithm for training these networks includes computing explicit estimates of the Jacobian of the covariance, which can be a bottleneck in scaling. Proper learning rate schedules are crucial for stochastic optimization theory, but often not utilized in deep learning practices. In deep learning, rate schedules are rarely used as asymptotic convergence is less important than reaching a local minimum. Constant learning rates are often sufficient for good performance, as demonstrated in experiments with quantum mechanics problems and unsupervised feature learning from video. An example includes using SpIN to solve the Schr\u00f6dinger equation for a two-dimensional hydrogen atom. In deep learning, rate schedules are rarely used as asymptotic convergence is less important than reaching a local minimum. Constant learning rates are often sufficient for good performance. An example includes using SpIN to solve the Schr\u00f6dinger equation for a two-dimensional hydrogen atom. The Hamiltonian operator is set, and a neural network is trained to approximate the wavefunction \u03c8(x) with different energy levels. Results show circular harmonics representing electron orbitals of hydrogen in two dimensions. The eigenfunctions and eigenvalues of hydrogen orbitals in two dimensions were inaccurately estimated without bias correction. However, with the bias correction term in SpIN, accurate estimation of eigenfunctions and convergence to true eigenvalues were achieved. The high accuracy of learned eigenvalues supports the effectiveness of the method. Moving forward, SpIN was applied to representation learning in vision using a convolutional neural network on a video example with bouncing balls. The model trained with 12 output eigenfunctions showed that most eigenfunctions encode for the position of balls independently, with higher eigenfunctions capturing higher frequency combinations. Some eigenfunctions also encode complex joint statistics of position, such as one eigenfunction with no clear relationship with the marginal position of a ball. The eigenfunctions discovered in the model encode complex joint relationships of ball positions, including whether all balls are crowded in one corner or spread out. These nonlinear features could not be detected by a shallow model and higher eigenfunctions may capture even more intricate relationships. The eigenfunctions do not seem to encode meaningful information about velocity due to rapid changes caused by collisions. The framework allows for spectral decompositions through stochastic gradient descent, enabling learning over high-dimensional spaces without the Nystr\u00f6m approximation. This extends unsupervised learning criteria without a generative model. The work introduces a method for unsupervised learning using slowness as a criterion and addresses biased gradients in finite batch sizes. It discusses the need to compute full Jacobians at each time step and suggests improving training scaling for future research. The application in physics is on a simple system, with potential for more complex systems. The learned representations from video data show structure and sensitivity, useful for tasks like object tracking and gesture recognition. The framework is general, with room to explore various kernels and architectures. The text discusses breaking the symmetry between eigenfunctions by optimizing a function that spans the top K eigenfunctions. It explains the approach of using masked gradients to recover ordered eigenfunctions and provides a derivation for the masked gradient expression. The process involves transforming functions into ordered eigenfunctions by orthogonalizing them and diagonalizing the symmetric matrix. The goal is to optimize the function while accumulating statistics to transform it into the desired form. The text discusses the struggles with numerical errors in eigenfunctions and introduces the masked gradient approach for improved numerical robustness. It explains the Cholesky decomposition of a matrix and its relation to positive-definite matrices. The goal is to provide an understanding of the method's correctness for optimization over a wide range of functions. The Cholesky decomposition of \u03a3 1:n,1:n is discussed, showing that the upper left block of the inverse of a lower triangular matrix can be written as DISPLAYFORM2 DISPLAYFORM3. The argument proceeds by induction, highlighting the importance of maximizing \u039b (n+1)(n+1) for optimal u n+1 (x). Orthogonalizing u 1:n+1 (x) by multiplication by L \u22121 1:n+1,1:n+1 is also mentioned. Orthogonalizing u 1:n+1 (x) by multiplication by L \u22121 1:n+1,1:n+1 will subtract anything in the span of the first n eigenfunctions off of u n+1 (x), making v n+1 (x) the (n + 1)th eigenfunction of K. The gradients of \u03a0 and \u03a3 with respect to u are given elementwise, allowing for the computation of gradients in closed form. To zero out relevant elements of the gradient \u2207 u \u039b kk, right-multiply by \u2206 k. Masked gradients can be expressed in closed form using triu and diag functions. TensorFlow implementation of masked gradient is provided in pseudocode. SpIN is a function in TensorFlow for learning, involving moving averages and masked gradients. It utilizes operations like tf.cholesky and tf.linalg.inv to handle gradients and matrices efficiently. To solve for the eigenfunctions with lowest eigenvalues, a neural network with 2 inputs, 4 hidden layers of 128 units, and 9 outputs was used. The batch size was 128, much smaller than the 16,384 nodes in the 2D grid for the exact eigensolver solution. A softplus nonlinearity log(1 + exp(x)) was chosen over the more common options. For the experiments, a softplus nonlinearity log(1 + exp(x)) was chosen over ReLU due to the Laplacian operator. RMSProp with a decay rate of 0.999 and learning rate of 1e-5 was used. Points were sampled uniformly at random from a box during training. The output of the network was multiplied by a factor to enforce zero boundary conditions. The finite difference approximation of the Laplacian was used with a small number. The neural network architecture was modified to aid in separating eigenfunctions. The network architecture had a block-sparse structure with overlapping blocks for each eigenfunction, allowing shared features in lower layers and distinct features in higher layers. Trained on 200,000 64x64 pixel frames, the network had 3 convolutional layers with 32 channels, 5x5 kernels, and a fully-connected layer with 128 units outputting 12 eigenfunctions. A constant first eigenfunction was added for zero-mean features. The weights had sparsity between entire feature maps for convolutional layers. Trained with RMSProp at a learning rate of 1e-6. The network was trained with RMSProp using a learning rate of 1e-6 and decay of 0.999. The covariance decay rate was set to \u03b2 = 0.01 for 1,000,000 iterations. The training involved using 24 clips of 10 consecutive frames per batch. The goal was to minimize the difference between consecutive frames in the input and the network's predicted features. Another network was trained to compute successor features and eigenpurposes using PCA on held-out frames. The same convolutional network architecture was used with a batch size of 32 and RMSProp with a learning rate of 1e-4 for 300k iterations. The network was trained with RMSProp using a learning rate of 1e-4 for 300k iterations, updating the target network every 10k iterations. Mean-centering successor features improved results. A spectral inference network was trained with the same architecture as the encoder of the successor feature network. SpIN was tested on 64k held-out frames. Comparison against PCA on pixels was done by averaging frames with largest activation for each eigenfunction/eigenpurpose. The SpIN model can encode features such as the presence/absence of sprites or different arrangements of sprites in a clear distinction between top and bottom rows. Unlike successor features, SpIN learns these features in an end-to-end fashion without pixel reconstruction loss. Future research will focus on exploring the usefulness of these features for faster reward accumulation through exploration strategies."
}