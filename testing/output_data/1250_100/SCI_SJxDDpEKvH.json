{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for transformations remains challenging without supervision. A non-statistical framework based on modular organization of the network is proposed for targeted interventions on image datasets. This allows for computationally efficient style transfer and robustness assessment in pattern recognition systems. Efforts have been made to design realistic images using Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) to produce disentangled latent representations. However, current models lack a mechanistic or causal interpretation of image properties. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds. This capability aligns with human representational abilities and the modular organization of the visual system. In this paper, a causal framework is proposed to explore modularity in generative models, aiming to assess how well they capture causal mechanisms. The framework relates to the principle of Independent Mechanisms, allowing for direct interventions in the network without affecting other mechanisms. This approach can help in understanding how outcomes would change with modifications to individual causal mechanisms. In this study, a causal framework is used to analyze the modularity of generative models by assessing the impact of different variables on outcomes. The research focuses on the disentanglement of internal variables in deep generative models and how changes in these variables affect the overall functioning of the models. The study demonstrates how VAEs and GANs trained on image databases exhibit modularity in their hidden units, allowing for counterfactual editing of generated images. The work also discusses the interpretability of convolutional neural networks in generative models, highlighting the need for a different approach due to the high-dimensional nature of intermediate representations. Various models like InfoGANs, \u03b2-VAEs, and other works are mentioned in the context of supervised learning. The curr_chunk discusses the disentanglement of latent variables in networks, introducing the concept of intrinsic disentanglement. It contrasts with other approaches like interventions on internal variables and group representation theory. The focus is on arbitrary continuous transformations, free from the strong requirements of representation theory. The interventional approach to disentanglement in a classical graphical model setting is also mentioned. The curr_chunk focuses on extrinsic disentanglement in a graphical model setting, developing measures of interventional robustness based on labeled data. It introduces a framework to define disentanglement and connects it to causal concepts, with details provided in Appendix A. The generative model M maps a latent space Z to a manifold Y M where data points live. The term representation refers to a mapping from Y M to a representation space R, with M being the latent representation of the data. The curr_chunk introduces a causal generative model (CGM) for mapping latent representations through a series of operations. It involves selecting endogenous variables in a causal graph to compute the mapping. The variables can include output activation maps of hidden layers in a neural network. The model ensures that the mapping is left-invertible, defining the internal representation of the network. The CGM framework defines unit-level counterfactuals by replacing subset variables with assignments. Counterfactuals transform the output of the generative model and introduce faithfulness in the mapping of interventions on internal variables. In the context of generative models, non-faithful counterfactuals can lead to outputs outside the learned distribution, affecting downstream neurons. The concept of disentangled representation involves sparse encoding of real-world transformations, driving supervised approaches to manipulation of latent variables. Unsupervised learning approaches aim to learn these transformations from unlabeled data, with SOTA methods focusing on encoding transformations through changes in latent factors. The statistical approach to disentanglement in generative models faces challenges such as confounding factors and ill-posed problems. Current unsupervised methods struggle with real-world datasets, showing lower visual quality compared to non-disentangled models. In our work, we propose a non-statistical definition of disentanglement by mathematically defining transformation-based insights. Disentanglement involves transforming the latent space to act on a single variable while leaving others available for encoding other properties. This notion, called extrinsic disentanglement, follows the causal principle of independent mechanisms. It is agnostic to subjective property choices and relies on transformations of the latent representation. The functional definition of disentanglement is agnostic to subjective property choices and statistical independence. It requires a different representation to uncover related properties that are disentangled according to this definition. The extension of disentanglement allows transformations of internal variables to be intrinsically disentangled with respect to a subset of endogenous variables. The internal representation space is transformed by T, affecting variables indexed by E. For faithful counterfactuals, E and its complement E should not share latent ancestors. Modularity is defined as a structural property allowing arbitrary disentangled transformations. If E is modular, any transformation within its input domain is disentangled. This framework links disentanglement with the network's intrinsic property, defining a disentangled representation. The trained network defines a disentangled representation by partitioning the intermediate representation into modules. This partition allows for valid transformations in the data space. The concept of representation may require grouping neurons into modules at a \"mesoscopic\" level for independent intervention. This insight is relevant for artificial and biological systems. The trained network partitions the intermediate representation into modules, allowing for valid transformations in the data space. Assigning a constant value to endogenous variables defines counterfactuals, aiming for faithful ones by constraining the value. Sampling from the marginal distribution of the variables in E avoids characterizing VEM. The hybridization procedure involves taking two independent examples of the latent variable z1 and z2 to generate original output examples. The network partitions the intermediate representation into modules for valid transformations. Original output examples are generated by taking two independent latent variables z1 and z2. The counterfactual hybridization framework assesses the causal effect of module E on the generator's output. The approach estimates an influence map by calculating the mean absolute effect of unit-level causal effects in the potential outcome framework. The result is a grayscale heat-map pixel map averaged across color channels, with a scalar quantity defining the magnitude of the causal effect. A fine to coarse approach is used to select subsets to intervene on in networks with a large number of units or channels per layer. The approach uses a fine to coarse approach to group influence maps by similarity, defining modules at a coarser scale. Representative EIMs for channels of convolutional layers suggest functional segregation, with some influencing finer face features and others affecting the background. Clustering of channels is done using EIMs as feature vectors, pre-processing each map by smoothing spatially and thresholding. The approach involves smoothing and thresholding influence maps to create binary images, followed by Non-negative Matrix Factorization (NMF) to extract cluster template patterns. NMF is chosen for its ability to isolate meaningful image components. The method will be compared to k-means clustering, and a toy generative model is introduced to further justify the NMF approach. The model consists of vector variables V k with matrices W k mapping endogenous variables to the output. Model parameters are randomly chosen with specific conditions on coefficients and indices, enforcing assumptions about image influences. The identifiability result is obtained for this model. The model consists of vector variables V k with matrices W k mapping endogenous variables to the output. The partition of the hidden layer corresponds to a disentangled representation, justifying the use of NMF for generating a binary matrix summarizing significant influences. Investigating modularity of generative models on the CelebA dataset, a basic \u03b2-VAE architecture was used. The full procedure included EIM calculations and clustering of channels into modules. The model involves EIM calculations, clustering channels into modules, and hybridizing generator samples. Setting the number of clusters to 3 resulted in interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed the robustness of the clustering with 3 clusters. The results show that clustering with 3 clusters is optimal, with high consistency (>90%) and outperforming k-means. The average cosine similarity between matching clusters is 0.9, indicating robust clustering. Influence maps reveal spread across image locations. Hybridizing the 3 modules maintains image structure without introducing discontinuity. The study explores the effectiveness of \u03b2-VAE for extrinsic disentanglement, but suggests it may not be optimal compared to other approaches. Further investigation is needed to determine if better extrinsic disentanglement could also benefit intrinsic disentanglement in models not explicitly designed for disentanglement. The findings were replicated in a DCGAN implementation, indicating the approach can be applied to models not optimized for disentanglement. The study tested a pretrained BEGAN model for generating high-resolution face images. Interventions on specific layers showed selective transfer of features between images. The model, trained on tightly cropped face images, demonstrated clear hair transfer and encoding of different facial features in separate modules. The study evaluated the quality of counterfactual images generated by a hybridization procedure using the Frechet Inception Distance. They tested the scalability of their approach on a BigGAN-deep architecture pretrained on the ImageNet dataset, showing the ability to generate hybrids by mixing features of different classes. The study evaluated the quality of counterfactual images generated by a hybridization procedure using the Frechet Inception Distance. Intervening on two successive layers within a Gblock was found to be effective in generating high-quality counterfactuals. Examples in Fig. 4 demonstrate the ability to create meaningful combinations of different objects. The generated counterfactual images were used to probe and improve the robustness of classifiers to contextual changes. Comparisons were made with several state-of-the-art pretrained classifiers available on Tensorflow-hub. The study evaluated the quality of counterfactual images generated by a hybridization procedure using the Frechet Inception Distance. High recognition rates were observed when intervening at layers closest to the output. Inception resnet outperformed other classifiers at intermediate blocks 5-6. Different classifiers rely on different aspects of image content for decision-making. A mathematical definition of disentanglement was introduced for unsupervised characterization of representation in deep generative architectures. Interpretable modules of internal variables were found in generative models trained on real-world datasets. The research explores the use of generative architectures for tasks such as style transfer and assessing object recognition systems' robustness. It aims to enhance interpretability and expand the use of deep neural networks beyond their original training. The approach could lead to more sustainable AI research in the future by leveraging trained generator architectures as mechanistic models. Structural causal models are used to represent these architectures, allowing for independent manipulation of model parts. The research explores the use of generative architectures for tasks like style transfer and assessing object recognition systems' robustness. It aims to enhance interpretability and expand the use of deep neural networks. Structural causal models are utilized to represent these architectures, enabling independent manipulation of model components. A Causal Generative Model (CGM) is introduced to capture computational relations between input latent variables, generator output, and endogenous variables forming an intermediate representation. This model can decompose the generator's output into two successive steps in a feed-forward neural network. The research introduces a Causal Generative Model (CGM) to represent generative architectures for tasks like style transfer and object recognition. The CGM decomposes the generator's output into two successive steps in a feed-forward neural network, involving endogenous variables and latent inputs. This model aligns with the definition of a deterministic structural causal model by Pearl (2009) and guarantees basic properties found in existing generative networks. The computational graph of existing generative networks ensures unambiguous assignment of endogenous variables once latent variables or a subset of endogenous variables are chosen. The latent and endogenous mappings define mappings from latent variables and endogenous variables to the output, respectively, constrained to subsets of their ambient space. The CGM is considered embedded when these mappings are proper embeddings. An embedded CGM is defined as a computational graph model where mappings from latent and endogenous variables to the output are proper embeddings. The image sets of the model are constrained by its parameters and should approximate the data distribution. The goal is to match the output with the target data distribution by manipulating properties respecting the topology of the output. An embedded CGM is a computational graph model where mappings from latent and endogenous variables to the output are proper embeddings. The model's image sets are constrained by its parameters to approximate the data distribution. Injectivity of the function gM is a key requirement for embedded CGMs, especially when the compactness of the latent space is considered. This framework allows for defining counterfactuals in the network following Pearl (2014). The interventional CGM M h is defined by replacing structural assignments for V |E with assignments {V e k := h k (z)} e k \u2208E. Counterfactuals induce a transformation of the generative model's output, relating to disentanglement. Intrinsic disentanglement in a CGM M involves a transformation T of endogenous variables that only affects variables indexed by E for any latent z \u2208 Z. Intrinsic disentanglement in a CGM involves a transformation T of endogenous variables that only affects variables indexed by E for any latent z \u2208 Z. This definition corresponds to the unambiguous assignment of Y based on endogenous values. The split node in Fig. 2d illustrates the notion of disentanglement, where V 2 is computed as in the original CGM before applying transformation T 2. Counterfactuals represent perturbations that may be disentangled given their faithfulness. Armstrong (2013) states that a continuous and injective g M is an embedding if Z is compact and the codomain of g M is Hausdorff. The text discusses the disentanglement of transformations in a CGM, emphasizing the importance of faithful mappings and unambiguous assignments of variables. It also highlights the relationship between endogenous variables and hidden layers in creating a disentangled representation. The structure of the model parameters and the i.i.d. assumption play a crucial role in ensuring modular subsets of variables. The \u03b2-VAE architecture, similar to DCGAN, ensures an injective mapping of model parameters and counterfactual hybridization of V k components. The conditions on I k and thresholding guarantee a rank K binary factorization of matrix B. The uniqueness of this factorization is guaranteed by classical NMF identifiability results. The architecture consists of three blocks of convolutional layers with skip connections for increased performance. The pretrained BigGan-deep model from Tensorflow-hub consists of ResBlocks with skip connections for sharp image generation. The architecture includes BatchNorm-ReLU-Conv Layers and upsampling transformations. Influence maps are generated by a VAE on the CelebA dataset, showing variance and perturbation influence. FID analysis of BEGAN hybrids measures distances between real and generated data clusters. The distances between real and generated data clusters are computed for hybrids created by intervention on cluster k. Hybrids show a small distance to generated data and each other, suggesting visually plausible images. Entropy values vary based on the quality of the module, with hybrids from Gblock 4 showing well-rendered object textures influencing classifier decisions. The study investigates the use of intervention procedures to assess classifier robustness by creating hybrids between different classes. The hybrids show a mix of shape properties, with high entropy indicating successful interventions. Discriminative models are tested on koala+teddy hybrids, resulting in teddy bears in a koala context. The study aims to evaluate the classifiers' ability to distinguish between these novel hybrids. The study evaluates classifier robustness by creating hybrids between different classes, such as koala+teddy hybrids. The classifiers must be sensitive to the object present in the scene, not the contextual information, to classify a teddy bear correctly even in a koala environment. Nasnet large is shown to be more robust to context changes compared to other classifiers."
}