{
    "title": "HyGBdo0qFm",
    "content": "Alternatives to recurrent neural networks, such as architectures based on attention or convolutions, are gaining popularity for processing input sequences. The computational power of these alternatives, like the Transformer and Neural GPU, has been studied. Both models are shown to be Turing complete based on their ability to compute and access internal representations of data, without needing external memory. Designing neural network architectures capable of learning algorithms is of interest, with a key requirement being Turing completeness for implementing arbitrary algorithms. The work by Siegelmann & Sontag (1995) established that recurrent neural networks (RNNs) are Turing complete with a bounded number of resources. This is based on RNNs' ability to compute internal representations of data and access them, releasing their full computational power without increasing complexity. Many early neural architectures are extensions of RNNs and are Turing complete. Recent trends show the benefits of designing networks that manipulate sequences without direct recurrence. In this work, the Turing completeness of the Transformer and Neural GPU models is explored based on their ability to compute and access internal representations of data without the need for external memory. The study demonstrates that both models are Turing complete for bounded architectures, using rational numbers for internal activations. The Transformer is shown to simulate a Turing machine directly, while the Neural GPU achieves Turing completeness by simulating standard sequence-to-sequence RNNs. The research also identifies the minimal elements required to achieve Turing completeness in these models. The study compares the computational power of Transformers and Neural GPUs, providing a formal approach to the comparison. Previous work has established the Turing completeness of neural networks with linear connections. There is a growing interest in enhancing RNNs with mechanisms like the Neural Turing Machine to support learning algorithms. The Neural Turing Machine (NTM) and other architectures like Stack-RNN and Transformer extend RNNs with memory for improved performance on language-processing tasks. While the Transformer architecture has achieved state-of-the-art results, there is a need to enrich its architecture to learn general procedures effectively. The original Transformer architecture struggles to generalize to input lengths not seen during training, but it is shown to be Turing complete based on different considerations, highlighting the differences between theory and practice. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors, capable of learning decimal multiplication end-to-end. It is argued to be Turing complete, similar to cellular automata, despite using a bounded number of cells during computation. The Neural GPU architecture utilizes dense representations of cells to achieve Turing completeness without needing to add extra cells beyond those used for input storage. Rational numbers of arbitrary precision are used for weights and activations, along with rational functions with rational coefficients. The piecewise-linear sigmoidal activation function \u03c3 : Q \u2192 Q is predominantly employed. The focus is on sequence-to-sequence neural network architectures. The seq-to-seq network architecture formalizes the process of generating output sequences based on input sequences. It involves using a seed vector and a fixed number of output vectors, rather than relying on an end of sequence mark for termination. This network can be seen as a language recognizer for strings, with specific criteria for accepting a string based on the generated output sequence. The language accepted by a recognizer A, denoted as L(A), consists of all accepted strings. The embedding function f: \u03a3 \u2192 Qd and set F should be computable in linear time. These restrictions prevent cheating while allowing for meaningful embeddings and stopping criteria. The class N of seq-to-seq neural network architectures defines the class LN of languages accepted by recognizers using networks in N. The class of seq-to-seq neural network architectures, denoted as N, defines the languages accepted by recognizers using networks in N. Turing completeness is achieved by encoder-decoder RNNs, even with specific restrictions on parameters and functions. A formalization of the Transformer architecture is presented, focusing on mathematical abstraction rather than efficient implementation. The Transformer is based on the attention mechanism, with scoring functions for query, keys, and values. Various scoring and normalization functions can be used, such as additive or multiplicative attention. The normalization function softmax is commonly used, but hardmax is used in proofs. The hardmax function is used in proofs for the Transformer encoder, where it sets f hardmax (x i ) = 1 if x i is the maximum value, and f hardmax (x i ) = 0 otherwise. This function is used for normalization, and the single-layer encoder of the Transformer is defined with parameters Q(\u00b7), K(\u00b7), V (\u00b7), and O(\u00b7) from Q d to Q d. Residual connections are included in the encoder, and when specific functions are not important, Z = Enc(X) is used. The Transformer encoder is defined as the repeated application of single-layer encoders with two final transformation functions. The L-layer Transformer encoder is defined recursively. A single-layer decoder includes attention to an external pair of key-value vectors. The decoder layer is parameterized by four functions and is defined to generate a query to attend the external pair. The Transformer decoder is a repeated application of single-layer decoders, with a final transformation function applied to the decoded sequence. The output is a single vector z \u2208 Q d. The Transformer network receives an input sequence X, a seed vector y 0, and produces a sequence Y = (y 1, ..., y r). The Transformer is order-invariant, producing the same output for permutations of input sequences due to the attention function's properties. The Transformer network is order-invariant, but positional encodings are used to include information about the order of the input sequence. Regular languages recognized by finite automata may not all be recognized by a Transformer network due to order-invariance. The Transformer satisfies a stronger invariance property called proportion invariance, which shows that not all regular languages recognized by finite automata can be expressed by a Transformer. The computational power of Transformer networks without positional encoding is limited, as they cannot recognize order-invariant regular languages. However, Transformer networks can recognize non-regular languages, showcasing their ability to express counting properties beyond regularity. The inclusion of positional encodings significantly impacts the capabilities of Transformer networks. The inclusion of positional encodings in Transformer networks radically changes the input representation for both the encoder and decoder. The positional encoding must be computable in linear time with respect to the size of the input. Transformers with positional encodings are proven to be complete, allowing them to simulate the execution of Turing machines. The input string is represented as a sequence of one-hot vectors with corresponding positional encodings, enabling the Transformer to process the input effectively. The construction of a transformer Trans M involves producing a sequence y i containing information about the state of M and the symbol under M's head at each time step. This is achieved by implementing M's transition function \u03b4 and storing relevant information using residual connections. The symbol at each index in time t + 1 is determined by the last symbol written by M at that index, allowing for effective processing of input sequences. The construction of a transformer Trans M involves producing a sequence y i containing information about the state of M and the symbol under M's head at each time step. This is achieved by implementing M's transition function \u03b4 and storing relevant information using residual connections. The decoder computation involves finding the maximum value i \u2264 t such that c (i ) = c (t+1) and copying v (i ) which is the symbol written by M at time step i. This process can be done with a self-attention layer, attending directly to position i and copying v (i ) which is s (t+1). The decoder also copies q (t+1) and s (t+1) into the output to construct y t+1. Various details in the construction include attending to the encoder at the beginning, setting a symbol as # when reaching an unvisited cell, and using feed-forward networks plus attention for implementation. The construction utilizes one encoder layer, three decoder layers, and vectors of dimension d = 2|Q| + 4|\u03a3| + 11 for storing one-hot representations of states, symbols, and additional working space. The general architecture closely follows Vaswani et al. (2017) but with different choices for functions and parameters. Hard attention is used instead of softmax, sin, and cos functions for positional encodings. Future work could explore arbitrary functions with restrictions like finite precision. The function O(\u00b7) in Equation (11) requires arbitrary precision for internal representations, crucial for storing and manipulating positional encodings. While arbitrary precision is standard in neural network studies, practical implementations often use fixed precision hardware. The Neural GPU BID11 architecture combines convolutions and gated recurrences over tridimensional tensors, parameterized by update, reset, and function functions. It produces a sequence of tensors recursively, with fixed precision hardware used in practical implementations. The Transformer with positional encodings and fixed precision is not Turing complete, but the computational power of fixed-precision Transformers can still be studied in the future. Neural GPUs combine convolutions and gated recurrences over tensors, resembling a gated recurrent unit. Functions U(\u00b7), R(\u00b7), and F(\u00b7) are defined as convolutions with a 4-dimensional kernel bank. The convolution operation updates cells based on neighbors' values, similar to cellular automata. Neural GPUs are modeled as a seq-to-seq architecture with input sequences arranged in a tensor. The output is generated by selecting a specific cell in the tensor. The number of parameters in a Neural GPU grows with the input size, leading to the introduction of uniform Neural GPUs to address this issue. Uniform Neural GPUs have a fixed architecture with a bias tensor of the same size as the input. The uniform Neural GPUs can be finitely specified with a constant number of parameters, making them Turing complete. By simulating a seq-to-seq RNN, the model achieves completeness. Using input tensor S, the encoder and decoder are defined by specific equations, allowing for the simulation of computations using kernel banks and bias tensors. The Neural GPUs use a gating mechanism for sequential updates of cells, ensuring that only certain positions are updated at each time step. This allows for simulating long computations by keeping vectors in D continuously updated. The model requires 3d + 3 components and utilizes kernels of shape (2, 1, d, d) to achieve Turing completeness. The detailed construction and correctness proof can be found in the appendix. The proof of Theorem 4.1 requires zero padding in convolution to differentiate internal cells from endpoints. Circular convolution leads to loss of Turing-completeness. Neural GPUs with circular convolutions cannot differentiate periodic sequences of different lengths, making them not Turing complete. Neural GPUs struggle with highly symmetric inputs like binary multiplication. Neural GPUs with circular convolutions are not Turing complete as they cannot differentiate periodic sequences of different lengths. Simplified Neural GPUs using piecewise linear activations and bidimensional input tensors achieve better results in training time and generalization. Theoretical evidence shows that these simplifications retain the full expressiveness of Neural GPUs while improving practical applicability. Analysis of Turing completeness in Transformer and Neural GPU architectures for sequence-processing tasks is presented, with plans for further refinement in the future. Our theoretical study investigates the essential features of +x i, +a i, +y i, and +p i summands in Equations (6-11) and the gating mechanism in Neural GPUs. We present abstract versions of both architectures, with different choices for functions and parameters compared to practical norms. While piecewise linear activations benefit Neural GPUs, further experimentation is needed for Transformers. Our results, though theoretical, may offer practical insights, such as the undecidability of certain problems in probabilistic language modeling with RNNs. The curr_chunk discusses the Turing completeness of RNNs and the implications for Transformers and Neural GPUs. It mentions the need for internal representations of arbitrary precision and plans for future theoretical studies on architectures with finite precision. The main idea of Siegelmann & Sontag's proof is briefly outlined. The curr_chunk discusses how RNNs can represent strings as rational numbers between 0 and 1 using base 4 encoding. This allows for simulating stack operations as affine transformations. A network N2 is constructed to simulate a two-stacks machine, with input provided as an initial state. N1 and N2 are combined to create a network N for processing input. The text discusses constructing a network N by combining N1 and N2 to simulate a two-stacks machine. Details include using an encoder-decoder RNN architecture, modifying the construction for biases, and ensuring a neuron in N2 switches to 1 at an accepting state. The text discusses extending the function PropInv to sequences of vectors and introducing a notation for simplifying the proof. It focuses on proving the equivalence of transformations for different inputs in the context of a constructed network simulating a two-stacks machine. The text discusses proving Proposition 3.1 by showing a property related to sequences of vectors. It involves equations and transformations within a constructed network simulating a two-stacks machine. The proof of Property FORMULA23 is completed by considering the encoder TEnc. It is shown that Att(q, K, V) = Att(q, K', V'). By induction on the layers of TEnc, it is proven that if xi = xj then ki = kj and vi = vj. This leads to a mapping for every i and j. Focusing on Att(q, K, V), it is shown that X \u2208 PropInv(X), which completes the proof. Additionally, Trans(X, y0, r) is defined recursively and proven to be equal using an inductive argument. The proof of Property FORMULA23 is completed by considering the encoder TEnc. It is shown that Att(q, K, V) = Att(q, K', V'). By induction on the layers of TEnc, it is proven that if xi = xj then ki = kj and vi = vj. This leads to a mapping for every i and j. Focusing on Att(q, K, V), it is shown that X \u2208 PropInv(X), which completes the proof. Additionally, Trans(X, y0, r) is defined recursively and proven to be equal using an inductive argument. On y 0, a language recognizer A using a Transformer network is constructed with a simple Transformer network with dimension d = 2 and one layer of encoder and decoder, such that L(A) = {w \u2208 {a, b}* | w has more symbols a than b}. The Transformer encoder and decoder functions are defined using self-attention and residual connections. External attention is also considered with arbitrary scoring and normalization functions. The proof involves introducing notation and showing that the network computes a sequence based on the input. The induction process is used to demonstrate the computation of the output sequence. The proof involves demonstrating how the Transformer network computes a sequence based on the input, using self-attention, external attention, and residual connections. By introducing notation and showing the computation process, it is shown that the network accepts a string if and only if a certain condition is met, completing the proof. The proof involves showing how a Turing machine can be simulated by a transformer network, ensuring certain assumptions are met. This construction involves using various values, sequences, and intermediate results to achieve the simulation. The construction and proof involve using values, sequences, and results to simulate a Turing machine with a transformer network. The encoder part processes input strings, while the decoder part simulates the machine's execution. The decoder computes values for each time step using self-attention and attention over the encoder. The construction involves simulating a Turing machine with a transformer network, using self-attention and attention over the encoder. The decoder computes values for each time step to mimic the machine's execution. The decoder computes values for each time step to mimic the machine's execution by producing q (i+1), v (i), and m (i) based on the input vector y i containing q (i) and s (i). To compute the symbol under the head of machine M at the next time step, two additional decoder layers are used. The symbol can be computed by attending to the encoder part and copying the value to y r+1. The decoder computes values for each time step to mimic the machine's execution by producing q (i+1), v (i), and m (i) based on the input vector y i. To compute the symbol under the head of machine M at the next time step, the index is computed using self-attention. The helping value (i) is defined as the last time in which M was pointing to the same cell. The decoder uses self-attention to compute values for each time step, mimicking the machine's execution by producing q (i+1), v (i), and m (i) based on the input vector y i. It determines the symbol under the machine's head at the next time step by calculating the index with self-attention. The value (i) represents the last time M pointed to the same cell, aiding in the computation process. The decoder utilizes self-attention to compute values for each time step, determining the symbol under the machine's head at the next time step. It selects the vector v j with hard attention based on the dot product q, k j to be as close to 0 as possible. The decoder uses hard attention to select the vector v j based on the dot product q, k j to be as close to 0 as possible. In the extreme case where all dot products are equal, attention behaves like an average of all value vectors. The vectors used in the Trans M layers are of dimension d = 2|Q|+4|\u03a3|+11, arranged in groups of values. Symbols in \u03a3 are represented by one-hot vectors in Q |\u03a3|. The construction of Trans M involves using embedding and positional encoding functions to represent input sequences. The encoder part of Trans M utilizes a single-layer encoder for TEnc M. The helping sequences are used to track the input string being read by M. The construction of Trans M involves using a single-layer encoder for TEnc M. The decoder part produces a sequence of outputs y1, y2, containing information about the state of M, the symbol under the head of M, and the direction of head movement. The architecture is described piece by piece, starting with a vector and assuming no head movement before time 0. The architecture of Trans M involves constructing a sequence of vectors y1, y2 from previous vectors y0 to yr. Using positional encodings, the input for the decoder's first layer is y0 + pos(1), y1 + pos(2), ..., yr + pos(r + 1). The first self-attention produces the identity, followed by attending over the encoder to obtain a vector a. The first decoder layer includes a two-layer feed-forward network and linear transformations to output the sequence z. The first decoder layer outputs sequence z, containing information about q(r+1) and m(r) needed for vector y(r+1). The symbol s(r+1) under machine M's head at time step r+1 is computed using two additional decoder layers. Lemma B.3 shows how values c(i) and c(i+1) can be represented for all possible indices i, including c(r+1) for the next time step. The third decoder layer is used to produce the desired s(r+1) value. The third decoder layer is utilized to generate the desired s(r+1) value by attending to position (i+1) and copying both values using functions Q3(\u00b7), K3(\u00b7), and V3(\u00b7) defined by feed-forward networks. This is achieved by leveraging the previously computed values c(i) and c(i+1) for every index i. The lemma is proven using intermediate notions and properties involving one-hot vectors constructed from enumerations of pairs in Q \u00d7 \u03a3. Three helping properties are then established for the transition function of machine M. The lemma is proven using intermediate notions and properties involving one-hot vectors constructed from enumerations of pairs in Q \u00d7 \u03a3. Three helping properties are then established for the transition function of machine M. There exists functions f1, f\u03b4, and f2. By denoting matrices S_i and vectors v(q,s), it is shown how to construct f1 using a piecewise-linear sigmoidal activation. To construct f1, a piecewise-linear sigmoidal activation is applied to obtain the desired result. Matrix M\u03b4 is defined to show (2), and functions f2 and f3 are also defined. The proof of the lemma involves constructing a function O1 using intermediate notions and properties involving one-hot vectors. The proof involves constructing functions Q3(\u00b7), K3(\u00b7), and V3(\u00b7) to compute the attention value, which is the average of all vectors in V2(Z). Linear transformations are defined by feedforward networks to find the value that maximizes the attention. It is shown that j = (i + 1) and V3(z2j) is the desired vector. The proof involves constructing functions Q3(\u00b7), K3(\u00b7), and V3(\u00b7) to compute the attention value, which is the average of all vectors in V2(Z). Linear transformations are defined by feedforward networks to find the value that maximizes the attention. It is shown that j = (i + 1) and V3(z2j) is the desired vector. To simplify the notation, we denote by \u03c7... (remaining text condensed) The proof involves constructing functions to compute the attention value, which is the average of all vectors in V2(Z). Linear transformations are defined by feedforward networks to find the value that maximizes the attention. It is shown that j = (i + 1) and V3(z2j) is the desired vector. To simplify the notation, we denote by \u03c7... The formulas of the Neural GPU in detail are as follows. Consider now an RNN encoder-decoder N of dimension d and composed of the equations with h 0 = 0 and g 0 = h n where n is the length of the input. We construct a Neural GPU network NGPU that simulates N as follows. Assume that the input of N is X = (x 1 , . . . , x n ). Then we first construct the sequence X = (x 1 , . . . , x n ) such that x i = [x i , 0, 0, 1, 1, 0] with 0 \u2208 Q d the vector with all values as 0. Notice that x i \u2208 Q 3d+3 , moreover it is straightforward that if x... The proof involves constructing functions to compute attention values using linear transformations defined by feedforward networks. The construction of kernel banks KU, KR, and KF is described for simulating an RNN with a bi-dimensional tensor. Matrices in KF are defined as block matrices, with properties proven for every t \u2265 0. The construction involves simulating an RNN with a bi-dimensional tensor using linear transformations defined by feedforward networks. The construction uses components to simulate encoder, decoder, and communication between them. Induction is used to prove the simulation process, with attention values computed for each step. The construction involves simulating an RNN with a bi-dimensional tensor using linear transformations defined by feedforward networks. Induction is used to prove the simulation process, with attention values computed for each step. A uniform Neural GPU processes tensors and outputs periodic sequences, showing that it cannot recognize the length of periodic inputs. The text discusses a language recognizer A defined by a neural GPU N that contains strings of even length. It presents a contradiction scenario involving strings of odd and even lengths, tensors S and T, and the outputs of the neural GPU for different inputs."
}