{
    "title": "BJxg7eHYvB",
    "content": "In this paper, a reinforcement learning based algorithm is proposed to reduce GPU memory usage in deep neural networks without compromising accuracy. The algorithm involves variable swapping between CPU and GPU memory and recomputation of feature maps during forward propagation. A deep Q-network is used to automatically decide which variables to swap or recompute. Results show that this approach outperforms several benchmarks in terms of memory efficiency and model performance. Deeper neural networks require more GPU memory for improved accuracy. Increasing input batch size can speed up training but also requires more GPU memory. Utilizing CPU memory for offloading and prefetching variables can help overcome GPU memory limitations. This approach leverages the larger size of CPU memory and GPU direct memory access engines for efficient data transfer. In the context of GPU memory limitations, utilizing CPU memory for offloading and prefetching variables can help overcome constraints. GPU engines can overlap data transfers with kernel execution, allowing for efficient variable swapping and recomputation during DNN training. Different engines can run parallelly, optimizing memory usage and computational resources. Our work proposes a DQN algorithm to optimize GPU memory usage by automatically planning for variable swapping and recomputation in deep neural networks. This approach considers more information from computational graphs compared to existing methods, leading to increased GPU utilization and reduced memory usage. Our method aims to optimize GPU memory usage by automatically planning for variable swapping and recomputation in deep neural networks. Unlike existing methods, our approach considers more information from computational graphs, leading to increased GPU utilization and reduced memory usage without compromising network accuracy. Our method utilizes DQN to minimize computation overhead within GPU memory limits. It considers GPU operations in a training iteration and interacts with an environment simulator to optimize memory usage. The algorithm provides a strategy that minimizes training time. The algorithm optimizes GPU memory usage by offloading, prefetching, removing, recomputing, or doing nothing to minimize training time. It focuses on optimizing DNNs and machine learning algorithms like K-means, using DQN to find an optimal plan based on GPU operations and memory limits. The algorithm is offline to exploit the iterative nature of DNNs, requiring actions of swapping, recomputation, or doing. To optimize GPU memory usage, actions like swapping, recomputation, or doing nothing are chosen using Q-learning with a deep Q-network. GPU operations such as malloc, write, read, and free are represented as nodes in a graph, with edges indicating dependencies. Agent states for DQN include information on variables being executed and those available for swapping. The curr_chunk discusses the representation of DNN structures and variables in agent states, including node states and graph states. It introduces notations for node states, weighted edges, offloaded variables, and learned parameter matrices. The process involves updating node states iteratively until reaching a final state. The curr_chunk discusses updating node states iteratively with six features for each node, including variable size, transfer duration, recomputation duration, revisit timing, action type, and validity. Neighbor nodes and edges are added for invariance and uniform node vector length. Graph state concatenates node states with executing node state. Graph feature representation allows training on small graphs and fine-tuning on large graphs. Actions include three types. The actions in the curr_chunk involve offloading, removing, and doing nothing. Variable swapping and recomputation have two phases each. Prefetching involves fetching reused variables. No swap-in or second phase recomputation actions are included. If swapping in a variable doesn't exceed memory usage, prefetching begins. In the curr_chunk, solid and dashed lines show time relations between nodes and agent transitions. Each node represents a GPU operation executed in sequence. Actions can be chosen in each node, such as doing nothing, removing X 0, or offloading X 0. GPU operations requiring variables need to suspend before prefetching. State transitions occur when actions are applied to the agent, changing the agent's state and the executing node. The agent moves to the next node after each operation, with offloading taking longer. The current GPU memory usage must be less than or equal to the memory limit to ensure the agent is in the correct node. Offloading X 0 incurs a 0.5 second forward overhead, with backward overhead calculated similarly. If GPU operations are not paused for variable swapping to stay within the memory limit and there is no recomputation, the reward is zero. Removing X 0 results in negative time for recomputing forward layer functions, while offloading X 0 incurs negative overhead for both forward and backward propagation. To determine state transitions and rewards accurately, the exact time for each GPU operation is crucial. To accurately determine state transitions and rewards, knowing the exact time for each GPU operation is essential. An idea was developed to address the challenge of fitting a large model into GPU memory. By utilizing fast memory operations like free and malloc from NVIDIA, the training time can be estimated roughly correct. While this method may not provide the precise derivative of weights, it allows for a rough estimation of GPU operation times. When an action is applied to the agent, it transitions to the next state and receives a reward based on predefined criteria. A simulator is used to simulate the training environment and update DQN, with assumptions that recomputing time can be estimated and variable swapping can run parallel with layer functions. Graph states are converted to Q values by concatenating the graph state and action node state into a vector and mapping it accordingly. We concatenate the graph state and action node state to a vector and map it to a value. Action nodes represent GPU operations and actions. We use a heuristic method to determine actions for nodes. An end-to-end DQN is trained with a loss function including rewards and a terminal state condition. The state becomes terminal if no more variables need to be removed or offloaded. Equation 4 is not updated with a single sample. Algorithm 1 updates weights using mini-batches from experience replay dataset. The greedy method is used to choose actions from {H 0 , H 1 }. DNN training follows the plan generated by DQN. GPU operations are executed in a time sequence. Actions are executed based on Section 4.6. Prefetching and recomputation follow the method in Section 4.5. Performance evaluation includes memory reduction and usage for variable swapping and recomputation. Testing is done on various architectures like ResNet, VGG, UNet, Kmeans, and LSTM. The method is extended to dynamic computation graphs. Datasets used include CIFAR-10 and Karpathy's char-RNN dataset. Our experiments involve training ResNet and VGG on CIFAR-10 and Karpathy's char-RNN dataset. The system used is equipped with an Intel Xeon E5 CPU, NVIDIA GeForce GTX 1080 Ti GPU, and 64GB RAM. Ubuntu 16.04, CUDA 9.0, and cuDNN 7.0 are used. Comparison is made with MXNet-memonger, SuperNeurons, and TFLMS for memory reduction techniques. MXNet-memonger trades computation for memory, SuperNeurons propose recomputation and variable swapping, while TFLMS only uses variable swapping. Our method utilizes recomputation layers, swapping mode, and swapping combined with recomputation mode to achieve better results compared to MXNet-memonger, SuperNeurons, and TFLMS. SuperNeurons use the LRU algorithm for variable swapping, while TFLMS only uses variable swapping and requires manual setting of swap variables. Our method automatically provides plans for users based on computation graphs, unlike SuperNeurons and MXNet-memonger which rely on expert knowledge for specific layer selection. Our method achieves higher GPU utilization during network training compared to existing methods by avoiding wastage of GPU resources for memory saving. It supports more general architectures like LSTM function CuDNNLSTM and ResNet with stochastic depth and K-Means. Our algorithm allows for easy memory limit adjustment, works well with a wide range of iterative machine learning algorithms, and provides automatic plans for users without requiring expert knowledge. Additionally, our method shows similar performance on ResNet and VGG architectures but differs in structure from UNet. The method proposed in the paper achieves higher GPU utilization during network training by avoiding wastage of GPU resources for memory saving. It supports general architectures like LSTM, ResNet with stochastic depth, and K-Means. However, the method faces challenges with UNet and SD ResNet due to variable swapping and recomputation issues, resulting in worse performance compared to ResNet and VGG. LSTM's slower execution of convolutional layers also leads to longer overhead compared to ResNet and VGG. The proposed DQN method aims to reduce memory usage by generating plans for variable swapping and recomputation. It is compatible with various network structures like ResNet, VGG, K-means, SD ResNet, and LSTM without compromising accuracy. Users can set a memory limit without needing background knowledge in DNN or machine learning algorithms."
}