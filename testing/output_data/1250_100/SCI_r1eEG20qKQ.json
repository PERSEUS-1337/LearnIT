{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization for neural networks involves adapting regularization hyperparameters by fitting compact approximations to the best-response function. This is achieved by modeling the best-response as a single network with gated hidden units, allowing for scalable approximations. The approach does not require differentiating the training loss with respect to hyperparameters, enabling tuning of discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training, outperforming other optimization methods on deep learning problems. Regularization hyperparameters like weight decay, data augmentation, and dropout are crucial for neural network generalization but challenging to tune. Traditional optimization methods like grid search and Bayesian optimization work well with low-dimensional spaces but ignore structure for faster convergence. Hyperparameter optimization can be formulated as a bilevel optimization problem with parameters denoted as w and hyperparameters as \u03bb. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training, outperforming other optimization methods on deep learning problems. Hyperparameter optimization can be formulated as a bilevel optimization problem with parameters denoted as w and hyperparameters as \u03bb. The best-response function w * (\u03bb) is difficult to compute, so we propose approximating it with a parametric function \u0175 \u03c6. This allows for joint optimization of \u03c6 and \u03bb, offering a scalable approximation for neural network weights. Our proposed Self-Tuning Networks (STNs) update hyperparameters online during training, providing advantages over other optimization methods. STNs model the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of hyperparameters, ensuring computational effort is not wasted and yielding superior hyperparameter schedules. The STN training algorithm allows tuning discrete hyperparameters without differentiating the training loss. Empirical evaluation shows STNs outperform baseline methods on deep-learning problems. Bilevel optimization involves solving upper and lower-level problems, with minimax problems being an example. In machine learning, bilevel problems are NP-hard even with linear objectives and constraints. Most work focuses on restricted settings, but we aim to find local solutions in nonconvex, differentiable settings. We seek to solve a problem subject to minimizing a function. Gradient-based algorithms are preferred for speed-ups over black-box methods. Simultaneous gradient descent is a simple method but may give incorrect solutions. The problem of solving bilevel optimization in machine learning is challenging, especially in nonconvex, differentiable settings. Simultaneous gradient descent can lead to incorrect solutions due to the dependence of variables. A more principled approach involves using the best-response function to convert the problem into a single-level one, allowing for gradient descent optimization. Conditions for unique optima and differentiability are crucial, with Lemma 1 providing sufficient conditions for their existence in a neighborhood of a given point. The gradient of F * decomposes into direct and response gradients, stabilizing optimization by converting the bilevel problem into a single-level one. Assuming uniqueness of a solution and differentiability of w * can yield fruitful algorithms in practice. Gradient-based hyperparameter optimization methods approximate the best-response w * or its Jacobian \u2202w * /\u2202\u03bb. The best-response w * or its Jacobian \u2202w * /\u2202\u03bb can be approximated using computationally expensive methods that struggle with discrete and stochastic hyperparameters. Lorraine & Duvenaud (2018) proposed global and local approximation algorithms to directly approximate w * as a differentiable function \u0175 \u03c6 with parameters \u03c6, and to locally approximate w * in a neighborhood around the current upper-level parameter \u03bb. The approach by Lorraine & Duvenaud (2018) involves approximating the best-response w* as a differentiable function \u0175\u03c6 with parameters \u03c6, locally approximating w* in a neighborhood around the upper-level parameter \u03bb, and using an alternating gradient descent scheme to update \u03c6 and \u03bb. The method worked for problems with L2 regularization on MNIST but its applicability to different regularizers, scales, high-dimensional w, setting \u03c3, and adapting to discrete and stochastic hyperparameters remains unclear. The authors propose a memory-efficient best-response approximation \u0175\u03c6 for large neural networks and a method to automatically adjust the scale of the neighborhood \u03c6 is trained on. The method involves automatically adjusting the scale of the neighborhood \u03c6 and describes an algorithm for handling discrete and stochastic hyperparameters. The resulting networks, called Self-Tuning Networks (STNs), update their own hyperparameters online during training. The architecture computes an affine transformation of the hyperparameters for a given layer's weight matrix and bias, making it tractable to compute and memory-efficient. The text discusses the implementation of hyperparameters in deep learning models, focusing on parallelism and perturbation for improved sample efficiency. It introduces a linear network with Jacobian norm regularization to represent the best-response function exactly. This model modulates hidden units based on hyperparameters, using a 2-layer linear network for prediction tasks. The text introduces a squared-error loss regularized with an L2 penalty on the Jacobian \u2202y/\u2202x, using a sigmoidal gating architecture to approximate the best-response for deep, nonlinear networks. This architecture can be simplified for a narrow hyperparameter distribution by replacing the sigmoidal gating with a linear approximation. The text discusses replacing sigmoidal gating with linear gating to make weights affine in hyperparameters for quadratic lower-level objectives. It shows that using an affine approximation yields the correct best-response Jacobian, ensuring convergence to a local optimum. The sampled neighborhood's size affects the approximation's gradient matching the best-response, with \u03c3 controlling the hyperparameter distribution scale. Adjusting the scale of hyperparameter distribution during training is crucial for capturing the best-response over samples. Varying \u03c3 based on sensitivity to sampled hyperparameters, with an entropy term weighted by \u03c4, helps enlarge \u03c3 entries. The objective is similar to variational inference, interpolating between variational optimization and inference. Such objectives have been beneficial for better training and representation learning. The algorithm described in the curr_chunk focuses on training a model with hyperparameters that other gradient-based algorithms cannot tune, such as discrete or stochastic hyperparameters. It involves minimizing a term to balance between shrinking and avoiding entropy penalties, using an unconstrained parametrization of hyperparameters. Training and validation losses are functions of the hyperparameters and parameters, with a focus on adjusting the scale of hyperparameter distribution during training for better representation learning. The algorithm presented focuses on training models with hyperparameters that traditional gradient-based methods struggle to optimize, such as discrete or stochastic hyperparameters. It involves minimizing a term to balance between shrinking and avoiding entropy penalties, using an unconstrained parametrization of hyperparameters. Training and validation losses are functions of hyperparameters and parameters, with a focus on adjusting the scale of hyperparameter distribution during training for improved representation learning. The algorithm utilizes a gradient descent scheme to update parameters and hyperparameters iteratively, with a specific approach for handling non-differentiability and discrete hyperparameters. The text discusses the use of the REINFORCE gradient estimator for estimating derivatives with respect to hyperparameters in neural networks. It introduces self-tuning CNNs and LSTMs, which adapt hyperparameters online for improved performance. The method outperforms fixed hyperparameter values and traditional optimization methods on datasets like CIFAR-10 and PTB. The text discusses using self-tuning LSTMs to adapt hyperparameters online, outperforming fixed values. An ST-LSTM discovered a schedule for output dropout that achieved better results than grid search. STNs outperformed standard LSTMs with perturbations for regularization. The limited capacity of hyperparameters did not act as a regularizer. The study trained a standard LSTM with a dropout schedule discovered by the ST-LSTM, showing similar performance to the STN. Using the final dropout value from the STN did not yield the same results. The STN found a consistent schedule regardless of initial hyperparameter values, implementing a curriculum of low dropout early in training and higher dropout later on. The STN schedule implements a curriculum with a low dropout rate early in training, gradually increasing dropout for better generalization. An ST-LSTM was evaluated on the PTB corpus using 2-layer LSTM with 650 hidden units and 650-dimensional word embeddings. 7 hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. LSTM tuning showed best results with a fixed perturbation scale of 1. STNs were compared to grid search, random search, and Bayesian optimization, with FIG2 showing the best validation perplexity achieved by each method over time. The STNs outperform other methods in achieving lower validation perplexity more quickly. The schedules found for each hyperparameter are nontrivial, with various forms of dropout utilized throughout training. ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture and tuned hyperparameters for data augmentation. The study compared STNs to other optimization methods for hyperparameter tuning, showing that STNs find better configurations in less time. The hyperparameter schedules found by STNs are illustrated in FIG3. Additionally, the text briefly discusses bilevel optimization and hypernetworks. The text discusses the use of hypernetworks in estimating best-response functions iteratively and gradient-based hyperparameter optimization techniques. Hypernetworks map functions to neural net weights, while the optimization techniques involve approximating values through gradient descent or using the Implicit Function Theorem. These methods have been applied in various studies for architecture search and hyperparameter optimization in neural networks. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance metrics given hyperparameters and dataset. Various methods can be used to construct the model iteratively, selecting the next hyperparameter to train on by maximizing an acquisition function. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance metrics given hyperparameters and dataset. The next hyperparameter to train on is chosen iteratively by maximizing an acquisition function. Model-free approaches like grid search and random search are also used, with some advocating for random search over grid search. Successive Halving and Hyperband extend random search by adaptively allocating resources to promising configurations. These methods ignore problem structure but can be easily parallelized and tend to perform well in practice. Hyperparameter Scheduling involves training a population of networks in parallel, evaluating their performance periodically, and replacing under-performing networks with better ones. Self-Tuning Networks (STNs) use gradients to tune hyperparameters during a single training run, discovering hyperparameter schedules that can outperform fixed hyperparameters. STNs efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting hidden units, allowing for gradient-based optimization of various regularization hyperparameters. Self-Tuning Networks (STNs) use gradients to tune hyperparameters during training, discovering schedules that outperform fixed hyperparameters. STNs achieve better generalization performance in less time, offering a path towards automated hyperparameter tuning for neural networks. The Hessian matrix is positive definite and invertible at a specific point. By the Implicit Function Theorem, a unique continuously differentiable function exists in a neighborhood that solves a specific problem. This discussion is based on previous work by Hastie et al. (2001) and involves the SVD decomposition of a data matrix. The function y(x; w) is simplified, leading to a standard L2-regularized least-squares linear regression loss function. The optimal solution for the L2-regularized least-squares linear regression is given by u*(\u03bb), and the unregularized version is given by u*. The change-of-basis matrix Q0 and solution s0 are related to the regression problem. Best-response functions Q*(\u03bb) and s*(\u03bb) are chosen based on specific criteria. The function f is quadratic, represented by matrices A, B, C, and vectors d, e. The function f is quadratic, represented by matrices A, B, C, and vectors d, e. By using matrix-derivative equalities, we find the best-response Jacobian \u2202w*/\u2202\u03bb(\u03bb) as given by Equation 34. Substituting U =... The best-response Jacobian \u2202w*/\u2202\u03bb(\u03bb) is derived from Equation 34. The model parameters were updated, but hyperparameters were not. Training was stopped when the learning rate fell below 0.0003. Variational dropout was tuned for input, hidden state, and output of the LSTM. Embedding dropout was also adjusted. DropConnect was used to regularize the hidden-to-hidden weight matrix. Activation regularization (AR) penalizes large activations. The curr_chunk discusses the tuning of scaling coefficients for AR and TAR regularization, as well as the hyperparameter ranges for baselines and ST-LSTM. It also details the training process for the baseline CNN using SGD and the search spaces for hyperparameters in baselines-grid search, random search, and Bayesian optimization. The ST-CNN's elementary parameters were trained using SGD with an initial learning rate of 0.01 and momentum of 0.9 on mini-batches of size 128. Hyperparameters were optimized using Adam with a learning rate of 0.003. The model alternated between training the best-response approximation and hyperparameters following the ST-LSTM schedule. The entropy weight was set to \u03c4 = 0.001, and cutout parameters were restricted to specific ranges. The ST-CNN showed robustness to hyperparameter initialization, with low regularization aiding optimization initially. In this section, the text explores how hyperparameter schedules can be seen as a form of curriculum learning. By gradually increasing the difficulty of the learning problem through adjusting hyperparameters over time, optimization and generalization can be improved. For example, increasing dropout over time increases stochasticity, making the learning problem more challenging. Results from grid searches show that greedy hyperparameter schedules can outperform fixed schedules. The text discusses the benefits of greedy hyperparameter schedules in training, showing that they can outperform fixed hyperparameter values. A grid search was conducted for input and output dropout values, with results indicating that larger dropout rates yield better validation performance as training progresses. A comparison between fixed hyperparameter values and a schedule based on the best dropout values at each epoch showed that the schedule leads to better generalization. The schedule achieves a fast decrease in validation perplexity by using small dropout values at the start of training, and better overall validation perplexity with larger dropout later. PyTorch code listings for best-response layers used in ST-LSTMs and ST-CNNs are provided, along with optimization steps for the training and validation sets."
}