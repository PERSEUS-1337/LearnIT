{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the importance of providing online explanations during execution to reduce human mental workload is emphasized. The challenge lies in the interdependence of different parts of an explanation, requiring a careful approach to generating online explanations. Three different implementations of online explanation generation are presented, based on a model reconciliation setting. Evaluation was done with human subjects in a planning competition domain and in simulation with various problems across domains. As intelligent robots interact more with humans, providing explanations for their decisions is crucial for maintaining trust and shared awareness. Prior work on explanation generation often overlooks the recipient's need to understand the explanation clearly. To address this, explanations should be generated considering the differences between the human and robot models. Model reconciliation is essential, where the robot's model (M R) and the human's model (M H) are used to generate explanations that request the human to adjust their expectations. The human generates expectations of the robot using model M H. When differences arise between the two plans, the robot must provide an explanation to reconcile the models. Model reconciliation is crucial for decision-making in the presence of such differences. One challenge is the mental workload required for understanding explanations, which should be provided online to reduce cognitive burden. The online explanation generation process aims to reduce cognitive burden by spreading out information smoothly. An example illustrates how Mark's plan to study with Emma differs due to their different preferences, highlighting the importance of effective communication in decision-making. Mark gradually reveals his plan to study with Emma at the library, emphasizing the importance of providing explanations online. He explains the need for a lunch break to maintain energy levels, strategically withholding information to simplify the interaction and reduce cognitive workload for Emma. In this paper, a new method for online explanation generation is developed, intertwining explanation with plan execution. Three different approaches are implemented, focusing on matching the plan prefix, making the next action understandable, and matching the robot's plan with a possible human plan. The goal is to reduce the mental workload of the recipient by breaking the explanation into parts communicated at different times during plan execution. The approaches are evaluated with human subjects and in simulation. Explainable AI is crucial for human-AI collaboration, as it helps improve human trust in AI agents and maintain shared situation awareness. The effectiveness of explainable agency is based on accurately modeling human perception of the AI agent, enabling the agent to generate legible motions. Using a model for explainable AI, an agent can generate legible motions, explicable plans, or assistive actions. The model can also be used for the agent to signal its intention before execution and explain its behavior by generating explanations. Research in this area focuses on generating the \"right\" explanations based on the recipient's perception model, but often ignores the mental workload required for understanding an explanation. In prior work, the influence of information ordering on explanation perception was studied. The concept of online explanation generation for complex explanations is introduced, intertwining explanation with plan execution. The problem definition is based on model reconciliation in planning problems, defined as a tuple (F, A, I, G) using PDDL. Explanation generation in a model reconciliation setting involves bringing two models, M H and M R, closer together by updating M H to make the robot's plan fully explainable and optimal in the human's model. This process considers the cost of the plan generated using M R and the cost of the optimal plan under M R. The reconciliation is achieved when the robot's behavior matches the human's expectations, as illustrated in FIG0. Explanation generation in a model reconciliation setting involves converting a planning problem into a set of features that specify the problem. An explanation is a set of unit feature changes that reconcile two models by minimizing the cost difference between the human's expected plan and the robot's plan. A complete explanation is one that ensures the robot's plan is optimal in the human's model, with a minimal complete explanation containing the fewest unit feature changes. Online explanation generation introduces a method to provide explanations during plan execution by splitting them into sub-explanations. This approach focuses on minimizing the mental workload of the human by only providing relevant information at each step. Explanation generation during plan execution involves generating sub-explanations to minimize human mental workload. The process involves considering model changes and ensuring that future changes do not disrupt previously reconciled plan prefixes. This is achieved by searching for the largest set of model changes that maintain plan prefixes integrity. Explanation generation during plan execution involves generating sub-explanations to minimize human mental workload. An OEG-PP consists of subexplanations (e k , t k ) where Prefix(\u03c0, t) returns the plan prefix up to step t k\u22121. The search process starts from the robot model and stops when the plan prefixes match, similar to MME BID6, but runs multiple times for matching prefixes. This approach focuses on matching prefixes rather than the entire plan at once, making it more computationally expensive. Explanation generation during plan execution involves generating sub-explanations to minimize human mental workload. The approach focuses on matching plan prefixes rather than the entire plan at once, making it more computationally expensive. The dotted line represents the border of the maximum state space model modification in the robot model, reconciling two models up to the current plan execution. Maximum updates to the robot model correspond to minimum updates to the human model. The process finds the largest set of model changes to the robot model such that the plan prefix matches with the desired model. The complement set of changes will be used for the next recursive step. This recursive process ensures that future plans remain compatible with the prefix. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. A recursive model reconciliation procedure is used to search for e k by finding the difference between two models, M DISPLAYFORM1 and M R, and modifying M R with respect to M H. The goal is to match the robot and human plan prefixes during plan execution, ensuring compatibility at each step. This approach relaxes the plan prefix condition, allowing the robot to reconcile between M R and M H to match the next action in the plan regardless of earlier actions. The OEG algorithm focuses on explaining the immediate next action that differs between human and robot plans, maintaining prefix match. It searches from M H \\M H for computational efficiency and uses a recursive model reconciliation procedure. The OEG algorithm combines search from M H and M R for better performance in explaining plan differences between human and robot, aiming to reconcile plans by finding optimal plan matches. The approach involves generating human optimal plans and checking for matches with the robot's plan prefix, using a compilation approach for computational efficiency. The OEG algorithm uses a compilation approach to ensure that a plan prefix in the robot's plan matches the human's model. By compiling the problem in the human's model, the algorithm checks if the cost of the human's optimal plan remains the same. If not, an explanation is needed. The key is to satisfy the plan prefix in the compiled model by adding predicates to actions. The algorithm then uses a recursive model reconciliation process to search for differences between the two models. The OEG algorithm uses a compilation approach to ensure that a plan prefix in the robot's plan matches the human's model. After each model update, the agent checks for a human optimal plan with the same plan prefix as the robot's plan. This process continues until an optimal human plan matches the robot's plan. The approach was evaluated for online explanation generation with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach on different problems in the rover and barman domains. The study evaluated online explanation generation in the rover domain, aiming to reduce mental workload and improve task performance. The rover's goal is to explore Mars, take rock and soil samples, images, and communicate results to the base station. Specific actions like calibrating the camera and having empty storage space are required for sampling. Multiple samples can only be taken by dropping the current one. The robot in the barman domain serves drinks using drink dispensers, glasses, and a shaker. Constraints include grabbing one object with an empty hand, grabbing one object with one hand, and ensuring a glass is empty and clean before filling it with a drink. Simulation results compare explanations in the rover and barman domains, showing differences in the number of shared model features and total features between minimally complete explanations (MCE) and OEG approaches. The OEG approaches (OEG-PP, OEG-NA, OEG-AP) share more information in total compared to MCE due to the dependence between features and planner behavior. OEG-AP considers all optimal plans, while OEG-NA only considers the immediate next action, leading to a plan action distance between robot and human plans. OEG-AP may not match the robot's plan as it considers all optimal human plans. The execution and explanation in OEG approaches are intertwined, illustrated in FIG3. In the OEG approaches, the plan distance between robot and human plans gradually decreases, leading to smoother adjustments for M H during execution. Model updates are sorted by feature size, with backtracking when needed. A human study compared three online explanation generation approaches, including one that randomly breaks explanations during execution. The experiment involved using Amazon Mechanical Turk with 3D simulation to test human subjects acting as rover commanders on Mars. Subjects had to determine if the rover's actions were questionable, with explanations provided. Additional spatial puzzles were included to increase cognitive demand. Certain information was deliberately removed to test subjects' responses. The experiment involved testing human subjects as rover commanders on Mars using Amazon Mechanical Turk and 3D simulation. Subjects had to determine if the rover's actions were questionable, with explanations provided. Certain information was deliberately removed to test subjects' responses, introducing differences between models and scenarios where explanations were necessary. The robot shared all information at the beginning in one setting, while in another, information was broken up and communicated at different steps. Different approaches of online explanation generation were used, intertwining explanation communication with plan execution. Subjects were asked to assess the robot's actions for sense-making. The study evaluated the efficiency of different explanation approaches by using the NASA Task Load Index (TLX) questionnaire. The NASA TLX is a tool to assess workload in human-machine interface systems, measuring mental demand, performance, effort, and frustration. The experiment involved 150 human subjects recruited on MTurk to assess the mental workload of different explanation approaches. After recruiting 150 human subjects on MTurk, with 30 subjects for each setting, valid responses were obtained from 94 participants. The study examined how well subjects understood the robot's plan with different explanations, comparing distances across five settings. The distance metric calculated the ratio of questionable actions to total actions in a plan, with lower values indicating better understanding. Results showed that OEG approaches were more effective in reducing human's workload. The study found that OEG approaches were more effective in reducing human's mental workload compared to MCE approaches. OEG approaches resulted in better performance in NASA TLX measures, with fewer questionable actions and higher accuracy in identifying correct actions. OEG-AP had the least questionable actions and the highest accuracy. Statistical analysis showed a significant difference in mental workload between OEG approaches and MCEs. In a comparison between OEG approaches and MCEs for mental workload, a significant difference was found with an overall p-value of 0.0068. Time analysis showed varying task completion times, with OEG-NA being the quickest and OEG-PP being the slowest. The accuracy of the secondary task did not differ significantly. A novel approach for explanation generation was introduced to reduce mental workload during human-robot interaction by breaking down complex explanations into smaller, easily understandable parts. Three different approaches focusing on explanation generation weaved in plan execution were provided. Our approaches for explanation generation in plan execution were evaluated using simulation and human subjects, showing improved task performance and reduced mental workload, a crucial step towards explainable AI."
}