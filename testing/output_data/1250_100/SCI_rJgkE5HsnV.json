{
    "title": "rJgkE5HsnV",
    "content": "The paper proposes a method for incremental domain adaptation using a recurrent neural network (RNN) with a parameterized memory bank. This memory bank is updated with new slots when adapting to a new domain, increasing the model capacity. New memory slots and existing parameters are learned and fine-tuned through back-propagation. Our approach for incremental domain adaptation outperforms previous methods like elastic weight consolidation and progressive neural network. It is more robust for old domains, supported by empirical and theoretical results. Incremental domain adaptation transfers knowledge from a source domain to a target domain in machine learning systems, crucial for data-hungry neural networks prone to overfitting. This method focuses on building a unified model that performs well on all encountered domains, useful in scenarios like long-term business partnerships where data access is limited to the current partner. The progressive neural network is a recent trend in domain adaptation in deep learning. It involves progressively growing the network capacity when a new domain is encountered, by adding new hidden states and a new predictor without interfering with existing knowledge. This approach aims to avoid catastrophic forgetting during fine-tuning and quick adaptation to new domains. In this paper, a progressive memory bank is proposed for incremental domain adaptation in neural networks. The model augments a recurrent neural network (RNN) with a memory bank to capture domain knowledge. The memory is retrieved using an attention mechanism, and as the model adapts to new domains, the memory bank slots are progressively increased. Unlike previous approaches, all parameters, including RNN and existing memory slots, are fine-tuned. Experimental results on Natural Language Inference and Dialogue Response Generation support the effectiveness of the proposed approach. The proposed approach in this paper involves augmenting a recurrent neural network (RNN) with a memory bank for incremental domain adaptation. The model outperforms other techniques and adapts well to target domains without forgetting the source. The RNN is enhanced with an external memory bank to increase model capacity, and attention probabilities are computed to retrieve information from the memory bank. The model in this paper enhances a recurrent neural network (RNN) with a memory bank for incremental domain adaptation. It computes attention probabilities to retrieve information from memory slots, which contain key and value vectors. The attention mechanism allows for end-to-end training of memory content and retrieval, along with other parameters. The model in this paper enhances a recurrent neural network (RNN) with a memory bank for incremental domain adaptation by expanding memory slots and computing attention probabilities. The approach is evaluated on natural language inference for classification tasks. The study compares the performance of RNN and memory-augmented RNN for incremental domain adaptation in natural language inference tasks. The BiLSTM model achieves an accuracy of 68.37 on the MultiNLI test set, showing fair implementation and readiness for IDA study. The details of network architecture and training are provided in the appendix. The study compares RNN and memory-augmented RNN for incremental domain adaptation in natural language inference tasks. Two domain adaptation methods, multi-task learning and fine-tuning, are applied. Fine-tuning performs slightly better on the source domain but worse on the target compared to multi-task learning. The goal is to improve IDA performance despite the observed catastrophic forgetting issue. Our approach outperforms fine-tuning alone on both source and target domains by increasing model capacity through memory expansion. This method is orthogonal to expanding vocabulary size and yields the best performance. Comparing it to expanding hidden states shows that memory expansion is more effective, especially on the source domain. The experiment provides empirical evidence supporting the theorem that expanding memory is more robust than expanding hidden states. Comparison with previous work on IDA shows inconsistent results with EWC, yielding satisfactory results in some applications but poor performance in others. Re-implementing the progressive neural network also results in low performance, particularly on the source domain. Statistical significance is measured using Wilcoxon's signed-rank test, with performance comparisons denoted by arrows. The experiment supports the idea that expanding memory is more robust than expanding hidden states. Our approach outperforms others on multiple domains, achieving high performance on both new and previous domains. The results show that our model is comparable to multi-task learning and outperforms EWC and the progressive neural network in all domains. This demonstrates the effectiveness of our approach with more than two domains. In this paper, a progressive memory network for incremental domain adaptation (IDA) is proposed. The network augments an RNN with an attention-based memory bank, adding new slots during IDA to prevent catastrophic forgetting. The progressive memory bank increases model capacity without overriding existing knowledge, outperforming previous methods like EWC and the progressive neural network. van der Smagt, P. discusses the challenges of using multiple predictors in recurrent networks for different domains. Yoon et al. propose a method to identify relevant hidden units for new tasks in progressive networks. However, sparse connections and dropout in RNNs can be harmful in NLP applications. Our work is related to memory-based neural networks. Sukhbaatar et al. (2015) propose an end-to-end memory network that assigns slots for entities and aggregates information through attention-based layers. This idea can be extended to various scenarios such as question answering with external knowledge (Das et al., 2017) and conversation systems with dialog history (Madotto et al., 2018). Another type of memory is the neural Turing machine (NTM) (Graves et al., 2016), where memory is read or written by a neural controller, serving as temporary scratch paper. Zhang et al. (2018b) combine slot-value memory and read-and-write memory for task-oriented dialog systems. Our memory bank stores knowledge in a distributed fashion, with each slot not corresponding to a concrete entity. The memory is directly parameterized, providing a natural way of incremental domain adaptation. The expanding memory is more stable than expanding hidden states, especially in the early stages of incremental domain adaptation. The theoretical analysis supports the approach for incremental domain adaptation. In a simplified case, the RNN has vanilla transition with linear activation. Model expansion is quantitatively measured by the expected norm difference on hidden states before and after expansion. The hidden state and memory slots are assumed to be d-dimensional. Variables in expanded memory and weights are iid with zero mean and variance \u03c3^2. The original dimensions in the hidden state change when expanding the RNN. The expanded hidden states and weights connecting them are denoted, focusing on the original D-dimensional space. The text discusses the expansion of memory slots in an RNN model. It explains the computation of attention and how it is affected by memory expansion. The attention probabilities are recalculated after memory expansion, and the variance of the attention vector is analyzed. The text also mentions the weight matrix connecting attention content to RNN states. After memory expansion, attention probabilities are recalculated in the RNN model. The total number of model parameters remains the same during memory and hidden state expansion. The theorem requires that existing memory slots receive more attention than progressively added slots, based on training and domain considerations. The study follows the MultiNLI paper for model selection and settings, using 300D RNN hidden states and GloVe embeddings. The Adam optimizer is used with a batch size of 32. Progressive memory slots are tuned, with 500 slots chosen for each domain. The IDA with progressive memory network shows quick adaptation to new domains in an incremental manner. Our approach shows gradual improvement in performance across domains without suffering from catastrophic forgetting. We evaluate it on dialogue response generation using a small dataset from the Ubuntu Dialog Corpus. The base model is a Seq2Seq neural network, and evaluation metrics include BLEU-2 and Word2Vec embedding similarity. The evaluation metrics for dialogue response generation include BLEU-2 and Word2Vec embedding similarity. Results show that the model variants outperform other approaches and achieve the best performance on most metrics. Additionally, a statistical test confirms the superiority of the proposed method over other IDA approaches. Our method outperforms other IDA methods, as shown by the evaluation results. The experiment highlights the effectiveness of our approach for both classification and generation tasks."
}