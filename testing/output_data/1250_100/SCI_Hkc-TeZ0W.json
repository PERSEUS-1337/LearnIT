{
    "title": "Hkc-TeZ0W",
    "content": "A hierarchical model is introduced for efficient placement of computational graphs on hardware devices in heterogeneous environments. The method learns to assign graph operations to groups and allocate them to available devices without human intervention. Experiments show that the algorithm can optimize placements for TensorFlow graphs with over 80,000 operations, outperforming human placements. Our method achieves significant runtime reductions of up to 60.6% per training step for models like Neural Machine Translation, surpassing previous state-of-the-art methods. Deep neural networks have been successfully applied to various practical problems, leading to increased demand for computational resources. To address this demand, a common approach is to use a distributed environment with CPUs and GPUs for model and data parallelism. However, manually placing operations on computing devices does not scale well or produce optimal results, especially for complex models. Automated device placement is essential for efficiently distributing computation in complex neural networks. Traditional graph partitioning methods, such as Scotch, provide a baseline for this task. Experimentation with Scotch's optimization techniques aims to balance computational load and minimize communication between connected processing nodes. In a distributed environment with shared CPUs and GPUs, traditional cost-based models like BID21 are not suitable due to dynamic costs. Recent work proposes using deep networks and reinforcement learning for combinatorial optimization, with an RNN policy network optimizing operation placement in computational graphs for faster computation. This method is limited to small graphs and requires human expert input for larger operations. In this paper, a two-level hierarchical network is proposed for optimizing device placement in training neural networks with tens of thousands of operations. The first model groups operations in the graph, while the second model places these groups onto devices. The network is trained using reinforcement learning to optimize for speed and feasibility without manual grouping. Our end-to-end method for optimizing device placement in training neural networks handles large graphs efficiently, outperforming default placements and human expert placements. The model consists of a Grouper and a Placer trained jointly using reinforcement learning, achieving significant reductions in training time per iteration. The proposed approach, known as the Hierarchical Planner, aims to optimize device placement for training neural network graphs by predicting placements that speed up training. Policy gradients are used to train the Hierarchical Planner, which in turn trains both the feed forward Grouper and the recurrent Placer. The Grouper assigns operations to groups, which are then used to compute device placements by the Placer. The final placement places each operation on the device assigned to its group. The proposed Hierarchical Planner optimizes device placement for training neural network graphs by predicting placements to speed up training. The Grouper is a feed forward model with a softmax layer, while the Placer is a sequence-to-sequence model with LSTM and attention mechanism to predict placements. Operation embeddings are generated for input to the Grouper, consisting of vectors for operation type information, output sizes, number of outputs, and adjacency information. The Placer sets the maximum number of incoming and outgoing edges to 12. It generates input by creating group embeddings with operation type counts, total output shapes count, and group adjacency information. The RNN encoder produces hidden states, and the decoder predicts device placements. Operations are placed in the same order as the input group embeddings. The decoder uses an attention mechanism to attend over encoder states. It samples a device per step from the Placer's softmax using a temperature and tanh constant. The placement decisions are then used to place the model. The planner optimizes training time for a target model based on decisions made by the Grouper and Placer, maximizing the expectation of runtime per training step for a predicted device placement. The planner aims to maximize the expectation of reward for device placement decisions. Parameters for the Grouper and Placer are denoted as \u03b8 g and \u03b8 d respectively. Derivatives of the cost function with respect to these parameters are calculated. The REINFORCE rule is used to derive equations from the cost function. The Grouper assigns operations to groups independently, while the Placer conditions group placement on previous placements. A baseline is subtracted from the reward to reduce variance. Distributed training is utilized for policy training. The policy is trained in a distributed manner using a parameter server shared among controllers. Each controller communicates with worker nodes to execute placements and report runtimes. In experiments, 4 controllers and 16 workers are used, totaling 36 GPUs. Gradients are computed using measured runtimes, with each controller maintaining a separate baseline to reduce variance. While more workers yield better results, it is possible to achieve comparable results with fewer resources. Our approach applies Hierarchical Planner to machine learning models in computer vision and natural language processing, achieving comparable results with fewer resources. We compare our method to heuristic and RL-based baselines, demonstrating better placements. Evaluations on deep neural networks like Inception-V3 show the effectiveness of our hierarchical architecture. The TensorFlow graph encoding various models for image classification and natural language processing contains a high number of operations. These models include ResNet, RNNLM, and NMT, each with different batch sizes and computational complexities. The NMT model, in particular, is more computationally expensive due to its many hidden states. To decrease training time, a hierarchical planner can significantly improve speed. Our Hierarchical Planner shows significant speed improvements in finding better placements for NMT models. Three versions were evaluated: a 2-layer, a 4-layer, and an 8-layer model. The models were compared to CPU-only and GPU-only baselines, with considerations for memory limitations and cross-device communication costsab. The text discusses different methods for optimizing device placement in TensorFlow. These methods include using the Scotch static mapper, MinCut optimizer, human expert placements, and the ColocRL method which uses policy gradient to train a neural network for device placement. The text discusses optimizing device placement in TensorFlow using a neural network trained with policy gradient. Rewards are based on runtime, with invalid placements receiving a large negative reward. Experiments are conducted on machines with specific hardware, and the policy network architecture is described. The method can be adapted for optimizing other metrics like inference speed and network congestion. The Policy Network architecture for optimizing device placement in TensorFlow includes a Grouper with a hidden size of 64 and a Placer with an LSTM hidden size of 256. The Grouper's softmax output size is set to 256, and the Placer's softmax output size is equal to the number of available hardware devices. Noise is added to encourage exploration in the first 500 policy training steps, updating the policy only for valid placements thereafter. The Policy Network architecture for optimizing device placement in TensorFlow includes a Grouper with a hidden size of 64 and a Placer with an LSTM hidden size of 256. In the first 500 steps, the policy is updated only for valid placements to prevent convergence to invalid placements. Results show the Hierarchical Planner's performance compared to Graph Partitioning Heuristics, with reductions in runtime achieved by optimizing GPU usage for different models like ResNet, RNNLM, and Inception-V3. The Hierarchical Planner efficiently distributes models across GPUs, as shown in the placement of a NMT (4-layer) model. The Hierarchical Planner optimizes device placement in TensorFlow by distributing layers across multiple GPUs, outperforming human experts by 53.7%. Experiments with 2, 4, and 8 layers show performance improvements of 60.6% and 53.7% for NMT models. Results for NMT (8-layer) are slightly slower than human experts but still valuable. Comparison with other methods like Scotch and MinCut shows significant improvements. Runtimes are faster but not directly comparable due to different hardware and software configurations. The Hierarchical Planner optimizes device placement in TensorFlow by distributing layers across multiple GPUs, outperforming human experts by 53.7%. For NMT (2-layer), the improvement over best heuristics is 60.6%. ColocRL, on the other hand, makes strong assumptions about colocating operations. The Hierarchical Planner's placements show high granularity, enabling parallelism that was previously infeasible. In a placement for NMT (4-layer), the runtime per training iteration is 53.7% faster than a hand-crafted placement. The automated device placement method outperforms previous methods by 53.7% and distributes operations across multiple GPUs for increased parallelism. The policy search space is large with 5 devices and 46,600 operations. The Grouper converges to using a small subset of groups during training, indicating efficient partitioning of the computational graph. The method is deployable without manual grouping efforts. The device placement problem is approached as a sequential decision-making task, with the Placer using a bi-LSTM architecture to optimize placements. Experiments on a NMT baseline showed less than 7% difference in placement speeds. Training a new policy for each model takes up to three hours, with the policy updated after 1000 iterations. The reward for a sampled placement is calculated by running the model for 5 training steps. The policy is a lightweight network trained on a single GPU, using actual runtime measurements for reward. Measuring runtime is done by worker nodes, with the goal of optimizing device placement. Results show efficient training even with limited hardware settings, using 1 worker for placement optimization. The policy loss reduction is demonstrated in scenarios with 1 and 4 workers, with the policy hosted on a single K40 GPU. The Hierarchical Planner is trained on a single GPU using actual runtime measurements for reward. It takes less than 2.5 hours for the policy to achieve a placement with a training step time of 1.94 seconds. This optimization saves significant GPU-hours even after the initial training time. The training of the Hierarchical Planner can be interleaved with the target model training for greater efficiency. Comparisons were made against alternative policy architectures. The Hierarchical Planner, trained on a single GPU using actual runtime measurements for reward, outperforms the Simple Planner in finding placements for larger models like NMT with 4 or 8 layers. The Hierarchical Planner breaks the problem into grouping and placing sub-tasks, enabling it to scale to much larger models efficiently. The Hierarchical Planner's sequence-to-sequence model allows for conditioning placement of operations based on previous placements. A comparison was made between a Hierarchical Planner with randomized grouping and one using learned groupings, showing that learned groupings significantly reduce runtime. This hierarchical method efficiently places operations of a computational graph onto devices. Our hierarchical model efficiently assigns operations to groups and places them onto devices using a policy gradient method. It can scale to graphs with over 80,000 operations and outperforms previous methods by up to 60.6% on various tasks like image classification and machine translation. Unlike previous approaches, our method is end-to-end and requires no manual effort."
}