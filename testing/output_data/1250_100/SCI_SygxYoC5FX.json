{
    "title": "SygxYoC5FX",
    "content": "Representation learning techniques on graphs have shown significant effects in downstream machine learning tasks. The GraphSAGE framework, proposed by Hamilton and Ying, aims to inductively learn representations for unobservable graph structures. However, GraphSAGE lacks selective neighbor sampling and memory of known nodes. To address these issues, an unsupervised method is presented that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias for each node in the graph. Experimental results show that this approach outperforms state-of-the-art methods for representation learning on graphs in various real-world applications. Node embedding in large graphs is effective for prediction and analysis tasks. GraphSAGE leverages node features to generate embeddings for new nodes, but it struggles with sampling relevant neighbors. A new approach proposes selective neighbor sampling based on co-occurring structures to optimize node representations. GraphSAGE focuses on training parameters of aggregator functions but overlooks preserving memory of training nodes, leading to inefficiencies. To address this, a bi-attention architecture is introduced for selective neighbor sampling in unsupervised learning scenarios. This approach assigns different weights to neighbors of each node to improve relevance in representation learning. The bi-attention architecture BID16 is utilized for selective neighbor sampling in unsupervised learning to improve relevance in representation learning. Global embedding biases are applied to each node's aggregated embedding to combine transductive and inductive approaches, enhancing the memory of training nodes. In this paper, a novel approach called BIGSAGE is proposed for unsupervised and inductive node embedding learning in large and evolving network data. The approach utilizes bi-attention architecture and global bias to explore relevant neighbors and preserve learned knowledge of nodes. Various unsupervised learning approaches such as DeepWalk, node2vec, LINE, and SDNE have been used to learn node embeddings based on random-walks and proximity preservation. Some methods also incorporate node attributes in addition to network structure. Approaches like TRIDNR, CENE, TADW, and GraphSAGE require rich node attributes for embedding. Inductive methods like Bojchevski & G\u00fcnnemann's work well for large networks. Inductive learning focuses on single node information and local neighborhood. Attention mechanisms are widely used in Deep Learning applications. The paper introduces a hierarchical bi-attended sampling and global-biased aggregating framework (BIGSAGE) inspired by previous works on attention mechanisms. It proposes a bi-attention layer to capture useful information in the neighborhood of nodes in a network. The framework includes training and embedding generation algorithms, a detailed implementation of the bi-attention architecture, and the integration of global bias. The paper introduces BIGSAGE, a framework for learning node representations in unsupervised settings using a graph-based loss function. The algorithm involves training on a graph with node attributes and global embedding bias, utilizing random walks and negative sampling. After optimization, GraphSAGE may miss structural information, so BIGSAGE aims to preserve it by considering the entire neighborhood of nodes. BIGSAGE aims to preserve structural information by rerunning random walks on a full graph, using a bi-attention mechanism to generate relevant node embeddings, and averaging these embeddings for downstream machine learning tasks. The generation process is detailed in Algorithm 2. The framework utilizes a bi-attention mechanism to find the most relevant neighborhood match between nodes within a similarity matrix. By applying softmax and summing up by column or row, attention is focused on T neighborhoods. The aggregation process with bi-attention architecture is illustrated in FIG1. Training a global bias allows the framework to learn aggregator functions for inductive learning and preserve embedding information. This global bias is applied not only to the last layer but also to other layers for efficiency. Applying global bias vectors through all layers improves the expressivity of representations on hidden layers and training efficiency. The parameters from lower layers can be updated more directly by the involving global bias vectors, leading to instant impact on lower layers. The proposed aggregating process with global bias is evaluated against strong baselines on three benchmark tasks. In our comparison experiments, we evaluate the performance of BIGSAGE against GraphSAGE and Graph2Gauss. We analyze the impact of our bi-attention architecture and global bias separately. GraphSAGE utilizes different aggregators like Mean, LSTM, and Maxpool to learn graph representations, while Graph2Gauss focuses on node attributes for representation learning. Our method aims to strike a balance between sampling granularity control and embedding effectiveness in inductive learning. Our proposed approach, SPINE, utilizes RootedPageRank to represent high-order structural proximities in neighborhood nodes. Parameters such as hierarchical layers, neighbor sampling sizes, random walks, and embedding dimensionality are set for efficiency. The approach is implemented in Tensorflow and trained with the Adam optimizer. Comparison results are reported in a table. Inductive node embedding learning techniques in large and evolving graphs require efficient information extraction strategies and stability. The task involves predicting the community of a post in a forum using a dataset with word2vec embeddings. The dataset has 232,965 nodes/posts with an average degree of 492. Comparison results show that BIGSAGE outperforms GraphSAGE in mean and pool aggregator. Another evaluation is done on the Pubmed dataset with 19717 nodes and 44324 edges. 20% of nodes are removed for training. The protein-protein-interaction(PPI) networks dataset consists of 24 graphs corresponding to different human tissues. Preprocessed data provided by BID8 is used for training, validation, and testing. Each node has 50 features representing gene sets and 121 labels from gene ontology. Our method outperforms GraphSAGE by 14% on the PPI data, with the Mean-aggregator showing the best results. The micro-averaged F1 score of SPINE on the same dataset is also mentioned. In this section, adjustments are made to BIGSAGE for tests on PPI to study the effects of bi-attention layer and global bias. Three variants are explored: BIGSAGE-ba (bi-attention layer only), BIGSAGE-sg (bi-attention layer with global bias on last layer embeddings), and BIGSAGE-cb (bi-attention layer with global bias on all layers during training but reset to zero for embeddings). Results show improvements over GraphSAGE, with bi-attention layer proving effective and global bias enhancing efficiency. Memory of known nodes is crucial, as seen in BIGSAGE-cb. Comparison with Graph2Gauss on Pubmed and PPI datasets shows our model and GraphSAGE outperforming Graph2Gauss. BIGSAGE is an unsupervised and inductive network embedding approach that outperforms GraphSAGE and Graph2Gauss in evolving network data and generalizing across different graphs. It utilizes a bi-attention architecture to capture relevant representations of co-occurring nodes and combines inductive and transductive approaches efficiently. Experiments demonstrate its superiority over state-of-the-art baselines on unsupervised and inductive tasks."
}