{
    "title": "BJlA6eBtvH",
    "content": "Continual learning is the challenge of learning new tasks while retaining previously acquired knowledge. Catastrophic forgetting is a major issue for neural networks in dynamic data scenarios. A Differentiable Hebbian Consolidation model is proposed to address this, incorporating a rapid learning plastic component to retain learned representations. The model is evaluated on various benchmarks including Permuted MNIST and Split MNIST datasets, as well as an imbalanced variant of Permuted MNIST. The proposed model addresses catastrophic forgetting in neural networks when faced with non-stationarity in real-world deployment. This phenomenon hinders performance when models are trained with new data after initial learning, leading to a degradation in performance. In continual learning, deep neural networks (DNNs) aim to adapt and learn consecutive tasks without forgetting previous ones, facing challenges like concept drift and imbalanced class distributions. This stability-plasticity dilemma is a known issue for neural networks, requiring a balance between plasticity and stability for efficient long-term learning. In continual learning, deep neural networks face a stability-plasticity dilemma, balancing plasticity for integrating new knowledge and stability for preserving existing knowledge. Two theories explain human continual learning: synaptic consolidation in the neocortex and the complementary learning system theory. Recent work on differentiable plasticity shows neural networks with \"fast weights\" can leverage this balance effectively. Recent work on differentiable plasticity has demonstrated that neural networks can be trained end-to-end through backpropagation and stochastic gradient descent to optimize both standard \"slow weights\" and the amount of plasticity in synaptic connections. Fast weights, which change quickly based on input representations, act as short-term memory to reactivate long-term memory traces in slow weights. Networks with learned plasticity have shown superior performance compared to those with uniform plasticity. Various approaches have been proposed to address the catastrophic forgetting problem by dynamically adjusting synaptic plasticity based on importance for retaining past memories. Differentiable Hebbian Consolidation 1 model is developed to adapt quickly to changing environments and consolidate previous knowledge by adjusting synapse plasticity. The model combines traditional softmax layer modification with plastic weights in the final fully-connected layer. It integrates task-specific synaptic consolidation approaches to overcome catastrophic forgetting. The model unifies Hebbian plasticity, synaptic consolidation, and CLS theory for rapid adaptation to new tasks. The proposed method combines Hebbian plasticity, synaptic consolidation, and CLS theory to adapt quickly to new tasks and prevent catastrophic forgetting. Testing on benchmark problems shows superior performance with task-specific synaptic consolidation methods. Hebbian learning theory suggests that weight plasticity is crucial for learning and memory, involving the modification of existing synapse strength based on correlated neural activation. The Hebbian learning theory emphasizes the strengthening of connections between neurons through correlated activation. Recent studies have introduced fast weights in neural networks for one-shot and few-shot learning. Different approaches include augmenting FC layers with fast weights, Hebbian Softmax layers for rare class learning, and differentiable plasticity for optimizing synaptic connections. Our work introduces a method for training neural networks using slow and plastic weights, demonstrated on recurrent neural networks for pattern memorization and maze exploration. We address catastrophic forgetting by augmenting slow weights with fast weights using DHP, updating only the softmax output layer for fast learning and knowledge retention. Strategies include Task-specific Synaptic Consolidation and CLS Theory for memory retention and structured representations extraction. The neocortex gradually learns structured representations while the hippocampus performs rapid learning and storage of new instances. Task-specific synaptic consolidation methods inspired by overcoming catastrophic forgetting are used as regularization strategies in continual learning. These methods estimate the importance of each parameter or synapse to prevent changes to important parameters of previously learned tasks when learning new tasks. In continual learning, various regularization strategies are used to estimate the importance of parameters or synapses to prevent forgetting of previously learned tasks. Methods like Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS) compute the importance of parameters differently to control forgetting while learning new tasks. Our work is inspired by CLS theory, focusing on neuroplasticity techniques to address catastrophic forgetting. Previous approaches include pseudo-rehearsal, exact replay, and generative replay methods. iCaRL utilizes rehearsal and regularization with an external memory for storing exemplar patterns. Our goal is to leverage CLS theory for mitigating forgetting, building on earlier research on synaptic connections and long-term knowledge storage. Recent research has explored the use of slow and fast weights in neural networks to prevent catastrophic forgetting during continual learning. Various methods have been proposed, such as replacing soft attention mechanisms with fast weights, augmenting slow weights with a fast weights matrix, and incorporating differentiable plasticity. These approaches are designed for rapid learning on simple tasks or meta-learning over a distribution of tasks. The Hebbian Softmax layer adjusts parameters for one-shot and few-shot learning, switching between Hebbian and SGD updates based on task complexity. In continual learning, a local learning rule for fast weights is metalearned via slow weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights and a Hebbian plastic component with a plasticity coefficient and Hebbian trace. The Hebbian Softmax layer adjusts parameters for one-shot and few-shot learning, switching between Hebbian and SGD updates based on task complexity. In continual learning, a local learning rule for fast weights is metalearned via slow weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights and a Hebbian plastic component with a plasticity coefficient and Hebbian trace. The Hebbian traces accumulate hidden activations of the final hidden layer for each target label in the mini-batch. The network parameters are optimized by gradient descent as the model is trained sequentially on different tasks. The weight connection in the model has fixed weights, equivalent to setting plasticity coefficients to zero. The model updates Hebbian traces during training and uses them for predictions at test time. Hidden activations are accumulated directly into the softmax output layer weights, leading to better initial representations and long-term retention of learned representations. This optimization scheme prevents competing activations from different classes, allowing for fast learning. The model utilizes a highly plastic weight component for fast learning and improved test accuracy within a single task. The plastic component decays between tasks to prevent interference but selectively consolidates old memories. This approach enables the model to learn and remember by modeling plasticity over various timescales, forming a learned neural memory. The DHP Softmax method simplifies implementation without requiring additional space or computation, allowing for easy scalability with an increasing number of tasks. The DHP Softmax method improves learning of rare classes and speeds up binding of class labels to deep representations without introducing additional hyperparameters. It utilizes Hebbian Synaptic Consolidation to update network parameters online, similar to existing regularization strategies like EWC, Online EWC, SI, and MAS. A sample implementation using PyTorch is provided in Appendix B. The network parameters in our model are the weights of connections between pre-and post-synaptic activities of neurons. We adapt task-specific consolidation approaches and only regularize the slow weights of the network. Our model includes a plastic component in the softmax layer to prevent catastrophic forgetting. Comparison experiments with Online EWC, SI, and MAS show that our approach increases the capacity of the DNN. Our approach adds slow weights to the softmax output layer of a standard neural network to match the capacity increased by plastic weights. Tested on various benchmarks, including Permuted MNIST and Split MNIST, we evaluate model performance based on classification accuracy for all tasks learned so far. Forgetting is measured using the backward transfer metric, BWT, to assess the impact of learning new tasks on previous ones. In a benchmark study, neural networks were trained with Online EWC, SI, and MAS consolidation methods on sequential tasks. The input distribution changes between tasks, causing concept drift. The models used a multi-layered perceptron network with specific hyperparameters and a cross-entropy loss function. The plastic component's \u03b7 value was set at 0.001. The study compared the performance of a network with DHP Softmax to a fine-tuned vanilla MLP network, showing improvement in alleviating catastrophic forgetting. Task-specific consolidation methods were used, with DHP Softmax maintaining higher test accuracy during sequential task training. An ablation study examined network structural parameters and Hebb traces for interpretability. Figure 8 illustrates the behavior of \u03b7 during training on 10 tasks in the Permuted MNIST benchmark. Initially, \u03b7 increases rapidly in task T1, then decays after the 3rd task to reduce plasticity. The Frobenius Norm of the Hebb trace shows controlled growth, while the Frobenius Norm of \u03b1 indicates increasing plasticity coefficients within each task. Gradient descent and backpropagation are used for meta-learning to adjust structural parameters in the plastic component. The Imbalanced Permuted MNIST problem introduces imbalanced distributions for each task. The Imbalanced Permuted MNIST benchmark involves tasks with imbalanced class distributions, aiming to address challenges posed by class imbalance and concept drift. DHP Softmax achieves 80.85% accuracy after learning 10 tasks, showing a 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered longer, resulting in DHP Softmax with MAS achieving an average test accuracy of 88.80% and outperforming all other methods. In a study involving continual learning on a sequence of 5 vision datasets, DHP Softmax alone achieved 98.23% accuracy, showing a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and led to higher average test accuracy across all tasks, especially the most recent one, T5. In a study involving continual learning on a sequence of 5 vision datasets, DHP Softmax alone achieved 98.23% accuracy, showing a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and led to higher average test accuracy across all tasks, especially the most recent one, T5. FashionMNIST, SVHN, and CIFAR-10 datasets were used with a CNN architecture similar to previous studies. Training was done with mini-batches of size 32 and plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. DHP Softmax plus MAS resulted in a 2.14% improvement in average test accuracy over MAS alone, while SI with DHP Softmax outperformed other methods with an average test performance of 81.75% after learning all five tasks. The study demonstrated that adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax automatically adjusts the plastic connections, leading to improved performance across benchmarks compared to traditional softmax layers. This approach does not introduce additional hyperparameters and allows for effective learning without interference. The DHP Softmax method, without introducing extra hyperparameters, adjusts structural parameters \u03b1 and \u03b7 for plasticity. It shows flexibility by combining with Hebbian Synaptic Consolidation techniques like EWC, SI, or MAS to mitigate catastrophic forgetting in sequential learning tasks. DHP Softmax with SI performs best on Split MNIST and 5-Vision Datasets, while combining with MAS yields superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. Hebbian plasticity enables neural networks to learn continually and remember distant memories, reducing catastrophic forgetting in dynamic environments. Continual synaptic plasticity plays a key role in learning from limited labeled data and adapting at long timescales. This work aims to explore gradient descent optimized Hebbian consolidation for learning and memory in DNNs to enable continual learning on sequential tasks. Continual learning trains a model on sequential tasks with associated training data. Each task has its own loss function to prevent forgetting. The model learns an approximated mapping to the true function, mapping new inputs to target outputs for all tasks learned. Experiments were conducted on Nvidia GPUs, training on tasks with different classes using mini-batches and plain SGD optimization. We train for at least 10 epochs, perform early stopping if validation error doesn't improve for 5 epochs. If validation error increases for more than 5 epochs, training on task Tn is terminated, network weights and Hebbian traces reset to values with lowest test error, and move to next task. Hyperparameters for Permuted MNIST experiments: \u03bb=100 for Online EWC, \u03bb=0.1 for SI, \u03bb=0.1 for MAS. Grid search used for finding best hyperparameter combination for each consolidation method. For the Imbalanced Permuted MNIST problem, training samples are removed based on random probabilities from each class in the original MNIST dataset. The distribution of classes for each task is shown in Table 2. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters \u03bb were set to 400 for Online EWC, 1.0 for SI, and 0.1 for MAS. A grid search was performed to find the best hyperparameter combinations for each method. In the Split MNIST experiments, hyperparameters were adjusted accordingly. In Figure 2b, regularization hyperparameters \u03bb were set to 400 for Online EWC, 1.0 for SI, and 1.5 for MAS. A grid search was conducted to optimize these values for each synaptic consolidation method. The experiments were performed on a sequence of 5 tasks involving different image classification datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The notMNIST dataset comprises font glyphs for letters 'A' to 'J', with 500,000 training and 19,000 testing grayscale images of size 28\u00d728. FashionMNIST, SVHN, and CIFAR-10 are image classification datasets with different numbers of grayscale and color images of various sizes. The CNN architecture used for these datasets includes 2 convolutional layers with specific channel numbers and kernel sizes, followed by LeakyReLU nonlinearities and max-pooling operations. A multi-headed approach was employed due to different class definitions between datasets. In the benchmark problems, a trainable \u03b7 value is assigned to each connection in the final output layer, improving optimization stability and convergence. Separate \u03b7 parameters for each connection allow for individual modulation of plasticity rates. Using a single \u03b7 value across all connections led to optimization instability on certain tasks. Hyperparameters for the 5-Vision Datasets Mixture experiments included regularization values for different consolidation methods. Random search was conducted to find the best hyperparameter combination for each method. In the continual learning setup, sensitivity analysis was conducted on the Hebb decay term \u03b7 to assess its impact on test performance. Low values of \u03b7 were found to mitigate catastrophic forgetting across various benchmarks like Permuted MNIST and Split MNIST. Different values of \u03b7 were tested, showing that setting it low led to better performance. Average test accuracy for MNIST-variant benchmarks was presented, reflecting the sensitivity analysis results. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to a neural network through plastic connections. It emphasizes simplicity of implementation using popular ML frameworks. The model includes an initial learning rate value for plastic connections and updates Hebbian traces for the next iteration. The DHP Softmax model, implemented in PyTorch, incorporates compressed episodic memory into a neural network through plastic connections. It focuses on simplicity of implementation with popular ML frameworks. The model was tested on CIFAR-10 and CIFAR-100 datasets, outperforming Finetune in class-incremental learning. Test accuracies show DHP Softmax performing as well as training from scratch on some tasks."
}