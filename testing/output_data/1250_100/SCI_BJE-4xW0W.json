{
    "title": "BJE-4xW0W",
    "content": "We introduce causal implicit generative models (CiGMs) for sampling from observational and interventional distributions. Adversarial training can be used to learn a CiGM with a structured generator architecture based on a causal graph. A two-stage procedure is devised for learning a CiGM over binary labels and images, using Wasserstein GAN and conditional GAN. New architectures CausalGAN and CausalBEGAN are proposed for conditional GAN. The optimal generator of CausalGAN samples from image distributions conditioned on labels, demonstrating the effectiveness of the proposed architectures. Implicit generative models like GANs can sample from high-dimensional distributions without explicit parameterization. GANs use a generator network to produce samples and a discriminator network to distinguish between real and generated samples. The goal is for the generator to fool the discriminator into thinking its samples are real. GANs have been successful in generating images and other types of data. GANs have been successful in generating samples from distributions like images and videos. An extension of GANs involves sampling from class conditional data distributions by providing class labels to the generator. Various neural network architectures have been proposed for this task. The architecture discussed in the paper aims to capture the dependence between labels and the causal effect between labels in conditional image generation. The architecture discussed in the paper aims to capture the causal effect between labels in conditional image generation. Labels determine the image distribution, with the generator mapping labels to images non-deterministically. Causal graphs show relationships like Gender causing Mustache, allowing for sampling from conditional and interventional distributions. The paper discusses causal implicit generative models (CiGM) that can sample from joint, conditional, and interventional probability distributions based on a given causal graph. The architecture aims to capture causal effects in conditional image generation by using Wasserstein GAN (WGAN) to train CiGM for binary image labels. The paper introduces a two-step procedure for training a CiGM for images and image labels, utilizing CausalGAN and CausalBEGAN. Adversarial training is used to structure the generator architecture based on the causal graph, showing that WGAN can train a CiGM for binary labels. Additionally, a novel conditional GAN architecture and loss function are proposed for conditional and interventional sampling of images. The paper introduces CausalBEGAN, an extension of BEGAN that accepts labels, producing high-quality images. The CiGM training framework is evaluated on labeled CelebA data, showing label-consistent images even for interventions not seen during training. Previous works on conditional GANs are discussed, including CGAN, ACGAN, and InfoGAN. In BID15, authors introduce InfoGAN, a new architecture aiming to maximize mutual information between inputs and images. BiGAN and ALI extend the GAN framework by learning a mapping from image space to latent space. CoGAN learns a joint distribution over image and binary label by enforcing weight sharing. SD-GAN splits the latent space into \"Identity\" and \"Observation\" portions, allowing for generation of faces with fixed identity portion. This is the only extension of BEGAN to accept labels before CausalBEGAN. In BID0, authors use CGAN with a one-hot encoded vector to change the age attribute of a face image. Generative models are also used in compressed sensing, with guarantees for recovering a vector close to the output of a trained model. Recent attention has been on using causal principles in deep learning, such as observing the connection between GAN layers and structural equation models in BID3. BID5 proposes using a neural network to discover causal relations between image class labels. Bahadori et al. introduce causal regularization for training neural networks to ensure predictive causality. In recent works, authors have explored the connection between GANs and causal generative models. They use neural networks to learn causal graphs, mimicking structural equations. Causality is introduced through Pearl's framework, which involves structural causal models using directed acyclic graphs to represent causal relationships between random variables. The causal sufficiency assumption states that if X causes Y, there exists a function and an unobserved variable E, independent from X, determining the value of Y based on X and E. This is represented in a causal graph as X \u2192 Y. A causal graph is a directed acyclic graph implied by structural equations, where the parents of a node represent the causes of that variable. A structural causal model consists of functions, random variables, exogenous variables, and a probability distribution. The causal graph is constructed based on the relationships between variables and functions. An intervention changes the underlying causal mechanism in the model. Intervention changes the causal mechanism and corresponding causal graph by disconnecting nodes from their parents. Post-interventional distribution in Bayesian networks can be calculated by factorizing the observational distribution. Identifying the true causal graph for a set of variables is generally not possible. In general, identifying the true causal graph for a set of variables without experiments or additional assumptions is challenging. This paper focuses on learning a causal model given a known causal graph, exploring the use of Bayesian networks to sample from observational distributions. The proposed causal implicit generative models allow sampling from both observational and interventional distributions. Generative adversarial networks can be used to train causal implicit generative models by arranging the generator neural network connections to reflect the causal graph structure. This approach allows for sampling from both observational and interventional distributions, with feedforward neural networks representing the functions f X , f Y , f Z. The noise terms (N X , N Y , N Z) can be chosen as independent, and Gaussian distributed variables can be used for a rich enough function class. This feedforward neural network can represent causal models with a specific graph structure. Generative adversarial networks can train causal implicit generative models by aligning the generator neural network with the causal graph structure. This enables sampling from observational and interventional distributions using feedforward neural networks. The noise terms can be independent Gaussian variables, allowing for representation of causal models with a specific graph structure. CausalGAN architecture divides the task of learning a CiGM into two subtasks: training a generative model over labels and then training a generative model for images conditioned on the labels. The new architecture and loss function ensure that the generator outputs label-conditioned image distributions, assuming a strictly positive joint probability distribution over the labels. The CausalGAN architecture introduces a Causal Controller for generating images based on labels. The model utilizes WGAN for sampling from a discrete label distribution, overcoming limitations of standard GAN training. The CausalGAN architecture introduces a new conditional GAN model with a Causal Controller to generate images based on labels. It includes a Labeler and Anti-Labeler neural networks, along with a generator that aims to produce realistic images consistent with the labels. CausalGAN utilizes an Anti-Labeler network to prevent label-conditioned mode collapse and improve convergence speed. The Anti-Labeler discourages the generator from outputting only typical faces for a fixed label combination, addressing issues with diversity within a batch of images. Results for a single binary label are presented, with potential extension to more labels. The CausalGAN model incorporates a perfect Causal Controller and various mappings for the generator, discriminator, Labeler, and Anti-Labeler. The generator loss function includes label loss terms, GAN loss, and an additional loss term from the discriminator. By adding this term, the optimal generator can output the class conditional image distribution. The model also works for multiple binary labels. The optimization problems for the Anti-Labeler, Labeler, discriminator, and generator are outlined. The CausalGAN generator samples from the class conditional image distribution when the Causal Controller is at its best. The optimal generator in the CausalGAN model samples from the class conditional image distribution when the discriminator, Labeler, and Anti-Labeler are at their best. The global minimum of the virtual training criterion is achieved when the generator's output matches the true label distribution. The two-stage procedure can train a causal implicit generative model for any causal graph where the Image variable is a sink node. The generator samples from the class conditional distributions given a single binary label, and the objective is to extend this to cases with multiple binary labels. The authors propose an alternative architecture for cases with multiple binary labels, extending the single binary label setup. The generator aims to capture the joint label posterior given the image, ensuring that the class conditional distributions align with the data distribution. The authors propose an alternative architecture for cases with multiple binary labels, extending the single binary label setup. The generator samples from the class conditional distributions. To accommodate interventional sampling, they use a Causal Controller to produce labels. A Labeler network is used for architecture modifications, serving the dual purpose of labeling real images well and generated images poorly. The authors introduce a new architecture for handling multiple binary labels, utilizing a Causal Controller for interventional sampling. They train CausalGAN and CausalBEGAN on the CelebA Causal Graph, showing the effectiveness of margin modifications in improving label and image quality. The authors proposed a novel generative model with label inputs, allowing for sampling from interventional distributions. They demonstrated the effectiveness of margin modifications in improving label and image quality using CausalGAN and CausalBEGAN on the CelebA Causal Graph. The authors introduced a generative model with label inputs for sampling from interventional distributions. They showed how margin modifications improved label and image quality using CausalGAN and CausalBEGAN on the CelebA Causal Graph. The research highlighted the impact of causality on generative models, making them more creative in producing diverse samples. The authors introduced a generative model with label inputs for sampling from interventional distributions, improving label and image quality using CausalGAN and CausalBEGAN on the CelebA Causal Graph. The model highlighted the impact of causality on generative models, enhancing creativity in producing diverse samples. The causal graph D represents a Bayesian network for the joint probability distribution over observable variables V, with interventional distributions directly calculable from conditional probabilities and the causal graph. In this section, the authors discuss the joint data distribution and the generator's distribution. Proposition 2 states the optimal discriminator for a fixed generator. The optimum Labeler and Anti-Labeler are identified, with Lemmas 1 and 2 providing insights into their optimal strategies. The definitions assume causal sufficiency in the absence of exogenous variables affecting multiple observables. The authors discuss causal sufficiency and the distribution of exogenous variables in Pearl's model. They introduce the complete graph \"cG1\" and its reverse rcG1. Theorem 1 defines the generator loss C(G) and conditions for achieving the global minimum. The optimal Labeler, Anti-Labeler, and discriminator are determined, leading to the Kullback-Leibler objective. The text discusses a causal implicit generative model and a generator that can sample from the image distribution conditioned on given labels. The generator objective minimizes the Kullback-Leibler divergence, ensuring joint distribution consistency over labels and images. The model is consistent with the causal graph, and if perfect, samples from the true label and image distributions. The concatenated model serves as a causal implicit generative model for graph D, allowing sampling from true observational and interventional distributions. Generalizing to multiple binary labels poses challenges, with solutions including estimating label combinations or relying on Labelers to estimate individual label probabilities. The section presents an extension of estimating label probabilities and introduces the optimum Labeler with respect to loss. It discusses the Lemma and the Labeler loss function, emphasizing the importance of considering only combinations with positive probability to achieve a finite loss. The optimum Labeler network provides the posterior probability of label combinations based on observed images. The Anti-Labeler network optimizes the conditional entropy of labels given the generated images. The generator optimizes the conditional entropy of labels given the image under a fixed discriminator, Labeler, and Anti-Labeler. The global minimum of the virtual training criterion is achieved when the generator samples from the class conditional image distributions given a specific label combination. Theoretical guarantees for the CausalGAN architecture with d labels are provided under the assumption of deterministic relationship between images and labels. The Anti-Labeler optimizes the generator's conditional entropy of labels given the image, while the generator aims to sample from class conditional image distributions. The generator in the CausalGAN architecture aims to achieve the global minimum of the virtual training criterion by matching the distributions of generated and real images for all labels. To ensure correct conditional sampling, it is assumed that the image determines all labels, which is practical in datasets like CelebA. The person's gender and facial hair can be determined as a function of the image. A lemma states that a joint probability distribution with kronecker delta functions as marginal distributions is the product of these marginals. The joint distribution is zero everywhere except at specific points, leading to a contradiction. The text discusses the application of Bayes' rule to show that the optimum generator samples from class conditional image distributions in a proposed extension of BEGAN. The joint distribution is proven to be a product distribution based on kronecker delta functions as marginal distributions. In this section, a new extension of BEGAN is proposed where image labels are fed to the generator. The extension introduces a new loss and margins that reflect the idea that label gradients are most informative when image quality is high. The loss functions are formulated to include labels in the generator training process. The loss functions for incorporating image labels in the generator training process include a margin-coefficient tuple (b2, c2) to address the use of margins in the BEGAN formulation. The generator minimizes two loss terms, but image quality may suffer if label loss is incorporated without considering the image quality margin. A new margin of margins term, b3, is introduced to encourage the generator to incorporate label loss only when the image quality margin is large compared to the label margin. The margin equations and update rules are summarized with learning rates for the coefficients. Our extension of BEGAN introduces a monotonically decreasing scalar to track gradient descent optimization convergence. We explore the behavior of causal implicit generative models on synthetic data from different causal graphs, such as \"line\", \"collider\", and \"complete\". Each node in the dataset is computed using a cubic polynomial, and we report averaged results from 20 runs for each model. The study explores causal implicit generative models on synthetic data from different causal graphs (line, collider, complete). Results from 20 runs for each model are compared in terms of convergence to the true joint distribution. Generators structured based on different causal graphs show varying convergence behavior. The study compares the convergence behavior of different generator architectures based on causal graphs (line, collider, complete) when trained with adversarial training. Results show that line graph and complete graph architectures perform well, while fully connected networks with 3 layers show good performance but those with 5 and 10 layers perform worse. Using the wrong Bayesian network, the collider, also leads to worse performance. The study compares the convergence behavior of different generator architectures based on causal graphs (line, collider, complete) when trained with adversarial training. Fully connected networks with 3 layers show the best performance, while 5 and 10 layers perform worse. Line and collider graphs perform poorly, with the complete graph showing decent performance. Using the wrong Bayesian network, the collider, also leads to worse performance. Data is generated using the causal graph X1 \u2192 X2 \u2192 X3. The study compares different generator architectures based on causal graphs for adversarial training. The correct graph gives the closest scatter plot to the original data, while using the wrong Bayesian network results in a different distribution. The CelebA Causal Graph is used for experiments, showing that using the incorrect Bayesian network generates Male and Young independently, which is incorrect. The study compares different generator architectures based on causal graphs for adversarial training. The CelebA Causal Graph generates Male and Young independently, which is incorrect. Comparison of pairwise distributions shows that a reasonable approximation to the true distribution is learned for {Male, Young} jointly. Despite inaccuracies, both graphs lead to Causal Controllers that never output the label combination {Female, Mustache}. Wasserstein GAN with a modified version assures convergence in distribution of the Causal Controller output to the discretely supported distribution of labels. Good convergence is demonstrated for both graphs in the sampled joint label distribution. The study compares different generator architectures based on causal graphs for adversarial training. The CelebA Causal Graph generates Male and Young independently, which is incorrect. Comparison of pairwise distributions shows that a reasonable approximation to the true distribution is learned for {Male, Young} jointly. Despite inaccuracies, both graphs lead to Causal Controllers that never output the label combination {Female, Mustache}. Wasserstein GAN with a modified version assures convergence in distribution of the Causal Controller output to the discretely supported distribution of labels. Good convergence is demonstrated for both graphs in the sampled joint label distribution. In addition, the Wasserstein Causal Controller is tested on a subset of binary labels from the CelebA dataset, showing that the generator can learn a mapping from continuous noise to a discrete distribution with high accuracy. The study compares different generator architectures based on causal graphs for adversarial training. The CelebA Causal Graph generates Male and Young independently, which is incorrect. Comparison of pairwise distributions shows that a reasonable approximation to the true distribution is learned for {Male, Young} jointly. Despite inaccuracies, both graphs lead to Causal Controllers that never output the label combination {Female, Mustache}. Wasserstein GAN with a modified version assures convergence in distribution of the Causal Controller output to the discretely supported distribution of labels. Good convergence is demonstrated for both graphs in the sampled joint label distribution. In addition, the Wasserstein Causal Controller is tested on a subset of binary labels from the CelebA dataset, showing that the generator can learn a mapping from continuous noise to a discrete distribution with high accuracy. The discrete distribution shows that 96% of the samples appear in 0.05\u2212neighborhood of 0 or 1. TVD is used as a measure of convergence, with cG1 and rcG1 showing decreasing TVD to 0 while G1 asymptotes to around 0.14. This suggests that any complete causal graph can lead to a nearly perfect implicit causal generator over labels, and partially incorrect causal graphs can still give reasonable convergence. Additional results are presented in FIG3, 13, showing the effects of intervening vs conditioning on Wearing Lipstick in the CelebA Causal Graph. The CelebA Causal Graph is used to train CausalBEGAN on the CelebA dataset. Intervening on Narrow Eyes does not affect Smiling, but conditioning on Narrow Eyes increases the proportion of smiling images. The Causal Controller is pretrained with a Wasserstein loss for training the CausalBEGAN. The need for the margin of margins in the training process is empirically justified. In the CelebA Causal Graph, intervening on Bald and Mouth Slightly Open labels shows differences in image quality for rare labels. Conditioning on these labels affects the probability of certain features in the dataset. In this section, additional simulations for CausalGAN and CausalBEGAN are provided. The conditional image generation properties of CausalGAN are demonstrated by sweeping a single label while keeping others fixed. Image diversity and mode collapse are examined with 256 randomly sampled images. For CausalBEGAN, the impact of the third margin term on image quality of rare labels is shown. The extension of a scalar \"M\" that decreases monotonically during training is also discussed. The CausalBEGAN architecture learns a discrete function for label input parameters, with label interpolation initially and later becoming more step-like. Image diversity is shown through random sampling of 256 images. Joint training of an implicit causal generative model for labels and images is attempted, treating the image as part of the causal graph. One hypothesis is that the discriminator focuses on labels without providing useful gradients to the image. The implementation details of the Wasserstein Causal Controller for generating face labels are explained, using the total variation distance as a metric. The gradient term penalty is estimated by evaluating points interpolated between real and fake batches. The generator architecture is structured based on a causal graph, with uniform noise as exogenous variables and neural networks mapping parents to children. Training involves 25 Wasserstein discriminator updates per generator update. In training our model, we use 25 Wasserstein discriminator updates per generator update with a learning rate of 0.0008. We employ DCGAN Radford et al. (2015) as the base model and enhance it with Labeler networks and a Causal Controller network. Unlike DCGAN, we perform 6 generator updates for each discriminator update on average. The discriminator and labeler networks are updated concurrently in a single iteration. The loss terms are modified to accommodate a d-dimensional label vector, with the Labeler and Anti-Labeler loss terms being averaged for every label. This differs from the architecture where the discriminator outputs a length-2d vector to estimate label probabilities. In the implementation, the order of terms in the cross entropy expressions for labeler losses was swapped to improve image sharpness during training. The labels input to CausalBEGAN are from the Causal Controller, with minimal parameter tuning using a learning rate of 0.00008 for both generator and discriminator. The model is not very sensitive to parameter values, achieving good performance without hyperparameter tweaking. Customized parameter values like \u03b31 = \u03b32 = \u03b33 = 0.5 are used. In the study, customized margin learning rates are utilized to reflect the asymmetry in generator response. The best models exhibit active margins near 0 while occasionally taking small positive values. Comparing CausalGAN with and without Anti-Labeler network shows that using Anti-Labeler leads to faster convergence and more diverse images for rare labels."
}