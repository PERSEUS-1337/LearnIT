{
    "title": "rJleN20qK7",
    "content": "In this work, a two-timescale network (TTN) architecture is introduced to enable linear methods to learn values with a nonlinear representation learned at a slower timescale. The approach allows for the use of algorithms developed for linear settings to provide nonlinear value estimates. Convergence for TTNs is proven, with empirical demonstrations showing the benefits compared to traditional methods like temporal difference learning and Q-learning. Value function approximation relies on the quality of state representation. Strategies include designing a basis for linear function approximation or learning representations with neural networks. Learning representations allows for scaling to high-dimensional observations like images. Linear function approximation for estimating value functions has benefits over nonlinear estimators, enabling least-squares methods and eligibility traces for faster learning. Various algorithms have been developed for linear setting, both for on-policy and off-policy learning, with theoretical and empirical exploration. In this work, a simple strategy is pursued to leverage the advantages of linear methods while learning the representation. Two learning processes run in parallel: one learns nonlinear features using a surrogate loss, and the other estimates the value function linearly. These Two-timescale Networks (TTNs) converge by effectively fixing the features for the fast linear value function estimator. This approach differs from previous basis adaptation methods by separating the loss for the representation and value function. The separation of the loss for the representation and value function in Two-timescale Networks (TTNs) enables simpler objectives for driving the representation while still using the mean squared projected Bellman error (MSPBE). This approach avoids the complexity of nonlinear MSPBE but maintains its useful properties. Previous basis adaptation methods did not separate the value learn and basis adaptation, resulting in more complex algorithms. Two-timescale Networks (TTNs) offer a promising approach for nonlinear function approximation by using two different heads to drive the representation and learn the values. The effectiveness of TTNs is demonstrated through empirical evidence showing their superiority over other nonlinear value function approximations, especially in utilizing benefits from linear value approximation algorithms. Linear policy evaluation algorithms can be selected using Two-timescale Networks (TTNs), which enable the use of fitted Q-iteration as an alternative to target networks for control with neural networks in a finite Markov Decision Process (MDP). The goal is to compute the value function under a fixed policy \u03c0, defined recursively for each state s \u2208 S. When using linear function approximation, the goal is to find parameters w \u2208 R d to approximate the value function. Linear policy evaluation algorithms aim to find parameters w \u2208 R d to approximate the value function V \u03c0 under a fixed policy \u03c0. Nonlinear function approximation can be used to estimate V \u03c0, with a focus on the objective of learning the function V. The Bellman operator B \u03c0 defines a recursive formula for the fixed point, which may require finding a projected fixed point \u03a0 F B \u03c0V = V when using a restricted value function class like linear value functions. Various linear policy evaluation algorithms, such as TD, least-squares TD, and gradient TD, estimate this projected fixed point. However, the objective for this projected fixed-point is more complex for nonlinear function approximation compared to linear function approximation. The projection operator simplifies for linear function approximation, but for nonlinear function classes, it becomes more complex and expensive to compute. The nonlinear mean-squared projected Bellman error (MSPBE) and resulting algorithm are complex and have not seen widespread use. Two-timescale Networks (TTNs) are introduced as a strategy for nonlinear value function approximation, along with different surrogate objectives within TTNs that are useful. Two-timescale Networks (TTNs) use concurrent optimization processes for network parameters \u03b8 and value function parameters w. The slow process for \u03b8 changes the representation, while the fast process for w updates the value estimate. The separation between these processes can be problematic as the target problem of estimating the value function does not influence the representation. Selecting a surrogate loss related to the value estimation process is crucial for TTNs. The slow part of the network in Two-timescale Networks (TTNs) minimizes the mean-squared TD error (MSTDE) as a surrogate loss for estimating the value function. This allows for straightforward stochastic gradient descent, but it has been found to provide worse value estimates compared to the mean-squared Bellman error (MSBE). The MSTDE is a surrogate loss for estimating the value function in Two-timescale Networks, providing worse value estimates than the MSPBE. Other surrogate losses related to the value function, like MSRE, suffer from sampling issues, while the MSBE has the double sampling problem. The gradient of MSTDE is simpler than MSPBE, making it a simple choice for updating the representation. The ability to predict the next state and reward is important for enabling value prediction in reinforcement learning. Various methods, such as using auxiliary tasks or self-supervised tasks, have shown success in learning representations that aid in predicting next states and rewards. These tasks can be used for the surrogate objective without degrading performance. Separating representation learning in TTNs does not impact performance. The focus is on simpler surrogate objectives to demonstrate the effectiveness of this approach. TTNs are trained online using stochastic gradient descent to reduce surrogate loss. A linear policy evaluation algorithm is integrated with the network. Convergence of TTNs requires the network to evolve slowly relative to linear prediction weights. This is achieved by adjusting step sizes to ensure equilibrium between the network and linear components. The linear prediction algorithms in neural networks need to converge for any set of features, especially linearly dependent ones, which can be a technical bottleneck. To address this, a differential inclusion based analysis is used for GTD2. Stability of the iterates is ensured by projecting them to compact, convex sets. The analysis shows that TTNs converge asymptotically to stable equilibria, capturing the mean dynamics of the algorithm. The results are applicable when using TD(\u03bb) or GTD2 as the linear prediction method. The TTN algorithm converges almost surely to stable equilibria within a compact, convex subset. It outperforms other nonlinear policy evaluation algorithms and optimizes the MSPBE for value estimates. The TD(\u03bb) algorithm also converges almost surely to a limit w* within the TTN setting. The TTN algorithm is compared to other nonlinear policy evaluation algorithms to determine if it provides advantages. Experiments were conducted in five environments, including Puddle World, Acrobot, Cartpole, Catcher, and Puck World. Different observation types were used to analyze algorithm performance in low-dimensional and high-dimensional settings. The algorithms were evaluated in various environments, including Puddle World, Acrobot, Cartpole, Catcher, and Puck World. The value estimates were assessed using root-mean-squared value error (RMSVE) across 500 states. Different settings were used for the algorithms, such as minimizing mean-squared TD error (MSTDE) using the AMSGrad optimizer. The neural network architecture varied between environments, with a single hidden layer of 128 units in Puddle World and 256 units in other environments. Hyperparameters were chosen after a preliminary sweep on a broad range. In Appendix D, results are reported for hyperparameters in a refined range, chosen based on RMSVE over the latter half of a run. TTN is compared to various algorithms, showing competitive performance, especially in Puddle World. Nonlinear GTD also performs well, indicating advantages for theoretically-sound algorithms. The utility of optimizing the MSPBE is highlighted, with TTN benefiting from a second head learning at a faster timescale. In Appendix D, results are reported for hyperparameters in a refined range, chosen based on RMSVE over the latter half of a run. TTN is compared to various algorithms, showing competitive performance, especially in Puddle World. Nonlinear GTD also performs well, indicating advantages for theoretically-sound algorithms. The utility of optimizing the MSPBE is highlighted, with TTN benefiting from a second head learning at a faster timescale. First, we show that TTN benefits from having a second head learning at a faster timescale by comparing prediction errors using different optimization processes. Results in FIG2 show that optimizing the MSPBE gives better results than optimizing the MSTDE. Despite being a poor objective for learning the value function, using the MSTDE can still be effective for driving feature-learning. TTNs provide flexibility in choosing linear policy evaluation algorithms for the fast part. The text discusses the use of newer temporal difference methods like GTD and ETD, along with their true-online variants, for learning the value function. These methods offer better convergence properties and increased stability. Least-squares methods are often avoided due to computational issues, but for TTNs, there is no disadvantage in using LSTD methods. FLSTD, which progressively forgets older interactions, is also mentioned as advantageous when feature representation changes over time. Incremental versions of least-squares algorithms are used for TTN to maintain estimates online. The text discusses the use of least-squares algorithms with eligibility traces for online estimation of quantities in TTNs. Nonlinear function approximation poses challenges for deriving eligibility traces, but they can be extended for use in nonlinear TD(\u03bb). Results show that TTNs benefit from using different linear policy evaluation algorithms, with LSTD being dominant. Sensitivity analysis reveals that most TTN variants benefit from a nonzero \u03bb value, except for least-squares methods where LSTD performs similarly across \u03bb values. Surrogate loss functions were compared in the experiments, showing that different objectives can be used. Results indicate that there is no universally superior surrogate loss, and choosing the appropriate one can yield benefits in certain domains. Preliminary results for the control setting were also provided, with standard additions to competitor learning algorithms included. The DQN algorithm uses experience replay and a target network to stabilize training. An alternative strategy for target networks is used in TTN, inspired by fitted Q-iteration. TTNs allow for direct use of FQI, solving for weights on the entire replay buffer. Regularization is incorporated to prevent significant weight changes between updates. Each FQI iteration involves solving a least squares problem on the entire buffer. TTN utilizes FQI for weight updates on the entire replay buffer, reducing computation to O(nd^2/k). Experimental details differ for control, with hyperparameter tuning for TTN and DQN on non-image Catcher. TTN performs well on both non-image and image Catcher versions. TTN performs well on both versions of Catcher, learning more quickly than DQN variants. It achieves higher average returns in the image version and stabilizes on a better policy despite suffering from catastrophic forgetting. TTNs are a promising direction for improving sample efficiency in control, designed to be simple and easy to train with convergence guarantees for various learning components. TTNs offer guarantees for a wide range of learning choices, improving learning by enabling the use of linear methods like least-squares algorithms and eligibility traces. Stochastic approximation algorithms with traces show significant effects in TTNs, unlike nonlinear TD methods. The use of traces in TTNs is a promising outcome, allowing for exploration of linear value function algorithms in complex domains with learned representations. Additionally, TTNs present opportunities for off-policy learning and the potential use of emphatic algorithms for improved asymptotic properties. TTNs offer guarantees for various learning choices, enabling the use of linear methods like least-squares algorithms and eligibility traces. Off-policy learning in TTNs can avoid variance issues from large updates, providing stability in learning. Preliminary experiments support this hypothesis, showing the effectiveness of TTNs in driving learning with a different objective. The text discusses mathematical definitions and assumptions related to the Frechet differentiability of a function in the context of TTNs. Key concepts include inner-product, norm, feature matrix, Markov chain properties, and step-size sequences. The assumptions ensure stability and convergence in learning algorithms. The text discusses the asymptotically stationary Markov chain and the existence of a unique steady-state distribution. It also introduces a sample trajectory of the transition dynamics in a Markov Decision Process (MDP) and analyzes the long-run behavior of algorithms using ODE-based analysis. This analysis guarantees that the limit points of the stochastic recursion will belong to compact connected sets. The stochastic recursion's limit points will almost surely belong to the compact connected internally chain transitive invariant set of the equivalent ODE. The algorithm follows a multi-timescale stochastic approximation framework, utilizing a more generalized multi-timescale differential inclusion based analysis. There is a unilateral coupling between the neural network and policy evaluation algorithms, where the algorithms depend on feature vectors \u03b8 t but not vice-versa. Due to stability concerns, a projected stochastic recursion is considered for analysis. The stochastic recursion updates \u03b8 t using a projection onto a compact and convex subset \u0398. The iterates {\u03b8 t} converge almost surely to a set of stable equilibria K inside \u0398. The analysis is based on an ODE framework, with a unilateral coupling between neural network and policy evaluation algorithms. The stochastic recursion updates \u03b8 t using a projection onto a compact and convex subset \u0398. The iterates {\u03b8 t} converge almost surely to a set of stable equilibria K inside \u0398. The analysis is based on a ODE framework with a unilateral coupling between neural network and policy evaluation algorithms. \u0393 \u0398 is a Lipschitz continuous function in \u03b8 and t+1 is a truncated martingale difference noise. The noise sequence {M 1 t+1} is a martingale-difference noise sequence w.r.t to the filtration {F t+1}, and by taking t \u2192 \u221e, C3 follows directly from the ergodicity and finiteness of the underlying Markov chain. Iterates {\u03b8 t} are stable and bounded almost surely. The stochastic recursion updates \u03b8 t using a projection onto a compact and convex subset \u0398, ensuring stability. The recursion converges to stable equilibria of an ODE inside \u0398. Determining \u0398 without prior knowledge of the ODE's limit set is challenging. A pragmatic approach is to start with an arbitrary convex, compact set and gradually expand. It is important to consider the Lipschitz continuity of \u0393 with respect to the features\u0176\u03b8. The stochastic recursion updates \u03b8 using a projection onto a compact and convex subset \u0398 for stability. The Lipschitz continuity of \u0393 with respect to the features is crucial. The TD(\u03bb) algorithm with linear function approximation can estimate the value function using a neural network. Parameters include \u03b1 t > 0, \u03bb \u2208 [0, 1], and initialization values for w and e. The step-size sequences converge relatively faster to 0. The asynchronous convergence behavior of the feature parameter sequence {\u03b8 t } is influenced by the disparity in learning rates between the neural network SGD and the TD(\u03bb) recursion. This pseudo heterogeneity induces multiple perspectives, with the slower timescale recursion appearing quasi-static and the faster timescale recursion equilibrated. Analyzing the asymptotic behavior reveals the neural network SGD as quasi-stationary, while the faster timescale stochastic recursion exhibits equilibration. The asynchronous convergence behavior of the feature parameter sequence {\u03b8 t } is influenced by the learning rate difference between neural network SGD and TD(\u03bb) recursion. The faster timescale stochastic recursion equilibrates while the neural network SGD is quasi-stationary. The TD(\u03bb) algorithm converges almost surely within the TTN setting, with similar results for ETD, LSPE, and LSTD methods. However, GTD2 and TDC algorithms cannot be directly applied without the non-singularity assumption of specific matrices. In the TTN setting, ensuring the boundedness of GTD2/TDC iterates without the non-singularity assumption is challenging. Due to the neural network administration of features, controlling the network for desired non-singularity characteristics is not straightforward. Therefore, projected versions of these algorithms, like the projected GTD2 algorithm, are considered. Projection operators onto predetermined convex, compact subsets are used to ensure uniqueness and stability. The GTD2 algorithm in the TTN setting involves a projection onto a compact subset of R d with a smooth boundary to ensure stability. The algorithm utilizes predetermined step-size sequences and follows a quasi-stationary argument. The asymptotic behavior of the algorithm is analyzed under the assumption of a quasi-static feature vector. The equations in the algorithm constitute a multi-timescale stochastic approximation recursion with bilateral coupling between stochastic recursions. The step-size sequences satisfy a specific condition for convergence. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between stochastic recursions. The step-size sequences satisfy a specific condition for convergence, leading to a pseudo asynchronous convergence behavior due to disparity in learning rates. The faster timescale recursion appears quasi-static when observed from the slower timescale, and vice versa. The slower timescale stochastic recursion can be considered quasi-stationary, while analyzing the limiting behavior of the faster timescale recursion. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between recursions. The faster time-scale recursion of the GTD2 algorithm is analyzed under a quasi-stationary premise. The iterates {w t } are stable, and {M 2 t+1 } is a martingale-difference noise sequence. The recursion is Lipschitz continuous with respect to w. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between recursions. The iterates {w t} are stable, and the recursion is Lipschitz continuous with respect to w. By appealing to Theorem 2, Chapter 2 of BID4, it is concluded that the stochastic recursion asymptotically tracks an ODE, with w t converging to stable equilibria almost surely. Qualitative analysis shows that the stable limit set is the solutions of a linear system inside W. The linear system of equations is consistent and can be viewed as the least squares solution to a specific equation. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between recursions. The iterates {w t} are stable, and the recursion is Lipschitz continuous with respect to w. The stochastic recursion asymptotically tracks an ODE, with w t converging to stable equilibria almost surely. The stable limit set is the solutions of a linear system inside W, consistent with the least squares solution to a specific equation. The slower time-scale stochastic recursion of the GTD2 algorithm is analyzed, showing the quasi-stationary condition and rearranging equations accordingly. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between recursions. The recursion is Lipschitz continuous with respect to w and asymptotically tracks an ODE. The slower timescale recursion has multiple limit points, unlike previous assumptions. In the TTN setting, non-singularity of certain matrices cannot be guaranteed due to features provided by a neural network. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between recursions. The algorithm generates features with desired non-singularity properties. Analyzing the algorithm's behavior under the relaxed singularity setting involves viewing the stochastic recursion as a stochastic recursion inclusion and applying recent results on multi-timescale stochastic recursive inclusions. Key observations include the singularity of h3(u) for each u \u2208 U and the boundedness of W and U leading to a martingale-difference noise sequence. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between recursions. The algorithm generates features with desired non-singularity properties. Analyzing the algorithm's behavior under the relaxed singularity setting involves viewing the stochastic recursion as a stochastic recursion inclusion and applying recent results on multi-timescale stochastic recursive inclusions. Key observations include the singularity of h3(u) for each u \u2208 U and the boundedness of W and U leading to a martingale-difference noise sequence. The asymptotic behavior of the GTD2 algorithm is characterized by the set of asymptotically stable equilibria of a specific ODE. The main result states that the stochastic sequence generated by the TTN converges almost surely to a set of stable equilibria. Additionally, the TD(\u03bb) algorithm within the TTN setting converges almost surely to a limit w* under certain assumptions. The GTD2 algorithm in the TTN setting involves a multi-timescale stochastic recursion with bilateral coupling between recursions. The GTD2 algorithm in the TTN setting generates stochastic sequences {w t } t\u2208N and {u t } t\u2208N that satisfy certain stability conditions. Policy evaluation experiments were conducted on an image-based catcher with 2 stacked 64x64 frames as input. In the classic Cartpole environment, the agent must balance a pole on a cart by applying force left or right. Rewards are given at each timestep, and the episode ends if the pole dips too low or the cart moves too far from the center. The policy evaluated involves applying force based on the pole's movement direction with a probability of 0.9. In Puck World (Tasfi, 2016), the agent moves towards a good puck while avoiding a bad puck in a 8-dimensional state. Actions increase velocity in cardinal directions with a penalty for being near the bad puck. The policy moves towards the good puck with a cap on velocity. Eligible actions are chosen to determine the agent's movement. In Puck World, the agent's movement is determined by choosing eligible actions that move towards the good puck while avoiding the bad puck. The agent picks randomly from eligible actions, with restrictions based on velocity and proximity to the apple. An experiment is conducted to test the advantage of TTN in the off-policy setting, with different behavior and target policies resulting in importance sampling ratios ranging from 0 to 8.7. TTN is compared to off-policy Nonlinear TD using three algorithms (TD, TDC, and LSTD) with sampling ratios from 0 to 8.7. TTN optimizes features with MSTDE on the behavior policy and learns values off-policy, while Nonlinear TD updates the entire network off-policy. TTN outperforms Nonlinear TD in average error and shows reduced variance, suggesting increased stability in the off-policy setting. LSTD version used in policy evaluation experiments initializes with specific parameters and policies are described for Puddle World and Catcher scenarios. In Catcher, the velocity towards the apple increased if the agent was within 25 units, otherwise a \"None\" action was chosen randomly. Nonlinear TD uses semi-gradient TD update with nonlinear function approximation, known to have divergence issues. Nonlinear GTD, on the other hand, has proven convergence results with nonlinear function approximation. ABTD and ABBE are adaptive bases algorithms optimizing different objectives, while ABPBE is omitted due to computational inefficiency. Nonlinear TD-Reg algorithm is adapted from Levine et al.'s control setting algorithm, using regularization on the last layer's weights for policy evaluation. Refined hyperparameter ranges were tested for experimental runs in Puddle World and Catcher environments. Learning rates were set for two-timescale networks, except for surrogate losses where a range of values was tested. The experiments involved testing various algorithms with different hyperparameters, including learning rates and trace parameters. Control experiments in the Catcher environment were conducted with modified settings. Hyperparameters were refined for the Nonlinear TD-Reg algorithm, which uses regularization on the last layer's weights for policy evaluation. The agent in the Catcher environment has 1 life per episode, with a reward of +1 for catching an apple and -1 at the end. The discount factor is 0.99. For image-based Catcher, two consecutive frames are stacked as the state for perceiving paddle movements. Both DQN and TTN algorithms use an -greedy policy with specific parameter settings and optimizer choices. Hyperparameters were manually tuned for image catcher due to long training times, while nonimage catcher focused on important hyperparameters with a specific tuning strategy. For TTN with FQI, hyperparameters focused on step sizes for features and regularization factor. For DQN, adjustments were made to step size and number of steps for annealing the probability of picking a random action. Final hyperparameters for nonimage catcher: TTN -\u03b1 slow = 10 \u22123, \u03bb reg = 10 \u22122, DQN -\u03b1 = 10 \u22123.75, target network updated every 1000 steps. For LS-DQN, FQI update every 50,000 steps with regularization weight of 1. Final hyperparameters for image catcher: TTN -\u03b1 slow = 10 \u22125, \u03bb reg = 10 \u22123, DQN -\u03b1 = 10 \u22123.75, target network updated every 10,000 steps. LS-DQN FQI update every 500,000 steps with regularization weight of 1."
}