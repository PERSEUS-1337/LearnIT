{
    "title": "rkhlb8lCZ",
    "content": "Wavelet Pooling is introduced as an alternative to traditional neighborhood pooling in Convolutional Neural Networks for image and object classification. This method decomposes features into a second level decomposition, reducing feature dimensions and addressing overfitting. Experimental results show that Wavelet Pooling outperforms or performs comparably with other pooling methods like max, mean, mixed, and stochastic pooling. The strength of Convolutional Neural Networks (CNNs) lies in their ability to classify images, objects, and videos with higher accuracy compared to vector-based deep learning techniques. Researchers constantly upgrade the key components of CNNs, such as the convolutional and pooling layers, to improve accuracy and efficiency. Pooling, which reduces spatial dimensions of data, aims to decrease parameters, increase computational efficiency, and prevent overfitting. Max pooling and average pooling are popular methods, but have limitations that hinder optimal network learning. The text discusses different pooling operations used in Convolutional Neural Networks to improve network learning. It introduces a wavelet pooling algorithm that aims to minimize artifacts and improve feature representation compared to traditional methods like max, mean, mixed, and stochastic pooling. The proposed method is tested on benchmark image classification datasets to validate its effectiveness. The text discusses pooling operations in Convolutional Neural Networks, focusing on max pooling and average pooling. Pooling involves condensing the output of the convolutional layer by summarizing regions into one neuron value. Max pooling selects the maximum value of a region, while average pooling calculates the average value. The paper is organized into sections on background, proposed methods, experimental results, and summary/conclusion. The text discusses pooling operations in Convolutional Neural Networks, focusing on max and average pooling methods. Max pooling can erase image details with low intensity, while average pooling dilutes significant details. To address these issues, researchers have developed probabilistic pooling methods like mixed pooling, which randomly selects between max and average pooling during training. Mixed pooling is a method applied in three different ways within a layer, involving random selection between max and average pooling. Stochastic pooling, another probabilistic method, improves upon max pooling by sampling from neighborhood regions based on activation probabilities. The process involves normalizing activations, sampling from a multinomial distribution, and selecting activations based on probabilities. The proposed wavelet pooling method reduces feature map dimensions using wavelets to minimize artifacts and improve image classification. It discards first-order subbands to capture data compression organically, reducing jagged edges. The method performs a 2nd order decomposition in the wavelet domain according to the fast wavelet transform for efficient implementation. The discrete wavelet transform (DWT) involves using approximation and detail coefficients to decompose images into subbands (LH, HL, HH, LL) at different resolution levels. After 2nd order decomposition, image features are reconstructed using wavelet subbands. The wavelet pooling algorithm performs backpropagation by reversing the forward propagation process, reducing feature map dimensions and improving image classification. The backpropagation algorithm of wavelet pooling involves 1st order wavelet decomposition, upsampling detail coefficient subbands, and reconstructing image features using Haar wavelet basis. Experiments are conducted using MatConvNet, stochastic gradient descent, and Zeilers network structure with variations in regularization techniques. The study examines how regularization techniques impact pooling results, using a 2x2 window for fair comparison. Results show the proposed method outperforms others, with max pooling showing signs of overfitting. Mixed and stochastic pooling have a stable trajectory, while average and wavelet pooling show smoother error reduction. The study compares different pooling methods with and without dropout layers on the CIFAR-10 dataset. Results show that the proposed method has the second highest accuracy, with wavelet pooling resisting overfitting. The change in learning rate prevents overfitting, and mixed and stochastic pooling show consistent learning progression. The network structure for the SHVN experiments includes training and test data from the SHVN dataset. Two cases are considered: one with no dropout using 55,000 images and one with dropout using 73,257 images. The proposed method shows the second lowest accuracy, with max and wavelet pooling slightly overfitting the data. Mixed, stochastic, and average pooling maintain a slow progression of learning. The KDEF experiments involve training and test data from the KDEF dataset, containing 4,900 images of 35 people displaying seven basic emotions. The KDEF dataset consists of 4,900 images of 35 people showing seven basic emotions through facial expressions. Errors in the dataset, such as missing or corrupted images, are fixed by mirroring counterparts in MATLAB and cropping to match specified dimensions. The data is split into 3,900 training images and 1,000 test images, resized to 128x128. Different pooling methods are evaluated for stability and overfitting, with the proposed method showing the second highest accuracy. Wavelet pooling and stochastic pooling show consistent learning progression compared to other methods. Computational complexity is a challenge for wavelet pooling, presented as a proof-of-concept with potential for improvement. Efficiency in terms of mathematical operations is a key area for enhancement, with the code implementation not optimized. The results serve as a starting point for future research to enhance accuracy and efficiency. Wavelet pooling and stochastic pooling demonstrate consistent learning progress compared to other methods. Computational complexity is a challenge for wavelet pooling, with potential for improvement in efficiency of mathematical operations. Average pooling is the most computationally efficient method, followed by mixed pooling and max pooling, while stochastic pooling is the least efficient. Wavelet pooling requires significantly more mathematical operations than average pooling due to subband coding implementation. By implementing subband coding with improved FTW algorithm and utilizing multidimensional wavelets, our proposed method shows potential to outperform traditional methods in CNNs. It excels in the MNIST dataset, performs well in CIFAR-10 and KDEF datasets, and responds effectively to network regularization with dropout and batch normalization. Our results confirm the efficiency and competitiveness of wavelet pooling in comparison to other pooling methods. Our results suggest that different pooling methods perform better depending on the dataset and network structure. Future work could explore varying wavelet bases and adjusting upsampling/downsampling factors for improved image feature reduction. Retaining discarded subbands for backpropagation may lead to higher accuracies. Improving the FTW method could enhance computational efficiency. Analyzing the structural similarity of wavelet pooling versus other methods could further validate our approach."
}