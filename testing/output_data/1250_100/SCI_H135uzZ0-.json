{
    "title": "H135uzZ0-",
    "content": "In this work, state-of-the-art visual understanding neural networks are trained on the ImageNet-1K dataset using Integer operations on General Purpose hardware. The focus is on Integer Fused-Multiply-and-Accumulate (FMA) operations with a shared exponent representation of tensors and a Dynamic Fixed Point (DFP) scheme. Efficient integer convolution kernel development is explored, including handling overflow of the INT32 accumulator. CNN training for various networks like ResNet-50, GoogLeNet-v1, VGG-16, and AlexNet achieves or exceeds SOTA accuracy. In this study, VGG-16 and AlexNet achieved state-of-the-art accuracy on the ImageNet-1K dataset using INT16 training on GPU hardware. This approach offers a 1.8X improvement in training throughput compared to FP32, with potential speedups of up to 2X using half-precision arithmetic. The interest in half-precision training has grown due to its efficiency in deep learning tasks. In this study, VGG-16 and AlexNet achieved state-of-the-art accuracy on the ImageNet-1K dataset using INT16 training on GPU hardware. The precision and range of INT16 and FP16 differ, with INT16 offering higher precision but lower dynamic range. There are algorithmic and semantic differences between the two data types, impacting tensor representation and multiply-and-accumulate operations. Proper selection of tensor representation, operation semantics, down-conversion scheme, scaling, normalization, and overflow management is crucial for successful half-precision training. Failure to consider all these factors can lead to inaccurate conclusions. In this work, a mixed-precision training setup is described, utilizing INT16 tensors with shared tensor-wide exponent and specialized low-precision instructions for operations like GEMM, convolution, and dot-product. The approach includes down-conversion schemes, overflow management, and performance optimizations without solely relying on half-precision representation or operations. Proper selection of tensor representation and operation semantics is crucial to avoid inaccurate conclusions. The proposed mixed-precision training strategy utilizes INT16 tensors for compute-intensive operations, achieving high accuracy on visual understanding CNNs like ResNet-50 without changing hyperparameters. Top-1 accuracies on ImageNet-1K dataset match or exceed single precision results, with a significant improvement in accuracy compared to half-precision training methods. Our methodology achieves state-of-the-art accuracy with int16 training on GoogLeNet-v1, VGG-16, and AlexNet networks, the first such results using int16 training. The paper discusses literature on half-precision training, dynamic fixed point format, kernels, and experimental results. Reduced precision for Deep learning is an active research topic with various data representations like floating-point and custom fixed point schemes. Mixed precision training uses 16-bit floating point storage for activations, weights, and gradients. The study demonstrates that using a combination of FP16 and FP32 precision in deep learning training requires loss scaling for near-state-of-the-art accuracy. Fixed point representations offer more flexibility and accuracy compared to floating-point schemes, with dynamically scaled fixed point representation showing significant improvement for convolutional neural networks. BID studies show significant improvements in deep learning training using low precision fixed point computation, with some proposing binary weights and activations for increased efficiency. Dynamic Fixed Point (DFP) representation is proposed as a more general approach for deep neural networks, aiming to leverage general purpose hardware using the integer compute pipeline. DFP tensors combine an integer tensor I and an exponent E s, shared across all integer elements, denoted as DFP-P = I, E s. This method is shown to achieve numerical parity with FP32 and outperform FP16 in accuracy for large networks on the ILSVRC classification task. DFP-16 data format offers a trade-off between float and half-float in terms of precision and dynamic range. It can achieve higher compute density and effective precision compared to half-floats due to its larger 15-bit mantissa. Blocked-DFP representation extends the dynamic range by using fine-grained quantization. BID12 has shown the effectiveness of fine-grained quantization for low-precision inference tasks. Arithmetic operations on DFP tensors are performed using standard commodity integer hardware, with exponent handling and precision management done in software. DFP-16 data format offers higher compute density and precision compared to half-floats. Arithmetic operations on DFP tensors are done in software, with primitives for end-to-end mixed-precision training. Converting floating point tensors to DFP involves deriving a shared exponent from the maximum value exponent. Common DFP primitives include multiplication and addition of DFP-16 tensors resulting in 32-bit tensors with new shared exponents. When adding two DFP-16 tensors, a 32-bit I tensor and a new shared exponent are generated. Fused Multiply and Add operations result in products with the same shared exponent. Down-Conversion scales DFP-32 output to DFP-16 for the next layer. Neural network training involves forward propagation, back-propagation, weight gradient computation, and solver operations. In CNNs, forward-propagation, back-propagation, and weight-gradient computation are compute-intensive steps involving GEMM-like convolution operations. In this work, a method using INT16 operations for convolutions and GEMM is proposed. Dynamic Fixed Point is used for neural network training, with core compute kernels like FP, BP, and WU convolution functions. These functions take DFP-16 tensors as input and produce FP32 tensors as output. Quantization steps convert FP32 tensors to DFP-16 tensors for operations in the next layer. Stochastic Gradient Descent is used for weight updates. The text discusses the use of Dynamic Fixed Point (DFP-16) tensors in neural network training, particularly in Stochastic Gradient Descent (SGD) for weight updates. Efficient implementations of core compute kernels using Integer FMA instructions, such as AVX512_4VNNI, are explored for operations like weight-gradients and weights updates. The FPROP convolution kernel is specifically written using AVX512_4VNNI instruction for 2-way horizontal accumulation operations. The text discusses the use of Dynamic Fixed Point (DFP-16) tensors in neural network training, particularly in Stochastic Gradient Descent (SGD) for weight updates. It explores efficient implementations of core compute kernels using Integer FMA instructions like AVX512_4VNNI for weight-gradients and weights updates. The dimensions of activations and weights are detailed, with a focus on preventing overflows in accumulate chains during neural network training. Converting INT32 intermediate outputs into FP32 before accumulation is suggested as a solution. The text discusses preventing overflows in accumulate chains during neural network training by using a scale of 2 and partial accumulations into INT32. The performance impact is minimal, with overflow management adding less than 3% overhead in most cases. The text discusses optimizing accumulate chains in neural network training by selecting a chain length of more than 200 and shifting inputs to prevent overflow. Several CNNs were trained for ImageNet-1K classification using mixed precision DFP16, with the first convolution layer and fully connected layers in FP32. The text discusses achieving state-of-the-art accuracy on ImageNet-1K classification task using mixed precision DFP16 training for CNNs. Results show DFP16 closely tracks full precision training, with validation/test loss closer to training loss. Reduced precision computation leads to better generalization and accuracies. Speed improvements are seen in convolution kernels going from FP32 to DFP16. The text discusses speed improvements in convolution kernels from FP32 to DFP16, resulting in a 1.5\u00d7 overall speedup. Memory prefetch optimization and batchnorm computation are also faster with DFP16, achieving a 2\u00d7 speedup. Fusion of ReLU and EltWise layers with batchnorm further enhances performance. Overall training throughput for ResNet-50 is 276 images/sec with a 1.8X speedup over FP32. SGD computation has been improved by 3\u00d7, reaching a training throughput of 317 images/sec. The text discusses the implementation of mixed precision DFP16 in Intel-Caffe, achieving a training throughput of 317 images/sec, with a 1.6X speedup compared to FP32. It demonstrates reduced precision INT-based training on large networks, maintaining accuracy and potentially saving 2\u00d7 in computation, communication, and storage. Additionally, a dynamic fixed point representation scheme is proposed for general purpose hardware, showing successful implementation in training various CNNs for image classification tasks. Future plans include exploring the method's effectiveness in other network types like RNNs. In the future, the method's efficacy will be demonstrated on other types of networks like RNNs, LSTMs, and GANs, extending its applications."
}