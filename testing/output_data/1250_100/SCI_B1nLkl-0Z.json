{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions (Q-values) are fundamental in reinforcement learning, with algorithms like SARSA and Q-learning. A new concept of action value is proposed, defined by a Gaussian smoothed version of the expected Q-value in SARSA. Smoothed Q-values still satisfy a Bellman equation, making them learnable from experience. Gradients of expected reward with respect to a parameterized Gaussian policy can be derived from the smoothed Q-value function. New algorithms for training a Gaussian policy directly from a learned Q-value approximator are developed, showing strong results on continuous control benchmarks. In reinforcement learning, Q-values are crucial for policy evaluation and improvement. Different methods like SARSA, Q-learning, Soft Q-learning, and PCL use various notions of Q-values to update policies and maximize returns. The choice of Q-value function significantly impacts the algorithm's behavior, affecting policy expression and exploration strategies. A new concept of action value, the smoothed action value function Q\u03c0, is introduced in this work, offering a unique approach compared to traditional Q-values. The smoothed Q-value function associates a value with a distribution over actions, allowing for Gaussian-smoothed or noisy versions of expected Q-values. It satisfies single-step Bellman consistency and can be used for training function approximators in reinforcement learning algorithms. Smoothed Q-values offer advantages for Gaussian policies, as the optimization objective can be expressed in terms of them, enabling updates to policy parameters based on derivatives of the smoothed Q-value function. Smoothie is an algorithm proposed to train a policy using derivatives of a smoothed Q-value function, avoiding high variance in standard policy gradient algorithms. It utilizes a non-deterministic Gaussian policy for better exploratory behavior and can incorporate proximal policy optimization techniques for improved stability and performance. The algorithm Smoothie improves stability and performance in training policies by using derivatives of a smoothed Q-value function. Results are competitive with state-of-the-art on standard continuous control benchmarks, especially for more challenging tasks with limited data. The model-free RL framework involves an agent interacting with a stochastic environment to maximize cumulative discounted reward through a Markov decision process. The agent's behavior is modeled using a stochastic policy \u03c0 that produces a distribution over feasible actions at each state. The optimization objective is the expected discounted return expressed in terms of the expected action value function Q \u03c0 (s, a). The policy gradient theorem expresses the gradient of a policy with tunable parameters as reinforcement learning algorithms trade off variance and bias when estimating Q-values using function approximation. The focus is on multivariate Gaussian policies over continuous action spaces, parametrized by mean and covariance functions. The formulation reviews prior work on learning Gaussian policies, focusing on the deterministic policy gradient for policies with small covariance. This approach estimates expected future return and expresses the gradient of the optimization objective for parameterized policies. In the context of learning Gaussian policies with small covariance, the text discusses optimizing value function approximators by minimizing Bellman error. It mentions using off-policy data for better sample efficiency and introduces smoothed action value functions for optimizing Gaussian policy parameters. Smoothed action values are defined for Gaussian policies, allowing direct bootstrapping of Q-values with Bellman consistency. This approach differs from prior work by learning a function approximator for Q \u03c0 (s, a) directly, enabling efficient optimization of Gaussian policy parameters. The text discusses how to optimize Q-values for Gaussian policies by parameterizing a policy and using derivatives to learn mean and covariance. The Bellman equation enables direct optimization of Q-values, with a focus on computing derivatives w.r.t. covariance parameters. The approach involves using second derivatives of Q-values to compute derivatives w.r.t. covariance. Multiple samples are used to optimize Q-values, with fixed target values for training. The training procedure for optimizing Q-values for Gaussian policies involves using a single function approximator and drawing phantom actions from a replay buffer. The procedure aims to minimize a weighted Bellman error and reach an optimum when Q \u03c0 w (s, a) satisfies the Bellman equation. Keeping track of sampling probabilities is unnecessary in practice. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods. These methods constrain gradient steps within a trust region or penalize KL-divergence from a previous policy. Stabilizing techniques have not been applicable to deterministic policies like DDPG, but a proposed formulation allows for trust region optimization by augmenting the objective with a penalty term. The paper proposes a method for optimizing policies using Q-value functions, similar to deterministic policy gradient methods. The approach generalizes deterministic policy gradient by updating the policy covariance. Unlike Stochastic Value Gradient (SVG), this method updates both the mean and covariance of the policy. The paper introduces a method for optimizing policies by updating the mean and covariance of the policy using Q-value functions. This approach extends deterministic policy gradient methods and avoids noisy Monte Carlo estimates by estimating the smoothed Q-value function. The method, called expected policy gradients (EPG), simplifies updates by directly estimating integrals and leveraging neural network function approximators. The paper proposes a new method called Smoothie for optimizing policies by estimating the smoothed Q-value function using neural network function approximators. It introduces a novel training scheme for learning the covariance of a Gaussian policy and discusses the perspective of Q-values representing the averaged return of a distribution of actions. The approach is distinct from recent advances in distributional RL, focusing on the single average return of a distribution of actions. The paper introduces Smoothie, a method for optimizing policies by estimating the smoothed Q-value function using neural network function approximators. It uses the gradient and Hessian of this approximation to train a Gaussian policy. Algorithm 1 provides a pseudocode of the algorithm, which is evaluated against DDPG on a synthetic task to study its behavior in a restricted setting. Smoothie optimizes policies by estimating the smoothed Q-value function using neural networks. It successfully learns both mean and variance, while DDPG struggles to escape local optima due to limited updates for the mean. Smoothie adjusts covariance during training and is guided towards better Gaussians by the derivative of the smoothed reward function. Smoothie adjusts the covariance \u03a3 \u03c6 during training, decreasing initially and then increasing as convexity is reached. It successfully escapes lower-reward local optima by adjusting policy variance based on the smoothed reward function. Implementation details for continuous control benchmarks in OpenAI Gym using feed forward neural networks are provided. Smoothie is competitive with DDPG, learning the optimal noise scale during training and outperforming in tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient. Results compared in FIG2 show hyperparameter search for actor and critic learning rates. DDPG explores noise scale, while Smoothie adjusts weight on KL-penalty. Smoothie performs competitively or better than DDPG across all tasks, with notable improvements in Hopper, Walker2d, and Humanoid. The introduction of a KL-penalty further enhances Smoothie's performance, especially on harder tasks. This highlights the benefits of using a learnable covariance and not restricting a policy to be deterministic. Smoothie algorithm introduces a KL-penalty on harder tasks in FIG3 to improve stability in training, addressing the inherent instability in DDPG. By using a proximal policy optimization method, significant performance improvements are seen in Hopper and Humanoid without sacrificing sample efficiency. The new Q-value function, Q \u03c0, allows for successful learning of both mean and covariance during training, leading to performance that can match or surpass DDPG, especially when penalizing divergence from a previous policy. Learning Q \u03c0 is considered more sensible than learning Q \u03c0 as it smooths the true reward surface, making it easier to learn. The smoothed Q-values have a more direct relationship with the expected discounted return objective. Future work should investigate these claims and techniques to apply the motivations for Q \u03c0 to other policies. The specific identity mentioned can be derived using standard matrix calculus. The equations are presented succinctly, omitting unnecessary details."
}