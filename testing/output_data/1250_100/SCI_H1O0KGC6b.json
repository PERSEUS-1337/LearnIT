{
    "title": "H1O0KGC6b",
    "content": "One of the main challenges in deep learning is choosing the right training strategy. A new approach called post-training optimizes only the last layer of the network, improving performance by utilizing data representation efficiently. This method, tested on various architectures and datasets, consistently boosts performance by ensuring the best use of data embedding for the task at hand. The choice of an appropriate training strategy is crucial for deep learning models. Stochastic gradient descent (SGD) is commonly used to train deep networks by updating weights based on gradients. Variance reduction techniques like Adagrad, RMSprop, and Adam are proposed for faster convergence. Pre-training, where each layer is constructed using unsupervised learning, is often used to find a good starting point for network weights. Pretraining strategies using unsupervised learning are applied successfully to various applications like classification, regression, robotics, and information retrieval. Different layers in deep neural networks play different roles, with the first layers learning general feature extractors and the last layers being more task-specific. Deep learning generally outperforms shallow structures, but the latter are easier to train and more stable. Convex models like logistic regression have a convex training problem when the data representation is fixed, highlighting the importance of separating representation and model learning for stability. The importance of model learning for stability is emphasized, especially in end-to-end models. A new training step called post-training is introduced to improve the use of learned representation for the task. This step focuses on training specific layers while keeping general layers fixed, aiming to optimize the representation for the desired task. The post-training step introduced focuses on training specific layers while keeping general layers fixed to optimize the representation for the desired task. It can be easily added to most learning strategies, is computationally inexpensive, and shows a link to kernel techniques. The last layer's weights correspond to the kernel associated with the feature map from previous layers, making the representation efficient for the task at hand. The post-training step focuses on optimizing specific layers while keeping general layers fixed to improve representation for the task. It is computationally inexpensive and shows a connection to kernel techniques. The neural network architecture and data sets are discussed in subsequent sections. The training objective of neural networks is to find weights that solve a specific problem, with the last layer playing a special role. When certain conditions are met, training the last layer is equivalent to regression of labels using data embeddings. Classical optimization techniques can efficiently produce optimal weights for the last layer, which is the idea behind post-training. The post-training step involves fixing the first L \u2212 1 layers and only updating the last layer to further optimize the learned data representation. This process follows the regular training step where interesting features are obtained through deep learning training using various strategies like stochastic gradient descent and regularization techniques. Different combinations of training strategies and stopping criteria are tested to improve the overall performance. During post-training, the last layer \u03c6 L is trained while the first L \u2212 1 layers are fixed. This step utilizes the mapping \u03a6 L\u22121 as an embedding of the data in X L to learn the best linear predictor in a lower dimensional space. A 2-regularization is added to reduce overfitting and improve the efficiency and generalization of the model. The regularization is independent of the one used in regular training and is motivated by experimental results and comparisons with the kernel framework. During post-training, it is advised not to apply Dropout on previous layers as it can alter the feature function \u03a6 L\u22121. The kernel k associated with \u03a6 L\u22121 is continuous positive definite and belongs to the Reproducing Kernel Hilbert Space (RKHS) H k. The post-training problem is related to the RKHS space H k, where the generalized representer theorem can be applied. This leads to the existence of an optimal solution g * = g W W W * for the problem. Relation between the regularization norms in the optimization problem can be seen as an approximation of the optimal value. The choice of regularization norm affects the relationship between H and 2 norms. The 2-norm is preferred for ease of computation in experiments. In specific cases, the problem reduces to a classical Kernel Ridge Regression. In specific cases, the problem reduces to a classical Kernel Ridge Regression where optimal weights can be computed using certain kernel methods. Multidimensional output spaces can also be considered, with post-training showing influence on performances across different data sets and network architectures. Experiments were conducted using python and Tensorflow, with results discussed in depth. The post-training method can be easily applied to feedforward convolutional neural networks for solving real-world problems. It was tested on three benchmark datasets: CIFAR10, MNIST, and FACES. The CIFAR10 dataset consists of 60,000 images representing objects from 10 classes. The experiments used the default architecture proposed by Tensorflow, with 5 layers including common tools like local response normalization, max pooling, and RELU activation. The performance of the neural network on the CIFAR10 dataset improved with post-training, as shown in Figure 3. The network is trained for 90k iterations with batches of size 128 using stochastic gradient descent, dropout, and exponential weight decay for the learning rate. Figure 3 compares the performance of the network with classic training using SGD and post-training. The results show a mild increase in training cost with post-training, but overall performance is improved. Post-training improves generalization of the solution, with a slight increase in training cost. Post-training iterations are 4\u00d7 faster than classic iterations. Evaluation on MNIST and FACES datasets with two different convolutional neural networks shows the influence of network complexity on post-training. Post-training improves test performance of networks with minimal iterations, providing a consistent improvement regardless of network complexity or dataset. The concept can also be applied to LSTM networks, as demonstrated in an experiment using the PTB dataset. The recurrent neural network is trained with a 10000 words vocabulary using a specific architecture. The network is trained to minimize perplexity for 100 epochs with gradient descent and dropout for regularization. Post-training is shown to improve test performance even after convergence. The experiment evaluates a close-form solution for regression tasks with specific activation and loss functions. The recurrent neural network is trained with a 10000 words vocabulary using a specific architecture to minimize perplexity for 100 epochs with gradient descent and dropout for regularization. Post-training improves test performance even after convergence. A close-form solution for regression tasks with specific activation and loss functions is evaluated using a neural network on two regression problems with real and synthetic data sets. The input consists of 5,875 instances of 17-dimensional data, and the output is a one-dimensional real number. A neural network with two fully connected hidden layers of size 17 and 10 is trained for 250, 500, and 750 iterations with batches of size 50. The layer weights are regularized with a fixed parameter \u03bb = 10^-3. Post-training is done for 200 iterations, and the results are compared to closed-form solutions. A synthetic data set is used for regression, with inputs generated from a uniform distribution and outputs computed using randomly generated weights. The neural network is trained with two hidden layers for 250, 500, and 750 iterations. The neural network with tanh for the first layer and RELU for the second layer is trained for 250, 500, and 750 iterations with batches of size 50. Post-training is done for 200 extra iterations, improving performances towards optimal solutions. Overfitting tendencies are observed when the first layers are fully optimized. Post-training enhances the performances of all networks, including recurrent, convolutional, and fully connected networks. The gap between losses with and without post-training is more pronounced when training is stopped early, reducing as the network converges to a better solution. The reduction of the gap between test performances with and without post-training is evident as the network converges to a better solution. Post-training significantly improves performance, especially when the network has not fully converged. However, the advantage of post-training diminishes if the network is allowed to overfit by training for a very large number of iterations. Post-training is beneficial when the network has not fully converged, improving performance significantly. However, its advantage diminishes if the network overfits. The computational cost of post-training is low compared to full training, with faster iterations and performance gains seen after as few as 100 batches. The efficiency is due to reaching a local minimum rapidly, cheaper iterations, and convex optimization leading to rapid convergence to optimal weights. The post-training step in optimization converges rapidly to optimal weights for the last layer without evidence of overfitting. The regularization parameter \u03bb plays a crucial role, with significant results observed for \u03bb values reasonably small (i.e 10^-5 \u2264 \u03bb \u2264 10^-2). This parameter is linked to the regularization parameter of kernel methods. The post-training step can be applied to most trained networks without degrading performance, providing a consistent gain at low computational cost. Numerical experiments show a link between post-training and kernel methods, with optimal weights boosting network performance. However, computing optimal weights for the last layer is only feasible for small datasets. Post-training can be connected to the idea of pre-training, aiming to find a representation capturing enough information. In this work, the concept of post-training is explored as an additional step after regular training, focusing on training only the last layer to take advantage of learned data representation. Empirical evidence shows that post-training is computationally efficient and significantly improves performance in neural networks. The relationship between the number of frozen layers in post-training and performance improvements is an interesting area for further study. The post-training problem is proven to be convex for softmax activation in the last layer and cross entropy loss. The function F is shown to be convex, with the hessian being positive semidefinite."
}