{
    "title": "rJfW5oA5KQ",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in learning complex real-world distributions, but recent works have highlighted issues such as lack of diversity and mode collapse. Theoretical work by Arora et al. (2017a) suggests a dilemma regarding GANs' statistical properties: powerful discriminators can lead to overfitting, while weak discriminators may not detect mode collapse. In this paper, it is shown that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by designing discriminators with strong distinguishing power against specific generator classes. The Integral Probability Metric induced by these discriminators can approximate the Wasserstein distance and/or KL-divergence, ensuring that the learned distribution is close to the true distribution and cannot drop modes. Preliminary experiments suggest that the lack of diversity in GANs may be due to optimization sub-optimality rather than statistical inefficiency. Recent advancements in Generative Adversarial Networks (GANs) have shown great success in generating high-quality samples across various domains. However, concerns have been raised about mode collapse and lack of diversity in the learned distributions. The paper suggests that designing discriminators with strong distinguishing power can help alleviate mode collapse, particularly against specific generator classes. In this paper, the focus is on Wasserstein GAN (WGAN) formulation BID0 and the F-Integral Probability Metric (F-IPM) between distributions. The goal is to learn the data distribution by setting up a family of generators and discriminators. The use of parametric families of functions, like neural networks, helps in approximating Lipschitz functions. One of the main concerns with GANs is the issue of \"mode collapse\" in the learned distribution. The issue of \"mode collapse\" in Wasserstein GANs arises from the weakness of the discriminator, leading to low-diversity examples. Increasing the discriminator's strength to larger families like all 1-Lipschitz functions is a proposed solution, but the Wasserstein-1 distance lacks good generalization properties. This is evident even for typical distributions like spherical Gaussians, highlighting the challenge of optimizing the empirical Wasserstein distance. The paper addresses the challenge of mode collapse in Wasserstein GANs by proposing a discriminator class F that is strong against a specific generator class G. This class has restricted approximability with respect to G and the data distribution p, ensuring diversity and avoiding overfitting issues. The paper focuses on discriminator class F with restricted approximability to address mode collapse in Wasserstein GANs. This class ensures diversity and avoids overfitting by approximating the Wasserstein distance for data distribution p and any q \u2208 G. The paper introduces a theoretical framework for analyzing the statistical properties of Wasserstein GANs with polynomial samples. It focuses on designing a discriminator class F with restricted approximability to prevent mode collapse and ensure diversity in generator classes. The framework provides guarantees on the distance between data distributions p and q, addressing both diversity and generalization properties. The paper discusses designing a discriminator class F to prevent mode collapse and ensure diversity in generator classes. It focuses on analyzing the statistical properties of Wasserstein GANs with polynomial samples, showing diversity guarantees for distributions like Gaussians and exponential families. Additionally, it explores distributions generated by invertible neural networks and the use of special neural network discriminators to handle exponential modes. The paper explores the statistical properties of Wasserstein GANs with polynomial samples, focusing on preventing mode collapse and ensuring diversity in generator classes. It discusses distributions generated by invertible neural networks and the use of special neural network discriminators to handle exponential modes. The crux of the technical part is establishing the approximation of Wasserstein distance by IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE approach in learning distributions with low-dimensional supports. The paper discusses the advantages of GANs over MLE in learning distributions with low-dimensional supports. It introduces the concept of using IPMs to approximate the Wasserstein distance for invertible generator families. The IPM could serve as an alternative measure of diversity and quality in more complex settings where KL-divergence or Wasserstein distance is not measurable. The lack of diversity in GANs may be due to sub-optimal optimization rather than statistical inefficiency. Various tests have been developed to assess diversity, memorization, and generalization in GANs. Mode collapse can occur from a weak discriminator, leading to proposed solutions by different architectures and algorithms. Feizi et al. (2017) demonstrated provable guarantees in training GANs. The text discusses the challenges and advancements in training Generative Adversarial Networks (GANs), including the use of quadratic discriminators and statistical guarantees in Wasserstein distance for distributions like injective neural network generators. Liang (2017) explores GANs in a non-parametric setup, emphasizing the importance of the smoothness of the generator family in learning GANs efficiently. The text discusses the challenges in training GANs, emphasizing the importance of smoothness in the generator family. It mentions the use of invertible generator structures and the implications for successful GAN training in terms of IPM and KL-divergence. The theory suggests that real data cannot be generated by an invertible neural network, but if data can be generated by an injective neural network, Wasserstein distance can bound the closeness between learned and true distributions. The text discusses the challenges in training GANs, emphasizing the importance of smoothness in the generator family. It mentions the use of invertible generator structures and the implications for successful GAN training in terms of IPM and KL-divergence. The theory suggests that real data cannot be generated by an invertible neural network, but if data can be generated by an injective neural network, Wasserstein distance can bound the closeness between learned and true distributions. F-IPM is referred to as the neural net IPM, and distances of interest include KL divergence and Wasserstein-2 distance. The Rademacher complexity of a function class is defined, and the training IPM loss for the Wasserstein GAN is discussed. Generalization of the IPM is governed by the Rademacher complexity. The text discusses designing discriminators for parameterized distributions like Gaussian distributions using one-layer neural networks with ReLU activation. It proves that these networks can distinguish Gaussian distributions with restricted approximability guarantees. The set of Gaussian distributions with bounded mean and well-conditioned covariance is considered, showing that the discriminators have restricted approximability. The text discusses designing discriminators for parameterized distributions like Gaussian distributions using one-layer neural networks with ReLU activation. It proves that these networks can distinguish Gaussian distributions with restricted approximability guarantees. The discriminator family can also be extended to mixture of Gaussians and exponential families, showing that linear combinations of sufficient statistics can serve as discriminators with restricted approximability. The text discusses designing discriminators for parameterized distributions like Gaussian distributions using neural networks. It proves that these networks can distinguish distributions with restricted approximability guarantees. The discriminator family can also be extended to mixture of Gaussians and exponential families. In Section 4.1, invertible neural networks generators with proper densities are considered. Section 4.2 extends the results to injective neural networks generators, allowing latent variables of lower dimension than observable dimensions. The generators are parameterized by invertible neural networks, allowing for non-spherical variances to model data around a \"k-dimensional manifold\" with noise. Neural networks generators are discussed in Section 4.1, focusing on invertible networks with proper densities. The networks are parameterized by invertible neural networks, allowing for non-spherical variances to model data around a \"k-dimensional manifold\" with noise. The networks are assumed to be invertible, with activation functions satisfying specific conditions to prevent pseudo-random functions. The function log p \u03b8 can be computed by a neural network with limited layers and parameters. The text discusses neural networks generators parameterized by invertible neural networks to compute log p \u03b8 with limited layers and parameters. The proof involves a change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian can be computed by adding a bias on the final output layer, freeing from structural assumptions on weight matrices. The text discusses the restricted approximability of discriminator class F w.r.t. invertible-generator distributions G. The proof involves relating KL divergence to IPM and bounding the Wasserstein distance. The text discusses bounding the Wasserstein distance for invertible-generator distributions G by establishing transportation inequalities. It also presents workarounds for Lipschitz functions in F and their impact on the upper bound. The combination of restricted approximability and generalization bound leads to small expected IPM for successful training. In the context of bounding Wasserstein distance for invertible-generator distributions, the text discusses the importance of small expected IPM for successful training. It introduces a novel divergence between distributions that can be optimized as IPM, focusing on injective neural network generators for realistic image modeling. The text introduces a variant of the IPM to approximate the Wasserstein distance for distributions generated by neural nets. It defines a smoothed F-IPM between distributions p and q, showing that for certain discriminator classes, it approximates the Wasserstein distance. The theorem states that for any pair of distributions p, q, there exists a discriminator class F such that the F-IPM approximates the Wasserstein distance. The theorem implies that if d(p n ,q n ) is small for n poly(d), then mode collapse is avoided. Theoretical results show that mode collapse is prevented as long as the discriminator family F has restricted approximability with respect to the generator family G. Specific discriminator classes are designed to guarantee this, with synthetic experiments confirming the theory's consistency in practice. The IPM is well correlated with the Wasserstein / KL divergence, indicating that restricted approximability may hold in GAN training. The difficulty of GAN training may stem from optimization challenges rather than statistical inefficiency. Experiments show good statistical behaviors on \"typical\" discriminator classes. Synthetic experiments with WGANs demonstrate correlations between IPM and Wasserstein distance, as well as IPM and KL divergence. Training GANs on various curves in two dimensions, such as the unit circle and \"swiss roll\" curve, further illustrate these relationships. The Wasserstein distance is used to measure the quality of learned generators on a one-dimensional manifold in R 2. WGANs show good performance in learning distributions, with the IPM W F strongly correlated with the Wasserstein distance. Standard neural net architectures are used for generators and discriminators, with specific optimizer settings and learning rates. Two metrics are compared between ground truth and learned distributions during training. The neural net IPM W F (p, q) and Wasserstein distance W 1 (p, q) are computed on fresh batches from p, q. The empirical Wasserstein distance W 1 (p, q) is a good proxy for the true Wasserstein distance. Results show that the learned generator closely matches the ground truth distribution at iteration 10000. The IPM and Wasserstein distance are well correlated, with larger values at iteration 500 indicating the generators have not fully learned the true distributions yet. Sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence are presented. The analysis technique involves designing discriminators with restricted approximability tailored to the generator class, aiming to avoid mode collapse and improve generalization. The hope is to extend these techniques to other distribution families with tighter sample complexity bounds by exploring approximation theory results in the context of GANs. Theorems and proofs are provided to support the approach of using discriminators with restricted approximability. The curr_chunk discusses the lower bound for the mean distance and covariance distance between two Gaussians, using a linear discriminator and perturbation bounds. The analysis technique involves designing discriminators with restricted approximability tailored to the generator class to avoid mode collapse and improve generalization. Theorems and proofs are provided to support this approach. The curr_chunk discusses using perturbation bounds to establish a lower bound for the mean difference between two Gaussians. The W 2 distance is used to bridge the KL and F-distance, with the analysis involving bounding the growth of gradients. The Rademacher contraction inequality is applied to bound the right-hand side. The curr_chunk discusses bounding the right-hand side using exponential family properties and Wasserstein bounds. It also explores the Rademacher complexity for a mixture of Gaussians using neural networks. The curr_chunk discusses reparametrizing a neural network and bounding the Rademacher complexity. The text discusses bounding the Rademacher complexity of a neural network reparametrization by showing Lipschitz properties and using a one-step discretization bound. It also covers the expected supremum over a covering set and utilizes sub-Gaussian maxima bounds. The text presents upper bounds on f-contrast by Wasserstein distance for two distributions on R^d with positive densities. It includes bounds for truncated W1 and W2 distances, along with a proof involving a truncation argument and coupling. The text discusses upper bounds on f-contrast using Wasserstein distance for two distributions on R^d with positive densities. It introduces bounds for truncated W1 and W2 distances, along with a proof involving a truncation argument and coupling. Additionally, it explores representing log p \u03b8 (x) by a neural network and computing the log determinant of the Jacobian through network branches. The text discusses representing log p \u03b8 (x) with a neural network and computing the log determinant of the Jacobian through network branches. It introduces bounds for truncated W1 and W2 distances, along with upper bounds on f-contrast using Wasserstein distance for two distributions on R^d with positive densities. The theorem follows by combining lemmas showing restricted approximability bounds in terms of the W2 distance. The text discusses the L2-sub-Gaussian property of a random variable, satisfying the Gozlan condition. It then explores upper bounding the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 using neural network representations. The W2 bound is shown, along with an upper bound on f-contrast using Wasserstein distances for two distributions. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. It introduces the W1 bound and provides a tail bound for X2. Reparametrization is done to represent weights and biases of the inverse network z = G\u22121 \u03b8 (x). The Rademacher complexity of F is also discussed. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. It introduces the W1 bound and provides a tail bound for X2. Reparametrization is done to represent weights and biases of the inverse network z = G\u22121 \u03b8 (x). The Rademacher complexity of F is also discussed. The sum of the normalizing constant for Gaussian density and the sum of log det(W i ) are considered, leading to the one-step discretization bound. Lemmas on discretization error and expected max over a finite set are presented, with a focus on generalization error. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. It introduces the W1 bound and provides a tail bound for X2. Reparametrization is done to represent weights and biases of the inverse network z = G\u22121 \u03b8 (x). The Rademacher complexity of F is also discussed. The sum of the normalizing constant for Gaussian density and the sum of log det(W i ) are considered, leading to the one-step discretization bound. Lemmas on discretization error and expected max over a finite set are presented, with a focus on generalization error. The term n \u2265 \u03b4 dominates term I, and it suffices to show that for any inverse network G \u22121, the Lipschitzness of hidden layers is maintained. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. It introduces the W1 bound and provides a tail bound for X2. Reparametrization is done to represent weights and biases of the inverse network z = G\u22121 \u03b8 (x). The Rademacher complexity of F is also discussed. The sum of the normalizing constant for Gaussian density and the sum of log det(W i ) are considered, leading to the one-step discretization bound. Lemmas on discretization error and expected max over a finite set are presented, with a focus on generalization error. The term n \u2265 \u03b4 dominates term I, and it suffices to show that for any inverse network G \u22121, the Lipschitzness of hidden layers is maintained. Preceding two bounds and that |K \u2212 K | \u2264 \u03b5, we show that the random variable DISPLAYFORM32 is suitably sub-exponential. The term h , A \u03b3 h is a quadratic function of a sub-Gaussian random vector, hence is subexponential. The parameter K is upper bounded by DISPLAYFORM37. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. It introduces the W1 bound and provides a tail bound for X2. Reparametrization is done to represent weights and biases of the inverse network z = G\u22121 \u03b8 (x). The Rademacher complexity of F is also discussed. The sum of the normalizing constant for Gaussian density and the sum of log det(W i ) are considered, leading to the one-step discretization bound. Lemmas on discretization error and expected max over a finite set are presented, with a focus on generalization error. The term n \u2265 \u03b4 dominates term I, and it suffices to show that for any inverse network G \u22121, the Lipschitzness of hidden layers is maintained. Preceding two bounds and that |K \u2212 K | \u2264 \u03b5, we show that the random variable DISPLAYFORM32 is suitably sub-exponential. The term h , A \u03b3 h is a quadratic function of a sub-Gaussian random vector, hence is subexponential. The parameter K is upper bounded by DISPLAYFORM37. By focusing on mean-zero sub-exponentiality and using a standard covering argument, the expected maximum is bounded, leading to a quantitative theorem statement. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. It introduces the W1 bound and provides a tail bound for X2. Reparametrization is done to represent weights and biases of the inverse network z = G\u22121 \u03b8 (x). The Rademacher complexity of F is also discussed. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.1. Suppose the generator class G satisfies the assumption E.1, and truncates the distribution to a very high-probability region. Several regularity conditions for the family of generators G are introduced. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. It introduces the W1 bound and provides a tail bound for X2. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.2 introduces a family of neural networks F that can approximate the log density of p \u03b2 for every p \u2208 G. The approach involves approximating p \u03b2 (x) using Laplace's method of integration. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.2 introduces a family of neural networks F that can approximate the log density of p \u03b2 for every p \u2208 G. The integral is dominated by its maximum value, calculated using a greedy \"inversion\" procedure. The proof of Theorem E.1 involves neural networks N1, N2 \u2208 F approximating log p \u03b2 and log q \u03b2. The lower bound is proven using Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W1/6. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.2 introduces a family of neural networks F that can approximate the log density of p \u03b2 for every p \u2208 G. The integral is dominated by its maximum value, calculated using a greedy \"inversion\" procedure. The proof of Theorem E.1 involves neural networks N1, N2 \u2208 F approximating log p \u03b2 and log q \u03b2. The lower bound is proven using Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W1/6. Proceeding to the claim, consider the optimal coupling C of p, q, and consider the induced coupling C z on the latent variable z in p, q. The rest of the section is dedicated to the proof of Theorem E.2, which will be finally in Section E.3. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.2 introduces a family of neural networks F that can approximate the log density of p \u03b2 for every p \u2208 G. The integral is dominated by its maximum value, calculated using a greedy \"inversion\" procedure. The proof of Theorem E.1 involves neural networks N1, N2 \u2208 F approximating log p \u03b2 and log q \u03b2. The lower bound is proven using Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W1/6. Proceeding to the claim, consider the optimal coupling C of p, q, and consider the induced coupling C z on the latent variable z in p, q. The rest of the section is dedicated to the proof of Theorem E.2, which will be finally in Section E.3. Suppose the claim holds for i. Then, DISPLAYFORM6 where the last inequality holds by the inductive hypothesis, and the next-to-last one due to Lipschitzness of \u03c3 \u22121. DISPLAYFORM7, we have DISPLAYFORM8 This implies that DISPLAYFORM9 which in turns means DISPLAYFORM10 which completes the claim. Turning to the size/Lipschitz constant of the neural network: all we need to notice is that \u0125 i = \u03c3 \u22121 (W The integral on the right is nothing more than the (unnormalized) cdf of a Gaussian with covariance DISPLAYFORM11 is positive definite with smallest eigenvalue bounded by DISPLAYFORM12 where G i is the i-th coordinate of G. We claim DISPLAYFORM13 The latter follows from the bound on r and Cauchy-Schwartz. For the former, note that we have DISPLAYFORM14 up to a multiplicative factor of 1 \u00b1 O poly(d) (\u03b2 log(1/\u03b2)), since the normalizing factor satisfies DISPLAYFORM15 We will first present the algorithm, then prove that it:(1) Approximates the integral as needed.(2) Can be implemented by a small, Lipschitz network as needed. The algorithm is as follows:Algorithm 1 Discriminator family with restricted approximability for degenerate manifold 1: Parameters: DISPLAYFORM16 and let S be the trivial \u03b2 2 -net of the matrices with spectral norm The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.2 introduces a family of neural networks F that can approximate the log density of p \u03b2 for every p \u2208 G. The integral is dominated by its maximum value, calculated using a greedy \"inversion\" procedure. The proof of Theorem E.1 involves neural networks N1, N2 \u2208 F approximating log p \u03b2 and log q \u03b2. The lower bound is proven using Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W1/6. Proceeding to the claim, consider the optimal coupling C of p, q, and consider the induced coupling C z on the latent variable z in p, q. The rest of the section is dedicated to the proof of Theorem E.2, which will be finally in Section E.3. Suppose the claim holds for i. Then, DISPLAYFORM6 where the last inequality holds by the inductive hypothesis, and the next-to-last one due to Lipschitzness of \u03c3 \u22121. DISPLAYFORM7, we have DISPLAYFORM8 This implies that DISPLAYFORM9 which in turns means DISPLAYFORM10 which completes the claim. Turning to the size/Lipschitz constant of the neural network: all we need to notice is that \u0125 i = \u03c3 \u22121 (W The integral on the right is nothing more than the (unnormalized) cdf of a Gaussian with covariance DISPLAYFORM11 is positive definite with smallest eigenvalue bounded by DISPLAYFORM12 where G i is the i-th coordinate of G. We claim DISPLAYFORM13 The latter follows from the bound on r and Cauchy-Schwartz. For the former, note that we have DISPLAYFORM14 up to a multiplicative factor of 1 \u00b1 O poly(d) (\u03b2 log(1/\u03b2)), since the normalizing factor satisfies DISPLAYFORM15 We will first present the algorithm, then prove that it:(1) Approximates the integral as needed.(2) Can be implemented by a small, Lipschitz network as needed. The algorithm is as follows:Algorithm 1 Discriminator family with restricted approximability for degenerate manifold 1: Parameters: DISPLAYFORM16 and let S be the trivial \u03b2 2 -net of the matrices with spectral norm. Let\u1e91 = N inv (x) be the output of the \"invertor\" circuit of Lemma E.4. Calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91) by the circuit implied in Lemma E.7. Let M be the nearest matrix in S to H and E i , i \u2208 [r] be s.t. M + E i has \u2126(\u03b2)-separated eigenvalues. Let (e i , \u03bb i ) be approximate eigenvector/eigenvalue pairs of H + E i calculated by the circuit implied in Lemma E.6. We claim that there exist matrices E 1 , E 2 , . . . , E r , r = \u2126(d log(1/\u03b2)), s.t. if M \u2208 S, at least one. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The main theorem states that for certain F,d F approximates the Wasserstein distance. Theorem E.2 introduces a family of neural networks F that can approximate the log density of p \u03b2 for every p \u2208 G. The integral is dominated by its maximum value, calculated using a greedy \"inversion\" procedure. The proof of Theorem E.1 involves neural networks N1, N2 \u2208 F approximating log p \u03b2 and log q \u03b2. The lower bound is proven using Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W1/6. Proceeding to the claim, consider the optimal coupling C of p, q, and consider the induced coupling C z on the latent variable z in p, q. The rest of the section is dedicated to the proof of Theorem E.2, which will be finally in Section E.3. Suppose the claim holds for i. Then, DISPLAYFORM6 where the last inequality holds by the inductive hypothesis, and the next-to-last one due to Lipschitzness of \u03c3 \u22121. DISPLAYFORM7, we have DISPLAYFORM8 This implies that DISPLAYFORM9 which in turns means DISPLAYFORM10 which completes the claim. Turning to the size/Lipschitz constant of the neural network: all we need to notice is that \u0125 i = \u03c3 \u22121 (W The integral on the right is nothing more than the (unnormalized) cdf of a Gaussian with covariance DISPLAYFORM11 is positive definite with smallest eigenvalue bounded by DISPLAYFORM12 where G i is the i-th coordinate of G. We claim DISPLAYFORM13 The latter follows from the bound on r and Cauchy-Schwartz. For the former, note that we have DISPLAYFORM14 up to a multiplicative factor of 1 \u00b1 O poly(d) (\u03b2 log(1/\u03b2)), since the normalizing factor satisfies DISPLAYFORM15 We will first present the algorithm, then prove that it:(1) Approximates the integral as needed.(2) Can be implemented by a small, Lipschitz network as needed. The algorithm is as follows:Algorithm 1 Discriminator family with restricted approximability for degenerate manifold 1: Parameters: DISPLAYFORM16 and let S be the trivial \u03b2 2 -net of the matrices with spectral norm. Let\u1e91 = N inv (x) be the output of the \"invertor\" circuit of Lemma E.4. Calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91) by the circuit implied in Lemma E.7. Let M be the nearest matrix in S to H and E i , i \u2208 [r] be s.t. M + E i has \u2126(\u03b2)-separated eigenvalues. Let (e i , \u03bb i ) be approximate eigenvector/eigenvalue pairs of H + E i calculated by the circuit implied in Lemma E.6. We claim that there exist matrices E 1 , E 2 , . . . , E r , r = \u2126(d log(1/\u03b2)), s.t. if M \u2208 S, at least one. The text discusses reparametrization of neural networks to bound the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398. The data is generated from a ground-truth invertible neural net generator. The weight matrices are well-conditioned with singular values between 0.5 to 2. The discriminator architecture is chosen with restricted approximability guarantee. Training involves generating stochastic batches and solving the min-max problem in the Wasserstein GAN formulation. We perform 10 updates of the discriminator in between each generator step, using various regularization methods. The RMSProp optimizer is used as our update rule. Evaluation metrics include KL divergence, training loss (IPM W F train), and neural net IPM (W F eval). The text discusses training a discriminator in norm balls without regularization to find an approximate maximizer for contrast. The theory shows that WGAN can learn the true generator in KL divergence, and F-IPM is indicative of this. Experiments with a two-layer neural net generator in 10 dimensions using Vanilla WGAN or WGAN-GP show promising results. The main findings of the study show that WGAN training with a restricted approximability discriminator design can learn the true distribution in KL divergence. The KL divergence starts around 10-30 and the best run achieves a KL lower than 1, indicating that GANs are finding the true distribution without mode collapse. The W F (eval) and KL divergence are highly correlated, with adding gradient penalty improving optimization. Results with vanilla fully-connected discriminator nets also show good correlation with KL-divergence. The KL-divergence between the true distribution and learned distribution in WGAN training correlates well with the estimated IPM. The inferior performance of the WGAN-Vanilla algorithm in KL divergence is attributed to training performance rather than statistical properties. The study conjectures similar phenomena occur in training GANs with real-life data. The correlation between perturbed generators and their KL divergence and neural net IPM is directly tested. In testing the correlation between perturbed generators, the KL divergence, and neural net IPM, pairs of generators were generated with small Gaussian noise added. The neural net IPM was computed by optimizing the discriminator from 5 random initializations. A positive correlation was observed between the KL divergence and neural net IPM, with most points falling around the line W F = 100D kl. Outliers with large KL divergence were attributed to perturbations causing poorly conditioned weight matrices. Experiments were redone with vanilla fully-connected discriminator nets. Results from experiments with different discriminator structures show that vanilla discriminator architectures can lead to good generator convergence in KL divergence, although correlation is slightly weaker compared to architectures with restricted approximability. The correlation between KL divergence and neural net IPM is consistent, with a correlation coefficient of 0.7489 for vanilla fully-connected discriminators."
}