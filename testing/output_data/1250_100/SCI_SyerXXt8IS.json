{
    "title": "SyerXXt8IS",
    "content": "We aim to enhance input features for machine learning models dealing with limited training data. Biological neural nets (BNNs) like the insect olfactory network excel at fast learning by utilizing competitive inhibition, sparse connectivity, and Hebbian updates. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, outperforming traditional methods like PCA and NNs. These \"insect cyborgs\" show significantly improved performance on MNIST and Omniglot datasets, reducing test set errors by 20% to 55%. This highlights the potential of BNN-inspired feature generators in machine learning. The limited-data constraint in machine learning (ML) poses challenges for deployment and problem-solving. To address this, a new architecture is proposed to automatically generate class-separating features from existing ones, inspired by the rapid learning ability of biological neural nets (BNNs) like the insect olfactory network. This network, with elements such as competitive inhibition, sparse layers, and Hebbian updates, can learn quickly from just a few samples. This approach shows promise for improving ML methods' ability to learn from limited data. The MothNet model is a computational model of the M. sexta moth AL-MB that demonstrates rapid learning of vectorized MNIST digits with superior performance to standard ML methods given limited training samples. It includes competitive inhibition in the AL, sparsity in the MB, and Hebbian updates for weight adjustments. The MothNet model, based on the M. sexta moth AL-MB, was tested as a feature generator for an ML classifier. The AL-MB network improved ML method accuracies on a non-spatial task using MothNet-generated features. This demonstrated that the network encoded class-relevant information inaccessible to ML methods alone. The MothNet model significantly outperformed other methods in generating features for ML accuracy improvement. vMNIST data set was used with 85 pixels-as-features. Baseline ML methods did not achieve full accuracy at low N. MothNet instances were randomly generated from connectivity templates. Full details and code for experiments can be found in references [11] and [12]. The experiments compared Cyborg vs baseline ML methods on vMNIST. MothNet was trained using stochastic differential equation simulations and Hebbian updates. The ML methods were then retrained with MothNet outputs as additional features. Trained ML accuracies of baselines and cyborgs were compared to assess gains. MothNet features were found to outperform features generated by conventional ML methods. The study compared Cyborg vs baseline ML methods on vMNIST. MothNet features significantly improved accuracy, outperforming conventional ML methods like PCA and PLS. The new features were used as a front end to SVM and Nearest Neighbors, showing gains in performance. MothNet architecture effectively captured new class-relevant features, demonstrating its effectiveness in enhancing ML methods. MothNet features showed significant improvements in accuracy compared to traditional feature generators like PCA, PLS, and NN on the vMNIST ML baseline test set. The gains ranged from 10% to 88%, with NN models benefiting the most. Even when the baseline accuracy exceeded MothNet's ceiling, the new features still enhanced ML accuracy by providing clustering information. Gains were observed in almost all cases with N > 3, and p-values of the improvements are provided in Table 1. The MothNet architecture, consisting of a competitive inhibition layer (AL) and a high-dimensional sparse layer (MB), showed significant improvements in accuracy over traditional feature generators like PCA, PLS, and NN on the vMNIST dataset. When using a pass-through AL layer, cyborgs still achieved notable accuracy enhancements, with gains ranging between 60% and 100% of those with a normal AL layer. This suggests that the high-dimensional trainable layer (MB) played a crucial role in the performance improvements. The MothNet architecture, with a competitive inhibition layer (AL) and a high-dimensional sparse layer (MB), improved accuracy over traditional methods like PCA, PLS, and NN on the vMNIST dataset. The AL layer added value by generating strong features, benefiting neural networks the most. A bio-mimetic feature generator with competitive inhibition, sparse projection, and Hebbian weight updates significantly enhanced learning abilities on vMNIST and vOmniglot datasets. MothNet features were more useful than PCA, PLS, NNs, and pre-training features, suggesting that the competitive inhibition layer enhances classification by focusing on class-relevant features. The sparse connectivity from AL to MB has computational benefits and increases the effective distance between samples. The insect MB is similar to sparse autoencoders but has differences such as not seeking to match the identity function and having a greater number of active neurons. The MB requires very few samples to improve classification and differs from Reservoir Networks as MB neurons have no recurrent connections. The Hebbian update mechanism in MB is distinct from backpropagation, with weight updates occurring on a local basis. The dissimilarity of optimizers (MothNet vs ML) increased total encoded information."
}