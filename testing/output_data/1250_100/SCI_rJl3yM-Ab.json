{
    "title": "rJl3yM-Ab",
    "content": "In this paper, the authors propose two re-ranking methods, strength-based and coverage-based, to aggregate evidence from multiple passages for open-domain question answering. Their model outperformed existing QA models on three public datasets, achieving an 8% improvement on two of them. The authors propose a method to improve open-domain question answering by aggregating evidence from multiple passages, inspired by previous results analysis. The authors propose a method to enhance open-domain question answering by aggregating evidence from multiple passages. Correct answers are often supported by multiple passages, while incorrect answers lack comprehensive evidence. Aggregating evidence from various passages can help infer the correct answer by covering all aspects effectively. The authors propose aggregating evidence from multiple passages to enhance open-domain question answering. By re-ranking answer candidates based on strengthened and complementary evidence, they aim to improve answer accuracy without increasing model complexity. The re-rankers aim to improve answer accuracy by ranking answer candidates based on strength-based and coverage-based criteria. The strength-based re-ranker considers the frequency of evidence occurrence in different passages, while the coverage-based re-ranker ranks candidates higher if their contexts cover more aspects of the question. The proposed re-ranking-based framework utilizes evidence from multiple passages in open-domain QA, with two re-rankers - a strength-based and a coverage-based re-ranker. The approach achieves state-of-the-art results on Quasar-T, SearchQA, and TriviaQA datasets, outperforming previous methods by up to 8% in F1 score. The method involves running an IR model to find relevant web passages and using an RC model to extract answers from them. The proposed re-ranking-based framework in open-domain QA utilizes evidence from multiple passages. An RC model is used to extract answers from these passages, with a focus on training without labeled answer positions. The R3 RC model is applied to extract candidate answers, which are then reranked based on evidence aggregation from multiple passages. The re-ranking strategies in open-domain QA aim to prioritize answers based on evidence strength and coverage. By counting the occurrences of each answer span in the top-K answers generated by the RC model, the method ranks answers with higher evidence support. This approach leverages the hypothesis that answers supported by more passages are more likely to be correct. Strength-based re-ranking in open-domain QA prioritizes answers based on evidence strength and coverage. By summing up the probabilities of answer spans referring to the same answer, the final prediction is the one with the highest probability. This method does not require training and has negligible time complexity at test time. The re-ranker considers evidence matching the question and treats answer candidates with equal evidence support the same, even if one has complementary evidence satisfying all aspects of the question. The proposed coverage-based re-ranker ranks answer candidates based on the union of evidence from different passages to cover the question. The method involves concatenating passages containing the answer into a \"pseudo passage\" and using a match-LSTM model to measure how well the passage entails the answer for the question. The coverage-based re-ranker ranks answer candidates by concatenating passages containing the answer into a \"pseudo passage\" and using a match-LSTM model to measure how well the passage entails the answer for the question. This involves word-by-word attention and comparison modules using matrices A, Q, and P for answer candidate, question, and passage embeddings. The attention mechanism in the model involves computing attention vectors for each word in the answer and question, followed by a matching function to determine if aspects in the question can be matched by the passage. The model uses element-wise operations to check if words in the answer and question match evidence in the passage. It integrates lexical matching representations into aspect-level matching representations. Additionally, a bi-directional LSTM is added to aggregate aspect-level matching information and measure how well the entire question is matched by the passage. Different ways to make the matching process aware of an answer's positions in the passage are also explored. The model integrates lexical matching representations into aspect-level matching representations and uses a bi-directional LSTM to capture conjunction information among aspects. Different methods to consider answer positions in the passage are explored, with a focus on re-ranking based on the entire matching representation. The model integrates lexical matching representations into aspect-level matching representations using parameters for non-linear transformation. The re-ranker combines strength-based and coverage-based approaches without further training, aiming to improve performance on open-domain QA datasets. The study weighted and combined answer scores from strength-based and coverage-based rankers to make final predictions on open-domain QA datasets. Experiments were conducted on Quasar-T, SearchQA, and TriviaQA datasets, each containing passages retrieved from search engines. The datasets varied in the number of passages collected for each question, with human performance evaluated in an open-book setting. The study utilized various baseline models for open-domain QA datasets, including GA, BiDAF, AQA, and R3. Comparisons were made with the R3 baseline as TriviaQA does not provide a public leaderboard. A pre-trained R3 model was used to generate candidate answers, which were then ranked using a coverage-based re-ranker optimized with Adam. Word embeddings were initialized with GloVe. In this section, the study presents results and analysis of different re-ranking methods on three public datasets. The evaluation focuses on TriviaQA, which evaluates RC models over filtered passages with guaranteed correct answers. Results show that model R3 achieved F1 56.0, EM 50.9 on Wiki domain and F1 68.5, EM 63.0 on Web domain. The study presents results of different re-ranking methods on public datasets. Model R3 achieved competitive performance on Wiki and Web domains. The full re-ranker outperforms previous best performance by a large margin, especially on Quasar-T and SearchQA. The model surpasses human performance on SearchQA dataset. Coverage-based re-ranker shows consistently good performance on three datasets. The BM25-based re-ranker improves F1 scores compared to the R3 model but still lags behind the coverage-based re-ranker with neural network models. BM25 sometimes gives lower EM scores due to its reliance on bag-of-words representation and preference for shorter answers. The F1 score can be improved by considering context information and phrase similarities. The coverage-based re-ranker outperforms the baseline in different answer lengths and question types on TriviaQA and Quasar-T. The strength-based re-ranker shows improvement but is less stable across datasets. The probability-based re-ranker tends to have results close to the baseline, dominated by predicted probabilities. The coverage-based and strength-based re-ranker (counting) have similar trends, except for the strength-based re-ranker performing significantly worse in some cases. The strength-based re-ranker performs worse on \"why\" questions due to non-factoid answers. Table 3 shows the potential improvement of re-rankers, with a clear gap in performance compared to the baseline. The selection of K for coverage-based re-ranker significantly impacts the recall of top-K predictions. Using a larger K increases the likelihood of good answers but also makes re-ranking harder. There is a trade-off between coverage and difficulty, with an appropriate K being crucial for performance. The selection of K for strength-based re-ranker also affects performance. Experiments show that K=50 yields the best results, while performance drops with K=200 due to an increase in incorrect answers. The computation speed is not significantly impacted by different K values. The selection of K for strength-based re-ranker affects performance, with K=50 yielding the best results. An example from Quasar-T dataset shows how the re-ranker corrected a wrong answer predicted by the baseline, ranking the correct answer higher. Open domain question answering involves producing answers by exploiting resources like documents, webpages, or knowledge bases. Recent efforts in improving neural open-domain QA systems involve utilizing machine reading comprehension and search-and-read QA techniques. These methods rely on a document retrieval module to retrieve passages for extracting answers. Various approaches such as using bi-gram passage index, reducing passage length, and noise reduction in passage ranking have been proposed. Our work focuses on text evidence aggregation, modeling the relationship between questions and multiple passages for evidence aggregation, a novel problem in QA research. This approach differs from traditional answer re-ranking methods. Our work introduces re-ranking methods to neural open-domain QA and multi-passage RC, addressing the limitations of traditional QA systems and KB-QA systems. This two-step approach is similar to previous multi-step approaches for single-passage RC and relates to the Epireader model in cloze-test tasks. The curr_chunk discusses a two-step extractor-reasoner model that extracts answer candidates and constructs hypotheses by matching them with sentences in a passage. It contrasts with the Epireader model in terms of sentence matching and handling phrases as answers. Another method mentioned is BID7, which combines answer candidates and uses language models to re-rank hypotheses based on consistency with documents. The curr_chunk discusses a two-step approach for generative QA that extracts answer clues and generates answers from passages. It focuses on combining evidence from multiple passages to improve open-domain QA results significantly. The proposed methods advance the state-of-the-art on three QA datasets, but challenges remain in reasoning and commonsense inference. The curr_chunk discusses future directions for open-domain QA, aiming to generalize the proposed approach to more challenging multipassage reasoning scenarios. It acknowledges support from a DSO grant and expresses gratitude to Mandar Joshi for testing the model on the TriviaQA dataset."
}