{
    "title": "HJ3d2Ax0-",
    "content": "The paper analyzes the impact of depth on the ability of recurrent networks to express correlations over long time-scales. It introduces a measure called Start-End separation rank to quantify the information flow across time supported by the network. Deep recurrent networks are shown to support Start-End separation ranks that reflect the distance from a function that models no interaction between the beginning and end of the input sequence. Deep recurrent networks have exponentially higher Start-End separation ranks compared to shallow networks, allowing them to correlate different parts of input sequences as they extend. Depth provides a significant advantage in modeling long-term dependencies, demonstrated using Recurrent Arithmetic Circuits (RACs). This highlights the importance of depth in recurrent networks for modeling sequential data. Recurrent networks, including deep recurrent networks, have shown success in learning complex functional dependencies for sequences of varying lengths. The stacking of layers in deep recurrent networks allows for hierarchical processing of information, leading to superior performance compared to shallow networks. The importance of depth in recurrent networks for modeling long-term dependencies is highlighted, with ongoing efforts to enhance their ability to integrate data over time. In this paper, the authors address the advantage of depth in recurrent networks for modeling long-term dependencies. They show that deep recurrent networks can model correlations corresponding to longer time-scales than shallow networks. The findings suggest that depth enhances the complexity and temporal capacity of recurrent networks, with evidence from experiments supporting this claim. The authors also discuss the efficiency of depth in neural networks and its impact on the expressiveness of recurrent networks. The authors explore the advantage of depth in recurrent networks for modeling long-term dependencies. They introduce a recurrent arithmetic circuit (RAC) that shows depth efficiency in modeling long-term dependencies, similar to Convolutional Arithmetic Circuits. This connection between machine learning algorithms and arithmetic circuits has precedence in neural networks. The authors introduce Recurrent Arithmetic Circuits (RACs) as a depth-efficient model for long-term dependencies, similar to Convolutional Arithmetic Circuits. They connect RACs to Multiplicative RNNs and Multiplicative Integration networks, showing performance improvements. The study also links RACs to Tensor Train decomposition, suggesting a relationship with a generalized TT-decomposition. The concept of Start-End separation rank is introduced as a measure of a recurrent network's ability to model long-term dependencies over sequential inputs. The study introduces Recurrent Arithmetic Circuits (RACs) as a depth-efficient model for long-term dependencies and connects them to Multiplicative RNNs. It discusses the concept of Start-End separation rank as a measure of a network's ability to model long-term dependencies over sequential inputs. The depth L = 2 RACs are proven to have exponentially higher separation ranks than shallow networks, allowing them to model more elaborate input dependencies over longer periods of time. Additionally, the separation rank of deep recurrent networks grows exponentially with sequence length, while that of shallow networks remains independent of sequence length. The study introduces Recurrent Arithmetic Circuits (RACs) as a depth-efficient model for long-term dependencies in sequential data. It discusses how shallow recurrent networks are inadequate for modeling correlations in long input sequences compared to deep recurrent networks. The Start-End separation rank of recurrent networks grows exponentially with network depth, highlighting the advantages of depth in modeling dependencies. The operation of RACs on sequential data is similar to standard RNNs, with the main difference being the type of non-linearity used in the calculation. The study introduces Recurrent Arithmetic Circuits (RACs) as a depth-efficient model for long-term dependencies in sequential data. It discusses the differences between shallow and deep recurrent networks in modeling correlations in long input sequences. The basic framework of shallow recurrent networks is described, focusing on a sequence to sequence classification task. The input is mapped to vectors for encoding, such as words to vectors or images. The function f(xt) in R^M is an encoding, mapping words or images to vectors through a trained ConvNet. The shallow recurrent network with R hidden channels at time t in [T] is represented by ht in R^R. The learned parameters \u0398 are input, hidden, and output weights matrices, with a non-linear operation g. For common RNNs, the non-linearity is typically sigmoid or tanh, while for RACs, g involves element-wise multiplication between vectors known as Multiplicative Integration. Integration of deep recurrent networks is a natural extension, with each layer acting as a recurrent network receiving the hidden state of the previous layer as input. The output of the depth L recurrent network with R hidden channels in each layer is constructed using learned parameters. The weights matrices for inputs and hidden states are defined, with the output weights matrix representing scores for all classes. The type of deep recurrent network is determined by the non-linear operation, with a common deep RNN using g = g RNN and a deep RAC using g = g RAC. The newly presented class of RACs is considered a good surrogate for common RNNs. The newly presented class of RACs is seen as a good substitute for common RNNs due to structural similarities and performance advantages. The use of arithmetic circuits as surrogates for convolutional networks suggests potential adaptations for recurrent networks. The algebraic properties of RACs are utilized to analyze the benefits of depth in recurrent networks and their ability to model long-term temporal dependencies in sequential data. The Start-End separation rank of a function realized by a recurrent network measures the information flow across time. It is tied to grid tensors, allowing for an exponential boost in modeling long-term temporal dependencies. The separation rank quantifies a function's distance from separability with respect to two disjoint subsets of inputs. The Start-End separation rank quantifies a function's distance from separability with respect to two disjoint subsets of inputs. It is connected to various applications such as chemistry, particle engineering, and machine learning. Researchers have linked the separation rank to the L2 distance of functions and quantum entanglement measures in many-body quantum systems. The Start-End separation rank measures the interaction modeling capability of a recurrent network. A low rank indicates separability between input subsets, while a higher rank signifies modeling dependency. This is crucial for learning long-term temporal dependence in functions. In section 4, deep RACs are shown to support exponentially larger Start-End separation ranks than shallow RACs, making them better suited for modeling long-term temporal dependencies. The use of grid tensors is introduced to evaluate these separation ranks. Tensors are multi-dimensional arrays with modes and dimensions, and the tensor product is a key operator in tensor analysis. The text discusses the matricization of tensors and the computation of scores using a shallow RAC with hidden channels. It introduces the concept of the shallow RAC weights tensor and its construction process. The tensor products in the equations are related to the Multiplicative. The tensor products in the sums are related to the Multiplicative Integration property of RACs. The construction of shallow RAC weights tensor involves a Tensor Train decomposition. Grid tensors are used for function discretization on an exponentially large grid in the input space. The grid tensors of functions realized by recurrent networks allow for calculating separations ranks and determining the benefits of depth in these networks. The tensorial structure of a function realized by a shallow RAC can be tied to its Start-End separation rank through the grid tensor. This establishes an equality between the separation rank and the rank of the matrix obtained from the grid tensor matricization. The grid tensors of functions realized by deep RACs establish a fundamental relation between a function's Start-End separation rank and the rank of the matrix obtained by matricization. This relation holds for all functions and is formulated for deep RACs. The text discusses the exponential enhancement of the Start-End separation rank in deep recurrent networks compared to shallow ones. The main theoretical contributions include results showing the memory capacity difference between deep and shallow recurrent networks, along with a quantitative conjecture on the memory capacity of deep networks. The text discusses the enhanced ability of deep recurrent networks compared to shallow ones in modeling long-term temporal dependencies in sequential input. Theorem 1 shows that deep networks can exponentially model correlations between the beginning and end of input sequences, indicating depth efficiency in memory capacity. The text highlights the depth efficiency of deep recurrent networks in capturing elaborate correlations over longer periods of time, showing that shallow networks would require exponentially more parameters to implement the same function. This is supported by the Start-End separation rank as a correlation measure between the beginning and end of sequences, indicating the superior ability of deep networks in modeling long-term dependencies. The rank of deep RACs increases with sequence length T, while the Start-End separation rank of shallow RACs remains constant. Shallow networks are limited in modeling long-term correlations compared to deep networks. The proof for theorem 1 outlines the relationship between the Start-End separation rank and the matrix rank of shallow RAC weights. The TT-decomposition of A T,1,\u0398 c is used to show the efficiency of deep networks in capturing correlations over time. The combinatorial coefficient DISPLAYFORM6 is exponentially dependent on R, with the rank of the matrix obtained by matricizing any tensor equal to a min-cut separating S from E in the Tensor Network graph. For a deep network, the Start-End separation rank of the function realized by a depth L = 2 RAC is lower bounded by the rank of the corresponding grid tensor matricization. The rank is trivially upper-bounded by the dimension of the matrix, M T /2. The rank of the matricized grid tensor is determined by a weight assignment that resembles raising a rank-R matrix to the Hadamard power of T/2. This results in a matrix with a rank upper-bounded by a multiset coefficient. The assignment achieves this upper bound and provides a lower bound on the Start-End separation rank of deep recurrent networks. It is conjectured that a tighter lower bound exists for networks of depth L > 2. The conjecture suggests a tighter lower bound for deep recurrent networks with depth L > 2, indicating exponential growth in memory capacity. This is motivated by investigating the combinatorial nature of computation in deep RACs using Tensor Networks. The visualization of this perspective is detailed in appendix A, outlining a graphical tool for algebraic operations. An example in FIG3 illustrates a depth L = 3 RAC computation after T = 6 time-steps, showcasing a well-defined computation graph with weight matrices. The inputs are integrated in a depth dependent and time-advancing manner in a Tensor Network. A specific assignment of network weights is found to effectively connect \"Start\" and \"End\" inputs, providing a lower bound on the separation rank of deep recurrent networks. The basic unit in the deep RAC Tensor Network graph connects \"Start\" and \"End\" inputs, with the number of repetitions equal to a specific value for any depth L. Conjecture 1 suggests a potential exponential separation between recurrent networks of different depths, highlighting the advantages of depth in memory capacity and efficiency. In this paper, the concept of depth efficiency in recurrent networks is explored, highlighting the need for a quantifier of 'time-series expressivity'. A measure called Start-End separation rank is proposed to quantify the memory capacity of recurrent networks in modeling long-term temporal dependencies. This measure adjusts to the input series' temporal extent and evaluates the network's ability to correlate sequential data over time. The analysis focuses on Recurrent Arithmetic Circuits. The study analyzed Recurrent Arithmetic Circuits, showing that deep RACs have exponentially increasing Start-End separation rank with input sequence length, while shallow RACs are independent of input length. Depth provides an advantage in modeling long-term dependencies. The analysis combined tools from various fields and can be extended to other recurrent network architectures. The proposed Start-End separation rank can quantify memory capacity in LSTM networks and provide new insights into architectural choices. The study also found that correlations in vanilla shallow recurrent networks do not adapt to sequence length. The experiments in Hermans and Schrauwen (2013) suggest that shallow recurrent networks are related to short time-scales, while deeper layers support longer time-scales. Levine et al. (2017) provide practical conclusions on the number of hidden channels in deep convolutional networks. The conjecture in the current text proposes that the Start-End separation rank of recurrent networks grows exponentially with depth, offering insights into enhancing memory capacity. These analyses can lead to a better understanding of the role of deep layers in recurrent networks. The text discusses the importance of deep layers in recurrent networks for memory capacity. It introduces Tensor Networks (TNs) and their application in calculating shallow and deep Recurrent Activation Chains (RACs). The construction of deep RACs is highlighted for modeling complex temporal dependencies. Additionally, a conjecture is presented on the exponential growth of RACs' Start-End separation rank with depth. A TN is described as a weighted graph where nodes represent tensors, with edges corresponding to the node's degree. A Tensor Network (TN) is a weighted graph where nodes represent tensors, with edges corresponding to the node's degree. The edges in a TN represent the different modes of the corresponding tensor, with each edge's weight equal to the dimension of the tensor mode. The connectivity properties of a TN involve edges representing operations between corresponding tensors, with contracted indices indicating a summation over all possible values. The Tensor Network (TN) involves contracting indices to calculate the tensor represented by the network. This operation is a generalization of matrix multiplication and can be seen as a temporal concatenation in a shallow recurrent network. The Tensor Network (TN) involves contracting indices to calculate the tensor represented by the network, which is a generalization of matrix multiplication and can be seen as a temporal concatenation in a shallow recurrent network. The unit cell in the network consists of input weights matrix, contracted with inputs vector, hidden weights matrix, contracted with hidden state vector of previous time-step, and a 3-legged triangle representing the \u03b4 tensor. The recursive relation defined by the unit cell is shown in FIG5 (b), where h_t = (W_H h_(t-1)) (W_I f_(x_t)). The restricted \u03b4 tensor in the shallow RAC yields element-wise multiplication. After T repetitions of unit cell calculation with sequential input {x_t}^T_t=1, final multiplication of hidden state vector h_T by output weights matrix W_O yields output vector y_T,1,\u0398. The TN representing the order T shallow RAC weights tensor A can be drawn in the form of a standard MPS TN, allowing representation with linear amount of parameters. This TN corresponds to the Tensor Train (TT) decomposition of rank R. The presentation of shallow recurrent network in terms of TN enables min-cut analysis. The computation of a deep recurrent network in the language of TNs is more complex than that of the shallow case due to the reuse of hidden states in each layer at every time-step. This reuse of information poses challenges in representing the network in TNs, as duplicating a vector for different calculations is not feasible in this framework. The claim states that duplicating a vector in a Tensor Network is not possible, which poses a challenge in representing a deep recurrent network. However, a workaround can be achieved using a simple 'trick'. The workaround for representing a deep recurrent network involves duplicating the input data to model the duplication in the computation. This technique allows for the construction of elaborate Tensor Networks (TNs) for analysis, although they are not meant for implementation. The TNs grow exponentially in size as the depth of the recurrent network increases, enabling the modeling of intricate correlations over longer periods of time. The deep recurrent network involves duplicating input data to model complex correlations over longer periods of time. This technique allows for the creation of detailed Tensor Networks (TNs) for analysis. The TNs grow exponentially in size with the network depth, enabling the modeling of intricate correlations. The TNs representing deep recurrent networks exhibit a fractal structure with self-similarities, leading to increased complexity with depth. This complexity is shown to impact the network's ability to model long-term temporal dependencies. The construction of TNs for deep RACs motivates a combinatorial lower bound on Start-End separation rank. The formal motivation for the lower bound on the Start-End separation rank of deep recurrent networks is presented. The conjecture relies on specific network parameters to achieve a certain rank, combining claim 2 and lemma 1. Claim 2 establishes the lower bound by grid tensor matricization, while lemma 1 shows that finding a single example with a rank greater than the desired lower bound is sufficient. The rank of the matricized grid tensor is crucial for deep recurrent networks, as finding an example with a rank higher than the desired lower bound implies the desired inequality holds for most network parameter values. By choosing a specific weight assignment that separates the first layer from higher layers, the computation in deeper layers only adds a constant factor to the tensor. Evaluating the rank involves considering graph segments involving only certain indices, which under matricization, result in a constant multiplying each row of the matrix. The rank of the matricized grid tensor is crucial for deep recurrent networks. Evaluating the rank involves considering graph segments with specific weight assignments, resulting in a constant multiplying each row of the matrix. The effective TN after T time-steps in a RAC with L layers and R hidden channels per layer increases exponentially with the depth of the RAC. The rank of the matricized grid tensor is important for deep recurrent networks. Evaluating the rank involves specific weight assignments, resulting in a constant multiplying each row of the matrix. The effective TN after T time-steps in a RAC with L layers and R hidden channels per layer increases exponentially with the depth of the RAC. The function computes the output after T time-steps of an RAC with L layers, R hidden channels per layer, and weights denoted by \u0398. The tensor V reflects the contribution of the \"Start\" set indices. The argument of the chain of products in the expression is an order t2 tensor, exactly given by the TN representing the computation of a depth L = 1 RAC after t2 time-steps. The lower bound presented in conjecture 1 is derived by considering a rank R matrix obtained through matricization of the TN basic unit. The grid tensor's matricization is defined by specific template vectors x (1) , . . . , x (M ) \u2208 X. The exponential advantage of deep recurrent networks over shallow ones in modeling is proven in theorem 1. The study demonstrates the superiority of deep recurrent networks over shallow ones in capturing long-term dependencies. Bounds on the Start-End separation rank for shallow and deep RACs are proven. The Tensor Network construction of a shallow RAC is illustrated, showing the representation of weights tensor using a Matrix Product State Tensor Network. The rank of a matrix obtained by matricizing a tensor is discussed, based on a min-cut separating specific partitions in the Tensor Network graph. The MPS Tensor Network represents weights tensor using a Matrix Product State. The rank of the matrix obtained by matricizing the tensor is discussed, based on a min-cut separating partitions in the Tensor Network graph. The rank of the function realized by a deep network is lower bounded by the rank of the matrix, for any choice of template vectors. An assignment of weight matrices and initial hidden states is provided to achieve the rank for all configurations of the recurrent network weights. The MPS Tensor Network uses a Matrix Product State to represent weights tensors. The rank of the function realized by a deep network is lower bounded by the rank of the matrix, for any choice of template vectors. An assignment of weight matrices and initial hidden states is provided to achieve the rank for all configurations of the recurrent network weights. The output for the corresponding class after T time-steps is calculated based on the chosen set of template vectors. The MPS Tensor Network utilizes Matrix Product State to represent weight tensors. The grid tensor is defined with a specific form, split into two expressions mapping to a vector and a matrix. The matrix can be expressed as a sum of rank-1 matrices, showing linear independence of vectors. The curr_chunk discusses the maximal matrix rank for matrices with polynomial functions. It introduces Claim 5, stating that if there exists a point x where the rank of the matrix is at least r, then the set of points where this condition holds is non-empty. This information complements the previous discussion on the MPS Tensor Network and the representation of weight tensors using Matrix Product State. The curr_chunk explains that for a polynomial function, if a point x exists where the rank of the matrix is at least r, then the set of points where this condition holds has zero measure. It is sufficient to provide a specific assignment of recurrent network weights to achieve a certain rank in order to prove a theorem. Additionally, if a matrix with polynomial entries has a contributor with the highest degree of x in the determinant, it is fully ranked for all x values except for a finite set. The lemma proves that a matrix with polynomial entries is fully ranked for all x values except for a finite set, based on the leading term of its determinant. This helps confirm the assignment of recurrent network weights to achieve the required rank. Additionally, a vector rearrangement inequality is established using a theorem from Hardy et al. (1952). The rearrangement inequality, known as lemma 2, ensures that the matrix \u016a satisfies the conditions of lemma 1 and is fully ranked. This is proven by showing that for all permutations \u03c3 in SN, there exists a component \u0135 in [R] where the inequality is hard to achieve. This is derived from the conditions of equality in the rearrangement inequality, involving a set of N different vectors in RR. The lemma concludes that the inequality for \u03c3 = IN is implied, leading to a convoluted expression that is strictly less than its respective contribution in \u03c1*."
}