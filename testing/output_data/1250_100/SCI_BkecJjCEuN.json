{
    "title": "BkecJjCEuN",
    "content": "The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask and self-supervised learning. Training an end-to-end audio feature extractor based on WaveNet, the study demonstrates improved performance in supervised classification tasks by up to 6% through simultaneous training with self-supervised tasks. Incorporating data augmentation into the multitask setting leads to further gains in performance. Incorporating self-supervised audio tasks during model training can significantly improve performance by utilizing WaveNet as a feature extractor for raw audio data. This approach aims to address the challenge of limited labeled datasets by leveraging unlabeled data sources for unsupervised learning. WaveNet-based models can adapt to subtle variations in tasks efficiently. The framework is applied to supervised classification tasks like audio tagging, speaker identification, and speech command recognition. Leveraging unlabeled data improves performance, and self-supervised tasks can be used for pre-training and transfer learning. Models trained for multiple tasks may uncover underlying structures for better single-task performance with less data. The text discusses the use of self-supervised learning in the audio domain to create a common embedding of acoustic waveforms. This approach leverages unlabeled data to improve performance in tasks like image completion, colorization, and motion segmentation. The network architecture is based on WaveNet, with task-specific \"head\" networks processing the embedded data. The text describes the use of self-supervised learning in the audio domain to create a common embedding of acoustic waveforms. The WaveNet trunk consists of 3 blocks of 6 dilation stacks, with each stack having a gate and filter module. The setup is tested on supervised tasks like audio tagging, speaker identification, and speech command recognition, along with up to three self-supervised tasks trained with unlabeled data. The dataset used for training includes 11,073 audio files from the FSDKaggle2018 dataset. Each audio segment is cropped to 2 seconds and padded with zeros if needed. The WaveNet trunk produces embeddings that are averaged across time and fed into a fully-connected layer for classification. Speaker identification is trained on the VoxCeleb-1 dataset with data from 1251 speakers. The speech command recognition task involves processing audio segments cropped to 2 seconds, normalized, and filtered before being fed into a neural network. The dataset consists of 65,000 utterances of 30 words, with 12 categories including words like yes, no, up, down, left, right, on, off, stop, go, unknown, and silence. The recognition head architecture includes three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity. The speech recognition task involves processing 2-second audio segments through a neural network with 1D convolutions. The output is evaluated using a cross-entropy loss and includes self-supervised tasks like next-step prediction and noise reduction. These tasks were trained on both labeled and unlabeled data from the Librispeech dataset. The goal was to create a generic multitask framework for audio using waveform inputs instead of high-level features like spectrograms. State-of-the-art baseline models for audio tasks may vary in network architectures, limiting information gained from self-supervised tasks. Multitask learning improves performance without increasing training data, closing the gap between spectral representations and waveform models is future work. Joint training with three self-supervised tasks benefits supervised tasks like audio tagging. Multitask learning with additional unsupervised tasks improved performance metrics, with a MAP@3 increase of up to .056 with 500 hours of unlabeled data. Speech command classification and speaker identification tasks also showed improvement with more unlabeled data. Speaker identification performance increased from 73.81% to 75.22% with multitask learning. These results demonstrate the effectiveness of multitask learning in enhancing supervised tasks without the need for additional labeled data. Training a single task model on audio tagging with pitch-shift and noise augmentation resulted in performance gains. Pitch-shift augmentation produced a MAP@3 increase of .066, while noise augmentation showed a smaller increase of .024. Combining pitch-shift augmentation with self-supervised tasks yielded the highest performance increase of .089, indicating the complementary nature of these methods for improving label efficiency. Transfer learning from self-supervised tasks trained on unlabeled data to supervised tasks has also shown promise in computer vision. Transfer learning experiments were conducted on self-supervised tasks trained on unlabeled data, followed by fine-tuning with a small amount of labeled data for a supervised task. Results favored transfer learning over training all tasks simultaneously. The study showed that jointly training supervised tasks with self-supervised tasks using a WaveNet-based model on raw audio waveforms led to improved performance, which scaled with the amount of unlabeled data. This approach is expected to generalize to various audio classification tasks. Our approach using a WaveNet-based model on raw audio waveforms for transfer learning with self-supervised tasks showed improved performance on various audio classification tasks. The multitasking model learned to forecast audio frames, remove noise, and perform upsampling, forming a representation of the audio that can be further explored for broader auditory tasks. WaveNet models process high temporal resolution raw audio signals using causal dilated convolutions, making them faster to train than RNNs. The architecture includes task-specific neural networks built on a task-agnostic trunk, with blocks of dilated causal convolutions. Each block consists of dilated causal convolution layers with increasing dilation factors, residual connections, and nonlinearities. The architecture of WaveNet models involves a task-agnostic trunk with blocks of dilated causal convolutions. Each block has increasing dilation factors, residual connections, and nonlinearities. The trunk has a total receptive field of 190, corresponding to about 12 milliseconds of audio sampled at 16kHz. Task-specific heads are simple neural networks that share the trunk with other tasks. The WaveNet model architecture includes a task-agnostic trunk with dilated causal convolutions. Each task has its own objective function and optimizer, with supervised tasks as primary and self-supervised tasks as auxiliary. The head architectures are designed to be simple, forcing the trunk to learn a representation suitable for multiple audio tasks. The next-step prediction task involves predicting the next value in a sequence of audio waveform frames. The next-step prediction task in the WaveNet model involves predicting the next value in a sequence of audio waveform frames using a 2-layer convolutional neural network. The task is treated as a regression problem, with the model predicting the next audio frame based on the input data from the trunk. This differs from the original WaveNet implementation, which treated the task as a classification problem. In noise reduction tasks, the model is trained to predict clean audio samples from noisy ones using a regression approach. The noise reduction head structure is similar to the next-step prediction head, minimizing a smoothed L1 loss between clean and noisy waveforms. For the upsampling task, the original audio is downsampled to 4 kHz and then repeated to mimic the 16 kHz sample rate. The network infers high frequency information lost during the transform using a structure similar to the denoising task, with a smooth L1 loss function for comparison. The model was trained using raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets. Audio samples were cropped to two seconds, downsampled to 16 kHz, and normalized. Noise-reduction task involved adding noise from ChiME3 datasets at random SNR levels. Hyperparameter search was conducted for the number of blocks, layers, units, and learning rate. The model was trained on raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets, with hyperparameter search conducted for the number of blocks, layers, units, and learning rate. The performance of the network was largely unaffected by architecture specifications, with a focus on learning rate. The model was jointly trained on all tasks simultaneously using a weighted sum of losses for each task. The \"Adam\" optimizer was used with specific parameters. The \"Adam\" optimizer BID6 was used with specific parameters \u03b2 0 = 0.9, \u03b2 1 = 0.99, \u03b5 = 10 \u22128. The learning rate decayed by a factor of .95 every 5 epochs. A batch size of 48 was used for all experiments. Noise reduction and upsampling tasks required separate forward propagation of noisy and downsampled audio. Important parameters can be found in TAB3."
}