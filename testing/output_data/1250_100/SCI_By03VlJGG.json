{
    "title": "By03VlJGG",
    "content": "Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Our approach proposes a multimodal embedding using different neural encoders for various data types, including text, images, and numerical values. We introduce two new benchmarks, YAGO-10-plus and MovieLens-100k-plus, with additional relations like textual descriptions and images. Our model effectively utilizes this additional information to improve accuracy and predict missing multimodal attributes. Knowledge bases are crucial for various computational systems but often suffer from incompleteness and noise in their entries. The focus of active research is on learning relational knowledge representation to address deficiencies in knowledge bases, such as incompleteness and noise. Different data types in real-world knowledge bases, including numerical attributes, textual attributes, and images, provide crucial evidence for knowledge base completion. These diverse types of relations cannot be directly represented as links in a graph but can enhance accuracy and efficiency in inferring missing facts. In this paper, a multimodal embedding approach is introduced for modeling knowledge bases with various data types like textual, images, numerical, and categorical values. The approach aims to go beyond traditional link-based graph views, utilizing all observed information and representing uncertainty in relational evidence. The method extends the DistMult approach by incorporating neural encoders for different evidence data types to learn vectors for entities and relations. The approach introduced in the paper involves using neural encoders for different types of evidence data to encode entities and relations in a unified model for relational data modeling. Evaluation on two relational databases shows that the model effectively utilizes additional information like textual descriptions, numerical attributes, and images to improve link-prediction accuracy. The paper introduces a model that uses neural encoders to encode entities and relations for relational data modeling. It evaluates the model on two databases, showing improved link-prediction accuracy with additional information like textual descriptions, numerical attributes, and images. The goal is to train a machine learning model to score the truth value of factual statements represented as triplets of subject, relation, and object. The paper presents a model using neural encoders for relational data modeling, focusing on the DistMult approach for learning parameters. It involves training data with observed facts in triple form and uses a pairwise ranking loss to score existing triples higher than non-existing ones. The proposed work introduces a method to incorporate various types of objects into existing relational models like DistMult. It suggests learning embeddings for attributes such as title, poster, genre, or release year of a movie using domain-specific encoders. This allows for a more flexible representation of objects in knowledge bases, which may not be restricted to a fixed set of entities. The model embeds subject entity, relation, and object value to score the truth value of a triple using DistMult operation. Deep learning is used to construct encoders for different data types to estimate if a fact holds. An example is given for a knowledge base with movie details, where subject and relation are embedded using direct lookup, and object is encoded based on its data type. Training the model is similar to DistMult, with appropriate encoders used for each data type. The model uses different encoders for multimodal objects, such as embedding subject entity and relation using one-hot encoding passed through a dense layer. For categorical object entities, a dense layer with selu activation is used. Numerical objects are embedded using a feed forward layer after basic normalization, projecting them into a higher-dimensional space. This approach contrasts with methods that treat numbers as distinct entities. The model uses different encoders for multimodal objects, such as embedding subject entity and relation using one-hot encoding passed through a dense layer. For categorical object entities, a dense layer with selu activation is used. Numerical objects are embedded using a feed forward layer after basic normalization, projecting them into a higher-dimensional space. This approach contrasts with methods that treat numbers as distinct entities. Entities are learned independently, with vectors for numbers 39 and 40 being similar to each other based on data. Different encoders are used depending on the length of the strings involved, such as character-based stacked, bidirectional LSTM for short attributes like names, and CNN over word embeddings for longer strings like detailed descriptions of entities. Images can also provide useful evidence for modeling entities, extracting details like gender, age, job, etc. The model uses various encoders for different types of data, such as images for extracting details like gender, age, and job. It can also compactly represent semantic information in images using pretrained networks and bilinear pooling. The framework can be extended to other data types like speech/audio, time series, and geospatial coordinates with appropriate encoders. The model utilizes different types of information, including text, numerical values, and images, in the encoding component to create relational triples. Various methods incorporate extra information as observed features for entities, such as numerical values, images, and text. Additionally, some approaches address multilingual relation extraction by considering raw text as an extra feature and using matrix factorization to jointly embed knowledge bases and textual relations. Our model differs from existing approaches in three main aspects: it integrates different types of information in a unified model, treats this information as relational triples of structured knowledge, and represents uncertainty to support missing values. Two new benchmarks are provided by extending existing datasets with additional information like posters for MovieLens 100k and image/textual data for YAGO-10 from DBpedia. The MovieLens-100k dataset from YAGO-3 contains 100,000 ratings from 1000 users on 1700 movies. It includes rich relational data about users and movies, with genre represented as a binary vector. Movie posters are collected from TMDB, and 5-point ratings are treated as relations in KB triple format. 10% of ratings are used for validation. This dataset is small but specialized. Another dataset is more suitable for knowledge graph completion and link prediction. The YAGO3-10 knowledge graph consists of 120,000 entities and 37 relations. Additional relations like wasBornOnDate and happenedOnDate are identified, with dates as values. The model's ability to utilize multimodal information is evaluated through link prediction tasks. The model's capability in genre prediction on MovieLens and date prediction on YAGO is examined. A qualitative analysis on title, poster, and genre prediction for MovieLens data is provided. In this section, the model's performance in link prediction tasks is evaluated. The evaluation includes metrics such as mean reciprocal rank (MRR) and Hits@K. The model is trained on MovieLens data using Rating as the relation between users and movies. The results are presented in a filtered setting, ranking triples in the test data against those not appearing in the train or test datasets. The evaluation of the model's performance in link prediction tasks is based on metrics like mean reciprocal rank (MRR) and Hits@K. Using Rating as the relation between users and movies, different encoding methods are employed for various relations. The evaluation on MovieLens dataset focuses on rating triples, with models labeled based on the type of information used. The R+M+U+T model outperforms others, highlighting the importance of incorporating additional information. Hits@1 for the baseline model is 40%, matching existing recommendation systems. Incorporating movie titles has a greater impact than poster information. The evaluation of link prediction models on the YAGO dataset shows that models incorporating various types of information perform better than those using only text. The model that encodes all types of information consistently outperforms others, while the model using only text performs the second best. Model S is surpassed by all other models, emphasizing the importance of utilizing different data types for higher accuracy. Additionally, a comparison with the state-of-the-art ConvE BID4 model is made, showing potential for future incorporation of different scoring methods. Further analysis on the YAGO dataset provides insights into the performance of the models. The evaluation on multimodal attributes prediction shows that models incorporating textual description benefit certain relations, while images are useful for detecting genders. Numerical data is more effective for certain relations. Link prediction evaluation on MovieLens dataset demonstrates that utilizing all information outperforms other methods. The model utilizing all information outperforms other methods, incorporating information from posters and titles to predict movie genres. TAB6 shows link prediction evaluation on YAGO-10-plus with test data consisting of numerical triples. S+N+D+I outperforms other methods in modeling numerical information. The model utilizes multimodal values for more fruitful modeling. In this setting, examples are provided where a model ranks existing posters based on a query for a multimodal attribute. The selected posters show visual similarity to the original poster in terms of background, appearance of a face, and movie title. Genres and titles are also found to be similar, with at least one original genre appearing in the predicted ones. For instance, predicted titles for \"Die Hard\" relate to dying and being buried. In this study, a novel neural approach to multimodal relational learning was introduced to improve link prediction accuracy by utilizing multiple sources of information. A universal link prediction model was proposed that incorporates different types of information to model knowledge bases. The model includes a compositional encoding component to learn unified entity embeddings that encode various information for each entity. The evaluation showed that the model outperformed a common link predictor, emphasizing the importance of leveraging diverse information for accurate predictions. Additionally, two new benchmarks, YAGO-10-plus and MovieLens-100k-plus, were introduced to address the lack of extra information in existing datasets. The model effectively utilizes extra information to benefit existing relations. Datasets and open-source implementation will be released. Future work includes investigating different scoring functions for link prediction, modeling decoding of multimodal values, and exploring efficient query algorithms for knowledge bases."
}