{
    "title": "rJxlc0EtDr",
    "content": "Recent research has focused on developing neural network architectures with external memory, often using the bAbI question and answering dataset. A classic associative inference task was employed to assess the reasoning capacity of existing memory-augmented architectures, revealing their struggle with long-distance associations. A novel architecture called MEMO was introduced, featuring components that enable reasoning over longer distances, such as a separation between memories and items in external memory, and an adaptive retrieval mechanism with variable 'memory hops'. MEMO is a neural network architecture that allows for variable 'memory hops' to solve reasoning tasks. It supports inferential reasoning by recombining single experiences to infer relationships, which is supported by the hippocampus. The hippocampus stores memories independently through pattern separation to minimize interference between experiences. The hippocampus stores memories independently through pattern separation to minimize interference between experiences. Recent research shows that the integration of separated experiences occurs at retrieval, allowing for inference and supporting episodic memories. Neural networks with external memory, like the Differential Neural Computer and end-to-end memory networks, have shown remarkable abilities in computational and reasoning tasks. Neural networks with external memory have shown remarkable abilities in computational and reasoning tasks. To overcome limitations in traditional neural networks, a new task called Paired Associative Inference (PAI) was introduced to capture inferential reasoning. PAI forces neural networks to learn abstractions to solve previously unseen associations, followed by tasks involving finding the shortest path and bAbi to investigate memory representations for reasoning. The MEMO approach enhances memory-based reasoning by retaining all facts in memory and utilizing a recurrent attention mechanism for flexible weighting. It addresses the issue of prohibitive computation time in neural networks by introducing new architectural components. The input is adjusted for computation efficiency, inspired by human associative memory models like REMERGE. The network uses adaptive computation time to decide when to halt processing and output an action in reinforcement learning. The network uses adaptive computation time to decide when to halt processing and output an action in reinforcement learning. The halting policy is trained using REINFORCE to adjust weights based on the optimal number of computation steps. The approach encourages the network to prefer representations and computation that minimize the required computation steps. Contributions include a new task emphasizing reasoning and an investigation of memory. The curr_chunk discusses an investigation into memory representation for inferential reasoning and enhancements to memory architectures. It introduces a REINFORCE loss component for learning the optimal number of iterations for task-solving. Empirical results on paired associative inference, shortest path finding, and bAbI tasks are presented. The setup involves predicting answers from knowledge inputs and queries. The multilayer, tied weight variant of End-to-End Memory Networks is compared to the proposed architecture. The curr_chunk discusses memory representation for inferential reasoning in tasks like bAbI. It explains how End-to-End Memory Networks (EMN) embeds words and calculates weights over memory elements. EMN is trained using cross entropy loss. Another approach, MEMO, embeds inputs differently using a common embedding. MEMO embeds input differently by deriving a common embedding for each input matrix, adapting them to be keys or values without using hand-coded positional embeddings. Multiple heads are used to attend to memory, allowing for flexible capturing of input sentences. MEMO utilizes a unique attention mechanism with multi-head attention, DropOut, and LayerNorm for improved generalization and learning dynamics. The attention mechanism involves matrices for transforming logits and queries, as well as an output MLP for producing answers. While it shares some features with prior work, such as normalization factors and multiheading, it also has distinct differences. MEMO utilizes a unique attention mechanism with multi-head attention, DropOut, and LayerNorm for improved generalization and learning dynamics. It differs from prior work by preserving the query separated from the keys and values, leading to linear computational complexity. To determine the number of computational steps required to answer a query effectively, MEMO uses observations processed by GRUs and an MLP to define a binary policy and approximate its value function. The input to this network is the Bhattacharyya distance between attention weights at the current and previous time steps. The network in MEMO is trained using REINFORCE and adjusts parameters using n-step look ahead values. A new term, L Hop, is introduced in the loss function to minimize the expected number of hops and encourage the network to prefer certain representations and computations. The network in MEMO is trained using REINFORCE to minimize computation. Variance issues with training discrete random variables are addressed, and a binary halting random variable's variance is bounded. The reward structure is defined by the target answer and prediction. The final layer of M LP R is initialized with bias init to increase the probability of one more hop. A maximum number of hops, N, is set, and gradient sharing is not present between the hop network and the main MEMO network. In recent years, there has been a growing interest in memory-augmented networks for abstract and relational reasoning tasks. The Differential Neural Computer (DNC) and other models have been developed to read and write to a memory store, with some models showing success in solving algorithmic problems. Various alternative memory-augmented architectures have since been created, each with its own unique approach. Recurrent Entity Network, Working Memory Network, and RelationNet are new models that excel at relational reasoning tasks. Adaptive Computation Time (ACT) and Adaptive Early Exit Networks are approaches to adjust computational budget based on task complexity. Conditional computation techniques such as using REINFORCE to adjust the number of computation steps have been applied to various neural network architectures, including recurrent neural networks and networks with external memory. These approaches allow the network to exit prematurely or skip unnecessary computation steps based on task complexity. Graph Neural Networks (GNNs) use iterative message passing to propagate embeddings in a graph for various learning tasks. Unlike GNNs, our method adapts the number of message passing steps and does not require passing messages between memories. The paper introduces a task called paired associative inference (PAI) to probe the reasoning capacity of neural networks by testing their ability to appreciate distant relationships among elements in memories. This task involves associating random pairs of images and requires inference to link them together. The paper introduces a task called paired associative inference (PAI) to test neural networks' reasoning capacity by linking random pairs of images. The task involves indirect queries that require inference across multiple episodes, testing episodic memory. The network is presented with image A, a cue, and must choose between two possible choices: the match, image C, originally paired with B, or the lure, another image C paired with B forming a different triplet. This task is analogous to appreciating associations between elements, like recognizing two people walking with the same little girl. The study compared MEMO with other memory-augmented architectures like EMN, DNC, and UT. MEMO and DNC achieved the highest accuracy on the A-B-C set, while EMN and UT struggled. MEMO outperformed other architectures on longer sequences. Further analysis showed that MEMO required fewer steps than DNC to achieve the same accuracy on a length 3 PAI task. MEMO outperformed other memory-augmented architectures like EMN, DNC, and UT on the A-B-C set, achieving the highest accuracy. In analyzing how MEMO approached the task, attention weights of an inference query were examined. The network retrieved memories in different slots to associate cues with matches and avoid interference. The sequence A-B-C was not directly experienced together, but stored in different slots. MEMO assigned appropriate probability masses to support correct inference decisions, confirming its accuracy. The sequence of memories activation in MEMO is similar to computational models of the hippocampus and observed in neural data. The number of hops taken by the network influences the algorithm used to solve the inference problem. Ablation experiments confirmed that specific memory representations and recurrent attention mechanisms support successful inference in MEMO. The study compared the adaptive computation mechanism in MEMO with ACT and found MEMO to be more data efficient. The weights analysis of an inference query in the length 3 PAI task was presented, showing the network's use of hops for solving the inference problem. Synthetic reasoning experiments on randomly generated graphs were also conducted. The study compared MEMO with ACT in adaptive computation, finding MEMO more data efficient. Synthetic reasoning experiments on randomly generated graphs showed MEMO outperforming EMN in predicting the first node of the shortest path on more complex graphs. MEMO also outperformed DNC in highly connected graphs, showing scalability in considering more paths as the number of hops increases. Universal Transformer had varying performance in predicting nodes of the shortest path compared to MEMO. In comparison to MEMO, UT showed slightly lower performance in direct reasoning tasks. Test results for the best 5 hyper-parameters for MEMO are reported, along with results from the best run for EMN, UT, and DNC. The study also focused on the bAbI question answering dataset, training the model on a 10k training set and achieving high accuracy. Ablation experiments were conducted to analyze the contribution of each architectural component to the model's performance. In an in-depth investigation of memory representations for inferential reasoning, MEMO, an extension to existing architectures, achieved state-of-the-art results on reasoning tasks. MEMO excelled in a new task, paired associative inference, and was the only architecture to solve long sequences in challenging graph traversal tasks. The use of layernorm in recurrent attention mechanisms was crucial for stable training and improved performance. MEMO achieved state-of-the-art results on reasoning tasks, including solving long sequences and the bAbI dataset tasks. The flexible weighting of individual elements in memory, combined with a powerful recurrent attention mechanism, contributed to its success. The dataset used ImageNet images, with sequences of length three, four, and five items. Each dataset contained training, validation, and testing images, with no repetition in sequences. The batch was built with sequences of length 3, consisting of memory, query, and target entries. N sequences were selected to create a single entry, with N = 16. The memory content included pair wise associations between items in the sequence, resulting in 32 rows for S = 3. Queries were generated with cues, matches, and lures, with 'direct' and 'indirect' types. 'Direct' queries test episodic memory by retrieving episodes stored in different memory slots. The queries presented to the network consist of three image embedding vectors (cue, match, lure) in a randomized order to avoid degenerate solutions. The task requires appreciating the correct connection between images and avoiding interference from other items in memory. Each batch contains balanced direct and indirect queries, with targets selected randomly from all possible queries supported by the current memory store. The network is presented with direct and indirect queries, with targets being the class of the matches. Longer sequences result in more indirect queries that require different levels of inference. Different models use memory and query inputs in various ways to make predictions. The model uses architecture described in Section H, with output used as the model output. Results are from the evaluation set after training, each set containing 600 items. Graph generation follows a method similar to Graves et al. (2016), with graphs generated by sampling two-dimensional points from a unit square. Graph representation includes a graph description, query, and target, with connections between nodes represented as tuples of integers. Training involves sampling a. During training, a mini-batch of 64 graphs is sampled with queries and target paths. Queries are represented as a matrix of size 64 \u00d7 2, targets as 64 \u00d7 (L \u2212 1), and graph descriptions as 64 \u00d7 M \u00d7 2. The upper bound M is determined by the maximum number of nodes multiplied by the out-degree of the nodes. Networks are trained for 2e4 epochs with 100 batch updates each. EMN and MEMO use the graph description as their memory and the query as input. The model predicts answers for nodes sequentially, with the first node predicted before the second. An important difference between MEMO and EMN is that for EMN, the ground truth answer of the first node is used as the query. During testing, different approaches were used to enhance performance and test capabilities. For EMN, the ground truth answer of the first node is used as the query for the second node, while MEMO uses the model's predicted answer. Universal Transformer and DNC also embed queries and graph descriptions, with DNC presenting information sequentially. The weights for each answer are not shared in any of the models. During testing, different approaches were used to enhance performance and test capabilities. For EMN, the ground truth answer of the first node is used as the query for the second node, while MEMO uses the model's predicted answer. Universal Transformer and DNC also embed queries and graph descriptions, with DNC presenting information sequentially. The weights for each answer are not shared in any of the models. The models are trained using Adam with cross-entropy loss and evaluated on a batch of 600 graph descriptions, queries, and targets. DNC and UT have a 'global view' on the problem, allowing them to reason and work backwards from the end node, while MEMO has a 'local view' where the answer to the second node depends on the answer about the first node. The performance of MEMO and EMN models was compared by using different training regimes. When MEMO was given the ground truth for node 1 as the query for node 2, its performance improved significantly compared to using the prediction of the first node. On the other hand, when EMN was trained in a similar way to MEMO, its performance dropped to almost chance level. These results were consistent across different scenarios and datasets. The text discusses pre-processing steps for datasets, including converting text to lowercase, handling punctuation, and separating words. It also mentions how queries are separated from stories in the dataset and used in training. Additionally, it explains the input sizes for queries and stories in the training process for different models like EMN, MEMO, and DNC. In the case of DNC and UT, stories and queries are embedded similarly to MEMO. The models are trained using Adam optimization with specific hyperparameters. To handle temporal context in tasks, MEMO includes a time encoding column vector. Networks are trained for 2e4 epochs with 100 batch updates each. Evaluation involves sampling a batch of 10,000 elements and computing mean accuracy. MEMO was trained using cross entropy loss to predict class ID, node ID, and word ID in different tasks. The halting policy network parameters were updated using RMSProp. The temporal complexity of MEMO is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where various factors contribute to the complexity. MEMO is trained using cross entropy loss to predict class ID, node ID, and word ID. The halting policy network parameters are updated using RMSProp. The temporal complexity of MEMO is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), with various factors contributing to the complexity. In experiments, parameters A, N, H, I, S, d are fixed constants. The spatial complexity of MEMO is O(I \u00b7 S \u00b7 d), where the size of memory is fixed. The halting unit h in MEMO is defined differently than in the original ACT, with a binary policy \u03c0 t. The halting mechanism in MEMO+ACT is defined with a slight change from the original ACT, using a binary policy \u03c0 t. This change aims to increase fairness in comparison and enable more powerful representations. The halting probability is defined as T = min{t : where , with a fixed value of 0.01. The answer provided by MEMO+ACT is determined by a t, the answer provided at hop t. The architecture used is similar to Graves et al. (2016), with hyperparameters searched for optimization. The implementation and hyperparameters for 'universal_transformer_small' can be found at the given GitHub link. Hyperparameters used are detailed in Table 15, with a search conducted for optimal training ranges on specific tasks."
}