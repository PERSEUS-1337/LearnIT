{
    "title": "Bkl87h09FX",
    "content": "The paper discusses the progress in contextualized word representation through unsupervised pretraining tasks like language modeling with methods such as ELMo. It presents a systematic study comparing different pretraining tasks, highlighting the effectiveness of language modeling. However, strong baselines and varied results across tasks suggest limitations in the pretraining and freezing of sentence encoders for further research. During pretraining, a shared encoder and task-specific model are trained for each task, then the shared encoder is frozen and the task-specific model is retrained for each evaluation task. State-of-the-art models for NLP tasks include components for extracting sentence representations, typically trained directly for the task. Pretraining for sentence encoding has gained interest due to the limited training data for NLP tasks. Recent papers have shown that pretrained sentence encoders can achieve strong performance on NLP tasks. Different pretraining tasks were evaluated to determine the most effective approach. The study trained reusable sentence encoders on 17 pretraining tasks and evaluated them on nine target language understanding tasks. In a study evaluating different pretraining tasks for sentence encoders, language modeling was found to be the most effective. Multitask learning during pretraining also showed gains, but ELMo-style pretraining without fine-tuning was deemed limiting. Trivial baseline representations performed nearly as well as the best pretrained encoders, highlighting the need for further research on general-purpose pretrained encoders. The recent surge in progress with pretrained encoders like CoVe, ULMFit, ELMo, and Transformer LM has sparked interest in reusable sentence encoders. However, the question of which tasks should be used to create them remains unanswered. Zhang & Bowman found that language modeling uncovers the most syntactic structure, while Peters et al. delved into model design issues for ELMo. Further research is needed on general-purpose pretrained encoders. The study delves into model design issues for ELMo, showing that standard architectures for sentence encoding can be pretrained effectively with similar performance. Multitask representation learning in NLP has been well studied, with promising results in combining translation and parsing, as well as the benefits of multitask learning in sentence-to-vector encoding. The study explores the effectiveness of multitask learning for lower-level NLP tasks by comparing pretrained encoders on various tasks. A random encoder baseline, initialized with zero examples, performs surprisingly well, outperforming bag-of-words encoders. This result aligns with findings from previous work on ELMo-like models and Reservoir Computing. The model's skip connection allows task-specific models to directly access word representations. The curr_chunk discusses the use of pretrained ELMo models for various NLP tasks, including acceptability classification, sentiment classification, semantic similarity, textual entailment, and more. The tasks are used for pretraining language models on datasets like WikiText-103 and 1 Billion Word Language Model Benchmark. Additionally, machine translation models are trained on datasets like WMT14 English-German and WMT17 English-Russian. The curr_chunk discusses the SkipThought and DisSent models trained on WikiText-103 data for predicting discourse markers in text. These models reconstruct comment threads from Reddit using a dataset of comment-response pairs. The models are implemented using the AllenNLP toolkit with a two-layer bidirectional LSTM architecture. The curr_chunk describes the use of a pretrained character-level CNN word encoder from ELMo in a two-layer bidirectional LSTM model for downstream tasks. The model leverages the top-level LSTM hidden states as contextual representations and incorporates skip connections for input representations. Additionally, the pretrained ELMo model is utilized for multitask learning, with lower layers pretrained on language modeling and higher layers on specific tasks. This approach is chosen due to the extensive tuning of the ELMo model compared to in-house resources. The study compares different pretraining tasks to evaluate their effectiveness in complementing large-scale language model pretraining. The curr_chunk discusses the evaluation of pretrained language models for downstream tasks, focusing on the use of ELMo and skipping CoVe due to its lower effectiveness. The evaluation is done using the GLUE benchmark, which offers a variety of classification tasks for sentence encoders. The shared encoder is evaluated by freezing weights and comparing performance on multiple tasks. The evaluation of the shared encoder involves training target-task models on representations produced by the encoder for nine tasks in the GLUE benchmark. Different approaches are used for single-sentence and sentence-pair tasks, including linear projection, max-pooling, and heuristic matching. Attention mechanisms and BiLSTM are utilized for sentence-pair tasks like MNLI, QNLI, QQP, and STS. The text discusses the pretraining tasks for GLUE tasks, using an architecture without attention mechanism to improve cross-task transfer performance. Different architectures are used for sentence pair classification tasks and sequence-to-sequence pretraining tasks. Multitask pretraining involves all GLUE tasks, all outside pretraining tasks, and all. The text discusses multitask pretraining for various tasks, including GLUE tasks and outside pretraining tasks. ELMo representations are excluded from language modeling in multitask runs. Tasks are randomly sampled during multitask learning to balance data sizes. Early stopping is based on an average of pretraining tasks' validation metrics. Models are trained with the AMSGrad optimizer and early stopping is performed based on dev set performances. The experiments involved pretraining one encoder and training nine target-task models, taking 1-5 days on an NVIDIA P100 GPU. Hyperparameter tuning was limited due to resource constraints, with commonly-used values chosen. Results on the GLUE dev set for pretrained encoders with ELMo BiLSTM layers are shown in RESULTS TAB0. N/A baselines are untrained encoders, while Single-Task baselines aggregate results from nine GLUE runs. The Single-Task baselines are results from nine GLUE runs using pretrained encoders specific to each task. Only three pretrained encoders were evaluated on test data due to GLUE's restrictions. Comparable results can be found in prior work, but are not included here due to space limitations. The best test result using a frozen pretrained encoder is 68.9, similar to our GLUE E multitask model. The best overall result for a model similar to our GLUE E multitask model is 72.8, achieved by fine-tuning the model entirely for each target task. Variance of the GLUE score was estimated by re-running setups with different random seeds, showing substantial but not meaningless variation. Only one model reached the most frequent class performance of 56.3 on the explicitly adversarial WNLI dataset. The CoLA task benefits significantly from ELMo pretraining, while the STS benchmark sees good results. The STS benchmark shows good results with various pretraining methods, but does not benefit much from ELMo. Language modeling performs best among pretraining tasks, followed by MNLI. Adding ELMo improves performance across all tasks, with MNLI and English-German translation performing the best. Multitask models with ELMo perform the best overall, but models trained on individual tasks also perform well. The multitask model generalizes better than the single-task model on test data for tasks like STS. The multitask model generalizes better than the single-task model on test data for tasks like STS where the test set contains new out-of-domain data. TAB1 presents correlations between pairs of tasks over pretrained encoders, showing that different tasks benefit from different forms of pretraining. Models that overfit the WNLI training set tend to perform best overall, with negative correlations between WNLI and overall GLUE score. STS also shows a negative correlation, while CoLA has a strong correlation with overall GLUE scores. ELMo or LM pretraining improves CoLA performance significantly. The use of ELMo or LM pretraining significantly enhances CoLA performance, while most other pretraining methods have minimal impact. Different pretraining tasks show varying effects on overall GLUE metric and target task performance. Combining ELMo with other pretraining tasks like LM and MT yields promising results. Increasing data quantities benefit all target tasks without obvious diminishing returns. The study compares the benefits of pretraining tasks for sentence-level BiLSTM encoders like ELMo and CoVe. Results show that increasing data quantities improve performance across tasks, with pretraining tasks like language modeling being the most effective. Multitask pretraining can outperform single tasks and set new benchmarks. The study compares the benefits of pretraining tasks for sentence-level BiLSTM encoders like ELMo and CoVe. Results suggest that while language model data improves performance, the pretrain-and-freeze paradigm may not be ideal for future work. Different tasks benefit from different forms of pretraining, indicating limitations in current methods for producing truly general-purpose sentence encoders. Further work on language modeling is deemed worthwhile. The author(s) believe that future work on language modeling will require a better understanding of how neural network target task models can benefit from outside knowledge and data, as well as new methods for pretraining and transfer learning. The study also discusses extracting discourse model examples from the WikiText-103 corpus and the Reddit classification task. Additionally, the paper acknowledges the limitations of large-scale comparisons in NLP datasets. The study discusses the challenges in reaching development-set performance on various NLP datasets and outlines the validation process and optimizer used during training. Learning rate adjustments and decay are also detailed for different tasks. During training, the learning rate is reduced by 0.5 if validation performance does not improve for more than 4 checks, stopping if it falls below 1e-6. Early stopping is implemented after 20 validation checks without improvement. Dropout is applied at various layers, with an increased rate for small-data tasks. Preprocessing includes tokenizer selection and sequence length limits. Different tokenization methods are used for text generation and translation tasks, with word embeddings trained on the decoder side for sequence-to-sequence tasks. Target-task-specific parameters are adjusted accordingly. During training, the learning rate is adjusted based on validation performance, with early stopping implemented. Dropout rates vary for different tasks, and preprocessing involves tokenizer selection and sequence length limits. Word embeddings are trained on the decoder side for sequence-to-sequence tasks, with target-task-specific parameters adjusted accordingly. Attention is utilized for some pretraining tasks but not for others, with different model specifications for tasks of varying sizes. Our multitask learning experiments involve mixing tasks with varying amounts of training data and optimizing the shared encoder quality. We experiment with sampling tasks during training and controlling over-and underfitting using hyperparameters. Little work has been done on this problem, so we conduct a small experiment here. During multitask learning experiments, tasks are sampled based on data availability and loss is scaled at each update. Experiments are conducted on different subsets of tasks, showing no significant performance difference between various sampling and scaling methods. Power 0.75 task sampling and uniform loss scaling are used in the final multitask setup. In multitask learning experiments, power 0.75 task sampling and uniform loss scaling are used. Results on GLUE diagnostic set categories show that ELMo and unsupervised pretraining are beneficial for world knowledge and lexical-semantic knowledge tasks, but less effective for tasks involving complex logical reasoning or sentence structure."
}