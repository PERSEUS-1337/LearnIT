{
    "title": "rkxoNnC5FQ",
    "content": "Deep Learning for Computer Vision relies on supervision sources. SPIGAN is an unsupervised domain adaptation algorithm using Simulator Privileged Information and Generative Adversarial Networks. It improves performance in semantic segmentation by training on real-world and synthetic data. Reducing human supervision is a challenge in Machine Learning, especially in Computer Vision tasks like labeling images and videos. Training in simulation has shown advancements in various tasks. In contrast to supervised domain adaptation, recent unsupervised domain adaptation algorithms utilize Generative Adversarial Networks (GANs) for pixel-level adaptation. These methods leverage simulators as generators of training samples, incorporating Privileged Information (PI) not available in the real world. This additional information enhances learning in computer vision tasks like semantic segmentation. The paper proposes SPIGAN, an adversarial learning algorithm that leverages Simulator PI for GAN-based unsupervised learning. It involves learning four networks: a generator G, a discriminator D, a task network T, and a privileged network P. The main contribution is the method to leverage PI from a simulator via the privileged network P, acting as an auxiliary task and regularizer. The approach is evaluated on semantic segmentation in urban scenes using real-world data from Cityscapes BID6 and Vistas BID33 datasets and SYNTHIA BID39 as simulator output. The paper introduces SPIGAN, an algorithm that utilizes simulator privileged information for unsupervised learning. It focuses on semantic segmentation tasks and outperforms existing unsupervised domain adaptation methods. The approach involves learning a semantic segmentation network T without real-world labels, bridging the sim-to-real gap. The rest of the paper is organized into sections discussing related works, the SPIGAN algorithm, quantitative experiments, and conclusions. The problem involves learning a model for semantic segmentation in a target domain by combining unlabeled data from the target domain with labeled data from a related source domain. The main challenge is overcoming the domain gap without supervision from the target domain. Different approaches like DANN and Curriculum Domain Adaptation have been successful in reducing the domain gap. Adversarial domain adaptation techniques based on GANs have also shown promising results for unsupervised domain adaptation at the pixel level. CycleGAN and Variational Auto-Encoders (VAEs) are used for image-to-image translation, with GAN-based unsupervised domain adaptation methods addressing domain gaps between synthetic and real-world images. SimGAN and Sadat leverage simulation and synthetic data effectively. SPIGAN learns from unlabeled real-world images and simulator outputs, utilizing four networks for generation and discrimination. SPIGAN utilizes four networks for image translation: a generator, discriminator, perception task network, and privileged network. Unlike previous methods, SPIGAN focuses on semantic segmentation as the task network and uses a curriculum learning approach to reduce domain gap. Other methods like PixelDA and BID4 also address domain adaptation but with different strategies. Our approach introduces Privileged Information from a simulator by incorporating a privileged network in our architecture. This significantly improves semantic segmentation in urban scenes, especially in challenging real-world conditions with a large sim-to-real domain gap. Inspired by Learning Using Privileged Information (LUPI), our method leverages additional data only available at training time for unsupervised domain adaptation. Our goal is to utilize privileged information from simulators for sim-to-real unsupervised domain adaptation in neural networks. This involves learning a model for perception tasks like semantic segmentation without ground truth data from the target domain. The source domain includes labeled synthetic images and Privileged Information (PI) to bridge the sim-to-real domain gap. The target domain has unlabeled images, while the source domain includes labeled synthetic images and Privileged Information (PI) from a simulator. The challenge is bridging the gap between the synthetic source domain and the real-world target domain for generalization without target supervision. The PI, such as depth or optical flow, is used to guide and constrain the training of the task network within a GAN framework called SPIGAN. The approach involves using a generator to transform images from a source domain to an adapted domain, aiming to match the target domain for improved task predictor accuracy. A discriminator distinguishes between adapted and real images in an adversarial game. The task network is trained on synthetic images to predict labels, while a privileged network predicts privileged information. During testing, only the task network is used for inference. The approach involves using a generator to transform images from a source domain to an adapted domain, aiming to match the target domain for improved task predictor accuracy. The main learning goal is to train a model that can correctly perform a perception task in the target real-world domain. All models are trained jointly to exploit available information and constrain the solution space. The joint learning objective includes a set of loss functions and domain-specific constraints related to the main prediction task. The optimization includes weights for adversarial loss, task prediction loss, PI regularization, and perceptual regularization. Instead of a standard adversarial loss, a least-squares based adversarial loss is used for stability and better image results. The text discusses the use of a generator to adapt images from a source domain to a target domain for improved task predictor accuracy. It mentions optimizing the task network's loss over synthetic images and their adapted versions, with a focus on label-preserving generators. Different loss functions are used for tasks like semantic segmentation, PI regularization, and perceptual regularization. The joint learning objective includes various loss functions and constraints to train models effectively. The text discusses optimizing the task network's loss over synthetic images for unsupervised domain adaptation in semantic segmentation. It utilizes a generator to adapt images from a source domain to a target domain, focusing on label-preserving generators and various loss functions. The joint learning objective includes optimizing parameters of the discriminator and generator alternately. The evaluation is done on the SYNTHIA dataset for urban scene simulation. The dataset SYNTHIA-RAND-CITYSCAPES is used for autonomous driving simulation with pixel-wise segmentation and depth labels. Cityscapes BID6 and Mapillary Vistas BID33 datasets are used for real-world domains. Cityscapes dataset has images from urban streets in Europe, while Mapillary Vistas has a wider variety of scenes. Evaluation is done on adaptation from SYNTHIA to Cityscapes with 16 classes. No labels from real-world domains are used during training. In the study by Hoffman et al., they conducted an ablation study on the adaptation from SYNTHIA to Cityscapes and Vistas using a common 7-category ontology. They used standard intersection-over-union (IoU) metrics and adapted models from CycleGAN and BID23. The generator consisted of two down-sampling convolution layers and nine ResNet blocks, while the discriminator was a PatchGAN network with 3 layers. They used the FCN8s architecture for the task predictor and privileged network, and implemented perceptual loss using a pre-trained VGG19 network. Hyperparameters were set using a coarse grid search on a small validation set. The hyper-parameters for the joint adversarial loss in the study were set to \u03b1 = 1, \u03b2 = 0.5, \u03b3 = 0.1, \u03b4 = 0.33 for the GAN, task, privileged, and perceptual objectives respectively. The GAN and task losses were identified as the most important factors in the objective, with regularization terms playing a secondary role. The stopping criterion was highlighted as a critical hyper-parameter for unsupervised learning. The stopping criterion for early stopping in adversarial learning is based on the discriminator loss being consistently better than the generator loss. Evaluation is done at two resolutions: 320 \u00d7 640 and 512 \u00d7 1024. Images are resized during training and evaluation, with different crop sizes used for lower and higher resolutions. The Adam optimizer is used with an initial learning rate of 0.0002 in the PyTorch implementation. In the context of adapting a semantic segmentation network from SYNTHIA to Cityscapes, the SPIGAN algorithm utilizes depth maps from SYNTHIA as PI. Results show that SPIGAN achieves state-of-the-art semantic segmentation adaptation in terms of mean IoU on Cityscapes, outperforming other domain adaptation algorithms. The SPIGAN algorithm utilizes depth maps from SYNTHIA as PI to improve semantic segmentation adaptation on Cityscapes, achieving a 3% improvement in mean IoU. The use of PI helps estimate layout and object-related classes, reducing artifacts during training. This approach leverages synthetic data and PI to bridge the sim-to-real domain gap, confirming its effectiveness in improving generalization performance. Further experiments compare SPIGAN with PI, SPIGAN without PI, and SPIGAN without both PI and perceptual regularization, showing the impact of PI on performance. The SPIGAN algorithm with perceptual regularization improves semantic segmentation adaptation on Cityscapes and Vistas datasets. Results show SPIGAN with perceptual regularization outperforms SPIGAN-base, providing better adaptation in all categories. For Cityscapes, SPIGAN improves mean IoU by 17.1%, with perceptual regularization contributing 7.4% improvement. SPIGAN with perceptual regularization shows significant improvements in various categories, including \"nature\", \"construction\", \"vehicle\", and \"human\". The adaptation from SYNTHIA to Cityscapes and Vistas datasets demonstrates a decrease in domain gap and improved generalization performance. The use of perceptual regularization is crucial for enhancing adaptation, as shown by the negative transfer in SPIGAN-no-PI case. The difference in visual diversity between Cityscapes and Vistas datasets impacts the adaptation results. Cityscapes and Vistas datasets have different visual diversity, impacting adaptation results. SPIGAN with perceptual regularization shows improvements in various categories. Adaptation from SYNTHIA to Cityscapes and Vistas reduces domain gap. Perceptual regularization is crucial for enhancing adaptation, as seen in SPIGAN-no-PI case. Domain gap is addressed by consistent depth in SPIGAN results. PI guides training to reduce domain shift effectively. Unsupervised adaptation methods share similarities across datasets. SPIGAN is a novel method for unsupervised domain adaptation that leverages synthetic data and Privileged Information (PI) to bridge domain gaps in deep networks. It focuses on improving adaptation in various categories, with a notable improvement in the \"vehicle\" category for Cityscapes and Vistas datasets. The method addresses challenges in adapting to real-world domains, such as semantic segmentation of urban scenes, by jointly learning a generative adaptation network and target task network. Future work includes exploring SPIGAN for additional tasks with different types of PI. SPIGAN is being explored for additional tasks using different types of Privileged Information obtained from simulation."
}