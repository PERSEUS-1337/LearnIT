{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP represents words with single-mode embeddings, but a new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets and outperform baselines on various NLP tasks. The curr_chunk discusses the limitations of single embedding models in capturing multiple senses of words and topics in sentences. It introduces word sense induction methods and multi-mode word embeddings to address this issue, illustrating with the example of real property having different meanings. The curr_chunk discusses the challenges of extending multi-mode representations to phrases or sentences due to efficiency issues. It highlights the difficulty of clustering unique sequences and the large number of parameters required for clustering-based approaches. The curr_chunk introduces a compositional model that predicts cluster centers from a sequence of words in a target phrase to reconstruct co-occurring distributions efficiently. It addresses the challenge of sparseness in co-occurring statistics and overfitting in clustering approaches by using a neural encoder and decoder. Unlike previous methods, the model learns a mapping between the target sequence and cluster centers instead of clustering co-occurring words at test time. The proposed model predicts cluster centers from a sequence of words in a target phrase to capture compositional meanings better than traditional methods. It uses a nonnegative and sparse coefficient matrix to match predicted cluster centers with observed word embeddings during training, allowing for better unsupervised phrase similarity tasks and measuring asymmetric relations like hypernymy. The model outperforms single-mode alternatives and trains the whole model jointly and end-to-end. The multimode representation outperforms single-mode alternatives in sentence representation, as shown in an extractive summarization experiment. The training setup, objective function, and architecture of the prediction mode are formalized in Sections 2.1, 2.2, and 2.3, respectively. The model represents each sentence with multiple codebook embeddings predicted by a sequence to embeddings model, encouraging the generation of embeddings that can reconstruct co-occurring words while avoiding common topics. The training signal for sentence and phrase representation involves reconstructing neighboring words in a fixed window size. Different models are needed for phrases and sentences. The goal is to cluster words that could potentially occur beside the sequence, focusing on semantics rather than syntax. Learning to predict these words is done by observing co-occurring words from similar sequences. In this work, the model considers word order information in the input sequence but ignores the order of co-occurring words. Co-occurring words are modeled in a pre-trained word embedding space, arranged into a matrix. The predicted cluster centers of the input sequence are represented as a matrix, with a fixed number of clusters to simplify the prediction model design. The reconstruction loss of k-means clustering in the word embedding space is discussed. The curr_chunk discusses the use of non-negative sparse coding (NNSC) to improve clustering in the word embedding space. NNSC allows for more diverse cluster centers compared to kmeans, which tends to collapse to fewer modes. The smoother NNSC loss is easier to optimize for neural networks, leading to better reconstruction of co-occurring words. The curr_chunk discusses the efficient estimation of M Ot using convex optimization and back-propagation in neural networks for end-to-end training. The loss function for each sequence is defined to prevent the network from predicting the same global topics. The method is a generalization of Word2Vec. Our method is a generalization of Word2Vec that encodes compositional meaning and decodes multiple embeddings. The neural network architecture is similar to a transformation-based seq2seq model. The encoder maps sentences with similar word distribution closer together. Unlike typical seq2seq models, our decoder outputs a sequence of embeddings instead of words, allowing for predicting all codebook embeddings in a single pass. Different linear layers are used to capture different aspects of the embeddings. The encoder in the model utilizes linear layers before feeding into the decoder. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation. The flexibility of the framework allows for the replacement of encoder and decoder with other architectures, such as (bi-)LSTMs. Visualizing cluster centers predicted by the model helps in summarizing the target sequence effectively. More codebook embeddings capture additional semantic facets of a phrase or sentence. The curr_chunk discusses the use of pre-trained GloVe embeddings for sentence and phrase representation in unsupervised semantic tasks. The model is trained on Wikipedia 2016 with stop words removed, and only considers noun phrases in experiments. The models do not require additional resources like PPDB and are compared with baselines using raw text. This approach is practical for domains with low resources, such as scientific literature. The curr_chunk discusses the limitations of training models with low computational resources compared to BERT. BERT, trained on a masked language modeling loss, outperforms models with smaller sizes and less training data. The performance of BERT is still superior due to more parameters, output dimensions, and computational resources used during training. The curr_chunk discusses benchmarks for evaluating phrase similarity, including BiRD and WikiSRS. Semeval 2013 task involves distinguishing similar phrase pairs. Turney's method identifies the most similar unigram to a query bigram. Two scoring functions measure phrase similarity using transformer encoder embeddings. The curr_chunk introduces a symmetric distance SC for comparing phrase embeddings and evaluates its performance against 5 baselines including GloVe Avg, Word2Vec Avg, BERT CLS, BERT Avg, and FCT LM Emb. Results show that the proposed model significantly outperforms all baselines in 4 datasets, with strong performances in Turney dataset confirming the effectiveness of the encoder in incorporating word order information. The results show that non-linearly composing word embeddings in order to predict co-occurring word embeddings is effective. Our model outperforms baselines in 4 datasets, with strong performances in Turney dataset. The performance of Ours (K=1) is slightly better than Ours (K=10), supporting the finding that multi-mode embeddings may not improve word similarity benchmarks. The similarity performance is not sensitive to the number of clusters, alleviating the problem of selecting K in practice. STS benchmark is a widely used sentence similarity task where models predict semantic similarity scores between sentence pairs. In addition to comparing performances on lower half datasets, the benchmark STSB Low is introduced. Various methods like word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos) are compared. Weighting words in sentences according to a proposed method by Arora et al. (2017) is also explored. Post-processing methods like GloVe SIF and GloVe Prob_avg are discussed, with a focus on the strong performance of average embeddings. The study suggests considering word embeddings along with sentence embeddings for measuring sentence similarity. Multi-facet embeddings allow for estimating word importance by computing cosine similarity with predicted codebook embeddings. Importance weighting is applied to different GloVe weighting vectors to generate results for Our Avg, Our Prob_avg, and Our SIF. Results are compared in the development and test sets. The study compares different methods for measuring sentence similarity, with Ours SC outperforming WMD and BERT Avg, especially in STSB Low. Multi-mode representation with attention weighting boosts performance, and a variant using bi-LSTM and LSTM performs worse than the transformer alternative. The model is applied to HypeNet for hypernymy detection. The study compares different methods for measuring sentence similarity, with Ours SC outperforming WMD and BERT Avg, especially in STSB Low. Multi-mode representation with attention weighting boosts performance, and a variant using bi-LSTM and LSTM performs worse than the transformer alternative. The model is applied to HypeNet for hypernymy detection, where asymmetric scoring function is used to detect hypernyms better than baselines. Our methods show improved performance in detecting hypernyms compared to symmetric similarity measurement. The extractive summarization method described in the curr_chunk optimizes sentence embeddings by selecting sentences based on importance. Multiple codebook embeddings are generated to represent different aspects of each sentence. The approach is compared with other methods such as average word embeddings and using all words in the sentences as aspects. The gain of the reconstruction loss is normalized by sentence length for longer sentences. The method W Emb normalizes the gain of the reconstruction loss by sentence length and uses a fixed number of codebook embeddings to avoid problems. Baselines like selecting random sentences (Rnd) and first n sentences (Lead) are tested on the CNN/Daily Mail dataset. Results are compared using ROUGE F1 scores, with methods choosing 3 sentences. Unsupervised methods like Lead-3 are strong baselines, with performances similar to supervised methods like RL. The focus is on evaluating the quality of unsupervised sentence embeddings that do not assume the first few sentences form a good summary. Our method allows for setting a large cluster number K to improve performance in discovering different sets of topics efficiently. Neural networks have been shown to discover coherent topics, but our approach focuses on discovering topics on small word subsets. Sparse coding and neural networks are used to model multiple aspects of a word. The challenges of extending methods for parameterizing word embeddings using neural networks to longer sequences are not addressed in previous studies. One main challenge is designing a neural decoder for sets rather than sequences, requiring a matching step between sets and computing distance loss. Various matching loss options, such as Chamfer distance, are proposed to measure symmetric distances between ground truth and predicted sets. The goal is to reconstruct a set using a set of different sizes. The current work focuses on efficiently predicting clustering centers to reconstruct observed instances, overcoming challenges in learning multi-mode representations for long sequences like phrases or sentences. A neural encoder models the target sequence's meaning, while a neural decoder predicts codebook embeddings as sentence or phrase representations. Training involves using a sparse coefficient matrix to match predicted embeddings to observed words. The proposed models can predict interpretable clustering centers based on an input sequence, outperforming BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings excel with complex input sequences, while both multi-facet and single-facet embeddings perform similarly well with simpler inputs. Future plans include training a single model for generating multi-facet embeddings for both phrases and sentences, evaluating it as a pre-trained embedding approach for supervised or semi-supervised settings, and applying the method to other unsupervised learning tasks. The model is kept simple to converge training loss quickly, without fine-tuning hyper-parameters. A smaller transformer model is used with similar architecture to BERT. Sparsity penalty weight is set at 0.4, sentence size at 50, and co-occurring words limited to 30. Transformer dimensions are 300, with different settings for sentence and phrase representation. The model uses a smaller transformer with a sparsity penalty weight of 0.4 and sentence size of 50. The number of codebook embeddings is determined by training performance, with K chosen based on task performance. The model has fewer parameters than BERT base and requires less computational resources for training. BERT Large outperforms BERT Base in similarity tasks but performs worse in hypernym detection. Increasing model size boosts BERT performance, but the method is still superior in most cases, especially in phrase similarity tasks. Comparison with other baselines suggests that shorter sentences may impact performance. The study compares the performance of different unsupervised summarization methods based on sentence length. The results show that shorter sentences may impact performance, with BERT being the best choice for longer summaries. The study suggests that combining their method with BERT could lead to the best performance in summarization tasks. Visualizations of predicted embeddings from sentences in the validation set are provided in a format similar to Table 1."
}