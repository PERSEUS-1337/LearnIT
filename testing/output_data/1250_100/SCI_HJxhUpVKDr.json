{
    "title": "HJxhUpVKDr",
    "content": "In the context of multi-task learning, neural networks with branched architectures are used to jointly tackle tasks. These networks start with shared layers and then branch out into task-specific layers. Prior methods for determining layer sharing have been ad hoc or expensive. This paper proposes a principled approach to automatically construct branched multi-task networks based on task affinities. The approach generates architectures with task-agnostic shallow layers and task-specific deeper layers within a specific budget. Experimental analysis shows that this method consistently produces high-performing networks for various multi-tasking datasets. The curr_chunk discusses the advantages of multi-task networks in deep learning, which aim to improve generalization and processing efficiency by jointly learning related tasks. Inspired by the brain's multi-tasking strategy, these networks share early processing layers to reduce memory footprint. Compared to separate deep neural networks for individual tasks, multi-task networks offer benefits such as improved performance and parameter efficiency. Multi-task networks in deep learning share early processing layers to reduce memory footprint and improve inference speed. They may outperform single-task networks and offer benefits like improved performance and parameter efficiency. Designing multi-task networks poses a challenge in deciding on shared layers, with researchers exploring alternatives like routing to optimize architecture efficiently. In this paper, researchers propose a novel approach to determine the degree of layer sharing between tasks in multi-task networks. They base the layer sharing on measurable levels of task affinity or relatedness, aiming to eliminate the need for manual exploration and improve performance by avoiding negative transfer of information between unrelated tasks. The researchers propose a more efficient approach using representation similarity analysis (RSA) to measure task affinity in a neural network. Their method automatically constructs a branched multitask network by grouping similar tasks together and separating dissimilar tasks to reduce negative transfer. Extensive empirical evaluation shows the superiority of their approach in terms of multi-task performance and computational resources. Multi-task learning involves jointly learning multiple tasks under a single model, with early approaches relying on sparsity constraints to select shared features. Clustering tasks based on similarity can prevent negative transfer. In deep learning, MTL models use soft or hard parameter sharing mechanisms to handle cross-task communication. Cross-stitch networks and sluice networks are examples of soft parameter sharing methods. Multi-task learning networks utilize soft or hard parameter sharing mechanisms for cross-task communication. Sluice networks extend cross-stitch networks, allowing selective sharing of layers and skip connections. Hard parameter sharing divides parameters into shared and task-specific sets, often using a shared encoder and task-specific decoders. Multilinear relationship networks extend this by placing tensor normal priors on fully connected layers. Guo et al. proposed a hierarchical network for predicting increasingly difficult tasks. Branching points in existing approaches are determined ad hoc, leading to limitations. Our branched multi-task networks automatically determine the degree of layer sharing based on task affinities, unlike ad hoc approaches. Task groupings are decided based on feature affinity scores, not example difficulty, and the tree structure is determined offline for optimal global task groupings. This approach achieves significantly better results, especially on challenging datasets like Taskonomy. Neural architecture search (NAS) aims to automate network architecture construction, with different algorithms characterized by search space, strategy, and performance estimation. Most NAS works focus on task-specific models due to the complexity of jointly optimizing layer sharing with layer types and connectivity. To reduce computation burden, recent studies explore evolutionary architecture search, routing, stochastic filter grouping, and feature partitioning for multi-task networks. These methods do not start architecture from scratch but use a predefined backbone network with an automatically determined layer sharing scheme. Transfer learning is used to measure levels of task affinity by computing correlations between models pretrained on different tasks. Loss weighting is a challenge in jointly learning multiple tasks, with methods like homoscedastic uncertainty and gradient normalization being used to balance the learning of tasks. In this paper, the authors propose a method to jointly solve N different tasks given a computational budget, using a shared encoder-decoder architecture with task-specific layers. The method aims to dynamically adapt gradient magnitudes, prioritize difficult tasks, alleviate destructive interference, and find a Pareto optimal solution in multi-task learning. The proposed method aims to group related tasks together in the same branches of the tree by deriving task affinity scores at various locations in the encoder. These scores are used to construct a branched multi-task network within a computational budget. The pipeline involves using RSA to measure task affinity and calculating correlation between feature matrices to create an affinity tensor. The output is a branched multi-task network similar to NAS. The proposed method groups related tasks in the same branches of a branched multi-task network by deriving task affinity scores at encoder locations. Task affinity is measured using RSA, assigning tasks to branches based on affinity levels within a computational budget. The proposed method groups related tasks in the same branches of a branched multi-task network by deriving task affinity scores at encoder locations. Task affinity is measured using RSA, assigning tasks to branches based on affinity levels within a computational budget. To calculate task affinity scores, single-task models are trained for each task using a shared encoder and task-specific decoder. The decoder contains task-specific operations and is smaller in size compared to the encoder. Task affinities are calculated at selected locations in the encoder to create a three-dimensional tensor holding the task affinities. The method groups related tasks in branches of a multi-task network by deriving task affinity scores at encoder locations. Task affinities are calculated by comparing representation dissimilarity matrices of single-task networks at specified locations using a subset of images. Dissimilarity scores between feature representations are computed using the Pearson correlation coefficient. Similarity between RDMs of different single-task networks is measured using Spearman's correlation coefficient. The method focuses on deriving task affinity scores in a multi-task network by comparing feature representations of single-task networks at specific locations. It uses Spearman's correlation coefficient to measure similarity and constructs a task affinity tensor. The sharing of layers in the sharable encoder is determined based on a computational budget, with each layer represented as a node in a tree structure. The branched multi-task network is built to separate dissimilar tasks by assigning them to separate branches. Task affinity scores are calculated a priori to determine task clustering offline. The tree with the lowest task dissimilarity score is selected based on a computational budget. The proposed method aims to find an optimal task grouping by maximizing the distance between dissimilarity scores in each cluster. A top-down approach is used to derive the tree structure, with spectral clustering performed at each step to select the top-n task groupings with minimal cost. This beam search strategy is utilized in CelebA experiments to evaluate the effectiveness of the method. In this section, the proposed method is evaluated on diverse multi-tasking datasets like Cityscapes, focusing on urban scene understanding with tasks such as semantic segmentation, instance segmentation, and monocular depth estimation using a ResNet-50 encoder. Results are obtained after a grid search on hyperparameters for fair comparison. The task affinity decreases in deeper layers of the ResNet-50 model, with features becoming more task-specific. Performance of task groupings generated by our method is compared with other approaches, visualized in Fig. 2b. Our method selects task groupings based on computational budget, outperforming task groupings based on task affinity measure proposed by Lu et al. (2017). The proposed method selects task groupings based on computational budget, achieving higher performance compared to other approaches. It can effectively sample architectures between baseline multi-task models and more complex ones like cross-stitch networks. Soft parameter sharing becomes less scalable as the number of tasks increases greatly. The Taskonomy dataset contains images annotated for various tasks such as scene categorization, semantic segmentation, edge detection, and monocular depth estimation. The study focuses on task groupings for various image annotation tasks like scene categorization, semantic segmentation, edge detection, monocular depth estimation, and keypoint detection. The dataset used contains 275k train, 52k validation, and 54k test images. The architecture and training setup are based on ResNet-50, with a fully convolutional decoder for pixel-to-pixel prediction tasks. Task affinity is measured after each ResNet block, and the performance of different task groupings is compared against a previous method. The results are detailed in Table 2. Our models consistently outperform FA models in various image annotation tasks. Multi-task performance is influenced by task groupings, with negative transfer affecting performance. Branched multi-task networks handle diverse tasks positively, unlike cross-stitch networks and NDDR-CNNS. Our approach shows consistent performance across different multi-tasking scenarios and datasets, separating dissimilar tasks to limit negative transfer. Table 3 shows quantitative analysis on the CelebA test set, comparing different architectures in terms of accuracy and parameters. Our models demonstrate stable performance across various experimental setups, outperforming other models in image annotation tasks. The CelebA dataset contains over 200k real images of celebrities labeled with 40 facial attribute categories. The training, validation, and test sets have 160k, 20k, and 20k images respectively. The prediction of each facial attribute is treated as a single binary classification task. The study uses a branched multi-task network that outperforms earlier works on the CelebA test set. Our Thin-32 model outperforms previous works on CelebA attribute classification tasks, matching VGG-16's performance with significantly fewer parameters. Additionally, our Thin-64 model performs better than ResNet-18 with a uniform loss weighing scheme and matches its performance with a reduced parameter count. This paper introduces a method to construct branched multi-task networks efficiently by leveraging task affinities for layer sharing, akin to NAS for MTL. The proposed approach focuses on layer sharing optimization for multi-task learning, outperforming existing methods in terms of multi-tasking performance vs number of parameters. An attempt to re-implement the MTAN model using a ResNet-50 backbone did not yield meaningful results on the Cityscapes dataset for all three tasks jointly. The setup involved rescaling input images to 256 x 256 pixels and using a ResNet-50 encoder with modifications in the decoder architecture. The decoder in the proposed approach consists of five convolutional layers with alternating convolutional and transposed convolutional layers. ReLU is used as non-linearity, and batch normalization is applied in every layer except the output layer. Kaiming He's initialization is used for both encoder and decoder. Different loss functions are used for depth, edge detection, keypoint detection, and scene categorization tasks. The multi-task models are optimized with specific task weights. Heatmaps are linearly rescaled, and the depth map is normalized during training. Single-task models use an Adam optimizer with a specific learning rate schedule and batch size. No additional data augmentation is applied during training. The batch size is set to 32 for 120000 iterations with a weight decay term of 1e-4. Multi-task models use the same optimization procedure as single-task models. Branched multi-task models show architectures generated by the method. Cross-stitch networks reuse hyperparameter settings and the VGG-16 model for the CNN architecture. The branched multi-task network is trained with a minimum number of convolutional features and 2 \u00b7 \u03c9 features in fully connected layers. Stochastic gradient descent with momentum 0.9 and initial learning rate 0.05 is used for training with batches of size 32 and weight decay 0.0001. The model is trained for 120000 iterations with the learning rate decreasing by a factor of 10 every 40000 iterations, using a sigmoid cross-entropy loss function with uniform weighing scheme."
}