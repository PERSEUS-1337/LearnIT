{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework using spelling error correction, tokenization, and neural models was proposed to standardize non-systematic names to systematic names, achieving 54.04% accuracy on the test dataset. There are over 100 million named chemical substances worldwide, each assigned systematic names based on their structures. Chemical substances can have various names, including systematic names defined by IUPAC, common names for everyday use, and proprietary names in the industry. For example, sucrose is commonly known as sugar, while Aspirin's systematic name is 2-Acetoxybenzoic acid. Different names may exist due to historical reasons, idiomatic usage, and competition among producers. Chemical information extraction involves converting text into a database using standard chemical names. Various databases like PubChem and SciFinder store chemical information such as names, structures, and formulas. Extracting information from chemical papers to update these databases is ongoing work. Converting systematic names to representations like SMILES and InCHI can generate structural formulas efficiently. OPSIN is a system that can automatically convert systematic names to SMILES with high precision. In natural language processing, non-systematic names can have four types of errors: spelling, ordering, common name, and synonym errors. These errors can occur simultaneously in a single non-systematic name, making the task challenging. The task of converting non-systematic chemical names to systematic names is challenging due to spelling, ordering, common name, and synonym errors. A framework is proposed to address these errors, including spelling error correction, BPE tokenization, and a sequence to sequence model. Previous work like ChemHits by BID2 relied heavily on chemical knowledge, while this framework utilizes a sequence to sequence model commonly used in neural machine translation. Our framework for converting non-systematic chemical names to systematic names is trained end-to-end without external chemical knowledge. It achieves 54.04% accuracy on the test dataset from a corpus of chemical names extracted from high-impact Chemical Journals. The corpus contains 384,816 data pairs of non-systematic and systematic names, with an overview of the Levenshtein distance distribution shown in FIG1. In the experiment, different datasets are used for training, testing, and development. The focus is on correcting spelling errors in chemical names by separating them into elemental words. Two vocabularies are created: one for systematic elemental words and one for non-systematic elemental words. The non-systematic names vocabulary consists of common names or synonyms. The experiment focuses on correcting spelling errors in chemical names by separating them into elemental words. Two vocabularies are created: one for systematic elemental words and one for non-systematic elemental words, which consist of common names or synonyms. The BK-Tree structure is used to efficiently correct spelling errors in non-systematic names by finding the vocabulary item with the smallest Levenshtein distance. This method is scalable and allows for easy insertion of new training data. To correct spelling errors in chemical names, the experiment separates them into elemental words and uses a BK-Tree structure. This method efficiently corrects non-systematic names by finding the closest match in the vocabulary. The sequence-to-sequence model is trained using tokenized chemical names with Byte Pair Encoding (BPE) BID11. Pair Encoding (BPE) BID11 is used for tokenization by initializing a symbol set with single characters, then iteratively counting symbol pairs to create new symbols. BPE helps with out-of-vocabulary problems and separates names into meaningful subwords. The tokenized pairs are used to train a sequence-to-sequence model, which is widely used in machine translation. The sequence model BID12 is adapted from OpenNMT BID5 for machine translation. It consists of an encoder with a BiLSTM for source sequences and a decoder for target sequences. The context vector H is generated by the encoder, and the decoder calculates the probability of output sequences. The spelling error correction stage only requires adjusting the threshold parameter. In the spelling error correction stage, the only parameter is the threshold of the BK-Tree. Different threshold values were tested: 1, 2, and 3. At the BPE stage, the number of merge operations is the only parameter, with values tested at 2500, 5000, 10000, and 20000. For the sequence to sequence model, word embeddings and hidden states dimensions are both 500. The vocabulary size is determined by basic characters and BPE merge operations. Encoder and decoder both have 2 layers. Parameters are trained jointly using SGD with a cross-entropy loss function. The decay factor of 0.5 is applied every epoch from epoch 8 onwards or when perplexity stops decreasing on the validation set. The dropout rate is 0.3, and the model is trained for 15 epochs with a beam size of 5 for decoding. An experiment using Statistical Machine Translation (SMT) model with Moses system and KenLM for language modeling is also conducted. Data augmentation techniques, including spelling error correction and error insertion, are explored to improve neural model learning with noisy data. In the experiment, different insertion methods are randomly applied to two characters, with accuracy and BLEU score used to measure standardization quality. Results show that a combination of spelling error correction, BPE tokenization, and sequence to sequence model performs best. The framework outperforms SMT model and ChemHits system, with 5000 being the optimal value for BPE merge operation. The results show that spelling error correction and data augmentation are beneficial for the framework. Overcorrection may occur with large thresholds, reducing standardization quality. Examples in Table 6 demonstrate the capabilities of the sequence to sequence model in correcting non-systematic names, including fixing non-alphabet spelling errors and correcting synonym errors. The sequence to sequence model successfully standardizes non-systematic chemical names, correcting common name and ordering errors. Visualization in FIG2 illustrates the model's ability to find relations and correct errors. Failed standardization attempts are analyzed by manually labeling error types from 100 samples. The distribution of error types in failed standardization attempts is shown in TAB6. Synonym errors are the most common, while spelling errors are handled well by the system. Common errors are challenging due to the lack of rules for unseen names. The accuracy of systematic names varies based on length, with the best performance for names between 20 and 40 characters. The model does not consider chemical rules, limiting its effectiveness. The model proposed in the study aims to automatically convert non-systematic chemical names to systematic names using spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. Despite achieving an accuracy of 54.04% on the dataset, some names generated by the model do not adhere to chemical rules, leading to inaccuracies in predictions. The new framework for chemical information extraction is significantly more accurate than previous rule-based systems, achieving nine times the accuracy. It is trained end-to-end, data-driven, and does not rely on external chemical knowledge. This work opens up a new research direction in the field of chemical information extraction."
}