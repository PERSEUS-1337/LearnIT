{
    "title": "SyjjD1WRb",
    "content": "We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables. Two models are used to investigate applicability: a noisy-OR Bayes Net and Binary Sparse Coding. Learning of generative models is formulated as approximate maximum likelihood optimization using variational expectation maximization. Truncated posteriors with discrete latent states are used as variational distributions. Evolutionary algorithms can be used for variational optimization by treating latent states as genomes and defining fitness based on joint probabilities from the generative model. The novel evolutionary EM approach is applied to optimize parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data. Scalable variational EM algorithms are obtained through point mutations and single-point cross-over, efficiently improving data likelihood. Evolutionary algorithms are introduced as tools for solving Machine Learning problems, leveraging standard and recent results in evolutionary optimization for parameter optimization in generative models. Evolutionary algorithms (EAs) are often used to solve specific sub-problems in tasks like classification with Deep Neural Networks. The combination of probabilistic generative models and expectation maximization (EM) approaches offers a general framework for parameter optimization, encompassing various algorithms from clustering to deep learning. However, for most generative data models, EM is computationally intractable and requires approximations like Variational EM. Variational EM is a prominent approximation method for solving optimization problems in high-dimensional spaces. It formulates latent states as variational parameters, where evolutionary algorithms (EAs) can be used for optimization. The generative model generates data points using hidden variables, and learning aims to adjust model parameters to match real data points through maximum likelihood estimation. The goal is to find maximum likelihood parameters for a generative model by maximizing the data log-likelihood. To efficiently approximate these parameters, variational distributions are used to approximate the intractable posterior distributions. This approach can be applied to any generative model with binary latent variables. Variational EM algorithm involves maximizing F(\u039b, \u0398) iteratively in the E-step and M-step. Truncated variational distributions are chosen for evolutionary algorithms, with populations of hidden states for data points. The expectation values are computed with respect to these distributions. The variational EM algorithm involves maximizing the free-energy iteratively in the E-step and M-step. Truncated distributions with populations of hidden states are used for data points, approximating expectations well. The free-energy takes a simplified form, and the E-step involves finding populations that maximize the joint probability of hidden states and data points. The variational EM algorithm maximizes the free-energy iteratively by increasing it, which is computationally easier than full maximization. By choosing a fitness function for evolutionary optimization, mutations that increase fitness will increase free-energy. The algorithm monotonously increases free-energy through M-step optimizations and efficient parent selection. The fitness function chosen enables efficient computation and numerical stability. The algorithm seeks to optimize a fitness function for evolutionary optimization by iteratively increasing free-energy through M-step optimizations and efficient parent selection. The fitness function chosen enables efficient computation and numerical stability, with the goal of sampling states proportionally to their fitness. The algorithm includes genetic operators such as parent selection, single-point crossover, and stochastic mutation to improve overall fitness over N g generations. The genetic algorithm involves parent selection, crossover, and mutation over N generations to optimize fitness. Parent selection aims to balance exploitation of high fitness parents and exploration of mutations for diversity. Different strategies like fitness-proportional and random uniform selection are explored. Crossover involves selecting random parent pairs and assigning a single crossover point. The genetic algorithm involves parent selection, crossover, and mutation over N generations to optimize fitness. Parent selection aims to balance exploitation of high fitness parents and exploration of mutations for diversity. Crossover involves selecting random parent pairs and assigning a single crossover point. The parents swap bits to produce offspring, which then undergo random bitflips for increased diversity. Different bitflip algorithms are compared for sparsity compatibility. If crossover is skipped, a different bitflip mutation is performed on copies of each parent. The evolutionary algorithm involves updating candidates and children using M-steps with K until F increases sufficiently. It produces new states and selects the fittest individuals as the new population. The algorithm never decreases free-energy and is used in an unsupervised learning algorithm for optimizing log-likelihood. The noisy-OR model is a non-linear data model with binary values and Bernoulli priors for latent variables. Binary Sparse Coding (BSC) is a model for continuous data with binary latent variables following a Bernoulli distribution. The observables are drawn from a Gaussian distribution, and the model parameters include weights and variance. M-step update rules for BSC can be derived by optimizing the free energy. Numerical experiments were conducted to test the model. In the context of Binary Sparse Coding (BSC), numerical experiments were conducted to test the applicability and scalability of Evolutionary EM (EEM) using different evolutionary algorithms. The experiments involved artificial data with known ground-truth components, such as the standard bars test with non-linear superposition and Gaussian noise. In the context of Binary Sparse Coding (BSC), Evolutionary EM (EEM) was tested using different evolutionary algorithms on artificial data with known ground-truth components. The experiments focused on the standard bars test with noisy-OR using various configurations of the EA. Reliability was used as a performance metric, showing that \"fitparents-sparseflips\" outperformed \"randparents-randflips\" on 8x8 images. The addition of crossover reduced the probability of finding all bars and led to an overestimation of crowdedness. Evolutionary EM (EEM) was tested on a bars data-set with sensible overlaps using noisy-OR as a generative model. Results showed that all bars were recovered in 13 out of 25 runs, demonstrating competitive performance without additional assumptions. EEM also performed well on the linear BSC model, solving standard bars tests reliably even with random bitflips. Evolutionary EM (EEM) was tested on a bars data-set with sensible overlaps using noisy-OR as a generative model. The task was made more challenging by increasing the dimensionality of the data, the number of components, and the average number of bars per data point. Results showed that basic approaches like random uniform selection of parents and bitflips worked well, but more sophisticated EAs improved performance. Sparseness-driven bitflips generally led to poor performance. The results show that sparseness-driven bitflips result in poor performance, even with different parent selection methods. This may be due to the initialization of K n, making it harder for sparseness-driven EAs to find solutions. The approach is then tested on natural data using patches of images from the van Hateren database. The image patches are raw and reflect light intensities without preprocessing. The study involved extracting random square subsections from an image of overlapping grass wires. Brightest pixels were removed, and data points were scaled to gray-scale values. Binary data points were created by sampling from a Bernoulli distribution. A non-linear generative model, noisy-OR, was applied due to expected non-linear superimposition in light-intensity images. Generative fields were learned using an evolutionary algorithm, showing curved edges. Pre-processed image patches were also considered for sparse coding approaches. The study involved extracting random square subsections from an image of overlapping grass wires. Brightest pixels were removed, and data points were scaled to gray-scale values. Binary data points were created by sampling from a Bernoulli distribution. A non-linear generative model, noisy-OR, was applied due to expected non-linear superimposition in light-intensity images. Generative fields were learned using an evolutionary algorithm, showing curved edges. Pre-processed image patches were also considered for sparse coding approaches. Sparse coding was applied to 100,000 patches of size 16x16, with the highest 2% of amplitudes clamped. ZCA whitening was used, retaining 95% of the variance. The BSC model was trained for 4,000 iterations with 300 hidden units and 200 variational states, resulting in generative fields primarily in the form of Gabor functions. More than five units were activated per data point, indicating the learned code's use of multiple causes structure. The training of generative models in Machine Learning often requires approximations, such as sampling or variational EM. Evolutionary algorithms have also been used in conjunction with EM, as seen in the application of EAs for clustering with Gaussian mixture models. EAs are used to select the best GMM models for the clustering problem, similar to DNN optimization where EAs optimize hyperparameters in an outer loop. Evolutionary algorithms (EAs) have been combined with Expectation-Maximization (EM) for training generative models. This integration addresses optimization challenges and establishes a theoretical link between EAs and EM. Basic EAs have shown potential in training models with large hidden spaces and local optima. Specialized EAs could further improve accuracy in future applications. In experiments, Evolutionary Algorithms (EAs) have been used to train generative models like noisy-OR and sparse coding models. This approach offers a novel mathematically grounded way to utilize EAs for generative models with binary latents. The update rule for \u03c0 is straightforward, but the weights W dh do not have a closed form solution. The update rule for the weights W dh does not have a closed form solution. Instead, a fixed-point equation is used to express each new W dh as a function of all current W, allowing for convergence towards the exact solution of the maximization step. This approach provides a simple mechanism to monitor and correct for any misbehaviors during training. The free energy for BSC can be derived by inserting equations and update rules can be obtained by optimizing the resulting expression separately for model parameters. Truncated variational approximations are used for higher latent dimensionality due to computational intractability of exact posteriors. With higher latent dimensionality, truncated variational distributions are used to approximate exact posteriors for BSC. The truncated free energy takes a specific form when performing sparsity-driven bitflips. Constraints are imposed on the probabilities of flipping each bit, with comparisons to other algorithms showing EMM for noisy-OR performs well but with higher reliability approaches available. Most approaches recovering more than 15 bars require additional assumptions. MCA 3 is a generative model without constraints, exploring sparse combinations with up to 3 components. It evaluates over 60000 states per data point per iteration with 32 latents. In comparison, EEM for noisy-OR evaluates around 100 states per data point per iteration. Generative fields learned with EEM for noisy-OR show a crowdedness of 1.6 with 200 latent variables."
}