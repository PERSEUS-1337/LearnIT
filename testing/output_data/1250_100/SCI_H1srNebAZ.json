{
    "title": "H1srNebAZ",
    "content": "Neural networks trained through stochastic gradient descent (SGD) have been around for over 30 years, but their inner workings remain elusive. This paper takes an experimental approach, focusing on the behavior of single neurons in deep neural networks. The experiments reveal that hidden neurons function as binary classifiers during training and testing, with consistent categorization of inputs. This sheds light on the internal mechanics of deep neural networks and has the potential to influence future theoretical and practical advancements. The curr_chunk discusses the intriguing behavior of deep neural networks, including their ability to serve as object detectors and be re-used on new tasks. It also mentions the limitations of deep neural networks, such as difficulties with continuous learning, robustness, and unsupervised learning. The text highlights the need for a better understanding of deep neural networks to address these challenges. The curr_chunk emphasizes the importance of experiments in understanding the mechanisms behind the success of neural networks, particularly focusing on hidden neurons and their interpretability in detecting semantically relevant concepts. It suggests that hidden neurons represent abstract concepts that evolve with layer depth, supported by evidence from studies on convolutional neural networks. This paper explores the interpretability of hidden neurons in neural networks, focusing on how they encode information. The main finding is that a neuron's encoding can be characterized by a binary classifier, with consistent behavior during training and partitioning inputs into two categories. Testing experiments show that this binary partition is maintained. The hidden neurons in neural networks exhibit a binary partition behavior that encodes core information for predictions. This behavior is consistent across different layers, networks, and problem scales. Previous works focused on interpreting neurons in terms of relevant concepts, while recent studies have visualized how neurons are activated by input images. Recent studies have visualized how neurons are activated by input images, revealing important parts of the scene for neuron activation. Methods have been developed to quantify the interpretability of the signal extracted from visualization methods, showing that individual neurons can capture visually consistent structures. Object detection emerges when considering units with highest activation in a CNN trained to recognize scenes, suggesting a binary form of encoding within the network. Further investigation is needed to determine if these observations reflect all relevant information captured by the feature map. The emergence of concepts into neurons is motivated by the observation that object detection techniques only work on a subset of feature maps. The paper focuses on the validation of information encoding in neurons through experiments, emphasizing binary encoding. Previous works on network binarization for power consumption are related, with methods like BID8, BID23, and BID12 approximating filters and inputs with binary values. The study challenges the binary nature of individual neurons, showing a bimodal activation pattern naturally emerges during training. The study challenges the binary nature of individual neurons, showing a bimodal activation pattern naturally emerges during training. This observation is further explored by analyzing gradients with respect to activations on single samples, providing insights into the representation learned by neurons in a neural network. In the study, neurons are associated with activation functions, defining one neuron as the application of a non-linear function to a single value. Different pixels of a feature map are considered as different activations from the same neuron in convolutional layers. Three different architectures are experimented with: a 2-layer MLP with dropout trained on MNIST, a 12-layer CNN with batchnorm trained on CIFAR-10, and a 50-layer ResNet trained on ImageNet. Five different models are analyzed, including versions with ReLU and sigmoid activation functions, as well as a CNN without non-linear activation. Specific layers of these networks are referred to throughout the paper. The study focuses on understanding neurons in a CNN model using the ResNet50 network. The analysis involves watching gradients flowing through neurons to gain insights into training dynamics. The experiments were conducted using Keras and Tensorflow libraries. The research delves into the combination of block outputs and skip connections in the network. During training of the cifar CNN and MNIST MLP networks, gradients of the loss with respect to activations were recorded regularly. The average sign of partial derivatives for each (input sample, neuron) pair indicated whether increased activation benefited or penalized sample classification. Zero partial derivatives appeared when samples were correctly classified due to float32 precision. During training, the average sign of partial derivatives for each (input sample, neuron) pair indicated whether increased activation benefited or penalized sample classification. The regularity of training in neural networks was observed through the lens of activations, showing a clear and regular signal in the activation gradients. The activation of a sample in a neuron seemed to partition the input distribution into two distinct categories. The activation of samples in neurons should be consistently pushed in the same direction during training to improve predictions. The regularity of activation gradients is more pronounced in layers closer to the output, with early layers showing more sign changes and noise. The presence of noise in gradients, especially in ReLU networks, raises questions about its impact on training dynamics. The linear version of the cifar CNN provides a clearer signal compared to the ReLU version, suggesting that noise may be inherent to the architecture and training process. The gradients in neurons indicate an attempt to separate input categories during training. Categories 'low' and 'high' are distinguished based on activation partial derivatives. Results for different neuron types are shown in Figure 2. Training reveals a struggle to separate categories, but stops before complete separation. The mechanism behind well-partitioned samples in neurons is a crucial question for discussion in Section 6. The dynamics of neuron activation during training are highlighted by analyzing the average sign of partial derivatives of the loss. Different categories of sample activations are observed, indicating consistent information received by the neuron for binary classification. The mechanism defining high and low categories remains a question for further exploration. The high and low categories in neuron activation are determined by the average sign of the loss function derivative with respect to the activation of a sample, which remains constant during training. The category definition is mainly influenced by the network's parameter initialization, with the derivative sign heavily dependent on the input class. Neurons in the output layer have derivative signs based solely on the class label. Category definition involves selecting a random subset of classes based on initial parameters. Further exploration of these mechanisms is suggested for future work. Neurons in different layers are categorized as high or low based on the average partial derivative sign over training. The final highest pre-activations of the high category are highlighted, showing that neurons operate like binary classifiers during training. The study aims to determine if all information transmitted by a neuron is encoded in the binary partition observed in the previous section. This is done by modifying activations through quantization and binarization to test neural network performance changes. The strategies for quantization and binarization of a trained layer aim to reveal structural components and test neural network robustness. The experiment involves testing if a neural network can transmit relevant information with only two distinct values per neuron. Quantization is based on percentiles of pre-activations, with thresholds separating pre-activations into two sets. Eleven thresholds are tried out between 0 and 100. Neural networks are remarkably robust to quantization of pre-activations, with performance being consistent across different layers. Only the conv1 layer from ResNet50 shows a significant decrease in accuracy, possibly due to poor gradient quality in early layers. The experiment suggests that each neuron transmits a binary signal to the next layer, supporting the hypothesis. The experiment involves a sliding window binarization approach to understand how pre-activations are encoded in neural networks. By using two thresholds to create a window, activations within the window are mapped to 1 while those outside are mapped to 0. This method helps determine the size and distribution of low and high categories of pre-activations. The experiment involves a sliding window binarization approach to understand how pre-activations are encoded in neural networks. By using two thresholds to create a window, activations within the window are mapped to 1 while those outside are mapped to 0. The results can provide insights about the organization of the representation and allow us to indirectly observe the binary partition used to encode information. Performance of the network should be better when the pre-activations inside Quantization. The experiment involves a sliding window binarization approach to understand how pre-activations are encoded in neural networks. The networks are robust to quantization, with neurons providing a binary signal to the next layers. Performance should decrease in fuzzy regions. The results show a clear signal across all layers and networks. The experiment used a sliding window binarization approach to analyze how pre-activations are encoded in neural networks. Results indicate a fuzzy partition of two categories around the 50th percentile rank, with neurons dividing inputs into distinct but overlapping categories. The binary behavior observed is not directly linked to the activation functions used. The experiment used a sliding window binarization approach to analyze how pre-activations are encoded in neural networks. Results indicate a fuzzy partition of two categories around the 50th percentile rank, with neurons behaving like binary classifiers. This behavior is consistent during training for layers close to the output, with direct implications on the interpretability of neurons. The experiment focused on high activations above the 99.5 percentile in BID5, showing that these activations do not fully represent neuron behavior. Neurons tend to learn concepts that distinguish half of observed samples, prompting further investigation into the regularity of gradients in deep network layers. This could reveal hidden training dynamics or the unstable nature of backpropagated gradients essential for first layer neuron convergence. The study offers a new perspective on neuron roles in deep networks. Our work provides a new perspective on the role of activation functions in deep networks, suggesting a precise role in promoting binary encoding in neurons. This insight could lead to the design of activation functions with strategically positioned binarization thresholds. Additionally, our findings offer a new angle for addressing the generalization gap observed in previous studies. The study observed a prioritization effect during training, where pre-activations inside a specific window are mapped to 1 and outside to 0. This binary partition of inputs reveals important information for classification, showing a clear pattern across all layers and networks. The findings suggest an encoding based on a fuzzy, binary partition of inputs into two categories of nearly equal size. This insight could provide valuable information about the generalization puzzle in deep networks. The study found that the sign of the loss function derivative remains constant for neurons close to the output during training, indicating a partitioning of samples based on positive/negative derivatives. Binarizing neuron pre-activations in any layer can preserve task information, supporting the idea of separating large and small pre-activations. These observations challenge traditional beliefs and offer insights into the generalization puzzle in deep networks. The unique observations in the paper raise questions about network learning capabilities, including convergence of first layer neurons with noisy partial derivatives, activation function design, and the generalization puzzle. Training information includes learning rates, batch sizes, and architecture details. Gradients and training information are discussed in Section 4. In Section 4 of the paper, gradients and pre-activations are recorded for different numbers of samples across various layers of ResNet50. Percentile ranks of ReLU thresholds are analyzed, showing convergence in the last layers. The ReLU threshold does not seem to be the cause of binary neuron behavior observed in the study. The paper analyzes the behavior of neurons in different layers of a network trained on MNIST and CIFAR-10 datasets. Neurons show consistent information about affecting sample activations, with some always increasing and others decreasing. Neurons can act as binary classifiers, with half of input samples having negative derivatives and the other half positive. The study also examines the evolution of pre-activation distributions in the network. The study examines the evolution of pre-activation distributions in different layers of a network trained on MNIST and CIFAR-10 datasets. Neurons are analyzed for their consistent behavior in affecting sample activations, acting as binary classifiers with both increasing and decreasing trends. The pre-activations are separated into high and low categories based on their average partial derivative signs during training. The final highest pre-activations of the high category are highlighted to show non-linear behavior. The study analyzes pre-activation distributions in different layers of a network trained on MNIST and CIFAR-10 datasets. Neurons act as binary classifiers with increasing and decreasing trends. The final highest pre-activations of the high category show non-linear behavior. Histograms demonstrate consistency between sample class and belonging to a neuron category."
}