{
    "title": "H1-IBSgMz",
    "content": "Self-normalizing discriminative models approximate the normalized probability of a class without computing the partition function, which is beneficial for computationally-intensive neural network classifiers. Recent studies have shown that language models trained with Noise Contrastive Estimation exhibit self-normalization, but the reason was unclear. This study provides a theoretical explanation by viewing NCE as a low-rank matrix approximation. The empirical investigation compares NCE to an explicit approach for self-normalizing language models and reveals a negative correlation between self-normalization and other factors. The ability of statistical language models to estimate word probabilities is crucial for NLP tasks. RNN language models have scalability issues due to softmax computation. Various methods like importance sampling, hierarchical softmax, BlackOut, and NCE have been proposed to address this problem. NCE has been used to train neural LMs with large vocabularies. Self-normalization was proposed as a solution to the high run-time complexity of predicting normalized probabilities at test time. A self-normalized discriminative model is trained to produce near-normalized scores, allowing for the avoidance of costly exact normalization at test time without sacrificing prediction accuracy. The study provides a theoretical justification for the self-normalization property of NCE, which was empirically observed in prior work. The study explores the self-normalization property of NCE in language modeling, showing that NCE's objective is to find the best low-rank approximation of conditional probabilities without estimating the partition function. Results indicate that models with better perplexities may have poorer self-normalization properties. The sum of self-normalized scores is negatively correlated with the entropy of the normalized distribution. NCE is a popular algorithm for training language models by transforming the parameter learning problem into a binary classifier training problem. NCE transforms the parameter learning problem into a binary classifier training problem by sampling from a mixture distribution. It uses an objective function to train a binary classifier to decide which distribution was used to sample the word. The NCE algorithm transforms parameter learning into a binary classifier training problem by sampling from a mixture distribution. It proposes to learn the normalization term during training and then use it to normalize the model at test time. However, setting the normalization term to 1 at train time doesn't affect model performance. At test time, computing the normalization factor is expensive due to the large vocabulary size. An alternative interpretation of NCE as a low-rank matrix approximation makes the normalization factor redundant during training. The NCE algorithm proposes learning the normalization term during training but it is found to be redundant as a low-rank matrix approximation. The NCE score reaches its maximum at the PCE matrix, indicating its effectiveness in corpus-based approximation. The NCE score reaches its global maximum at the PCE matrix, which can be easily verified by computing the derivative and setting it to zero. This implies that the global maximum of the function is obtained when certain conditions are met, with a concrete interpretation related to Kullback-Leibler and Jensen-Shannon divergences. The NCE score with k negative samples at the PCE matrix is optimized by the Jensen-Shannon divergence between joint word-context distribution and their marginal distributions. The NCE algorithm aims to minimize the difference between (k+1) times the divergence and the NCE score. In our interpretation, we seek the best unnormalized low-rank approximation of the PCE matrix without normalization factors. Previous work used Z=1 for normalization. The NCE algorithm aims to optimize the NCE score by setting a specific fixed value for Z, altering the mean input to the sigmoid function for improved training stability and performance. At test time, the NCE language model computes the conditional probability using a self-normalized matrix approximation of the PCE matrix. The NCE algorithm optimizes the NCE score by adjusting the mean input to the sigmoid function for better training stability. At test time, the NCE language model approximates the PCE matrix with a self-normalized matrix. Theorems 1 and 2 show that if the unnormalized score is close to log p(w|c), then log Zc is close to zero, indicating self-normalization of the LM learned by NCE. In this section, two language models are discussed: BID4 and BID0. BID4 encourages self-normalization during training by penalizing deviation from self-normalizing. However, it requires costly computation of Zc. On the other hand, BID0 proposes an efficient approximation by eliminating Zc in the first term and computing the second term on a sampled subset of the corpus. They justify this approach by showing that if a language model is self-normalized on a dense set of contexts, computing Zc on a small subset is sufficient. In this section, the importance of self-normalization in language modeling is discussed. Importance sampling (IS) is presented as an efficient alternative to NCE, where the normalization factor Zc is canceled out in the objective function. Unlike NCE, IS requires explicit computation of the normalization factor at test time. The self-normalization properties of NCE and its alternatives are empirically investigated. The study compared NCE language modeling with explicit self-normalization to DEV-LM softmax language modeling. The output bias terms were initialized to make both models self-normalized at init time. NCE-LM used k=100 negative sampling parameter. The implementation and hyperparameters followed previous studies. The evaluation of language models used two datasets, PTB and WIKI, with different sizes and vocabularies. Self-normalization was assessed using mean and standard deviation metrics. Models with low standard deviation were considered more self-normalizing. The evaluation of language models on PTB and WIKI datasets showed that self-normalization was assessed using mean and standard deviation metrics. In comparing NCE-LM and SM-LM, NCE-LM displayed self-normalization with low |\u00b5 z | and \u03c3 z values, while SM-LM was not self-normalized. Perplexity results showed SM-LM performing slightly better at low model dimensionality, but the performance gap closed at d = 650. The evaluation of language models on PTB and WIKI datasets showed that self-normalization was assessed using mean and standard deviation metrics. Comparing NCE-LM and SM-LM, NCE-LM displayed self-normalization with low |\u00b5 z | and \u03c3 z values, while SM-LM was not self-normalized. Perplexity results showed SM-LM performing slightly better at low model dimensionality, but the performance gap closed at d = 650. In further analysis, the self-normalization and perplexity performance of DEV-LM for different values of \u03b1 on validation sets was compared, showing that larger \u03b1 values improved self-normalization but at the expense of perplexity. The behavior of self-normalization and perplexity with model size was also explored, revealing a negative correlation between \u03c3 z and perplexity improvement. The evaluation of language models on PTB and WIKI datasets showed self-normalization performance using mean and standard deviation metrics. A technique to center log(Z) values around zero was proposed for test-set evaluation. Results of shifted NCE-LM and DEV-LM models with d = 650 showed near perfect \u00b5 z values and similar perplexity performance. Comparing NCE-LM and DEV-LM perplexities, they exhibited near identical performance. The performance of NCE-LM and DEV-LM models is compared based on the standard deviation of the normalization term. NCE-LM does not include the normalization term in its training objective, leading to faster training times. A correlation between entropy and log(Z) is analyzed, showing a negative correlation between log(Z) and entropy values. The value of log(Z c) is negatively correlated with entropy, with a stronger correlation for larger models. Low entropy distributions can have high log(Z c) values, deviating from the self-normalization objective. Examples show this phenomenon in predicted distributions. Larger models have larger variance in normalization terms, with potential for improvement in self-normalization algorithms. NCE is self-normalizing, performing reasonably well but not as good as explicitly trained language models. In future research, augmenting NCE's training objective with explicit self-normalization could improve self-normalizing models. Unexpected correlations were found between self-normalization, perplexity performance, and the partition function of self-normalized predictions. These insights may be valuable for enhancing self-normalizing models in future work."
}