{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs is introduced using the Wasserstein-2 metric proximal on the generators. The approach is based on the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experiments show improved speed and stability in training GANs in terms of wall-clock time and FID learning curves. The Kullback-Leibler divergence is challenging for low dimensional support sets, leading to interest in Wasserstein distance for defining discrepancy measures between densities. Wasserstein GAN has gained attention for defining loss functions and introducing structures for optimization, such as the Wasserstein steepest descent flow. This paper focuses on deriving the Wasserstein steepest descent flow for deep generative models in GANs using the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. The paper proposes using the Wasserstein-2 metric for GANs instead of the Fisher-Rao natural gradient due to difficulties with the KL divergence. They compute the proximal operator for GAN generators with a squared constrained Wasserstein-2 distance regularization, which can be approximated by a neural network. The constrained Wasserstein-2 metric simplifies computation in implicit generative models. They introduce a relaxed proximal operator for generators, making parameter updates simpler. This method can be easily implemented as a regularizer for generator updates. The paper introduces the Wasserstein natural gradient and a Wasserstein proximal method for GANs. It demonstrates the effectiveness of these methods in experiments with various types of GANs. The paper also reviews related work on optimal transport and its proximal operator on a parameter space, focusing on the Wasserstein-p distance with p = 2. The paper extends classic theory to cover parameterized density models using the constrained Wasserstein-2 metric function. This metric can be used for steepest descent optimization schemes and is different from the Wasserstein-2 distance on the full density set. The Fisher natural gradient and constrained Wasserstein-2 metric allow for obtaining a Riemannian metric structure, leading to the Wasserstein natural gradient. The Wasserstein gradient operator for a loss function is defined, with the steepest descent flow and gradient descent iteration given. The computation of the matrix G(\u03b8) \u22121 can be challenging, and an alternative numerical scheme using the proximal operator, known as the Jordan-Kinderlehrer-Otto (JKO) scheme, is also discussed. The Semi-Backward Euler method is a first-order scheme for the gradient flow of a loss function, which is easier to approximate than the forward Euler method. It does not require computing and inverting G(\u03b8) and is simpler than the backward Euler method (JKO). The method involves a constrained optimization over \u03a6, making it more tractable than the timedependent constraint in computing d W. The Semi-Backward Euler method in implicit generative models involves a generator function g(\u03b8, z) mapping noise prior Z to output samples with density X. The update in Proposition 3 utilizes a neural network to approximate variable \u03a6. The constrained Wasserstein-2 metric simplifies the formulation, allowing for the definition of the relaxed Wasserstein metric and a simple algorithm for the proximal operator on generators. The gradient constraint is relaxed for computations of the Wasserstein proximal operator in high-dimensional sample spaces. The update regularizes the generator by minimizing the squared difference in sample space. Algorithm 1 outlines the Relaxed Wasserstein Proximal method for minimizing a parameterized function. The effectiveness of Wasserstein proximal operator in GANs is illustrated using a toy example with a family of distributions. Proximal regularization for a loss function is defined, and different statistical distance functions are compared. The Wasserstein-2 and Euclidean distances are found to be effective, with the Euclidean distance being independent of the model structure. The Wasserstein-2 proximal operator is shown to outperform the Euclidean proximal in minimizing the Wasserstein-1 loss function. The Relaxed Wasserstein Proximal (RWP) algorithm improves speed and stability in training GANs by applying regularization on the generator, offering a novel approach compared to traditional discriminator regularization methods. The Relaxed Wasserstein Proximal algorithm improves GAN training by regularizing the generator. It introduces hyperparameters for updating the generator and discriminator, with a focus on the number of iterations. The algorithm is tested on Standard GANs, WGAN-GP, and DRAGAN using CIFAR-10 and CelebA datasets with DCGAN architecture. The Fr\u00e9chet Inception Distance is used to measure the quality of generated samples. The Relaxed Wasserstein Proximal regularization improves GAN training by enhancing convergence speed and stability. It shows improvements in sample quality for various GAN types, with a 20% enhancement in DRAGAN according to FID. The algorithm is tested on CIFAR-10 and CelebA datasets, demonstrating lower FID values and faster convergence times. Multiple generator iterations may hinder Standard GANs on CelebA initially, requiring algorithm restarts for successful learning. The Relaxed Wasserstein Proximal regularization improves GAN training by enhancing convergence speed and stability. It shows improvements in sample quality for various GAN types. Multiple generator updates compared to discriminator updates are examined, with RWP regularization leading to lower FID and improved speed. RWP regularization improves GAN training speed and achieves lower FID. Multiple generator iterations may cause initial learning issues, but successful runs are detectable. An experiment with 10 generator iterations per outer-iteration shows convergence and lower FID with RWP. Without RWP, training is highly variable with FID on a rising trend. The Semi-Backward Euler method on CIFAR-10 dataset shows comparable training to WGAN-GP loss. Training of SBE is more complex. The Semi-Backward Euler method is compared to norm WGAN-GP in optimizing three networks. The Wasserstein distance is commonly used as the loss function in GANs due to its statistical properties and ability to compare probability distributions. Further exploration of the Semi-Backward Euler method is left for future research. The Wasserstein distance is utilized in GANs as the loss function due to its properties in comparing probability distributions. In Wasserstein GANs, the discriminator must satisfy the 1-Lipschitz condition. The Wasserstein-2 metric forms a metric tensor structure, creating an infinite dimensional Riemannian manifold known as the density manifold. The gradient flow in the density manifold is linked to transport-related partial differential equations, such as the Fokker-Planck equation. Many approaches in learning communities leverage the gradient flow structure in probability space on the parameter space. Nonparametric models like the Stein gradient descent method and approximate inference methods for computing Wasserstein gradient flow have been studied. The Wasserstein gradient flow can be constrained on parameter space, with studies focusing on Gaussian families or elliptical distributions. A new approach applies the Wasserstein gradient to general implicit generative models, utilizing constrained Wasserstein gradient methods. In this work, the constrained Wasserstein gradient is applied to implicit generative models, focusing on regularizing the generator. The method computes the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space, leading to better minimization in terms of FID and faster convergence. The variational formulation introduces a Riemannian structure in density space, considering smooth and strictly positive probability densities. The elliptic operator \u2207\u00b7(\u03c1\u2207) identifies the function \u03a6 with the tangent vector V \u03a6 of the space of densities. The variational problem equation 5 is a geometric action energy in (P + , g W ). The Wasserstein gradient operator in (P + , g W ) is defined for a loss function F : P + \u2192 R. The Wasserstein-2 metric and gradient operator are constrained on statistical models defined by a triplet (\u0398, R n , \u03c1). A Riemannian metric is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The text discusses the constrained Wasserstein gradient operator in parameter space on a smooth Riemannian manifold. The proof involves the action function in Wasserstein statistical manifold and the derivation of a semi-backward method. The gradient operator and geodesic path are defined, and equations 6 and 7 are proven. Equation 6 is proven using DISPLAYFORM4, followed by the proof of equation 7. The Semi-backward method is derived as a consistent numerical method in time. Proposition 4 is proven in BID23, presenting the implicit model and the probability density transition equation satisfying the constrained continuity equation. The gradient and divergence operators are defined, leading to the derivation of the push-forward relation in equation 9. The last equality holds from the push forward relation equation 9, and using integration by parts w.r.t. x. Equation 12 equals equation 13 for any f \u2208 C \u221e c (R n ), proving equation 10. By the definition of the push forward operator equation 9, we have DISPLAYFORM17, proving equation 11. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 are provided. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments include different optimizers, step-sizes, and generator iterations for various datasets like CIFAR-10 and CelebA. The algorithm involves updating the discriminator and performing Adam gradient descent multiple times until a stopping condition is met. The Relaxed Wasserstein Proximal (RWP) method involves updating the discriminator and generator iteratively until a stopping condition is met. The differences between standard GAN training and RWP include specific terms and the number of generator iterations. Samples generated using RWP regularization show smooth transitions in the latent space, indicating no memorization. Specific hyperparameters for the Semi-Backward Euler (SBE) on WGAN-GP include a batch size of 64 and the DCGAN architecture for the discriminator and generator. The study utilized a one-hidden-layer fully connected network for the potential \u03a6 p, with layer normalization. The Adam optimizer with specific parameters was used for the generator, discriminator, and potential. The latent space dimension was set to 100, and updates were made to the discriminator, generator, and potential in each outer-iteration loop."
}