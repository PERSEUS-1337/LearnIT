{
    "title": "HydnA1WCb",
    "content": "We propose a novel architecture called Gaussian prototypical networks for k-shot classification on the Omniglot dataset. This model extends prototypical networks by incorporating a Gaussian covariance matrix to estimate confidence regions in the embedding space. By using uncertainties of individual data points as weights, our network constructs a direction and class dependent distance metric. Experimental results show that Gaussian prototypical networks outperform vanilla prototypical networks in 1-shot and 5-shot classification tasks on the Omniglot dataset. Additionally, artificially down-sampling a fraction of images in the training set improves performance, suggesting that this architecture may excel in less homogeneous, noisier datasets common in real-world applications. Few-shot learning aims to replicate humans' ability to recognize new object categories with minimal examples. Parametric deep learning excels with abundant data but struggles with rapid adaptation to new classes. In contrast, few-shot learning requires fast adaptation to new data, such as k-shot classification where unseen classes are learned with k labeled examples. Non-parametric models like k-nearest neighbors do not overfit but may struggle with performance. In this paper, a novel architecture called the Gaussian prototypical network is developed based on prototypical networks used in previous studies. The model maps images into embedding vectors and predicts an estimate of image quality along with a confidence region characterized by a Gaussian covariance matrix. It learns to construct a class-dependent distance metric in the embedding space, showing promising results on the Omniglot dataset for k-shot classification tasks. The paper introduces a novel Gaussian prototypical network that utilizes a direction and class dependent distance metric in the embedding space. The model aims to express confidence in individual data points to improve results, especially in noisy datasets. Experimental results on the Omniglot dataset show performance consistent with state-of-the-art in 1-shot and 5-shot classification tasks. The paper is structured with sections on related work, methods, episodic training scheme, dataset description, experiments, and conclusions. Non-parametric models like k-nearest neighbors (kNN) are suitable for few-shot classifiers but are sensitive to distance metric choice. Using a learned metric embedding for kNN classification has shown good results. Matching networks have also been proposed for learning distance metrics between image pairs, improving few-shot classification performance. Training schemes mimicking data-poor test conditions have been effective in enhancing model performance. Our approach focuses on image classification without using meta-learning. Building on a model that maps images into embedding vectors, we predict confidence levels for individual data points using a learned covariance matrix. This enriches the embedding space for better image projection and clustering, improving classification accuracy. The Gaussian prototypical network uses a direction and class-dependent distance metric for classification. It extends the prototypical network by predicting embedding vectors and confidence regions for individual data points. The model maps images into embedding vectors and uses support images to define prototypes and covariance matrices for each class, improving classification accuracy. The Gaussian prototypical network uses support images to define class prototypes and covariance matrices for classification. The encoder architecture is similar to the vanilla prototypical network, but the interpretation of encoder outputs and metric construction differ. The Gaussian network function is illustrated in FIG1, utilizing a multi-layer convolutional neural network to encode images into high-dimensional Euclidean vectors. The Gaussian prototypical network explores three variants for generating covariance matrices: Radius covariance estimate, Diagonal covariance estimate, and Full covariance estimate. The network uses two encoder architectures: a small architecture and a big architecture. The full covariance estimate method was deemed too complex and not further explored. The small and big architectures were used to validate experiments and explore the effect of increased model capacity on accuracy. Both architectures consisted of 4 blocks stacked together. Different methods were tested to translate the raw encoder output into a covariance matrix, with two approaches proving beneficial for training. The encoder applies sigmoid function componentwise to ensure data points are less important. The value of S is bounded between 1 and 2, with a flexible training regime using offset, scale, and div parameters. Episodic training involves selecting a subset of classes for support and query examples to calculate loss and classify query examples based on class prototypes in the embedding space. The Gaussian prototypical network uses class prototypes to classify query examples and calculate loss. It estimates the covariance of each embedding point and outputs the radius or diagonal of a covariance matrix along with the embedding vector. This network learns a class and direction-dependent distance metric in the embedding space, with training speed and accuracy depending on how distances are used to construct a loss. Linear Euclidean distances are found to be the best option for the loss function. The Gaussian prototypical network creates class prototypes from support points using a variance calculation. The network combines Gaussians centered on individual points into an overall class Gaussian. Accuracy is estimated on the test set for different numbers of support points, resulting in a k-shot classification accuracy curve. The study evaluates models on 5-way and 20-way test classification using the Omniglot dataset. The dataset contains 1623 character classes from 50 alphabets, with 964 unique classes in the training set and 659 in the test set. No class overlap exists between the two sets. Augmentation techniques were used to increase the number of classes. A designated validation set was not used, and the best model was chosen based on training accuracies alone. The dataset was augmented by rotating characters to increase the number of classes. This resulted in 77,120 images in the training set and 52,720 images in the test set. Due to rotational augmentation, characters with rotational symmetry were defined as multiple classes, making 100% accuracy unattainable. Many few-shot learning experiments were conducted on the Omniglot dataset. In exploring different aspects of networks, including embedding space dimensionalities, covariance matrix generation, and encoder capacities, the Gaussian variant was favored over vanilla prototypical networks. The radius method for predicting a single number per embedding point showed the best performance on Omniglot. Various factors such as encoder size, distance metrics, covariance matrix degrees of freedom, and dataset augmentation were investigated to improve performance. The models were trained using the Adam optimizer with a specific learning rate schedule. During training, each class in the mini-batch had 1 support point, with 19 query points per class. Using encoder outputs as covariance estimates was found to be more beneficial than adding parameters for additional embedding dimensions. The radius estimate method, predicting one real number per embedding vector, showed better performance, while the diagonal estimate did not improve results. The best performing model was trained on the undamaged dataset for 220 epochs, then continued training with down-sampled images for various epochs. Purposeful damage to the dataset encouraged the usage of covariance estimate, improving (k > 1)-shot results. Our models perform consistently with state-of-the-art in 1-shot and 5-shot classification on the Omniglot dataset. In 5-shot 5-way classification, we are close to perfect performance. A more complex dataset is needed for further few-shot learning algorithm development. The Gaussian prototypical network outperforms the vanilla version by predicting covariances of individual embedded images. Our models perform consistently with state-of-the-art in 1-shot and 5-shot classification on the Omniglot dataset. The Gaussian prototypical network improves classification by predicting covariances of embedded images. The network shows better performance on the Omniglot dataset compared to the vanilla version. The Gaussian prototypical network outperforms the vanilla version by predicting covariances of embedded images. It is beneficial to estimate a single real number on top of an embedding vector rather than a diagonal or full covariance matrix. Training in the 1-shot regime yields the best results, especially for 5-way classification on the Omniglot dataset. The network down-weights damaged examples by predicting a higher s, leading to close to perfect performance in 5-way classification. By down-sampling fractions of the training dataset, the network is encouraged to utilize covariance estimates, especially for (k > 1)-shot classification. Learning the embedding and its uncertainty is beneficial for heterogeneous datasets, where down-weighting data points can be crucial for accurate classification, as shown in experiments with Omniglot."
}