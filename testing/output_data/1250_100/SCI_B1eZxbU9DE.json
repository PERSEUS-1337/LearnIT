{
    "title": "B1eZxbU9DE",
    "content": "The proposed framework aims to learn a PPDDL representation of the domain model incrementally using experiences from current planning problems, addressing the complexity of interactions in real-world applications. Introducing reliability as intrinsic motivation for reinforcement learning helps in learning from failures to enhance efficiency and goal-directedness. The approach is evaluated with experimental results in three planning domains, focusing on learning models from data to improve planning in non-stationary environments. The incremental learning model (ILM) presented in this work learns action models incrementally over planning problems using reinforcement learning. It addresses the challenge of acquiring training data through acting, and improves upon previous action models without using past training data. The approach utilizes PPDDL for planning and a rules-based representation for the learning process. The incremental learning model (ILM) learns action models incrementally using prior knowledge to bias learning and reduce exploration. It estimates learning progress empirically and uses reliability to guide model search. It can exploit with Gourmand for solving MDP problems and learns from failure by recording and preventing failed executions. The incremental learning model (ILM) records failed executions to prevent similar failures, reducing the number of failed executions and increasing exploration efficiency. The paper discusses related work, presents ILM details, evaluates it in planning domains, and introduces algorithmic features for learning probabilistic actions. The training data includes state transitions (s t , a t , s t+1 ) to represent actions with probabilistic effects, unlike deterministic actions in previous works. Our work addresses incremental learning over planning problems using current training data. We consider prior knowledge with incomplete action models, unlike previous approaches. Model-based reinforcement learning, such as R-MAX, is sample-efficient but impractical for large state spaces. Additional assumptions like factored state spaces are needed for planning problems with such complexities. Our work focuses on incremental learning over planning problems using current training data. We consider prior knowledge with incomplete action models, unlike previous approaches. Action models in PPDDL are defined by preconditions and effects, restricted to conjunctions of predicates. Rules-based representation is used for learning action models, suited for the incremental nature of reinforcement learning. The key difference between PPDDL and rules representations lies in the addition of noise effects in the latter to avoid modeling rare effects. Rules cover state-action pairs and propositional rules are obtained from relational rules by grounding. MDPs model fully observable problems with uncertainty using a tuple format. Reinforcement Learning involves finding a policy to maximize expected rewards in a Markov Decision Process (MDP). Model-based reinforcement learning is used when transition functions are unknown. A new approach called ILM for incremental learning across planning problems is proposed, consisting of a rules learner and a reinforcement learning framework. Reliability is introduced as a measure of learning progress in action models. Reliability in reinforcement learning is defined as the exposure and success rate of actions, with a focus on learning progress. Success rate is determined by recent successful executions and error rates. The reliability of the prior action model is updated with a discount factor to reduce its significance over time. The execution status is 'failure' when the action's precondition is not met, leading to an unchanged state. 'Partial success' occurs when the post-state does not match the expected outcome. Volatility measures how much an action model changes after learning, with low volatility indicating convergence to the true model. It is computed recursively using a discount factor and normalized difference between rule sets. The difference between two sets of rules is calculated as the sum of differences of pairs of rules, with each rule paired at most once. Exposure measures the variability of pre-states in the training data, considering the unique pre-states in state transitions. Reliability is based on inferred probabilities of effects from successful state transitions. The rules learner from BID15 applies a search operator to modify rules, using a score function as heuristics. A deviation penalty is introduced to refine action models, penalizing complex rules to prevent over-specialization. The penalty increases as rules deviate further from the prior models. The rules learner in BID15 uses a deviation penalty to refine action models by penalizing complex rules to prevent over-specialization. The penalty increases as rules deviate further from the prior models. The scaling parameters, \u2206 drop and \u2206 add, are defined based on the pairings of rules. The algorithm for ILM is explained, along with subroutines for reinforcement learning and learning from failure. The inputs include prior action models, initial and goal states, and the maximum number of iterations. The main loop of the algorithm interleaves learning, planning, and acting. Exploration and exploitation are performed at the start of each iteration. Failed instances are recorded in tabu for learning, while synthetic state transitions are generated for successful actions. Learning is delayed until certain criteria are met to prevent incorrect rules from being learned. The algorithm interleaves learning, planning, and acting. Exploration and exploitation are balanced to prevent incorrect rules from being learned. The algorithm terminates after reaching the maximum number of iterations or when the goal is reached, returning learned rules, reliability, maximum exposure, and tabu for the next function call. The balance between exploration and exploitation is implemented in EE(s, g, R, RE, tabu, \u03b6) by computing counts for all applicable actions in s using a context-based density formula. The algorithm balances exploration and exploitation to learn rules under relational representations. Count-action pairs are sorted by reliability, with less reliable actions explored more. A state is known if counts exceed a threshold or if all action reliabilities surpass a constant. Exploitation is done using Gourmand planner if the state is known, otherwise exploration is attempted. Actions are selected randomly from applicable grounded actions if exploitation fails. The algorithm checks for grounded actions in tabu to avoid dead-ends. Failed executions due to unsatisfied preconditions are recorded. Predicates in a state are compared to entries in tabu before executing an action. Substitution is performed to match predicates, ensuring relational checks. The algorithm checks for grounded actions in tabu to avoid dead-ends by comparing predicates in a state to entries in tabu before execution. Substitution is performed to match predicates, ensuring relational checks. If f s does not have at least one predicate not in f t, then a is in tabu. The completeness of Algorithm 2 depends on failed instances in tabu, with the algorithm being sound as no erroneous instance is added to tabu. In experiments, actions in tabu lead to successful execution. Ten planning problems are attempted sequentially in increasing scale. Each trial starts with empty action models. 50 trials are conducted on a four-core Intel machine. Three planning domains are used: Tireworld, Exploding Blocksworld, and Logistics. In Tireworld, a flat tire can lead to a dead-end without spare tires. Exploding Blocksworld involves blocks detonating. In the Exploding Blocksworld domain, blocks may detonate when put down, destroying the block or table beneath. The goal states are random configurations of three blocks. Logistics problems involve one truck, airplane, and parcel per city. ILM's performance is evaluated based on correctness of the learned model and goal-directedness. ILM-R and ILM-T have specific configurations for learning and reliability. R-MAX uses past training data, unlike ILM variants. The correctness of a learned model can be measured by the variational distance between the model and the true distribution. Variational distances for Tireworld, Exploding Blocksword, and Logistics domains are shown in Figure 2. ILM learns action models incrementally, with ILM-R performing slightly worse due to low variability in training data. Learning from failure is highlighted by larger variational distances for ILM-T and R-MAX. Table 3 shows the average number of successful trials for each domain out of 50 trials. In the Tireworld, Exploding Blocksworld, and Logistics domains, successful trials were conducted with different learning algorithms. Exploding Blocksworld showed varying performance with ILM and R-MAX, while Logistics had consistent performance with ILM. R-MAX required more training data from failed experiences to outperform ILM. In the Tireworld domain, ILM-R outperforms ILM in rounds 1 to 3 by exploiting learned models from previous rounds, resulting in more successful trials. This advantage is highlighted by the average number of successful trials being larger in rounds 2 and 3 compared to round 1, despite the planning problem scales being the same. In the Exploding Blocksworld domain, ILM outperforms ILM-R in rounds 4 to 10 due to the need for exploitation when facing dead-ends caused by irreversible changes to the state. Despite challenges like blocks detonating with a 0.2 probability, ILM shows the most successful trials in all rounds. ILM has the most successful trials in all rounds, outperforming ILM-R due to the difficulty of reaching the goal state with exploration alone. R-MAX has lower variational distances than ILM for rounds 5 to 10 but does not learn from failure. ILM-T had significantly fewer successful executions compared to ILM. Exploration decreases while exploitation increases over rounds as action models are learned incrementally. The use of tabu in ILM shows a decline in entries added after round 1, with the number of actions found in tabu corresponding to failed executions. The number of successful trials increases even with larger planning problems in Logistics. In small-scale planning problems, driveTruck failed to execute repeatedly till round 3 with only two out of 432 grounded actions succeeding. The state space subset, including the goal state, was not reached, affecting the execution of loadTruck, unloadTruck, loadAirplane, and unloadAirplane actions. The action model for driveTruck was learned from a partially successful execution in round 3, leading to the discovery of an extraneous precondition in the learned rule. In small-scale planning problems, driveTruck failed to execute till round 3. The action model for driveTruck had an extraneous precondition, causing driveTruck to not be selected in rounds 4, 5, and 7. loadTruck and unloadTruck were not attempted due to being in tabu. A domain-independent framework, ILM, was introduced for incremental learning without past training data. Reliability was used to reduce exploration for reliable action models. ILM learns from failure by checking state-action pairs for actions. Experimental results of ILM on three benchmark domains showed a decrease in variational distances of learned action models over subsequent rounds, leading to improved correctness and goal-directedness by learning from failure. More training data is needed for complex domains, and using past data is not effective for non-stationary domains. Future exploration will focus on learning distributions from current training data and maintaining a fixed size of training data while maximizing exposure."
}