{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are effective for control policies in reinforcement and imitation learning. A new technique, Quantized Bottleneck Insertion, helps create finite representations of RNN memory vectors and observation features. This allows for better analysis and understanding of RNN behavior. Results on synthetic environments and Atari games show small finite representations can lead to improved interpretability in policies learned through deep reinforcement and imitation learning. In this paper, the focus is on understanding and explaining RNN policies by creating more compact memory representations. The challenge lies in interpreting the high-dimensional continuous memory vectors used in RNN policies, which are updated through complex gating networks. The goal is to quantize the memory and observation representation to capture discrete concepts, which could enhance explainability. This approach involves manipulating and analyzing the quantized system to gain insights into RNN memory usage. The main contribution of this approach is to transform an RNN policy with continuous memory and observations into a finite-state representation called a Moore Machine using Quantized Bottleneck Network (QBN) insertion. QBNs, which are auto-encoders with quantized latent representations, encode memory states and observation vectors encountered during RNN operation. By inserting QBNs into the RNN policy, a Moore Machine Network (MMN) with quantized memory and observations is created, nearly equivalent to the original RNN. The MMN can be used directly or fine-tuned to address inaccuracies introduced by QBN insertion. Our approach demonstrates the effectiveness of \"straight through\" gradient estimators in QBNs, showcasing accurate extraction of ground-truth MMNs and insights into RNN memory usage. Experiments on synthetic domains and benchmark grammar learning problems show near-equivalent MMNs with surprising small sizes. Insights reveal RNNs implementing reactive control or open-loop controllers in different games. Previous efforts have been made to understand the internals of Recurrent Networks. Our work is related to extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages. Previous approaches involve discretizing the memory space and using query-based learning algorithms. However, these methods do not directly apply to learning policies, which require extending to Moore Machines. Our approach inserts discrete elements into the RNN to preserve its behavior and allow for a finite state representation. Our work inserts discrete elements into the RNN to preserve its behavior and allow for a finite state representation. This approach extends previous work on learning FSMs and introduces the method of QBN insertion to transform pre-trained recurrent policies into a finite representation. Recurrent neural networks (RNNs) are commonly used in reinforcement learning to represent policies that require internal memory. An RNN maintains a hidden state that influences action choice based on observation features extracted from the current state. The high-dimensional nature of the hidden state and observation features can make interpreting memory challenging, motivating the goal of supporting interpretability over efficiency. Our goal is to extract compact quantized representations of memory states to investigate key features using Moore Machines and their deep network counterparts. Moore Machines are described by hidden states, observations, actions, transition functions, and policies. Moore Machine Networks represent these using deep networks for transition functions and policies. In this work, Moore Machine Networks (MMNs) use deep networks to represent transition functions and policies. MMNs map continuous observations to a finite discrete observation space and utilize quantized state and observation representations. MMNs are like traditional RNNs with memory composed of k-level activation units and environmental observations transformed to a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for complex problems. Learning Moore Machine Networks (MMNs) from scratch can be difficult for complex problems, such as training high-performing MMNs for Atari games. A new approach is introduced to leverage the ability to learn RNNs by using quantized bottleneck networks (QBNs) to embed continuous features into a k-level quantized representation. The QBNs are inserted into the original recurrent net to create a network that consumes quantized features and maintains quantized state, effectively forming an MMN. The purpose of dimensionality reduction in continuous space is achieved through quantized bottleneck networks (QBNs), which discretize a continuous space by quantizing the activations of units in the encoding layer. A QBN consists of a multilayer encoder E mapping inputs x to a latent encoding E(x) and a multilayer decoder D. The QBN output is quantized using a 3-level quantization scheme of +1, 0, and -1. To support 3-valued quantization, a specific activation function is used to ensure non-differentiability between the decoder and encoder, making it seemingly incompatible with backpropagation. The straight-through estimator is effective in dealing with gradients between the decoder and encoder in quantized bottleneck networks (QBNs). By treating the quantize function as the identity function during back-propagation, the QBN allows for a k-level encoding. Training QBNs as autoencoders using L2 reconstruction error, and running a recurrent policy in the target environment generates training sequences for observation, observation feature, and hidden state. Two QBNs, b f and b h, are trained on observed features and states, with low reconstruction error indicating high-quality k-level encodings. The QBNs b f and b h act as \"wires\" in the RNN, transmitting input to output with some noise. Inserted into the RNN, they provide a quantized representation of features and states. While imperfect reconstruction may lead to slight performance differences, fine-tuning can help align the MMN with the original RNN behavior. Fine-tuning the MMN by training on original RNN rollout data aims to match the softmax distribution over actions. Training in this manner is more stable than simply outputting the same action. Visualization tools can be used to analyze memory and feature bits for a semantic understanding. Another approach involves using the MMN to create a Moore Machine over discrete symbol spaces for further analysis. The Moore Machine is constructed from data with quantized states and features, leading to a transition function. Minimization techniques are applied to reduce the number of states and observations in the resulting Moore Machine. Experiments aim to extract Moore Machines from RNNs without performance loss, determine the magnitude of states and observations, and assess interpretability of recurrent policies. In this section, the study explores the use of Moore Machines in synthetic environments like Mode Counter and benchmark grammar learning problems to assess the interpretability of recurrent policies. Mode Counter Environments (MCEs) allow for varying memory requirements and types of memory usage, with agents needing to infer the mode through observations and memory use in order to receive rewards. The study explores the use of Moore Machines in Mode Counter Environments (MCEs) to assess the interpretability of recurrent policies. MCEs require agents to infer the mode through observations and memory use to receive rewards. Three MCE instances are examined: Amnesia, Blind, and Tracker, each utilizing memory and observations in different ways to achieve optimal performance. In MCE instances, a recurrent architecture with 4 modes is used for training using imitation learning. RNNs achieve 100% accuracy on the imitation dataset. Moore Machines are employed with varying bottleneck units for observation and hidden state. Training in MCE environments with QBNs is faster compared to RNN training. Training QBNs in MCE environments is faster than RNN training due to the lack of temporal dependencies. QBNs with bottleneck sizes of Bf \u2208 {4, 8} and Bh \u2208 {4, 8} were embedded into RNNs to create discrete MMNs. Fine-tuning was required in some cases to achieve optimal MMN performance, with an exception yielding 98% accuracy. The combined error accumulation of the two bottlenecks was found to be responsible for reduced performance. Moore Machine Extraction revealed the number of states and observations before and after minimization. After training QBNs in MCE environments, MMNs were created by embedding QBNs into RNNs. Fine-tuning was needed for optimal performance, with one exception. Moore Machine Extraction showed that before minimization, there were more states and observations compared to after minimization. However, after minimization, exact minimal machines were obtained for most MCE domains, indicating optimal learning in most cases. The ground truth minimal machines found were equivalent to the MMNs learned via QBN insertions. The study evaluated the approach over 7 Tomita Grammars, treating them as environments with 'accept' and 'reject' actions. RNNs were trained using imitation learning with a one-layer GRU and fully connected softmax layer. Test results for the trained RNNs are presented in TAB1. The study evaluated RNNs trained on Tomita Grammars, showing high accuracy except for grammar #6. MMN training involved bottleneck learning for hidden memory state. MMNs maintained RNN performance without fine-tuning. MM extraction and minimization resulted in reduced state-space while maintaining performance. In this section, the study applies the technique to RNNs trained on six Atari games using the OpenAI gym. Unlike previous experiments with known ground truth minimal machines, the complexity of Atari inputs makes it unclear what to expect. The recurrent architecture for all Atari agents is the same, with input observations being preprocessed image frames. There have been efforts to understand Atari agents, but this work aims to extract finite state representations for Atari policies. The input observation is an image frame preprocessed for an Atari game. The network architecture includes 4 convolutional layers, a GRU layer, and a fully connected layer. The A3C RL algorithm is used for training with specific parameters. The RNN performance on six games is reported. The QBN architecture is adjusted for continuous observation features. The decoder in the network has layers with different numbers of nodes. Training data for the RNN was generated using noisy rollouts for Atari games. Bottlenecks were trained for specific values and inserted into the RNN to create MMNs for each game. Performance of the MMNs before and after fine-tuning is shown in TAB2, with improvements seen in some games like Pong and Freeway. The MMNs after fine tuning achieve similar scores to the RNN in games like Pong, Freeway, Bowling, and Boxing, demonstrating the ability to learn a discrete representation of input and memory. However, in games like Breakout and Space Invaders, the MMNs achieve lower scores due to poor reconstruction in rare game states, highlighting the need for more intelligent training approaches. After fine tuning, MMNs achieve similar scores to RNN in games like Pong, Freeway, Bowling, and Boxing, showing the ability to learn a discrete representation of input and memory. However, in games like Breakout and Space Invaders, MMNs score lower due to poor reconstruction in rare game states, indicating the need for smarter training methods. MM Minimization reduces the number of states and observations significantly, making them easier to analyze manually. In Atari, memory use is observed with just three states and 10 discrete observation symbols in Pong, where each observation transitions to the same state regardless of the current state, requiring no memory. The Pong policy is similar to the Amnesia MCE, while Bowling and Freeway have open-loop policies that ignore input images. Freeway always takes the Up action, while Bowling has a looped action sequence. The MM extraction approach provides additional insight into the policy structures of these games. The approach involves extracting finite state Moore Machines from RNN policies by using bottleneck insertion to train Quantized Bottleneck Networks. This allows for the extraction of discrete Moore machines that can be transformed into minimal machines for analysis. Results show accurate extraction of ground truth in known environments and successful experiments in six Atari games. The study involves extracting finite state Moore Machines from RNN policies in Atari games. The learned MMNs maintain similar performance to the original RNN policies and provide insights into memory usage. The number of required memory states is small, and different policies show varying reliance on memory. Future work includes developing tools for analyzing observations and states to gain further insight into policies. The MCE hidden state consists of a mode and a count of time-steps in that mode. Mode changes occur based on a lifespan condition and transition distribution. Observations received by the agent are continuous-valued and determine the mode when the count is within a specified set. The agent must remember the current mode and use memory to track how long the mode has been active for optimal performance. The MCE instances tested include Amnesia, Blind, and Tracker. Amnesia does not use memory to track past information, Blind requires memory to keep track of a deterministic mode sequence, and Tracker pays attention to observations when needed. The most general instance of the environment requires an optimal policy to pay attention to observations at c t = 0 and use memory to track the current mode and mode count. The number of modes and their life-spans can lead to difficult problems. 'A' and 'R' are used to denote accept and reject states, respectively, with machines being 100% accurate except for Grammar 6."
}