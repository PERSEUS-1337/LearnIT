{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology constructs positive definite kernels from dissimilarity measures on structured inputs like time series, strings, histograms, and graphs. It uses random feature maps to build a kernel specified by the distance measure, improving generalizability over Nearest-Neighbor estimates. D2KE is a versatile approach that can handle complex structured inputs of variable sizes. Our proposed framework excels in classification experiments across various domains such as time series, strings, and histograms for texts and images. It outperforms existing distance-based learning methods in terms of accuracy and computational efficiency. Constructing a dissimilarity function between instances is often easier than creating a feature representation, especially for structured inputs like time series, strings, histograms, and graphs. Various well-developed dissimilarity measures exist for complex structured inputs, such as Dynamic Time Warping for time series and Edit Distance for strings. Standard machine learning methods are primarily designed for vector representations, with limited work on structured inputs. Distance-based methods for classification or regression on structured inputs are less common compared to vector representations. Nearest-Neighbor Estimation (NNE) is a common distance-based method that predicts outcomes by averaging nearest neighbors in the input space. However, NNE can be unreliable with high variance when neighbors are far apart. To address this, research has focused on global distance-based machine learning methods, utilizing connections to kernel methods or learning with similarity functions. The data similarity matrix is treated as a kernel Gram matrix, and standard kernel-based methods like Support Vector Machines (SVM) are used for prediction. Distance-based methods for classification or regression on structured inputs are less common compared to vector representations. Research has focused on global distance-based machine learning methods, utilizing connections to kernel methods or learning with similarity functions. The approach of estimating a positive-definite Gram matrix that approximates the similarity matrix has been explored, but modifications often lead to a loss of information. Another common approach involves selecting a subset of training samples as a held-out representative set for feature function calculation. In this paper, a novel general framework called D2KE (Distance to Kernel and Embedding) is proposed to construct a family of positive-definite kernels from a dissimilarity measure on structured inputs. This approach draws from Random Features literature but builds novel kernels specifically designed for a given distance measure. The kernels generated by this framework are Lipschitz-continuous with respect to the distance measure, providing better generalization properties than nearest-neighbor estimation. This framework produces a feature embedding and vector representation for each instance. The D2KE framework constructs positive-definite kernels from a given distance measure for structured inputs, providing better generalization properties than nearest-neighbor estimation. It produces feature embeddings for each instance, outperforming existing distance-based learning methods in testing accuracy and computational time. The framework also generalizes Random Features methods to complex structured inputs, accelerating kernel machines across various domains. Kernel machines can be applied to structured inputs like time-series, strings, and histograms. Existing approaches for Distance-Based Kernel Learning have limitations, such as strict conditions on the distance function or constructing empirical PD Gram matrices that may not generalize well. Some methods provide conditions for obtaining a PD kernel through simple transformations of the distance measure, but these conditions are not met by commonly used dissimilarity measures. Another approach involves finding a Euclidean embedding approximating the dissimilarity matrix. Additionally, a theoretical foundation for an SVM solver in Krein spaces has been presented. Interest in approximating non-linear kernel machines using randomized feature maps has surged in recent years due to a significant reduction in training and testing times for kernel based learning algorithms. Various explicit nonlinear random feature maps have been constructed for different types of kernels, including Gaussian and Laplacian Kernels, intersection kernels, additive kernels, dot product kernels, and semigroup kernels. Among them, the Random Fourier Features (RFF) method is commonly used. The Random Fourier Features (RFF) method approximates a Gaussian Kernel function using a Gaussian random matrix. To accelerate RFF on high-dimensional input data matrices, methods leveraging structured matrices have been proposed. However, existing RF methods only consider inputs with vector representations, while D2KE takes structured inputs of different sizes and computes the RF with a structured distance metric. D2KE constructs a new PD kernel through a random feature map, making it computationally feasible. D2KE introduces a new PD kernel using a random feature map and RF for computational efficiency. Contrasting with BID49, which is limited to single-variable time-series, D2KE offers a unified framework for various structured inputs like strings and graphs. It provides a theoretical analysis on KNN and distance-based kernel methods for estimating target functions from input samples. The ideal feature representation for learning tasks should be compact and result in a simple target function. A good dissimilarity measure should imply small differences in the function between objects and have a small expected distance among samples. Lipschitz Continuity is preferred for the target function to have a small Lipschitz-continuity constant with respect to the dissimilarity measure. The effective dimension in measuring the space of Multiset is discussed, along with the impact of Lipschitz Continuity on the dissimilarity measure. The covering number N(\u03b4; X, d) is introduced to measure the size of the space implied by a given dissimilarity measure in a structured input space X. The estimation error of a Nearest-Neighbor Estimator is analyzed in relation to the effective dimension and covering number. The (modified) Hausdorff Distance measures the distance between elements in a set. The covering number N(\u03b4; V, \u2206) measures the size of the space under the ground distance \u2206. The estimation error of the k-Nearest-Neighbor estimate of f(x) is bounded by a constant c > 0. The estimation error decreases slowly with n when p X,d is large. The error of k-NN decreases slowly with n, requiring the number of samples to scale exponentially in p X,d. An estimator based on a RKHS derived from the distance measure has better sample complexity for higher effective dimension problems. A simple approach, D2KE, constructs positive-definite kernels from a given distance measure using a family of kernels parameterized by p(\u03c9) and \u03b3. This kernel relates to the Distance Substitution Kernel and involves a soft minimum function. The kernel in Equation FORMULA12, parameterized by p(\u03c9) and \u03b3, is a soft version of the distance substitution kernel. When \u03b3 \u2192 \u221e, the value of Equation FORMULA15 is determined by min \u03c9 \u2208\u2126 d(x, \u03c9) + d(\u03c9, y), which equals d(x, y) if X \u2286 \u2126. Unlike the distance-substitution kernel, the kernel in Equation FORMULA13 is always positive definite by construction. Random Features can be used to approximate the kernel in Equation FORMULA12, which cannot be evaluated analytically in general. The kernel in Equation FORMULA12, parameterized by p(\u03c9) and \u03b3, is a soft version of the distance substitution kernel. Random Features can be used to approximate this kernel, allowing for efficient computation in large-scale settings. This approach involves learning a target function as a linear function of the RF feature map by minimizing a domain-specific empirical risk. Additionally, a recent work focuses on selecting random features through optimization in a supervised setting, which could be extended to develop a supervised D2KE method. The curr_chunk discusses the application of the exponent function parameterized by \u03b3 in contrast to traditional RF methods. It also mentions a detailed analysis of the estimator in Algorithm 1 and its statistical performance compared to K-nearest-neighbor. The relationship to the representative-set method is highlighted, showing how a Random-Feature approximation can be obtained. This involves creating an R-dimensional feature embedding, similar to the representative-set method, by interpreting Equation (8) as a random-feature. The curr_chunk discusses the importance of the choice of p(\u03c9) in the kernel, showing that \"close to uniform\" distributions yield better performance in various domains compared to the Representative-Set Method (RSM). Examples include time-series with DTW, string classification with edit distance, and classifying sets of vectors. The importance of choosing the distribution p(\u03c9) in the kernel is highlighted, showing that distributions close to uniform yield better performance compared to the Representative-Set Method (RSM) in various domains. The synthetic nature of p(\u03c9) allows for generating unlimited random features, resulting in a better approximation to the exact kernel. Additionally, the selected distribution captures more relevant semantic information, leading to significantly better results in estimating f(x) under the dissimilarity measure d(x, \u03c9). In this section, the proposed framework is analyzed from the perspective of error decomposition in the RKHS corresponding to the kernel. The population and empirical risk minimizers are denoted as DISPLAYFORM0 and DISPLAYFORM1 respectively. The estimated function from the random feature approximation is denoted as f R. The risk decomposition is discussed, focusing on the Function Approximation Error and the additional smoothness imposed via the RKHS norm constraint. The RKHS framework imposes smoothness through a norm constraint and a kernel parameter. The estimation error is defined using eigenvalues and a tuning parameter. The goal is to minimize the error by setting the parameter as small as possible. The estimation error for a RKHS estimator has a better dependency on n compared to other methods, especially for higher effective dimensions. Tighter bounds on the parameter could lead to better rates, but analyzing it for the specific kernel is challenging. The analysis of the kernel in Equation FORMULA12 is challenging due to the lack of an analytic form. The error from Random Feature Approximation can be bounded, focusing on the second term of empirical risk. The approximation error of the kernel leads to uniform convergence. Proposition 2 provides an approximation error in terms of kernel evaluation. The optimal solution of empirical risk minimization is considered, leading to Corollary 1 for guaranteeing a specific difference in empirical risk. The analysis focuses on bounding the error from Random Feature Approximation, leading to Corollary 1 which guarantees a specific difference in empirical risk. The proposed method is evaluated in various domains such as time-series, strings, texts, and images. The analysis focuses on dissimilarity measures and data characteristics in four domains: time-series, strings, texts, and images. Three well-known dissimilarity measures are discussed: Dynamic Time Warping for time-series, Edit Distance for strings, Earth Mover's distance for texts, and (Modified) Hausdorff distance for images. C-MEX programs were adapted for computational efficiency. Four datasets were selected for each domain, with varying lengths of observations for time-series data. The curr_chunk discusses datasets in four domains: time-series, strings, texts, and images. Time-series data have varying lengths of observations, string data have different alphabet sizes and string lengths, text data have varying document lengths, and image data have SIFT feature vectors of different sizes. The datasets were divided into train and test subsets, and compared against 5 baselines including KNN and distance substitution kernels. The curr_chunk discusses different distance substitution kernels and their computational complexities compared to a new method called D2KE, which has linear complexity in both the number of data samples and the length of the sequence. Parameters are optimized using 10-fold cross validation, and random samples are generated to achieve performance close to an exact kernel. The curr_chunk discusses the performance of D2KE compared to baseline methods in terms of classification accuracy and computation time. D2KE outperforms KNN and other distance substitution kernels, suggesting that a representation induced from a truly PD kernel makes better use of the data. In this work, a general framework is proposed for deriving a positive-definite kernel and feature embedding function from a dissimilarity measure between input objects. The framework is particularly useful for structured input domains like sequences, time-series, and sets. It subsumes existing approaches and suggests a new direction for creating embeddings based on distance to random objects. An extension could involve developing distance-based embeddings within a deep architecture for structured inputs in an end-to-end learning system. The text discusses deriving a positive-definite kernel and feature embedding function from a dissimilarity measure for structured input domains. It involves finding bounds using Hoefding's inequality and optimizing parameters through cross-validation. The framework suggests creating embeddings based on distance to random objects, with potential extensions for deep learning architectures. The study utilizes various kernels and methods for experiments, including random selection for data samples and a new method called D2KE. Linear SVM is used for embedding-based methods, while precomputed dissimilarity kernels are handled by LIBSVM. Datasets are sourced from public repositories, with detailed properties listed in a table. Computations are performed on a DELL system with Intel Xeon processors. The computations were carried out on a DELL system with Intel Xeon processors, using multithreading for distance computations. Gaussian distribution with parameters \u03c3 was used for all datasets. D2KE outperforms KNN in classification accuracy and computation time for multivariate time-series data. Our method outperforms other distance substitution kernels and KSVM on high-dimensional data like Auslan. Compared to RSM, our method samples random sequences to denoise and find patterns in the data, leading to a more abundant feature space. This approach is more efficient and effective in handling noise and redundant information in time-series data. Our method, D2KE, outperforms other distance-based baselines on high-dimensional data like Auslan. It uses Levenshtein distance as a distance measure to capture global alignments of strings. D2KE consistently performs better than other baselines, showing a clear advantage. Additionally, DSK_RBF with Levenshtein distance produces a kernel similar to our method's, indicating its effectiveness on large datasets. Our method, D2KE, achieves superior performance on large datasets compared to other baselines with quadratic complexity. It uses the earth mover's distance for text data and google pretrained word vectors. Random documents are generated for parameter optimization. D2KE outperforms DSK_RBF and DSK_ND on mnist-str8 with higher accuracy and significantly less runtime. Results show that D2KE outperforms other baselines on all datasets, with distance-based kernel methods performing better than KNN. D2KE also excels in document classification, especially with short random documents, achieving a significant speedup compared to other methods. For image data, the modified Hausdorff distance is used as the distance measure between images, showing excellent performance in the literature. Random images of SIFT-descriptors are generated for analysis. In experiments with random images of SIFT-descriptors, D2KE outperforms other baselines in various cases, showing superiority over KNN and RSM. The quadratic complexity of other methods makes scaling to large datasets challenging, while D2KE remains a strong alternative across applications."
}