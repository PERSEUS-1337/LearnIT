{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention for handling scenarios where the target domain may have unseen classes. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC) to address this issue by incorporating category-agnostic clusters in the target domain. These clusters provide domain-specific visual cues to improve adaptation for both closed-set and open-set scenarios. SE-CC utilizes target samples to create category-agnostic clusters, enhancing the learnt representation with mutual information maximization. It outperforms state-of-the-art approaches in open-set and closed-set domain adaptation on Office and VisDA datasets. Unsupervised domain adaptation addresses domain shift by leveraging labeled source samples and unlabeled target samples to generalize a target model. Existing models struggle in open-set scenarios due to the challenge of distinguishing unknown target samples from known ones. One approach is to use an additional binary classifier to assign known/unknown labels to target samples. In unsupervised domain adaptation, a novel approach is proposed to address the challenge of distinguishing unknown target samples from known ones. Instead of using a binary classifier, clustering is performed on all unlabeled target samples to explicitly model the diverse semantics of both known and unknown classes in the target domain. This method aims to create domain-invariant representations for known classes and discriminative representations for unknown and known classes in the target domain. In unsupervised domain adaptation, a new approach is introduced to distinguish unknown target samples from known ones by clustering all unlabeled target samples. This method aims to create domain-invariant representations for known classes and discriminative representations for both unknown and known classes in the target domain. To refine the learnt representations and preserve the inherent structure of the target domain, a new Self-Ensembling with Category-agnostic Clusters (SE-CC) is presented. This includes integrating an additional clustering branch into the student model to predict the cluster assignment distribution of each target sample and minimize the KL-divergence to enforce feature preservation. The SE-CC framework aims to enhance feature representation in unsupervised domain adaptation by minimizing KL-divergence and maximizing mutual information among input features, output distribution, and cluster assignment. Previous works have used methods like Maximum Mean Discrepancy (MMD) and domain confusion loss to learn domain-invariant representations in CNNs. The domain confusion loss is used to enforce domain invariance in representation learning. Open-set domain adaptation addresses scenarios with new and unknown classes in the target domain. Various approaches like adversarial training and subspace factorization have been proposed to tackle this challenge. In the target domain, a private subspace is tailored to model unknown class samples, while a shared subspace models known classes. Clustering is used to decompose unlabeled target samples into category-agnostic clusters, enhancing feature representation in both closed-set and open-set scenarios. SE-CC utilizes unlabeled target samples for learning task-specific classifiers in the open-set scenario, leveraging category-agnostic clusters for representation learning. The feature is driven to preserve the target data structure during domain adaption, enabling effective alignment of sample distributions within known and unknown classes. The preservation is exploited to enhance representation learning by maximizing mutual information among input features, clusters, and class probability distributions. In this paper, the Self-Ensembling with Category-agnostic Clusters (SE-CC) model is introduced for open-set domain adaptation. The goal is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown samples. The method leverages category-agnostic clusters to align sample distributions and enhance representation learning. French et al. (2018) introduced Self-Ensembling, building upon the Mean Teacher model for semi-supervised learning. The approach aims to ensure consistent classification predictions between teacher and student models under small input perturbations. The self-ensembling loss penalizes differences in classification predictions, with the teacher model's weights updated as an exponential moving average of the student's weights. Additionally, unsupervised conditional entropy loss is used to train the student's classification branch. The Self-Ensembling approach incorporates unsupervised conditional entropy loss to train the student's classification branch, aiming to move decision boundaries away from high-density regions in the target domain. The overall training loss includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data, balanced with tradeoff parameters. Open-set domain adaptation is challenging as it requires classifying both inliers and outliers into known and unknown classes. To address this, clustering is used to model diverse semantics in the target domain. Incorporating clustering to model diverse semantics in the target domain, the approach integrates category-agnostic clusters into Self-Ensembling for domain adaptation. Utilizing k-means clustering, the target samples are decomposed into clusters, revealing the underlying structure tailored to the domain. The learned feature representations aim to be domain-invariant for known classes and more discriminative for unknown and known classes in the target domain. The approach integrates category-agnostic clusters into Self-Ensembling for domain adaptation by representing target samples as output features of pre-trained CNNs. The inherent cluster distribution of each sample is measured through cosine similarities with cluster centroids, with a clustering branch in the student model designed to predict the distribution over all clusters. The clustering branch in the student model predicts the distribution over all clusters for cluster assignment of each target sample. It infers cluster assignment distribution based on input features and is trained with KL-divergence loss to preserve data structure for unknown and known classes. The student model in SE-CC predicts cluster assignments for target samples and is trained with KL-divergence loss to preserve data structure. It incorporates inter-cluster relationships to ensure similarity between semantically similar clusters. Mutual Information Maximization is used to enhance target feature learning in an unsupervised manner. The MIM module in the student model aims to estimate and maximize local and global mutual information among input feature maps, output classification distribution, and cluster assignment distribution. It encodes the output feature map into a global feature vector and concatenates it with classification and cluster assignment distributions for discrimination by a global Mutual information discriminator. The global Mutual Information discriminator is implemented with three stacked fully-connected networks and a softplus function. It estimates the probability of discriminating real input features with matched distributions. Local Mutual Information is also utilized to compare input features with classification and cluster assignment distributions at each spatial location. The input features are concatenated with the distributions and fed into the local Mutual Information discriminator for further discrimination. The local Mutual Information discriminator is constructed with three stacked convolutional layers and nonlinear activation. The final objective for the module integrates cross entropy loss, unsupervised self-ensembling loss, conditional entropy loss, KL-divergence loss, and local & global Mutual Information estimation on target data. Tradeoff parameters are used to balance the different losses. The SE-CC model is empirically validated through experiments on datasets like Office Saenko et al. VisDA. The SE-CC model is evaluated through experiments on the VisDA dataset, which consists of 280k images from three domains: synthetic images from 3D CAD models (training domain), real images from COCO (validation domain), and video frames from YTBB (testing domain). The synthetic images are used as the source and COCO images as the target for evaluation. Open-set adaptation is conducted with 12 known classes in both domains and unknown classes in the source and target domains. Three metrics (Knwn, Mean, Overall) are used for evaluation. The SE-CC model utilizes ResNet152 for clustering and adaptation in closed-set and open-set scenarios. Open-Set Domain Adaptation on Office is evaluated with different models, including AODA and SE-CC \u2666. SE-CC \u2666 learns classifiers without unknown source samples and outperforms other state-of-the-art models in closed-set and open-set adaptation. Our SE-CC model outperforms other closed-set and open-set adaptation models on various transfer directions, especially on challenging transfers like D \u2192 A and W \u2192 A. It leverages category-agnostic clusters to create domain-invariant features for known classes while effectively segregating target samples from known and unknown classes. Open-set adaptation techniques like AODA, ATI-\u03bb, and FRODA excel by excluding unknown target samples during domain adaptation, surpassing models like RTN and RevGrad. This highlights the importance of aligning data distributions between source and target domains while excluding unknown samples for successful open-set adaptation. Our SE-CC model surpasses other closed-set and open-set adaptation models by leveraging category-agnostic clusters to create domain-invariant features for known classes. It outperforms state-of-the-art techniques like AODA, ATI-\u03bb, and FRODA in both open-set and closed-set scenarios, demonstrating the advantage of exploiting the underlying data structure in target domains. The SE-CC model utilizes Conditional Entropy (CE), KL-divergence Loss (KL), and Mutual Information Maximization (MIM) to improve performance in open-set domain adaptation. CE enhances classifier performance, KL aligns cluster distributions, and MIM maximizes mutual information. Performance improvements on VisDA show CE as a general enhancement, while KL and MIM in SE-CC contribute to a total performance boost of 4.2%. The SE-CC model utilizes category-agnostic clusters for domain adaptation, separating unknown target samples from known ones. Clustering decomposes target samples into clusters, aligning cluster distributions to preserve data structure. Mutual information is used to enhance learned features. The SE-CC model utilizes category-agnostic clusters for domain adaptation, separating unknown target samples from known ones. Clustering decomposes target samples into clusters, aligning cluster distributions to preserve data structure. Mutual information is used to enhance learned features. The implementation of SE-CC is mainly developed with PyTorch and optimized with SGD. Performance improvements are observed when comparing to state-of-the-art techniques. The SE-CC model uses KL-divergence for clustering branch evaluation and mutual information maximization. Different variants of MIM module are compared, showing improvements in performance. The hyper-parameter search is restricted for each transfer in a specific range. The SE-CC model utilizes KL-divergence for clustering branch evaluation and mutual information maximization, leading to improved performance by exploiting mutual information between input features and output. By combining outputs from both branches, CLS+CLU achieves a significant performance boost. SE helps align source and target distributions for domain-invariant representation, but struggles with recognizing unknown target samples. In contrast, SE-CC preserves the target data structure, separating unknown from known target samples while maintaining indistinguishability between known samples in different domains."
}