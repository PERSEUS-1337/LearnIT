{
    "title": "Byl5NREFDr",
    "content": "In the context of model extraction in natural language processing, an adversary can reconstruct a victim model without real training data by using random word sequences and task-specific heuristics. This exploit is possible due to the prevalence of transfer learning methods in NLP. Defense strategies like membership classification and API watermarking can be effective but may be circumvented by clever adversaries. Machine learning models are valuable intellectual property, often only accessible through web APIs. Malicious users may attempt \"model stealing\" by training a local copy of the model using input-output pairs collected from queries. Extracted models can leak sensitive information or be used to generate adversarial examples. NLP APIs based on ELMo and BERT have become popular. In this paper, it is demonstrated that NLP models fine-tuned from BERT can be extracted without access to training data. Extraction attacks are possible with randomly sampled queries and simple heuristics, contrasting with prior work that required access to relevant data. Extraction attacks on NLP models can be conducted using randomly sampled queries and simple heuristics, without the need for access to training data. The process involves querying a victim BERT model and fine-tuning one's own model based on the predicted answers. Despite the nonsensical nature of the queries, they are effective in extracting good models, with queries closer to the original data distribution yielding better results. In the study, it was found that pretraining on the attacker's side facilitates model extraction. Two defenses against extraction - membership classification and API watermarking - were tested, showing effectiveness against simple adversaries but failing against more sophisticated ones. The research aims to inspire stronger defenses against model extraction and enhance understanding of vulnerabilities in models and datasets. The work is linked to prior studies on model extraction, particularly in computer vision, and involves synthesizing queries for extracting models, relating to zero-shot distillation and NLP system input studies. Prior work on model extraction in NLP systems focused on pool-based active learning for selecting natural sentences from WikiText-2. In contrast, this study explores extraction on modern BERT-large models using nonsensical inputs for tasks like question answering. It is related to data-efficient distillation methods but differs in that it does not require white-box access to the teacher model. Rubbish inputs, randomly-generated examples yielding high-confidence predictions, have been studied in the model extraction literature. In contrast to prior work on model extraction using natural inputs, this study focuses on using nonsensical inputs to train BERT-large models for NLP tasks. BERT is a 24-layer transformer that contextualizes word sequences into vector representations. The effectiveness of unnatural inputs in training models is demonstrated, showing success in tasks without real examples during training. BERT's parameters are learned through masked language modeling on natural text. The release of BERT revolutionized NLP by achieving state-of-the-art performance with minimal supervision. Fine-tuning is done by using a task-specific network to construct a composite function. In extraction attacks, a malicious user attempts to reconstruct a model without training data by using nonsensical word sequences as queries. The attacker uses nonsensical word sequences as queries to train a model gT, fine-tuning a public release of fbert,\u03b8* on the resulting dataset. Extraction attacks are conducted on various NLP tasks with different input and output spaces, using query generators RANDOM and. The study explores two query generators, RANDOM and WIKI, for training models on complex tasks. Additional heuristics are applied for tasks like MNLI and SQuAD/BoolQ to enhance query generation. Representative example queries and outputs are provided in Table 1. The study evaluates extraction procedure accuracy using different query budgets and metrics. Commercial cost estimates are provided for query budgets using Google Cloud Platform's Natural Language API calculator. Extracted models show high accuracies on original development sets even with nonsensical inputs. The study evaluates extraction procedure accuracy using different query budgets and metrics. Extracted models show high accuracies on original development sets even with nonsensical inputs. However, agreement between extracted models is only slightly better than accuracy, and is lower on held-out sets. Functional equivalence between victim and extracted models is poor, as indicated by low agreements on SQuAD. An ablation study with alternative query generation heuristics for SQuAD and MNLI is conducted. Classification with argmax labels only is also explored, showing varying results on different datasets. In analyzing model extraction accuracy with varying query budgets, it is observed that even with small budgets, extraction can be successful. The effectiveness of nonsensical input queries in the extraction process is highlighted, raising questions about the properties that make them suitable for extraction. The study delves into understanding these queries and their impact on model extraction without relying on large pretrained language models. In analyzing model extraction accuracy with varying query budgets, it is observed that even with small budgets, extraction can be successful. The study delves into understanding the properties of nonsensical queries used in the extraction process to determine their impact on model performance. The research specifically examines the RANDOM and WIKI extraction configurations for SQuAD to investigate the agreement between victim models on answers to nonsensical queries. The models show high agreement on SQuAD training and development set queries but significantly lower agreement on WIKI and RANDOM queries, indicating a deviation from the original data distribution. The study explores the impact of high-agreement queries on model extraction accuracy. High-agreement queries show significant improvements in F1 scores compared to random and low-agreement subsets. This suggests that agreement between victim models is a good indicator of input-output pair quality for extraction. The research also raises questions about the interpretability of high-agreement nonsensical queries to humans and suggests further investigation into leveraging this observation for better extraction methods in the future. The study investigates the impact of high-agreement nonsensical queries on model extraction accuracy. Annotators matched victim models' answers 23% on WIKI and 22% on RANDOM subsets, but scored significantly higher on original SQuAD questions (77% exact match). Annotators used a word overlap heuristic to select answer spans, but most nonsensical question-answer pairs remain mysterious to humans. In practical scenarios, the attacker's lack of information about the victim's architecture raises questions about model extraction accuracy. Comparing different base models and pretraining setups, it is observed that starting from BERT-large yields higher accuracy. Training from scratch gives attackers a significant advantage, emphasizing the importance of pretraining in model extraction. Training a QANet model on SQuAD without pretraining shows a significant drop in F1 score when using nonsensical queries, highlighting the importance of starting from a good representation of language. BERT-based models are vulnerable to model extraction, prompting the investigation of defense strategies that preserve API utility while remaining undetectable to attackers. The defense strategy against model extraction involves using membership inference to detect nonsensical or adversarial inputs, issuing random outputs to eliminate extraction signals. Membership inference is treated as a binary classification problem, utilizing datasets labeled as real or fake examples. The classifier is trained using logits and final layer representations of the victim model for effective detection. The curr_chunk discusses the use of model confidence scores and rare word representations for membership inference, showing that classifiers transfer well to balanced development sets and are robust to query generation processes. A defense strategy against extraction is watermarking, where a fraction of queries are modified to return incorrect outputs to detect memorization by deep neural networks. The curr_chunk discusses the effectiveness of watermarking as a defense strategy against extraction in deep neural networks. Watermarking involves modifying a fraction of queries to return incorrect outputs, revealing memorization by the models. The study shows that watermarked models perform differently from non-watermarked models, especially on the watermarked subset of the training data. Model extraction attacks against NLP APIs serving BERT-based models are effective, even with nonsensical input queries. Fine-tuning large pretrained language models makes extraction easier for attackers. Existing defenses are generally inadequate, requiring further research for robust defenses against adaptive adversaries. Future directions following the results in this paper include leveraging nonsensical inputs to improve model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity. Cost estimates from Google Cloud Platform's Calculator were used in the study. Cost estimates for entity and sentiment analysis APIs vary depending on factors such as computing infrastructure and revenue models. Attackers could exploit free query budgets or emulate HTTP requests for large-scale data extraction. It is crucial to focus on the low costs of extracting datasets rather than specific estimates. The cost estimates for entity and sentiment analysis APIs can vary based on factors like computing infrastructure and revenue models. It is important to focus on the low costs of extracting datasets rather than specific estimates. For tasks like machine translation and speech recognition, the costs are relatively inexpensive. For example, it costs -$430.56 to extract a large conversational speech recognition dataset and $2000.00 for 1 million translation queries. Input generation algorithms are detailed for each dataset, such as building a vocabulary using wikitext103 and randomly sampling tokens. The process involves replacing words not in the top-10000 wikitext103 vocabulary with random words from the vocabulary. The final hypothesis is constructed by randomly choosing words from the premise and replacing them with other random words from the vocabulary. This process is repeated three times. Additionally, a vocabulary is built using wikitext103 for paragraph construction, where tokens are sampled from the vocabulary based on chosen lengths. In Table 11, various extraction datasets for SQuAD 1.1 are compared. Findings show that starting questions with common question starter words like \"what\" improves performance, especially with RANDOM schemes. Starting questions with common question starter words like \"what\" improves performance, especially with RANDOM schemes. An ablation study on MNLI showed that lexical overlap between premise and hypothesis affects model predictions. Using frequent words aids extraction. Human studies involved fifteen annotators annotating question sets, including original SQuAD questions. In an ablation study on input features for the membership classifier, two candidates were considered: the logits of the BERT classifier and the last layer representation. Results showed that the last layer contains lexical, syntactic, and semantic information about the inputs. The study also revealed that the average pairwise F1 follows a specific order, indicating closeness to the actual input distribution. The ablation study compared the effectiveness of using the last layer representations versus the logits in distinguishing between real and fake inputs. Results showed that both feature sets together yielded the best results. Last layer representations were found to be more effective in classifying points as real or fake."
}