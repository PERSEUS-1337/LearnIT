{
    "title": "SJeQi1HKDH",
    "content": "In this work, social influence is integrated into reinforcement learning to enable agents to learn from both the environment and peers. The Interior Policy Differentiation (IPD) algorithm encourages agents to develop unique policies while solving tasks, leading to improved performance and diverse behaviors. Reinforcement Learning (RL) involves learning through interaction with the environment to maximize rewards, inspired by animal studies and cognition. Biodiversity and skill development are essential for evolution and continuation. In this work, the focus is on policy differentiation in RL to enhance agent diversity while maintaining task-solving abilities. Previous approaches include designing rich environments for varied skills and encouraging exploration beyond reward maximization. The concept is inspired by social influence in animal societies. The concept of social influence in reinforcement learning is inspired by animal societies. The learning scheme involves the target agent maximizing rewards while differentiating its actions from other agents. Social influence is implemented as social uniqueness motivation, treated as a constrained optimization problem. A policy distance metric is defined to compare agent similarity, and an optimization constraint is developed for immediate feedback. Interior Policy Differentiation (IPD) is introduced as a novel method for learning with social influence. The curr_chunk discusses the use of intrinsic motivation methods in reinforcement learning, such as Variational Information Maximizing Exploration (VIME) and curiosity-driven methods. These methods aim to tackle sparse reward problems by adding intrinsic reward terms to encourage exploration. In reinforcement learning, various methods like Random Network Distillation (RND) and Competitive Experience Replay (CER) have been proposed to quantify intrinsic rewards and address the trade-off between external and intrinsic rewards. The Task-Novelty Bisector (TNB) learning method aims to optimize both types of rewards by updating the policy in the direction of the angular bisector of the gradients of the extrinsic and intrinsic objectives. The foundation of joint optimization for extrinsic and intrinsic objectives in reinforcement learning is not solid. Additional computation expenses are required for creating intrinsic reward functions and evaluating novelty. Different RL algorithms may converge to different policies for the same task. Rich environments can encourage diverse locomotion behaviors, but extra manual efforts are needed in designing such environments. In reinforcement learning, algorithms based on policy gradient converge to a local optimum in Pitfall, while off-policy and value-based algorithms learn sophisticated strategies. This paper focuses on learning different policies with a single algorithm to avoid local optima. Kurutach et al. (2018) use an ensemble of deep neural networks to maintain model uncertainty. A metric is defined to measure policy differences, forming the basis for the proposed algorithm. Learned policies are denoted as \u03b8i, with parameters \u03b8i \u2208 \u0398. The metric space satisfies identity, symmetry, and triangle inequality properties. The Total Variance Divergence is used to measure policy distance in reinforcement learning. It can be extended to continuous spaces and is calculated using Monte Carlo estimation. The goal is to maximize the uniqueness of new policies within existing ones. Sampling from continuous states can be challenging due to the need for sufficient samples. The Total Variance Divergence is a measure of policy distance in reinforcement learning, extended to continuous spaces using Monte Carlo estimation. To improve sample efficiency, it is proposed to approximate \u03c1(s) using a fixed behavior policy \u03b8. This approximation requires that the domain of possible states is similar between different policies. In practice, this condition is typically met by adding noise to \u03b8. Sampling from S\u03b8 \u222a S\u03b8j is necessary to satisfy the properties in Definition 1. The last term in policy differentiation is related to the domain S \u03b8. Sufficient exploration and initialization can make this term disappear. Proposition 1 states that estimating \u03c1 \u03b8 (s) using a single trajectory \u03c4 is unbiased. The learning algorithm aims to maximize the expectation of cumulative rewards g = t=0 \u03b3 t r t. To enhance behavioral diversity, the learning objective considers both the reward from the primal task and the policy uniqueness. Previous approaches combine the reward from the primal task with the intrinsic reward g int = t=0 \u03b3 t r int,t. The objective is to balance the primal task reward and intrinsic reward, with a weight parameter \u03b1. The selection of \u03b1 and formulation of intrinsic reward r int are crucial. To address this, the optimization problem is transformed into a constrained one, with a threshold r 0 for minimal uniqueness. This approach aims to treat uniqueness as a constraint rather than an additional target, inspired by social uniqueness motivating people passively. The optimization problem involves balancing task and intrinsic rewards with a weight parameter \u03b1. The approach transforms the problem into a constrained one with a threshold r 0 for minimal uniqueness. The work proposes solving the constrained optimization problem using Interior Point Methods (IPMs) instead of Task Novel Bisector (TNB) methods. This method avoids the computational challenges and numerical instability associated with directly applying IPMs. In a proposed RL paradigm, the learning process is influenced by peers, allowing for a more natural way to handle the computational challenges of applying IPMs. By bounding collected transitions within a feasible region using previously trained policies, new agents are terminated if they step outside this region. This ensures that all valid samples collected during training are unique, eliminating the need to balance intrinsic and extrinsic rewards. The approach, inspired by IPMs, results in a more robust learning process without objective inconsistency. The Interior Policy Differentiation (IPD) method is demonstrated on the MuJoCo environment in OpenAI Gym, specifically on locomotion environments like Hopper-v3, Walker2d-v3, and HalfCheetah-v3. Experiments show that different policies can be generated by selecting different random seeds. The proposed method is based on PPO and compared with TNB and weighted sum reward methods. The proposed Interior Policy Differentiation (IPD) method is compared with TNB and weighted sum reward (WSR) approaches in generating unique policies in locomotion environments. The uniqueness metric is utilized directly in learning new policies without reshaping. Qualitative results are visualized in Fig.2, showing the motion of agents in different time steps. The visualization of experimental results in terms of uniqueness and performance is presented. The proposed method outperforms others in Hopper and HalfCheetah environments. Detailed comparisons on task-related rewards are provided in Table 1, with performance depicted in Fig.5 and Fig.6. Success rate is also used as a metric for comparison. Our method consistently outperforms the PPO baseline in terms of success rate, ensuring improved performance in the Hopper and HalfCheetah environments. The proposed approach prevents policies from falling into suboptimal behaviors, leading to noticeable performance improvements. Our proposed method prevents policies from getting stuck in local minima, encouraging exploration of different action patterns to improve performance. This enhancement in traditional RL schemes is exemplified in the HalfCheetah environment, where agents receive termination signals from peers to avoid repetitive actions and high control costs. Our method encourages exploration of diverse strategies by allowing agents to receive termination signals from peers, preventing them from acting randomly and avoiding heavy control costs. The learning process involves imitating previous policies before pursuing higher rewards, creating an implicit curriculum. The difficulty of finding unique policies increases as the number of learned policies with social influence grows. The performance decrease is more pronounced in the Hopper environment due to its limited 3-dimensional action space. Our approach motivates RL to learn diverse strategies inspired by social influence. Our proposed method, Interior Policy Differentiation (IPD), utilizes Interior Point Methods to encourage RL to learn diverse strategies inspired by social influence. Experimental results show that IPD can help agents avoid local minimums and learn various well-behaved policies. While other methods may optimize uniqueness reward directly, this can lead to a decrease in task-related performance. Careful hyper-parameter tuning and reward shaping are necessary to address this trade-off. In our proposed method, Interior Policy Differentiation (IPD), we use deterministic policies in the calculation of DTV and MLP with 2 hidden layers for actor models in PPO. Training timesteps are fixed for each task, and the constraint threshold r0 can control policy uniqueness. Different thresholds result in varied policy behaviors. In the proposed method IPD, deterministic policies are used with 2 hidden layers for actor models in PPO. The constraint threshold r0 controls policy uniqueness, leading to varied policy behaviors. Larger thresholds may result in poorer performance as the learning algorithm struggles to find feasible solutions. Constraints are based on cumulative uniqueness rather than forcing every action to be different. Performance under different thresholds is analyzed in Fig. 9 and Table 2. Constraints can be applied after a certain number of timesteps for long-term differences. The WSR, TNB, and IPD methods are approaches in constrained optimization problems. The Penalty Method handles constraints by incorporating them into a penalty term and solving the unconstrained problem iteratively. The Feasible Direction Method finds a direction that satisfies the constraints for small increments. The selection of parameters like \u03b1 in WSR significantly impacts the final solution. The TNB method considers constraints by finding a direction that satisfies them for small increments. The learning stride is fixed, leading to reliance on the selection of the shape of g. Introducing a barrier term can help optimize the objective, with the solution getting closer to the primal objective as the barrier factor decreases. Choosing a sequence of decreasing barrier factors can improve the optimization process. Applying a more natural method to bound feasible transitions during training can lead to a new policy with sufficient uniqueness. This eliminates the need to balance intrinsic and extrinsic rewards, making the learning process more robust and avoiding objective inconsistency. The pseudo code of IPD based on PPO is shown in Algorithm.1, with additional lines for the new method."
}