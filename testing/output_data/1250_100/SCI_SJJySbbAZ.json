{
    "title": "SJJySbbAZ",
    "content": "We propose using Optimistic Mirror Decent (OMD) for training Wasserstein GANs to address limit cycling behavior. OMD has faster regret rates in zero-sum games, which is the context of WGANs. It resolves the limit cycling problem and converges to an equilibrium in bi-linear zero-sum games. OMD outperforms GD variants in training models for generating DNA sequences in bioinformatics, achieving smaller KL divergence. Additionally, we introduce Optimistic Adam as an optimistic variant of optimization algorithms. Generative Adversarial Networks (GANs) have been successful in fitting generative models in complex spaces like image distributions. GANs involve a zero-sum game between a Generator and a discriminator, where the Generator learns to generate samples similar to the data distribution. A new algorithm, Optimistic Adam, is introduced as an optimistic variant of Adam for WGAN training on CIFAR10, showing improved performance in terms of inception score. Despite their success in generating visually appealing samples for image generation tasks, GANs are difficult to train due to the instability of the training process. Training GANs involves solving a zero-sum game using a variant of Stochastic Gradient Descent algorithm for both players. However, no-regret dynamics in zero-sum games can often lead to oscillatory behavior instead of convergence to an equilibrium. Recent theoretical results show that no variant of GD falling under Follow-the-Regularized-Leader algorithms can converge to an equilibrium in terms of the last-iterate. In this paper, the authors propose training GANs, specifically Wasserstein GANs, using Optimistic Mirror Descent (OMD) to achieve faster convergence rates in zero-sum games. OMD takes advantage of the predictability of the opponent's strategy to reach equilibrium faster than traditional gradient descent algorithms. The authors propose using Optimistic Mirror Descent (OMD) for training GANs, showing faster convergence rates in zero-sum games. OMD offers better worst-case guarantees compared to Gradient Descent (GD) and can lead to stability and last-iterate convergence in GAN training. The theoretical result demonstrates that OMD converges to an equilibrium in bi-linear games, providing strong evidence for its effectiveness. Additionally, simulations highlight the qualitative difference between OMD and GD, emphasizing the potential performance improvements in distribution learning tasks. In a distribution learning setting, Optimistic Mirror Descent (OMD) outperforms Gradient Descent (GD) in learning DNA sequence distributions for cellular functions. OMD shows better performance in terms of Kullback-Leibler (KL) divergence between the learned and true distributions. The Optimistic Adam algorithm is introduced for training GANs for images, demonstrating improved results. The Optimistic Adam algorithm is proposed for training GANs on CIFAR10 images, outperforming Adam in terms of inception score. It aims to learn a generative model using a deep neural network and focuses on faster convergence rates using the last iteration's gradient as a predictor for the next iteration's gradient. The update rule of Optimistic Mirror Descent (OMD) is a small adaptation to Gradient Descent (GD) and can be used in training WGANs. OMD is parameterized by a predictor of the next iteration's gradient, which could be the last iteration's gradient or an average of past gradients. This modification in the GD update rule differs from existing adaptations in GAN training. The intuition behind OMD can be understood through the Follow-the-Regularized-Leader formulation, where GD is equivalent to the algorithm with a regularizer. Optimistic Mirror Descent (OMD) augments FTRL by adding a predictor of the next iteration's gradient. In practice, the true distribution Q is replaced with an empirical distribution Qn over samples and random noise samples Fn. Computing the gradient of the expected loss with respect to Qn or Fn may be impractical, but using unbiased estimators can still lead to small loss. Replacing gradients with unbiased estimators can lead to small loss. Expectation with respect to Qn or Fn can be replaced by evaluating gradients at a single sample or small batch. In a WGAN example, the goal is for the generator to learn the unknown parameter v. The WGAN loss simplifies when optimizing true expectations rather than relying on samples. The expected zero-sum game involves the generator choosing \u03b8 = v and the discriminator choosing w = 0. Gradient descent (GD) dynamics lead to a limit cycle, while Optimal Mirror Descent (OMD) dynamics converge to v in terms of the last iterate. OMD stability remains even with Stochastic Gradients as long as the batch size is decent. Adding gradient penalty and Nesterov momentum to the game loss improves stability in GD and OMD dynamics. Training with OMD converges to equilibrium in terms of last-iterate convergence, unlike GD which leads to a non-vanishing limit cycle. OMD dynamics converge to min-max solutions for bilinear functions. In Appendix D, the convergence of Optimistic Mirror Descent for the min-max problem with bilinear functions is extended to include terms linear in players' strategies. The initialization of the algorithm requires specific choices for x and y, with the assumption that \u03bb \u221e \u2264 1. Scaling the matrix A does not affect the min-max solutions. Optimistic Mirror Descent converges to equilibrium solutions of the minimax problem, where x is in the null space of A^T and y is in the null space of A. Gradient Descent diverges for the problem min x max y x^T y, even when A is the identity matrix. The main result shows convergence of OMD with specific initialization and conditions on A and \u03b7. Optimistic Mirror Descent (OMD) converges to equilibrium solutions of the minimax problem with specific initialization and conditions. OMD dynamics show convergence as t approaches infinity, with the last iterate being a certain distance from the equilibrium space. OMD is applied to generating DNA sequences from observed distributions, specifically using a position-weight matrix (PWM) to model probabilities of nucleotides. This serves as a practical problem to evaluate training methods in terms of KL divergence. In experiments, 40,000 DNA sequences were generated using a position weight matrix. WGANs with different optimization methods were trained and evaluated based on KL divergence. CNNs were used as the discriminator and generator. Model selection considered learning rate and training epochs. The study evaluated WGANs with various optimization methods by training 50 independent models for each learning rate and optimizer. Different strategies like momentum, Nesterov momentum, and Adagrad were compared using KL divergences. Training schemes included training the discriminator 5 times. The study compared different optimization methods for WGANs, training 50 models for each learning rate and optimizer. Training schemes involved training the discriminator multiple times to optimize performance. Momentum, Nesterov momentum, and Adagrad were used with stochastic variants experimented on. WGAN trained with Stochastic OMD achieved lower KL divergence compared to other SGD variants. Optimistic Adam outperforms other SGD variants in training WGANs on images, achieving better KL divergence with a 1:1 generator-discriminator training ratio. The success of Adam in training image WGANs led to the use of an optimistic version of the algorithm. Parameters for training WGANs on CIFAR10 images with Optimistic Adam are outlined, emphasizing its potential beyond WGAN training. Optimistic Adam, with hyper-parameters matched to Gulrajani et al. (2017), outperforms Adam in terms of inception score for WGANs. It achieves high scores after few training epochs by training the discriminator once after each generator iteration. Vanilla Adam performs poorly with this approach, even when trained 5 times in between generator iterations. The same learning rate and betas as Gulrajani et al. (2017) were used for all. In the study, various modifications of Gradient Descent training were compared using specific hyper-parameters and update rules. The Wasserstein GAN relies on the discriminator approximating 1-Lipschitz functions, requiring bounded gradients. Weight-clipping is one method to achieve this but may introduce instability during training. Gulrajani et al. (2017) proposed a regularized WGAN loss by adding a penalty to the loss function of the zero-sum game. This penalty is the 2 norm of the gradient of D w (x) with respect to x. The gradient penalty modified WGAN maintains the gradient of the modified loss function with respect to \u03b8 unchanged, but modifies the gradient with respect to w. Nesterov's momentum in GD is identical to momentum in the absence of gradient penalty due to the bilinear nature of the function. Asymmetric training can help reduce cycling in GANs by training the discriminator more frequently than the generator. This approach brings the generator's problem closer to convex minimization, improving convergence. However, it does not completely eliminate cycling, but rather reduces its range. The text discusses different gradient descent dynamics variations with added penalties and momentum to improve GAN training. It also mentions the benefits of using OMD over GD in a simple illustrative example with a mean zero multi-variate normal data distribution. The text discusses gradient descent dynamics variations with penalties and momentum to enhance GAN training. It highlights the benefits of using OMD over GD, especially in cases with non-convexity and multiple optimal solutions. In the context of enhancing GAN training, the text discusses the use of regularization and momentum in gradient descent dynamics. It demonstrates how OMD can stabilize dynamics and converge pointwise, contrasting the instability of GD under stochastic dynamics. The goal of this section is to show that Optimistic Mirror Descent exhibits last iterate convergence to min-max solutions for bilinear functions. OMD takes a specific form for the min-max problem, with certain initializations required for meaningful iterations. Further notation is introduced for the proof of Theorem 1. The text introduces further notation and proves various theorems related to the dynamics of a specific problem. Theorem 3 states conditions under which certain properties hold for matrices A and A^T. The text presents proofs for various theorems related to the dynamics of a specific problem, including conditions under which certain properties hold for matrices A and A^T. The proofs involve equations and lemmas to establish the validity of the theorems. The proof involves using Lemmas 4, 5, and 6, induction hypothesis, and small enough \u03b7 to show the basis of the induction and complete the inductive step. It demonstrates that H(i, 3) holds for all i \u2208 N by showing the distance from the equilibrium space of the game for small enough \u03b7. The proof of Corollary 7 shows that solutions to the min-max problem exhibit last iterate convergence under certain conditions. Theorem 8 further elaborates on this by considering OMD dynamics and characterizing its motion when certain conditions are met. The text discusses the dynamics of OMD and proves convergence under certain conditions. It considers variable substitutions and decompositions of b and c to analyze the min-max problem. Theorem 8 elaborates on the last iterate convergence of solutions to the problem. The text discusses the dynamics of OMD and proves convergence under certain conditions. It considers variable substitutions and decompositions of b and c to analyze the min-max problem. Theorem 8 elaborates on the last iterate convergence of solutions to the problem. In particular, whenever (30) is finite, OMD exhibits last iterate convergence within a certain distance from the equilibrium points of the game. If (30) is infinite or undefined, the OMD dynamics travel to infinity linearly with fluctuations around the divergence. The text discusses the dynamics of OMD and proves convergence under certain conditions. It considers variable substitutions and decompositions of b and c to analyze the min-max problem. Theorem 8 elaborates on the last iterate convergence of solutions to the problem. In particular, whenever (30) is finite, OMD exhibits last iterate convergence within a certain distance from the equilibrium points of the game. If (30) is infinite or undefined, the OMD dynamics travel to infinity linearly with fluctuations around the divergence. The update rule for time t \u2212 2 is derived using norm calculations and defined variables, leading to the final desired result. Proof of Lemma 4: To prove this, first consider the trivial inequality."
}