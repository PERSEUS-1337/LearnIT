{
    "title": "B1xFhiC9Y7",
    "content": "Predicting structured outputs like semantic segmentation requires expensive per-pixel annotations for training convolutional neural networks. To address the challenge of generalizing to unlabeled domains, a domain adaptation method is proposed. This method involves learning discriminative feature representations of patches based on label histograms in the source domain and using an adversarial learning scheme to align feature distributions between source and target patches. The framework achieves state-of-the-art performance on semantic segmentation and is validated through experiments on various benchmark datasets. Recent deep learning methods have made progress in vision tasks like object recognition and semantic segmentation, relying on large-scale annotations for training. Domain adaptation methods have been developed to bridge the gap between annotated source domains and unlabeled target domains. While there have been advancements in image classification, there is still room for improvement in domain adaptation for pixel-level prediction tasks like semantic segmentation. This is crucial as annotating ground truth data is costly, especially for tasks like road-scene images with varying appearance distributions across cities and changing conditions within the same city. Existing methods use feature-level or output space adaptation to align distributions between source and target domains. The current chunk discusses the use of patch-level alignment in domain adaptation to address misalignment issues that may arise from differences in camera pose or field of view. By matching patches that are more likely to be shared across domains, the approach aims to improve adaptation accuracy. This method leverages adversarial learning to align patch distributions and considers label histograms of patches as a factor for discriminative learning. The proposed patch-level alignment method aims to improve adaptation accuracy by aligning source and target domain patches using discriminative representations and adversarial modules. It utilizes pixel-level annotations to extract label histograms for patch-level representation and applies K-means clustering to group patches into clusters for training a shared classifier across domains. The proposed method aligns source and target domain patches using discriminative representations and adversarial modules to improve adaptation accuracy. It utilizes pixel-level annotations for patch-level representation and applies K-means clustering to train a shared classifier across domains. Experiments include synthetic-to-real and cross-city scenarios, showing favorable performance with global and patch-level alignments. The proposed framework combines global and patch-level alignments to improve adaptation accuracy in structured output prediction. It outperforms state-of-the-art methods in accuracy and visual quality. The framework is general and can be applied to other structured outputs like depth. Key contributions include a domain adaptation framework utilizing adversarial learning modules, discriminative representations guided by label histograms, and favorable performance against baselines in semantic segmentation. Domain adaptation methods for image classification and pixel-level prediction tasks are discussed, along with algorithms for learning disentangled representations. Domain adaptation methods for image classification involve aligning feature distributions between domains using hand-crafted or deep features and adversarial learning. Recent work focuses on enhancing feature representations through pixel-level transfer and domain separation. In contrast, domain adaptation for structured pixel-level predictions, like semantic segmentation, is less explored. A study on adapting semantic segmentation for road-scene images from synthetic to real data introduces adversarial networks to align global feature representations across domains. The CDA method BID36 uses SVM classifier to capture label distributions on superpixels for domain adaptation. Class-wise domain adversarial alignment is performed by assigning pseudo labels to target data. Object prior from Google Street View aids alignment for static objects. The proposed framework learns discriminative representations for patches for patch-level alignment without requiring additional priors/annotations. Our algorithm focuses on learning patch-level representations that aid the alignment process by training the entire network in an end-to-end fashion. Learning a latent disentangled space has led to a better understanding for tasks such as facial recognition, image generation, and view synthesis. Various approaches use pre-defined factors to learn interpretable representations of the image, focusing on factors like pose and lighting for rendering 3D images. Our proposed domain adaptation framework focuses on learning discriminative representations for patches to aid in aligning distributions across domains. By leveraging label distributions as a disentangled factor, our framework eliminates the need to pre-define factors. The goal is to align the predicted output distribution of target data with the source distribution using supervised learning and adversarial loss. Additionally, a classification loss in a clustered space is incorporated to learn patch-level representations. The proposed framework focuses on learning discriminative patch-level representations to align distributions between source and target data. It incorporates classification and adversarial losses to achieve this goal, with a structured prediction and discriminative representation on source data. The adaptation task includes global and patch-level adversarial loss functions to align target distributions. The baseline model consists of a supervised cross-entropy loss and an output space adaptation module. The proposed framework utilizes supervised cross-entropy loss and an output space adaptation module for global alignment. It includes an adversarial loss for discriminating between source and target images. Patch-level alignment is emphasized for transferable structured output representations. The proposed framework emphasizes patch-level domain alignment by clustering patches from the source domain and guiding patches from the target domain to adapt to the disentangled space of source patch representations. This approach aims to learn discriminative patch representations for transferable structured output. In this work, per-pixel annotations in the source domain are used to construct a semantically disentangled space of patch representations. Label histograms for patches are utilized as the disentangled factor, achieved by sampling patches, extracting spatial label histograms, and applying K-means clustering. A classification module is added after the predicted output to incorporate the clustered space during training the network on source data. The learned representation F s is obtained through the softmax function and corresponds to patches of the input image. The learning process involves constructing a clustered space with a cross-entropy loss. Patch-level Adversarial Alignment aims to align target patches to the clustered space using an adversarial loss between F s and F t. The reshaped data F is used to align patches regardless of their location in the image. The adversarial objective is formulated to classify feature representations from the source or target domain. The network optimization involves updating discriminators Dg and Dl, and networks G and H in a min-max problem. Discriminators are trained to distinguish between source and target distributions, while the network aims to align target distribution with the source using optimized discriminators. The minimization problem in FORMULA6 involves combining supervised loss functions FORMULA1 and FORMULA4 with adversarial loss functions. The discriminator Dg uses a spatial map O as input with fully-convolutional layers, while Dl utilizes a K-dimensional vector with fully-connected layers. The generator consists of layers that enhance feature representations during testing, with runtime unaffected compared to the baseline approach. The generator in the proposed architecture includes a categorization module H and uses leaky ReLU activation with channel numbers {256, 512, 1}. The framework is implemented using PyTorch on a single Titan X GPU with specific optimization parameters for training the discriminators and generator. The initial learning rate is 2.5 \u00d7 10 \u22124 for the ablation study on GTA5-to-Cityscapes using the ResNet-101 network. Learning rates are decreased using polynomial decay with a power of 0.9. Hyper-parameters such as \u03bb d = 0.01, \u03bb g adv = \u03bb l adv = 0.0005 and K = 50 are used for all experiments. The model is first trained using loss L s for 10K iterations before training with all loss functions for 100K iterations. The proposed framework for domain adaptation is evaluated on semantic segmentation, showing favorable performance against state-of-the-art approaches on benchmark datasets. The study adapts synthetic datasets to real scenarios, including cross-city and weather condition variations. Cityscapes images are adapted to the Oxford RobotCar dataset for rainy scenes. A total of 895 training images and 271 test images are annotated for evaluation. Intersection-over-union (IoU) ratio is used as the evaluation metric. An ablation study on GTA5-to-Cityscapes scenario is conducted to analyze the impact of different loss functions and design choices. The study analyzes the impact of different loss functions and design choices in a proposed framework. Adding disentanglement without alignments improves performance. Combining global and patch-level alignments achieves the highest IoU. Losses Ld and Ll adv are necessary for patch-level alignment. Without ReshapedF module, the alignment process is affected. In the clustered space, patch-level alignment is crucial for performance improvement. Reshaping features as independent data points enhances alignment regardless of their locations. Visualization shows effective adaptation in the clustered space, with features embedded into groups and overlapping source/target representations. Experimental comparisons with state-of-the-art algorithms demonstrate the effectiveness of the proposed method in various scenarios. Experimental results show the effectiveness of adapting GTA5 to Cityscapes using VGG-16 and ResNet-101 architectures. The proposed method outperforms state-of-the-art adaptations in feature, pixel-level, and output space alignments. Similar improvements are observed when adapting SYNTHIA to Cityscapes. Visual comparisons are provided, and a cross-city case study demonstrates successful adaptation between different cities and weather conditions. Our method for domain adaptation in structured output combines global and patch-level alignments to improve segmentation results. By constructing a clustered space of source patches and using adversarial learning, we achieve better performance in semantic segmentation tasks, outperforming existing algorithms in various scenarios. Our approach for domain adaptation in structured output combines global and patch-level alignments to improve segmentation results. The model is trained in an end-to-end manner by sampling one image from each domain in a training iteration. The optimization strategy involves maintaining the aspect ratio of the image and down-sampling it to a specified size. A regularization technique using entropy loss is implemented to push the target feature representation towards the source clusters. This entropy regularization achieves an IoU of 41.9%, slightly lower than the proposed patch-level adversarial alignment at 43.2%. Our model utilizes source distribution guidance to learn discriminative representations for target patches by aligning them with the source distribution in a clustered space. Results show high similarity between source and target patches, demonstrating the effectiveness of patch-level alignment. Comparative results with and without adaptation are presented for Cityscapes to Oxford RobotCar adaptation, showing improved segmentation outputs with our proposed method. Additional visual comparisons for other adaptation scenarios are provided in figures 9 to 11. The adapted segmentation results for different adaptation scenarios are shown in figures 9 to 11, demonstrating the effectiveness of the proposed method in aligning source and target patches."
}