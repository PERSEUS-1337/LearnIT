{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this bias, a new dataset with 108,501 images balanced on race was created. The model trained on this dataset showed improved accuracy on novel datasets across race and gender groups. Commercial computer vision APIs were also compared for balanced accuracy across different demographics. Various systems have been developed for automated face detection, alignment, recognition, generation, modification, and attribute classification. These technologies have been applied in various fields such as security, medicine, education, and social sciences. However, existing public face datasets are biased towards Caucasian faces, with other races being underrepresented. This bias has been highlighted in recent studies, showing a lack of diversity in large-scale face databases. The model may not apply to all subpopulations due to biased data, leading to ethical concerns about fairness in automated systems. Commercial computer vision systems have been criticized for their asymmetric accuracy across different demographics, with biases in training data causing better performance on male and light faces. Biases in image datasets can easily occur due to biased selection and capture methods. Public large-scale face datasets are predominantly collected from platforms showing White people, leading to a lack of diversity in the data. The text discusses the proposal of a new face dataset with a balanced race composition to mitigate bias in existing datasets. The dataset includes 108,501 facial images from various sources and defines 7 race groups. It is shown that existing face attribute datasets do not generalize well to unseen data with more non-White faces. The new dataset performs better across racial groups and is the first to include Latino and Middle Eastern faces. Computer vision has expanded into fields like economics and social sciences, allowing researchers to analyze demographics using image data. Including major racial groups in datasets enhances the applicability of computer vision methods. Face attribute recognition aims to classify human attributes from facial appearance. Existing datasets are predominantly White, highlighting the need for balanced representation across different races and genders. To ensure fairness in computer vision systems, it is crucial to address racial bias issues. Incidents like Google Photos misidentifying African American faces and Nikon's cameras showing bias towards Asian users have led to service termination or feature removal. Commercial providers have ceased offering race classifiers due to these concerns. Face attribute recognition is used in demographic surveys for marketing and social science research to understand human behaviors and demographics. Social scientists utilize tools to infer demographic attributes and analyze behaviors from images of individuals. In the field of AI and machine learning, there is a growing focus on algorithmic fairness and dataset biases, particularly in demographic analyses of social media users using images. The cost of unfair classification can lead to over-or under-estimation of specific sub-populations, impacting policy implications. Research in fairness aims to ensure balanced accuracy in attribute classification independent of race and gender, with a focus on producing fair outcomes in areas like loan approval. Studies in algorithmic fairness involve auditing existing bias in datasets or systems to address these issues. The paper focuses on algorithmic fairness and dataset biases in demographic analyses of social media users using images. It addresses the biased results in gender classification systems, particularly on dark-skinned females, caused by skewed datasets. The contribution is to mitigate existing limitations and biases by collecting more diverse face images from non-White race groups. The paper addresses biased results in gender classification systems on social media by collecting diverse face images from non-White race groups, improving generalization performance to novel datasets. The dataset includes Southeast Asian and Middle Eastern races, defining 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is based on physical traits, while ethnicity is based on cultural similarities. The paper discusses dataset bias in race classification, dividing groups into White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Skin color is used as a proxy for race in some studies, but it has limitations due to lighting conditions and within-group variations. The paper discusses the limitations of using skin color as a proxy for race in dataset bias in race classification. It emphasizes the multidimensional nature of race compared to the one-dimensional aspect of skin color. The distribution of skin color does not effectively differentiate between race groups like East Asian and White. Despite using skin color measurements, the paper also relies on human annotators' judgments to annotate physical race. Additionally, existing face datasets sourced from public figures may introduce biases due to factors like age and attractiveness. The dataset aims to minimize selection bias by maximizing diversity and coverage. It was created from the Yahoo YFCC100M dataset, detecting faces without preselection. The dataset is smaller but more balanced on race, with 7,125 annotated faces sampled randomly. Demographic compositions of each country were estimated to adjust the number of images incrementally. The dataset was created from the Yahoo YFCC100M dataset to minimize selection bias and maximize diversity. Faces were detected without preselection, resulting in a smaller but more balanced dataset with 7,125 annotated faces. The number of images for each country was adjusted based on demographic compositions to avoid dominance by the White race. Images with specific Creative Commons licenses were used, and race, gender, and age group annotations were refined through multiple worker judgments. After creating a diverse dataset with 7,125 annotated faces, annotations were refined by training a model and manually verifying discrepancies. The dataset's race composition was analyzed, showing bias towards the White race in existing face attribute datasets. Gender balance was relatively better, ranging from 40%-60% male ratio. Model performance was compared using ResNet-34 architecture trained on each dataset with ADAM optimization. Face detection and attribute classification were done using dlib's CNN-based face detector, and the experiment was conducted in PyTorch. The experiment conducted in PyTorch compared a dataset with UTKFace, LFWA+, and CelebA. UTKFace and LFWA+ have race annotations for comparison, while CelebA was used only for gender classification. FairFace defines 7 race categories but only 4 were used for comparison. Cross-dataset classifications were performed using models trained from these datasets. Results for race, gender, and age classification across subpopulations were shown in Tables 2 and 3. Each model tended to perform better on certain datasets. Our model showed high accuracy on some variables in the biased LFWA+ dataset, which is the most diverse and generalizable. To test generalization, three novel datasets were used, including geo-tagged Tweets and media photographs from different countries. The study utilized a public dataset of tweet IDs from known media accounts and a public image dataset collected for a protest activity study. 8,000 faces were randomly sampled from each dataset and annotated for gender, race, and age. The gender classification accuracy on external validation datasets and different models' classification accuracy were reported. The FairFace model outperforms other models in race, gender, and age classification on novel datasets. Even with fewer training images, FairFace surpasses larger datasets like CelebA, indicating dataset size is not the sole factor for performance improvement. FairFace also shows more consistent results across different race groups, measured by standard deviations of classification accuracy. The FairFace model achieves the lowest maximum accuracy disparity in gender classification across different race groups, with less than 1% accuracy discrepancy between male and female, and White and non-White groups. Other models show a strong bias towards males, with the LFWA+ model having the biggest gender performance gap at 32%. The dataset used in the experiment, LFWA+, has the smallest gender performance gap at 32%. Recent work suggests that this bias is due to unbalanced representation in training data. The FairFace dataset shows diversity with non-typical examples, while LFWA+ contains multiple images of the same individuals. UTKFace focuses more on local clusters. The UTKFace dataset focuses on local clusters of faces, while the LFWA+ dataset contains diverse images of the same individuals. Pairwise distance analysis shows that UTKFace has tightly clustered faces, while LFWA+ exhibits more diversity despite majority white faces. This diversity may be influenced by the training data used for face embedding. In contrast to the UTKFace and LFWA+ datasets, which focus on local face clusters and diverse images of the same individuals, respectively, the FairFace dataset shows varied demographic groups for gender classification testing with different online APIs. The dataset includes diverse faces in terms of race, age, expressions, head orientation, and photographic conditions, making it a robust benchmark for bias measurement. The experiments were conducted on August 13th -16th, 2019, with consistent classification accuracies observed across different demographic groups. The gender classification accuracies of tested APIs are shown in Table 6. Not all faces were detected by the APIs except for Amazon Rekognition. Two sets of accuracies are reported, treating mis-detections as mis-classifications and excluding them. The results indicate a bias towards the male category, consistent with previous reports. The study found that there is a bias towards the male category in gender classification, consistent with previous reports. Dark-skinned females tend to have higher error rates, but there are exceptions. Face detection can also introduce gender bias, with Microsoft's model failing to detect many male faces. The paper proposes a new face image dataset balanced on race, gender, and age, which improves classification performance for gender, race, and age on non-White faces. The dataset derived from Yahoo YFCC100m shows balanced accuracy across race, unlike other datasets. It can be used to train and verify model accuracy. Algorithmic fairness is crucial in AI systems, especially as they impact decision making in society. The proposed novel dataset aims to mitigate race and gender bias in computer vision systems for better societal acceptance."
}