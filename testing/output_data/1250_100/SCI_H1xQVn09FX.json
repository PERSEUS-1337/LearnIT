{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. Extensive empirical investigations on the NSynth dataset show that GANs outperform WaveNet baselines in generating audio faster and more efficiently. Neural audio synthesis faces challenges in modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine-scale details but have slow sampling speeds. Efforts to speed up generation introduce overhead. GANs have shown success in generating high-resolution audio efficiently. Adversarial Networks (GANs) have been successful in generating high-resolution images efficiently. However, adapting GAN architectures for audio waveform generation has not achieved the same level of fidelity. Frame-based techniques struggle with aligning local periodicity in audio waveforms due to differences in periodicity and output stride. Transposed convolutional filters face challenges in covering all necessary frequencies and phase alignments to preserve phase coherence in audio. STFT allows unwrapping phase and deriving instantaneous radial frequency to express the relationship between audio and frame frequency. GAN progress in image modeling started with focused datasets like CelebA, which restricted variance in posture and pose. NSynth dataset was introduced for audio with a similar motivation, containing only individual notes. NSynth dataset consists of individual notes from musical instruments with attribute labels for conditional generation. Various models have been explored for audio generation, including autoregressive WaveNet and bottleneck spectrogram autoencoders. Adversarial training and noncausal convolutional generation are introduced to improve audio representation in GANs. Unlike images, audio waveforms are highly periodic, requiring logarithmically-scaled frequency selective filter banks. Tasks on the NSynth dataset involve forming logarithmically-scaled frequency selective filter banks for human hearing. Maintaining the regularity of periodic signals over short to intermediate timescales is crucial for human perception. Phase precession is a challenge for synthesis networks, requiring them to learn appropriate frequency and phase combinations to produce coherent waveforms. This phenomenon is similar to the short-time Fourier transform and occurs when filterbanks overlap. Another approach inspired by the phase vocoder is also discussed in the context of generating coherent waveforms. The approach to generating coherent waveforms is inspired by the phase vocoder. Unwrapping the phase causes it to grow linearly, leading to the concept of instantaneous frequency. The study investigates the synthesis of coherent audio with GANs, finding that generating log-magnitude spectrograms and phases directly can produce more coherent waveforms. Generating coherent audio with GANs involves estimating IF spectra for more coherent audio, avoiding harmonics overlapping, and utilizing global conditioning on latent and pitch vectors for smooth timbre interpolation. The NSynth dataset, with 300,000 musical notes from 1,000 instruments, is used for the study due to its diverse timbres and structured labels for pitch, velocity, instrument, and acoustic qualities. The study focused on generating audio using GANs with the NSynth dataset, containing 300,000 musical notes from 1,000 instruments. The dataset includes pitch, velocity, instrument, and acoustic qualities. Training was done on acoustic instruments and fundamental pitches within a specific range. A new test/train split was created, and progressive training methods were adapted for audio spectrum generation. The model samples a random vector and uses transposed convolutions to generate output data. The study focused on generating audio using GANs with the NSynth dataset, containing 300,000 musical notes from 1,000 instruments. Training involved upsampling data with a generator network and using a discriminator network for divergence estimation. The method included conditioning on musical pitch information for independent control of pitch and timbre. Spectral representations were computed using STFT magnitudes and phase angles. The study focused on generating audio using GANs with the NSynth dataset, containing 300,000 musical notes from 1,000 instruments. Training involved upsampling data with a generator network and using a discriminator network for divergence estimation. Spectral representations were computed using STFT magnitudes and phase angles. The STFT had a 256 stride and 1024 frame size, resulting in 75% frame overlap and 513 frequency bins. The Nyquist frequency was trimmed, and padding was done in time to create an \"image\" of size (256, 512, 2) with magnitude and phase channels. Magnitudes were logged, scaled to -1 and 1, and phase angles were also scaled to match the tanh output nonlinearity of the generator network. \"Phase\" and \"instantaneous frequency\" (\"IF\") models were created by unwrapping the phase angle and taking the finite difference. Increasing the STFT frame size and stride doubled the size of spectral images to (128, 1024, 2) for high frequency resolution variants. Additionally, log magnitudes and instantaneous frequencies were transformed to a mel frequency scale for better separation of lower frequencies. The study compares WaveGAN and WaveNet for audio generation using GANs with the NSynth dataset. They transform log magnitudes and instantaneous frequencies to a mel frequency scale without compression. WaveGAN is adapted to accept pitch conditioning and retrained on a subset of the NSynth dataset. WaveNet is the state of the art in generative modeling of audio, and strong baselines are created by adapting the architecture to accept pitch conditioning signals. The study compares WaveGAN and WaveNet for audio generation using GANs with the NSynth dataset. WaveNet is the state of the art in generative modeling of audio, with strong baselines created by adapting the architecture to accept pitch conditioning signals. Evaluating generative models is challenging due to the subjective nature of audio quality, which is why human evaluation is used as the gold standard. Amazon Mechanical Turk was used to compare examples from all models presented. The study compared WaveGAN and WaveNet for audio generation using GANs with the NSynth dataset. Amazon Mechanical Turk was used to evaluate the audio quality of examples from all models presented. 3600 ratings were collected, with each model involved in 800 comparisons. Metrics such as Number of Statistically-Different Bins (NDB) and Inception Score (IS) were used to measure diversity and evaluate GANs. The Inception Score (IS) measures the KL divergence between imageconditional output class probabilities and the marginal distribution. It penalizes models with examples that are not easily classified into a single class or belong to only a few classes. Additionally, Pitch Accuracy (PA) and Pitch Entropy (PE) assess distinct pitches and diversity. Fr\u00e9chet Inception Distance (FID) evaluates GANs based on the distance between multivariate Gaussians fit to features from a pretrained Inception classifier. The study compares audio quality of different model and representation variants using human evaluation and NDB score. Results show that quality decreases as output representations move from IF-Mel to Waveform, with IF-Mel being judged slightly inferior to real data. The WaveNet baseline occasionally breaks down, leading to comparable scores with IF GANs. NDB score follows the same trend as human evaluation, with high frequency resolution improving scores. Autoregressive sampling in WaveNet leads to lack of diversity in generated samples. The study evaluates the audio quality of different model variants using human evaluation and NDB score. Results show that quality decreases as output representations move from IF-Mel to Waveform. High frequency resolution improves scores, but WaveNet's autoregressive sampling leads to a lack of diversity in generated samples. FID scores reflect poor sample quality in phase models, while classifier metrics show models generating classifiable pitches, despite discrepancies in training distribution. The study evaluates audio quality of different model variants using human evaluation and NDB score. Results show quality decreases as output representations move from IF-Mel to Waveform. FID scores reflect poor sample quality in phase models, while classifier metrics show models generating classifiable pitches. Visualizations highlight differences in phase coherence among GAN variants. Listen to audio examples for a better understanding. The study evaluates audio quality of different model variants using human evaluation and NDB score. Results show quality decreases as output representations move from IF-Mel to Waveform. FID scores reflect poor sample quality in phase models, while classifier metrics show models generating classifiable pitches. Visualizations highlight differences in phase coherence among GAN variants. WaveNet autoencoders learn local latent codes controlling generation on a millisecond scale but have limited scope. Interpolating between examples in the raw waveform is compared using a pretrained WaveNet autoencoder. Interpolating between examples in the raw waveform using a pretrained WaveNet autoencoder and comparing it to the global code of an IF-Mel GAN. WaveNet improves mixing in timbre space but fails to maintain realism, while the GAN model with a spherical gaussian prior produces more realistic results. Linear interpolation in WaveNet ventures off the true data manifold, resulting in less realistic sounds. The IF-Mel GAN with global conditioning produces high-fidelity audio examples with smooth perceptual changes during interpolation. Timbre morphs smoothly between instruments while pitches follow a composed piece. The GAN's timbral identity remains consistent across varying pitch conditioning. The IF-Mel GAN with global conditioning maintains timbral identity across pitch conditioning, allowing for parallel processing of training and generation on modern GPU hardware. This results in significantly faster synthesis times compared to WaveNet, enabling real-time neural network audio synthesis on devices for exploring a wider range of expressive sounds. The curr_chunk discusses the use of deep generative models for audio synthesis, comparing them to speech synthesis models. It mentions the challenges of handling variable length conditioning and proposes a modification to the loss function of GANs for improved training stability. The work also builds on previous advances in GAN literature and highlights the under-exploration of audio generation for music compared to speech synthesis. The curr_chunk discusses improvements in training stability and architectural robustness in deep generative models for audio synthesis. It introduces progressive training for better generation quality and proposes architectural tricks for further improvement. The NSynth dataset is compared to CelebA for audio and advancements in timbre transformations are highlighted. Significant sampling speedups are achieved by training a frame-based regression model, but challenges with phase coherency and multimodal distributions remain. The architecture requires a large amount of channels, slowing down sample generation and training. High-quality audio generation with GANs on the NSynth dataset has been demonstrated, surpassing WaveNet while being much faster. Further validation and expansion to different types of signals are needed. Possible applications of adversarial losses to audio are also explored, addressing issues like mode collapse and diversity. The models were trained using the ADAM optimizer BID18, with optimal learning rates and classifier loss values identified. Both networks utilized box upscaling/downscaling and pixel normalization. The discriminator included the standard deviation of minibatch activations. Real data was normalized before passing to the discriminator. GAN variants were trained for 4.5 days on a single V100 GPU with a batch size of 8. The GAN variants were trained for 4.5 days on a single V100 GPU, with a batch size of 8. For nonprogressive models, training involved \u223c5M examples, while progressive models trained on \u223c11M examples due to faster training in earlier stages. The WaveNet baseline utilized a Tensorflow implementation with a decoder consisting of 30 layers of dilated convolution and a conditioning stack operating on a one-hot pitch conditioning signal. WaveNet models were trained with synchronous SGD using 32 V100 GPUs, converging in 2 days to 150k iterations. The 8-bit model used mulaw encoding and categorical loss, while the 16-bit model utilized a quantized mixture of 10 logistics BID29."
}