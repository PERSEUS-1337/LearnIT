{
    "title": "H1cT3NTBM",
    "content": "In this paper, the use of neural networks for music information retrieval tasks is explored, focusing on improving convolutional neural networks (CNNs) on spectral audio features. Three aspects of CNN design are investigated: network depth, residual blocks with grouped convolution, and global time aggregation. The study, centered on singer classification and singing performance embedding, demonstrates that global time aggregation significantly enhances CNN performance. Additionally, a singing recording dataset is released for training and evaluation. The research aims to leverage recent advancements in deep learning to enhance deep neural networks for music information retrieval tasks. The deep learning community can enhance deep neural networks by using convolution layers on time-frequency representations, similar to image input for computer vision models. ResNet and ResNeXt are variants that allow for deeper convolutional layers in audio and music analysis applications. A new architecture with more than 5 convolution layers is proposed and proven effective on audio time-frequency representations. Convolution layers learn local patterns in input matrices, while recurrent neural networks and attention mechanisms model temporal relations. The attention mechanism is seen as a global aggregation operation with learnable parameters along the time axis. Experimental investigation compares average, max, and attention mechanisms for singer classification and singing performance embedding tasks. The goal of singer classification is to predict the singer's identity from an audio recording. Singer performance embedding aims to create an embedding space where singers with similar styles are closer. The challenge is isolating the \"singer effect\" from the \"song effect\" in modeling singing. Classic audio features capture general similarity, but when different singers perform the same song, audio-based similarity is higher. The \"song effect\" in modeling singing needs to be minimized to emphasize singer similarity. Singer performance embedding aims to create an embedded space where recordings of the same singer are close together. This is achieved by using a siamese neural network instead of a classifier due to the impracticality of handling a large number of identities. The singer performance embedding aims to create an embedding space for singing voice recordings that clusters recordings of the same identity together. This is achieved using CNNs and a siamese architecture to learn the embedding space, enabling fast processing of singing recordings into fixed length vectors. The paper introduces a method that embeds singing recordings into fixed length vectors for efficient similarity comparison. A new dataset of \"balanced\" singing recordings is released for unbiased evaluation. The neural network architecture used in the experiments involves feeding input features as 2-D images into convolutional layers. The neural network architecture involves feeding input features as 2-D images into convolutional layers/blocks, then aggregating the feature maps globally in a time-wise manner. Different convolutional block designs are used, including vanilla convolution, ResNet with bottleneck blocks, and ResNeXt with grouped convolutional blocks. The ResNeXt configuration combines ResNet with grouped convolutions for enhanced performance. The neural network architecture involves feeding input features as 2-D images into convolutional layers/blocks with max pooling layers in between. Batch normalizations are applied after each non-linearity activation. The architecture includes ResNet configuration as a special case of ResNeXt with different convolutional block designs. Global Time-Wise Aggregation Layer and Dense Layers are used before the Output Layer. The neural network architecture in this paper reshapes 3-D feature maps before applying a feed-forward attention mechanism for prediction. The attention mechanism allows weighted access to information from all input hidden sequences. The feed-forward attention calculates a weight vector over time-steps using learnable parameters that can be trained through back-propagation. The feed-forward attention layer in the neural network architecture allows weighted access to information from input hidden sequences. It calculates a weight vector over time-steps using learnable parameters that can be trained through back-propagation. This attention operation is an aggregation operation over the time-axis, similar to max or average pooling. The network architecture includes convolutional and global aggregation parts, with tasks such as singer identity classification and singing performance embedding being explored. The singer classification problem provides evaluation criteria for model performance, while the embedding task explores spatial relationships between samples. Numerical metrics and plots of embedded samples from singing performances are provided for analysis. The DAMP dataset, with 34620 solo singing recordings by 3462 singers, is used for singer identity classification. The dataset is unbalanced, making it challenging for the learning algorithm to avoid bias towards singer-specific song collections. The DAMP-balanced dataset contains 24874 singing recordings by 5429 singers, with 14 songs. It is structured so that the last 4 songs are for testing, and the first 10 songs can be split into any 6/4 train/validation set. This dataset is suitable for singing performance embedding tasks, while the original DAMP dataset is used for singer identity classification algorithms. Time-frequency representations from raw audio signals are used as input for neural networks. The time-frequency representations extracted from raw audio signals, such as Mel-spectrogram and constant-Q transformed spectrogram (CQT), are used as input features for neural networks. Mel-spectrogram is preferred over CQT due to better performance in neural network configurations. The Mel-spectrogram is preferred over CQT for neural network input due to better performance. Audio recordings are resampled to 22050Hz, transformed into Mel-scaled magnitude spectrograms using FFT, and then converted to power spectrograms in decibels. Values below -60dB are clipped to zero and an offset is added for values between 0 and 60. Each singing performance audio recording is transformed into a Mel-spectrogram and chopped into overlapping matrices for analysis. The Mel-spectrogram of each recording is chopped into overlapping matrices with a duration of 6 seconds and 20% hop size. Gradient descent is optimized with ADAM, a learning rate of 0.0001, and a batch size of 32. L2 weight regularization is applied on all learnable weights. Hyperparameters are chosen using Bayesian optimization. Early stopping is applied every 50 epochs. The neural network uses rectified linear unit activation function, with specific filter sizes and hidden units in layers. A subset of 46 singers is used for the tasks. A subset of 46 singers (23 males and 23 females) with 460 solo singing recordings from the DAMP dataset are selected for singer classification. Different neural network configurations, including CNN and ResNeXt building blocks, are explored with various aggregation methods. Experimental results and measures of different models are displayed in TAB0, with a baseline SVM classifier included. The number of convolution filters is adjusted for parameter consistency across models. The baseline method achieved 27% accuracy, exceeding random prediction. Neural network models outperformed the baseline by at least 35%. Global aggregation methods improved performance by 5% to 10%. Average or feed-forward attention had better performance than max aggregation. A subset of 6/4/4 train/validation/test split from DAMP-balanced dataset was used for the singing performance embedding experiment. Siamese neural network architecture was utilized for embedding recordings by the same singer closer together. The embedding dimension for the linear fully connected output layer is chosen to be 16 by SPEARMINT. A siamese network learns the embedding by adjusting the distance between pairs of embedded vectors based on their label. Pairs of samples are labeled with a binary label denoting whether they have the same identity. The siamese network optimizes the squared euclidean distance and uses the contrastive loss as the optimization goal. Training involves randomly sampling pairs of samples from the same or different singers. The contrastive losses for different network configurations are shown in TAB1. The ResNeXt configurations in TAB1 have cardinalities of 4. Training and validation errors over epochs are plotted in FIG1, showing that feed-forward attention and average aggregation tend to overfit more than max and no aggregation. Shallow architectures perform slightly better than deeper ones with similar parameters. Figure 4 displays qualitative characteristics of the embeddings, comparing shallow ResNeXt architectures with handcrafted features for singer classification. The embeddings from the singing performance experiment grouped performances by the same singers while being invariant to the \"song\" effect. Leave-one-out k-nearest neighbor classifications were used for quantitative assessment, showing classification accuracies for different network configurations. The results of k-nearest neighbor singer classification using shallow ResNeXt configurations and handcrafted features are shown in Figure 5. The k-nearest neighbor classifications on singers and songs demonstrated the \"song effect\" and the effectiveness of singing performance embedding learning in diluting it. Global aggregation over time significantly improved performance, with feed-forward attention showing similar results to max and average aggregation strategies. The balanced nature of the dataset allowed for k-nearest neighbor classification on performed songs, highlighting the benefits of recent developments in deep learning for singer identification and embedding problems. The study compared aggregation strategies for neural networks, finding that feed-forward attention accelerates learning. The feed-forward attention layer learns \"frequency templates\" for each convolutional channel, enabling focus on different frequency parts. Training deep neural networks with over 15 convolutional layers on time-frequency input is feasible with global time aggregation. A dataset of 20000 single singing voice recordings, DAMP-balanced, was released for singer classification research. The DAMP-balanced dataset, a separate dataset from the original DAMP, was collected from the Sing! Karaoke app. It includes audio recordings and metadata in the same format as the original DAMP dataset. The dataset will be used for experiments in singer classification and other music information retrieval tasks. The neural network configurations will be tested with different aggregation methods and convolutional layer adjustments. The DAMP-balanced dataset was collected from the Sing! Karaoke app for experiments in singer classification and music information retrieval tasks. Queries for the dataset specifically ask for users who sang specific collections of songs, with only one performance returned for each song and user. Popular songs were selected for queries, with different combinations of splitting the songs into 6/4 collections. The train/validation sets used the first 6 songs for training and the next 4 songs for validation, resulting in 276 performances for training and 88 performances for validation. The DAMP-balanced dataset has different splits for training and validation sets, resulting in varying numbers of singers and performances. This allows for flexibility in creating balanced test sets for models training on other datasets."
}