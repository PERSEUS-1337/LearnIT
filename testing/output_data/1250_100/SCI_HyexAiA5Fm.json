{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models used for high-dimensional continuous measures. This paper introduces a scalable method for unbalanced optimal transport (OT) within the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the transfer of one measure to another. The proposed algorithm utilizes stochastic alternating gradient updates, akin to GANs, and is demonstrated through numerical experiments in population modeling. The focus is on solving the problem of unbalanced optimal transport, which involves finding the most efficient way to transform one measure into another by adjusting mass and transport. In population modeling, optimal transport models mass transport to account for evolving features and local mass variations. Modern approaches use the Kantorovich formulation for optimal probabilistic coupling between measures, with the Sinkhorn algorithm for efficient solution. Generative models like GANs can also learn transport maps in applications where a transport cost is not available. Generative models like GANs, including variants with conditioning or cycle-consistency, have been used for various transport problems such as image and natural language translation, domain adaptation, and biological data integration. These models aim to push a source distribution to a target distribution by training against an adversary. However, existing methods cannot handle mass variation between the source and target populations. Different formulations have been proposed to extend the theory of optimal transport to address this issue. Several formulations have been proposed for extending the theory of optimal transport to handle mass variation. A class of scaling algorithms has been developed for approximating the solution to optimal entropy-transport problems, allowing for unbalanced transport plans between discrete measures. Inspired by the success of GANs for high-dimensional transport problems, a novel framework for unbalanced optimal transport that directly models mass variation is presented. The contribution of the study includes proposing a Monge-like formulation of unbalanced optimal transport, developing scalable methodology for solving the problem, and demonstrating its application in population modeling using various datasets. Additionally, a new scalable method for solving the optimal-entropy transport problem in the continuous setting is proposed in the Appendix. The algorithm by BID27 extends work on unbalanced optimal transport in a scalable manner for large or continuous datasets. Optimal transport addresses transporting measures in a cost-optimal way, with the Kantorovich OT problem being a convex relaxation of the Monge problem. The OT problem is a convex relaxation of the Monge problem, formulating OT as a search over probabilistic transport plans. Conditional probability distributions specify stochastic maps from X to Y, akin to a \"one-to-many\" version of the deterministic map from the Monge problem. Recent advancements have introduced entropic regularization, simplifying the dual optimization problem and enabling efficient solutions using the Sinkhorn algorithm. Stochastic algorithms have been proposed for computing transport plans that can handle continuous measures. Unbalanced OT formulations extending classical OT to handle mass variation have been developed, with numerical methods based on optimal-entropy transport formulations obtained by relaxing marginal constraints using divergences. The optimal entropy-transport problem aims to find a measure \u03b3 that minimizes a cost function, allowing for mass variation between measures \u00b5 and \u03bd. A new algorithm is proposed for unbalanced optimal transport, directly modeling mass variation for high-dimensional continuous measures. The approach involves a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport. The Monge optimal transport problem involves finding a transport map and scaling factor to push one measure to another, allowing for mass variation. Stochastic maps are considered for practical problems like cell biology, where one cell can give rise to multiple cells. Unbalanced Monge optimal transport can model various problems, with examples including cell biology applications. Unbalanced Monge optimal transport models the transformation of points from a source measure to a target measure, considering growth or shrinkage. It can address class imbalances by adjusting the scaling factor to balance classes in the distributions. The optimization challenge lies in satisfying the constraint T # (\u03be\u00b5 \u00d7 \u03bb) = \u03bd. The optimization challenge in unbalanced Monge optimal transport lies in satisfying the constraint T # (\u03be\u00b5 \u00d7 \u03bb) = \u03bd. A relaxation using a divergence penalty is considered, leading to the Monge-like version of the optimal-entropy transport problem. This reformulation specifies a joint measure \u03b3 \u2208 M + (X \u00d7 Y) and changes the objective function to focus on optimal-entropy transport. The search space differs between formulations, with restrictions on the joint measures that can be specified. Theorem 3.4 states that solutions of the relaxed problem converge to solutions of the original problem under certain conditions.\u03b3 k converges weakly to \u03b3 if L(\u00b5, \u03bd) < \u221e. The relaxation of unbalanced Monge OT allows for learning the transport map and scaling factor using stochastic gradient methods. The optimization procedure involves parameterizing T, \u03be, and an adversary function f with neural networks, similar to GAN training. The objective is to minimize the divergence between transported samples and real samples from \u03bd, while cost functions encourage finding a cost-efficient strategy. The optimization procedure involves parameterizing T, \u03be, and an adversary function f with neural networks to minimize divergence between transported samples and real samples from \u03bd. The approach encourages finding a cost-efficient strategy and enables scalable optimization using stochastic gradient descent. The neural architectures of T, \u03be give structure to function classes for effective learning in high-dimensional settings. Algorithm 1 may not find the global optimum due to non-convexity, while the scaling algorithm of BID8 is proven to converge but has limited scalability. A new stochastic method in the Appendix handles transport between continuous measures, overcoming BID8's scalability limitations. However, the output is less interpretable compared to Algorithm 1. In Section 4, the advantage of learning a transport map and scaling factor using Algorithm 1 is demonstrated. The problem of balancing measures \u00b5 and \u03bd in causal inference is discussed, with a focus on eliminating selection biases in treatment effects inference. Algorithm 1 is applied to perform unbalanced optimal transport between modified MNIST datasets, with applications in population modeling. The study demonstrates Algorithm 1's effectiveness in learning a transport map and scaling factor for unbalanced optimal transport between modified MNIST datasets. The experiment simulates class imbalance and population drift, showing that the scaling factor reflects the imbalance ratio between source and target distributions. This validates Algorithm 1's ability to model growth or decline of different classes in a population. The study uses Algorithm 1 to model the evolution of MNIST distribution to USPS distribution, showing the reweighting process. The transport cost is based on Euclidean distance between original and transported images. The visualization in FIG1 illustrates how MNIST digits change in prominence in the USPS dataset. Despite limitations, many MNIST digits maintain their likeness during transport. The study applied Algorithm 1 on the CelebA dataset to model the transformation from young faces to aged faces using unbalanced optimal transport. A variational autoencoder was first trained on the dataset to encode samples into a latent space, and then the transport was performed based on Euclidean distance in the latent space. The results of the unbalanced transport are visualized in FIG2. The study applied Algorithm 1 on the CelebA dataset to model the transformation from young faces to aged faces using unbalanced optimal transport. The transported faces retain key features of the original faces, with a predicted growth in the prominence of male faces compared to female faces as the population ages. This gender imbalance was confirmed by checking ground truth labels. Lineage tracing of cells between different developmental stages or during disease progression is of great interest in biology. Algorithm 1 was applied to single-cell gene expression data from two stages of zebrafish embryogenesis to learn the scaling factor. Cells with higher scaling factors were found to be enriched for genes associated with differentiation and development of the mesoderm. The experiment on single-cell gene expression data from zebrafish embryogenesis revealed cells with higher scaling factors enriched for genes related to mesoderm differentiation and development. A stochastic method for unbalanced optimal transport (OT) based on a regularized dual formulation is presented, addressing a constrained optimization problem by adding a convex regularization term to the primal objective. This term encourages transport plans with high entropy, leading to a dual formulation that can be rewritten in terms of expectations. The text discusses a stochastic method for unbalanced optimal transport (OT) using a regularized dual formulation. It introduces Algorithm 2 for optimizing parameters using neural networks and stochastic gradient descent. The algorithm is a generalization of classical OT to unbalanced OT, with a focus on entropy regularization. The dual solution learned from Algorithm 2 can be used to reconstruct the primal solution, indicating the amount of mass transported between points in X and Y. The text discusses a stochastic algorithm for learning a map from the dual solution in unbalanced optimal transport. It introduces Algorithm 3 for optimizing parameters using gradient descent. The algorithm focuses on entropy regularization and reconstructs the primal solution by indicating the mass transported between points in X and Y. The text discusses the disintegration theorem in the context of optimal transport. It introduces a family of probability measures and measurable functions to represent the pushforward measure. The Radon-Nikodym derivative is used to show the relationship between the primal and dual solutions. The text discusses the disintegration theorem in the context of optimal transport, showing the relationship between primal and dual solutions using the Radon-Nikodym derivative. The proof involves various inequalities and theoretical results, leading to the uniqueness of the joint measure specified by any minimizer of the function. The uniqueness of the joint measure specified by any minimizer of the function L \u03c8 (\u00b5, \u03bd) is proven through theoretical analysis and standard results on constrained optimization. For certain cost functions and divergences, L \u03c8 defines a proper metric between positive measures \u00b5 and \u03bd, corresponding to the Hellinger-Kantorovich or Wasserstein-Fisher-Rao metric. Solutions of the relaxed problem converge to solutions of the original problem for an appropriate choice of divergence penalty. The proof of Theorem 3.4 shows that the sequence of minimizers \u03b3 k is bounded and equally tight under certain assumptions. By an extension of Prokhorov's theorem, a subsequence of \u03b3 k weakly converges to \u03b3. This implies the uniqueness of the joint measure specified by any minimizer of the function L \u03c8 (\u00b5, \u03bd). The proof of Lemma B.2 presents the convex conjugate form of \u03c8-divergence, transforming the main objective into a min-max problem. It states that for non-negative finite measures P, Q over T \u2282 R d, a certain equality holds under specific conditions. This result has been used in generative modeling and a rigorous proof can be found in a referenced work. The optimal cost function for the problem is determined by the subdifferential of \u03c8 * (f) over the support of Q. The choice of cost functions c1 and c2 is crucial for the problem to be well-posed, with c1 typically representing the cost of transport and c2 representing mass adjustment. It is important to choose a convex function for c2 that prevents \u03be from becoming too small or too large. The selection of \u03c8-divergence is also significant, as it impacts the overall optimization process. Algorithm 1, adapted from BID33, shows that any \u03c8-divergence can be used to train generative models by matching a generated distribution P to a true data distribution Q. Jensen's inequality states that for any convex lower semi-continuous entropy function \u03c8, D \u03c8 (P |Q) is minimized when P = Q. However, this may not hold true when P, Q are not probability measures. The original GAN paper's discriminative objective corresponds to D \u03c8 (P |Q) with specific constraints on \u03c8 to ensure divergence minimization matches P to Q. The text discusses the use of \u03c8-divergence to train generative models by matching distributions P and Q. If \u03c8(s) has a unique minimum at s = 1 with specific conditions, then P = Q. Otherwise, P = Q when D \u03c8 (P |Q) is minimized. The choice of function f is crucial and can be enforced using a neural network. In training generative models, the choice of activation layers and neural architectures is crucial. Fully-connected feedforward networks with ReLU activations were used in the experiments, with sigmoid and softplus functions for output activation layers to map pixel brightness and scaling factor weight to specific ranges."
}