{
    "title": "Byg-An4tPr",
    "content": "In this paper, a novel mechanism is developed to preserve differential privacy in adversarial learning for deep neural networks, ensuring provable robustness to adversarial examples. By leveraging sequential composition theory in differential privacy, a new connection is established between privacy preservation and robustness. An original differentially private adversarial objective function is designed to address the trade-off between model utility, privacy loss, and robustness, tightening the sensitivity of the model. The mechanism significantly enhances the robustness of differential privacy deep neural networks against attacks. Privacy-preserving mechanisms in machine learning, such as differential privacy and adversarial training algorithms, aim to enhance model robustness against adversarial attacks. However, existing approaches either focus on privacy or robustness, leaving models vulnerable to attacks. This one-sided approach poses risks to machine learning systems, as adversaries can exploit both privacy and robustness vulnerabilities. To ensure the safety of machine learning systems, models need to be private to protect training data and robust against adversarial attacks. However, developing a model that meets both criteria remains a significant challenge. Existing mechanisms and conditions cannot effectively address the trade-off between model utility, privacy loss, and robustness. The interplay among differential privacy preservation, adversarial learning, and robustness bounds complicates the issue further. The proposed novel differentially private adversarial learning (DPAL) mechanism aims to preserve privacy of training data, be robust against adversarial examples, and maintain high model utility. It injects privacy-preserving noise into inputs and hidden layers to achieve differential privacy in learning private model parameters. Ensemble adversarial learning is incorporated to enhance the decision boundary under differential privacy protections, introducing the concept of DP adversarial examples crafted from benign examples in private training data. A new DP adversarial objective function is proposed to balance model utility and privacy loss by reducing the amount of injected noise compared to existing works. The proposed mechanism introduces ensemble DP adversarial examples with dynamic perturbation size to enhance robustness under different attack algorithms. Privacy is maintained by splitting private training data into fixed batches, preventing privacy budget accumulation. A connection is established between privacy preservation, adversarial learning, and provable robustness. Noise injected into different layers provides varying levels of robustness, leading to a novel generalized robustness bound. The mechanism is the first to connect DP preservation with provable robustness against adversarial examples in adversarial learning. Rigorous experiments on MNIST and CIFAR-10 datasets validate the approach. The mechanism enhances the robustness of DP deep neural networks on MNIST and CIFAR-10 datasets. It defines a database D with tuples containing data x and ground-truth label y. A model outputs class scores f that map inputs to a vector of scores. The predicted label y(x) is the class with the highest score value. A loss function penalizes mismatches between predicted and original values. Notations and terminologies are summarized in Table 1. DP-preserving techniques in deep learning involve (\u03b5, \u03b4)-DP, where \u03b5 controls the difference in distributions induced by databases, and \u03b4 is a probability threshold. Algorithms can introduce noise into gradients, objective functions, or labels to preserve DP. The mechanism in this paper achieves better sensitivity bounds compared to existing works. Adversarial learning is also discussed for enhancing the robustness of DP deep neural networks on MNIST and CIFAR-10 datasets. Adversarial learning aims to find adversarial examples that deceive a target model by introducing perturbations. Different attacks, such as l p -norm bounded attacks, are used to minimize the risk over these adversarial examples. Two basic adversarial example attacks include single-step and iterative algorithms, like the FGSM algorithm. Multiple gradients are computed in iterative algorithms to find adversarial examples. Adversarial learning focuses on generating adversarial examples to deceive models. Prior work aims to produce correct predictions on adversarial examples without compromising accuracy on legitimate inputs and detect adversarial examples. Adversarial training, like the algorithm proposed by Kurakin et al. (2016b), shows promise for learning robust models by injecting new adversarial examples into training batches. Some algorithms also focus on Differential Privacy (DP) and Provable Robustness. Some algorithms have been proposed to achieve provable robustness by ensuring consistency under perturbations. Lecuyer et al. introduced PixelDP, which randomizes the scoring function to enforce Differential Privacy on a small number of pixels in an image, guaranteeing robust predictions against adversarial examples. The PixelDP algorithm ensures robust predictions by randomizing the scoring function to enforce Differential Privacy on image pixels. A certified robustness check is performed at prediction time, with a generalized condition based on expected values. The DPAL mechanism, different from PixelDP, is introduced to protect training data while achieving robustness. DPAL has three key components: preserving DP in learning the feature representation model, focusing on DP in adversarial learning, and computing robustness bounds at inference time for a deep neural network trained over T steps. It involves training with perturbed and DP adversarial examples, crafting adversarial examples, and verifying robustness. DPAL focuses on preserving differential privacy (DP) in learning the feature representation model using an auto-encoder. The auto-encoder learns DP parameters \u03b81 to ensure the output is DP. By injecting noise into coefficients of the approximated function, DP is maintained in the optimization process. This approach allows for training with perturbed and DP adversarial examples while verifying robustness bounds at inference time for deep neural networks. DPAL focuses on preserving differential privacy (DP) in learning the feature representation model using an auto-encoder. Laplace noise is injected into coefficients to maintain DP in the optimization process. This ensures that the computation does not access the original data, allowing for training with perturbed examples. The global sensitivity of the perturbed function is calculated, tightening the privacy budget consumption in computing the remaining hidden layers. The output is (1/\u03b3)-DP without using additional information from the original data. The computation of g(a(x, \u03b8 1 ), \u03b8 2 ) is (1/\u03b3)-DP, providing rigorous DP protection. The affine transformation h 1Bt is (1/\u03b3)-DP, and the batch B t as the input layer is (1/\u03b3 x)-DP. Optimizing R Bt (\u03b8 1 ) is (1/\u03b3 x + 1)-DP in learning \u03b8 1 with an (1/\u03b3 x)-DP batch. Adversarial examples x adv j are generated using perturbed benign examples x j to enhance model robustness. The text discusses preserving Differential Privacy (DP) in objective functions for adversarial attacks. It introduces a novel DP adversarial objective function L Bt (\u03b8 2 ) combining loss functions for benign and adversarial examples to optimize parameters \u03b8 2. The objective function aims to achieve DP in learning \u03b8 2 by protecting true class labels y i and y j in the output layer. The approach presented aims to preserve Differential Privacy (DP) in the objective function for benign examples by optimizing the function L 2Bt (\u03b8 2) while accessing the ground-truth label y ik. The sensitivity of the objective function is notably smaller than the state-of-the-art bound, crucial for improving model utility. The perturbed functions are optimized given two disjoint batches B t and B adv t, ensuring privacy budget utilization. The approach aims to achieve Differential Privacy (DP) by optimizing the objective function L 2Bt (\u03b8 2) with perturbed inputs and coefficients. It leverages parallel composition and post-processing properties of DP to extend the privacy guarantee across T training steps. Key properties include reading perturbed inputs and coefficients, using disjoint fixed batches, and maintaining batch consistency to prevent privacy leakage. The PixelDP mechanism ensures privacy by fixing batches across training steps to prevent additional privacy leakage. It achieves (1 + 1/\u03b3 x + 1/\u03b3 + 2)-DP parameters on private training data. The mechanism randomizes the scoring function to establish provable robustness against adversarial examples. Privacy noise is injected into input x to avoid directly injecting noise into coefficients. The correlation between DP preservation and provable robustness lies in the relationship between privacy noise and robustness noise. A robustness bound can be derived by projecting privacy noise on the scale of robustness noise. The mechanism achieves robustness against norm attacks with a certain probability confidence level. Additionally, perturbations are applied to the scoring function to ensure robustness against different types of attacks. These robustness conditions can be achieved through randomization processes during inference. The model achieves robustness through randomization processes in inference time, defending against l p -norm attacks. The general robustness bound is determined by \u03ba and \u03d5, with the model being robust to attacks within \u03b1 \u2208 l p (\u03ba + \u03d5). Sequential composition in DP is leveraged to answer this question theoretically, showing that the expected value is insensitive to small perturbations \u03b1 \u2208 l p (1). The composition of robustness is derived in Theorem 5. The composition of robustness in Theorem 5 states that given S independent mechanisms, the predicted label is robust to adversarial examples with probability \u2265 \u03b7. Noise injections into the input and its affine transformation are considered as two mechanisms, sequentially applied with independent draws in the noise. The composition of robustness in Theorem 5 states that predicted labels are robust to adversarial examples with probability \u2265 \u03b7. Group privacy is applied with sizes \u03ba and \u03d5, making scoring functions \u03ba r -DP and \u03d5 r -DP. Parameters \u03b8 1 and \u03b8 2 are independently updated using gradient descent. Verified inference returns a robustness size guarantee for each example x. The prediction on an example x is robust to attacks up to (\u03ba + \u03d5) max, with the failure probability 1-\u03b7 decreasing as the number of invocations of the model f(x) increases. Sensitivity bounds are used to determine robustness, and a new Monte Carlo Estimation method is proposed for better performance. Extensive experiments on MNIST and CIFAR-10 datasets show high model utility, strong DP guarantees, and protection against l \u221e -bounded adversaries. Our DPAL mechanism is compared with state-of-the-art mechanisms in DP-preserving algorithms and provable robustness. DP-SGD injects random noise into gradients, AdLM is a Functional Mechanism-based approach, PixelDP provides provable robustness, and SecureSGD combines PixelDP and DP-SGD with heterogeneous noise distribution. White-box attacks include FGSM, I-FGSM, MIM, and MadryEtAl. Our study compares our DPAL mechanism with state-of-the-art mechanisms in DP-preserving algorithms and provable robustness. We focus on the impact of privacy budget and attack sizes on model utility, privacy loss, and robustness bounds. Experimental results on the MNIST dataset show the conventional accuracy of each model as a function of the privacy budget under l \u221e (\u00b5 a )-norm. Our DPAL mechanism outperforms AdLM, DP-SGD, SecureSGD, and SecureSGD-AGM on the MNIST dataset under l \u221e (\u00b5 a )-norm attacks. It shows a significant improvement in accuracy compared to baseline approaches, with only a small degradation in conventional accuracy when the privacy budget is tight. Our DPAL mechanism achieves higher accuracy compared to SecureSGD and SecureSGD-AGM under adversarial attacks, with a significant improvement over baseline approaches on the MNIST dataset. The model is resistant to different attack sizes and outperforms existing algorithms by a large margin. The DPAL model shows improved accuracy under adversarial attacks compared to SecureSGD and SecureSGD-AGM on the MNIST dataset. It outperforms other models with a consistent certified accuracy to different attacks and attack sizes. The model's defense remains strong even with more aggressive attacks, with only a small drop in accuracy compared to PixelDP. The DPAL model demonstrates enhanced accuracy against adversarial attacks compared to SecureSGD and SecureSGD-AGM on the MNIST dataset. It outperforms other models in terms of certified accuracy under various attack algorithms and perturbation levels. Incorporating ensemble adversarial learning into DP preservation improves model consistency, robustness, and accuracy. Existing DP-preserving approaches are not as effective against adversarial examples. Results on the CIFAR-10 Dataset further support the superiority of DPAL over baseline models. Our DPAL mechanism outperforms baseline models in terms of accuracy and privacy protection, showing significant improvements over SecureSGD, SecureSGD-AGM, AdLM, and DP-SGD. Increasing the privacy budget leads to improved conventional accuracy, but the model still struggles against adversarial attacks. The model maintains consistency under different attacks and perturbations, highlighting the need for better robustness in adversarial learning. Our DPAL model outperforms baseline approaches in accuracy and privacy protection, with a significant improvement over SecureSGD. It achieves better accuracies in all cases and shows higher certified accuracy compared to baseline approaches. Additionally, a new DP-preserving mechanism was designed to address the trade-off between model utility, privacy loss, and robustness. The curr_chunk discusses the limitations of the model in providing DP protections in adversarial learning with robustness bounds. It highlights challenges in accuracy under adversarial attacks, scalability dependent on model structures, and the need to address threats from unseen attack algorithms. The study emphasizes the difficulties in working with complex networks like ResNet, VGG16, LSTM, and GAN. Efforts from research and practice communities are required to overcome these limitations. The curr_chunk introduces notations and terminologies related to adversarial examples, robustness budgets, feature representation learning models, sensitivity analysis, and privacy budgets. It also includes a pseudo-code of adversarial training. The curr_chunk discusses the computation of sensitivity for a function h, the injection of Laplace noise for privacy, and the perturbation of coefficients in adversarial training. The computation of sensitivity for a function h involves injecting Laplace noise for privacy and perturbing coefficients in adversarial training. The perturbation of coefficients \u03c6 in \u03a6 = { 1 2 h i , x i } is crucial for preserving differential privacy in the algorithm. The total privacy budget to learn the perturbed optimal parameters in Algorithm 1 is ( 1 /\u03b3 x + 1 )-DP. Additionally, the perturbations of coefficients h \u03c0i y ik in neighboring batches of benign examples are computed to preserve privacy. The computation of sensitivity for a function h involves injecting Laplace noise for privacy and perturbing coefficients in adversarial training. The perturbation of coefficients \u03c6 in \u03a6 = { 1 2 h i , x i } is crucial for preserving differential privacy in the algorithm. The total privacy budget to learn the perturbed optimal parameters in Algorithm 1 is ( 1 /\u03b3 x + 1 )-DP. Additionally, the perturbations of coefficients h \u03c0i y ik in neighboring batches of benign examples are computed to preserve privacy. Given \u2206 L2 = 2|h \u03c0 |, the optimization of L 2Bt \u03b8 2 preserves ( 1 /\u03b3 + 2 )-differential privacy without accessing additional information from the original input x i \u2208 B t. The optimal perturbed parameters \u03b8 2 derived from L 2Bt \u03b8 2 are ( 1 /\u03b3 + 2 )-DP. The algorithm achieves DP at the dataset level D by considering the computation of the first hidden layer with disjoint and fixed batches. The computation of h 1D is (1/\u03b3)-DP given the data D, with batches considered as disjoint datasets. The computation of h 1Bt at each training step t does not access the original data, only the perturbed batch of inputs. The optimization of R Bt (\u03b8 1) is (1/\u03b3 x + 1)-DP across T training steps. The optimization of R D (\u03b8 1) is (1/\u03b3 x + 1)-DP given the data D, preserving privacy. The optimization of the data reconstruction function R B adv t (\u03b8 1) given DP adversarial examples is also (1/\u03b3 x + 1)-DP across T training steps, following the post-processing property in DP. The computation of DP adversarial examples x adv j is (1/\u03b3 x)-DP and does not access the original data. The computation and optimization of the data reconstruction function R B adv t (\u03b8 1) given DP adversarial examples is (1/\u03b3 x + 1)-DP across T training steps. The first affine transformation h 1B for DP adversarial examples is also discussed. The Algorithm 1 preserves (1/\u03b3 + 2)-DP in optimizing the adversarial objective function across T training steps using disjoint and fixed batches. The privacy budget used is (1 + 1/\u03b3 + 2) to preserve DP in the objective functions. All computations and optimizations in Algorithm 1 are DP, ensuring privacy in learning on perturbed inputs and coefficients. Crafted DP adversarial examples protect ground-truth labels and do not disclose additional information. The DP guarantee in learning on the whole dataset is equivalent to learning on disjoint and fixed batches. The Algorithm 1 preserves (1/\u03b3 + 2)-DP in optimizing the adversarial objective function across T training steps using disjoint and fixed batches. The privacy budget used is (1 + 1/\u03b3 + 2) to preserve DP in the objective functions. The sequential composition theory in DP is utilized to ensure privacy in learning private parameters \u03b8 = {\u03b8 1 , \u03b8 2} across T training steps. Theorem 4 and Lemma 5 hold, providing guarantees on the expected output and privacy preservation. Proposition 1 recalls the application of Monte Carlo estimation for expected value estimation. The Algorithm 1 ensures (1/\u03b3 + 2)-DP in optimizing the adversarial objective function over T training steps using fixed batches. It utilizes sequential composition theory in DP to maintain privacy while learning private parameters \u03b8 = {\u03b8 1 , \u03b8 2}. Theorem 4 and Lemma 5 provide guarantees on expected output and privacy preservation. Proposition 1 discusses the use of Monte Carlo estimation for expected value estimation. In contrast, the curr_chunk discusses the challenges of distribution shifts in inference accuracy due to independent draws of noise when privacy budget 1 is small. It suggests a novel approach to drawing independent noise following the distribution of \u03c7 1 + /\u03c8) for the affine. The curr_chunk introduces a novel method for drawing independent noise following a specific distribution for an affine transformation, without affecting differential privacy (DP) bounds and provable robustness conditions. It discusses the convergence of the noise and its impact on the scoring function. Additionally, it mentions the datasets used in the experiments and the hardware specifications. The models for MNIST and CIFAR-10 datasets consist of 2 and 3 convolutional layers, respectively. Fully-connected and convolution layers are used in the representation learning model. Each feature map in the convolution layer requires DP computation. The sensitivity \u2206 R is the maximal sensitivity for a single feature map. Each hidden neuron reconstructs a unit patch of input units. For MNIST, two convolutional layers with 32 and 64 features are used, along with a fully-connected layer of 256 units. Adversarial examples were generated using I-FGSM, MIM, and MadryEtAl methods. The current chunk discusses the predefined total privacy budget, model architecture details, batch size, ensemble of attacks used, data augmentation techniques, learning rate, and computational efficiency of the mechanism. It also mentions the setting of parameters for the CIFAR-10 dataset and the control of privacy budget using a ratio. The mechanism is highlighted for its computational efficiency compared to existing DP-preserving algorithms in deep learning. The mechanism discussed in the current chunk efficiently computes robustness bounds for deep neural networks, regardless of network size or activation functions used. It has the potential to be applied in larger networks with larger datasets. The error of polynomial approximation approaches is computed using known error bound results. The mechanism efficiently computes robustness bounds for deep neural networks, regardless of network size or activation functions used. The error of polynomial approximation approaches is quantified by analyzing the maximum and minimum values of the error incurred by truncating the Taylor series approximate function. This analysis aligns with previous studies (Phan et al., 2016; Zhang et al., 2012). The error of polynomial approximation approaches is quantified by analyzing the maximum and minimum values of the error incurred by truncating the Taylor series approximate function. This analysis aligns with previous studies."
}