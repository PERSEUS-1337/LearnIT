{
    "title": "B1lf4yBYPr",
    "content": "Existing deep learning approaches for visual feature learning extract more information than necessary, compromising privacy. This research introduces a novel trust score metric for deep learning models and a framework to improve trust scores by suppressing unwanted tasks. Experiments on popular models show Inception-v1 has the lowest trust score. The framework is also applied to color-MNIST dataset and face attribute preservation. The primary objective of artificial intelligence is to imitate human intelligence, especially with the advent of deep learning models striving to perform complex tasks by learning relationships in unstructured data. There is a focus on privacy and security of learned models, with frameworks for data and model governance to control sharing of information. Model privacy revolves around protecting content from adversarial attacks, but models could potentially learn private information and become more intelligent than intended. Techniques like transfer learning and multi-task learning aim for model generalization. In the context of model privacy and security, techniques like transfer learning and multi-task learning encourage model generalization by learning generic features from data that can be used for multiple tasks. This can lead to models becoming more intelligent than intended, as seen in examples where a shape classifier can predict size and location of objects, or a DL model predicting age and identity from a face image. Privacy concerns arise when data and visual attributes need to be kept private from the model itself. The research focuses on ensuring model trust by restricting it to learn only specific tasks from input images, while unlearning others. The challenge lies in finding balanced datasets for multi-class classification tasks on the same image and ensuring the model learns only preserved tasks, not suppressed ones. The research proposes a framework to measure the trust score of a trained DL model and improve it during training. It includes a simulated multi-task dataset, a novel metric for trustworthiness score, and a model-agnostic solution to enhance trust scores by preserving certain tasks and suppressing others on the same image. Experimental analysis is conducted to compare this framework with existing approaches. The proposed framework aims to improve the trust score of a DL model by preserving certain tasks and suppressing others. Experimental analysis compares this approach with existing methods. Two groups of related work are discussed: k-anonymity preservation and attribute suppression, with examples from face recognition studies. The curr_chunk focuses on extracting meta information from face images without revealing identity. Studies have explored face de-identification and anonymizing gender information. Attribute suppression techniques perturb input data to protect against adversarial attacks. Using a constrained GAN, attributes can be suppressed to generate attribute-free face images. This approach is similar to decorrelating visual attributes in a study by Jayaraman et al. (2014). The curr_chunk discusses the debiasing of model learning by preserving specific attributes in images while suppressing others. It highlights the gaps in existing research, such as the focus on data perturbation rather than model perturbation and the lack of diverse benchmark datasets for evaluation. The lack of a well-curated benchmark dataset for evaluating the privacy-preserving capacity of deep learning models is highlighted. The dataset should allow deep learning models to focus on specific tasks while suppressing others, with varying numbers of classes to study task complexity. It should be noise-free, class-balanced, and designed to avoid influencing classification performance. The lack of a benchmark dataset for evaluating privacy-preserving deep learning models is highlighted. The new PreserveTask dataset is designed for benchmarking models against preserving task privacy, with multiple tasks and labels for each task. The PreserveTask dataset is created for benchmarking models against preserving task privacy. It includes five different classification tasks with various categories such as shape, color, size, location, and background color. The dataset consists of 1260 variations of images with a balanced distribution for training and testing. The PreserveTask dataset is designed to benchmark models for task privacy preservation in shared task learning. It includes various classification tasks like shape, color, size, location, and background color. The dataset contains 1260 image variations for training and testing. Different deep learning models can be trained on the dataset, with the goal of controlling the information learned by the model for task privacy preservation. In literature, techniques exist to suppress a model from learning certain attributes or tasks. A negative loss or gradient can be applied to prevent features from carrying specific information while retaining others. The proposed framework does not require task information during training and can be applied to any deep learning model. The input data x and tasks y can be learned without task information, providing a model-agnostic approach. The proposed framework aims to suppress certain classification tasks in deep learning models by generating random n-class labels in the gradient reversal branch. This approach allows for the training of a model with a custom loss function that can be adjusted based on the amount of sharing between tasks. The framework is both model-agnostic and loss function-agnostic, with PreserveTask serving as a benchmark dataset for evaluating the trust score of trained models. The model is evaluated against different tasks in the PreserveTask, obtaining a confusion matrix of performance accuracy. An ideal DL model would have 100% accuracy on trained tasks and random accuracy on others. Trust score is computed based on deviation from the ideal matrix, using weight W and matrices M and T. The trustworthiness of a trained DL model is computed based on deviation from an ideal matrix, with a trust score bounded between 0 and 1. Using random labels for unknown tasks can improve trustworthiness, with a trust score above 0.9 considered highly desirable. A trust score between 0.8 and 0.9 is acceptable, while anything below 0.8 is poor. The sensitivity of the metric is demonstrated by changing one non-diagonal element in the ideal matrix, resulting in a trust score of 0.98125. In this section, experimental results and analysis of the proposed framework are presented. The trustworthiness of existing models is measured, and the suppression of different tasks is demonstrated using the PreserveTask dataset. A popular deep learning model, Inception-v1, was trained from scratch for shape classification, achieving 99.98% accuracy. Additional visual attributes were studied by extracting the output of the last flatten layer and training neural network classifiers to predict size, color, location, and background color of objects. The experimental results show high prediction accuracies for size, location, and background color using features from Inception v1 model. However, color prediction performance is low due to the independence of shape and color tasks. Trust score of the trained Inception-v1 model is 0.7530, indicating poor performance. Similar experiments with other deep learning models also show similar results. The trust scores of various deep learning models, including Inception-v1 and DenseNet, are compared in Figure 5. Inception-v1 has a low trust score, raising questions about the need for additional intelligence in models. Experiments are conducted to suppress tasks during training, with the gradient reversal (GR) layer used to unlearn suppressed tasks. The gradient reversal (GR) layer is used to suppress known tasks during training, improving the trust of the baseline Inception-v1 model. Experimental results show that using random labels reduces color prediction performance, regardless of the preserved task being shape or size prediction. The features extracted from the flatten layer need to perform well on the preserved task while showing reduced performance on the suppressed task. The experimental setting involves using random n-class labels instead of actual ground truth labels during training to prevent memorization of a single task. Results show that using random labels can reduce performance in certain settings, such as color prediction. The proposed framework aims to suppress specific tasks while preserving others, leading to a decrease in performance for the suppressed task. Using random labels for task suppression in the colored MNIST dataset resulted in a performance reduction in color prediction tasks, with trust scores comparable to using known labels. The MobileNet model's trust score increased from 0.756 to 0.824 after applying the framework for task suppression with random labels and gradient reversal based training. In the DiF dataset, the Inception-v1 model's trust score increased from 0.7497 to 0.8606 using the framework for task suppression with known class labels. With random unknown class labels, the trust score further increased to 0.9069. In the IMDB-Wiki dataset, the model's trust scores were evaluated for gender and age classification tasks. The study focused on improving trust scores of the DenseNet model for gender and age classification tasks using a model-agnostic framework. Trust scores increased from 0.7846 to 0.7883 with known class labels and to 0.7860 with random unknown class labels. The framework has practical applications for face recognition systems and privacy preservation in deep learning models. A benchmark dataset called PreserveTask was created to evaluate a DL model's ability in suppressing shared task learning. Popular DL models like VGG16, VGG19, Inception-v1, DenseNet, and MobileNet showed poor trust scores and exhibited higher intelligence than trained for. A practical case study on face attribute classification using Diversity in Faces (DiF) and IMDB-Wiki datasets was presented. The supplementary material includes detailed hyper-parameters for model reproducibility and additional analysis and visualizations. The experiments utilized standard architectures like VGG16, VGG19, DenseNet, and MobileNet. Data was z-normalized before training. A two hidden layer neural network was used as a classifier with specific architecture and parameters. The models were trained with categorical cross-entropy and Adam optimizer. Validation data was 20% with early stopping after 100 epochs. Batch size of 32 was used for faster computation. In this section, additional analysis and visualizations are included, showing trust scores for various DL models. The Inception-v1 and DenseNet models have the lowest trust scores, while MobileNet has the highest. Various suppression techniques were used to improve trustworthiness, such as using random labels for unknown tasks. Performance heat-maps detail the shared task performance of the Inception-v1 model on the PreserveTask dataset. The performance of Inception-v1 model on the PreserveTask dataset is shown in heat-maps, detailing shared task performance. Trust scores in the Diversity in Faces dataset improved using random labels for unknown tasks."
}