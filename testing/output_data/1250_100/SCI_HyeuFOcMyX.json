{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences, a component lacking in current language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide word prediction, improving translation performance. Human speech planning is evidenced by errors and behaviors, contrasting with NMT models lacking this phase. In neural machine translation, a planning phase is introduced to control sentence structure. Planner codes are generated to guide word prediction, improving translation performance. This contrasts with human speech planning, which shows errors and behaviors absent in NMT models. In neural machine translation, planner codes are used to disambiguate uncertain information about sentence structure, improving translation performance. These codes are learned through a network that reconstructs the structure with both input and planner codes. Experiments show enhanced translation performance and the ability to manipulate output sentence structure by adjusting planner codes. In neural machine translation, planner codes are utilized to clarify uncertain information about sentence structure, enhancing translation performance. These codes are acquired through a model that reconstructs the structure using input and planner codes. The process involves simplifying POS tags to reduce uncertainty in decoding, with the aim of providing a structural annotation that gives an overview of the sentence. This annotation helps determine the sentence order and can be obtained through a two-step process that simplifies POS tags. The planner codes are then learned to eliminate uncertainty in sentence structure during translation. The code learning model architecture involves computing discrete codes based on simplified POS tags, which are then discretized into approximated one-hot vectors. These vectors are combined with input information to initialize a decoder LSTM for sequential prediction of tags. The model is optimized with crossentropy loss and functions as a sequence auto-encoder with an extra context input. The code learning model is optimized with crossentropy loss. After training, planner codes C are obtained for target sentences in the training data. The dataset is then modified to include (X, C Y ; Y ) pairs. During decoding, planner codes are used before emitting real words. Various methods have been proposed to improve syntactic correctness in translations, such as restricting the search space of the NMT decoder and incorporating target-side syntactic structures explicitly. The code learning model integrates CCG supertags with output words in the target side. Different approaches have been proposed, including training NMT models to generate parse trees and generating words and parse actions simultaneously. Evaluation is done on various tasks, with the model trained using Nesterov's accelerated gradient for a set number of epochs. The Nesterov's accelerated gradient (NAG) BID14 is used for code learning with different settings of code length N and code types K. A trade-off is observed between S Y accuracy and C Y accuracy, with the setting of N = 2, K = 4 showing a balanced trade-off. The NMT model architecture includes bidirectional LSTM encoders, LSTM decoders, Key-Value Attention, and residual connections. Dropout with a drop rate of 0.2 is applied during training. The NMT models are trained using the NAG optimizer with a learning rate of 0.25, and dropout is applied with a drop rate of 0.2. Conditioning word prediction on generated planner codes improves translation performance. However, applying greedy search on JaEn dataset results in lower BLEU scores compared to the baseline. Beam search followed by greedy search does not significantly change results. It is suggested that exploring multiple candidates with diverse structures simultaneously improves beam search but not greedy search. This aligns with a study showing beam search performance depends on candidate diversity. The beam search's effectiveness relies on candidate diversity. Manual selection of planner codes can also influence translation results. By manipulating codes, translations with different structures can be obtained. This method proves useful for generating diverse paraphrased translations. The distribution of learned codes for English sentences in the ASPEC Ja-En dataset is illustrated. The distribution of learned codes for English sentences in the ASPEC Ja-En dataset is shown in Fig. 3. The skewed distribution suggests that code capacity is not fully utilized, leaving room for improvement. A planning phase is added in neural machine translation to generate planner codes for controlling sentence structure. An end-to-end neural network with a discretization bottleneck is used to predict simplified POS tags, resulting in improved translation performance. The planning phase in neural machine translation generates planner codes to control sentence structure, improving translation performance. The framework can be extended to plan other latent factors like sentiment or topic."
}