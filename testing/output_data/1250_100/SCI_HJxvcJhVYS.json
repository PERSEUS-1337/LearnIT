{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A proposed method uses Bayesian optimization and Stein variational gradient descent to approximate these distributions, showing promise in likelihood-free inference for reinforcement learning environments. The approach aims to estimate parameters of a physical system based on observed data, utilizing a computational model to generate data. Recent methods address the problem of efficiency in simulations by constructing conditional density estimators or sequentially learning approximations to the likelihood function. Gutmann and Corander (2016) propose an active learning approach using Bayesian optimization to reduce the number of simulator runs. This paper investigates combining variational inference methods with Bayesian optimization to refine variational approximations to a black-box posterior. The approach uses Thompson sampling and Stein variational gradient descent over samples from a Gaussian process. The goal is to estimate a distribution that approximates a posterior distribution over simulator parameters given observations from a target system. The method aims to minimize a discrepancy measure between simulator outputs and observations without access to a likelihood function. The study combines variational inference with Bayesian optimization to improve variational approximations of a black-box posterior. It uses Thompson sampling and Stein variational gradient descent with a Gaussian process to estimate a distribution approximating a posterior over simulator parameters. The goal is to minimize a discrepancy between simulator outputs and observations without a likelihood function. The ABC literature provides various choices for \u2206 \u03b8, with background details on the KSD in the appendix. The GP approximates the expensive and non-differentiable \u2206 \u03b8, allowing for SVGD in the BO loop. Candidate distributions q n \u2208 Q are selected using Thompson sampling from the GP posterior, which accounts for uncertainty in the model. Thompson sampling has been successfully applied to BO problems for selecting point candidates \u03b8 \u2208 \u0398. The Thompson sampling approach involves sampling weights from a multivariate Gaussian to constitute a sample from the posterior of a SSGP. The acquisition function is defined based on an approximation to the target posterior. SVGD represents the variational distribution as a set of particles that are optimized via smooth perturbations. The gradients of logp n are available for SSGP models with differentiable mean functions. Having selected a distribution q n, we need to run evaluations of \u2206 \u03b8 from samples \u03b8 \u223c q n to update the GP model. Representing q by a large number of particles M improves exploration of the approximate posterior surface. Kernel herding constructs a set of samples that minimizes error on empirical estimates for expectations under a given distribution q. The distributional Bayesian optimization (DBO) algorithm utilizes kernel herding to select informative samples for the model based on the GP posterior kernel. This approach is compared against mixture density networks (MDNs) in synthetic data scenarios, specifically in OpenAI Gym's 3 cart-pole experiment. The experiment evaluates a Bayesian optimization approach on simulator parameters in OpenAI Gym's cart-pole environment. Results show that the method outperforms MDN in recovering the target system's posterior and providing better approximations. The approach is more sample-efficient for reinforcement learning applications. The method provides a sample-efficient approach for inferring parameters in reinforcement learning environments. The posterior over functions is determined by a Gaussian process, with fast incremental updates to reduce time complexity. The code is available on GitHub for further scalability and analysis."
}