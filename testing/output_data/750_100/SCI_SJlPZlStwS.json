{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. To address this, a unified framework called EdgeGANRob has been proposed, which focuses on extracting shape/structure features from images to improve CNN robustness. This framework uses a generative adversarial network (GAN) to reconstruct images with texture information. The proposed approach, EdgeGANRob, aims to enhance CNN robustness by extracting shape/structure features using a GAN. A robust edge detection method, Robust Canny, is introduced to reduce sensitivity to adversarial perturbations. Comparisons with EdgeNetRob show that EdgeGANRob improves clean model accuracy without sacrificing robustness. Extensive experiments demonstrate the resilience of EdgeGANRob in various scenarios. EdgeGANRob, introduced by EdgeNetRob, is resilient in different learning tasks under diverse settings. Convolutional neural networks (CNNs) have shown state-of-the-art performance in various learning tasks but are vulnerable to adversarial examples and data poisoning attacks. These attacks manipulate test or training data to tamper with predictions and reduce model generalization accuracy. Recent studies have shown that CNNs struggle to generalize due to learning surface statistical regularities instead of high-level abstractions. The issue of model robustness under distribution shifting remains unsolved, with research focusing on the vulnerability of CNNs to adversarial examples. Recent studies have highlighted the challenge of CNNs in generalizing due to learning surface statistical regularities instead of high-level abstractions. The issue of model robustness under distribution shifting persists, with a focus on CNNs' vulnerability to adversarial examples. Researchers suggest training classifiers only on \"robust features\" that are insensitive to small perturbations, as human recognition relies more on global object shapes than local patterns. This is contrasted with CNNs, which are biased towards local patterns, as demonstrated by experiments showing discrepancies in recognizing texture-shape cue conflicts between humans and CNNs. The bias towards local features in CNNs contributes to their vulnerability to adversarial examples and backdoor attacks. Previous research suggests that human object recognition relies more on object shapes than local patterns, unlike CNNs. Researchers propose improving CNN robustness by focusing on global object shapes. This paper proposes using edges as a shape representation to improve the robustness of CNNs to adversarial attacks and distribution shifting. The approach, EdgeGANRob, aims to make CNNs rely more on global shape structure for better performance. EdgeGANRob aims to enhance CNNs' robustness to adversarial attacks by leveraging structural information in images. A simplified version, EdgeNetRob, detects edges and trains the classifier on them, forcing CNNs to rely on shape information rather than texture/color. This approach eliminates texture bias and improves CNNs' robustness. The proposed Robust Canny algorithm improves the robustness of EdgeGANRob against attacks, outperforming adversarial retraining methods. EdgeNetRob enhances CNNs' robustness but reduces clean accuracy due to missing texture/color information, leading to the development of EdgeGANRob to refill texture/colors. The development of EdgeGANRob embeds a generative model to refill texture/colors based on edge images before input into the classifier. The main contributions include proposing a unified framework to improve CNNs' robustness against multiple tasks by extracting edge/structure information and refilling textural information with GAN. To remain robust against adaptive evasion attacks, a robust edge is proposed. More visualization results can be found on the anonymous website: https://sites.google.com/view/edgenetrob. The proposed Robust Canny edge detection approach aims to reduce sensitivity to adversarial perturbations. Evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Defense methods against adversarial examples have been proposed, but many are not robust against adaptive attacks. State-of-the-art methods are based on adversarial training, with a focus on evaluating against customized white-box attacks and strong adaptive attacks. Distribution shifting is more common than adversarial examples. In real-world applications, distribution shifting is more common than adversarial examples. CNNs tend to learn superficial statistical cues, but methods have been proposed to robustify them by penalizing predictive power of local representations. Benchmark datasets have been proposed to evaluate model robustness under common perturbations, including backdoor attacks. Backdoor attacks inject patterns into training data to manipulate model predictions. Tran et al. (2018) proposed a method to detect poisoned data using robust statistics. Neuron pruning can protect models from these attacks. Recent research connects recognition robustness to visual features, showing CNNs rely more on textures than global shape structure. In recent research, it has been shown that CNNs rely more on textures than global shape structure, while humans focus more on shape structure. Adversarially robust models tend to capture the global structure of objects. There are non-robust features in natural images that are highly predictive but not interpretable by humans. A new classification pipeline based on robust features, specifically edges, is proposed in this work. The new classification pipeline, EdgeGANRob, utilizes robust edge features extracted from images and reconstructed using a GAN. The generated image is then input into a classifier. The simplified backbone, EdgeNetRob, includes edge detection and inpainting GAN. Three settings evaluate the robustness of the method. EdgeNetRob consists of two stages: Edge maps are extracted from images using an edge detection method, and a standard image classifier is trained on these maps. The pipeline aims to make CNN decisions based solely on edges, reducing sensitivity to local textures. Training the edge classifier is necessary even if a pre-trained classifier on original data is available. EdgeNetRob consists of two stages: Edge maps are extracted from images using an edge detection method, and a standard image classifier is trained on these maps. Despite the simplicity of EdgeNetRob, it degrades the performance of CNNs over clean test data due to missing texture/color information. This led to the development of EdgeGANRob, which fills edge maps with texture/colors to improve clean accuracy. The robustness of this classification system depends on the edge detector used, as existing algorithms are vulnerable to attacks. This motivates the proposal of a robust edge detection method. The text describes the development of a robust edge detection algorithm named Robust Canny to address the low accuracy of neural network-based edge detectors when facing adversarial perturbations. Traditional methods like Canny are inherently robust but can become noisy with perturbations. The proposed algorithm aims to improve edge detection in the presence of adversarial perturbations. The proposed Robust Canny edge detector improves on traditional methods by truncating noisy pixels in intermediate stages. It includes stages such as noise reduction with a Gaussian filter, gradient computation using the Sobel operator, noise masking with thresholding, and non-maximum suppression for edge thinning. The Robust Canny edge detector enhances traditional methods by removing noisy pixels in intermediate stages. It involves noise reduction with a Gaussian filter, gradient computation using the Sobel operator, non-maximum suppression for edge thinning, and edge tracking by hysteresis. The masking stage in the robust Canny edge detector involves setting gradient magnitudes below a threshold to zero to reduce perturbation noise. Parameters like the standard deviation of the Gaussian filter and thresholds also impact the detector's robustness. The robust Canny edge detector's performance is influenced by parameters such as the standard deviation of the Gaussian filter and thresholds. Larger \u03c3 and higher thresholds \u03b8 l , \u03b8 h improve robustness but may reduce clean accuracy. Careful parameter selection is crucial for a robust edge detector. Training a Generative Adversarial Network (GAN) in EdgeGANRob involves generating color images from edge maps using the image-to-image translation framework (pix2pix). The inpainting GAN is trained in two stages following the pix2pix framework. The first stage involves training a conditional GAN with adversarial and feature matching losses. In the second stage, the GAN is fine-tuned along with a classifier to achieve high accuracy on generated RGB images. The objective function aims to minimize the classification loss of generated images by inpainting GAN. The method enhances robustness against adversarial attacks, distribution shifting, and backdoor attacks. EdgeGANRob is designed to improve robustness by focusing on edges, which are less susceptible to small adversarial perturbations. Consider a \u221e threat model, where attackers struggle to manipulate edge pixels with limited adversarial budget. Edge features can enhance model generalization in distribution-shifted test data. EdgeGANRob prioritizes shape structure, making it robust to distribution changes during testing and resistant to backdoor attacks. The proposed method, EdgeNetRob, focuses on removing malicious patterns to prevent backdoor attacks. It is evaluated for robustness against adversarial attacks and performance over distribution. EdgeNetRob is considered a robust recognition method with unique advantages in certain settings. The study evaluates the robustness of EdgeNetRob against adversarial and backdoor attacks on Fashion MNIST and CelebA datasets, specifically focusing on gender classification. The choice of datasets is justified by the limitations of MNIST and CIFAR-10 in providing meaningful benchmarks for the study. The same network architecture is used for classification in the experiments. The study evaluates the robustness of EdgeNetRob against adversarial attacks on Fashion MNIST and CelebA datasets. Evaluation includes \u221e adversarial perturbation constraints with specific budgets for each dataset. The methods are tested against adaptive attacks where the attacker is aware of the defense algorithm. The study evaluates the robustness of EdgeNetRob against white-box attacks using the BPDA attack. Three edge detection methods are compared: RCF, Canny, and Robust Canny, with a focus on the need for a robust edge detector to defend against adversarial attacks. The study compares the robustness of EdgeNetRob against white-box attacks using different edge detection methods. Results show that using edges generated by RCF is not robust under strong adaptive attacks. Adversarial training is highlighted as an effective defense method, achieving strong robustness to attacks. EdgeNetRob and EdgeGANRob show promising results in defending against adversarial attacks. EdgeNetRob and EdgeGANRob exhibit a slight drop in clean accuracy compared to the baseline model but outperform it when compared to adversarial training with = 8. EdgeGANRob shows higher clean accuracy than EdgeNetRob on the CelebA dataset, emphasizing the importance of using GANs on complex datasets. Both models remain robust against strong adaptive attacks, with EdgeGANRob showing better or comparable robustness levels to adversarial training baselines. EdgeNetRob demonstrates robustness comparable to adversarial training baselines without the time-consuming process. The method is tested for generalization ability under distribution shifting using perturbed Fashion MNIST and CelebA datasets with various transformations. Kernel transformations using Fourier filtering are introduced in Jo and Bengio (2017a) to preserve high-level semantics in images. Comparing with the state-of-the-art method PAR by Wang et al. (2019a), EdgeNetRob and EdgeGANRob show significant improvements in accuracy for negative color, radial kernel, and random kernel patterns. Results in Table 3 demonstrate the effectiveness of edge features for CNNs, even on greyscale images. Our method demonstrates high accuracy and the effectiveness of edge features for CNNs in generalizing to test data under distribution shifting. It can also serve as a defense against backdoor attacks, as shown by embedding invisible watermark patterns in images. The attack and target pairs chosen for Fashion MNIST and CelebA datasets are detailed, with qualitative results provided in figures. In the study, poisoning ratios of 20% and 30% for Fashion MNIST and 5% and 10% for CelebA were selected. A comparison was made with the Spectral Signature baseline method, showing high poisoning accuracy in attacking the vanilla Net on both datasets. Spectral Signature did not consistently perform well with invisible watermark patterns, unlike the proposed embedding method. The study compared poisoning accuracy of different methods on Fashion MNIST and CelebA datasets. EdgeNetRob and EdgeGANRob showed low poisoning accuracy with invisible watermark patterns. The edge detection algorithm removed the effect of invisible watermark patterns. EdgeGANRob had better clean accuracy than EdgeNetRob, validating the benefit of using an inpainting GAN for improving model robustness. Our method combines a robust edge feature extractor with a generative adversarial network to achieve competitive results in adversarial robustness and generalization. It also improves robustness against backdoor attacks by utilizing shape information. Data pre-processing involves resizing images in CelebA to 128 \u00d7 128 and normalizing data to [-1, 1]. Fashion-MNIST uses a LeNet-style CNN for analysis. For Fashion-MNIST, a LeNet-style CNN is used, while the CelebA dataset utilizes a standard ResNet with depth 20. Models are trained using stochastic gradient descent with momentum, Projected Gradient Descent (PGD), and the Carlini & Wagner \u221e attack (CW). Different PGD attacks are evaluated with varying step sizes. The CW attack is evaluated on 1,000 randomly sampled images due to its high computational complexity. Robust Canny is used for evaluating adversarial robustness. The hyper-parameters used in Robust Canny for evaluating adversarial robustness are reported here. For Fashion MNIST, \u03c3 = 1, \u03b8 l = 0.1, \u03b8 h = 0.2, \u03b1 = 0.3, and for CelebA, \u03c3 = 2.5, \u03b8 l = 0.2, \u03b8 h = 0.3, \u03b1 = 0.2. The last three steps in the algorithm involve non-differentiable transformations, which can be challenging in white-box attack scenarios. Athalye et al. (2018) discuss obfuscating gradients through nondifferentiable transformations as a defense technique. The Backward Pass Differentiable Approximation (BPDA) technique replaces non-differentiable transformations with differentiable approximations to create adversarial examples. To strengthen attacks, a differentiable approximation of the Robust Canny algorithm is found by breaking the transformation into two stages: C1 (steps 1-3) and C2 (steps 4-6). Thresholding operation in step 3 can be represented as a shifted ReLU function. The Backward Pass Differentiable Approximation (BPDA) technique replaces non-differentiable transformations with differentiable approximations to create adversarial examples. The thresholding operation in step 3 can be formulated as a shifted ReLU function. The output is a masked version of the input, where the mask is produced by steps 3-6. To obtain a differentiable approximation of R-Canny for BPDA, the mask is assumed to be constant. Gradients are only backpropagated through C1 (\u00b7), not M (\u00b7). Test accuracy changes under radial mask and random mask transformations are shown in Figure A. For radial mask transformation, the radius of the mask is varied in the Fourier domain. Random mask transformation involves sampling random masks with different probabilities. Additional visualization results for CelebA under distribution shifting are shown in Figure B and ??, while Figure D displays qualitative results of EdgeGANRob and EdgeNetRob for backdoor attacks on Fashion MNIST. EdgeNetRob can slightly remove the poisoning pattern, and the generated images do not share similar patterns."
}