{
    "title": "HkxARkrFwB",
    "content": "Deep learning NLP models use word embeddings like word2vec or GloVe to represent words as continuous vectors, enabling easier integration with neural layers. Storing all word embedding vectors in a dictionary requires significant space and may strain systems with limited GPU memory. Inspired by quantum computing, two methods, word2ket and word2ketXS, were proposed for storing word embedding matrices during training. Our approach, word2ket and word2ketXS, efficiently store word embedding matrices during training and inference, reducing storage space significantly without sacrificing accuracy in natural language processing tasks. Modern NLP models rely on word vector representations to convert human language into a format suitable for neural networks. Using one-hot representation for mapping words to a d \u00d7 d identity matrix is a common method for achieving this. Word embedding approaches like word2vec and GloVe use vectors of dimensionality much smaller than d to represent words, capturing semantic relationships and reducing neural network width requirements. The d \u00d7 p embedding matrix in GPU memory is crucial for training and inference, with vocabulary sizes reaching d = 10^5 or 10^6. The dimensionality of embeddings ranges from p = 300 to p = 1024, making the matrix a significant part of the parameter space. In classical computing, information is stored in bits, while quantum computing offers new possibilities. A qubit, representing an element from set B = {0, 1}, is fully described by a two-dimensional complex unit-norm vector in set C2. Entanglement allows interconnected qubits in a quantum register to have exponential dimensionality in state space, represented by C2n. Interconnected quantum bits exhibit entanglement, unlike classical bits. Quantum registers can be approximated classically, storing vectors of size m using O(log m) space. This approximation does not significantly impact NLP machine learning algorithms. The text discusses new methods, word2ket and word2ketXS, inspired by quantum computing, for efficiently storing word embedding matrices in NLP machine learning algorithms. These methods offer high space saving rates with little processing cost, as shown in empirical evidence from three NLP tasks. The new word2ket embeddings provide high space saving rates with minimal impact on downstream NLP model accuracy. A tensor product space of two separable Hilbert spaces V and W, denoted as V \u2297 W, is constructed using ordered pairs v \u2297 w, where v \u2208 V and w \u2208 W. The inner product between v \u2297 w and v \u2297 w is defined as a product of individual inner products v \u2297 w, v \u2297 w = v, v w, w. In a tensor product space V \u2297 W, vectors are formed by pairs v \u2297 w from unit-norm vectors in V and W. The basis {\u03c8 j \u2297 \u03c6 k} forms an orthonormal basis in V \u2297 W, with coefficients equal to the products of coefficients in V and W. The Kronecker delta \u03b4 z is one at z = 0 and null elsewhere. The tensor product space V \u2297 W allows for adding pairs of vectors by combining coefficients. The dimensionality of V \u2297 W is the product of the dimensionalities of V and W. Tensor product spaces can be created by multiple applications of tensor product. In Dirac notation, vectors are represented as kets with countable orthonormal basis. The tensor order of a tensor product space is denoted by n. The tensor product space contains vectors of the form v \u2297 w and their linear combinations, some of which cannot be expressed as \u03c6 \u2297 \u03c8. It is not always possible to find \u03c6 and \u03c8 to express v \u2297 w + v \u2297 w as \u03c6 \u2297 \u03c8. The tensor product space involves vectors of the form v \u2297 w and their combinations, some of which cannot be expressed as \u03c6 \u2297 \u03c8. Tensors with rank greater than one are called entangled. The maximum rank of a tensor in a tensor product space of order higher than two is not known in general. A p-dimensional word embedding model maps word identifiers into a p-dimensional real Hilbert space. In word2ket, a word embedding model represents a single word as an entangled tensor with vectors of rank r and order n, resulting in a vector v of dimension p = qn. The word2ket model represents a word as a tensor with vectors of rank r and order n, resulting in a vector v of dimension p = qn. The calculation of inner product between word embeddings takes O(rq log q log p) time and O(1) additional space. The word2ket model represents words as tensors with vectors of rank r and order n. Processing a batch of b words requires O(bp + rq log q log p) space. Reconstructing a single p-dimensional word embedding vector from a tensor of rank r and order n takes O(rn log 2 p) operations. The word2ket model represents words as tensors with vectors of rank r and order n. The proposed embedding representation involves differentiable arithmetic operations, allowing gradients to be defined. The balanced tree structure enables parallel processing, reducing sequential processing to O(log2n). The word2ket model represents words as tensors with vectors of rank r and order n, enabling parallel processing with a balanced tree structure. To address high Lipschitz constant gradients, LayerNorm is used at each node in the tensor product tree. Linear operators A and B are defined, with A \u2297 B being a linear operator mapping vectors from V. Linear operator B : W \u2192 Y maps vectors from V \u2297 W to U \u2297 Y. In the finite-dimensional case, A \u2297 B is represented as an mn \u00d7 mn matrix. A word embedding model can be seen as a linear operator F : Rd \u2192 Rp mapping one-hot vectors to word embeddings. The word embedding linear operator T maps word vectors to embedding vectors. The matrix representation of this operator is stored as a d \u00d7 p matrix M, with M^T representing the linear operator F. The resulting matrix F has dimensions p \u00d7 d and takes up O(rq log qt log t log p log d) space. The linear operator T maps word vectors to embedding vectors using a d \u00d7 p matrix M. The resulting matrix F has dimensions p \u00d7 d and takes up O(rq log qt log t log p log d) space. Additional space efficiency is achieved by applying tensor product-based exponential compression horizontally and vertically to the whole embedding matrix. Lazy tensors are used to avoid reconstructing the full embedding matrix each time a small number of rows is needed for multiplication by a weight matrix in the neural NLP model. The proposed space-efficient word embeddings use lazy tensors to reconstruct rows efficiently from underlying matrices. These embeddings were tested in text summarization, language translation, and question answering tasks, showing comparable accuracy to regular embeddings storing p-dimensional vectors for a d-word vocabulary. In text summarization experiments, a bidirectional forwardbackward RNN encoder and attention-based RNN decoder were used with internal layers of dimensionality 256 and a dropout rate of 0.2. The models were trained for 20 epochs starting from random weights and embeddings, using the GIGAWORD dataset with 200K examples in training. The validation set was utilized for evaluation. The text chunk discusses the results of using different dimensionalities in word2ket models for text summarization. It shows that word2ket can achieve a 16-fold reduction in trainable parameters with a slight drop in Rouge scores. Additionally, word2ketXS is more space-efficient, matching word2ket scores while allowing for a 34,000 fold reduction in parameters. In the evaluation on NLP tasks, word2ketXS offers significant space reduction with only a slight decrease in Rouge scores. For German-English machine translation using the IWSLT2014 dataset, BLEU scores were used to measure performance with different embedding dimensions. The results showed a small drop in BLEU scores for varying tensor order and matrix dimensions. The results in Table 2 show a drop in BLEU scores for different space saving rates in the evaluation of NLP tasks. The third task involved the Stanford Question Answering Dataset (SQuAD) using DrQA's model with specific parameters. Increasing the tensor order in word2ketXS to four allows for higher space savings, with a 0.5 point drop in F1 score but a 1000-fold saving in parameter space. For order-4 tensor word2ketXS, there is a nearly 10^5-fold space saving rate with a less than 3% drop in F1 score. The computational overhead for word2ketXS embeddings was also investigated, showing an increase in training time from 5.8 to 7.4 hours for tensors of order 2. The experiments involved training a word2ketXS-based model using tensors of order 4, which increased the training time to 9 hours. Despite the longer training time, the model's dynamics remained largely unchanged. The results showed significant reductions in the memory usage of the word embedding component, impacting various parameters in the model. During inference, embedding and other layers dominate the memory footprint of transformer models like BERT, GPT-2, RoBERTa, and Sparse Transformers, requiring hundreds of millions of parameters to work. In RoBERTa BASE, 30% of parameters are for word embeddings. Memory is also needed to store activations during training, which often dominate the memory footprint. During training, memory footprint is dominated by embedding and other layers in transformer models like BERT, GPT-2, RoBERTa, and Sparse Transformers. Decreasing memory requirements for these networks is crucial given current hardware limitations. Various approaches such as dictionary learning, word embedding clustering, and bit encoding have been proposed to lower space requirements for word embeddings. Various approaches have been proposed to compress models for low-memory inference and training, including pruning, quantization, sparsity, and low numerical precision methods. Fourier-based approximation methods have also been used for approximating matrices. However, none of these approaches can match the space-saving rates needed for transformer models like BERT, GPT-2, RoBERTa, and Sparse Transformers. Various methods have been used for compressing models to save space, but none have achieved the space-saving rates of word2ketXS. Approaches like bit encoding and parameter sharing have limitations, while tensor product spaces have been used for document embeddings."
}