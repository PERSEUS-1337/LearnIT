{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the focus is on generating online explanations during execution to reduce human mental workload. The challenge lies in the interdependence of different parts of an explanation, requiring a general formulation for online explanation generation. Three implementations satisfying various online properties are presented, based on a model reconciliation setting. Evaluation is done with human subjects in a standard planning context. Our approaches are evaluated with human subjects in a planning competition domain and in simulation with ten different problems across two domains. As intelligent robots become more prevalent, human-AI interaction is crucial. Explanations from AI agents help maintain trust and shared awareness. Prior work on explanation generation often overlooks the recipient's need to understand. A good explanation should be generated. To ensure trust and shared awareness in human-AI interaction, explanations from AI agents should be generated in a lucid fashion from the recipient's perspective. The agent should consider discrepancies between human and AI models, encapsulating them as model differences. Explanation becomes a request for model reconciliation, where M R represents the robot's model and M H represents the human's model. When plans differ, the robot should explain by generating an explanation. The robot should explain model differences between human and AI models to ensure trust in human-AI interaction. Model reconciliation is necessary when plans differ, with the robot generating explanations to align behavior with the updated model. However, the mental workload for understanding explanations remains an issue in current research. In this work, it is argued that online explanations should be provided to reduce mental workload. Generating online explanations spreads out information while considering dependencies between parts. Online explanations aim to reduce mental workload by spreading out information and considering dependencies between parts. This concept is illustrated through a scenario where two friends, Mark and Emma, have different preferences for studying together. Mark plans to break the study session into two parts with lunch in between, while Emma prefers to study in one session and have lunch afterwards. Mark and Emma have different preferences for studying together. Mark plans to break the study session into two parts with lunch in between, while Emma prefers to study in one session and get lunch afterwards. Mark goes to the library with Emma, studies for 60 minutes, then suggests going to lunch for energy. He refrains from mentioning he also needs a walk, to avoid Emma suggesting he take a walk alone. This example highlights the importance of communication in planning. Mark demonstrates the importance of providing explanations in an online fashion to Emma, despite their differing values. He gradually reveals his reasoning during plan execution, minimizing unnecessary explanations to reduce mental workload. In this paper, a new method for online explanation generation is developed to reduce mental workload during plan execution. Three different approaches are implemented, each focusing on different \"online\" properties such as matching the plan prefix and simplifying explanations. The focus is on matching the plan prefix and making the next action understandable to the human teammate. A model search method ensures that earlier information does not affect later parts of the explanation, reducing mental workload. The approaches are evaluated with human subjects and in simulation. Explainable AI is crucial for human-AI collaboration, improving trust and maintaining shared situation awareness by helping humans understand the decision-making process of AI agents. The effectiveness of explainable AI is evaluated based on its ability to accurately model human perception of the AI agent, including modeling other agents' perception of itself. This allows the agent to generate understandable motions, plans, and assistive actions, often prioritizing explicability over cost optimality. The model for explainable AI considers cost and explicability. It can be used for intention signaling before execution and generating explanations for the agent's behavior. Research focuses on generating the right explanations based on the recipient's perception model. In the context of explainable AI, the importance of providing online explanations efficiently is highlighted, especially for complex scenarios requiring a large amount of information to be conveyed. The mental workload required for understanding an explanation is often overlooked, emphasizing the need for concise yet sufficient information delivery. Explanation generation aims to provide a minimal amount of information to explain the current part of interest in the plan, intertwining it with plan execution. The problem is based on the model reconciliation setting from prior work and is closely related to planning problems defined in PDDL. The text discusses the model reconciliation setting for generating explanations in the context of planning problems defined in PDDL. It focuses on the robot's optimal plan according to a given model, considering the human's expectations of the robot's behavior. Explanation generation in a model reconciliation setting involves updating the human's model to align with the robot's plan, ensuring it is fully explainable in the human's perspective. A mapping function is used to convert planning problems into features for this purpose. The problem involves a mapping function that transfers planning problems to a state in the feature space. Explanation generation in model reconciliation aims to align human and robot models by minimizing cost differences in expected plans. Explanation generation in model reconciliation aims to align human and robot models by minimizing cost differences in expected plans. A complete explanation is an optimal plan in the human's model after a minimal complete explanation that minimizes unit feature changes. Online explanation generation addresses the mental workload requirement for understanding explanations. Online explanation generation is introduced to minimize cost differences in expected plans between human and robot models. It involves providing minimal information during plan execution to explain the part of the plan that is of interest and not explainable. An online explanation consists of sub-explanations (e k , t k ), where e k represents the kth set of unit features to be made at step t k in the plan. The robot can split an explanation into multiple parts, ensuring that actions before each sub-explanation match the human's expectation. The robot can generate online explanations by splitting them into multiple parts based on different approaches like Plan Prefix matching, Next Action matching, or any prefix matching. These sub-explanations help in aligning the sequence of model changes with the human's expectations during plan execution. The robot generates online explanations by splitting them into multiple parts based on different approaches like Plan Prefix matching. Model changes must ensure that plan prefixes in M H are not altered. The search process illustrated in FIG1 involves finding the largest set of model changes to ensure the plan prefix remains unchanged after further sub-explanations. An OEG-PP is a set of subexplanations (e k , t k ) where Prefix(\u03c0, t) returns the plan prefix up to step t k\u22121. The process is performed recursively for each sub-explanation, moving along the plan as long as the prefix matches with the human model M H. Our approach involves starting the search process from the robot model and stopping when the plan prefixes for the updated human model and robot model match. This differs from the previous approach where the search process starts from the human model. While our method may be more computationally expensive as it requires running the process multiple times, it is more akin to MME BID6 in matching prefixes rather than the whole plan in one shot. Our approach starts the search process from the robot model, finding the largest set of model changes that match the plan prefixes. This allows us to beat both MCE and MME in computation by considering only a small set of changes at a time. The dotted line represents the border of the maximum state space model modification in the robot model, reconciling the two models up to the current plan execution. The algorithm for finding e_k in model space OEG is presented in Algorithm 1. It starts from the robot model, matching model changes with plan prefixes to optimize computation. The process ensures compatibility with the prefix for future steps, maintaining the optimal plan for online explanation. The algorithm presented in Algorithm 1 finds e_k in model space OEG through a recursive model reconciliation procedure. It aims to modify the robot model to match the human's plan, ensuring compatibility with plan prefixes for optimal online explanation. The algorithm in Algorithm 1 aims to reconcile the robot model with the human's plan to provide optimal online explanations. It relaxes the plan prefix condition to focus on matching the very next action, considering the human's limited cognitive memory span. The agent focuses on explaining the next action that differs between the most recent human plan and the robot plan. It performs a model reconciliation procedure on the model space, starting the search from a different point for computational efficiency. Instead of comparing the entire plan prefix, it only explains the immediate next action that does not match in the human and robot plans. The agent in the OEG-PP approach focuses on explaining the immediate next action that differs between the human and robot plans. The search process is similar to MME in BID6 but must be executed multiple times in an online fashion. The algorithms combine search from M H and M R for better performance, assuming the robot has only one correct plan. The robot's goal is to reconcile the human's plan with its own using model space search. In a setting with optimal plans, the robot aims to match the human's plan prefix without the need for explanation. One solution is to generate all human optimal plans and check for a match with the robot's plan, but this is computationally expensive. To reconcile the human's plan with the robot's plan, a compilation approach is implemented to ensure that a plan prefix in the robot's plan matches the human's model. By compiling the human's model into a new problem, it can be determined if the robot's plan prefix aligns with the human's optimal plan. If the costs match, an optimal plan exists in the human's model; otherwise, an explanation is needed. The key is to guarantee that a plan prefix is always satisfied. The key is to ensure that a plan prefix is always satisfied in the compiled model by adding predicates as effects and prerequisites for consecutive actions. The search for e k involves a recursive model reconciliation process on the model space. The main difference in this approach is the model update after each action. After each model update, the agent checks for a human optimal plan that matches the robot's plan up to the next action. This process continues until an optimal human plan aligns with the robot's plan. The approach was evaluated with human subjects and in simulation. The study evaluated different approaches for explanation generation in both human subjects and simulation, comparing them with the Minimally Complete Explanation (MCE) BID6 approach. The goal was to assess how online explanations differ from MCE in terms of information needed and computation time. The evaluation was conducted on ten problems in the rover and barman domains. Differences between M H and M R were made by randomly removing preconditions from model features. The human subject study aimed to confirm the benefits of online explanation generation. The study aimed to confirm the benefits of online explanation generation in a human subject study using a modified rover domain on Mars. The rover's goal is to explore space, take rock and soil samples, and communicate results to the base station. The rover must calibrate its camera and have empty storage space to perform tasks. In the rover domain, the robot can only store one sample at a time and must drop the current sample to take another. The robot's goal is to serve drinks using drink dispensers, glasses, and a shaker, following constraints on object grabbing and glass cleanliness. The simulation results compare minimally complete explanations (MCE) with OEG-PP, OEG-NA, and OEG-AP approaches for problems in the rover and barman domains. OEG has fewer shared model features at each instance compared to MCE, but the total number of model features in an explanation is similar for MCE and OEG-PP. In some cases, OEG-PP and OEG-NA have more model features than MCE. The OEG-PP and OEG-NA explanations have more model features than MCE, as OEG focuses on generating minimal information at each time step. The reason for sharing more information in OEG-PP and OEG-NA lies in the dependence between features and planner behavior. OEG-AP shows the advantage of considering all optimal plans, but there is still a distance between the robot's plan and MCE/OEG-PP. In OEG-AP approaches, there is a remaining distance between the robot's plan and the human's plan due to considering all optimal human plans. The plan distance gradually decreases towards 0 in OEG approaches as execution and explanation are intertwined. The plan distance between the robot's plan and the human's plan in OEG-AP approaches gradually decreases towards 0, suggesting a smoother adjustment for M H during execution. Model updates are sorted based on feature size, with backtracking performed if consistency check fails. This search process takes advantage of the fact that later information often does not affect previous sub-explanations. To test our hypothesis, a human study was conducted comparing three approaches for online explanation generation with minimally complete explanation (MCE) BID6. Another approach, MCE-R, randomly breaks MCE during plan execution to ensure performance differences are not solely due to breaking information into multiple pieces. The experiment used Amazon Mechanical Turk with 3D simulation in the rover domain. Subjects were given a 30-minute limit to finish the task with explanations provided in plain English. In a human study, subjects acted as rover commanders in a 3D simulated scenario on Mars, observing the rover's actions and determining their validity with explanations provided by different approaches. Each subject could only perform the task once to avoid influence between runs. To enhance the observation of mental workload, spatial puzzles were added as a secondary task in a rover commander study on Mars. Certain key information was deliberately withheld, leading to differences in model reconciliation settings between subjects. In model reconciliation settings, explanations may be required when key details are not explicitly provided to the subject. The robot communicates information differently in MCE and MCE-R settings, intertwining explanation with plan execution. The subjects receive missing information at various steps in the OEG setting. In different settings, subjects are given missing information to determine if the robot's actions are logical. Explanations are generated based on BID6 and online approaches. Subjects evaluate the efficiency of these explanations using the NASA Task Load Index (TLX) questionnaire. NASA TLX assesses the workload of human-machine interface systems. NASA TLX is a subjective measurement used to capture mental workload, calculating an overall score based on sub-scales like mental demand, performance, effort, and frustration. The experiment excluded physical demand questions. The academic survey was created using Qualtrics and 150 human subjects were recruited on MTurk. After recruiting 150 human subjects on MTurk, with 30 subjects for each setting, valid responses were obtained from 94 participants. The age range was 18 to 70, with 29.8% being female. The study focused on understanding how well subjects grasped the robot's plan with different explanations, comparing distances across five settings. The distance between the robot's plan and the human's expected plan was computed. The study focused on comparing distances between the robot's plan and the human's expected plan across different settings. Results showed that OEG approaches reduced human's mental workload better than MCE approaches, as evidenced by improved performance in NASA TLX measures. The OEG approaches resulted in better performance in NASA TLX measures by creating more temporal demand during the explanation process and plan execution. Subjects showed more trust towards robots in OEG cases with significantly lower questionable actions. OEG approaches also had higher accuracy in identifying correct actions compared to MCEs. Among the three approaches, OEG-AP performed the best. The OEG approaches showed higher accuracy and fewer questionable actions compared to MCEs. Statistical analysis revealed a significant difference in mental workload between OEG approaches and MCEs. Time analysis showed that OEG-NA had the shortest task completion time, followed by OEG-AP, MCE-R, MCE, and OEG-PP. In this paper, a novel approach for explanation generation is introduced to reduce mental workload during human-robot interaction. The approach involves breaking down complex explanations into smaller parts and conveying them in an online fashion. The accuracy of the secondary task is not significantly different between the various approaches. In this study, three different approaches were developed to generate easily understandable explanations during plan execution, aiming for explainable AI. Evaluation with simulation and human subjects showed improved task performance and reduced mental workload."
}