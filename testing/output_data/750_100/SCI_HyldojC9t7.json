{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology creates positive definite kernels from dissimilarity measures on structured inputs like time series, strings, histograms, and graphs. It differs from Random Features by building a kernel from a random feature map specified by the distance measure. It uses a finite number of random objects to produce a random feature embedding for each instance. D2KE provides better generalizability than universal Nearest-Neighbor estimates, subsuming representative-set method and relating to distance substitution kernel. It also generalizes Random Features methods to handle complex structured inputs. Our proposed framework excels in classification experiments across various domains like time series, strings, and histograms for texts and images, outperforming existing distance-based learning methods in accuracy and computational efficiency. It is often easier to define a dissimilarity function between instances than to create a feature representation, especially with structured inputs like real-valued time series, strings, histograms, and graphs. The curr_chunk discusses the challenge of representing structured inputs with varying sizes and the lack of distance-based methods for classification or regression on such inputs. It mentions dissimilarity measures like Dynamic Time Warping and Nearest-Neighbor Estimation as common methods. The curr_chunk discusses the limitations of Nearest-Neighbor Estimation (NNE) for predicting outcomes on structured inputs due to high variance when neighbors are far apart. Research has focused on developing global distance-based machine learning methods to address this issue. The curr_chunk discusses the challenges of using similarity measures as kernels in machine learning models, highlighting the issue of non-positive definite kernels and non-convex optimization problems. The curr_chunk discusses methods for estimating a positive-definite Gram matrix to approximate the similarity matrix in machine learning models. These methods include clipping, flipping, or shifting eigenvalues, or explicitly learning a PD approximation. However, these modifications often result in a loss of information and may only hold on the training data, leading to inconsistency between testing and training samples. The curr_chunk introduces a novel framework called D2KE that constructs positive-definite kernels from dissimilarity measures on structured inputs. This approach offers a more general and diverse set of kernels compared to traditional methods, showing superior performance in various application domains. D2KE is a novel framework that constructs positive-definite kernels from dissimilarity measures on structured inputs. It builds novel kernels from a random feature map designed for a given distance measure, ensuring Lipschitz-continuity in the corresponding RKHS. The framework provides a tractable estimator for functions in the RKHS with better generalization properties than nearest-neighbor estimation. Our framework generates a feature embedding for each instance, improving classification accuracy and computational efficiency compared to distance-based methods. It excels in diverse domains like strings, time series, and histograms for texts and images. Key contributions include proposing a methodology for constructing positive-definite kernels for structured inputs. Our methodology constructs a family of PD kernels via Random Features from a given distance measure for structured inputs, accelerating kernel machines on inputs like time-series, strings, and histograms. This approach improves classification accuracy and computational efficiency compared to distance-based methods. Distance-Based Kernel Learning aims to create PD kernels through simple transformations of the distance measure. Existing approaches have strict conditions on the distance function or use empirical PD Gram matrices that may not generalize well. Common dissimilarity measures like Dynamic Time Warping, Hausdorff distance, and Earth Mover's distance do not satisfy these conditions. The error bound for similarity-as-kernel approach in BID7 is provided only for positive-definite similarity functions. Different approaches include finding a Euclidean embedding approximating the dissimilarity matrix, theoretical foundation for an SVM solver in Krein spaces, and specific methods for building PD kernels on structured inputs like text and time-series. Randomized feature maps have gained interest for approximating non-linear kernel machines, leading to reduced training and testing times for kernel-based learning algorithms. This approach addresses the diagonal-dominance problem in kernel Gram matrices caused by the summation over a large number of alignments with a sample itself. Various explicit nonlinear random feature maps have been developed for different types of data. Numerous explicit nonlinear random feature maps have been constructed for different types of kernels, including Gaussian, Laplacian, intersection, additive, dot product, and semigroup kernels. The Random Fourier Features (RFF) method approximates a Gaussian Kernel function by multiplying the input with a Gaussian random matrix. Various methods have been proposed to accelerate RFF on high-dimensional input data matrices by leveraging structured matrices for faster computation. D2KE differs from existing Random Fourier (RF) methods by considering structured inputs of different sizes and computing the RF with a structured distance metric. Existing RF methods assume a user-defined kernel, while D2KE computes the RF with dynamic programming or optimal transportation. D2KE constructs a new PD kernel through a random feature map, making it computationally feasible via RF. Contrary to BID49, which is limited to single-variable real-valued time-series, our unified framework extends to various structured inputs like strings, histograms, and graphs. We provide a general theoretical analysis regarding KNN and other distance-based kernels. The text discusses the estimation of a target function from samples using a dissimilarity measure between input objects instead of a feature representation. It also mentions the equivalence between similarity and Euclidean matrices. Equivalence between PD of similarity matrix and Euclidean of dissimilarity matrix can be found in BID4. The dissimilarity measure for learning a target function should be a metric and satisfy certain properties. The ideal feature representation for the learning task should be compact and result in a simple function of the target function. The target function should have small Lipschitz-continuity with respect to the dissimilarity measure. Simply having Lipschitz-continuity may not be enough, as we need a measure of the space size implied. The effective dimension of a structured input space X with a distance measure d and covering number N(\u03b4; X, d) affects the estimation error of a Nearest-Neighbor Estimator. The effective dimension p X,d > 0 is the minimum p satisfying a certain condition. An example is provided for measuring the space of Multiset, which allows duplicate elements. The Hausdorff Distance is discussed in relation to covering numbers and sets of size bounded by L. The effective dimension p X,d > 0 is the minimum p satisfying a certain condition for measuring the space of Multiset. The estimation error of the k-Nearest-Neighbor estimate of f (x) is bounded by a constant c > 0 for \u03c3 > 0, by minimizing w.r.t. the parameter k. The proof is similar to a standard analysis of k-NN's estimation error, with dimension replaced by effective dimension. When p X,d is large, the k-NN estimation error decreases slowly with n, requiring exponential scaling of samples in p X,d for error to be bounded. An estimator based on a RKHS derived from the distance measure with better sample complexity for higher effective dimension is developed. A simple approach D2KE is introduced to construct positive-definite kernels from a given distance measure. The approach D2KE constructs positive-definite kernels from a given distance measure on a structured input domain X. The kernels are parameterized by a distribution p(\u03c9) over random structured objects \u03c9 and \u03b3. The kernel can be interpreted in relation to the Distance Substitution Kernel. The kernel in Equation FORMULA12 is defined with the soft minimum function, parameterized by p(\u03c9) and \u03b3. It can be seen as a soft version of the distance substitution kernel BID22. When \u03b3 \u2192 \u221e, the value of Equation FORMULA15 is determined by min \u03c9 \u2208\u2126 d(x, \u03c9) + d(\u03c9, y), which equals d(x, y) if X \u2286 \u2126. Unlike the distance-substitution kernel, the kernel in Equation FORMULA13 is always positive definite. The kernel in Equation FORMULA13 is always positive definite. To approximate it, draw R samples from p(\u03c9) to get {\u03c9 j } R j=1 and solve the problem for some \u00b5 > 0 using Random Feature Approximation. This allows the kernel to be used in both small and large-scale settings with a large number of samples. Random Feature Approximation can be used in large-scale settings with a large number of samples. It allows for the direct learning of a target function as a linear function of the RF feature map by minimizing domain-specific empirical risk. This approach is different from recent work that selects random features in a supervised setting. The D2KE method uses a RF based empirical risk minimization approach with structured distance measures for random feature embeddings. This differs from traditional RF methods that use matrix multiplication with random Gaussian matrix. Detailed analysis of the estimator is provided in Section 5. The D2KE method utilizes a RF-based empirical risk minimization approach with structured distance measures for random feature embeddings, contrasting its statistical performance with K-nearest-neighbor. The relationship to the representative-set method is discussed, where a naive choice of p(\u03c9) relates the approach to RSM. This involves obtaining a Random-Feature approximation to the kernel by creating an R-dimensional feature embedding. By interpreting Equation (8) as a random-feature approximation to the kernel in Equation (4), a nicer generalization error bound is obtained even with infinite representatives. The choice of p(\u03c9) is crucial in the kernel, with close to uniform distributions showing promising results. In various domains, close to uniform choices of p(\u03c9) in the kernel yield better performance than data distribution p(\u03c9) = p(x). For example, in time-series with DTW, random time series with Gaussian elements outperform RSM. In string classification, random strings from \u03a3 outperform RSM with edit distance. When classifying sets of vectors with the Hausdorff distance, using a distribution of random sets with elements drawn uniformly from a unit sphere yields better performance than RSM. The synthetic nature of this distribution allows for an unlimited number of random features, resulting in a better approximation to the exact kernel. The proposed framework uses a distribution of random sets with elements drawn uniformly from a unit sphere, which provides a better approximation to the exact kernel compared to RSM. The selected distribution generates objects capturing relevant semantic information for estimating f(x) in the RKHS corresponding to the kernel. The proposed framework utilizes a distribution of random sets with elements drawn uniformly from a unit sphere to approximate the exact kernel. The RKHS corresponding to the kernel is denoted as H, with population and empirical risk minimizers represented as DISPLAYFORM0 and DISPLAYFORM1 respectively. The estimated function from the random feature approximation is denoted as f R, with population and empirical risks denoted as L( f ) and L( f ). The risk decomposition is discussed, focusing on the function approximation error in a smaller function space than the RKHS. The RKHS framework aims to impose additional smoothness on functions by constraining them within a smaller function space than Lipschitz-continuous functions. The hope is that the best function within this class approximates the true function well in terms of the approximation error. The RKHS assumption provides a qualitatively better result. The RKHS framework imposes smoothness on functions within a smaller function space than Lipschitz-continuous functions, aiming for better approximation error. The estimation error in RKHS has a dependency of n^-1/2, which is an improvement compared to other methods. The RKHS estimator shows improved dependency on n (i.e. n^-1/2) compared to the k-nearest-neighbor method, especially for higher effective dimension. Analyzing the error from Random Feature Approximation is challenging due to the lack of an analytic form of the kernel. The empirical risk function can be used to bound the error terms in RF approximation. In Equation FORMULA3, the focus is on the second term of empirical risk, analyzing the approximation error of the kernel DISPLAYFORM3. Uniform convergence is achieved with a form of P max DISPLAYFORM4, where p X,d is the effective dimension of X under metric d(., .). To guarantee |\u2206 R (x 1 , x 2 )| \u2264 with probability at least 1 \u2212 \u03b4, it suffices to have DISPLAYFORM5. Proposition 2 provides an approximation error in terms of kernel evaluation, and the bound on empirical riskL(f R ) \u2212L(f n ) is considered by the optimal solution of empirical risk minimization. By the Representer theorem, to guarantee a small difference between the risk of the regularized model and the empirical risk, it suffices to have a number of Random Features proportional to the effective dimension. The framework proposed can achieve this approximation error by combining error terms. The proposed framework can achieve -suboptimal performance by combining error terms. It is shown that the target function lies close to the population risk minimizer in the RKHS spanned by the D2KE kernel. The method is evaluated in various domains such as time-series, strings, texts, and images. The proposed framework achieves suboptimal performance by combining error terms in various domains such as time-series, strings, texts, and images. It discusses dissimilarity measures and data characteristics for experiments involving Dynamic Time Warping, Edit Distance, Earth Mover's distance, and (Modified) Hausdorff distance for measuring semantic distances between different types of data. The study focuses on measuring semantic closeness between Bags of Visual Words using SIFT vectors. Computational complexity was addressed by adapting C-MEX programs and using Matlab for coding. Four datasets were selected for experiments, including multivariate time-series data with varying lengths. The study involved analyzing IQ samples from a wireless communication system, string data with alphabet sizes between 4 and 8, text data with varying document lengths, and image data with SIFT descriptors. Datasets were divided into train and test subsets for analysis. The datasets were divided into 70/30 train and test subsets. Baselines compared D2KE against 5 state-of-the-art methods, including KNN, DSK_RBF, DSK_ND, and KSVM. D2KE was compared against 5 state-of-the-art methods, including KNN, DSK_RBF, DSK_ND, and KSVM. The baselines have quadratic complexity in both the number of data samples and the length of the sequences, while D2KE has linear complexity in both. Parameters were optimized on the training set using 10-fold cross-validation. D2KE outperforms baseline methods in terms of accuracy by using random samples from the distribution to achieve performance close to an exact kernel. The best number of samples falls within the range of R = [4, 4096]. Linear SVM is employed for embedding-based methods, while LIBSVM BID5 is used for precomputed dissimilarity kernels. D2KE consistently outperforms baseline methods in classification accuracy with less computation time. It performs better than KNN and achieves better performance than other distance substitution kernels and KSVM methods. The representation induced from a truly PD kernel makes significantly better use of data. In this work, a general framework is proposed for deriving a positive-definite kernel and feature embedding function from a dissimilarity measure between input objects. The framework is particularly useful for structured input domains like sequences, time-series, and sets. The method outperforms RSM in practical construction of feature matrix, with random objects sampled by D2KE showing significantly better performance. Detailed experimental results for each domain are provided in Appendix C. The framework proposed in this work derives a positive-definite kernel and feature embedding function from a dissimilarity measure between input objects. It subsumes existing approaches and opens up a new direction for creating embeddings based on distance to random objects. A potential extension is to develop distance-based embeddings within a deep architecture for structured inputs in an end-to-end learning system. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous, and the goal is to bound the magnitude of Hoefding's inequality for a given input pair (x1, x2). The framework proposed in this work derives a positive-definite kernel and feature embedding function from a dissimilarity measure between input objects. It subsumes existing approaches and opens up a new direction for creating embeddings based on distance to random objects. A potential extension is to develop distance-based embeddings within a deep architecture for structured inputs in an end-to-end learning system. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous, and the goal is to bound the magnitude of Hoefding's inequality for a given input pair (x1, x2). Hoefding's inequality is bounded by choosing \u03b3 \u2264 1, leading to the result P max. The study focuses on optimizing parameters for different methods using cross-validation. Various kernels and distance measures are utilized in the experiments. A new method, D2KE, involves generating random samples for analysis. The results are obtained by applying Theorem 2 after ensuring certain conditions are met. The study optimizes parameters for different methods using cross-validation, employing various kernels and distance measures. A new method, D2KE, generates random samples for analysis. Linear SVM is used for embedding-based methods, while LIBSVM is used for precomputed dissimilarity kernels. Datasets are collected from popular public websites for Machine Learning and Data Science research. The datasets used in the study were collected from various sources including BID18, LibSVM Data Collection BID5, Kaggle Datasets, and a time-series dataset IQ from researchers at George Mason University. Computations were performed on a DELL dual-socket system with Intel Xeon processors and 250 GB of memory running SUSE Linux. Multithreading with 12 threads was used for distance computations. DTW distance measure was employed for time-series data. A Gaussian distribution was found for all datasets. The study used various datasets from different sources and employed the DTW distance measure for time-series data. A Gaussian distribution was found to be applicable for all datasets. D2KE outperformed KNN in terms of classification accuracy, with a 26.62% higher performance on IQ_radio. Our method outperforms KNN on datasets like IQ_radio and Auslan due to its better performance with high-dimensional data sets. Compared to DSK_RBF and DSK_ND, our method shows significantly better results, suggesting that a representation induced from a truly p.d. kernel utilizes data more effectively. D2KE's random time series sampling performs better than RSM in practical construction of feature matrix. Our method outperforms RSM by sampling short random sequences to denoise and find patterns in data, utilizing an unlimited number of possible sequences for a more abundant feature space. RSM's quadratic complexity may lead to significant computational costs for long time-series. The Levenshtein distance is chosen as the measure for string data, with parameters for \u03b3 and length of random strings optimized. D2KE consistently outperforms other distance-based baselines, showing better results in experiments. D2KE outperforms other distance-based baselines, including DSK_RBF with Levenshtein distance. It achieves better performance on large datasets with less computation compared to baselines with quadratic complexity. D2KE achieves higher accuracy with significantly less runtime compared to DSK_RBF, DSK_ND, and KSVM due to lower computational costs for kernel matrix construction and eigendecomposition. For text data, the earth mover's distance is used as the distance measure between documents, showing strong performance when combined with KNN for document classifications. Bag of Words is computed for each document, represented as a histogram of word vectors using google pretrained word vectors of dimension size 300. The study utilized google pretrained word vectors of dimension size 300 to create random documents for analysis. D2KE outperformed other baselines on all datasets, showcasing the effectiveness of distance-based kernel methods over KNN for text data classification. D2KE outperformed other baselines in document classification due to its speedup with random features. For image data, the modified Hausdorff distance was used as the distance measure between images, showing excellent performance in the literature. SIFT-descriptors with dimension 128 were generated using the OpenCV library. The OpenCV library is used to generate SIFT-descriptors with dimension 128, followed by using MHD to compute distances between sets of SIFT-descriptors. Random images of each SIFT-descriptor are generated uniformly sampled from the unit sphere of the embedding vector space R 128. The best parameters for \u03b3 and the length of random SIFT-descriptor sequences are searched for in specific ranges. Results show that D2KE outperforms other baselines in all cases, with DSK_RBF performing best on dataset decor due to the ineffectiveness of underlying SIFT features in finding good patterns quickly. The quadratic complexity of DSK_RBF, DSK_ND, and KSVM makes it difficult to scale to large data. D2KE performs better than KNN and RSM, suggesting it as a strong alternative across applications."
}