{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets of the phrase's meaning by summarizing the distribution of co-occurring words in a pre-trained word embedding space. An end-to-end trainable neural model is proposed to predict the set of cluster centers directly from the input text sequence. The per-phrase/sentence codebook embeddings provide a more interpretable semantic representation and outperform strong baselines on various NLP tasks. These embeddings are derived from the distribution of co-occurring words in a pre-trained word embedding space and are predicted directly from the input text sequence. Word and sentence embeddings like skip-thoughts, ELMo, and BERT are used for representing sentences and phrases. However, single embeddings may not capture multiple senses or topics without supervision. To address this, word sense induction methods and multi-mode word embeddings represent each target word with multiple points. In contrast to topic modeling like LDA, Athiwaratkun & Wilson (2017) and Singh et al. (2018) represent target words with multiple points in a semantic space by clustering neighboring words. For example, real property can refer to real estate or a true characteristic, and previous approaches use clustering to identify these senses. This approach requires solving a distinct clustering problem for each target word. The approaches discussed require solving distinct clustering problems for each target word, unlike topic modeling which clusters all words in the corpus. Extending multi-mode representations to phrases or sentences faces efficiency challenges due to the large number of unique sequences and parameters needed for clustering-based approaches. Storing a large number of parameters takes time and space, especially for long sequences with many unique sequences. Our compositional model learns to predict embeddings of cluster centers from the sequence of words in the target phrase to reconstruct the co-occurring distribution efficiently. In this work, a neural encoder and decoder are used to address the challenge of sample efficiency in clustering approaches. Instead of clustering co-occurring words at test time, a mapping between target sequences is learned to compress redundant parameters in local clustering problems. During training, a neural network learns a mapping between target sequences and cluster centers to predict them directly at test time. A nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed word embeddings. Gradients are back-propagated to update cluster centers. The proposed model captures compositional meanings of words better than baseline methods in unsupervised phrase similarity tasks. It can also measure asymmetric relations like hypernymy without supervision and outperforms single-mode alternatives in sentence representation, as demonstrated in extractive summarization tasks. The approach for sentence representation is formalized in Section 2.1, with the objective function and architecture described in Section 2.2 and 2.3. The model represents each sentence with codebook embeddings predicted by a sequence to embeddings model, encouraging the generation of embeddings that can reconstruct co-occurring words. The model reconstructs co-occurring words in sentences to avoid predicting common topics. The sequence of words in the corpus is represented as a target sequence with start and end positions. The training signal is to reconstruct neighboring words within a fixed window size for sentence representation. The model reconstructs co-occurring words in sentences by clustering words that could possibly occur beside the sequence. Different training models are needed for phrases and sentences due to varying training signals. The model reconstructs co-occurring words in sentences by clustering words that could possibly occur beside the sequence. It focuses on semantics rather than syntax, considering co-occurring words as a set in a pre-trained word embedding space. The model reconstructs co-occurring words in sentences by clustering words in a pre-trained word embedding space. It uses a neural network model to predict cluster centers for input sequences, with a fixed number of clusters to simplify the design. The effect of different cluster numbers will be discussed in the experimental section. In the experimental section, different cluster numbers will be discussed. The reconstruction loss of k-means clustering in the word embedding space is defined using a permutation matrix M. Non-negative sparse coding (NNSC) relaxes constraints by allowing positive values for coefficients M k,j. This relaxation is adopted in this work for all neural architectures. In contrast to k-means clustering, models using NNSC loss generate diverse cluster centers, capturing conditional co-occurrence distribution better. The smoother NNSC loss is easier to optimize for neural networks, allowing predicted clusters to play different roles in reconstructing word embeddings. The reconstruction error is defined with a hyper-parameter \u03bb controlling sparsity. The hyper-parameter \u03bb controls the sparsity of M to avoid predicting centers with small magnitudes. The proposed loss minimizes L2 distance in a pre-trained embedding space, making it efficient. M Ot can be estimated on the fly using convex optimization. Our method, a generalization of Word2Vec, uses RMSprop for training. After estimating M Ot, gradients are back-propagated for end-to-end training. The loss function prevents the network from predicting the same global topics. SGD is used to solve for F in our experiment. The neural network architecture is similar to transformation-based sequence models. Our neural network architecture, similar to the transformation-based seq2seq model, utilizes encoder TE(It) to create contextualized embeddings. Unlike typical seq2seq models, our decoder outputs a sequence of embeddings instead of words, allowing for predicting all codebook embeddings in a single pass while maintaining output dependency. The neural network architecture utilizes encoder TE(It) to create contextualized embeddings and the decoder outputs a sequence of embeddings instead of words, allowing for predicting all codebook embeddings in a single pass while maintaining output dependency. To capture different aspects, the embeddings of <eos> are passed to different linear layers before becoming the input of the decoder. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation due to the complexity of compressing multiple facets into a single embedding. The framework is flexible, allowing for the replacement of encoder and decoder with other architectures such as transformers or (bi-)LSTMs. This flexibility also enables the incorporation of other input features. Cluster centers predicted by the model are visualized in Table 1, similar to how the meaning of the red cluster center is visualized in Figure 2 using the word song or Music. The red cluster center's meaning is visualized in Figure 2 using the word song or music. Codebook embeddings capture semantic facets of a phrase or sentence well. Evaluating topics conditioned on input sequence is challenging, but codebook embeddings can enhance unsupervised semantic tasks. GloVe embeddings are used for sentence and phrase representation. Our model is trained on Wikipedia 2016 using the uncased version (42B) for phrase representation. Stop words are removed from co-occurring words, and only noun phrases are considered in the experiments. Sentence boundaries and POS tags are detected using spaCy. Our models do not require additional resources like PPDB or other multi-lingual resources, making them practical for domains with low resources such as scientific literature. Our models, trained on Wikipedia 2016 using the uncased version for phrase representation, do not require additional resources like PPDB or other multi-lingual resources. Limited by computational resources, we train all models using one modern GPU within a week. However, our models underfit the data after a week due to their relatively small size. It is challenging to compare our models with BERT, which is trained on a masked language modeling loss and can produce effective pretrained embeddings for supervised tasks. The BERT base model is trained with more parameters, output dimensions, and computational resources compared to the models discussed. Despite using a word piece model to address out-of-vocabulary issues, unsupervised performances based on cosine similarity are provided as a reference. Various benchmarks like Semeval 2013 task 5(a) English and Turney 2012 are used for evaluating phrase similarity. BiRD and WikiSRS are recent datasets used for evaluation. The Semeval 2013 task involves distinguishing similar phrase pairs from dissimilar ones. Turney's approach aims to identify the most similar unigram to a query bigram. BiRD and WikiSRS datasets measure phrase relatedness and similarity. Two scoring functions are evaluated for phrase similarity in our model. Our model evaluates two scoring functions for phrase similarity, one based on contextualized word embeddings and cosine similarity (Ours Emb), and the other based on reconstruction error from codebook embeddings (SC). Negative distance is used to represent similarity when ranking for similar phrases, compared to 5 baselines including GloVe Avg and Word2Vec Avg. Our model outperforms 5 baselines including GloVe Avg and Word2Vec Avg in phrase similarity evaluation using contextualized word embeddings and cosine similarity. BERT CLS, BERT Avg, and FCT LM Emb are also compared, with our models showing significant performance improvements across 4 datasets. Our models outperform baselines in 4 datasets, showing strong performances in Turney (10) and incorporating word order information in phrase embeddings. Non-linearly composing word embeddings improves prediction of co-occurring word embeddings. Ours (K=1) performs slightly better than Ours (K=10), supporting the finding that multi-mode embeddings may not enhance word similarity benchmarks. The performance of Ours (K=10) remains strong compared to baselines in sentence similarity tasks, indicating insensitivity to the number of clusters. The STS benchmark is widely used for this task, predicting semantic similarity scores between sentence pairs. In addition to BERT CLS, BERT Avg, and GloVe Avg, our method is compared with word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). Arora et al. (2017) propose weighting words in sentences based on a constant \u03b1 and word probability p(w) in the corpus. \u03b1 is set to 10^-4 in STS benchmark. After setting \u03b1 to 10^-4 in the STS benchmark, the method GloVe SIF removes the first principal component estimated from the training distribution. This post-processing step is optional, as shown by the performance of GloVe Prob_avg before removing principal components. Previous studies suggest considering word embeddings along with sentence embeddings for measuring sentence similarity. The method involves using multi-facet embeddings to estimate word importance in a sentence by computing cosine similarity with predicted codebook embeddings and summing the similarities. This approach allows for measuring sentence similarity by considering word embeddings in the same semantic space. The method uses multi-facet embeddings to determine word importance in a sentence by calculating cosine similarity with predicted codebook embeddings and summing the similarities. This enables the measurement of sentence similarity based on word embeddings in the same semantic space. Our approach outperforms WMD and BERT Avg, especially in STSB Low, demonstrating the benefits of multi-mode weighting. The proposed method uses multi-mode representation to improve performance, particularly in STSB Low. A variant with bi-LSTM encoder and LSTM decoder performs worse than the transformer alternative. The architecture is similar to skip-thoughts but decodes a set instead of a sequence. The proposed method, similar to skip-thoughts, decodes a set instead of a sequence. It outperforms ST Cos and ignores the order of co-occurring words in the NNSC loss. The model is applied to HypeNet for hypernymy detection, assuming that co-occurring words of a phrase are less related to some of its hyponyms. The asymmetric scoring function outperforms baselines in detecting hypernyms. Our methods perform well in comparison, with Ours (K=1) similar to Ours (K=10). The objective is to discover a summary A with normalized embeddings that reconstruct the distribution effectively. The extractive summarization method aims to generate a summary with normalized embeddings that effectively reconstruct the distribution of word embeddings in the document. The model can produce multiple codebook embeddings to capture different aspects of the text. The model generates multiple codebook embeddings to represent each sentence in the document, comparing different approaches for modeling sentence aspects. This includes using average word embeddings as a single-aspect sentence embedding (Sent Emb) and using all words in the sentences as different aspects (W Emb), with normalization for sentence length. The method, denoted as W Emb, utilizes a fixed number of codebook embeddings to represent sentences, avoiding issues related to sentence length. Results on the CNN/Daily Mail testing set are compared using ROUGE F1 scores, with all methods selecting 3 sentences. Some methods do not consider sentence order information in the documents. The unsupervised methods in CNN/Daily Mail news corpora prioritize sentence order information. Lead-3 and RL are strong baselines, with larger cluster numbers yielding better results. Setting K = 100 gives the best performance after selecting 3 sentences. Our method allows for setting a relatively large cluster number K to improve computational efficiency. Topic modeling has been widely studied and applied for its interpretability and flexibility in incorporating different input features. Neural networks have also been shown to discover coherent topics efficiently. Our goal is to efficiently discover different sets of topics on small word subsets using sparse coding on word embedding space and parameterizing word embeddings with neural networks. Representing words as single or multiple regions in Gaussian embeddings helps capture asymmetric relations. Challenges exist in extending these methods to longer text. The challenges of extending neural decoder methods to longer sequences are not addressed in previous studies. One main challenge is designing a neural decoder for sets rather than sequences, requiring a matching step between two sets and computing distance loss. Various matching loss options, such as Chamfer distance, are widely adopted in auto-encoder models for point clouds. The goal is to measure symmetric distances between elements in sets. The studies focus on measuring symmetric distances between ground truth and predicted sets, while the set decoder reconstructs a set using fewer bases. Different methods for achieving permutation invariants loss for neural networks include removing predicted elements, beam search, predicting permutations using CNNs, transformers, or reinforcement learning. The goal is to efficiently predict clustering centers for set reconstruction. Our goal is to efficiently predict clustering centers for set reconstruction by overcoming computational and sampling efficiency challenges in learning multi-mode representations for long sequences like phrases or sentences. We use a neural encoder to model the compositional meaning of the target sequence and a neural decoder to predict codebook embeddings as the representation of sentences or phrases. During training, a non-negative sparse coefficient matrix dynamically matches predicted codebook embeddings to observed co-occurring words. The proposed models use codebook embeddings to predict clustering centers for sequences, outperforming BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings excel with complex sequences, while single-facet and multi-facet embeddings perform similarly well with simpler sequences. In the future, the goal is to train a single model to generate multi-facet embeddings for phrases and sentences, and evaluate it for supervised or semi-supervised settings. The method may also be applied to other unsupervised learning tasks relying on co-occurrence statistics. The model is kept simple to converge training loss quickly, without fine-tuning hyperparameters due to computational constraints. The model uses a smaller version of BERT with similar architecture and hyper-parameters. The sparsity penalty weight is set to 0.4, sentence size limit is 50, and co-occurring words limit is 30. Transformer dimensions are set to 300, with 5 layers on the decoder side for sentence representation. The number of transformer layers on the decoder side varies for different representations, with dropout on attention also varying. The window size is set to 5, and hyperparameters are determined by validation loss. Codebook embeddings are chosen based on the self-supervised co-occurring word reconstruction task. The number of codebook embeddings K is chosen based on the self-supervised co-occurring word reconstruction task. Performance is not sensitive to the numbers as long as K is large enough. Larger K may lead to longer training time and insufficient convergence in 1 week. Skip-thoughts hidden embedding is set to 600 and retrained in Wikipedia 2016 for 2 weeks. The model has fewer parameters than BERT base model and requires less computational resources. The model presented in the experiment sections has fewer parameters than the BERT base model and uses fewer computational resources for training. BERT Large is compared with the method in Table 6, Table 7, and Table 8, showing that BERT Large performs better in similarity tasks but worse in hypernym detection. The results suggest that training a larger model could be a promising future direction, although the current method outperforms BERT in most cases. Our method outperforms BERT in most cases, especially in phrase similarity tasks. The comparison is done with other baselines when using the same number of sentences, revealing potential issues with shorter sentences in some methods. Our method outperforms BERT in most cases, especially in phrase similarity tasks. When comparing unsupervised summarization methods that do not use sentence order information, we observe that Ours (K=100) significantly outperforms W Emb (GloVe) and Sent Emb (GloVe) when summaries have similar length. Additionally, W Emb (*) usually outperforms Sent Emb (*) when comparing summaries with similar length, although this comparison may not be fair due to the ability of W Emb (*) to select more sentences. In extractive summarization, choosing the right method is crucial for fluency and ROUGE F1 score. Our method (K=100) is best for shorter summaries, while W Emb (BERT) is better for longer ones. Combining our method with BERT could yield promising results. Combining our method with BERT could be a promising direction for achieving the best performance in extractive summarization tasks. Visualizing predicted embeddings from randomly selected sentences in the validation set shows the format of the file similar to Table 1, with embeddings visualized by their nearest neighbors in a GloVe embedding space."
}