{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are effective for control policies in reinforcement and imitation learning. A new technique called Quantized Bottleneck Insertion is introduced to learn finite representations of RNN memory vectors and observation features, making them easier to analyze. Results show small finite representations in synthetic environments and Atari games. The study introduces Quantized Bottleneck Insertion to create small finite representations of RNN memory vectors and observation features for improved interpretability in deep reinforcement and imitation learning. This technique resulted in surprisingly small representations in synthetic environments and Atari games, enhancing the understandability of learned policies. In this paper, the focus is on comprehending and explaining RNN policies by learning more compact memory representations. The challenge lies in interpreting high-dimensional continuous memory vectors updated through complex gating networks like LSTMs and GRUs. The hypothesis is that continuous memory may be capturing and updating discrete concepts. The paper focuses on understanding RNN policies by creating more compact memory representations. It aims to quantize the memory and observation representation used by an RNN to capture discrete concepts. The main contribution is introducing a method to transform an RNN policy with continuous memory and observations into a finite-state representation called a Moore Machine. The paper introduces the concept of Quantized Bottleneck Network (QBN) insertion to transform an RNN policy into a Moore Machine Network (MMN) with quantized memory and observations, nearly equivalent to the original RNN. The paper presents a method to transform an RNN policy into a Moore Machine Network (MMN) with quantized memory and observations, nearly equivalent to the original RNN. The approach involves using \"straight through\" gradient estimators for training quantized networks, which proved effective in experiments on synthetic domains and benchmark grammar learning problems. Additionally, experiments on 6 Atari games using RNNs achieved state-of-the-art results. The study explores RNN memory usage in Atari games, showing that MMNs can be extracted with insights into memory usage not obvious from RNN policy observation. It identifies games where RNNs use memory reactively or implement open-loop control. Our work is related to extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages. Typical approaches involve discretizing the continuous memory space via gridding or clustering. Our approach involves directly inserting discrete elements into the RNN to extract FSMs, allowing for fine-tuning and visualization. This differs from previous approaches that produce FSM approximations separated from the RNN. Our approach involves inserting discrete elements into the RNN to extract FSMs, allowing for fine-tuning and visualization. This extends previous work focused on learning FSMs and introduces the method of QBN insertion to learn MMNs from a continuous RNN. Our approach focuses on learning discrete representations of memory and observations within a recurrent neural network (RNN), while allowing the rest of the network to use arbitrary activations and weights. This is done to support interpretability rather than efficiency in reinforcement learning policies that require internal memory. Our approach involves using an RNN to learn discrete representations of memory and observations, supporting interpretability in reinforcement learning policies that require internal memory. The RNN maintains a hidden state that influences action choice based on current observation and state, extracting observation features and outputting actions according to a policy function. The text discusses the function of transitioning to a new state using gating networks like LSTMs or GRUs to interpret memory. It introduces Moore Machines and their deep network counterparts for extracting compact quantized representations of memory and observations. A Moore Machine is a standard finite state machine labeled by output values corresponding to actions. It is described by hidden states, observations, actions, a transition function, and a policy mapping hidden states to actions. Moore Machine Networks (MMN) are Moore Machines where the transition function determines the next hidden state based on the current state and observation. A Moore Machine Network (MMN) is a Moore Machine where the transition function and policy are represented by deep networks. MMNs map continuous observations to a finite discrete observation space using a deep network. Quantized state and observation representations are considered, with discrete vectors for each state and observation. The quantization level is denoted as k, with dimensions B_h and B_f for the vectors. An MMN is essentially a traditional RNN with memory composed of k-level activation units and environmental observations transformed to a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for complex problems, even though RNNs can be learned relatively easily. The new approach for learning MMNs leverages the ability to learn RNNs by using quantized bottleneck networks (QBNs) to embed continuous features and hidden states into a k-level representation. The QBNs are inserted into the original recurrent net with minimal behavior changes, allowing for fine-tuning after insertion. This results in a network that consumes quantized features and maintains quantized state. A QBN is an autoencoder with a k-level bottleneck that discretizes continuous space by quantizing activations in the encoding layer. It is represented by a multilayer encoder E mapping inputs x to a latent state. A QBN is represented by a multilayer encoder E mapping inputs x to a latent encoding E(x) and a corresponding multilayer decoder D. The QBN output is quantized using 3-level quantization of +1, 0, and -1. To support 3-valued quantization, an activation function \u03c6(x) = 1.5 is used instead of tanh due to its flatter gradient near zero input. The activation function used in the QBN is \u03c6(x) = 1.5 tanh(x) + 0.5 tanh(\u22123x). The quantize function in QBN makes b(x) non-differentiable, posing a challenge for backpropagation. The straight-through estimator treats the quantize function as the identity function during back-propagation to address this issue effectively. The inclusion of the quantize function in the QBN allows us to view the last layer of E as producing a k-level encoding. Training a QBN as an autoencoder using L2 reconstruction error for a given input x. Running a recurrent policy in the target environment produces training sequences of triples (o t , f t , h t ). Training two QBNs, b f and b h , on observed features and states respectively. Training two QBNs, b f and b h , on observed features and states respectively allows for low reconstruction error, viewing the latent \"bottlenecks\" as high-quality encodings. These \"wires\" are inserted into the original RNN to propagate input to output with some noise. The RNN can be viewed as an MMN with bottlenecks b f and b h providing a quantized representation of features f t and states h t. Fine-tuning may be needed if QBNs do not achieve perfect reconstruction, but the resulting MMN often performs similarly to the original RNN. During fine-tuning, the MMN is trained on the original RNN's rollout data to match its softmax distribution over actions. Training in this manner is more stable than directly outputting the same action. Visualization tools can be used to analyze the MMN's memory and feature bits for a semantic understanding. The full interpretation problem is not addressed in this work. To gain insight, the MMN can be used to create a Moore Machine over atomic state and observation spaces. By running the MMN, a dataset of consecutive quantized states, features, and actions can be produced for analysis. The Moore Machine's state-space corresponds to distinct quantized states in the data. The Moore Machine is constructed from data to capture transitions, and then minimized using standard techniques to reduce the number of states and observations. This results in a minimal equivalent Moore Machine BID19, significantly reducing the complexity. In experiments, the number of states and observations is reduced to extract MMNs from RNNs without significant loss in performance. The study focuses on the interpretability of recurrent policies in two known domains with ground truth Moore Machines. In benchmark grammar learning problems, Mode Counter Environments (MCEs) vary memory requirements for policies. MCEs are a type of Partially Observable Markov Decision Process with M modes transitioning over time. Each mode has an associated action, and the agent receives a reward for taking the correct action at the end of the episode. The agent receives a reward based on the active mode's correct action at each time step. Memory and observations are used differently in three MCE instances, testing optimal performance. One instance, Amnesia, challenges memory use in determining the optimal policy. In three MCE instances, memory and observations are used differently to test optimal performance. The types include Amnesia, Blind, and Tracker, each requiring varying levels of memory use for selecting optimal actions. In three MCE instances, memory and observations are used differently to test optimal performance. The types include Amnesia, Blind, and Tracker, each requiring varying levels of memory use for selecting optimal actions. For each MCE instance, a recurrent architecture is used with 4 modes. The input feeds into a feed-forward layer with 4 Relu6 nodes, followed by a 1-layer GRU with 8 hidden units, and a fully connected softmax layer for action distribution. Imitation learning is used for training, achieving 100% accuracy on the imitation dataset. MMN Training involves observation and hidden-state with the same architecture. The observation and hidden-state QBN have similar architectures, varying in the number of quantized bottleneck units. Encoders consist of tanh nodes feeding into quantized bottleneck nodes, while decoders have symmetric structures. Training in MCE environments was faster for QBNs compared to RNNs due to not needing to learn temporal dependencies. QBNs were trained with bottleneck sizes of 4 and 8. QBNs with bottleneck sizes of 4 and 8 were embedded into an RNN to create a discrete MMN. Performance was measured before and after fine-tuning, with most cases not requiring fine-tuning due to low reconstruction error. Exceptionally, Tracker (Bh = 4, Bf = 4) required fine-tuning and achieved 98% accuracy. After fine-tuning, Tracker (Bh = 4, Bf = 4) achieved 98% accuracy. Inserting one bottleneck at a time yielded perfect performance, indicating combined error accumulation of the two bottlenecks reduced performance. Moore Machine Extraction showed a decrease in states and observations after minimization, suggesting improved efficiency in MMN learning. After fine-tuning, Tracker achieved 98% accuracy. Moore Machine Extraction showed a decrease in states and observations after minimization, indicating improved efficiency in MMN learning. The MMNs learned via QBN insertions were equivalent to the true minimal machines in most cases, as shown in the Appendix. The machines for Blind and Amnesia demonstrate different memory use. The machine for Blind has only one observation symbol, while the machine for Amnesia shows that each observation symbol leads to the same state. The focus is on policy learning problems using Tomita Grammars as environments with two actions. The focus is on policy learning problems using Tomita Grammars as environments with two actions. The RNN for each grammar is trained with imitation learning using the Adam optimizer and a learning rate of 0.001. The training dataset consists of accept/reject strings with lengths in the range [1, 50]. The RNNs were trained on accept/reject strings with lengths in the range [1, 50]. Test results showed high accuracy except for grammar #6. Bottlenecks were added to the RNNs to create MMNs, with experiments conducted using B h values of 8 and 16. The RNNs were transformed into MMNs by adding bottlenecks. Results in TAB1 show MMNs maintain RNN performance without fine-tuning. MM extraction and minimization in TAB1 demonstrate reduced state-space while preserving performance. This indicates MMN learning leads to minimal machines. The Tomita grammars are 7 languages over the alphabet {0, 1} and the minimized machines are identical to the minimal machines known for these grammars. Applying the technique to RNNs learned for six Atari games using OpenAI gym, it was unclear how large the MMs might be due to the complexity of input observations. The Atari agents have a recurrent architecture with specific preprocessing steps for input observations. The network consists of 4 convolutional layers with Relu activation. This work aims to extract finite state representations for Atari policies, a unique approach compared to previous efforts. The recurrent Atari agents used Relu activation in 4 convolutional layers. A GRU layer with 32 hidden units and a fully connected layer with n+1 units were employed, with softmax for policy and value function prediction. A3C RL algorithm was used with a learning rate of 10^-4 and discount factor of 0.99. Loss on the policy was computed using Generalized Advantage Estimation. Trained RNN performance on six games was reported. We adjusted the encoder input and decoder output sizes to match the dimension of the continuous observation features. The encoder for b h has 3 feed-forward layers with varying nodes, while the decoder is symmetric. Training data for b f and b h in Atari domains was generated using noisy rollouts to increase diversity. We trained bottlenecks for B h \u2208 {64, 128} and B f \u2208 {100, 400} for Atari games, leading to more robust learning of QBNs. The number of potential discrete states for B h can be large, but the actual number observed may be smaller. Each bottleneck was trained to saturation before being inserted into the RNN to create an MMN for each game. Performance of the MMNs was evaluated before and after finetuning for different combinations of B h and B f. After training bottlenecks for different sizes, MMNs were created for Atari games. Fine-tuning was necessary for some games like Boxing and Pong to match RNN performance, while others like Freeway and Bowling did not require it. Breakout and Space Invaders showed lower scores after fine-tuning, but still performed relatively well. After fine-tuning, some Atari games like Breakout and Space Invaders achieved lower scores than the original RNNs, mainly due to poor reconstruction in rare game states. For example, in Breakout, the policy failed to press the fire-button after clearing the first board, resulting in a score drop. This highlights the need for more intelligent approaches to train QBNs to capture critical information in such rare states. MM minimization revealed that MMs often had a large number of discrete states and observations before the minimization process. After fine-tuning Atari games like Breakout and Space Invaders, some achieved lower scores due to poor reconstruction in rare game states. MM minimization showed a significant reduction in the number of states and observations, making analysis by hand feasible. However, understanding complex policies may still be challenging due to the need to interpret observations and states. In Atari games like Pong, Bowling, and Freeway, different types of memory use were observed. Pong has three states and transitions to the same state regardless of the current state, requiring no memory. Bowling and Freeway have only one observation symbol in the minimal Markov Model, simplifying analysis. In Atari games like Bowling and Freeway, the minimal Markov Model has only one observation symbol, leading to open-loop policies that ignore input images. Freeway's policy always takes the Up action, while Bowling has a more complex initial sequence of actions followed by a loop. The MM extraction approach provides insight into the open-loop structure of policies in Atari games like Breakout, Space Invaders, and Boxing. Further analysis of discrete observations and states is needed for future work. Our approach involves extracting finite state Moore Machines from RNN policies by training Quantized Bottleneck Networks to produce binary encodings of memory and input features, which are then inserted into the RNN. This allows us to extract a discrete Moore machine from the MMN for analysis and usage. Our results demonstrate the accuracy of our approach in extracting these machines in known environments. Our approach involves accurately extracting ground truth machines in known environments and demonstrating similar performance to original RNN policies in six Atari games. The extracted machines reveal insights into memory usage, showing small memory states and cases where policies rely solely on memory or observations. This work is the first of its kind. The curr_chunk discusses the need for tools and visualizations to attach meaning to discrete observations in policies. It also suggests analyzing finite-state machine structure for further insight. The MCE is parameterized by mode number, mode transition function, mode life span mapping, and count set. The MCE hidden state consists of a mode and a count of time-steps in that mode. The mode changes based on a lifespan and transition distribution. The agent receives continuous-valued observations at each step. The MCE hidden state includes a mode and a count of time-steps in that mode, with continuous-valued observations determining the mode. The agent must remember the current mode and track how long it has been active to optimize performance. The experiments involve three MCE instances: Amnesia, where memory is not used for decision-making; Blind, where memory is essential for optimal performance. The experiments involve three MCE instances: Amnesia, where memory is not used for decision-making; Blind, where memory is essential for optimal performance. The optimal performance can only be achieved by using memory to keep track of the deterministic mode sequence. This allows testing whether the extraction of an MMN could infer that the recurrent policy is ignoring observations and only using memory. The MCE Tracker is similar to Amnesia but allows for larger \u2206(m) values. An optimal policy needs to pay attention to observations when c t = 0 and use memory to keep track of the current mode and mode count, resulting in difficult problems as the number of modes and their life-spans grow. The experiments involve different MCE instances with varying memory usage. Machines are labeled as accept or reject states, with most being 100% accurate except for Grammar 6."
}