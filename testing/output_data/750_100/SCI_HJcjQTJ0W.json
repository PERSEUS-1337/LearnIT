{
    "title": "HJcjQTJ0W",
    "content": "Massive data on user local platforms cannot support deep neural network (DNN) training due to resource constraints. Cloud-based training poses privacy risks from excessive data collection. To enable cloud-based DNN training while protecting data privacy, we propose using intermediate representations by splitting DNNs between local platforms and the cloud. Local NN generates feature representations without local training to safeguard data privacy. Cloud NN is utilized for training. To enable cloud-based DNN training while protecting data privacy, the local NN generates feature representations without training. The cloud NN is then trained based on these representations for the target learning task. PrivyNet optimizes the local NN topology to balance accuracy and privacy loss, demonstrated with the CIFAR-10 dataset. The use of PrivyNet for cloud-based DNN training with the CIFAR-10 dataset is demonstrated. Cloud services offer an alternative for intensive model training, but pose privacy risks due to excessive data collection. To address this, different data pre-processing schemes are proposed to protect user data privacy during cloud-based training. The utility and privacy requirements for effective intermediate representations in cloud-based DNN training are highlighted. The transformation scheme should be flexible to accommodate different platforms and data types. In recent years, various transformations have been proposed to explore the trade-off between privacy and utility. Syntactic anonymization methods like k-anonymity, l-diversity, and t-closeness aim to protect sensitive attributes in static databases. Differential privacy is proposed as a more formal privacy guarantee compared to syntactic anonymization methods like k-anonymity, l-diversity, and t-closeness. It involves adding noise to data to prevent adversaries from gaining additional knowledge, but it does not limit the total information leakage from released representations. Existing works typically require local platforms to be involved in achieving differential privacy. Existing works for achieving differential privacy often involve local platforms in the backward propagation process, making deployment on lightweight platforms challenging. Non-invertible linear and non-linear transformations are proposed for data anonymization. Linear transformations rely on covariance between data and labels or linear discriminant analysis (LDA) to filter training data, but they may not provide sufficient privacy protection. Nonlinear transformations like minimax filter or Siamese networks offer better privacy protection. PrivyNet is a DNN training framework that allows for fine-grained control of the trade-off between privacy and utility. It divides a DNN model into local and cloud platforms, with the local NN derived from pre-trained NNs for feature extraction and the cloud NN trained for the target learning task. This framework addresses the challenge of controlling privacy loss during interactive training between the cloud and local platforms. PrivyNet divides a DNN model into local and cloud platforms. The local NN generates intermediate representations using non-linear transformations like convolution and pooling. The cloud NN is trained based on these representations to achieve privacy protection. The local NN is derived from pre-trained NNs to avoid local training. PrivyNet is a framework that splits a DNN model for cloud-based training with privacy control. It uses pre-trained NNs to extract general features and protect privacy by selecting specific features to release. The framework characterizes privacy loss and utility of using CNN as the local NN, based on three key factors. The utility of using CNN as the local NN is detailed, with three key factors identified for the privacy and utility trade-off. A hierarchical strategy is proposed to optimize the local NN's topology considering constraints on computation, storage, and privacy loss. PrivyNet is validated using CNN-based image classification, demonstrating efficiency and effectiveness in leveraging pre-trained NNs for intermediate representation generation. The overall characterization flow involves generating feature representations using a pre-trained NN, training an image classification network (ICN) based on these features, and training an image reconstruction network (IRN) to reconstruct original images. Utility is measured by target learning task accuracy, and privacy is measured by the distance between reconstructed and original images. When training the IRN, the original images and feature representations are assumed known, while the transformation (FEN) is unknown. This aligns with the adversarial model detailed in Section 3 and 4. The collection of N training instances is represented by images with D channels and dimensions W \u00d7 H. The label indicator vector y_i \u2208 {0, 1} K denotes correct labels for each image. The transformation t induced by FEN has output feature representations with depth D and dimensions W \u00d7 H. The output feature representations have a depth of D and each feature has dimensions W \u00d7 H. The utility is evaluated by learning a classifier with minimized empirical risk for the target learning task. The utility is defined based on the loss function u = 1 if the classifier's output matches the label y_i, and u = 0 otherwise. The utility of the transformed representations is measured by the accuracy achieved by the classifier, while privacy is evaluated by the reconstruction model minimizing the distance between reconstructed and original images, using the peak signal-to-noise ratio (PSNR) as a metric for privacy loss. The impact of FEN topology on privacy and utility is characterized using PSNR values to measure privacy loss from transformed representations. The FEN is derived from VGG16 pre-trained on Imagenet dataset, with CNN used for image classification and reconstruction tasks. Architectures of VGG16, ICN, and IRN are detailed in Appendix B. The FEN topology, influenced by factors like layer number and output depth, is crucial for the PrivyNet framework. By altering the FEN topology, different representations are generated for evaluation by ICN and IRN. The impact on utility and privacy is illustrated in FIG2, showing how both are affected by these changes. The FEN topology in PrivyNet framework is crucial, impacting utility and privacy. Changes in the number of FEN layers and output depth affect privacy loss and accuracy differently. Smaller PSNR indicates less privacy loss with reduced output depth or increased FEN layers. Accuracy shows small degradation with reduced output depth and remains stable with increased FEN layers, except when both FEN layers and output depth are large. The trade-off between accuracy and PSNR in the PrivyNet framework is shown in FIG2 (c). Two key observations from the figure guide the framework: FEN with different topologies have similar utility when privacy loss is high, and FEN with more layers provide better utility when privacy loss is low. The selected subset of output channels also impacts privacy and utility. The impact of channel selection on privacy and utility of output representations is analyzed. Comparing utility and privacy loss for transformed representations from each single channel, it is found that the best channel achieves 4X utility compared to the worst channel when FEN consists of 4 VGG16 layers. Detailed statistics are provided in Figure 4 (a) and Table 4 (c), showing a 6 dB privacy loss difference between the best and worst channels. The impact of output channel selection on privacy and utility is compared using 6 VGG16 layers to generate the FEN. Different sets of output channels are evaluated for privacy and utility with varying FEN layers and output depth configurations. The impact of output channel selection on privacy and utility is compared using 6 VGG16 layers to generate the FEN. Privacy and utility depend more on the number of FEN layers and output channel depth than on the output channel selection. Leveraging pre-trained CNN for FEN construction allows for exploring the trade-off between utility and privacy by controlling the FEN topology. The trade-off between privacy and accuracy can be managed by adjusting the number of FEN layers. In the next section, the framework PrivyNet is proposed to optimize utility while considering privacy constraints, local computation capability, and storage. The idea of deriving the FEN from a pre-trained NN to control the privacy-utility trade-off has been validated. The impact of FEN topology on computation and storage on local platforms, especially lightweight ones like mobile devices, must be considered when designing. The PrivyNet framework optimizes utility under constraints of privacy, local computation, and storage by leveraging cloud-based services for privacy characterization and conducting performance profiling of different NNs on local platforms. Based on pre-characterization, FEN layers and output depth are determined considering privacy, local computation, and storage constraints. A supervised channel pruning step is conducted to remove ineffective channels for the learning task. Output channels are randomly selected to determine FEN topology, assuming availability of original images. In an adversarial model, it is assumed that the attackers do not know the transformation induced by the FEN, as this would make it difficult to evaluate and limit privacy loss. The assumption is necessary to provide a worst-case evaluation of privacy loss when releasing feature representations, with the possibility of attackers injecting images into a database to obtain corresponding representations. The FEN, derived from pre-trained NNs, needs anonymity protection to prevent privacy loss. The pre-characterization stage involves performance/storage profiling on local platforms and cloud-based privacy characterization for NNs. Performance and storage characterization is crucial for profiling different pre-trained NNs on local platforms. The FEN, derived from pre-trained NNs, requires anonymity protection to prevent privacy loss. Performance and storage profiling on local platforms is crucial for different pre-trained NNs. Privacy characterization for the pre-trained NNs is done using cloud-based services, and the reconstruction network is trained on publicly available data. Verification of this characterization is done by comparing PSNR for FEN with different datasets like CIFAR-10 and CIFAR-100. The PSNR for FEN with different topologies is compared using datasets like CIFAR-10 and CIFAR-100. Experiments show that less than 1000 samples are needed for an accurate characterization with data augmentation. Detailed PSNR values are not crucial for privacy characterization, reducing the training sample requirement. The focus now is on determining the topology for the FEN. In PrivyNet, the topology for the FEN is determined based on the number of layers and output channel depth, considering constraints on local computation, storage, and privacy. The relation between privacy, local computation, and storage on a mobile class CPU is shown in Figure 8. The strategy is established based on these observations. Based on the observations in Section 2 and Figure 8, the strategy for selecting the FEN topology in PrivyNet is as follows: For high privacy requirements, a deep FEN with more layers is chosen, while for low privacy requirements, a shallow FEN is selected. The output depth is determined based on privacy constraints to achieve the required privacy level and minimize local computation. Based on privacy constraints, the channel depth is adjusted to minimize local computation and storage consumption. For high privacy requirements, a deep FEN with more layers is chosen, while for low privacy requirements, a shallow FEN is selected. The output depth is determined based on privacy constraints to achieve the required privacy level and minimize local computation. Output:\nOutput channel selection is crucial after determining the number of layers and output depth. Large discrepancies in utility and privacy are observed for a single channel, indicating the need for channel pruning to avoid poor utility with privacy leakage. In Figure 4, it is shown that utility and privacy loss for a single channel are not correlated. Figure 10 demonstrates the negligible correlation, where 4 channels with the largest privacy loss are also among the worst in utility. This observation holds for different output channel depths and FEN layers as shown in Figure 9. This insight allows for optimizing utility while suppressing privacy loss during channel pruning, considering both factors in the process. The privacy loss for each channel can be determined from offline pre-characterization to prune channels with the highest privacy loss. Fisher's linear discriminability analysis (LDA) is used to identify channels with poor utility by measuring the distance of representations within the same class and among different classes using the covariance matrix. Fisher's LDA is found to be a good criterion for this analysis. Fisher's LDA is a good criterion to identify ineffective channels in the matrix. It evaluates the linear discriminability of output channels based on representations within and among classes. The linear discriminability of output channels is evaluated using Fisher's LDA criterion, which considers between-class and within-class variances. The algorithm prunes channels with poor utility to improve accuracy in the learning task, based on the eigenvector corresponding to the largest eigenvalue of S w S b. The effectiveness of the LDA-based supervised channel pruning algorithm is verified in experiments. The experimental setup involves leveraging Fisher's discriminability to identify ineffective channels in the pruning algorithm. Using the first 6 VGG16 layers, 32 output channels with the worst utility are pruned. Results show that 69.7% of the worst 32 channels can be pruned on average with the proposed method, compared to only 50.3% with random pruning, leading to a 33.5% reduction in the probability of selecting a bad channel randomly. The experimental setup involves using Fisher's discriminability to prune ineffective channels in the algorithm. Results show that 69.7% of the worst 32 channels can be pruned on average with the proposed method, leading to a 33.5% reduction in the probability of selecting a bad channel randomly. The number of samples required for LDA-based pruning is explored, showing similar results for pruning 32 or 64 channels with the worst utility. The experimental results show that the LDA-based pruning process has low computational complexity proportional to the number of samples. The effectiveness of supervised channel pruning is demonstrated by setting the layer of FEN to 6 and output depth to 8. Three settings for comparing privacy and utility are considered: random selection, channel pruning based on privacy and utility characterization, and channel pruning based on privacy characterization and LDA. In the pruning process, 64 channels are pruned. After conducting LDA-based characterization and pruning, 64 channels with the worst utility and 32 channels with the largest privacy loss are pruned. Results show that after pruning, there is an improvement in utility and less privacy leakage compared to random selection. After conducting LDA-based characterization and pruning, 64 channels with the worst utility and 32 channels with the largest privacy loss are pruned, resulting in improved utility and reduced privacy leakage. Our supervised pruning strategy outperforms random selection and achieves similar accuracy with less privacy loss compared to a strategy based on characterization results. In this section, the utility and privacy comparison is discussed for three settings: random selection without pruning, random selection after pure characterization-based pruning, and random selection after LDA-based pruning. The adversarial model adopted in the paper assumes that the transformation induced by the FEN is unknown to attackers, providing better privacy protection. However, strategies are needed to protect the anonymity of the FEN derived from pre-trained NNs, whose structure and weights are available to attackers. In the framework discussed, strategies are proposed to protect the anonymity of the FEN derived from pre-trained NNs. This includes building a pool of pre-trained NNs and applying channel selection procedures to make it harder for attackers to guess how the FEN is derived. In the framework discussed, strategies are proposed to protect the anonymity of the FEN derived from pre-trained NNs by making selected channels unknown to attackers. The privacy and utility are empirically verified by reducing the channel depth of the first convolution layer in VGG16. After reducing the channel depth of the first convolution layer in VGG16, privacy and utility are maintained with a significant decrease in runtime. Further reduction in channel depth for each layer also shows similar results. By selecting channels for intermediate layers, it becomes difficult for attackers to determine the number of channels in the pre-trained NN used for the FEN. PrivyNet is a flexible framework designed to protect the anonymity of the FEN derived from a pre-trained NN. It enables cloud-based training and provides privacy protection for resource-constrained platforms like hospitals. Models can be trained from patient data for disease diagnosis, prevention, and treatment. PrivyNet offers a framework for hospitals to release informative features without compromising patient data privacy. It can also be used on mobile platforms to collect and upload health-related data to the cloud. PrivyNet enables mobile platforms to upload data to the cloud while protecting private information. It offers fine-grained privacy control and is applicable for various end-users. The CIFAR-10 dataset has 60000 color images in 10 classes, while CIFAR-100 has images in 100 classes. The CIFAR-100 dataset consists of 600 images per class, with 500 for training and 100 for testing. The images are 32 \u00d7 32 in size. The FEN is derived from the pre-trained VGG16 BID20 model for privacy and accuracy analysis. CNN is used to construct h for image classification and g for image reconstruction, with architectures detailed in the appendices. BID9 has shown good performance in various image recovery tasks like super resolution and denoising autoencoder. An image IRN is constructed following BID12, using a structure with 8 ResNet blocks per cluster. In experiments, a gradient descent optimizer is used with specific learning rates and mini-batch sizes for tasks like image reconstruction and classification. The learning rate is adjusted during training, with data augmentation techniques applied. The topology of the IRN is determined to ensure accurate image recovery. Experiments are conducted to evaluate image reconstruction quality based on different FEN topologies. In experiments, the PSNR of reconstructed images saturates with the increase of ResNet block clusters. 2 clusters with 8 blocks each are chosen. Performance profiling of VGG16 on mobile and server CPUs is shown, along with storage requirements with increasing VGG16 layers. With the increase of VGG16 layers, there is a rapid rise in local computation and storage requirements. Convolution layers contribute most to computation, while fully connected layers dominate storage, especially with larger input image sizes. Different platforms may face varying bottlenecks due to computation and storage configurations, leading to significant runtime differences. The necessity for a flexible framework considering local computation and storage differences is highlighted by significant runtime variations across platforms. The complexity of extra computation in the second part is determined by the number of samples and output dimensions. The complexity is O((K + N LDA)W^2H^2 + W^3H^3) for the second part of the computation. The extra computation induced by the learning process is determined by N LDA, with the overall introduced computation overhead being small due to usually small N LDA being sufficient for good pruning results."
}