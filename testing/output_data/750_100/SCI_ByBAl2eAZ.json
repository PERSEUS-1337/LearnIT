{
    "title": "ByBAl2eAZ",
    "content": "Deep reinforcement learning methods can use noise injection in the action space for exploratory behavior. An alternative approach is adding noise directly to the agent's parameters, leading to more consistent exploration. Combining parameter noise with traditional RL methods benefits both off- and on-policy methods, as shown in experimental comparisons of DQN, DDPG, and TRPO on various environments. Exploration is a key challenge in deep reinforcement learning to prevent premature convergence to local optima. Various methods have been proposed for efficient exploration in high-dimensional and continuous-action environments, often involving complex additional structures. Various methods have been proposed for efficient exploration in deep reinforcement learning, such as adding temporally-correlated noise or parameter noise to improve exploration. These approaches have shown promise in increasing the exploratory nature of algorithms, but are mainly evaluated for the on-policy setting with small function approximators. This paper explores combining parameter space noise with deep RL algorithms like DQN, DDPG, and TRPO to enhance exploratory behavior. Results show that parameter noise outperforms traditional action space noise in both high-dimensional discrete environments and continuous control tasks. Parameter noise is shown to outperform traditional action space noise in tasks with sparse rewards. The RL framework involves an agent interacting with a fully observable environment modeled as a Markov decision process (MDP). The environment is defined by states, actions, initial state distribution, reward function, transition probabilities, time horizon, and discount factor. The agent's goal is to optimize a policy parametrized by \u03b8, which can be deterministic or stochastic. The agent's goal is to maximize the expected return by optimizing a policy, which can be deterministic or stochastic. Off-policy RL methods like Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG) are considered in this paper. DQN uses a deep neural network to estimate the optimal Q-value. DQN utilizes a deep neural network to estimate the optimal Q-value function, following the Bellman optimality equation. The policy is implicitly defined by Q, and a stochastic greedy or Boltzmann policy is derived from the Q-value function to encourage exploration. The Q-network predicts Q-values for each action and is updated using off-policy data from a replay buffer. DDPG, similar to DQN, uses off-policy data to estimate the Q-value function with a critic and actor. The actor maximizes the critic's Q-values through back-propagation. Exploration is achieved with a stochastic policy involving action space noise. In contrast to off-policy methods, on-policy algorithms like TRPO update function approximators based on the current policy. Trust Region Policy Optimization (TRPO) is an extension of traditional policy gradient methods that improves upon REINFORCE by computing an ascent direction that ensures a small change in the policy distribution. TRPO solves a constrained optimization problem using discounted state-visitation frequencies and an advantage function. This work focuses on policies represented as parameterized functions, denoted as \u03c0 \u03b8, with \u03b8 as the parameter vector. Policies are sampled by adding Gaussian noise to the parameter vector, resulting in a perturbed policy \u03c0 := \u03c0 \u03b8 sampled at the beginning of each episode and kept fixed for the entire rollout. State-dependent exploration is crucial in reinforcement learning. Action space noise and parameter space noise have significant differences. Gaussian action noise leads to different actions being sampled for the same state, while parameter space noise is independent of the current state. This distinction is important for understanding exploration strategies in continuous action spaces. Perturbing deep neural networks with spherical Gaussian noise ensures consistency in actions and introduces a dependence between state and exploratory action. This method was demonstrated by Salimans et al. (2017). Layer normalization BID2 is used between perturbed layers to achieve consistency in actions and introduce a dependence between state and exploratory action in deep neural networks. Adaptive noise scaling is necessary to pick a suitable scale \u03c3, which may vary over time as parameters become more sensitive to noise during learning. Adaptive noise scaling is proposed to adjust the parameter space noise over time, linking it to the variance in action space. This approach aims to address limitations and optimize the perturbed policy in action space. Parameter space noise can be applied above a certain threshold for both off-policy and on-policy methods. In the off-policy case, data collected off-policy can be used for exploration by perturbing the policy and training the non-perturbed network. In on-policy methods, parameter noise can be incorporated using an adapted policy gradient. Incorporated in an on-policy setting, an adapted policy gradient is used to optimize a stochastic policy. The expected return is expanded using likelihood ratios and the re-parametrization trick for N samples. The value of \u03a3 is fixed to \u03c3 2 I and scaled adaptively. This section addresses the benefits of incorporating parameter space noise in state-of-the-art RL algorithms. Parameter space noise is beneficial for RL algorithms, aiding in exploring sparse reward environments effectively. It is compared to evolution strategies for deep policies in terms of sample efficiency. Implementations of DQN and DDPG with adaptive parameter space noise are available online for both discrete-action and continuous control tasks. In discrete-action environments, DQN is compared with parameter noise using ALE benchmark. The scale of parameter noise is adjusted based on KL divergence for fair comparison. By reparametrizing the network to represent the policy \u03c0 implied by Q-values, perturbing the policy instead of Q results in more effective parameter noise implementation in discrete-action environments. This approach ensures a fair comparison between action space noise and parameter space noise without introducing additional hyperparameters. In discrete-action environments, perturbing the policy instead of Q leads to more effective parameter noise implementation. The Q-network is trained following standard DQN practices, while the policy is trained to output the greedy action based on the current Q-network. The parameter space noise approach is compared against regular DQN and two-headed DQN with -greedy exploration. The study compared parameter space noise with regular DQN and two-headed DQN in discrete-action environments. Actions were randomly sampled for the first 50 thousand timesteps to fill the replay buffer before training. Parameter space noise was found to perform better when combined with a bit of action space noise. The experiment included training 21 games of varying complexity for 40 million frames, with overall performance estimated by running each configuration three times. The study compared parameter space noise with regular DQN and two-headed DQN in discrete-action environments. Performance was evaluated on 21 games for 40 million frames, showing that parameter space noise often outperforms action space noise, especially on games requiring consistency. Learning progress starts sooner with parameter space noise. The study compared parameter space noise with regular DQN and two-headed DQN in discrete-action environments, showing that parameter space noise often outperforms action space noise, especially on games requiring consistency. However, parameter space noise is unable to sufficiently explore in extremely challenging games like Montezuma's Revenge, suggesting more sophisticated exploration methods like BID4 may be necessary. It would be interesting to evaluate the effect of parameter space noise when combined with exploration methods like traditional action space noise. Proposed improvements to DQN like double DQN were also mentioned. The study compared parameter space noise with regular DQN and two-headed DQN in discrete-action environments, showing that parameter space noise often outperforms action space noise. Proposed improvements to DQN like double DQN, prioritized experience replay, and dueling networks are orthogonal to our improvements. Experimental validation of this theory is left for future work. Comparing parameter noise with action noise on continuous control environments in OpenAI Gym, DDPG is used as the RL algorithm with similar hyperparameters as the original paper, except for the addition of layer normalization after each layer before the nonlinearity. In continuous control tasks, different noise configurations are compared for their impact on performance: no noise, uncorrelated additive Gaussian action space noise, correlated additive Gaussian action space noise, and adaptive parameter space noise. Parameter space noise is adjusted to match the effects of Gaussian action space noise. In continuous control tasks, various noise configurations are compared for their impact on performance. The evaluation is done on several tasks, with results shown for three environments. Each agent is trained for 1 million timesteps, with evaluations every 10 thousand steps using no noise for 20 episodes. Parameter space noise outperforms other configurations on HalfCheetah, as other exploration schemes converge to a local optimum quickly. Parameter space noise outperforms correlated action space noise on HalfCheetah, indicating a significant difference between the two. DDPG is capable of learning good policies even without noise, and this trend is consistent across different environments. In the Walker2D environment, adding parameter noise reduces performance variance between seeds, aiding in escaping local optima. Parameter noise is evaluated on environments with sparse rewards to see if it enables existing RL algorithms to learn effectively. In a scalable toy example, parameter noise is evaluated on a chain of N states where the agent starts in state s2 and can move left or right. Rewards of r=0.001 in state s1 and r=1 in state sN are compared using adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN. The performance of DQN, bootstrapped DQN, and \u03b5-greedy DQN is evaluated on a chain of N states with varying chain lengths. The policy is evaluated after each episode by performing a rollout with noise disabled. The problem is considered solved if one hundred subsequent rollouts achieve the optimal return. Experimental details are available in Section A.3, with green indicating problem solved and blue indicating no solution found within 2 thousand episodes. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN in solving the simple environment where the optimal strategy is always to go right. However, in more complex environments where the optimal action depends on the state, parameter space noise may not work as well. In challenging continuous control environments with sparse rewards, parameter space noise may not guarantee optimal exploration compared to action space noise. The environments only provide a reward after significant progress towards a goal, such as in SparseCartpoleSwingup where a reward is given if the paddle is raised above a certain threshold. SparseCartpoleSwingup, SparseDoublePendulum, SparseHalfCheetah, SparseMountainCar, and SwimmerGather are challenging environments with sparse rewards. DDPG and TRPO are used to solve these tasks with a time horizon of T = 500 steps. Performance of DDPG is shown in FIG6, while TRPO results are in Appendix F. The performance of DDPG in solving challenging environments with sparse rewards is evaluated by running different configurations with five random seeds. SparseDoublePendulum is relatively easy to solve, while SparseCartpoleSwingup and SparseMountainCar require parameter space noise for successful policies. SparseHalfCheetah also shows some success with DDPG. Parameter space noise can improve exploration behavior in off-the-shelf algorithms like DDPG, but success is not guaranteed for all cases. Evolution strategies also use noise in parameter space for exploration. Evolution strategies (ES) introduce noise in the parameter space to enhance exploration behavior. However, ES lacks temporal information and relies on black-box optimization. By combining parameter space noise with traditional RL algorithms, temporal information can be included while still benefiting from improved exploration. Performance comparison between ES and traditional RL with parameter space noise was conducted on 21 ALE games, showing potential for enhanced exploratory behavior. The comparison between Evolution Strategies (ES) and traditional RL with parameter space noise showed that DQN outperformed ES on 15 out of 21 Atari games, demonstrating the combination of desirable exploration properties of ES with the sample efficiency of traditional RL. In the context of deep reinforcement learning, various techniques have been proposed to enhance exploration, but they are challenging to implement. In the context of deep reinforcement learning, various techniques have been proposed to enhance exploration, such as perturbing policy parameters. R\u00fcckstie\u00df et al. (2008) introduced this method, showing its effectiveness over random exploration. Their study focused on policy gradient methods with low-dimensional policies and state spaces. In contrast, our method is evaluated for both on and off-policy algorithms. Our method is applied and evaluated for both on and off-policy settings, using high-dimensional policies and environments with large state spaces. It is closely related to evolution strategies (ES) and has shown effectiveness in high-dimensional environments like Atari and OpenAI Gym continuous control problems. However, it may suffer from sample inefficiency and disregard temporal structure in trajectories. Bootstrapped DQN has been proposed to aid with more directed and consistent exploration. Parameter space noise is proposed as a simpler and sometimes superior method for exploration compared to traditional approaches like -greedy and additive Gaussian noise. This concept utilizes parameter perturbations directly on the network, showing effectiveness in both on and off-policy settings with high-dimensional policies and large state spaces. Parameter space noise is suggested as a more effective method for exploration in deep RL algorithms like DQN, DDPG, and TRPO compared to traditional action noise. It allows for solving environments with sparse rewards and is seen as a viable alternative to action space noise. The network architecture for ALE BID3 includes 3 convolutional layers with different filter sizes and strides, followed by a hidden layer with 512 units and a linear output layer. ReLUs are used in each layer, and layer normalization is applied in the fully connected part. A policy network with a softmax output layer is included for parameter space noise. Target networks are updated every 10,000 timesteps. The Q-value network is trained using the Adam optimizer with a learning rate of 10^-4 and a batch size of 32. The replay buffer can hold 1 million state transitions. The -greedy baseline linearly anneals from 1 to 0.1 over the first 1 million timesteps. Parameter space noise is scaled adaptively to enforce a maximum KL divergence between perturbed and non-perturbed policies. The policy is perturbed at the beginning of each episode, with the standard deviation adapted every 50 timesteps. To avoid getting stuck, -greedy action selection with = 0.01 is used. 50 K random actions are performed to collect initial data for the replay buffer before training starts. \u03b3 = 0.99, rewards are clipped to [-1, 1], and gradients for the output layer of Q are clipped to [-1, 1]. Observations are down-sampled to 84 \u00d7 84 pixels and converted to grayscale. The observations are down-sampled to 84 \u00d7 84 pixels and converted to grayscale. A concatenation of 4 subsequent frames is used for the network. The network architecture for DDPG includes 2 hidden layers with 64 ReLU units each, with layer normalization applied to all layers. Target networks are soft-updated with \u03c4 = 0.001, and the critic is trained with a learning rate of 10 \u22123 while the actor uses a learning rate of 10 \u22124. The critic is regularized using an L2 penalty with 10 \u22122 and the replay buffer holds 100 K state transitions with \u03b3 = 0.99. Observation dimensions are normalized by an online estimate of mean and variance. Parameter space noise with DDPG is scaled to match action space noise, with \u03c3 = 0.2 for dense environments and \u03c3 = 0.6 for sparse environments. TRPO uses a step. TRPO uses a step size of \u03b4 KL = 0.01, a policy network with 2 hidden layers for nonlocomotion tasks and 2 hidden layers for locomotion tasks. The batch size per epoch is set to 5 K timesteps, and the baseline is a learned linear transformation of observations. OpenAI Gym environments like Swimmer and rllab environments are used for tasks. The curr_chunk describes different reward conditions for various environments like SparseDoublePendulum and DISPLAYFORM5. It uses a simple network with 2 hidden layers for DQN. The state encoding follows BID20's proposal. The curr_chunk describes training multiple agents with a Q-value function using 2 hidden layers and layer normalization. Each agent is trained for up to 2 K episodes with varying chain lengths. Performance is evaluated after each episode, and the problem is considered solved if optimal return is achieved in one hundred subsequent trajectories. The environment for testing exploratory behavior is shown in Figure 6. Figure 6 shows a simple and scalable environment for testing exploratory behavior. Different DQN algorithms are compared, including adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN. The parameters and settings for each algorithm are specified, such as the number of heads, masking probability, annealing schedule, replay buffer size, target network update frequency, and optimizer used. The network is trained using the Adam optimizer with a learning rate of 10^-3 and a batch size of 32 over 100 timesteps. A stochastic policy \u03c0 \u03b8 (a|s) with \u03b8 \u223c N (\u03c6, \u03a3) is used to expand the expected return using likelihood ratios and the reparametrization trick for N samples i \u223c N (0, I) and \u03c4 i \u223c (\u03c0. This method allows for the subtraction of a variance-reducing baseline b i t. The proposed adaption method is used to re-scale \u03a3 := \u03c3^2 I appropriately, with parameter space noise requiring a suitable scale \u03c3. The proposed solution involves adapting the scale of parameter space noise over time by using a time-varying scale \u03c3 k, which is related to the action space variance and updated every K timesteps. This method aims to address limitations in understanding the scale of parameter space noise and its impact on learning. The proposed solution involves updating \u03c3 k every K timesteps based on a distance measure between non-perturbed and perturbed policies. The choice of distance measure depends on the policy representation, with \u03b1 = 1.01 used consistently. For DQN, the policy is implicitly defined by the Q-value function, requiring a careful distance measure selection. The proposed solution involves updating \u03c3 k every K timesteps based on a distance measure between non-perturbed and perturbed policies. The distance measure depends on the policy representation, with \u03b1 = 1.01 used consistently. For DQN, the policy is implicitly defined by the Q-value function, requiring careful distance measure selection to avoid pitfalls. The softmax function is used to calculate Q values for actions in policies, with a probabilistic formulation that measures distance in action space using KL divergence. This approach normalizes Q values and relates to -greedy action space noise, eliminating the need for an additional hyperparameter. The KL divergence between greedy and -greedy policies can be used to set an additional hyperparameter \u03b4 in DDPG by scaling \u03c3 to match the distance between the policies. The distance measure between non-perturbed and perturbed policies is used to set an adaptive parameter space threshold in DDPG. This results in effective action space noise with the same standard deviation as regular Gaussian action space noise. For TRPO, noise vectors are scaled by computing a natural step. In TRPO, noise vectors are adapted by computing a trust region around the noise direction to keep the perturbed policy close to the non-perturbed version. This is done using the conjugate gradient algorithm and a line search along the noise direction for constraint conformation. Learning curves for 21 Atari games are provided in FIG8, and the final performance of ES is compared to DQN with -greedy exploration and parameter space noise exploration in TAB2. The performance of different exploration approaches is compared in reinforcement learning tasks. Adaptive parameter space noise shows stable performance on InvertedDoublePendulum. Overall, results are comparable to other methods, with no noise in action or parameter space achieving similar outcomes. Adding parameter space noise improves the performance of TRPO on challenging sparse environments, as shown in FIG10. This noise, scaled according to parameter curvature, complements action noise in aiding consistent learning."
}