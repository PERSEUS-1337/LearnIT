{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. This paper provides a theoretical justification for using STE by explaining why searching in its negative direction minimizes the training loss. The paper justifies the use of the Straight-Through Estimator (STE) in training activation quantized neural networks. It explains how choosing the proper STE leads to a positive correlation between the expected coarse gradient and the population gradient, making it a descent direction for minimizing the population loss. Additionally, a poorly chosen STE can result in instability. Deep neural networks have been successful in various machine learning applications. However, their deployment requires significant memory and computational resources. A study shows that choosing the right Straight-Through Estimator (STE) is crucial for stability during training, as a poor choice can lead to instability near local minima. This was confirmed through experiments on CIFAR-10. Recent efforts have focused on training coarsely quantized DNNs to achieve memory savings and energy efficiency during inference. This involves solving a challenging optimization problem of minimizing a nonconvex empirical risk function subject to weight quantization constraints. Weight quantization in DNNs has been extensively studied. Weight quantization of DNNs has been extensively studied in the literature. The gradient in training activation quantized DNNs is almost everywhere zero, making standard back-propagation inapplicable. To address this issue, a non-trivial search direction is constructed by modifying the chain rule, replacing zero derivatives with related surrogates. The straight-through estimator (STE) is a proxy derivative used in the backward pass, proposed as an alternative approach to stochastic neurons. Feasible target propagation algorithm was also introduced for learning hard-threshold networks via convex combinatorial optimization. The idea of STE originates from the perceptron algorithm in the 1950s for learning single-layer perceptrons. The algorithm does not calculate the gradient through the standard chain rule but uses a modified chain rule with the derivative of the identity function as a proxy. Convergence has been extensively discussed in the literature. Hinton extended this idea to train multi-layer networks with binary activations, while Bengio proposed a variant using the derivative of the sigmoid function. In training deep neural networks (DNN), the derivative of the signum activation function is substituted with 1 in the backward pass, known as the saturated STE. This idea has been applied to DNN with general quantized ReLU activations, including derivatives of vanilla ReLU and clipped ReLU. Despite empirical success, there is limited theoretical understanding of using STE in training DNN with stair-case activations. Goel et al. (2018) showed convergence of the Convertron algorithm using leaky ReLU activation in a one-hidden-layer network. Recent studies by Wang et al. (2018) and Athalye et al. (2018) discuss scenarios where certain layers are not ideal for back-propagation. Wang et al. proposed using an implicit weighted nonlocal Laplacian layer for classification in DNN, while Athalye et al. used the derivative of a pre-trained fully connected layer in the backward pass. The backward pass differentiable approximation introduced by Athalye et al. in 2018 broke defenses relying on obfuscated gradients. The coarse gradient, obtained through the STE-modified chain rule, is not the standard gradient of the loss function. The reason why searching in its negative direction minimizes the training loss is not explained by the standard gradient descent algorithm. The choice of Straight-Through Estimator (STE) in training quantized ReLU nets is non-unique. To understand STE in optimization, three representative STEs are considered for learning a two-linear-layer network with binary activation and Gaussian data. These include derivatives of the identity function, vanilla ReLU, and clipped ReLUs. The model of population loss minimization is adopted for this analysis. Loss minimization is proven to be achievable with proper choices of Straight-Through Estimator (STE), leading to descent training algorithms. Negative expected coarse gradients based on STEs of vanilla and clipped ReLUs are descent directions for minimizing population loss, resulting in monotonically decreasing energy during training. However, the identity STE can lead to unstable training near certain local minima. Empirical performance of the three STEs on MNIST is also examined. In experiments on MNIST and CIFAR-10, the clipped ReLU STE performs best on deeper networks like VGG-11 and ResNet-20. Training with identity or ReLU STE can lead to instability and lower generalization accuracy, indicating poor STEs generate coarse gradients. The convergence guarantees of perceptron and Convertron algorithms were proved for the identity STE, but these results do not apply to networks with two trainable layers. The identity STE is not a good choice in this case, and it is unclear if their analyses can be extended to other STEs. Monotonicity of quantized activation functions, like in Convertron with leaky ReLU, plays a role in coarse gradient descent. The quantized activation function's role in coarse gradient descent is explored in different STEs. The clipped ReLU matches quantized ReLU at the extrema to avoid instability issues. The energy landscape of a two-linear-layer network with binary activation and Gaussian data is studied in section 2, with main results presented in section 3. Empirical performances of different STEs in 2-bit and 4-bit activation quantization are compared in section 4, highlighting instability phenomena associated with poor STEs in CIFAR. The text discusses the instability phenomena of training algorithms due to poor STEs in CIFAR experiments. Technical proofs and figures are deferred to the appendix. Notations for vectors, matrices, and inner products are defined. The model considered outputs predictions for input Z using trainable weights w and v. The text discusses the instability of training algorithms in CIFAR experiments due to poor STEs. It introduces the model's trainable weights w and v for predicting input Z, with details on the activation function and loss function used. The text introduces the activation function and loss function used in the model, assuming Gaussian distribution for the input Z. The learning task is framed as a population loss minimization problem, with the gradient of the objective function being analytically derived. The text discusses the replacement of the zero component \u03c3 in the gradient calculation with a non-trivial function \u00b5 in the training process of neural networks. This approach aims to improve the back-propagation process by using a derivative of a (sub)differentiable function \u00b5. The text introduces the use of the STE \u00b5 to train a two-linear-layer CNN, resulting in a coarse gradient descent algorithm. It also discusses the landscape of the population loss function and defines the angle between vectors w and w*. The text elaborates on the analytic expressions of the population loss function and the partial gradients with respect to the variables v and w. It discusses the conditions for local minimizers and global minimizers of the model. The text discusses the conditions for local minimizers and global minimizers of the model, showing that stationary points can only be saddle points and certain points are potential spurious local minimizers. The population gradient is proven to be Lipschitz continuous on bounded domains. The text discusses the Lipschitz constant for the population gradient and focuses on the complex case with saddle points and spurious local minimizers. The main results analyze the behavior of gradient descent using different activation functions. The algorithm converges to a critical point with ReLU or clipped ReLU derivatives but not with the identity function. Algorithm 1 with ReLU or clipped ReLU derivatives converges to a saddle point or local minimizer under certain conditions. The convergence guarantee for gradient descent is established assuming infinite training samples. Gradient descent is effective with infinite training samples, descending along the negative coarse gradient direction. As sample size increases, empirical loss becomes more monotonic and smooth, explaining the success of STE in deep learning with large datasets. The same results hold even with a weakened Gaussian assumption on input data. The mathematical analysis for these results is outlined in the following section. In this section, the mathematical analysis for the main results is outlined. The plots show the empirical loss moving in the direction of negative coarse gradient with different sample sizes. Choosing the derivative of ReLU as the STE, the expressions for expected coarse gradient are derived. The key observation is the non-negative correlation between the coarse partial gradient and the population partial gradient. The non-negative correlation between the coarse partial gradient and the population partial gradient is highlighted in the mathematical analysis. The expected coarse gradient direction for minimizing population loss is discussed, along with conditions for descent direction. The behavior of coarse gradient descent compared to gradient descent on the function is also examined. The significance of estimate (12) in guaranteeing the descent property of Algorithm 1 is highlighted. With Lipschitz continuity of \u2207f from Lemma 3, energy decreases monotonically until convergence if \u03b7 is small enough. When Algorithm 1 using ReLU STE converges, it only converges to a critical point of the population loss function. The coarse partial gradient using clipped ReLU STE generally correlates positively with the true partial gradient of the population loss and vanishes only at critical points. If certain conditions are met, there exists a relationship between the expected coarse and true gradients. Lemma 8 states that when Algorithm 1 converges, the coarse gradient and true gradient vanish simultaneously at saddle points. However, Lemma 9 shows that the coarse gradient derived from the identity function does not vanish at local minima, potentially preventing Algorithm 1 from converging there. The expected coarse partial gradient w.r.t. w is non-zero at local minimizers if certain conditions are met. Lemma 10 further discusses the inner product between the expected coarse and true gradients, indicating that the coarse gradient descent may not converge near spurious minimizers. The training loss increases and instability arises when {(v t , w t )} approaches a local minimizer due to the descent property not holding. Empirical performances of vanilla and clipped ReLUs differ on deeper nets. Comparison of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations is done in this section. A plot of the 2-bit quantized ReLU and its clipped version is provided for illustration. In experiments, weights are kept float and the resolution \u03b1 for quantized ReLU must be carefully chosen for accuracy. A modified batch normalization layer is used to approximate a unit Gaussian distribution for activation input. The best \u03b1 can be pre-computed for the activation layer. The resolution \u03b1 for quantized ReLU is pre-computed using a variant of Lloyd's algorithm applied to simulated data. Batch normalization is added before each activation layer. The quantization approach used is similar to HWGQ, with uniform quantization. Stochastic gradient descent with momentum = 0.9 is used as the optimizer for training LeNet-5 on MNIST for 50 epochs. The experimental results for training LeNet-5 on MNIST and VGG-11, ResNet-20 on CIFAR-10 are summarized in Table 1, showing training losses and validation accuracies. Among the three STEs, clipped ReLU performs the best overall, followed by vanilla ReLU and then the identity function. On ResNet-20 with 4-bit activations, using the identity function leads to instability, as predicted. Coarse gradient descent with vanilla and clipped ReLUs converge to minima with high validation accuracies, while the identity function results in lower accuracy. The empirical loss function landscape is not affected by the choice of STE used in training. Training is initialized with two improved minima and the identity STE is used. Starting with a tiny learning rate, training loss and validation error increase within the first 20 epochs. After 20 epochs, the normal learning rate schedule is followed for 200 additional epochs. The training with the identity STE results in a worse minimum. The training with identity STE results in a worse minimum due to the coarse gradient not vanishing at good minima. Similarly, ReLU STE performs poorly on 2-bit activated ResNet-20 due to instability at good minima. When initialized with weights from vanilla and clipped ReLUs on ResNet-20 with 4-bit activations, coarse gradient descent using identity STE is repelled from good minima. The learning rate is set to 10 \u22125 until epoch 20. The first theoretical justification for the concept of STE was provided, showing that it gives rise to a descent training algorithm. Three STEs were considered: derivatives of the identity function, vanilla ReLU, and clipped ReLU, for learning a two-linear-layer CNN with binary activation. Explicit formulas of expected coarse gradients for the STEs were derived, demonstrating that negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing population loss, while the identity STE generates incompatible coarse gradients. The instability issue with coarse gradient descent using ReLU STE was confirmed in CIFAR experiments. Future work aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. When initialized with weights from clipped ReLU STE on ResNet-20 with 2-bit activations, coarse gradient descent using ReLU STE with a learning rate of 10^-5 is unstable, leading to increasing classification and training errors. Lemma 12 discusses Gaussian random vectors with specific vector configurations and their expected values. The proof involves various identities and assumptions about the vectors' components. Lemma 12 states that for Gaussian random vectors with specific configurations, certain expected values hold true. The proof involves identities and assumptions about the vectors' components. The proof of Lemma 14 involves Cauchy-Schwarz inequality and projections of vectors onto complement spaces. If w = 0n, the population loss f(v, w) is determined by specific calculations involving row vectors of Z. Lemma 2 states that if w = 0n and \u03b8(w, w*) \u2208 (0, \u03c0), the partial gradients of f(v, w) with respect to v and w are calculated. The proof involves showing the second claim by deriving equations and inequalities. The objective function is rewritten and its Hessian matrix is found to be indefinite, indicating saddle points at the stationary points. The unique minimizer to the quadratic function is identified, leading to further analysis. The minimizer to the quadratic function is found, showing that for small changes, the function value increases. Additionally, a Lipschitz constant is established to validate the claim. Lemma 14.2 validates the claim by combining inequalities. The expected partial gradient w.r.t. v and w are calculated. Lemma 4 proves the first claim using max and sigma functions. Lemma 5 discusses the inner product between expected coarse and true gradients. Lemma 5 discusses the inner product between expected coarse and true gradients w.r.t. w, showing the existence of saddle points satisfying a certain condition. Lemma 7 discusses the inner product between expected coarse and true gradients w.r.t. w, under certain conditions. It also introduces a constant A crelu > 0 depending on C v and c w. Proof of Lemma 7 involves computing the inner product between expected coarse and true gradients with certain conditions. Lemma 8 proof is similar to Lemma 6. Lemma 9 states that the expected coarse partial gradient with respect to w is calculated using certain identities. Lemma 10 discusses the inner product between expected coarse and true gradients when w is a zero vector and \u03b8(w, w*) is between 0 and \u03c0."
}