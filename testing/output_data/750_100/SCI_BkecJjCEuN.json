{
    "title": "BkecJjCEuN",
    "content": "The aim of this work is to improve label efficiency of large neural networks on audio data using multitask and self-supervised learning. An end-to-end audio feature extractor based on WaveNet is trained, along with task-specific neural networks. Various self-supervised learning tasks are described for unlabeled audio data, showing significant improvements in scenarios with limited labeled data. In scenarios with limited labeled training data, simultaneous training with self-supervised tasks can improve supervised classification tasks by up to 6%. Incorporating data augmentation in multitask setting leads to further performance gains in deep neural networks for modeling and classifying auditory data. In scenarios with limited labeled training data, incorporating self-supervised audio tasks during model training can improve generalization. The technique aims to identify appropriate self-supervised tasks and demonstrate their effectiveness in enhancing model performance. WaveNet can be used as a feature extractor for audio tasks, improving performance by learning multi-scale representations from raw waveform data. This approach is demonstrated on supervised tasks like audio tagging, speaker identification, and speech command recognition, showing the benefits of leveraging unlabeled data for improved performance. The authors demonstrate leveraging unlabeled data to improve performance on various tasks, including using self-supervised tasks for pre-training and transfer learning. The underlying structure common to tasks is discussed, such as Gabor filters for visual processing and gammatone filters for auditory processing. Models trained for multitask learning may synergize to uncover underlying structures like Gabor and gammatone filters in visual and auditory processing. Shared representations allow pooling data from different datasets, but the scarcity of cleanly labeled datasets remains a challenge. Solutions like leveraging unlabeled data and self-supervised tasks for pre-training show promise. Self-supervised learning has shown promising results in leveraging unlabeled data in the visual domain. An end-to-end audio processing network was implemented to find a common embedding of the acoustic waveform within a network modeled after the WaveNet architecture. The WaveNet architecture is used to process the acoustic waveform in an end-to-end audio processing network. The network consists of a trunk with 3 blocks of 6 dilation stacks, each containing gate and filter modules with 64 convolutional units. The trunk is then processed by task-specific \"head\" networks for each experiment. The WaveNet architecture consists of a trunk with 3 blocks of 6 dilation stacks, each containing gate and filter modules with 64 convolutional units. The trunk has an effective receptive field length of 190 samples. The setup was tested on audio tagging, speaker identification, and speech command recognition tasks using labeled and unlabeled datasets. The audio tagging task was trained on the FSDKaggle2018 dataset. The dataset consists of 11,073 audio files in PCM format, divided into training and test sets. Audio segments are cropped to 2 seconds and padded if needed. The WaveNet model produces embeddings that are averaged across time and fed into a fully-connected layer with 512 units and ReLU activation, followed by a softmax output layer for classification. Training involves minimizing cross entropy with one-hot encoded labels. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Individual clips are sourced from interviews with celebrities in various settings. Data from each individual is sourced from multiple interviews, with one interview held out for the test set. Audio segments are cropped to 2 seconds and normalized before being fed to the network. The speech command recognition task involves normalizing clips, applying a pre-emphasis filter, and using a head architecture with global average pooling, 2-layer perceptron, batch normalization, ReLU nonlinearity, and softmax layer with cross-entropy loss. The task is trained on the Speech Commands dataset with 65,000 utterances of 30 short words in 12 categories. The recognition head consists of three 1D convolutions. The speech command recognition head consists of three 1D convolutions with batch normalization, dropout, ReLU nonlinearity, and a final softmax layer. Self-supervised tasks include next-step prediction, noise reduction, and upsampling, trained on both labeled and unlabeled data from the Librispeech dataset. The multitask framework developed for audio uses unlabeled data from the Librispeech dataset for auxiliary tasks. The framework includes convolutional layers with 128 filters and a regression-type loss function. Waveform inputs are preferred over high-level feature representations like spectrograms. State-of-the-art baseline models for audio tasks may vary in network architectures, limiting information gained from self-supervised tasks. Emphasizing models with fewer assumptions about input representation is key for understanding learning dynamics across tasks. Multitask learning improves performance compared to single task training on raw audio. Closing the gap between models trained on spectral representations and waveforms is left for future work. Joint training with self-supervised tasks benefits supervised tasks, with multitask training enhancing audio tagging performance without increasing training data. Incorporating larger versions of Librispeech into training regimen improved performance metrics, with a MAP@3 increase of up to .056 with an additional 500 hours of unlabeled data. Swapping the audio tagging task with speech command classification or speaker identification showed a similar trend with increasing amounts of unlabeled data. The addition of 500 hours of unlabeled data improved performance in speaker identification on the VoxCeleb dataset, with top-5 classification reaching 75.22% from a baseline of 73.81%. Multitask learning enhanced supervised tasks without extra labeled data, outperforming data augmentation techniques. Training a single task model on audio tagging with pitch shifting and additive noise data augmentation resulted in an increase in MAP@3 of .066 and .024 respectively. Combining pitch-shift augmentation with self-supervised tasks yielded the highest performance increase of .089, suggesting effective methods for improving label accuracy. In transfer learning experiments, self-supervised tasks are pre-trained on unlabeled data before fine-tuning with a small amount of labeled data for a supervised task. This approach showed promising results, with a performance increase of .089, indicating the effectiveness of combining different methods for improving label accuracy. Transfer learning experiments were conducted on a trio of tasks, favoring transfer learning over training all tasks simultaneously. The study showed that jointly training a supervised task with self-supervised tasks using a WaveNet-based model on raw audio waveforms can lead to performance gains, especially with limited labeled data. The improved performance scales with the quantity of unlabeled data and can enhance existing data augmentation schemes. Our approach of using self-supervised tasks with a WaveNet-based model on raw audio waveforms can supplement existing data augmentation schemes and lead to performance gains on various audio classification tasks. The multitasking model's ability to forecast frames, remove noise, and perform upsampling suggests a learned representation of the audio, raising questions about the limits and expected improvements in performance. Our model, based on the WaveNet architecture, can handle a broader range of auditory tasks by processing high temporal resolution raw audio signals. This approach supplements existing data augmentation schemes and leads to performance gains on various audio classification tasks. Our WaveNet model processes high temporal resolution raw audio signals using causal dilated convolutions for faster training compared to RNNs. The architecture includes task-specific neural networks built on top of a task-agnostic trunk with stacked, dilated, and causal convolutions. The WaveNet trunk consists of N blocks with dilated causal convolution layers, increasing dilation factors, residual connections, and nonlinearities. Each block is labeled with indices for layers, each consisting of a \"residual atom\" with \"Filter\" and \"Gate\" computations producing hidden state vectors and layer outputs. The WaveNet trunk comprises N blocks with dilated causal convolution layers and increasing dilation factors. Each block has a total effective receptive field of 1 + b(2S-1), resulting in a total effective receptive field of \u03c4 = 1 + N(2S-1). After a hyperparameter search, N = 3 blocks with S = 6 layers were chosen. The WaveNet trunk consists of N = 3 blocks with S = 6 layers each, resulting in a total receptive field of \u03c4 = 190. Each task-specific head is a neural network that shares the trunk with other tasks, allowing independent processing. Tasks have their own objective functions, optimizers, and learning rates. In the experiments, supervised tasks are designated as primary, while self-supervised tasks are auxiliary. Tasks share a WaveNet trunk with independent task-specific heads. The head architectures are kept simple to encourage the trunk to learn a versatile audio representation. The next-step prediction task involves predicting the next value in a sequence of audio waveform frames using a 2-layer convolutional neural network. This task allows for the creation of large training datasets from unlabeled audio data. The next-step prediction task involves using a 2-layer convolutional neural network to predict the next frame of audio in a sequence, treating it as a regression problem with mean squared error as the loss function. The original WaveNet implementation treated this prediction as a classification problem. The denoising task involves training a model to predict clean audio samples from noisy ones by treating noise as an additive random process on the true signal. The model is trained to predict the clean sample given a window of noisy samples. Our noise reduction head is structured similarly to the next-step head, trained to minimize a smoothed L1 loss between clean and noisy waveform inputs. The smooth L1 loss was chosen for stable convergence in the denoising task. The unsupervised upsampling task involves downsampling the audio source and using the downsampled signal as input data, with the original source as the target. This task is analogous to the \"super-resolution\" task in computer vision. The original audio is first downsampled to 4 kHz and then repeated every time-point 4 times to mimic the original signal's 16 kHz sample rate. The network's job is to infer the high frequency information lost during downsampling. The network's task is to infer high frequency information lost during downsampling. An upsampling head with a structure similar to next-step prediction and noise reduction tasks was used. A smooth L1 loss function was used to compare the estimated upsampled audio with the original. Training was done using raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets. All code was written in PyTorch framework. Audio samples were cropped to two seconds and downsampled to 16 kHz. The audio samples were cropped to two seconds, downsampled to 16 kHz, and normalized to lie in the interval [-1, 1]. Noise for the noise-reduction task was added from ChiME3 datasets at SNRs ranging from 10dB to 15dB. Noise types included booth, bus, cafe, pedestrian area, and street junction. A hyperparameter search was conducted over the number of blocks in the trunk and layers for the main task. The study conducted a hyperparameter search over the architecture specifications of the network, including the number of blocks in the trunk and layers per block. The performance of the network was found to be largely unaffected by these specifications, with the learning rate being a key factor. Additionally, searches were done over the depth and width of each auxiliary task head, as well as the learning rate for the head, by pairing each task individually with the main task. The final choice of hyper-parameters was made by selecting values that optimized performance on both the main task and auxiliary tasks. The model was jointly trained on all tasks simultaneously using a uniform weighting strategy for the loss function calculation. The \"Adam\" optimizer was utilized for training. The \"Adam\" optimizer BID6 with specific parameters was used for the tagging task. A batch size of 48 was chosen due to computational limitations. Noise reduction and upsampling tasks required separate forward propagations. Important model parameters can be found in TAB3."
}