{
    "title": "SkgpBJrtvS",
    "content": "Knowledge distillation is a common method for transferring representational knowledge between neural networks. However, it may overlook important structural information of the teacher network. An alternative approach called contrastive learning aims to train a student network to capture more information from the teacher's data representation. Contrastive learning is a new objective that outperforms knowledge distillation in various knowledge transfer tasks, including model compression and cross-modal transfer. When combined with knowledge distillation, it sets a state of the art in many transfer tasks, sometimes even surpassing the teacher network."
}