{
    "title": "SJg9z6VFDr",
    "content": "Recently, various neural networks have been proposed for irregularly structured data such as graphs and manifolds. All existing graph networks have discrete depth, but a new model called graph ordinary differential equation (GODE) has been introduced to extend the idea of continuous-depth models to graph data. The derivative of hidden node states is parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. Two end-to-end methods for efficient training of GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. The GODE model introduces bijective blocks for memory efficiency and accurate gradient estimation. It can be easily adapted to different graph neural networks, improving accuracy in various tasks. Graph neural networks, like convolutional neural networks (CNN), have been successful in tasks such as image classification, video processing, and machine translation. However, CNNs are limited to data represented in a grid, hindering their application in irregularly structured datasets. Graph data structures, on the other hand, represent objects as nodes and relations as edges, making them suitable for modeling irregularly structured data like social networks and protein interaction networks. Graph neural networks (GNN) have been proposed as a new class of models to model graphs, inspired by the success of CNNs. They generalize convolution operations to graphs to capture local information, overcoming the limitations of traditional methods like random walk and graph embedding. GNNs are suitable for modeling irregularly structured data such as social networks, protein interaction networks, and citation graphs. There are two main types of methods for graph convolution: spectral and non-spectral. Spectral methods compute the graph Laplacian and filter in the spectral domain, while non-spectral methods directly perform convolution in the graph domain. GraphSAGE learns a convolution kernel in an inductive manner. GraphSAGE learns a convolution kernel in an inductive manner. Existing GNN models have discrete layers, making it difficult to model continuous diffusion processes in graphs. The neural ordinary differential equation (NODE) treats a neural network as an ODE, extended to graphs as graph ordinary differential equations (GODE) for message propagation. NODEs are trained with the adjoint method. In this work, the neural ordinary differential equation (NODE) is trained with the adjoint method and offers adaptive evaluation and accuracy-speed control. However, NODEs perform poorly in image classification tasks compared to discrete-layer models. The inferior performance is attributed to errors in gradient estimation during training, prompting the proposal of a memory-efficient framework for accurate gradient estimation. The framework proposed improves gradient estimation for free-form ODEs, enhancing performance in benchmark tasks such as classification on CIFAR10. It is memory-efficient and extends to graph data with GODE models, showcasing improved performance. The text discusses the proposal of GODE models for graph data and the improved performance on various datasets. Previous studies have viewed neural networks as differential equations, with NODE treating the network as a continuous ODE. The adjoint method has been used in training NODE. Spectral and non-spectral GNNs are two categories. Spectral GNNs filter in the Fourier domain of a graph, requiring information of the entire graph. Spectral GNNs perform filtering in the Fourier domain of a graph, requiring information of the whole graph to determine the graph Laplacian. Non-spectral GNNs, on the other hand, only consider message aggregation around neighbor nodes, making them localized and computationally efficient. Various spectral methods have been introduced to address the heavy computation burden of non-localized filters, such as incorporating a graph estimation procedure and parameterizing spectral filters into a localized version with smooth coefficients. Defferrard et al. (2016) and Kipf & Welling (2016) introduced fast localized spectral filtering and first-order graph convolution, respectively, for efficient graph processing. Non-spectral methods like MoNet (Monti, 2017) use CNN mixtures to generalize graph convolutions. GraphSAGE focuses on neighbor nodes for convolution operations on graphs. Invertible blocks in neural networks ensure accurate reconstruction of inputs from outputs. Different graph convolution methods like MoNet, GraphSAGE, graph attention networks, and GIN have been developed for efficient graph processing. Invertible blocks are used in normalizing flow models to ensure invertibility for calculating log-density of data distribution. Bijective blocks were later used to build invertible networks, allowing for memory-efficient structures by discarding activation of middle layers. In discrete-layer models with residual connections, the states in each layer are represented by x_k and f_k(\u00b7) is a differentiable function. Adding more layers with shared weights leads to a neural ordinary differential equation (NODE) in the continuous case using z(t) and in the discrete case using x_k as hidden states. The key difference between the two cases is that in the discrete case, different layers have their own function f_k. In discrete-layer models, different layers have their own function f_k, while in the continuous case, f is shared across all time t. The forward pass of a model with discrete layers involves applying an output layer on x_K. Integration in the forward pass can be done using various ODE solvers. The Euler Method, Runge-Kutta Method, VODE solver, and Dopris Solver are commonly used in optimal process control and functional analysis. Model parameters are denoted as \u03b8, independent of time. The adjoint method is illustrated in Figure 1, comparing back-propagation methods on NODE. The ODE solver is discretized at points {t0, t1, ..., tN} during the forward pass, while the adjoint method involves solving the hidden state in reverse-time. The direct back-propagation through ODE solver involves saving evaluation time points during the forward pass and re-building the computation graph during the backward pass to accurately reconstruct the hidden state and evaluate the gradient. In the forward pass, Eq. 2 is solved forward in time, while in the backward pass, Eq. 2 and Eq. 6 are solved in reverse time to optimize \u03b8 and minimize the loss function L. The reverse-time ODE solver is used, with initial conditions from Eq. 5 at time T, but it may cause inaccurate gradient evaluations. The reverse-time ODE solver in adjoint methods may lead to inaccurate gradients due to instability, causing a mismatch between hidden states solved forward and reverse in time. This error impacts the gradient calculation, especially if the ODE is unstable in reverse-time. The Jacobian eigenvalues determine stability in forward and reverse-time ODEs. If eigenvalues have non-zero real parts, ODE may be unstable. High |Re(\u03bb)| leads to sensitivity to numerical errors. Instability affects solution accuracy and gradient computation in adjoint methods. The adjoint method is sensitive to numerical errors when solving ODEs in reverse-time. To address this, direct back-propagation through the ODE solver is proposed. This involves accurately reconstructing hidden states at evaluated time points using two methods: saving activation values in cache or rebuilding the computation graph at those time points. The adjoint method is proposed for accurate gradient estimation in ODE solvers for free-form functions. It involves reconstructing hidden states at evaluated time points using direct back-propagation, ensuring accuracy regardless of the stability of the equations. The adjoint for each step in discrete forward-time ODE solution is defined, with detailed derivations provided in appendix E and F. Algorithm 1 outlines the process for accurate gradient estimation in ODE solvers. The adjoint method is proposed for accurate gradient estimation in ODE solvers for free-form functions. Define model with integration time T and forward and backward functions. Algorithm 1 summarizes the method for accurate gradient estimation in ODE solvers. The method summarized in Algorithm 1 for accurate gradient estimation in ODE solvers involves adaptive stepsize during forward pass, saving memory by deleting middle activations, rebuilding computation graph during backward pass, and performing reverse-time integration. It supports free-form continuous dynamics without constraints. The algorithm for accurate gradient estimation in ODE solvers involves adaptive stepsize during forward pass, saving memory by deleting middle activations, and reverse-time integration. Memory consumption can be reduced by performing step-wise checkpoint method. The memory consumption in ODE solvers can be reduced by using invertible blocks, allowing for more memory-efficient computation. By restricting the form of the function to invertible blocks, the memory consumption can be reduced to O(Nf). This involves splitting the input into two parts and using bijective blocks for forward and inverse operations. By splitting input x into x1 and x2 with shape N \u00d7 C2, bijective blocks can be used for forward and inverse operations. The output of a bijective block is denoted as (y1, y2) with the same size as (x1, x2). Differentiable neural networks F and G are used, along with a bijective function \u03c8(\u03b1, \u03b2) and its inverse \u03c8\u22121(\u03b1, \u03b2). Theorem 1 states that if \u03c8(\u03b1, \u03b2) is bijective w.r.t \u03b1 when \u03b2 is given, then the block defined is a bijective mapping. This allows for the application of different \u03c8 functions for various tasks without the need to store activations. Graph neural networks can be memory-efficient as x can be reconstructed from y without storing activations. The process involves discrete and continuous layers, extending to graph ordinary differential equations (GODE). GNNs are typically represented in a message passing scheme, with nodes and edges in a graph. Graph neural networks operate at different layers with edges represented by e u,v. N(u) is the set of neighbor nodes for node u. \u03b6 is a permutation invariant operation. \u03b3(k) and \u03c6(k) are functions parameterized by neural networks. GNNs can be viewed as a 3-stage model for a specific node u: (1) Message passing from neighbor nodes v to u, (2) Message aggregation by node u from its neighbors, using function \u03b6. Graph neural networks operate at different layers with edges represented by e u,v. N(u) is the set of neighbor nodes for node u. \u03b6 is a permutation invariant operation. The states of a node are updated according to its original states and aggregation of messages. A discrete-time GNN can be converted to a continuous-time GNN using a graph ordinary differential equation (GODE). GODE can capture highly non-linear functions and potentially outperform its discrete-layer counterparts. Graph neural networks can be converted from discrete-time to continuous-time using a graph ordinary differential equation (GODE), which has the potential to outperform its discrete-layer counterparts. The asymptotic stability of GODE is related to over-smoothing phenomena. Graph convolution is a special case of Laplacian smoothing, where the continuous smoothing process involves eigenvalues of the symmetrically normalized Laplacian. The ODE derived from the symmetrically normalized Laplacian has real and non-positive eigenvalues, leading to asymptotic stability. As time progresses, all trajectories converge, potentially causing a drop in classification accuracy. Experiments with a CNN-NODE on CIFAR10 and CIFAR100 datasets were conducted to evaluate the method. Experiments were conducted with a CNN-NODE on various image and graph classification tasks, including CIFAR10, CIFAR100, bioinformatic graph datasets (MUTAG and PROTEINS), social network graph datasets (IMDB-BINARY, REDDIT-BINARY), and citation networks (Cora, CiteSeer, PubMed). The raw datasets were inputted into the models without pre-processing for graph classification tasks, and transductive inference was used for node classification tasks. The train-validation-test split by Kipf & Welling (2016) was strictly followed. For image classification tasks, a ResNet18 model was modified into its corresponding NODE model with a sequence of conv-bn-relu layers. GODE can be applied to any graph neural network by replacing functions in the model. GODE can be easily generalized to existing structures like GCN, GAT, ChebNet, and GIN. Different depths of layers were trained for fair comparison, with the best results reported for each model structure. The best results were reported for different model structures using the same hyper-parameters. Channel numbers and layer depths were varied for graph and node classification tasks. Experimentation with different number of hidden layers was conducted, comparing adjoint method and direct back-propagation on the same network. Direct back-propagation consistently outperformed the adjoint method on image classification tasks, demonstrating higher accuracy for both CNN-NODE and graph networks. This validates the analysis on the instability of the adjoint method, attributed to the reverse-time ODE instability. Our training method for NODE18 reduced error rates compared to the adjoint method. Our training method for NODE18 significantly reduces error rates on CIFAR10 and CIFAR100 compared to the adjoint method. NODE18, with the same parameters as ResNet18, outperforms deeper networks like ResNet101 on both datasets. Additionally, our method consistently outperforms the adjoint method on various benchmark graph datasets. Our method supports NODE and GODE models with free-form functions, demonstrating the generalizability of bijective blocks.\u03c8(\u03b1, \u03b2) can be any differentiable bijective mapping w.r.t. \u03b1 when \u03b2 is given. The study demonstrates the effectiveness of GODE models over discrete-layer models in various tasks, showing that different \u03c8 functions behave similarly. The continuous-time model is highlighted as more crucial than the coupling function \u03c8, with lower memory cost validated. Results from experiments comparing GODE models with discrete-layer counterparts show that GODE models performed significantly better on graph classification tasks. Integration time was tested during inference, with results indicating that a shorter integration time led to the network not gathering enough information. The study compared GODE models with discrete-layer counterparts on graph classification tasks. Results showed that a shorter integration time led to insufficient information gathering, while a longer time caused over-smoothing issues. GODE proposes a memory-efficient back-propagation method for gradient estimation in NODEs, demonstrating superior performance in image classification and graph data tasks. The paper also links GNN over-smoothing to ODE asymptotic stability. Our paper addresses the gradient estimation problem for NODE, improving accuracy on benchmark tasks to match state-of-the-art discrete layer models. Experiments are conducted on various datasets including citation networks, social networks, and bioinformatics datasets. The structure and experiments for the invertible block are explained, with details summarized in Table 1 and shown in Fig. 1. Invertible blocks' structure is detailed in Fig. 1, following modifications by Gomez et al. (2017). The algorithm includes a parameter state checkpoint method for accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch. \"Inversion\" refers to reconstructing input from output, and \"backward\" to gradient calculation. Memory consumption is reduced in the forward process. The bijective block is memory efficient, using \"backward\" for gradient calculation. Outputs y1, y2 are kept in the forward function, while other variables are deleted. In the backward function, x1, x2 are calculated from y1, y2, and gradients are computed. Memory consumption comparison was done with a GODE model using memory-efficient and memory-inefficient methods on the MUTAG dataset. Memory consumption of bijective blocks was measured with a batchsize of 100 on the MUTAG dataset. Results showed that the memory-efficient method only increased from 2.2G to 2.6G when depth increased from 10 to 20, while the conventional method increased from 5.3G to 10.5G. The bijective block theoretically takes O(1) memory as it only needs to store outputs in cache and deletes activations of middle layers. The memory-efficient bijective blocks method only slightly increases memory consumption compared to the conventional method. The algorithm involves caching outputs and deleting activations of middle layers, with minimal memory usage. The stability of an ODE in forward-time and reverse-time is determined by the eigenvalues of the Jacobian of the ODE's function. The stability of an ODE in forward-time and reverse-time is determined by the eigenvalues of the Jacobian of the ODE's function. For a bijective block, the forward and reverse mappings must be defined, and the mappings need to be injective and surjective for the block to be bijective. The mapping is proven to be injective by showing that if F orward(x1, x2) = F orward(x3, x4), then x1 = x3 and x2 = x4. By utilizing the bijective property of \u03c8, it is demonstrated that the mapping is injective. Additionally, the construction of x1, x2 is shown to satisfy the forward function. The text discusses the construction of x1, x2 to satisfy the forward function, demonstrating the surjective and bijective properties of the mapping. It also includes a computation graph with gradient back-propagation. The gradient of parameters in a neural-ODE model is derived from an optimization perspective, extending from continuous to discrete cases. The continuous model is defined by an ODE with hidden states denoted as z(t) and parameters as \u03b8. The forward pass and loss function are defined, with the training process formulated as an optimization problem. The text also discusses the gradient back-propagation in a neural-ODE model. The text discusses using the Lagrangian Multiplier Method to solve optimization problems in a neural-ODE model. It mentions the Karush-Kuhn-Tucker (KKT) conditions as necessary for optimal solutions and derives results from the KKT condition. The derivative with respect to \u03bb is calculated at the optimal point. The text discusses deriving results from the KKT condition in a neural-ODE model. It transitions from continuous to discrete cases to solve optimization problems using the Lagrangian Multiplier Method. The text discusses deriving results from the KKT condition in a neural-ODE model, transitioning from continuous to discrete cases to solve optimization problems using the Lagrangian Multiplier Method. Terms are presented as the discrete version of Eq. 29, corresponding to the analysis in Eq. 10 and 11."
}