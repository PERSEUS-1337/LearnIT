{
    "title": "Syx_Ss05tm",
    "content": "Deep neural networks are vulnerable to adversarial attacks in computer vision, where perturbations to images can cause the network to misclassify objects. Unlike previous attacks that degrade performance or manipulate specific outputs, these new attacks reprogram the model to perform tasks chosen by the attacker without specifying the desired output for each input. This single perturbation can be added to all inputs to manipulate the model's behavior. Adversarial reprogramming involves adding perturbations to machine learning models' inputs to make them perform tasks chosen by the attacker, even if the model was not trained for those tasks. This can be demonstrated on ImageNet classification models, repurposing them for tasks like counting and classifying MNIST and CIFAR-10 examples. The study of adversarial examples focuses on the danger of attackers causing model prediction errors with minimal input changes, such as affecting a self-driving car's behavior. Adversarial attacks involve manipulating machine learning models' inputs to produce specific outcomes, such as tricking a self-driving car with a sticker or altering photos to inflate insurance claims. Methods have been developed to create and defend against these attacks, which can be untargeted or targeted in nature. Adversarial attacks can be untargeted, aiming to manipulate a model without a specific output, or targeted, where the attacker designs a perturbation for a specific output. It is important to anticipate various adversarial goals to enhance machine learning system security. This work explores a new challenging goal: reprogramming the model. In this work, the focus is on reprogramming a model to perform a task chosen by the attacker without the need to compute the specific desired output. The adversary aims to compute a function g(x) for inputs x, different from the original task of the model producing outputs f(x). By learning reprogramming functions h f and h g, the adversary can map between the two tasks, converting inputs from one domain to another. In this work, the focus is on reprogramming a model to perform a task chosen by the attacker without the need to compute the specific desired output. The adversary aims to compute a function g(x) for inputs x, different from the original task of the model producing outputs f(x). By learning reprogramming functions h f and h g, the adversary can map between the two tasks, converting inputs from one domain to another. The parameters \u03b8 of the adversarial program are adjusted to achieve the desired mapping between the tasks. The concept of adversarial reprogramming involves repurposing a model to perform a new task by transforming input/output formats. The attack does not necessarily have to be imperceptible to humans to be successful. Adversarial reprogramming can have consequences such as theft of computational resources and repurposing of services. Adversarial reprogramming involves repurposing AI-driven assistants into spies or spam bots, abusing machine learning services, and theft of computational resources. The flexibility of neural networks allows for changes to inputs to repurpose the network for new tasks. The number of unique output patterns achievable by moving along a one-dimensional trajectory in input space increases exponentially with network depth. Networks can be trained to high accuracy even with parameter updates restricted to a low dimensional subspace. Adversarial reprogramming involves repurposing AI-driven assistants into spies or spam bots, abusing machine learning services, and theft of computational resources. In this paper, the authors introduce adversarial reprogramming, which involves crafting adversarial programs to make neural networks perform new tasks. Experimental demonstrations show how these programs can alter the function of convolutional neural networks from ImageNet classification to tasks like counting squares, classifying MNIST digits, and classifying CIFAR-10 images. The susceptibility of trained and untrained networks is also examined. Adversarial reprogramming involves crafting programs to make neural networks perform new tasks, altering their function from ImageNet classification to tasks like counting squares, classifying MNIST digits, and CIFAR-10 images. The susceptibility of trained and untrained networks to adversarial reprogramming is examined, along with the possibility of concealing adversarial programs and data. Adversarial examples are intentionally designed inputs to cause machine learning models to make mistakes. Adversarial attacks are designed to cause machine learning models to make mistakes by creating images that lead to incorrect predictions. These attacks can be untargeted or targeted, and have been proposed in various domains such as malware detection, generative models, and network interpretation. The network, when presented with adversarial images, predicts incorrect ImageNet labels. Reprogramming methods aim to produce specific functionality rather than a hardcoded output in adversarial attacks. Adversarial examples can be created by applying the same modification to different inputs, as seen with the \"adversarial patch\" designed to switch model predictions to a specific class. Adversarial reprogramming involves using a single program to manipulate a model's processing of multiple input images, akin to parasitic computing and weird machines that exploit network protocols and run arbitrary code on targeted computers. Adversarial reprogramming repurposes neural networks to perform new tasks, similar to transfer learning methods. Neural networks have versatile properties that make them useful for various tasks. Neural networks can be repurposed for new tasks through transfer learning, where features resembling Gabor filters are developed in early layers. It is possible to train a convolutional neural network for one task and use a linear SVM classifier to make it work for other tasks. This differs from adversarial reprogramming, which allows for model adaptation. Transfer learning allows model parameters to be changed for new tasks, unlike adversarial reprogramming where an attacker manipulates the input without altering the model. Adversarial reprogramming across tasks with different datasets is more challenging than transfer learning. The adversary's objective is to reprogram a neural network to perform a new task by crafting an adversarial program. The adversarial program is formulated as an additive contribution to network input, not specific to a single image but applied to all images. It is defined by parameters W, image width n, and masking matrix M. This method can be extended to various settings beyond ImageNet classification. The adversarial program is defined by parameters W, image width n, and masking matrix M. The adversarial image is created by applying a perturbation to a sample x from the dataset, with the mask M defining the area in the equivalent ImageNet size image X. The adversarial image is created by applying a perturbation to a sample x from the dataset, with the mask M defining the area in the equivalent ImageNet size image X. The adversarial goal is to maximize the probability P(hg(yadv)|Xadv) by mapping adversarial task labels to ImageNet labels. After setting up the optimization problem with a weight norm penalty, the adversarial program has minimal computation cost for the adversary. It only requires computing Xadv and mapping the resulting ImageNet label to the correct class. Adversarial reprogramming must exploit this property during inference. Adversarial reprogramming exploits the nonlinear behavior of the target network, unlike traditional adversarial examples. Experiments on six architectures trained on ImageNet showed successful reprogramming for tasks like counting squares, MNIST, and CIFAR-10 classification. The study explored adversarial reprogramming on various trained models, including ImageNet precisions. It investigated resistance to reprogramming, susceptibility of trained networks, and concealing adversarial data. The procedure started with a simple task of counting squares. The study focused on adversarial reprogramming of ImageNet models, starting with a task of counting squares in images. Images with white squares on gridpoints were embedded in an adversarial program, resulting in larger images with squares at the center. Adversarial programs were trained per ImageNet model, with the first 10 labels representing the number of squares. The study involved reprogramming ImageNet models to count squares in images using adversarial programs. Despite the unrelated ImageNet labels, the adversarial program accurately mastered the counting task for all networks. In this section, adversarial reprogramming is demonstrated on the task of classifying MNIST digits. Test and train accuracy are measured to show that the adversarial program did not simply memorize training examples. MNIST digits are embedded in a frame representing the adversarial program, with ImageNet labels assigned to them. Adversarial programs are trained for each ImageNet model. Examples are shown in FIG1. In this section, adversarial reprogramming is demonstrated on ImageNet models to classify CIFAR-10 images. The adversarial program successfully repurposes the networks, showing generalization from training to test set and robustness to input changes. Examples of the resulting adversarial images are shown. Our adversarial program successfully increased the accuracy on CIFAR-10 images from chance to a moderate level, using minimal computation cost. Adversarial programs trained on CIFAR-10 show visual similarities with other tasks, such as possessing low spatial frequency texture in ResNet architecture. The Inception V3 model trained on ImageNet data using adversarial training is still vulnerable to reprogramming, as shown by the results. The Inception V3 model trained with adversarial training remains vulnerable to reprogramming, with minimal reduction in attack success. Standard defense approaches are ineffective against this type of attack due to differences in goals, program magnitude, and data generalization. Adversarial reprogramming attacks were conducted on models with random weights, using the same setup as in previous experiments. While pretrained networks on ImageNet performed well on the MNIST task, random networks struggled to achieve high accuracy. Only ResNet V2 50 could train to a similar accuracy as trained ImageNet models. The appearance of adversarial programs was noticeably different in random networks. The appearance of adversarial programs in random networks was qualitatively distinct from those in networks pretrained on ImageNet, indicating the importance of the original task for adversarial reprogramming. Randomly initialized networks may perform poorly due to issues like poor scaling of network weights. The initialized networks may perform poorly due to poor scaling of network weights at initialization. Adversarial reprogramming can be influenced by similarities between original and adversarial data, as shown in experiments with randomized pixels on MNIST digits. Despite lacking spatial structure, pretrained ImageNet networks were successfully reprogrammed to classify the shuffled MNIST digits. Reprogramming pretrained ImageNet networks to classify shuffled MNIST digits and CIFAR-10 images showed comparable accuracy to standard datasets, indicating the transferability of neural network reprogramming. The results suggest that transferring knowledge between original and adversarial data does not fully explain susceptibility to reprogramming. It is possible to reprogram across tasks with unrelated datasets and domains by limiting the visibility of adversarial perturbations through size, scale, or concealing the task. In experiments using an Inception V3 model pretrained on ImageNet, adversarial reprogramming was successful in classifying MNIST digits with limited program size. Even with imperceptible adversarial programs, reprogramming remained successful. The study tested concealing adversarial tasks by hiding data and programs within normal images from ImageNet. The adversarial data structure was hidden by shuffling pixels and limiting the scale of the program and data. The reprogramming method was extended to combine adversarial data with the program. The study demonstrated the possibility of concealing adversarial tasks by optimizing the adversarial program to classify MNIST digits using a simple shuffling technique and ImageNet images. The resulting adversarial images closely resembled normal ImageNet images, successfully reprogramming the network with lower accuracy. Our study showed that trained neural networks are more vulnerable to adversarial reprogramming than random networks. Reprogramming remains successful even when the data structure differs significantly from the original task, highlighting the flexibility of repurposing trained weights for new tasks. This suggests the potential for dynamic reuse of neural circuits. Our study demonstrated that trained neural networks are susceptible to adversarial reprogramming, even when the task differs significantly. This indicates the potential for dynamic reuse of neural circuits in modern artificial neural networks, leading to more flexible and efficient machine learning systems. Recent research has focused on building large dynamically connected networks with reusable components. Future research directions include exploring adversarial reprogramming in different domains such as audio, video, and text. Reprogramming trained networks to classify shuffled images suggests potential for cross-domain reprogramming. Adversarial reprogramming of recurrent neural networks, especially those with attention, is an intriguing area for further study. Reprogramming recurrent neural networks (RNNs) with attention or memory can lead to adversarial attacks, allowing attackers to manipulate the network to perform different computational tasks. This could result in theft of computational resources or other nefarious activities. The theft of computational resources through reprogramming RNNs can lead to adversarial attacks, allowing attackers to manipulate the network for nefarious activities, including violating ethical guidelines of system providers. The proposed adversarial attacks aim to reprogram neural networks for novel tasks, showing their surprising flexibility and vulnerability. Future research should focus on understanding and defending against adversarial reprogramming. Neural networks are vulnerable to adversarial reprogramming, even when the adversarial data is unrelated to the original task data. Shuffled MNIST digits are combined with an adversarial program to successfully reprogram the Inception V3 model, despite the unrelated nature of the data."
}