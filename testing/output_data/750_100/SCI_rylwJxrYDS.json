{
    "title": "rylwJxrYDS",
    "content": "The proposed vq-wav2vec algorithm learns discrete representations of audio segments using a self-supervised context prediction task. It utilizes gumbel softmax or online k-means clustering for quantization, allowing for the application of NLP algorithms. Experiments show BERT pre-training achieves state-of-the-art results in phoneme classification and speech recognition tasks. Learning discrete speech representations has gained recent interest, with autoencoding being a popular approach. The vq-wav2vec algorithm learns discrete speech representations through a context prediction task, enabling the application of NLP algorithms to speech data. The encoder maps raw audio to a dense quantized representation, allowing for efficient aggregation into context. The vq-wav2vec algorithm learns discrete representations of audio segments by quantizing raw audio and aggregating it into context representations. Acoustic models are trained using vq-wav2vec and BERT to output transcriptions. The algorithm utilizes wav2vec loss and architecture, with a Gumbel-Softmax approach for choosing discrete variables. The study utilizes online k-means clustering and a Deep Bidirectional Transformer (BERT) on discretized unlabeled speech data to improve acoustic model performance. Results show that BERT representations outperform log-mel filterbank inputs and dense wav2vec representations on TIMIT and WSJ benchmarks. Discretization of audio allows for the application of NLP algorithms to speech data, such as using a sequence to sequence model for speech recognition. The sequence to sequence model from NLP literature can be used for speech recognition using WAV2VEC, which learns audio representations through a self-supervised context-prediction task. The model utilizes convolutional neural networks to produce representations for each time step and aggregates them to distinguish future samples. The model is trained to distinguish future samples by minimizing contrastive loss for different step sizes. Representations from the context network are used in the acoustic model instead of log-mel filterbank features. BERT is a pre-training approach for NLP tasks using a transformer encoder model. The vq-wav2vec approach learns vector quantized representations of audio data using a future time-step prediction task, while BERT is a pre-training approach for NLP tasks using a transformer encoder model. The vq-wav2vec approach utilizes quantized representations of audio data through a future time-step prediction task. It involves convolutional networks for feature extraction, a quantization module for discrete representations, and an aggregator for optimization tasks. The quantization module in vq-wav2vec replaces the original representation with a fixed size codebook for context prediction. It utilizes Gumbel-Softmax and online k-means clustering for one-hot representations and multiple vector quantizations to prevent mode collapse. The quantization module in vq-wav2vec utilizes Gumbel-Softmax for selecting codebook variables in a differentiable way. It involves a linear layer, ReLU, and Gumbel-Softmax for outputting logits. During training, probabilities for variable selection are determined using uniform samples. The vector quantization approach by van den Oord et al. (2017) is an alternative method. The vector quantization approach in vq-wav2vec uses Gumbel-Softmax for codebook variable selection in a differentiable manner. It optimizes future time step prediction loss instead of autoencoder reconstruction loss. Gradients for the encoder network are obtained by back-propagating dL wav2vec /d\u1e91. The final loss includes additional terms with sg(x) \u2261 x, d dx sg(x) \u2261 0 as the stop function. The final loss in vector quantization approach includes terms for future prediction task, moving codebook vectors closer to encoder output, and ensuring encoder outputs are close to a centroid. This prevents mode collapse where only some codewords are used. In a vector quantization approach, strategies like re-initializing codewords or using additional regularizers have been used to prevent mode collapse. Another strategy involves independently quantizing partitions of z, resulting in larger dictionaries and improved performance. The feature vector z is organized into groups and represented by integer indices for efficient processing. In vector quantization, the feature vector is organized into groups and represented by integer indices. The codebook can be shared across groups or not, with shared variables generally yielding competitive results. Training a vq-wav2vec model allows for discretizing audio data, making it suitable for algorithms needing discrete inputs. Using the discretized training data, BERT pre-training can be applied to predict masked input tokens based on context. The BERT model can then improve speech recognition by feeding representations into an acoustic model. Recent BERT training advancements focus on masked input token prediction, with each discretized token representing around 10 ms of audio. BERT training is modified by masking spans of consecutive discretized speech tokens to make masked token prediction harder and improve accuracy. The model is pre-trained on 960h of Librispeech data and then discretized to 345m tokens. After pre-training on 960h of Librispeech data, the model is discretized to 345m tokens. Ablations are performed on a clean 100h subset with 36M tokens. Evaluation is done on TIMIT, a 5h dataset with phoneme labels, and Wall Street Journal, an 81h dataset for speech recognition. Fairseq implementation of wav2vec is used for training. Phoneme evaluation on TIMIT includes 39 different phonemes, while acoustic models on WSJ are trained on 31 graphemes. The fairseq implementation of wav2vec uses vqwav2vec/wav2vec models with 34 \u00d7 10 6 parameters. The encoder has 8 layers with 512 channels each, kernel sizes, and strides. The aggregator consists of 12 layers with skip connections between blocks. Training is done with the wav2vec context prediction loss for 400k. Training for the wav2vec model involves predicting future steps, warming up for 500 steps, and using a cosine schedule for learning rate annealing. The batch size is 10, with random cropping of 150,000 frames for each example. Models are trained on 8 GPUs, with variations for ablations and experiments on the 100h Librispeech subset. The model consists of convolutional layers with specific kernel sizes and strides in the encoder and aggregator. It is trained for 40k updates and uses Gumbel-Softmax with 2 groups and 320 latents per group. The temperature is annealed from 2 to 0.5 over 70% of updates. After training on 960h of Librispeech, 13.5k unique latents are obtained. After training on 960h of Librispeech and quantizing the dataset, 13.5k unique codewords combinations are obtained. Using k-means Models with 2 groups and 320 variables per group, vq-wav2vec on full Librispeech yields 23k unique codewords. BERT base models have 12 layers, model dimension 768, inner dimension 3072, and 12 attention heads. The learning rate is warmed up over the first 10,000 updates to a peak value of 1 \u00d7 10 \u22125, and then linearly decayed over a total of 250k updates. Training is done on 128 GPUs with a batch size of 3072 tokens per GPU. For ablations, a smaller setup with model dimension 512, FFN size 2048, 8 attention heads, and dropout 0.05 is used. Models are trained for 250k updates with a batch size of 2 examples per GPU. Wav2letter is used as the acoustic model and trained for 1k epochs on 8 GPUs for both TIMIT and WSJ. Decoding on WSJ includes a lexicon and a separate language model trained on WSJ data only. The language model evaluation includes a 4-gram KenLM model and a character-based convolutional model. A vq-wav2vec model is trained on Librispeech data, then used to estimate a BERT model. A wav2letter acoustic model is trained on WSJ using BERT or vq-wav2vec representations instead of log-mel filterbanks. Comparisons are made with wav2vec results from the literature. The study compares different language models for wav2vec and vq-wav2vec models, showing that vq-wav2vec with BERT training achieves a new state of the art WER of 2.34 on nov92. The fastest setting is when no language model is used, and vq-wav2vec with Gumbel-Softmax uses 13.5k distinct codewords for audio signal representation. Gumbel-Softmax uses 13.5k codewords for audio signal representation, enabling BERT training with a small vocabulary. Comparison with k-means for vector quantization shows vq-wav2vec with a large number of codewords performs better. Table 3 shows TIMIT phoneme recognition results for various models, including vq-wav2vec with Gumbel and k-means clustering + BERT small. Table 4 displays Librispeech results for a sequence to sequence model without BERT pre-training. Gumbel-Softmax and k-means clustering perform comparably in the no language model setup. In the TIMIT phoneme recognition task, vq-wav2vec and BERT achieve a new state of the art with 11.67 PER, a 21% error reduction compared to wav2vec. The large codeword model narrows the gap to the original wav2vec model. In preliminary experiments, an off-the-shelf Big Transformer was trained on the vq-wav2vec Gumbel-Softmax discretized Librispeech corpus and evaluated on the Librispeech dev/test sets with a 4k BPE output vocabulary. Results show promise but not as good as the state of the art due to the lack of data augmentation. The next investigation focuses on how well vq-wav2vec can compress audio data by training models with different numbers of. vq-wav2vec compresses audio data by training models with varying numbers of groups and variables to adjust codebook size. The compression is measured using bitrate at a sampling rate of 100Hz, showing a tradeoff between bitrate and accuracy on phoneme recognition. Experimental models range from 1 to 32 groups and 40 to 1280 variables, achieving a bitrate range from 0.53 to 33.03 kbit/s. The quantization module is placed after the aggregator module in the small vq-wav2vec setup. The study explores compression algorithms like Codec2, Opus, MP3, and Ogg Vorbis on TIMIT audio data. Results show vq-wav2vec models outperforming across various bitrate settings. Acoustic models on vq-wav2vec outperform other compression algorithms across different bitrate settings. Masking entire spans of tokens yields better results than individual tokens. BERT training on discretized audio data is robust to masking large parts of the input. vq-wav2vec quantizes unlabeled audio data, making it suitable for algorithms requiring discrete data. This approach improves performance on WSJ and TIMIT benchmarks by leveraging BERT pre-training. Future work includes applying other algorithms to audio data and exploring self-supervised pre-training methods. Future work includes finetuning the pre-trained model to output transcriptions instead of feeding features to a custom ASR model. The relationship between variables and groups is investigated, showing that multiple groups are beneficial compared to a single group with many variables. Results on the TIMIT dev set for vq-wav2vec models trained on Libri100 are presented in Table 6, based on three random seeds."
}