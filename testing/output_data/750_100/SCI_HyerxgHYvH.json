{
    "title": "HyerxgHYvH",
    "content": "We propose a Lego bricks style architecture for evaluating mathematical expressions, using small neural networks for specific operations. Eight fundamental operations are identified and learned using feed forward neural networks. Using simple feed forward neural networks, different operations can be designed by reusing smaller networks. For example, larger and more complex networks can be developed to solve n-digit multiplication, n-digit division, and cross product. This bottom-up strategy introduces reusability and allows for generalization in computations involving n-digits, with results shown for up to 7 digit numbers. Unlike existing methods, this solution generalizes for both positive and negative numbers. The success of feed-forward Artificial Neural Networks lies in their ability to learn and handle computations involving an arbitrarily large number of digits. Artificial Neural Networks (ANN) learn from data during training, but often struggle with generalization, leading to performance degradation on unseen data. Techniques like Domain Adaptation can help address these issues, but the behavior indicates a need for improvement in the learning process of neural networks. The learning process in artificial neural networks (ANN) is primarily based on memorization, lacking quantitative reasoning and systematic abstraction. In contrast, other living species, like children, demonstrate the ability to extrapolate numerical operations to higher digits based on understanding and reusing examples. The key to generalization in artificial neural networks (ANN) lies in understanding and reusing memorized examples for higher digit numbers. Complex operations are combinations of simple functions, and developing quantitative reasoning among ANNs involves identifying and learning fundamental operations that can be reused to create complex functions. This approach is inspired by the learning methodology of humans. The study proposes a generalized solution for arithmetic operations using simple feed forward neural networks, which are then scaled up to solve more complex problems like n-digit multiplication and division. This approach aims to reuse memorized examples for higher digit numbers and is inspired by human learning methodology. Our solution is the first to work for both positive and negative numbers, utilizing neural networks known for approximating mathematical functions. Previous work has focused on simple arithmetic operations like multiplication and division, but the challenge lies in generalizing over unseen data, making the network architecture complex. Recent advancements in networks like Resnet, highway networks, and dense networks have also been explored. Recent advancements in neural networks have focused on generalizing over minimal training data for arithmetic functions. Previous models like EqnMaster and Neural Arithmetic Logic Unit (NALU) have attempted to approximate arithmetic functions, but NALU stands out for its use of linear activations and gate operations to predict outputs. However, NALU still struggles to generalize well beyond 3-digit numbers. Optimal Depth Networks using binary logic gates can efficiently perform simple arithmetic functions, unlike NALU which faces extrapolation issues in end-to-end learning tasks. A Feed Forward Network can also solve arithmetic expressions but may not be the most efficient solution. Neural networks inspired by digital circuits can solve simple arithmetic problems efficiently. Building on Binary Multiplier Neural Networks, a new network is proposed to predict arithmetic functions on decimal digits. The proposed model aims to handle arithmetic operations on both positive and negative decimal integers, unlike existing models limited to positive integers. Instead of using a single neural network for different tasks, the approach involves training multiple smaller networks for specific subtasks like signed multiplication and division. These smaller networks are then combined to perform complex arithmetic operations, similar to LSTM's loop unrolling technique. The proposed model aims to handle arithmetic operations on both positive and negative decimal integers by training multiple smaller networks for specific subtasks like signed multiplication and division. These networks can be easily scaled by increasing the shifters and accumulators for accurate arithmetic operations on digital circuits commonly used in computing equipment. Initial work has shown that neural networks can be utilized for these operations. Neural networks can simulate digital circuits for arithmetic operations like multiplication and division. Six neural networks were designed for fundamental operations such as addition, subtraction, multiplication, and place value shifting. Complex neural networks were also created for more advanced tasks. Neural networks can simulate digital circuits for arithmetic operations like multiplication and division. Several complex neural networks were designed for functions such as multiplication and division, which can be used to create an arithmetic equation calculator. Neurons in a network perform a sum transform by multiplying inputs with weights and passing them through an activation function to produce the final output. The addition module is implemented using a single neuron with two inputs and weights set to {+1, +1}. The subtraction module consists of a single neuron with two inputs and weights set to {+1, \u22121}. It facilitates shift-and-add multiplication by multiplying digits of the multiplier with digits of the multiplicand from right to left. The output is then placed at the appropriate position using a place value shifter and combined to obtain the final output. The proposed feed forward network for single digit multiplication has two input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. It takes two 1-digit integers as input and produces 82 possible outcomes. The highest-ranked prediction is selected as the output. The network for single digit multiplication has 2 hidden layers. The first layer performs x + x and x \u2212 x operations, the second layer is a maxpool layer, and the final output layer subtracts the input from the maxpool output. The input sign calculator extracts the sign of an input number x using a specific activation function. The output sign calculator in the neural network model computes the resultant sign from a multiplication or division of two numbers. It uses a combination of 1 and -1 as inputs, with a neural network structure consisting of 2 hidden layers. The first layer adds the two numbers, the second layer applies modulus as an activation function, and the final layer subtracts 1 from the output of the previous layer. The neural network model uses sign and magnitude inputs passed to a hidden layer of 10 neurons, with a soft-sign activation in the output layer for sign multiplication prediction. It assigns a sign to complex operation outputs like multiplication and division. The process involves converting numbers to positive integers, using input and output sign calculators to determine signs, and performing multiplication with 8 fundamental operations. The process involves converting numbers to positive integers, performing multiplication using single digit operations, and assigning a sign to the final output. The multiplication model involves converting numbers to positive integers, performing single-digit multiplication, and assigning a sign to the final output. The division model separates the sign and magnitude during pre-processing, inspired by the long division model. The division model involves multiplying the n-digit divisor with single digit multipliers and subtracting from the n-digit chunk of the dividend. The smallest non-negative integer is selected from the outputs using additional layers. The selected node represents the remainder and quotient of the division. The quotient is combined over iterations and the remainder is carried over to the next digit in the divisor. The architecture of the multiplication network is shown in Figure 2(b,d), and a division model based on digital circuitry for decimal digits can be generated. The comparison between the division architecture proposed in the paper and the Neural Arithmetic and Logic Unit (NALU) implementation is conducted by training the NALU to match claimed results on a prediction dataset. Test results are calculated for both implementations on a dataset ranging from 0 to 30 uniform numbers. Our model outperforms recurrent and discriminative networks in arithmetic operations on integers up to 7 digits, achieving 100% accuracy within their testing range. The signed multiplication capability is exclusive to our model. Experiment 2 compares our results to the state-of-the-art model for arithmetic operations on positive integers. The Neural Arithmetic and Logic Unit (NALU) network achieves 100% accuracy in arithmetic operations on integers up to 7 digits, with signed multiplication capability. Comparison with the state-of-the-art model for arithmetic operations on positive integers is also conducted. In this paper, it is shown that complex tasks can be divided into smaller sub-tasks, which can be solved by training independent small neural networks. Fundamental arithmetic operations are identified and learned using simple feed forward neural networks for solving more complex tasks. The paper demonstrates dividing complex tasks into smaller sub-tasks solved by training independent neural networks for fundamental arithmetic operations. The limitation of using float operation in the tokenizer is noted, but it does not hinder the current work. Future plans include resolving this issue and testing the accuracy of a cross product network. Additionally, there is ongoing work on developing a point cloud network. In future work, a point cloud segmentation algorithm will be developed using a larger number of identical smaller networks to compute normal vectors from 3D points as input."
}