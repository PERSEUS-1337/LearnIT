{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. Additionally, combining quantization-aware training with weight matrix factorization significantly reduces model size and computation for small-footprint keyword spotting while maintaining performance. Incorporating context information by stacking frames in the input improves performance of keyword spotting models deployed on devices. Quantization-aware training helps mitigate performance degradation caused by quantization in keyword models trained using full-precision arithmetic. This method optimizes weights against quantization errors by considering quantized weights in full precision representation. Quantization-aware training is used to optimize weights against quantization errors in building a small-footprint low-power keyword spotting system. Training the wake word model involves using 8 bit and 4 bit quantized models successfully. Dynamic quantization approach is employed for quantizing DNN weight matrices independently column-wise. The accuracy loss due to quantization is incorporated via quantization-aware training. An in-house 500 hrs far-field corpus of diverse speech data is used for experiments. Training is organized into 3 stages using GPU-based distributed DNN training method. The training process involves 3 stages with a small ASR DNN pre-trained in the 1st stage using full ASR phone-targets. The performance of 'naively quantized' models is evaluated, showing AUC improvement with quantization-aware training. DET curves for different quantized models are compared. The 16 bit and 8 bit quantized-models were not significantly different from the full-precision model."
}