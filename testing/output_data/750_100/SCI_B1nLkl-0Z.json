{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions, known as Q-values, are commonly used in reinforcement learning algorithms like SARSA and Q-learning. A new concept of action value is introduced, defined by a Gaussian smoothed version of the expected Q-value in SARSA. These smoothed Q-values still satisfy a Bellman equation and can be learned from experience sampled from an environment. The gradients of expected reward can be obtained from the gradient and Hessian of the smoothed Q-value function, allowing for the development of new algorithms to train a Gaussian policy directly from a learned Q-value. The approach involves developing new algorithms for training a Gaussian policy directly from a learned Q-value approximator, with the ability to learn both mean and covariance during training. This method achieves strong results on standard continuous control benchmarks by incorporating proximal optimization techniques and penalizing KL-divergence from a previous policy. Different notions of Q-value have led to distinct families of RL methods. SARSA uses the expected Q-value, Q-learning exploits a hard-max notion, while Soft Q-learning and PCL use a soft-max form. The choice of Q-value function significantly impacts the resulting policy improvement. In this work, a new notion of action value called the smoothed action value function Q \u03c0 is introduced. Unlike previous notions, it associates a value with a distribution over actions rather than a specific action at each state. This new approach has implications for policy improvement in reinforcement learning algorithms. Smoothed Q-values are introduced as a Gaussian-smoothed version of expected Q-values, possessing properties beneficial for reinforcement learning algorithms. They satisfy single-step Bellman consistency, enable bootstrapping for training function approximators, and allow optimization objectives to be expressed in terms of smoothed Q-values for Gaussian policies. The Smoothie algorithm utilizes derivatives of a smoothed Q-value function to train a policy, avoiding high variance in updates seen in standard policy gradient algorithms like DDPG. Unlike DDPG, Smoothie aims to improve exploratory behavior in reinforcement learning. Smoothie algorithm utilizes a non-deterministic Gaussian policy for better exploratory behavior in reinforcement learning, unlike DDPG. It can incorporate proximal policy optimization techniques by adding a KL-divergence penalty, improving stability and performance. Our formulation improves stability and performance in continuous control benchmarks, competing with state-of-the-art results. In the model-free RL framework, an agent interacts with a stochastic environment to maximize cumulative rewards through Markov decision processes. In a model-free RL framework, an agent interacts with a stochastic environment to maximize cumulative rewards through Markov decision processes. The agent encounters states and emits actions, receiving rewards and transitioning to new states. A stochastic policy produces a distribution over feasible actions, and the optimization objective is expressed in terms of the expected action value function. The policy gradient theorem expresses the gradient of a policy's objective function with respect to its tunable parameters. Reinforcement learning algorithms balance variance and bias when estimating the expected action value function, often using function approximation to estimate accurately. In this paper, the focus is on multivariate Gaussian policies for continuous action spaces in reinforcement learning. The policies are parametrized by mean and covariance functions, mapping the observed state to a Gaussian distribution. New RL training methods are developed for these parametric policies. The paper focuses on multivariate Gaussian policies for continuous action spaces in reinforcement learning, parametrized by mean and covariance functions. A new formulation of the policy gradient, called the deterministic policy gradient, is presented for Gaussian policies where the policy covariance approaches zero. This results in a deterministic policy where sampling always returns the Gaussian mean. The expected future return from a state can be estimated under this policy, allowing for optimization objective gradient calculation. The gradient of the optimization objective for a parameterized policy can be expressed as a characterization of the policy gradient theorem for deterministic policies. The Bellman equation can be re-expressed in the limit of approaching zero. Value function approximators can be optimized by minimizing the Bellman error for transitions sampled from interactions. Algorithms like DDPG alternate between improving the value function and policy through gradient descent. In practice, to improve sample efficiency, BID5 and BID21 use an off-policy distribution based on a replay buffer. This substitution may alter the policy gradient identity, but it has been found to work well in practice. Smoothed action value functions are introduced in this paper, providing an effective signal for optimizing Gaussian policy parameters. Smoothed Q-values, denoted Q \u03c0 (s, a), differ from ordinary Q-values by not assuming the first action is fully specified, but rather only the mean of the distribution is known. To compute Q \u03c0 (s, a), an expectation of Q \u03c0 (s, \u00e3) for actions \u00e3 drawn near a is performed. This approach differs from prior work by learning a function for a Gaussian policy. The approach presented involves directly learning a function approximator for smoothed Q-values, Q \u03c0 (s, a), for Gaussian policies. By leveraging the derivatives of Q \u03c0, one can learn the mean (\u00b5) and covariance (\u03a3) of the policy. This method allows for direct bootstrapping of smoothed Q-values by incorporating a notion of Bellman consistency. The method involves learning Q \u03c0 (s, a) for Gaussian policies by utilizing derivatives to learn the mean (\u00b5) and covariance (\u03a3) of the policy. The Bellman equation enables direct optimization of Q \u03c0, with parameters \u03b8 and \u03c6 for the mean and covariance. The gradient of the objective w.r.t. mean parameters follows the policy gradient theorem, while estimating the derivative w.r.t. covariance parameters requires observing the second derivative of Q \u03c0 w.r.t. actions. The derivative of Q \u03c0 w.r.t. actions can be computed exactly, with a proof provided in the Appendix. The full derivative w.r.t. \u03c6 can be optimized in two ways for Q 2, using multiple samples. One approach involves treating target values as fixed, achieving a fixed point when Q \u03c0 w (s, a) satisfies the Bellman equation recursion. The second approach uses a single function approximator for Q \u03c0 w (s, a), resulting in a simpler optimization process. The second approach involves using a single function approximator for Q \u03c0 w (s, a) and optimizing it by minimizing a weighted Bellman error. Sampling from a replay buffer with knowledge of the sampling probability q(\u00e3 | s), a phantom action a is drawn and optimized for Q \u03c0 w (s, a). When using target networks, training reaches an optimum when Q \u03c0 w (s, a) satisfies the Bellman equation. It is unnecessary to track probabilities q(\u00e3 | s) and assume a near-uniform distribution of actions in the replay buffer. It is possible to save the probability of sampled actions during interactions with the environment. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods to constrain gradient steps. These methods have not been applicable to deterministic policies like DDPG, but a new formulation proposed in this paper allows for trust region optimization. The paper introduces a trust region optimization method for policy gradient algorithms in continuous control problems. It builds on previous work using Q-value functions to learn stable policies and utilizes gradient information to train a policy, similar to deterministic policy gradient methods. The proposed method is a generalization of the deterministic policy gradient, with updates for training the Q-value approximator and policy mean. It differs from Stochastic Value Gradient (SVG) by providing an update for the covariance and estimating the mean update with a noisy Monte Carlo. The proposed method generalizes the deterministic policy gradient by updating the Q-value approximator and policy mean. It introduces updates for the covariance and estimates the mean update with a noisy Monte Carlo sample. Expected policy gradients (EPG) provide updates for the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. The proposed method generalizes the deterministic policy gradient by updating the Q-value approximator and policy mean. It introduces updates for the covariance and estimates the mean update with a noisy Monte Carlo sample. Expected policy gradients (EPG) provide updates for the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. Our process estimates an integral of the expected Q-value function directly, avoiding approximate integrals and simplifying updates. The novel training scheme proposed for learning the covariance of a Gaussian policy relies on Gaussian integrals. This perspective differs from recent advances in distributional RL, focusing on the averaged return of a distribution of actions rather than the distribution of returns of a single action. Although the paper focuses on Gaussian policies, the approach presented is distinct from previous methods. Smoothie is a new RL algorithm introduced in this paper, focusing on Gaussian policies. It utilizes insights from Section 3 to train a parameterized Q \u03c0 w and a Gaussian policy \u00b5 \u03b8 , \u03a3 \u03c6. Algorithm 1 provides a simplified pseudocode for the algorithm, with input parameters including environment, learning rates, discount factor, KL-penalty, batch size, training steps, and target network lag. Smoothie is evaluated against DDPG as a baseline due to its use of gradient information in Q-value approximation. Smoothie's performance is studied on a synthetic task to understand its behavior. Smoothie is evaluated in a synthetic task with a reward function mixture of two Gaussians. The policy mean is initialized on the worse Gaussian. Smoothie learns both mean and variance, while DDPG only learns the mean. DDPG struggles to escape local optima due to fixed exploratory noise. Smoothie successfully solves the task by pointing the policy mean towards the better Gaussian, unlike DDPG which struggles to escape local optima despite high exploration scale. Smoothie adjusts the covariance during training, with the standard deviation decreasing and then increasing before approaching the global optimum. It can escape lower-reward local optima, unlike DDPG. Smoothie adjusts its policy variance based on the reward function's convexity/concavity. Implementations for continuous control benchmarks use feed forward neural networks for policy and Q-values, with exploration determined by an Ornstein-Uhlenbeck process. Additional details are in the Appendix. Each plot displays average reward and standard deviation. Smoothie adjusts policy variance based on reward function convexity/concavity. It competes with DDPG, outperforming in tasks like Hopper, Walker2d, and Humanoid. TRPO lacks sample efficiency. Results compared in FIG2 after hyperparameter search. Smoothie outperforms DDPG in tasks like Swimmer and Ant, showing competitive or better performance overall. Smoothie adjusts policy variance based on reward function convexity/concavity, while DDPG relies on hyperparameter search for exploration. Smoothie exhibits a slight advantage in Swimmer and Ant, with more significant improvements in Hopper, Walker2d, and Humanoid. The average reward for Hopper is doubled, and Humanoid shows the best published results for a method training on millions of environment steps. TRPO, the only other algorithm with better performance, requires tens of millions of steps for comparable reward, highlighting the benefits of using a learnable covariance and non-deterministic policies. The introduction of a KL-penalty in Smoothie improves performance, especially on harder tasks. This penalty encourages stability, addressing the inherent instability in DDPG training. The algorithm shows significant performance improvements in Hopper and Humanoid without sacrificing sample efficiency. The new Q-value function, Q \u03c0, is a Gaussian-smoothed version. The new Q-value function, Q \u03c0, is a Gaussian-smoothed version that allows Smoothie to successfully learn mean and covariance during training, matching or surpassing DDPG performance, especially with a penalty on policy divergence. Learning Q \u03c0 is more sensible than Q \u03c0, as the smoothed Q-values improve the true reward surface. The smoothed Q-values of Q \u03c0 make the reward surface smoother and have a direct relationship with the expected return objective. Future work should explore these claims and apply the motivations for Q \u03c0 to other policies. The specific identity mentioned can be derived using standard matrix calculus. Using standard matrix calculus, we simplify equations for smoother reward surfaces in Q \u03c0."
}