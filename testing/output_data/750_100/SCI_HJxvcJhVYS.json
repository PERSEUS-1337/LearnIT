{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A model based on differential equations is used, but the inference over parameters is challenging. A proposed generalization of Bayesian optimization helps approximate the posterior distribution for better inference. The method uses Stein variational gradient descent on Gaussian process model estimates to approximate the posterior distribution for likelihood-free inference in reinforcement learning environments. It addresses the problem of estimating parameters of a physical system when the likelihood function is unavailable, resorting to likelihood-free methods like approximate Bayesian computation (ABC). Recent methods aim to improve efficiency in simulations for robotics and reinforcement learning applications. This is done by constructing conditional density estimators from joint data or sequentially learning approximations to the likelihood function. Recent methods aim to improve efficiency in simulations for robotics and reinforcement learning applications by combining variational inference methods with Bayesian optimization. A Thompson sampling strategy is used to refine variational approximations to a black-box posterior in order to propose parameters for new simulations. The approach combines variational inference methods with Bayesian optimization to refine black-box posterior approximations using Stein variational gradient descent (SVGD) over Gaussian process (GP) samples. It includes a method to optimally subsample variational approximations for batch evaluations of simulator models. The goal is to estimate a distribution q that approximates a posterior distribution p(\u03b8|y) over simulator parameters \u03b8 given observations y from a target system, without access to a likelihood function p(y|\u03b8). The text discusses using Bayesian optimization to find the optimal distribution q that approximates the target distribution p without access to a likelihood function. The approach involves minimizing the discrepancy between q and p using a kernelized Stein discrepancy and a black-box method that doesn't require gradients of the target distribution. The text discusses using Bayesian optimization to find the optimal distribution q that approximates the target distribution p without access to a likelihood function. It involves using a sampling acquisition function and kernel herding to select candidate distributions and samples of simulator parameters. Instead of placing a GP to model the map from q's parameters to the corresponding KSD, the approach learns q directly via Stein variational gradient descent (SVGD). Gradients of the target log p are not available, so a GP is used to model a synthetic likelihood function. The text discusses using Bayesian optimization to find the optimal distribution q that approximates the target distribution p without access to a likelihood function. The approach involves using a GP to model a synthetic likelihood function for the simulations-observations discrepancy \u2206 \u03b8, which is expensive to evaluate and not differentiable. The GP provides an approximation that is cheap to evaluate and differentiable for smooth kernels, allowing the application of SVGD in the BO loop. Candidate distributions q n \u2208 Q are selected based on certain criteria. Thompson sampling is used to select candidate distributions q n \u2208 Q based on a GP posterior sampling approach, which accounts for uncertainty by sampling functions from the GP posterior. For models like SSGPs, the approach involves sampling weights w n from a multivariate Gaussian to constitute a sample from the posterior. The acquisition function for a SSGP with mean function \u00b5 0 and feature map \u03c6 is defined using SVGD, where particles are optimised through perturbations guided by a SSGP kernel. The acquisition function for a SSGP with mean function \u00b5 0 and feature map \u03c6 is defined using SVGD, where particles are optimized through perturbations guided by a SSGP kernel. The first term in the definition of \u03b6 guides particles to local maxima of logp n, while the second term encourages diversification by repelling nearby particles. Gradients of sample functions are available for SSGP models with differentiable mean functions. For a uniform prior, \u2207 \u03b8 log p(\u03b8) = 0 almost everywhere. Running evaluations of \u2206 \u03b8 from samples \u03b8 \u223c q n updates the GP model with a large number of particles M. The acquisition function for optimizing particles in a SSGP model is defined using SVGD, allowing exploration of the posterior surface. To avoid using a large number of particles directly in simulations, a subset of query parameters is selected through optimal subsampling of candidate q. Kernel herding constructs a set of samples for efficient exploration. Kernel herding constructs a set of samples to minimize error on empirical estimates for expectations under a given distribution q, bounded by the maximum mean discrepancy (MMD). In the case of SSGPs, the algorithm selects informative samples based on the GP posterior kernel. The GP posterior kernel encodes information for the model, providing an embedding for q based on previously observed locations in the GP data. The distributional Bayesian optimisation (DBO) algorithm is outlined in Algorithm 1, with experimental results comparing it to mixture density networks (MDNs) in synthetic data scenarios. The experiment evaluates a method on OpenAI Gym's 3 cart-pole environment using a dataset of parameters sampled from the prior p(\u03b8) and simulator outputs. Summary statistics were calculated and a uniform prior with specific bounds for the environment was used. Results show the method can recover the target system's behavior. Further details can be found in Appendix B. The study presented a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations to the posterior compared to the MDN approach. Results indicate that distributional Bayesian optimization is more sample-efficient for inferring parameters in reinforcement learning applications. The study presented a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations to the posterior compared to the MDN approach. Results indicate that distributional Bayesian optimization is more sample-efficient for inferring parameters in reinforcement learning applications. The method outperforms other likelihood-free inference methods in inferring parameters of a classical reinforcement learning environment. Future work includes scalability and theoretical analysis. The code is available at: https://github.com/rafaol/dbo-aabi2019. To improve time complexity in updating the GP posterior with new observations, Gijsberts and Metta (2013) suggest using Cholesky factors to avoid recomputing A^-1. This method allows for updating the GP posterior with O(M^2) time complexity, which remains constant regardless of the number of data points N."
}