{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions but typically require fully-observed samples for training. However, it is often costly or impossible to obtain complete samples, making partial, noisy observations more practical. A new method proposes learning an implicit generative model using only lossy measurements of samples, showing that the true distribution can be recovered even with per-sample information loss. AmbientGAN is a new method for training Generative Adversarial Networks using measurement models. It shows substantial improvements in qualitative and quantitative results on benchmark datasets. Generative models trained with this method can achieve 2-4 times higher inception scores than baselines. The generator's output is passed through a simulated random measurement function, and the discriminator distinguishes between real and generated measurements. This work addresses the challenge of training a generative model with limited data by directly training from noisy or incomplete samples. The framework allows for learning a generative model from various types of measurements, assuming the measurement process is known and meets specific conditions. Several measurement processes are presented to demonstrate the feasibility of learning a generative model from a dataset of measured samples. Our approach, AmbientGAN, trains GANs by distinguishing real measurements from simulated measurements of generated images. It can construct generative models from noisy observations and low-dimensional projections with information loss. Our approach, AmbientGAN, trains GANs by distinguishing real measurements from simulated measurements of generated images, resulting in drastic per-sample information loss. Theoretical results show that the distribution of measured images uniquely determines the distribution of original images. Our approach, AmbientGAN, trains GANs by distinguishing real measurements from simulated measurements of generated images, resulting in per-sample information loss. The distribution of measured images uniquely determines the distribution of original images. Results are shown for different measurement models, including dropout and random projection. Empirical work explores models without provable guarantees, with results presented for the celebA dataset under occlusions. Incorporating the measurement process into GAN training improves sample quality. Learning from noisy, blurred images from the celebA dataset leads to poor results, but our models produce cleaner samples. Incorporating the measurement process into GAN training improves sample quality. Learning from 2D images in the MNIST dataset using pairs of 1D projections, AmbientGAN recovers underlying structure but struggles with distribution identification up to rotation or reflection. The adversarial framework is powerful for modeling complex data distributions like images, video, and 3D models. Generative models have various applications, including solving inverse problems and making synthetic data more realistic using GANs. The use of GANs in creating realistic synthetic data and translating images between domains has been explored in various studies. Different approaches, such as training generators against discriminators on low-dimensional projections, have shown improvements in stability. Additionally, there are connections to creating 3D object shapes from 2D projections within the AmbientGAN framework. The curr_chunk discusses the use of 2D projections within the AmbientGAN framework for creating realistic synthetic data. It introduces the concept of lossy measurements performed on samples from a real underlying distribution, with each measurement being an output of a measurement function parameterized by \u03b8. The curr_chunk introduces the concept of stochastic measurement functions with parameters distributed by p \u03b8. It aims to create an implicit generative model of p r x using IID realizations from p r y. The goal is to create an implicit generative model of p r x by combining the measurement process with adversarial training. The approach involves using a random latent vector Z and learning a generator G to match the distribution of X g with p r x using a dataset of measurements from Y \u223c p r y. The main idea is to simulate random measurements on generated objects X g and use a discriminator to distinguish real from fake measurements. The discriminator predicts if a given y is from the real or generated measurement distribution, with a quality function q(\u00b7) defining the objective based on the discriminator output. The AmbientGAN objective involves differentiable functions f \u03b8 and utilizes feedforward neural networks for G and D. The model is end-to-end differentiable, allowing for training similar to standard GAN procedures. Stochastic gradients are computed using sampled Z, \u0398, and Y r, updating parameters of G and D alternately. The approach involves updates to parameters of G and D through backpropagation. It is compatible with improvements to GAN objectives, network architectures, and training procedures. Additional information like per sample labels can be easily incorporated. Experimental models include DCGAN, Wasserstein GAN, and Auxiliary Classifier Wasserstein GAN with gradient penalty. The measurement models used in the study are tailored for 2D images, but the AmbientGAN framework is versatile for other data formats. The models include Block-Pixels where each pixel is set to zero independently with a probability, and Convolve+Noise where measurements are obtained by convolving with a kernel and adding noise. The measurements in the study involve various techniques such as Block-Patch, Keep-Patch, Extract-Patch, and Pad-Rotate-Project, which manipulate image patches and pixels to obtain measurement vectors. Unlike previous methods, these techniques may result in loss of patch location information. In the study, different measurement techniques like PadRotate-Project-\u03b8 and Gaussian-Projection are used to obtain measurement vectors from image channels. These methods aim to recover the true underlying distribution of the observed measurements. The mapping of distributions of samples to measurements is invertible, providing a consistency guarantee with the AmbientGAN training procedure. This lemma discusses the unique probability distribution that induces a given measurement distribution in the vanilla GAN model. The previous lemma discusses the unique probability distribution that induces a given measurement distribution in the vanilla GAN model. The following theorems demonstrate that this assumption holds true for various measurement models, such as Gaussian-Projection, Convolve+Noise, and Block-Pixels, within the AmbientGAN framework. These conditions are easily met in scenarios involving Gaussian blurring kernel with additive Gaussian noise. Theorem 5.4 assumes a finite set of pixel values for images, which is common in practical scenarios. It provides a sample complexity result for learning distributions in the AmbientGAN framework. Each image pixel is in a set P, with x \u2208 P n \u2282 R n and 0 \u2208 P. The Block-Pixels measurement model is considered with p as the probability of blocking. The Block-Pixels measurement model with probability p < 1 induces distribution p r y. Three datasets used in experiments: MNIST (28x28 handwritten digits), CelebA (64x64 RGB face images), CIFAR-10 (32x32 RGB images). The CIFAR-10 dataset contains 32x32 RGB images from 10 classes. Different generative models were used for MNIST, celebA, and CIFAR-10 datasets in the experiments. The CIFAR-10 dataset uses an Auxiliary Classifier Wasserstein GAN with gradient penalty (ACWGANGP) following the residual architecture. Different discriminator architectures are used for 2D and 1D outputs. Baseline approaches were implemented to evaluate the AmbientGAN framework's performance. The AmbientGAN framework was evaluated using baseline approaches on a dataset of IID samples. A \"ignore\" baseline was used to test generative model approximation, while a stronger baseline considered invertible measurement functions. In the AmbientGAN setting, the measurement functions may not be invertible, and we may not observe \u03b8 i for each measurement y i in the dataset. Despite these challenges, an approximate inverse function can be used to train a generative model by \"unmeasuring\" the measurements to obtain estimates of x i. In the AmbientGAN setting, measurement functions may not be invertible. To obtain estimates of x i, an approximate inverse function is used to \"unmeasure\" the measurements. Different methods are used for various measurement models, such as blurring for Block-Pixels, Wiener deconvolution for Convolve+Noise, and total variation inpainting for Block-Patch. For Block-Patch measurements, the Wiener deconvolution is used to approximate the inverse function. Inverting Extract-Patch measurements is challenging due to the loss of patch position information. Pad-Rotate-Project-\u03b8 measurements require sampling multiple angles for inverting the Radon transform. Inverting Pad-Rotate-Project measurements is challenging due to the lack of information about \u03b8. Results with AmbientGAN models are reported on a subset of experiments, showing samples generated by baselines and our models. Samples are shown for selected parameter settings, with more results available in the appendix. Results on MNIST, celebA, and CIFAR-10 are provided in the appendix. Our models produce images with good visual quality, while baselines struggle to invert the measurement process due to heavy degradation and noise. Our models demonstrate the ability to create coherent faces and digits by observing only parts of one image at a time, while other measurement models show drastic signal degradation. The generated images have good visual quality, with our models outperforming baselines on MNIST, celebA, and CIFAR-10 datasets. The model can produce images of digits with consistent orientation per class, while another model produces upright digits but of lesser visual quality. The model trained on celebA dataset shows a crude outline of a face using Pad-Rotate-Project-\u03b8 measurements with a DCGAN. The model trained on celebA dataset with a DCGAN has learned a crude outline of a face, highlighting the difficulty in learning complex distributions. Inception scores are used to quantify the quality of generative models in the AmbientGAN framework for CIFAR-10 and MNIST datasets. The final test set accuracy of the model with two conv+pool layers and two fully connected layers was 99.2%. Different models were trained with varying probabilities of blocking pixels, showing that AmbientGAN models outperformed baseline models as the probability of blocking pixels increased. For Convolve+Noise measurements on MNIST, models perform well with low noise variance. However, as noise levels increase, Wiener deconvolution and \"ignore\" baseline deteriorate while AmbientGAN models maintain high scores. Inception scores for 1D projection measurements are reported for AmbientGAN models. The Pad-Rotate-Project model achieves an inception score of 4.18, while the model with Pad-Rotate-Project-\u03b8 measurements scores 8.12. The vanilla GAN model reaches a score of 8.99. The second model trained on 1D projections performs closely to the fully-observed case. In the context of generative models, the inception score vs the probability of blocking pixels on CIFAR-10 is shown. The total variation inpainting method is slow, with performance similar to the unmeasure-blur baseline on MNIST. Inpainting baselines are not run on CIFAR-10. The trend in the plot indicates the superiority of the approach over baselines, with the inception score shown as a function of training iteration. Relaxing the requirement of a large dataset for constructing generative models is discussed. Relaxing the requirement of a high-quality dataset for constructing generative models is discussed, focusing on learning distributions from incomplete, noisy measurements. This approach aims to enable the creation of new generative models for distributions without high-quality datasets. The lemma introduces the data distribution, distribution over parameters, and induced measurement distribution, highlighting the uniqueness of the probability distribution that induces the given measurements. The lemma discusses learning distributions from incomplete, noisy measurements to create new generative models. It highlights the uniqueness of the probability distribution that induces the given measurements, showing that any sequence of random vectors matching the 1D marginals must converge to the true underlying distribution. The unique probability distribution that can match all 1D marginals obtained with Gaussian projection measurements is denoted as p subscripted with the variable name. The Convolve+Noise measurement model with specific conditions leads to a unique distribution that induces the measurement distribution. The probability density functions are denoted by p with the variable name. There is a bijective map between X and Z, with a continuous transformation. The pdfs of X and Z are related through a Jacobian function. The pdf of Y, a sum of two random variables, is a convolution of individual pdfs. The reverse map uniquely determines the true underlying distribution from the measurement distribution, concluding the proof. Theorem 1 from BID11 is stated for the discrete setting using indicator functions. Lemma 10.1 discusses the empirical version of the vanilla GAN objective for a dataset of measurement samples. The optimal discriminator for this objective is defined, and it is shown that any optimal generator must satisfy a certain condition. The proof involves replacing the real data distribution with the empirical version in the context of Empirical Risk Minimization (ERM). The proof of Theorem 5.4 involves comparing real data distribution with the empirical version. It discusses a unique distribution under the Block-Pixels measurement model with a probability of blocking a pixel. A transition matrix is used for random measurement functions on samples from a discrete distribution. The transition matrix A in the context of random measurement functions on samples from a discrete distribution is discussed. If A is invertible, the distribution p x can be recovered from p y. The sample complexity is analyzed using the minimum eigenvalue magnitude of A. The optimal generator must satisfy p. For Block-Pixels measurement, images are divided into classes based on zero pixel values. The transition matrix A is considered for images with zero pixels after measurement. The transition matrix for images with zero pixels after measurement is lower triangular. Each image has at least a (1 \u2212 p) n chance of being unaffected by measurements, forming diagonal entries in the matrix with strictly positive values. The DCGAN model on MNIST uses a noise input with 100 dimensions sampled from a uniform distribution. The generator has two linear layers and two deconvolutional layers, while the discriminator has two convolutional layers and two linear layers. Labels are concatenated with the inputs at each layer. The WGANGP model on MNIST uses two convolutional layers and two linear layers with batch-norm. The generator takes a 128-dimensional latent vector sampled from a uniform distribution. The discriminator has three convolutional layers and one linear layer without batch-norm. The unconditional DCGAN model on celebA uses a 100-dimensional latent vector with one linear layer for the generator. The ACWGANGP model on CIFAR-10 uses a 128-dimensional latent vector with a residual architecture. The generator includes a linear layer and three residual blocks, each consisting of conditional batch normalization, nonlinearity, and upconvolution layers. The generator in the ACWGANGP model on CIFAR-10 includes a linear layer, three residual blocks, conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator consists of one residual block with two convolutional layers, three residual blocks, and a final linear layer. The study explores scenarios where the parameter distribution is only approximately known, unlike the assumed exact knowledge in previous analyses and experiments. The AmbientGAN approach is shown to be robust to mismatches in parameter distribution of the measurement function through an experiment using the Block-Pixels measurement model on the MNIST dataset. Blocking probability values are varied to train AmbientGAN models. After training AmbientGAN models with the dataset, the inception score peaks at p = 0.5 and gradually drops on both sides, indicating robustness to parameter distribution mismatch. The generator learned through AmbientGAN captures the data distribution well and shows potential for compressed sensing applications. Using an AmbientGAN trained with Block-Pixels on MNIST with p = 0.5, we compared reconstruction error vs number of measurements with Lasso. Results show a reduction in measurements using AmbientGAN trained with corrupted samples."
}