{
    "title": "HylzTiC5Km",
    "content": "The Subscale Pixel Network (SPN) is a conditional decoder architecture designed to generate high fidelity images by encoding vast previous context and preserving global semantic coherence and exactness of detail. The Subscale Pixel Network (SPN) is a decoder architecture that generates images as a sequence of slices, capturing spatial dependencies efficiently. Multidimensional upscaling is used to grow images in size and depth through intermediate stages. SPNs are evaluated on generating CelebAHQ and ImageNet images, achieving state-of-the-art likelihood results. Autoregressive models trained with maximum likelihood estimation have shown high fidelity and generalization on held-out data in various domains such as text, audio, images, and videos. These models have achieved state-of-the-art fidelity in many domains, except for unconditional large-scale samples. In the domain of unconditional large-scale image generation, autoregressive models have not yet achieved long-range structure and semantic coherence. The relationship between MLE scores and sample fidelity poses challenges for high-fidelity image generation. MLE ensures model generalization but requires capacity to support the entire empirical distribution. The high dimensionality of large images poses challenges for autoregressive models in achieving long-range structure and semantic coherence. The model needs sufficient capacity to learn dependencies among the 196,608 positions in a 256 \u00d7 256 \u00d7 3 image, leading to high memory and computation requirements. The text discusses the challenges of generating high-fidelity 8-bit RGB images up to 256 \u00d7 256 in size. The model aims to focus on visually salient bits first before moving on to less salient ones. The text discusses generating high-fidelity 8-bit RGB images up to 256 \u00d7 256 in size by focusing on visually salient subsets of the distribution. Multidimensional Upscaling is used to map between subsets by upscaling images in size or depth. The text discusses generating high-fidelity 8-bit RGB images up to 256 \u00d7 256 in size using Multidimensional Upscaling. It involves training three networks: a decoder for small size, low depth images, a size-upscaling decoder, and a depth-upscaling decoder. The Subscale Pixel Network (SPN) is developed to address training difficulties. The Subscale Pixel Network (SPN) architecture is developed to generate high-fidelity 8-bit RGB images up to 256 \u00d7 256 in size using Multidimensional Upscaling. It divides images into sub-images and predicts target slices based on previously generated slices, capturing a rich spatial structure. SPN consists of a conditioning network and a decoder that share weights for image slices. The Subscale Pixel Network (SPN) is an image decoder that can be used for size upscaling. It shares weights for image slices and can generate high-fidelity images up to 256 \u00d7 256. SPN is evaluated on CelebAHQ-256 and ImageNet benchmarks, showing state-of-the-art results in image generation. The Subscale Pixel Network (SPN) achieves state-of-the-art results on CelebAHQ-256 and ImageNet-64, demonstrating strong benefits of multidimensional upscaling and SPN in producing high-quality images. Samples generated at full 8-bit resolution are comparable to those from GANs, with successful samples on unconditional ImageNet-128 showcasing the impact of SPN and multidimensional upscaling on sample quality. The SPN and multidimensional upscaling have a significant impact on sample quality, setting a fidelity baseline for future methods. The PixelCNN model generates color images using a deep neural network, following a raster scan ordering conventionally used in AR models. Each conditional distribution is parametrized by the network. The PixelCNN model generates color images using a deep neural network with an alternative ordering that divides large images into slices, allowing for compact encoding of long-range dependencies and spatial structure alignment. This ordering also enables consistent application of the same decoder to all slices within the SPN. The ordering in the PixelCNN model divides images into smaller slices for self-attention in the SPN without local contexts. The subscale ordering is defined with a scaling factor S, creating interleaved slices in the original image. Each slice is specified by its row and column offset, referred to as the \"meta-position.\" The subscale ordering in the PixelCNN model divides images into slices with meta-positions defined by row and column offsets. Size upscaling can be done explicitly by training a single slice decoder on subimages. The PixelCNN model divides images into slices with meta-positions defined by row and column offsets. Size upscaling can be achieved by training a single-slice decoder on subimages, or by using the Parallel Multi-Scale BID12 ordering where pixels are doubled at every stage by distinct neural networks in parallel. Multidimensional upscaling applies upscaling not just in height and width. Multidimensional upscaling involves upscaling in height, width, and channel depth. It is done in stages where each network generates bits of the image based on the previous bits. Depth upscaling involves generating bits of lower significance only after more significant bits have been generated in a previous stage. The goal is to focus on visually salient bits of an image. This method is related to Grayscale PixelCNN, which models 4-bit greyscale images subsampled from colored images. The existing AR approaches require a lot of computation and memory for processing images. The memory requirements become limiting for larger images, affecting the global context. The Subscale Pixel Network (SPN) addresses challenges in learning global dependencies in image processing by using a scaling factor to obtain slices of the original image. This architecture overcomes limitations in existing approaches that neglect global context. The Subscale Pixel Network (SPN) uses a scaling factor to obtain slices of the original image, ensuring memory and computation requirements remain constant. The SPN architecture consists of an embedding part for slices at preceding metapositions, conditioning the decoder for the current slice being generated. The Subscale Pixel Network (SPN) uses padding slices to maintain meta-positions of preceding slices in the input tensor. This ensures equivariance in the embedding architecture with respect to slice offsets. The padding slices also keep the depth of the input slice tensor consistent for all target slices. The embedding part of the decoder receives the meta-position of the target slice and pixel intensity values as input. The context embedding network utilizes self-attention layers. The decoder processes the target slice in raster-scan order and uses a hybrid architecture combining masked layers. The decoder uses a hybrid architecture combining masked convolution and self-attention to process the target slice in raster-scan order. It employs 1D self-attention network to gather context, followed by masked 1D self-attention layers. The output is reshaped and given as input to a Gated PixelCNN network. The Gated PixelCNN network models the target slice with full masking over pixels and channel dimensions, resulting in significantly lower memory requirements. The log-likelihood derived from equation 2 decomposes as a sum over slices, capturing the entire previously generated context at each position of the decoder. An unbiased estimator of the log-loss is obtained by uniformly sampling a choice of target slice and evaluating its logprobability conditioned upon previous slices. The SPN serves as a size-upscaling network by performing maximum likelihood learning through stochastic gradient descent on a Monte Carlo estimate. It can also be used to upscale the depth of channels in an image. The smaller subimages used for initialization and training of the SPN decoder are identical to each other. The SPN can upscale the depth of channels in an image by dividing it into slices and concatenating them along the channel dimension. This allows for high fidelity samples at high resolution. Our model can generate high-quality unconditional CelebA-HQ samples and achieve state-of-the-art log-likelihoods on high-resolution ImageNet images. The network operates on small images and can train large networks with deep layers and a high number of hidden units. The context-embedding network has 5 convolutional layers and 6-8 self-attention layers. The masked decoder includes a PixelCNN with 15 layers, and the 1D Transformer has 8-10 layers. The hybrid decoder performs well on 32 \u00d7 32 Downsampled ImageNet, outperforming state-of-the-art models. In low-resolution settings with S = 2 and S = 4, SPN performs poorly due to small image slices and coarse graining. Achieving a state-of-the-art log-likelihood of 3.52 bits/dim on 64 \u00d7 64 Downsampled ImageNet, SPN scores similarly with 3.53 bits/dim at this resolution. The improvement over Glow in the 5-bit setting is significant. Experiments were conducted using the standard ILSVRC Imagenet dataset resized with Tensorflow's function. SPN improves log-likelihood on 128 \u00d7 128 ImageNet from 3.55 bits/dim to 3.08 bits/dim. Samples with depth upscaling show significant semantic coherence. Multidimensional upscaling increases overall sample success rate. At 256 \u00d7 256, high-fidelity celebrity face samples are produced from the CelebAHQ dataset, showing quality comparable to other models. Improved MLE scores are achieved, as seen in TAB5. Samples for 8-bit CelebAHQ-256 are showcased in FIG6, with additional samples in Figures 7, 8, and 9 in the Appendix. The SPN and Multidimensional Upscaling model achieves state-of-the-art MLE scores on large-scale images like CelebAHQ-256 and ImageNet-128. It can generate high fidelity 8-bit samples without altering the output distribution. The generated samples exhibit high semantic coherence and detail accuracy even at large scale sizes of 128 \u00d7 128 and 256 \u00d7 256 images. The entropy of the softmax output distributions can be adjusted using a \"temperature\" divisor on the predicted logits. The experiments are conducted on a large scale in terms of compute and network size, with batch size adjusted accordingly. The batch size is adjusted proportionately to maintain the number of pixels in a batch regardless of subscaling. Large batch sizes up to 2048 are achieved using Google Cloud TPU pods, with 64 tensorcores for Imagenet 32 and 128 tensorcores for ImageNet 64, 128, and 256. Decreasing batch size and using 32 tensorcores is preferred for small datasets like CelebA-HQ to address overfitting issues. The SPN architectures have varying parameter counts, with the highest reaching around 650M for ImageNet 128 in a multidimensional upscaling setting. This is achieved by using two separate networks with untied weights and a separate decoder-only network for sizeupscaling. The SPN architecture for ImageNet 128 utilizes a decoder-only network with around 150M parameters, while the total parameter count reaches approximately 650M in a multidimensional upscaling setting."
}