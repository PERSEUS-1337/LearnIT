{
    "title": "Byekm0VtwS",
    "content": "Uncertainty is a key feature in intelligence, making the brain flexible and powerful. Crossbar-based neuromorphic computing chips, which use analog circuits, can mimic the brain but current deep neural networks do not consider their uncertainty. A proposed uncertainty adaptation training scheme (UATS) aims to improve performance on neuromorphic computing chips by incorporating uncertainty in the training process. The experimental results demonstrate that neural networks can achieve comparable inference performances on uncertain neuromorphic computing chips with the uncertainty adaptation training scheme. Uncertainty reasoning is essential in human thinking and intelligent systems, with fuzziness and stochasticity being two types of uncertainties. Fuzziness helps the brain efficiently process real-world information by ignoring redundant details when making distinctions like identifying a cat or a dog. The brain's ability to ignore redundant details like expressions and number of legs when distinguishing between a cat and a dog is not present in most AI systems. Existing AI systems use 32-bit or 64-bit floating numbers to describe weights and activations, but some researchers suggest that 8-bit integers are sufficient for many applications. Some researchers argue that 8-bit integers are adequate for many applications. Methods like network quantization and Bayesian networks address issues in training procedures. Neuromorphic computing chips offer a hardware solution to uncertainty in DNNs. Nanotechnology devices and crossbar structure-based chips have seen significant advancements in recent years. The crossbar structure in neuromorphic computing utilizes Ohm's law and Kirchhoff's law for efficient vector-matrix multiplication. Nanoscale nonvolatile memory devices at each cross point provide additional storage capability. This computing in memory architecture can alleviate memory bottlenecks in traditional architectures, making neuromorphic computing chips more energy and area efficient for AI applications. Neuromorphic computing is a promising approach for AI applications, with VMMs and high memory requirements. Uncertainty is an important feature in neuromorphic computing chips, stemming from fuzziness and stochasticity. The crossbar structure utilizes Ohm's law and Kirchhoff's law for efficient computation. The stochasticity in neuromorphic computing chips is caused by ADCs and NVM devices. Kirchhoff's law states that VMM results are a summarization of analog currents, which need to be converted to digital voltages for data transfer using ADCs. The stochasticity of NVM devices is due to random particle movement, leading to varied conductance and different output currents even with the same voltage. In this work, a training scheme is proposed to utilize the stochasticity of NVM devices to improve the performance of neuromorphic computing chips. Different types of NVM devices exhibit varying levels of stochasticity due to intrinsic physical mechanisms. Different types of NVM devices exhibit varying levels of stochasticity due to different intrinsic physical mechanisms. The Gaussian distribution is used to model device stochasticity, with the mean representing the conductance value of the stable state and the variance corresponding to the mean. It is challenging to establish a singular model for the relations between the mean and variance of various devices. The conductances of devices are sampled from a Gaussian distribution with a mean and variance. The standard deviation is linearly correlated to the mean, and conductances below a minimum value are cut off. The conductance of devices in neuromorphic computing chips is determined by sampling from a Gaussian distribution with a mean and variance. Writing the conductance of each device is crucial for AI applications, involving mapping weights of a neural network to device conductance levels within a specified range. In neuromorphic computing chips, device conductances are crucial for AI applications. The difference between G pos and G neg determines weight w, which should be expressed using lower conductances for higher energy efficiency. A mapping algorithm is used to utilize the entire conductance range, but accuracy is affected by device stochasticity and circuit fuzziness. The conductance of the device is affected by stochasticity and precision of the ADC. A model using Gaussian distribution is used to describe fuzziness, with parameters like mean and variance. The target conductance is determined in the mapping process, with a level of device fuzziness denoted by \u03b2. In the mapping process, \u03b2 represents device fuzziness, linked to stochasticity, ADC precision, and writing strategy. Uncertainty can impact DNN performance, reducing classification accuracy. Uncertainty adaptation training scheme (UATS) can mitigate accuracy decrease and even improve it by guiding neural networks during training. The uncertainty adaptation training scheme (UATS) introduces stochasticity and fuzziness models during the training process to guide neural networks in dealing with uncertainty. Stochasticity model uses random variable samples in feed forward processes, while fuzziness model is introduced after every k epochs. The fuzziness model is introduced during training, replacing weights with random variable samples. G pf and G nf are obtained using the fuzziness model, and target conductances are calculated accordingly. The network is trained in an uncertain way to improve performance evaluation and loss function calculation. The loss function is calculated by the average output of n FF processes with the same input batch. The effect of uncertainty without UATS on MNIST dataset was evaluated using multiple models. Two MLP models and a CNN model were used, with 50,000 images for training and 10,000 for validation. Test error was calculated using 10,000 images in the test set. G min = 1\u00b5S, G low = 5\u00b5S, G high = 50\u00b5S were used. The models were trained and tested with different levels of uncertainty using fuzziness and stochasticity models. Test errors for MLP and CNN models increased with higher uncertainty levels. The CNN model (LeNet-5) performed the best. The CNN model (LeNet-5) has the best performance without uncertainty, but is most affected by it. The 'mlp2' model is more robust to uncertainty than 'mlp1'. UATS was used to tune weights and retrain models, significantly improving accuracies. UATS significantly improves accuracies in retraining and fine-tuning experiments, outperforming the ideal case in some instances. Validation on CIFAR-10 dataset with ResNet-44 model shows UATS can achieve lower error rates. The results show that UATS can achieve lower error rates than ideal cases with proper hyper-parameters. UATS performs better with more layers in the neural network, acting as a regularization method to ease DNN training. Uncertainty is crucial in intelligent systems, and Bayesian networks are useful for building uncertain neural networks. However, controlling the weight distribution is challenging in neuromorphic computing chips, unlike UATS which offers more convenience. To manipulate the conductance distribution of the device, various distributions were tried, including Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli. Despite different behaviors, the network performance using each distribution with the same mean and variance remains similar. The VMM transforms individual device distributions into a summary of random parameters, making the computation intensive. Methods to reduce random number requirements include sampling weights for inputs or batches instead of VMM, and using VMM uncertainty model results instead of weights to accelerate simulation speed while achieving similar results."
}