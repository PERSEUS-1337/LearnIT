{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new paradigm for image captioning, splitting the process into semantic representation extraction and recursive caption construction. This approach aims to better preserve semantic content through explicit factorization of semantics and syntax, using a compositional generation procedure. The proposed compositional generation procedure for caption construction follows a recursive structure that fits human language properties. It requires less. Image captioning has gained attention recently, with models using an encoder-decoder paradigm to generate captions for images. Despite its effectiveness, the sequential model used has a fundamental problem. Sequential models used in image captioning struggle to reflect the hierarchical structures of natural languages, leading to drawbacks such as reliance on n-gram statistics and favoring frequent n-grams in training sets. Our proposed paradigm for image captioning addresses issues with current models by separating the extraction of semantics and the construction of syntactically correct captions into two stages. This approach aims to improve generalization by explicitly representing the semantic content of images with noun-phrases like \"a white cat\" or \"two men\". The image captioning model generates captions by recursively composing noun-phrases to form higher-level phrases. The captions are based on a set of noun-phrases like \"a white cat\" or \"two men\". The model aims to improve generalization by explicitly representing the semantic content of images. The proposed paradigm for image captioning involves a compositional procedure using parametric modular nets for phrase composition and completeness evaluation. This approach factors semantics and syntax, preserving semantic content and allowing for easy interpretation and control in caption generation. The proposed paradigm for image captioning involves a recursive composition procedure that captures hierarchical dependencies among words and phrases. It increases caption diversity, preserves semantic correctness, generalizes well to new data, and maintains good performance with limited training data. The literature in image captioning has evolved from early bottom-up, detection-based approaches to the current neural network era. Recent works on image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. Vinyals et al introduced a neural image captioner that represents input images with a single feature vector and uses an LSTM for captioning. This approach aims to improve caption diversity, semantic correctness, generalization to new data, and performance with limited training data. Recent works on image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. Xu et al BID3 extended their work by representing the input image with a set of feature vectors and applying an attention mechanism to extract relevant image information. Lu et al BID0 adjusted the attention computation to also attend to the generated text. Anderson et al BID1 added an additional LSTM for better attention control. Dai et al BID18 reformulated latent states as 2D maps to capture semantic information in input images. Recent approaches in image captioning focus on extracting phrases or semantic words directly from input images. Various methods have been proposed, such as predicting frequent training words, treating noun-phrases as hyper-words, and using a hierarchical approach with multiple LSTMs. Despite advancements in model architectures, these approaches still generate captions sequentially. Our proposed paradigm in image captioning represents input images with noun-phrases and constructs captions through a bottom-up recursive composition procedure. This approach preserves semantics effectively, requires less data to learn, and results in more diverse captions compared to sequential generation methods. The proposed compositional paradigm in image captioning involves mining noun-phrases from input images and recursively composing them to generate diverse captions. This approach contrasts with Kuznetsova et al's method, which can only produce captions with a single object. The proposed two-stage framework for image captioning involves composing noun-phrases from input images to generate diverse captions following a hierarchical structure. The proposed framework for image captioning involves generating captions using noun-phrases in a bottom-up manner. Unlike traditional methods, this approach considers nonsequential dependencies among words and phrases in a sentence, providing a more explicit semantic representation of image content. In the proposed framework for image captioning, noun-phrases are used to represent image semantics explicitly. These phrases capture object categories and associated attributes, extracted from the input image. The approach aims to complete the visual understanding paradigm by extracting distinct noun-phrases from the dataset. In the study, it was found that datasets have fewer distinct noun-phrases compared to images. Noun-phrase extraction is formalized as a multi-label classification problem, with a list of selected noun-phrases derived from training captions. Visual features are extracted from images using a Convolutional Neural Network. The visual features are extracted from images using a Convolutional Neural Network, followed by binary classification for each noun-phrase. The input image is represented using top scoring noun-phrases, which are further pruned through Semantic Non-Maximum Suppression to retain only the most relevant ones. CompCap is a recursive compositional procedure used to construct captions by connecting ordered pairs of noun-phrases with a Connecting Module (C-Module) to generate longer phrases. The C-Module also computes scores for the generated phrases. The Connecting Module (C-Module) computes scores for phrases and selects the one with the maximum score as the new phrase. An Evaluation Module (E-Module) assesses if the new phrase is a complete caption. If not, the process repeats with updated phrases until a complete caption is obtained. The Connecting Module (C-Module) selects a connecting phrase given left and right phrases, evaluating the connecting score. It differs from filling in blanks as it deals with incomplete captions. In contrast to the Connecting Module, the C-Module focuses on incomplete captions and generates connecting phrases as a classification problem. The number of distinct connecting phrases is limited, as semantic words like nouns and adjectives are not included. For example, in MS-COCO BID4, there are over 1 million samples collected for the connecting module, containing only about 1,000 distinct connecting phrases. The Connecting Module mines distinct connecting sequences from training captions and defines them as different classes. A classifier is used to score left and right phrases, encoded using a two-level LSTM model. Global and regional image features are also incorporated in the model. The Connecting Module uses a two-level LSTM model to encode left and right phrases, with different parameters for each. The softmax output determines connecting scores, selecting the phrase with the highest score to connect. The Connecting Module uses a two-level LSTM model to encode left and right phrases, with different parameters for each, to determine connecting scores for phrases. The Evaluation Module (E-Module) is then used to determine if a phrase is a complete caption by encoding it into a vector. The E-Module encodes input phrases into a vector and evaluates the probability of them being complete captions. It can also check other properties like caption quality using a caption evaluator. Extensions include generating diverse captions using beam search or probabilistic sampling to form multiple beams. The framework allows for generating diverse captions by forming multiple beams for beam search and using probabilistic sampling. It can also be extended to incorporate user preferences or conditions for influencing the resultant captions. In the Experimental section, examples are shown of controlling captions by filtering noun phrases. Experiments are conducted on MS-COCO BID4 and Flickr30k BID5 datasets, each containing a large number of images with ground-truth captions. The vocabulary is standardized by converting words to lowercase and removing infrequent or non-alphabetic words. The vocabulary sizes for MS-COCO and Flickr30k datasets are 9,487 and 7,000 respectively. Training captions are truncated to 18 words. Ground-truth captions are parsed into trees using NLP toolkit BID31 for training data. The C-Module and E-Module are separately trained for classification tasks, making the compositional procedure modularized and less sensitive to training statistics. During testing, each step is done via two forward passes. During testing, each step of the procedure is done via two forward passes. A complete caption generally requires 2 or 3 steps to obtain. Several methods are compared with CompCap, including Neural Image Captioner (NIC) BID2, AdapAtt, TopDown BID1, and LSTM-A5 BID19. These methods encode images as semantical feature vectors and predict the occurrence of semantical concepts as additional visual features. To ensure a fair comparison, all methods are re-implemented and trained with the same hyperparameters. ResNet-152 BID16 pretrained on ImageNet BID32 is used to extract image features, with activations of the last convolutional and fully-connected layer as regional and global feature vectors. ResNet-152 is fixed without finetuning during training, and a learning rate of 0.0001 is set for all methods. Parameters that yield the best performance are selected for testing. When testing, parameters yielding best performance on validation set are used to generate captions. CompCap selects 7 top-scoring noun-phrases to represent input image. Quality of generated captions is compared on MS-COCO and Flickr30k test sets using various metrics. CompCap with predicted noun-phrases performs best under the SPICE metric, but lags behind baselines in terms of CIDEr, BLEU-4, ROUGE, and METEOR. These results highlight the differences between sequential and compositional caption generation methods, with SPICE focusing on semantic analysis while other metrics favor frequent n-grams. The ablation study on the proposed compositional paradigm in CompCap shows that using groundtruth noun-phrases from associated captions significantly improves caption generation metrics. This indicates that CompCap effectively preserves semantic content, leading to better caption generation for input images. CompCap generates better captions by integrating ground-truth noun-phrases in a composing order, boosting metrics except for SPICE. The proposed compositional paradigm disentangles semantics and syntax, allowing CompCap to compose semantics into a syntactically correct caption effectively. CompCap is effective at composing semantics into syntactically correct captions, handling out-of-domain content, and requiring less data to learn. Two studies were conducted to verify this, with experiments on data ratios and testing on different datasets showing significant drops in SPICE and CIDEr metrics. Competitive results are obtained for CompCap trained using in-domain and out-of-domain data, showing the benefit of disentangling semantics and syntax. The ability of CompCap to generate diverse captions is highlighted, with metrics evaluating diversity. The diversity of captions generated by CompCap is evaluated using five metrics. These metrics include the ratio of novel and unique captions, vocabulary usage percentage, and pair-wise editing distances. The ability of CompCap to produce diverse captions is emphasized, showcasing competitive results with in-domain and out-of-domain data. CompCap obtained the best results in all metrics, showcasing its ability to generate diverse captions for images. CompCap generated diverse and novel captions, with the best results in all metrics. Error analysis revealed that misunderstandings of input visual content led to errors in captions, which could be improved with more sophisticated techniques in noun-phrase extraction. The proposed method for image captioning involves a novel paradigm that generates captions in a compositional manner by extracting explicit noun-phrases in the first stage and using a recursive compositional procedure in the second stage. The proposed method for image captioning involves extracting noun-phrases and using a compositional procedure to generate captions in a hierarchical structure. This approach preserves semantics effectively, requires less training data, generalizes well across datasets, and produces diverse captions by finding semantically similar noun-phrases. The method involves using encoders in the C-Module to obtain encodings for noun-phrases, where semantically similar phrases have small normalized euclidean distances between their encodings. The C-Module uses encoders to compute normalized euclidean distances for noun-phrases, determining semantic similarity based on the sum of distances. Independent parameters are used for encoders to ensure different encodings for the same phrase. The C-Module uses encoders with independent parameters to compute normalized euclidean distances for noun-phrases, ensuring different encodings for the same phrase. Tunable hyperparameters for CompCap include beam search sizes for pair and phrase selection. Adjusting these hyperparameters individually can be seen in FIG6. The size of beam search for pair selection and connecting phrase selection has minor influence on the performance of CompCap, as shown in FIG6."
}