{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks advance 2D and 3D image classification. Wavelet Pooling reduces feature dimensions and addresses overfitting. Our proposed method addresses the problem encountered by max pooling, outperforming or performing comparably with other pooling methods in classification tasks. Convolutional Neural Networks (CNNs) are the standard in image and object classification, consistently achieving higher accuracy rates than vector-based deep learning techniques. The strength of CNN algorithms drives researchers to continuously enhance key components like the convolutional and pooling layers for improved accuracy and efficiency. Pooling, rooted in predecessors like Neocognitron and Cresceptron, subsamples convolutional layer results to reduce spatial dimensions and parameters, enhancing computational efficiency. Pooling operations in CNNs aim to reduce parameters, increase computational efficiency, and prevent overfitting. Max pooling and average pooling are common methods, but they have limitations. Other approaches like mixed pooling and stochastic pooling use probabilistic techniques to address these issues. All pooling operations involve subsampling in a neighborhood approach similar to nearest neighbor interpolation in image processing. Our proposed wavelet pooling algorithm utilizes second-level wavelet decomposition for subsampling features, avoiding artifacts like edge halos and blurring. It aims to minimize data discontinuities for network regularization and improved classification accuracy. The method is compared to max, mean, mixed, and stochastic pooling techniques. Our proposed wavelet pooling algorithm aims to minimize data discontinuities for network regularization and improved classification accuracy. It is compared to max, mean, mixed, and stochastic pooling methods on benchmark image classification datasets. The simulations are performed in MATLAB R2016b. The paper is organized into sections discussing background, proposed methods, experimental results, and a summary. In the proposed wavelet pooling algorithm, pooling is a method for subsampling the output of the convolutional layer by condensing dimensions. Max pooling selects the maximum value of a region for the condensed feature map, while average pooling calculates the average value. The proposed wavelet pooling algorithm uses max pooling to select the maximum value of a region for the condensed feature map, while average pooling calculates the average value. Max pooling can erase details from an image and commonly overfits training data. Researchers have developed probabilistic pooling methods to address the shortcomings of max and average pooling. Mixed pooling combines these methods by randomly selecting one over the other during training, with no set way to perform it. Probabilistic pooling methods like mixed pooling and stochastic pooling offer alternatives to traditional max and average pooling. Mixed pooling randomly selects between max and average pooling during training, while stochastic pooling samples from neighborhood regions based on activation probabilities. The stochastic pooling method samples activations from a multinomial distribution based on probabilities, avoiding the limitations of max and average pooling. It selects activations with the highest probabilities, but any activation can be chosen. In this example, the midrange activation is selected with a 13% probability. This method uses neighborhoods to subsample, similar to nearest neighbor methods. The proposed pooling method utilizes wavelets to reduce feature map dimensions and minimize artifacts from neighborhood reduction, improving image classification by capturing data compression more organically. The proposed wavelet pooling scheme in image classification utilizes a 2nd order decomposition in the wavelet domain, improving data compression by reducing feature map dimensions. The proposed wavelet pooling scheme in image classification utilizes a 2nd order decomposition in the wavelet domain. Using the FWT on images, detail subbands (LH, HL, HH) are obtained at each decomposition level, along with an approximation subband (LL) for the highest level. After the 2nd order decomposition, image features are reconstructed using only the 2nd order wavelet subbands. The method pools image features by a factor of 2 using the IFWT, based on the IDWT. The algorithm for forward propagation of wavelet pooling is illustrated in FIG3, and backpropagation is performed by reversing the process. The proposed wavelet pooling algorithm performs backpropagation by reversing the process of forward propagation. Image features undergo 1st order wavelet decomposition, followed by upsampling of detail coefficient subbands to create a new 1st level decomposition. This initial decomposition then becomes the 2nd level decomposition, which reconstructs the image feature for further backpropagation using the IDWT. All CNN experiments use MatConvNet, and training employs stochastic gradient descent. The wavelet basis is crucial for the proposed method. The proposed method utilizes the Haar wavelet basis for wavelet pooling, running experiments on a 64-bit system with specific hardware configurations. Different CNN structures are tested with various regularization techniques like Dropout and Batch Normalization for CIFAR-10 and SHVN datasets. The proposed method uses Haar wavelet basis for wavelet pooling and is tested on different CNN structures with regularization techniques like Dropout and Batch Normalization for CIFAR-10 and SHVN datasets. The pooling methods are compared using a 2x2 window on various datasets, with the proposed method outperforming all others according to TAB0. Our proposed method outperforms all others in the comparison of pooling methods on different CNN structures with regularization techniques. Max pooling shows signs of overfitting with a small number of epochs, while mixed and stochastic pooling have a rocky trajectory but do not overfit. Average and wavelet pooling exhibit a smoother descent in learning and error reduction. Two sets of experiments were conducted, one without dropout layers and the other with dropout and batch normalization, to observe the effects of these changes. Our network structure for CIFAR-10 experiments is shown in FIG7, using the full training and test data sets. Our proposed method ranks second in accuracy with and without dropout. Max pooling overfits quickly, while wavelet pooling resists overfitting. Adjusting the learning rate prevents overfitting and maintains a slower learning pace. Mixed and stochastic pooling show consistent learning progression. The experiments involve two sets with different pooling methods, one without dropout layers and the other with dropout. The SHVN dataset is used, with 55,000 images for no dropout and 73,257 images for dropout, along with a validation set of 30,000 images. The experiments involved two sets with different pooling methods, one without dropout and the other with dropout. The proposed method showed the second lowest accuracy, with max and wavelet pooling slightly overfitting the data. Our method followed the path of max pooling but performed slightly better in maintaining stability. Mixed, stochastic, and average pooling showed a slow progression of learning. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five poses. The dataset had errors which were fixed by mirroring missing or corrupted images in MATLAB. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five poses. Errors in the dataset were fixed by mirroring missing or corrupted images in MATLAB. Images were manually cropped to match dimensions set by the creators. The data was shuffled, with 3,900 images used for training and 1,000 for testing. Images were resized to 128x128 due to memory and time constraints. Dropout layers were used to regulate the network and prevent overfitting. The proposed method showed the second highest accuracy, with wavelet pooling resisting overfitting and stochastic pooling maintaining consistency. The construction and implementation of wavelet pooling is not efficient, presented as a proof-of-concept with potential for improvement in computational efficiency. The code implementation is not optimized, lacking resources for peak performance. The method presented is not optimized for efficiency, lacking resources for peak performance. The accuracy results and novelty serve as a starting point for future improvements by both the authors and other researchers. Efficiency is measured in terms of mathematical operations utilized by each method. Operations for max pooling are based on worst-case scenarios, while average pooling calculations involve additions and divisions. Mixed pooling combines average and max pooling, and operations for stochastic pooling are also calculated. The mean value of average and max pooling is calculated for stochastic pooling, involving mathematical operations and random value selection. Wavelet pooling computes operations for each subband in decomposition and reconstruction. Average pooling requires the least computations, followed by mixed pooling and max pooling. Stochastic pooling is the least efficient method, using about 3x more operations. The wavelet pooling method is the least computationally efficient, using 54 to 213x more mathematical operations than average pooling. However, with good coding practices and improvements to the FTW algorithm, this method can be a viable option. The FTW algorithm has been improved with multidimensional wavelets, lifting, parallelization, and other methods to enhance speed and memory efficiency. Wavelet pooling shows potential to outperform traditional methods in CNNs. The proposed method excels in the MNIST dataset, performs well in CIFAR-10 and KDEF datasets, and competes closely with superior pooling methods in the SHVN dataset. Dropout and batch normalization contribute to network regularization. The study explores different pooling methods in CNNs, showing that no single method is superior across all datasets. Results indicate that alternating between pooling methods can maximize effectiveness. Future work could involve varying wavelet basis to determine the best performing method. In exploring different pooling methods in CNNs, it is found that altering upsampling and downsampling factors can improve image feature reduction. Retaining discarded subbands for backpropagation may enhance accuracy. Improving the FTW method can boost computational efficiency. Analyzing SSIM of wavelet pooling compared to other methods can validate the effectiveness of this approach."
}