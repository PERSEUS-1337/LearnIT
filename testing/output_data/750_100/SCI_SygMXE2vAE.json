{
    "title": "SygMXE2vAE",
    "content": "BERT and other Transformer-based models have achieved state-of-the-art results in Natural Language Processing tasks. A layer-wise analysis of BERT's hidden states reveals valuable information beyond attention weights, particularly in models fine-tuned for Question Answering tasks. Our analysis of BERT's hidden states during Question Answering tasks shows that the system can incorporate task-specific information into its token representations. Fine-tuning has minimal impact on semantic abilities, and prediction errors can be identified. Transformer models have gained popularity in Natural Language Processing due to their improvements over RNNs in Machine Translation. With the use of large models and extensive pre-training, these models have become prevalent in the field. The curr_chunk discusses copyright permissions for a publication presented at CIKM '19 in Beijing, China. It mentions the author's rights, licensing to the Association for Computing Machinery, and the ISBN. Additionally, it highlights the effectiveness of solving Natural Language Processing tasks. The paper discusses BERT BID8, a popular Transformer model known for significant improvements in various benchmarks. It addresses the issue of black box models in deep learning, emphasizing the lack of transparency and reliability in these models. Transformers are considered moderately interpretable. This paper takes a different approach to interpreting Transformer Networks by examining hidden states between encoder layers directly. It aims to answer questions about how Transformers decompose questions, if different layers solve different tasks, the influence of fine-tuning on inner states, and if evaluating network layers can provide insights. The paper examines the impact of fine-tuning on a network's inner state and evaluates network layers to understand why a network may fail to predict correctly. It focuses on Question Answering tasks using BERT and GPT-2 models. The paper explores the effects of fine-tuning on network layers in BERT and GPT-2 models for Question Answering tasks. It introduces a layer-wise visualization of token representations and conducts NLP Probing Tasks, including Question Type Classification and Supporting Fact Extraction, to analyze BERT's abilities and the impact of fine-tuning on its transformations. BERT and GPT-2 models, both Transformer networks, show similar transformation phases when fine-tuned on different tasks. General language properties are encoded in earlier layers and utilized in later layers for solving downstream tasks. GPT-2, an improved version of GPT, has shown proficiency in language modeling despite not reaching BERT's level of success on leaderboards. Open-AI has decided not to release their pre-trained models like BERT, as larger versions have proven adept at language modeling. Other notable Transformer models include Universal Transformer BID7 and TransformerXL BID6, aiming to improve flaws by adding a recurrent inductive bias. Interpretability and explainability of neural models are key research areas, with various approaches highlighted in relevant work. Recent advances in research focus on probing tasks and methodologies applied to trained models like BERT. Tenney et al. introduced an \"edge-probing\" framework with nine tasks probing semantic and syntactic information in ELMo, BERT, and GPT-1 embeddings. Fine-tuned models are not specifically studied in this context. Recent research has focused on probing tasks and methodologies applied to trained models like BERT. Various studies have analyzed pre-trained models, with some specifically looking at BERT in the context of a Ranking task. Other work has explored models through qualitative visual analysis, such as a survey of different approaches limited to CNNs. Nagamine et al. studied phoneme recognition in DNNs by analyzing single node activations. Nagamine et al. BID24 analyze phoneme recognition in DNNs through single node activations. Hupkes et al. BID14 conduct a qualitative analysis and train diagnostic classifiers to support their hypotheses. Li et al. BID17 examine word vectors' specific dimensions' importance in sequence tagging and classification tasks. Liu et al. BID20 focus on layer-wise analysis of BERT's token representations, solely probing pre-trained models. Our work focuses on evaluating hidden states and token representations in fine-tuned BERT models, contrasting with previous studies that analyzed pre-trained models and disregarded fine-tuned ones. Our analysis focuses on fine-tuned BERT models, examining the transformations of input tokens qualitatively and quantitatively through language tasks. The architecture of BERT allows us to track token changes throughout the network, enabling a detailed analysis of token representations in each layer. The approach involves qualitatively analyzing token transformations in fine-tuned BERT models by collecting hidden states from each layer for randomly selected samples. Distances between token vectors are used as indicators of semantic relations, with dimensionality reduction applied to BERT's pre-trained models with vector dimensions of 1024 (large model) and 512 (base model). The text discusses the use of dimensionality reduction techniques like t-SNE, PCA, and ICA on token vectors from pre-trained BERT models with vector dimensions of 1024 and 512. PCA is used to identify distinct clusters in the data, and k-means clustering is applied to verify the representation of clusters in 2D space. After applying dimensionality reduction techniques like t-SNE, PCA, and ICA on token vectors from pre-trained BERT models, k-means clustering is used to identify clusters corresponding to observations in 2D space. Semantic probing tasks are then applied to analyze information stored within transformed tokens after each layer, investigating how language information is maintained or forgotten by the model. The principle of Edge Probing is utilized for this analysis. The principle of Edge Probing, introduced by Tenney et al., translates core NLP tasks into classification tasks focusing on labeling. Tasks like Named Entity Labeling, Coreference Resolution, Relation Classification, Question Type Classification, and Supporting Fact Identification are adopted for language understanding and reasoning. Named Entity Labeling requires the model to predict the entity category for a given token span. The model is tasked with predicting entity categories based on Named Entity Recognition, formulated as a Classification problem. Annotations are from the OntoNotes 5.0 corpus with 18 entity categories. Coreference Resolution involves predicting if two mentions refer to the same entity, built from the OntoNotes corpus. Relation Classification requires predicting relation types connecting two entities, constructed with samples from the SemEval 2010 Task 8 dataset. For the Edge Probing task, the model uses the Question Classification dataset by Li and Roth, consisting of 500 question types. The whole question is inputted into the model with its question type as the label. The model uses the question as input with its question type as a label to extract Supporting Facts, which are crucial for Question Answering tasks. By analyzing BERT's token transformations, we aim to understand how it distinguishes important context parts from distracting ones. A probing task is constructed to identify Supporting Facts, where the model predicts if a sentence contains relevant information for a specific question. This task tests the hypothesis that token representations carry information about their relevance to the question. The probing task involves identifying Supporting Facts by labeling sentences as true or false based on their relevance to the question. Input tokens are embedded using a fine-tuned BERT model for each probing task sample. Contrary to previous work, this approach aims to recognize relevant parts specific to each dataset. The probing task involves using a fine-tuned BERT model to extract tokens for each sample. The Edge Probing concept focuses on labeled edges within a sample for classification. Tokens are pooled and fed into an MLP classifier to predict label probabilities. This process is applied to pretrained BERT-base. The study analyzes the abilities learned by BERT models during pre-training and fine-tuning, focusing on Question Answering tasks using different datasets like SQUAD, bAbI, and HotpotQA. Detention is a common punishment in schools in the UK, Ireland, and other countries, where students have to stay in school after hours or on non-school days. This punishment involves sitting in a classroom and doing work or writing lines. The QA task of HotpotQA focuses on natural question-answer pairs with a context containing supporting and distracting facts. The context has an average size of 900 words, but the distracting facts are reduced by a factor of 2.7 to fit the input size of the pre-trained BERT model. Yes/no questions are excluded from the analysis. The bAbI tasks are artificial toy tasks designed to test neural models' reasoning abilities. The tasks in this section focus on Multihop QA with 20 tasks that require reasoning over multiple sentences. The tasks are designed to test neural models' abilities in Positional Reasoning, Argument Relation Extraction, and Coreference Resolution. The models analyzed are BERT BID8 and GPT-2 BID28, both of which are Transformers that build upon previous ideas from Transformer models, SemiSupervised Sequence Learning, ELMo, and ULMFit. The curr_chunk discusses the architecture and usage of BERT and GPT-2 models in experiments, focusing on pre-trained models like bert-base-uncased and bert-large for BERT, and the small model for GPT-2. The models are not applied directly but are fine-tuned for the experiments. In experiments, BERT and GPT-2 models are fine-tuned on datasets using hyperparameter tuning and training for 5 Epochs. Input length varies for different tasks, with evaluations done to select the best model. For bAbI tasks, single and multitask models are evaluated. The study compares single-task and multitask models trained on bAbI tasks and a multitask model trained on all 20 tasks. Two settings are distinguished: Span prediction and Sequence Classification. Span prediction involves appending all possible answers to the context for better comparison. HotpotQA is also analyzed, with a focus on the HotpotQA Support Only task. Our HotpotQA Distractor task simplifies the task by using only Supporting Facts as question context. The evaluation results of our best models show high accuracy on the SQuAD task, close to human performance. Tasks derived from HotpotQA are more challenging, with the distractor setting being the most difficult to solve. GPT-2 performs better on bAbI tasks compared to BERT, with validation error nearly 0. BERT struggles with tasks 17 and 19, which require positional or geometric reasoning. Qualitative analysis of vector transformations reveals recurring patterns. The text chunk discusses recurring patterns in different datasets and models, comparing results in macro-averaged F1 over network layers. It also mentions PCA representations of tokens in different layers. The text chunk discusses the phases observed in different QA tasks through PCA representations of tokens in various layers of the model. The four phases are described, starting with Semantic Clustering in early layers of BERT-based models. These initial layers group tokens into topical clusters similar to Word2Vec embeddings. In the middle layers of neural networks, entities are connected by their relation within a certain input context, forming task-specific clusters that filter question-relevant entities. This is different from the early layers which group tokens into topical clusters similar to Word2Vec embeddings. In the middle layers of neural networks, entities form task-specific clusters that filter question-relevant entities. This is different from early layers that group tokens into topical clusters. FIG4 and FIG6 show clusters related to common punishments in schools and identifying facts about Emily being a wolf. The HotpotQA model also displays similar clusters with more coreferences. The HotpotQA model shows improved abilities in recognizing entities, identifying coreferences, and finding relations in higher network layers. This is supported by probing results and visualized in FIG7. BERT-base and BERT-large models exhibit similar patterns. Matching questions with supporting facts requires identifying relevant context parts. BERT models transform tokens to match question tokens with relevant context tokens, as shown in FIG4. Probing tasks reveal the models' strong ability to distinguish in lower layers. Matching questions with supporting facts is crucial for Question Answering and Information Retrieval. The models' ability to distinguish relevant information in higher layers is demonstrated in FIG1 for SQuAD and bAbI. The fine-tuned HotpotQA model in FIG2 does not perform well, indicating BERT's struggle to identify Supporting Facts. Vector representations help identify important facts considered by the model. In the last layers of the model, it separates correct answer tokens from the rest, forming homogeneous clusters. The vector representation at this stage is task-specific and learned during fine-tuning, leading to a performance drop in general NLP tasks. This loss of information is particularly evident in the large BERT model fine-tuned on HotpotQA. The large BERT model fine-tuned on HotpotQA shows a performance drop in general NLP tasks, particularly in separating correct answer tokens in the last layers. The phases of answering questions can be compared to human reasoning, involving semantic clustering, building relations between context parts, separating important from irrelevant information, and grouping potential answer candidates. BERT and human reasoning have similarities in the phases of answering questions, such as semantic clustering and grouping answer candidates. However, BERT can process all input parts simultaneously, unlike humans who read sequentially. A key difference between BERT and GPT-2 models is how they handle hidden states. GPT-2's hidden states focus on the first token of a sequence, leading to separation of clusters. This problem occurs in all layers except for the Embedding Layer, first Transformer block, and last one. The first token is masked during dimensionality reduction. GPT-2, like BERT, separates relevant information in the vector. GPT-2, similar to BERT, separates relevant information in the vector space. It also extracts a sentence with similar meaning but not a Supporting Fact. Unlike BERT, the correct answer is not distinctly separated in GPT-2. These findings extend beyond BERT architecture to other Transformer networks. Future work will involve more probing tasks to confirm these observations. The visualizations in the current chunk show failure states and the difficulty of a task based on hidden state representations. For wrong predictions, the phases resemble correct predictions but focus on the wrong answer. Examining early layers can reveal insights into why the wrong candidate was chosen, such as selecting the wrong Supporting Fact. The wrong candidate was chosen due to factors like selecting the wrong Supporting Fact. When network confidence is low, transformations do not go through the usual phases, resulting in irrelevant extractions. The network maintains Phase (1) 'Semantic Clustering' even when confidence is low. Fine-tuning has little impact on core NLP abilities as pretrained model already contains sufficient information. The model maintains positional embedding throughout layers, crucial for Transformer network performance. Visualizations show its impact on tasks like SQuAD dataset. Ability to resolve question types is highlighted. The performance curves on the SQuAD dataset show that fine-tuning improves the model's ability to resolve question types. However, fine-tuning on bAbI tasks results in a loss of this ability, likely due to the static structure of the samples. The model fine-tuned on HotpotQA does not outperform the model without fine-tuning in recognizing question types. Both models can solve the task in earlier layers, indicating pre-training in BERT-large. Our work uncovers insights into Transformer networks' inner workings, with implications for future research. The qualitative analysis of token vectors in Transformer models reveals interpretable information stored in hidden states. This information helps identify misclassified examples, model weaknesses, and important context for decision-making. Lower layers may be more suitable for certain problems in Transfer Learning tasks, suggesting the need for individual layer depth selection. Further research is needed to process this information effectively. Our findings suggest that specific layers in Transformer networks solve different problems, indicating a potential modularity that can be leveraged in training. This could involve fitting parts of the network to specific tasks during pre-training instead of using an end-to-end language model task. Further exploration is recommended on skip connections in Transformer layers for direct information transfer between non-adjacent layers. Our work focuses on understanding internal processes in Transformer-based models and suggests further research to improve state-of-the-art models for downstream tasks."
}