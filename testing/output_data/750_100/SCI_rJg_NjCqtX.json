{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework is proposed in this paper to automatically standardize non-systematic names to systematic names using spelling error correction, byte pair encoding tokenization, and neural networks. The framework proposed in this paper aims to standardize non-systematic chemical names to systematic names using spelling error correction, byte pair encoding tokenization, and a neural sequence to sequence model. The standardization accuracy on the test dataset reached 54.04%, a significant improvement from previous results. There are over 100 million named chemical substances worldwide, each requiring unique identification based on their structures, following rules set by the International Union of Pure and Applied Chemistry (IUPAC). Chemical substances can have multiple names, including common names and systematic names. For example, sucrose is commonly known as sugar. In the pharmaceutical industry, new names are often created to distinguish substances. In the pharmaceutical industry, producers create proprietary names for chemical substances to differentiate their products. Chemical information extraction research relies on standard chemical names stored in databases like PubChem and SciFinder. Chemical databases store information like chemical names, structures, and formulas. Extracting chemical information from papers is ongoing work to update these databases. Systematic names can be converted to SMILES and InCHI representations to generate structural formulas. Online systems can automatically convert systematic names to SMILES strings. Automated systems like OPSIN BID7 can convert systematic names to SMILES strings accurately. Non-systematic names may have errors like spelling, ordering, and common name discrepancies. Many chemical substances have common names or proprietary names that differ from their systematic names. Synonym errors, where nonsystematic names share the same root word as systematic names, are common. Errors like ordering and synonym discrepancies can occur simultaneously in a single non-systematic name. The framework proposed addresses various types of errors in non-systematic chemical names, including spelling, ordering, and synonym errors. It consists of three main steps: spelling error correction, BPE tokenization, and a sequence to sequence model for fixing remaining errors. Despite the challenges, limited work has been done in the field of chemical name standardization. The BID2 work developed an online system called ChemHits for chemical name standardization using transformation rules and queries to online databases. Unlike BID2, the proposed framework uses a sequence to sequence model for standardization, similar to machine translation. This approach addresses various errors in non-systematic chemical names. The framework uses an end-to-end, fully data-driven approach to standardize chemical names, achieving 54.04% accuracy on the test dataset. The corpus for the work contains chemical names from high-impact Chemical Journals, collected and verified manually. The corpus used for standardizing chemical names includes non-systematic and systematic names of chemical substances, totaling 384816 data pairs. The Levenshtein distance between the names is analyzed to show differences. The experiment divides data into training, test, and development sets. The focus is on correcting spelling errors in chemical substance names. The text discusses correcting spelling errors in chemical substance names by separating them into elemental words and using vocabularies of systematic and non-systematic elemental words. The text discusses creating an elemental vocabulary by extracting common names or synonyms from non-systematic names and combining them with systematic elemental words. This vocabulary is structured using BK-Tree for efficient correction searches. BK-Tree is used to efficiently correct spelling errors in non-systematic names by quickly finding the closest match within a given threshold using Levenshtein distance. It also allows for easy insertion of new training data, making it scalable. The BK-Tree efficiently corrects spelling errors in non-systematic names by finding the closest match within a given threshold using Levenshtein distance. It allows for easy insertion of new training data, making it scalable. The process involves separating a chemical substance name into elemental words, inputting them into the BK-Tree, correcting them, and combining them to get the full name. This helps reduce noise in training the sequence to sequence model. The Levenshtein distance between nodes in a dataset is used to correct spelling errors in chemical names. Byte Pair Encoding (BPE) BID11 is used for tokenization in a sequence-to-sequence model. Byte Pair Encoding (BPE) BID11 is used for tokenization by initializing a symbol set with single characters, counting symbol pairs iteratively, and merging the most frequent pairs. The final symbol set size is equal to the initial character size plus the number of merge operations. BPE can handle out-of-vocabulary problems by generating a vocabulary set at the character level. The vocabulary set generated by BPE contains vocabularies at the character level, allowing for the separation of names into meaningful subwords. BPE tokenizes chemical names into small molecules, which are then used to train a sequence to sequence model. This model, consisting of two recurrent neural networks, is commonly used in machine translation. The model consists of two recurrent neural networks (RNN): an encoder that generates a context vector H from source sequences and a decoder that uses this vector to generate target sequences. The encoder uses a multilayer bidirectional LSTM (BiLSTM) BID3, which combines forward and backward hidden states at each time step. The context vector H is obtained by combining all hidden states at the final time step T of the encoder. The decoder calculates the probability of output sequences and tokens. Parameters for spelling error correction include the threshold of the BK-Tree, while for the BPE stage, it is the number of merge operations. Experimentation involved trying different threshold and merge operation values. Dimensions of word embeddings and hidden states are important for the sequence to sequence model. The sequence to sequence model uses 500 dimensions for word embeddings and hidden states. The vocabulary size includes basic characters and BPE merge operations. Both encoder and decoder have 2 layers. Spelling errors in training data are corrected before training. Parameters are trained jointly using SGD with a cross-entropy loss function. Loss is computed over minibatches of size 64 and normalized. Random uniform initialization is used for weights. The sequence to sequence model uses 500 dimensions for word embeddings and hidden states, with both encoder and decoder having 2 layers. Parameters are trained jointly using SGD with a cross-entropy loss function. The weights are initialized using a random uniform distribution ranging from -0.1 to 0.1. The initial learning rate is 1.0 with decay applied every epoch after epoch 8 or when perplexity does not decrease on the validation set. The dropout rate is 0.3 and the model is trained for 15 epochs. The beam size for decoding is set to 5. Another experiment replaces the sequence to sequence model with a Statistical Machine Translation (SMT) model using the Moses system and limiting training sequence length to 80 with a 3-grams language model by using KenLM. Data augmentation is a technique used for neural model learning to handle noisy data, such as spelling errors. In this experiment, errors are inserted into non-systematic names with a probability of 0.025, including randomly inserting, deleting, exchanging, or replacing characters. In the experiment, four insertion methods are applied with equal probability to handle noisy data. Accuracy and BLEU score BID10 are used to measure standardization quality. Accuracy is calculated based on successfully standardized non-systematic names. Results for different models on the test dataset are shown in TAB3, highlighting the effectiveness of spelling error correction and BPE tokenization. The combination of spelling error correction, BPE tokenization, and sequence to sequence model outperforms SMT model and ChemHits system. Results for different BPE merge operations show 5000 as the optimal value. Spelling error correction is beneficial, as shown by different Levenshtein distance thresholds and data augmentation results in TAB5. Data augmentation and spelling error correction are beneficial for the framework. However, spelling error correction outperforms data augmentation. Overcorrection may occur with large thresholds, reducing standardization quality. Examples of successfully standardized non-systematic names are shown in Table 6, demonstrating the capabilities of the sequence to sequence model in fixing spelling errors and synonym errors. The sequence to sequence model can fix non-alphabet spelling errors and synonym errors, as demonstrated in examples such as correcting the order of chemical names and standardizing proprietary names to systematic names. The model's capabilities are illustrated through visualization of attentions in an example. The seq2seq model can correct non-alphabet spelling errors and standardize chemical names, as shown in examples like fixing the order of names and converting proprietary names to systematic names. The model's abilities are demonstrated through attention visualization in a specific example. In this section, we analyze the fail standardization attempts of our system by randomly selecting 100 samples of failed attempts and labeling their error types. Synonym errors are the most confusing, while spelling errors are handled well. Common errors are challenging due to the lack of rules between common and systematic names. Out of the 100 samples, 10 are nearly correct and 7 are completely incorrect. The accuracy of standardizing non-systematic names is still around 50%. The model performs best for systematic names between 20 and 40 characters but poorly for names over 60 characters. The model has limitations as it does not consider chemical rules, leading to some generated names that do not follow the rules. Examples of failed attempts include names like 5-bromo-2-methylbenzene-1-sulfonyl chloride. In this work, a framework is proposed to automatically convert non-systematic chemical names to systematic names using spelling error correction, byte pair encoding tokenization, and a sequence to sequence model. The framework achieves an accuracy of 54.04% on the dataset, significantly outperforming previous rule-based systems. This advancement enables the extraction of related chemical information into practical use. The framework proposed in this work automatically converts non-systematic chemical names to systematic names, achieving a high accuracy of 54.04%. It enables the extraction of chemical information into practical use, starting a new research line in this field."
}