{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The teachers and student can have different structures and be trained on different tasks. The student becomes independent of the teachers after training. This approach is demonstrated to outperform on various supervised and reinforcement learning tasks. The new approach called knowledge flow transfers knowledge from multiple deep nets to a new deep net model, outperforming fine-tuning and other methods on supervised and reinforcement learning tasks. Research communities have developed numerous deep net architectures for various tasks, with new ones being added regularly. Some architectures are trained from scratch, while others are fine-tuned by initializing weights from a similar deep net trained on different data. In reinforcement learning, teachers have also been explored, such as the progressive neural net that utilizes multiple teachers to extract useful features for a new task. PathNet uses genetic algorithms for learning. Some deep net architectures are fine-tuned by initializing weights from a similar deep net trained on different data. PathNet uses genetic algorithms for learning new tasks. Various techniques like 'Growing a Brain', Actor-mimic, and Knowledge distillation aim to extract useful features from teachers for new tasks, but they have limitations. For example, progressive neural net models grow with the number of teachers, leading to a large number of parameters. Neural net models grow with the number of teachers, limiting the capacity of progressive neural nets. PathNet's pathway search is computationally intensive. Fine-tuning methods like 'Growing a Brain' and actor-mimic rely heavily on pretrained models. To overcome these limitations, knowledge flow is developed to transfer knowledge from multiple teachers to train a student, ensuring independence at the final stage. Our approach allows for knowledge transfer from multiple teachers to a student, ensuring independence in training. It is flexible in teacher model selection and applicable to various tasks. Evaluation includes tasks from reinforcement learning to fully-supervised training. The goal of reinforcement learning is to find a policy that maximizes the expected future reward from each state. The asynchronous advantage actor-critic (A3C) formulation is followed, where the policy mapping and value function are approximated by deep neural networks. The policy and value function in reinforcement learning are approximated by deep neural networks. The optimization of the policy parameters involves a loss function based on negative log-likelihood and entropy regularization. The goal is to maximize the expected future reward from each state. The text discusses optimizing the value function V \u03b8v in reinforcement learning using warm-start techniques and proposes a framework called knowledge flow to transfer knowledge from 'teachers' to a deep net under training, called the 'student.' The text introduces a framework called knowledge flow to transfer knowledge from pre-trained 'teachers' to a deep net under training, referred to as the 'student.' Initially, the student heavily relies on teacher one, but as training progresses, the student becomes independent. The framework is illustrated using example deep nets in a figure. In the knowledge flow framework, knowledge from multiple teachers is transferred to a student deep net by modifying the student net with transformed teacher representations. This involves adding intermediate representations from teachers to the student net, which are transformed and scaled using trainable parameters. The knowledge flow framework involves transferring knowledge from multiple teachers to a student deep net by modifying the student net with transformed teacher representations via trainable parameters. The normalized weights encode which representations to trust at each layer, allowing teachers to help the student at different levels of abstraction. During training, a high weight on the student representation is encouraged to ensure the student eventually captures all knowledge without relying on teachers. During training, the student initially relies heavily on teachers' knowledge but gradually becomes more independent. In the final stages, the student must master the task on its own. To encourage this transition, two additional loss functions are introduced: dependency loss and behavior stability loss. The student's behavior stability is ensured by a loss function that captures changes when teachers' influence decreases. Additional loss terms are combined with student net modifications to obtain DISPLAYFORM0 for supervised tasks and a transformed program for reinforcement learning, denoted as DISPLAYFORM1. The loss\u02dc \u00b7 \u00b7 (\u03b8, w, Q) originates from the original loss \u00b7 \u00b7 (\u03b8) by including cross-connections in the deep net, depending on w, Q, probability distribution f, and policy distribution \u03c0. Parameters from the current and previous iterations are denoted as \u03b8 and \u03b8 old. The strength of teacher influence in supervised and reinforcement learning is controlled by \u03bb 1 and \u03bb 2. A low \u03bb 1 allows the student to rely on teachers initially, but as training progresses, its value is gradually increased to promote independence. The proposed method reduces negative transfer by decreasing the weight for teacher layers. The proposed method decreases the weight for teacher layers to reduce negative transfer effects. Students may still benefit from teachers' low-level representation, observed in experiments. Modifications to deep nets and loss functions dep and KL are detailed to successively decrease teacher influence in student models. The student model combines layers from different teachers to create an intermediate representation. Normalized weights are used to decide which representation to trust at each layer. The number of introduced matrices in the framework is limited. In practice, not every layer of the student model is linked to a teacher network. In practice, it is recommended to link one teacher layer to one or two student layers for improved results. The results of state-of-the-art methods like A3C, PPO, and ACKTR are likely irrelevant to a student's top layer features. Additionally, linking a teacher's bottom layer to a student's top layer generally does not yield improvements. In the framework, additional trainable parameters Q and w are introduced to help the student learn faster. These parameters do not become part of the student network at the end of training. The influence of teachers is gradually decreased during training to encourage the student to become independent. During training, the normalized weight p j w (l j 0 ) increases to 1 for all layers of the student, promoting independence. Minimizing dependence cost encourages the student to rely less on teachers. However, a rapid decrease in teacher influence can harm performance as it takes time to find optimal transformations. During training, minimizing dependence cost encourages the student to rely less on teachers, but a rapid decrease in teacher influence can harm performance. To prevent this, a Kullback-Leibler regularizer is used to slow down changes in the student's output distribution. Results are reported by using only the student model to evaluate knowledge flow on reinforcement learning tasks using Atari games. The agent learns to predict actions based on rewards and input images from the environment, choosing an action every four frames. The model architecture consists of three hidden layers, with the first layer being a convolutional layer with 16. The model architecture of A3C BID17 consists of three hidden layers: a convolutional layer with 16 filters of size 8x8 and stride 4, a convolutional layer with 32 filters of size 4x4 and stride 2, and a fully connected layer with 256 hidden units. It has two sets of output - a softmax output for action probabilities and a scalar output for the estimated value function. The hyper-parameter settings are similar to BID17, except for using Adam with shared statistics instead of RMSProp, which yielded better results during training. The learning rate is set to 10 \u22124 and gradually decreased to zero for all experiments. To select \u03bb 1 and \u03bb 2 in our framework, we randomly sample \u03bb 1 \u2208 {0.05, 0.1, 0.5} and \u03bb 2 \u2208 {0.001, 0.01, 0.05}. \u03bb 1 is set to zero at the beginning of training and linearly increased to the sampled value at the end of training. Each experiment is repeated 25 times with different random seeds and randomly sampled \u03bb 1 and \u03bb 2. The top three results out of 25 runs are reported. A3C runs 16 agents on 16 CPU cores in parallel. Evaluation Metrics follow the procedure of BID16. The transfer reinforcement learning framework is evaluated using 30 episodes per game, following the 'no-op' procedure. Results show that compared to PathNet and PNN, the framework achieves higher performance with one teacher. State-of-the-art results on Atari games are also included for reference. In a transfer reinforcement learning framework, a student model trained with one teacher outperforms PathNet and PNN in 11 out of 14 experiments. With two teachers, the student model has significantly fewer parameters than PNN but still achieves higher scores in five out of seven experiments. Knowledge transfer from teachers to the student is effective, and performance improves when the number of teachers increases. Training curves for the experiments are shown in FIG1, with the average of the top three runs out of 25 displayed. The results of different combinations of environment/teacher settings are summarized in TAB2. Our approach generally performs well, outperforming PathNet and progressive neural network in knowledge transfer experiments. Our A3C implementation achieves better scores than those reported by BID17 for most games. Knowledge flow with expert teacher outperforms the baseline, transferring knowledge successfully. Additionally, knowledge flow with non-expert teachers also performs better than fine-tuning on a non-expert teacher, as the student can learn from multiple teachers and avoid starting from just one setting. The student can benefit from knowledge flow with expert and non-expert teachers, avoiding negative impacts and improving performance. Training curves are shown in FIG5, with more in the Appendix. The student model can benefit from intermediate representations of the teacher, even if input space, output space, and objectives differ. The student model benefits from learning from teachers and achieves higher scores compared to learning without teacher guidance. Various image classification benchmarks are used for supervised learning, with parameters \u03bb 1 and \u03bb 2 determined using validation sets. Evaluation is based on top-1 error rate on test sets, with results averaged over three runs with different random seeds. The study involves training teachers on CIFAR-10, CIFAR-100, and SVHN datasets before training the student model using a different approach. The experiments are conducted on CIFAR-10 and CIFAR-100 datasets with standard data augmentation, using Densenet as a baseline model. Training teachers on CIFAR-10, CIFAR-100, and SVHN datasets, then training the student model with different teacher combinations. Results show improvement over baseline with CIFAR-100 teacher, while SVHN teacher performs worse. Knowledge flow from both good and inadequate teachers improves by 13% over baseline, demonstrating the ability to leverage good teacher knowledge and avoid misleading influence. The results from training teachers on CIFAR-10, CIFAR-100, and SVHN datasets show improvement over baseline with CIFAR-100 teacher, while SVHN teacher performs worse. Knowledge flow from both good and inadequate teachers improves by 13% over baseline, demonstrating the ability to leverage good teacher knowledge and avoid misleading influence. Additional results on the CIFAR-100 dataset and properties of knowledge flow are discussed, along with the use of multiple pre-trained teacher nets. Our method introduces lateral connections to previously learned features, ensuring independence of the student during training. Distral, a combination of 'distill & transfer learning', involves joint training of multiple tasks with a shared policy to encourage consistency between tasks. Knowledge flow is different from multi-task learning frameworks like Distral. In multi-task learning, information from different tasks is shared to improve performance, while in knowledge flow, information from multiple teachers is used to help a student learn a new task. Other related work includes actor-mimic, learning without forgetting, and growing a brain policies. The curr_chunk discusses a general knowledge flow approach for training a deep net from multiple teachers, showing improvements in reinforcement learning and supervised learning. Future plans include learning when to use different teachers and actively swapping teachers during training. Knowledge distillation is used to transfer knowledge from a larger model to a smaller model. The curr_chunk discusses knowledge distillation, where a smaller student model learns from a larger teacher model. Experiments were conducted on various datasets using different model structures. The teacher model is a 50-layer ResNet BID8, and the student model is an 18-layer ResNet. The distilled student model's test error is summarized in TAB4. The framework outperforms KD as the student model benefits from both the output layer and intermediate layer representations of the teacher. The 'EMNIST Letters' dataset consists of 28x28 pixel images showing handwritten letters with 26 balanced classes. The training and test sets contain 124,800 and 20,800 images respectively. The 'EMNIST Digits' dataset has 10 balanced classes with images of size 28x28 pixels. Training and test sets contain 240,000 and 40,000 images respectively. Teachers were trained on EMNIST Digits, EMNIST Letters, and EMNIST Letters with 13 classes. The student model was compared to baseline, fine-tuning, and state-of-the-art results on EMNIST. In our framework with expert teacher (EMNIST Letters), semi-expert teacher (Half EMNIST Letters), and non-expert teacher (EMNIST Digits), student learning shows better performance compared to baseline and fine-tuning. The STL-10 dataset consists of colored images of size 96 \u00d7 96 pixels with 10 balanced classes. The training set has 5,000 labeled images and 100,000 unlabeled images, while the test set contains 8,000 images. The experiment only uses the 5,000 labeled images for training, utilizing the STL-10 model from Chen (2017) as the baseline, teacher, and student model. In our framework, teachers trained on CIFAR-10 and CIFAR-100 are compared to fine-tuning and baseline models. Results show that pretraining on these datasets reduces test errors by over 10%. In our framework, student model training further reduces test errors by 3% compared to fine-tuning. Results are obtained using fewer data and may not be directly comparable to other approaches. The accuracy over training epochs is illustrated in Fig. 5. Experiments are performed on Atari games, comparing to Distral BID26, a multi-task reinforcement learning framework. In experiments on Atari games, our framework outperforms Distral in reducing test errors by 3%. Distral, a multi-task agent, is suboptimal when the target task differs significantly from the source tasks. Our model is trained for 40M steps and can decrease a teacher's influence effectively. Our framework outperforms Distral in reducing test errors by 3% in Atari games. It can effectively decrease a teacher's influence, as shown in the C10 experiment where C100 teacher has higher normalized weight than SVHN teacher. In an ablation study, untrained teachers show worse performance compared to knowledgeable teachers in training. Learning with untrained teachers achieves an average reward of 15934, while learning with knowledgeable teachers achieves an average reward of 30928 in the target task of hero. Knowledge flow results in higher rewards than training. The results demonstrate that knowledge flow leads to higher rewards compared to training with untrained teachers in various environments and teacher-student settings. The KL term plays a crucial role in maintaining the student's output distribution, preventing drastic changes as teachers' influence decreases. Ablation study with KL coefficient set to zero shows significant performance drops without the KL term, highlighting its importance in maintaining stable rewards. Training with the KL term leads to higher rewards compared to training without it. At the end of training, the average reward with the KL term is 2907, while without it is 1215. Using a different architecture for the teacher model, specifically the BID16 model, also shows improved performance. The student model, based on the BID17 model, consists of 2 convolutional layers with 16 and 32 filters, followed by a fully connected layer with 256 ReLUs. The teacher model has 3 convolutional layers with 32, 64, and 64 filters, and a fully connected layer with 512 ReLUs. The teacher's layers are linked to corresponding layers in the student model for the target task. The experiment compared learning with teachers of different architectures to those with the same architecture for the target task of KungFu Master. Results showed similar performance between the two approaches, with average rewards of 37520 and 35012 respectively. More details can be found in FIG0. The experiment compared learning with teachers of different architectures to those with the same architecture for the target task of KungFu Master. Results showed that knowledge flow can enable higher rewards, even with different architectures. An average network can be used for the parameters \u03b8 old, computed using an exponential running average of the model weight with \u03b1 = 0.9, resulting in similar performance. Using an exponential average to compute \u03b8 old yields similar performance as using a single model. Results from the experiment show that an average network achieves an average reward of 96.2 for the target task of Boxing. Various techniques for knowledge transfer have been explored, such as fine-tuning, progressive neural nets, PathNet, 'Growing a Brain', actor-mimic, and learning without forgetting. In contrast to techniques like PathNet and Progressive Net, our approach involves multiple teacher nets trained to avoid catastrophic forgetting. This method allows for reusing parameters and discovering which weights can be reused for new tasks. Our method introduces lateral connections with normalized weights to avoid catastrophic forgetting, ensuring independence of the student during training. Distral combines distill and transfer learning, involving joint training of multiple tasks with a shared policy to encourage consistency between different tasks. Knowledge flow is different from multi-task learning as it focuses on a single task, leveraging information from multiple teachers to help a student learn a new task. In contrast, multi-task learning shares information from different tasks to boost performance. Knowledge distillation involves distilling information from a larger deep net into a smaller one, assuming both nets are trained on the same dataset. Our technique, Actor-mimic BID20, enables knowledge transfer between different domains, allowing an agent to learn multiple tasks simultaneously. It uses a single policy net guided by expert teachers to produce similar actions and representations. Unlike traditional methods, we leverage teachers' representations at the start of training to prevent forgetting and add new tasks efficiently. Learning without forgetting BID13 allows adding a new task to a deep net without losing original capabilities. Only data from the new task is used, while old capabilities are retained by recording the old network's output on the new data. This contrasts with techniques that transfer knowledge more explicitly from teacher networks."
}