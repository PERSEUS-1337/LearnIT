{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging. A non-statistical framework based on identifying modular organization of the network is proposed, relaxing the requirement of exploiting statistical independence. Experiments show modularity between groups of channels is achieved to a certain degree. Deep generative models have shown success in designing realistic images across various domains. Modularity between groups of channels has been achieved to a certain degree, allowing for targeted interventions on complex image datasets. This has led to applications such as computationally efficient style transfer and automated assessment of robustness in pattern recognition systems. State-of-the-art approaches like Generative Adversarial Networks (GAN) have been particularly effective in this domain. State-of-the-art generative models like GAN and VAE aim to produce disentangled latent representations for controlling interpretable image properties. However, these models lack a mechanistic or causal understanding where interpretable properties cannot be attributed to specific parts of the network architecture. Access to a modular organization of generative models would enhance interpretability and enable tasks like generating objects in a background. Generative models like GAN and VAE aim to produce interpretable image properties by leveraging modular organization. This allows for extrapolations, supporting adaptability and decision making. In this paper, a causal framework is proposed to explore modularity in deep generative architectures, aiming to allow for individual modifications of causal mechanisms without influencing each other. In this study, a causal framework is used to assess how well generative models capture causal mechanisms and the role of internal variables. The analysis includes evaluating disentanglement in deep generative models through unsupervised counterfactual manipulations with VAEs and GANs. Generative models like VAEs and GANs exhibit modularity in hidden units, allowing for counterfactual editing of generated images. Interpretability of convolutional neural networks has been extensively studied in discriminative architectures, while generative models require a different approach due to high-dimensional downstream effects. InfoGANs and other works have also explored this area. Intrinsic disentanglement is introduced to uncover the internal organization of networks, highlighting that many transformations are statistically dependent and unlikely to be disentangled in the latent space. This contrasts with a framework that requires semantic information for interventions on internal variables of a GAN. Our approach to disentanglement, introduced independently, is more flexible than previous proposals as it applies to arbitrary continuous transformations without the strong requirements of representation theory. Additionally, an interventional approach to disentanglement has been taken by Suter et al., focusing on extrinsic disentanglement in a classical graphical model setting. The text introduces a framework for disentanglement in generative models, focusing on mapping latent space to data points. The theory section is presented informally for easier understanding, with mathematical details provided in Appendix A. The text introduces a framework for disentanglement in generative models, focusing on mapping latent space to data points. It discusses the term representation as a mapping from Y M to a representation space R. A causal generative model (CGM) is proposed for implementing the generative model using a non-recurrent neural network. The text introduces a framework for disentanglement in generative models, focusing on mapping latent space to data points using a Causal Generative Model (CGM). It discusses the representation as a mapping from Y M to a representation space R, incorporating endogenous variables in the causal graph. The text introduces a framework for disentanglement in generative models, focusing on mapping latent space to data points using a Causal Generative Model (CGM). It discusses the internal representation of the network and the constraints on the endogenous variables in the latent case. The CGM framework allows defining counterfactuals in the network, following Pearl (2014). Unit-level counterfactuals are obtained by replacing subset variables and assignments, inducing a transformation of the generative model output. Faithfulness of counterfactual mapping is introduced to account for this transformation. In the context of generative models, non-faithful counterfactuals can lead to outputs that deviate from the learned data distribution, potentially causing artifactual results or enabling extrapolation to unseen data. The concept of disentangled representation suggests that individual latent variables encode real-world transformations sparsely. Latent variables sparsely encode real-world transformations, driving supervised disentanglement approaches. Unsupervised methods aim to learn these transformations from unlabeled data. State-of-the-art approaches enforce conditional independence between latent factors to achieve disentanglement. The statistical approach to disentanglement enforces conditional independence between latent factors, but faces issues with i.i.d. constraints on the prior distribution and the insufficiency of independence constraints to specify a disentangled representation. This makes finding an appropriate inductive bias for downstream tasks challenging. An open question remains on the appropriate inductive bias for learning representations that benefit downstream tasks. State-of-the-art unsupervised approaches have mainly been demonstrated on synthetic datasets, with limited success on complex real-world data beyond MNIST and CelebA. Disentangled generative models on complex real-world datasets often exhibit lower visual sample quality compared to non-disentangled models. A non-statistical definition of disentanglement is proposed, focusing on transformation-based insights. Disentanglement of properties in data involves transforming the latent space to act on individual variables, allowing for independent mechanisms in the representation. This concept is crucial for learning representations that benefit downstream tasks. The concept of extrinsic disentanglement involves transforming the latent representation to achieve independent mechanisms, regardless of the specific property being disentangled. This functional definition is agnostic to subjective choices and statistical notions of independence, but still requires statistical independence between disentangled factors in the latent space. The concept of extrinsic disentanglement involves transforming the latent representation to achieve independent mechanisms, regardless of the specific property being disentangled. In contrast, the extended definition of disentanglement allows transformations of internal variables of the network to be intrinsically disentangled with respect to a subset of endogenous variables. In contrast to extrinsic disentanglement, intrinsic disentanglement involves transforming internal variables to be independent with respect to a subset of endogenous variables. This structural property allows for the implementation of arbitrary disentangled transformations. Proposition 2 introduces the concept of implementing arbitrary disentangled transformations. A subset of endogenous variables is considered modular if any transformation applied to it within its input domain is disentangled. This framework is based on a functional definition of disentanglement that applies to transformations, linking it with an intrinsic property of the trained network. A disentangled representation involves partitioning the intermediate representation into modules, where transformations within each module lead to valid transformations in the data space. This partition requires introducing modules for the set of latent variables, which was not considered in classical approaches to disentanglement. Our framework suggests that single neurons may not represent meaningful information independently, but rather need to be grouped into modules at a mesoscopic level. Finding a modular structure in the network allows for a broad class of disentangled transformations. A modular structure in the network enables a broad class of disentangled transformations. Counterfactual interventions define transformations by assigning a constant value to endogenous variables, aiming for faithful results. Sampling from the marginal distribution of variables in E illustrates the procedure using a standard feed-forward multilayer neural network. The hybridization procedure involves selecting endogenous variables from output activations of channels in a neural network. Two original examples are generated using latent variables, and tuples of variable values are memorized. This process assumes a modular structure in the network. The hybridization framework involves generating hybrid examples by combining features from different images encoded by modular structures. This allows for assessing how a specific module affects the output of the generator network by quantifying its causal effect through generating pairs of latent vectors independently and creating hybrid examples. The hybridization framework involves generating hybrid examples by combining features from different images encoded by modular structures. This allows for assessing the causal effect of specific modules on the generator network's output by creating pairs of latent vectors independently and estimating an influence map through unit-level causal effects. The hybridization framework involves assessing the causal effect of specific modules on the generator network's output by creating pairs of latent vectors independently and estimating an influence map through unit-level causal effects. The value of the unit-level causal effects may vary across units, and the results are averaged over multiple interventions corresponding to different values of z2. A challenge is selecting subsets to intervene on, especially with networks containing many units or channels per layer. A fine to coarse approach is used to extract such groups. In a fine to coarse approach, influence maps are estimated for each output channel of convolutional layers in a network. These maps are then grouped by similarity to define modules at a coarser scale, showing functional segregation in VAE trained on CelebA face dataset. The text discusses how influence maps for each output channel of convolutional layers are grouped into modules based on similarity, showing functional segregation in VAE trained on CelebA face dataset. Channels are clustered using EIMs as feature vectors, pre-processed by local averaging and thresholding to create a matrix fed into the model. The text explains the process of generating cluster template patterns from an image using Non-negative Matrix Factorization (NMF) algorithm. The NMF algorithm helps in isolating meaningful parts of images into different components, which are then used to assign clusters to influence maps based on template pattern contributions. The text introduces a toy generative model for justifying the NMF based approach in image clustering. It involves a neural network with hidden layers and activation functions, along with random choice model parameters. The model parameters involve sampling coefficients from arbitrary distributions for H k 's and W k columns, with specific conditions on sets of indices I k to encode influence areas in the image. The model parameters involve sampling coefficients from arbitrary distributions for H k 's and W k columns, with specific conditions on sets of indices I k to encode influence areas in the image. In Model 1, the hidden layer partition corresponds to a disentangled representation, allowing for the use of NMF to generate a binary matrix summarizing significant influences on each output pixel. The study focused on the modularity of generative models trained on the CelebFaces Attributes Dataset (CelebA). A basic architecture, a plain \u03b2-VAE, was used for the investigation. The full procedure included EIM calculations. The study utilized a plain \u03b2-VAE architecture to analyze the modularity of generative models trained on the CelebFaces Attributes Dataset. The procedure involved EIM calculations, clustering channels into modules, and hybridizing generator samples. Setting the number of clusters to 3 resulted in interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed these observations. The cluster stability analysis confirmed that using 3 clusters is a reasonable choice, with consistency dropping considerably for 4 clusters. The results show that the clustering is robust across the number of clusters, with a large consistency (>90%) for 3 clusters. The NMF-based clustering outperforms k-means algorithm with large consistency (>90%) for 3 clusters. Cosine similarity between templates is .9, indicating robustness. Influence maps reflect the observation. The NMF-based clustering outperforms k-means algorithm with high consistency for 3 clusters. Influence maps show that some maps may spread over image locations reflecting different clusters. Applying the hybridization procedure to the resulting 3 modules obtained by clustering leads to a replacement of features while preserving the overall image structure. The \u03b2-VAE is designed for extrinsic disentanglement. Further work has shown that the \u03b2-VAE may not be optimal compared to other approaches, suggesting the need to explore better extrinsic disentanglement for intrinsic disentanglement. Investigating intrinsic disentanglement in models where disentanglement is not explicitly enforced, such as GAN-like architectures, is crucial. These architectures typically outperform VAE-like approaches in sample quality for complex image datasets. Results were replicated in the tensorlayer DCGAN implementation. Our approach was tested on various models, including the official tensorlayer DCGAN implementation and a pretrained Boundary Equilibrium GAN (BEGAN) known for high-quality face image generation. The simplicity of BEGAN's generator architecture allowed for minimal modifications to test our hypothesis effectively. Intervening on layers 5 and 6 of the BEGAN model showed a selective transfer of features from Original 2 to Original 1, particularly in hair transfer. The model was trained on face images with a tight frame, resulting in one module associated with hair and background features. The hybridization procedure in the study selectively transferred features from Original 2 to Original 1, with one module focusing on hair transfer. Evaluation using Frechet Inception Distance showed minimal impact on image quality. The approach was tested on high-resolution generative models and complex image datasets. To generate hybrids from complex image datasets, the BigGAN-deep architecture was used, pretrained on the ImageNet dataset. The architecture consists of 12 Gblocks with skip connections and direct input from latent variables and class labels. Mixing features of different classes was successful, and intervening on two successive layers within a Gblock was found to be effective for generating counterfactuals. Intervening on two successive layers within a Gblock was effective for generating counterfactuals, as shown in examples provided. High-quality counterfactuals with modified backgrounds and similar foreground objects were successfully created. Meaningful combinations of objects of different nature, such as a teddy bear in a tree or a \"teddy-koala\" hybrid, were also generated. The study investigated how counterfactual images can improve classifier robustness. Several pretrained classifiers were compared, showing higher recognition rates when intervening closer to the output layer. At intermediate blocks 5-6, interesting results were observed. At intermediate blocks 5-6, the Inception resnet outperformed other classifiers, showing different reliance on image content for decision-making. Mathematical definition of disentanglement was introduced for unsupervised characterization of representation in deep generative architectures. Our framework reveals interpretable modules in generative models, aiding in style transfer and object recognition system robustness assessment. This research enhances the understanding and utilization of deep neural networks. The curr_chunk discusses how deep neural networks can be better utilized and made more sustainable by enhancing interpretability and enabling them to be used for tasks they were not originally trained for. It proposes using a trained generator architecture as a mechanistic model that can be manipulated independently. This can be represented mathematically using structural causal models (SCMs) with structural equations (SEs). Structural equations (SEs) in SCMs assign values to variables based on other variables and external influences. SEs remain valid even after interventions, modeling operations in neural network computational graphs. A Causal Generative Model (CGM) captures computational relations between input latent variables, generator output, and endogenous variables in neural network implementations. The generator's output can be decomposed into two steps in a feed-forward neural network. A Causal Generative Model (CGM) in a neural network involves computational relations between input latent variables, generator output, and endogenous variables. The CGM comprises a directed acyclic graph and a set of structural equations. The graph includes endogenous variables, latent inputs, and the output, aligning with the definition of a deterministic structural causal model by Pearl. The CGM in neural networks involves specificities like variable assignments with latent/exogenous variables, allowing for feed-forward networks. This ensures unambiguous assignments of endogenous variables and output, resembling existing generative networks' computational graphs. The CGM in neural networks involves variable assignments with latent/exogenous variables for feed-forward networks, ensuring unambiguous assignments of endogenous variables and output. Useful mappings are introduced based on the choice of variables and dimensions, with latent and endogenous mappings defined for transforming latent and endogenous variables into outputs. The CGM in neural networks involves variable assignments with latent/exogenous variables for feed-forward networks, ensuring unambiguous assignments of endogenous variables and output. Latent and endogenous mappings are constrained to subsets of their euclidean ambient space, with proper embeddings defined as embedded CGMs. The example in Fig. 2b contains exactly two layers. The mappings g M and g M are well defined and surjective due to appropriate choices for domains and codomains. The image sets (V M, Y M, ...) of a trained model are constrained by the parameters of M and are not easy to characterize. The goal is for Y M to approximate the support of the data distribution. Learning the generator parameters to match Y M with the target data distribution is a major objective for generative models. Topology-respecting transformations are used to manipulate the output. Generative models aim to match the output distribution with the target data distribution by using topology-respecting transformations. For embedded CGMs, injectivity of the function is a key requirement, especially when the model's latent variables are uniformly distributed. Generative models based on uniformly distributed latent variables, like many GANs, are considered embedded CGMs if they are injective. VAEs typically have a non-compact latent space, but by restricting it to compact intervals, an embedded CGM can approximate the original model for most samples. This framework allows for defining counterfactuals in the network following Pearl (2014). Unit-level counterfactuals in a generative model involve replacing structural assignments for a subset of variables to obtain a new output. This concept is related to potential outcomes and induces a transformation in the model's output. Counterfactual mapping in an embedded CGM involves a continuous map that relates to disentanglement. Our approach relates counterfactuals to intrinsic disentanglement in a CGM. An endomorphism T is intrinsically disentangled with respect to a subset E of endogenous variables if a transformation T of endogenous variables affects only the variables indexed by E. This concept is illustrated in Fig. 2d. The split node in the generative model indicates disentanglement, expressing robustness to perturbations in subsystems. Counterfactuals are examples of perturbations that can be disentangled. A continuous and injective embedding is achieved when Z is compact and the codomain of g M is Hausdorff. The proof shows that an injective mapping g M implies embeddings on compact domains V M, leading to equivalence between faithful and disentangled transformations in the context of a generative model. The proof demonstrates that values in subsets A and B are unambiguously assigned by non-overlapping latent variables, ensuring T is an endomorphism of V M. This implies T is well defined and an endomorphism, following the i.i.d. assumption for components of Z and the structure of endogenous variables associated with each V k. The subsets of endogenous variables associated with each V k are modular and form a disentangled representation. The choice of increasing dimensions and i.i.d. sampling ensure an injective mapping, following embedded CGM assumptions. Counterfactual hybridization of any component of V k results in an influence map covering exactly I k. The conditions on I k and the thresholding approach guarantee a rank K binary factorization of matrix B. The \u03b2-VAE architecture, similar to DCGAN, consists of three blocks of convolutional layers with a filter size of (3, 3). Hyperparameters for both structures are specified in Table 1. The method proposed in Berthelot et al. (2017) was used for the CelebA dataset with a pre-trained model. The architecture used for image generation includes upsampling layers and skip connections to enhance image sharpness. The pretrained model is based on the BigGan-deep architecture by Brock et al. (2018) and consists of ResBlocks with BatchNorm-ReLU-Conv Layers and skip connections. More architectural details can be found in Berthelot et al. (2017). The curr_chunk discusses the generation of influence maps and FID analysis of BEGAN hybrids. It references architectural details from Higgins et al. (2018) and Brock et al. (2018). The influence maps show the impact of perturbations on pixels in the CelebA dataset. FID analysis measures the distance between real data, generated data, and hybrids created by intervention on different clusters. The FID analysis for BEGAN hybrids shows that Hybrids have a small distance to the generated data, indicating visually plausible images. The entropy is computed for the top ranking classes across all hybrids, normalized to provide a total probability of 1. The entropy values for hybrids based on interventions on Gblock 4 are smaller, suggesting that object texture is crucial for the classifier's decision. Larger collection of hybrids for the BIGAN between classes \"cock\" and \"ostrich\" are shown in Figure 15, with modules fixed and extracted by the NMF algorithm. The NMF algorithm extracts modules for hybrids based on interventions, with large entropy observed for the first module of Gblock 5. Classification outcomes of discriminative models for koala+teddy hybrids are shown in Table 3. The experiment investigates the robustness of classifiers using koala+teddy hybrids. The resultant hybrids resemble a teddy bear in a koala context. The ideal classifier should focus on the object present, not the context. Nasnet large is more robust to contextual changes compared to other classifiers. Nasnet large is more robust to contextual changes compared to other classifiers, as shown in Figure 17 with three koala+teddy hybrids as inputs."
}