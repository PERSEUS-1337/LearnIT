{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models successful in modeling high-dimensional continuous measures. A scalable method for unbalanced optimal transport (OT) is presented in this paper using the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the cost of pushing a source measure to a target measure. The formulation is theoretically justified and related to prior work by Liero et al. (2018). An algorithm based on stochastic alternating gradient updates is proposed for solving this problem efficiently. The paper presents a methodology for solving unbalanced optimal transport problems using stochastic alternating gradient updates, similar to GANs. It demonstrates how this approach can be applied to population modeling, specifically in transforming a source population into a target population by modeling mass transport and local mass variations. The text discusses mass variations in population modeling using optimal transport methods, specifically focusing on the Kantorovich formulation and the Sinkhorn algorithm for efficient solution. Regularizing the objective with an entropy term improves the dual problem's solvability, leading to proposed stochastic methods for optimization. Optimal transport methods, including the Sinkhorn algorithm, have been proposed for various applications such as computer graphics, domain adaptation, image translation, natural language translation, and biological data integration. Generative models like GANs can be used to learn transport maps when a transport cost is not available. Optimal transport methods, including the Sinkhorn algorithm, have been used in various applications. Biological data integration challenges have been addressed using GANs with conditioning or cycle-consistency strategies to enforce correspondence between populations. Several formulations have been proposed for extending the theory of Optimal Transport (OT) to handle mass variation, with scaling algorithms developed for approximating solutions to optimal entropy-transport problems. This formulation corresponds to the Kantorovich OT problem where hard marginal constraints are relaxed. The Kantorovich OT problem is extended to handle mass variation by relaxing hard marginal constraints using divergences. This approach has been used in various applications such as computer graphics, tumor growth modeling, and computational biology. However, current methods cannot perform unbalanced OT between continuous measures. A novel framework inspired by GANs is introduced to directly model mass variation in unbalanced optimal transport. The framework presented introduces a novel approach for unbalanced optimal transport by modeling mass variation directly. It proposes solving a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport. The methodology for solving the relaxed problem is developed using a convex conjugate representation of divergences. Our methodology applies convex conjugate representation of divergences to solve unbalanced optimal transport, demonstrated on various datasets including MNIST, USPS, CelebA, and zebrafish RNA-seq data. Additionally, a scalable method (Algorithm 2) is proposed in the Appendix for solving optimal-entropy transport problem in continuous settings, extending previous work to unbalanced OT for large datasets. Optimal transport (OT) is a scalable alternative to the BID8 algorithm for large datasets. It addresses the problem of transporting measures in a cost-optimal manner using pushforward operators and marginals. Monge (1781) introduced this concept. Optimal transport (OT) solves the problem of transporting measures cost-effectively. Monge formulated this as a search over deterministic transport maps. The Kantorovich OT problem is a convex relaxation of the Monge problem, using probabilistic transport plans. This approach involves stochastic maps from X to Y, a \"one-to-many\" version of the deterministic map from Monge. The relaxed optimal transport problem involves introducing entropic regularization, leading to a simpler dual optimization problem solvable with the Sinkhorn algorithm. Stochastic algorithms have been proposed for computing transport plans that can handle continuous measures and mass variation, extending classical optimal transport formulations. Existing numerical methods for handling mass variation have been proposed, based on optimal-entropy transport formulations. These methods involve relaxing marginal constraints using divergences to find a measure that minimizes a certain function. Mass variation is allowed in this approach, as the marginals of the measure are not constrained to be specific values. In the discrete setting, a class of iterative scaling algorithms BID8 generalizes the Sinkhorn algorithm for computing regularized OT plans BID13. A new algorithm for unbalanced OT directly models mass variation and can be applied to high-dimensional continuous measures. The goal is to learn a stochastic transport map and scaling factor to push a source to a target measure. The goal is to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner, involving unbalanced Monge OT with constraints on mass transport and variation. The goal is to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner, involving unbalanced Monge OT with constraints on mass transport and variation. In practical applications, stochastic maps are more suitable as they model one-to-many relationships, such as in cell biology where one cell can give rise to multiple cells. Examples of problems that can be modeled using unbalanced Monge OT include cell population dynamics. The transport map T models the movement of points from a source to a target measure, while the scaling factor \u03be represents growth or shrinkage of these points. Different transformation models are optimal based on costs of mass transport and variation. The transport map T moves points from a source to a target measure, with a scaling factor \u03be adjusting for class imbalances. A relaxation of the optimization constraint is considered using a divergence penalty instead of an equality constraint. The relaxation of the optimization constraint involves using a penalty instead of an equality constraint, with a joint measure \u03b3 specified by (T, \u03be) and reformulated to obtain the objective function for optimal-entropy transport. The difference lies in the search space, as not all joint measures \u03b3 can be specified by (T, \u03be). The optimization constraint is relaxed by using a penalty instead of an equality constraint, with a joint measure \u03b3 specified by (T, \u03be) for optimal-entropy transport. The support of \u03b3 is restricted to supp(\u00b5) \u00d7 Y to establish equivalence, as not all joint measures can be specified by (T, \u03be). Based on the relation between optimal entropy-transport and divergence penalty, solutions of the relaxed problem converge to solutions of the original problem. Theorem 3.4 states that under certain conditions, solutions converge for a sequence. The relaxation of unbalanced Monge OT allows for learning the transport map and scaling factor using stochastic gradient methods. The divergence term is minimized by defining it as a penalty with an adversary function, leading to convergence of solutions from the relaxed problem to the original problem under certain conditions. The optimization procedure involves using neural networks to parameterize T, \u03be, and f, optimizing with alternating stochastic gradient updates. It resembles GAN training, with T transporting points from X to Y, \u03be assigning importance weights, and f minimizing divergence between transported and real samples. Cost functions c1 and c2 guide T and \u03be to find efficient strategies. The optimization procedure uses neural networks to parameterize T, \u03be, and f, optimizing with alternating stochastic gradient updates. Cost functions c1 and c2 guide T and \u03be to find efficient strategies. The probabilistic Monge-like formulation is similar to the Kantorovich-like entropy-transport problem in theory but results in different numerical methods. Examples of divergences with corresponding entropy functions are provided in Table 1 in the Appendix. Further practical considerations for implementation and training are discussed in Appendix C. Algorithm 1 solves the non-convex formulation (6) using neural networks to learn a transport map T and scaling factor \u03be, enabling scalable optimization with stochastic gradient descent. The neural architectures imbue their function classes with a specific structure, facilitating effective learning in high-dimensional settings. The scaling algorithm of BID8 solves a convex optimization problem and is proven to converge, but is currently only practical for discrete problems and has limited scalability. A new stochastic method based on the same dual objective as BID8 is proposed in the Appendix to handle transport between continuous measures and overcome scalability limitations. The new stochastic method proposed in the Appendix handles transport between continuous measures, overcoming scalability limitations of BID8. However, the output in the form of the dual solution is less interpretable for practical applications compared to Algorithm 1. It is unclear how to obtain a scaling factor or a stochastic transport map that can generate samples outside of the target dataset. The advantage of directly learning a transport map and scaling factor using Algorithm 1 is shown in numerical experiments. The problem of learning a scaling factor to balance measures arises in causal inference, where the goal is to eliminate selection biases in treatment effects. BID23 proposed a method for learning the scaling factor, but did not consider transport. Algorithm 1 is illustrated to show its performance in practice. Algorithm 1 is demonstrated in practice for unbalanced optimal transport, focusing on population modeling. It is applied to modified MNIST datasets with class distribution variations, simulating class imbalance scenarios. Algorithm 1 is applied to modified MNIST datasets to model population drift by reflecting class imbalances between source and target distributions. The scaling factor learned by Algorithm 1 can be used to represent growth or decline of different classes in a population. The reweighting process during unbalanced OT.MNIST-to-USPS is illustrated in FIG4. Unbalanced OT is applied from the MNIST dataset to the USPS dataset to model population evolution. The evolution is modeled using Algorithm 1 with transport cost as the Euclidean distance between original and transported images, as shown in FIG1. The size of the image in the unbalanced OT model reflects the scaling factor of the original MNIST image when transported to the USPS dataset. Despite limitations, many MNIST digits retained their likeness during the transport, with brighter digits generally having higher scaling factors. MNIST digits with higher scaling factors appeared brighter and covered a larger area of pixels compared to those with lower scaling factors. This is consistent with the observation that USPS digits are generally brighter and contain more pixels. Algorithm 1 was applied to the CelebA dataset for unbalanced OT from young to aged faces, simulating a real application of modeling population transformation based on samples from different timepoints. The study used a variational autoencoder on the CelebA dataset to encode faces into a latent space and applied unbalanced optimal transport from young to aged faces. The transported faces generally retained the original features, as shown in FIG2. The study applied a variational autoencoder on the CelebA dataset to encode faces into a latent space and used unbalanced optimal transport from young to aged faces. The transported faces generally retained the original features, with exceptions like gender swaps. Young faces with higher scaling factors were significantly enriched for males compared to those with lower scaling factors. The model predicts a growth in the prominence of male faces compared to female faces as the CelebA population evolves from young to aged. Based on ground truth labels, a strong gender imbalance was found between young and aged populations, with young being mostly female and aged mostly male. Lineage tracing in biology involves tracking cells between different stages or during disease progression. Learning the scaling factor is relevant in single-cell gene expression data analysis. In single-cell gene expression data analysis, the scaling factor was determined by comparing cells from blastula stage with higher scaling factors to the rest using differential gene expression analysis. Cells with higher scaling factors showed significant upregulation of genes. In single-cell gene expression data analysis, cells with higher scaling factors were enriched for genes associated with differentiation and development of the mesoderm. A stochastic method for unbalanced OT based on the regularized dual formulation of BID7 is presented, which can lead to meaningful biological discovery. The dual formulation involves a constrained optimization problem that is challenging to solve. In single-cell gene expression data analysis, a stochastic method for unbalanced optimal transport involves adding a strongly convex regularization term to the primal objective. This term encourages plans with high entropy and the dual of the regularized problem is given by a supremum over functions. The relationship between the primal optimizer and dual optimizer is then rewritten in terms of expectations. In unbalanced optimal transport, neural networks can be used to optimize parameters u, v with stochastic gradient descent. This generalizes classical OT to unbalanced OT, with Algorithm 2 describing the process. The algorithm is a dual of the entropy-regularized classical OT problem. The dual solution learned from Algorithm 2 in unbalanced optimal transport can reconstruct the primal solution based on mass transportation between points in X and Y. The transport map \u03b3 * indicates mass variation implicitly built into the problem. The transport map \u03b3 * in unbalanced optimal transport indicates mass variation implicitly built into the problem. A stochastic algorithm for learning such a map from the dual solution is proposed in Algorithm 3. The formulations are equivalent when the search space is restricted to joint measures specified by (T, \u03be). This relation is formalized by Lemma 3.3, showing the inequality L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) for any solution (T, \u03be) and defined \u03b3. The inequality L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) holds for any solution (T, \u03be). By the disintegration theorem, there exists a family of probability measures {\u03b3 y|x} and measurable functions {T x : Z \u2192 Y} such that \u03b3 y|x is the pushforward measure of \u03bb under T x for all x \u2208 X. By hypothesis, \u03c0 X # \u03b3 is restricted to the support of \u00b5, i.e. \u03c0 X # \u03b3 \u226a \u00b5. Let \u03be be the Radon-Nikodym derivative d\u00b5. It follows from the Radon-Nikodym theorem that (T, \u03be) satisfy the same relation as in (12). By Fubini's Theorem for Markov kernels and change of variables, it implies that W c1,c2,\u03c8 (\u00b5, \u03bd) \u2265 L \u03c8 (\u00b5, \u03bd), completing the proof. Several theoretical results for optimal entropy-transport follow from the analysis by BID27. The analysis by BID27 yields theoretical results on optimal entropy-transport, including the existence and uniqueness of joint measures specified by minimizers of L \u03c8 (\u00b5, \u03bd) under certain conditions. The proof of Proposition B.1 shows that minimizers exist and are unique when \u03c8 \u221e = \u221e. The minimizers of L \u03c8 (\u00b5, \u03bd) exist and are unique under certain conditions, leading to uniquely determined marginals for any solution \u03b3 of W c1,c2,\u03c8 (\u00b5, \u03bd). The product measure generated by the minimizers is also unique, proving the uniqueness of \u03b3. L \u03c8 can define a proper metric between positive measures \u00b5 and \u03bd for certain cost functions and divergences. Theorem 3.4 states that solutions of the relaxed problem converge to solutions of the original problem. Proof of this theorem involves the convergence of a specific function to an equality constraint, leading to a lower bound on the Wasserstein-Fisher-Rao metric. Lemma 3.9 in BID27 shows that the limit of a specific function is greater than or equal to a certain value. The sequence of minimizers is bounded, and if certain assumptions are met, the sequence is also tight. The sequence \u03b3 k is equally tight under certain assumptions. By an extension of Prokhorov's theorem, a subsequence of \u03b3 k weakly converges to \u03b3. \u03b3 is a minimizer of W c1,c2,\u03b9= (\u00b5, \u03bd) and \u03b3 k is equivalent to the product measure induced by minimizers of L \u03b6 k \u03c8 (\u00b5, \u03bd). In this section, the convex conjugate form of \u03c8-divergence is used to rewrite the main objective as a min-max problem for non-negative finite measures P, Q over T \u2282 R d. The lemma states that under certain conditions, a subset of measurable functions can be equal to a specific function, with a simple proof provided. This result has been utilized in generative modeling. The lemma in BID27 shows that the optimal function f over the support of Q is obtained when dP dQ belongs to the subdifferential of \u03c8 * (f). Proposition B.1 provides conditions for the problem to be well-posed, particularly in choosing cost functions. In practice, the cost of transport, c1, is a measurement of correspondence between X and Y, often taken as the Euclidean distance between x and y. For the cost of mass adjustment, c2, a convex function is chosen that vanishes at 1 and prevents \u03be from becoming too small or too large. Various entropy functions can be used for c2. The text discusses the use of entropy functions and convex conjugates for training generative models. It explains how any \u03c8-divergence can be used to match a generated distribution to a true data distribution. Jensen's inequality is highlighted as a key concept in minimizing D \u03c8 (P |Q) when P = Q. An example from the original GAN paper is provided to illustrate the concept further. The text discusses using entropy functions and convex conjugates for training generative models. It explains how \u03c8-divergence can match generated distribution to true data distribution. Jensen's inequality is crucial in minimizing D \u03c8 (P |Q) when P = Q. Additional constraints on \u03c8 ensure divergence minimization matches P to Q. The text discusses using entropy functions and convex conjugates for training generative models. It explains how \u03c8-divergence can match generated distribution to true data distribution. Jensen's inequality is crucial in minimizing D \u03c8 (P |Q) when P = Q. Additional constraints on \u03c8 ensure divergence minimization matches P to Q. When D \u03c8 (P |Q) is minimized, P = Q in general. Choice of activation layers and neural architectures is crucial for training generative models using \u03c8-divergence. Activation layers should map to the correct range, while neural architectures like fully-connected feedforward networks with ReLU activations are commonly used. The neural architectures for training generative models using \u03c8-divergence commonly involve networks with 3 hidden layers and ReLU activations. The output activation layers map final pixel brightness to the range (0, 1) and the scaling factor weight to the range (0, \u221e) using sigmoid and softplus functions, respectively."
}