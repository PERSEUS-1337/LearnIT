{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. Adaptivity is implemented across manifolds in the cartesian product. Our generalization of adaptive methods to Riemannian manifolds shows faster convergence and lower train loss values compared to standard algorithms. Developing stochastic gradient-based optimization algorithms for various applications is crucial, with a focus on computational efficiency using first order methods. For computational efficiency, first order methods are commonly used when optimizing a large number of parameters. Recent developments have led to successful algorithms like ADAGRAD, ADADELTA, ADAM, and AMSGRAD for optimizing parameters in a Euclidean space. Recent advancements in optimization algorithms have focused on parameters on a Riemannian manifold, allowing for non-Euclidean geometries. These algorithms have been applied to various tasks such as solving Lyapunov equations, matrix factorization, and dictionary learning. Seminal methods like Riemannian stochastic gradient descent (RSGD) have been developed for this setting. In this work, the focus is on adapting successful optimization algorithms like Riemannian stochastic gradient descent (RSGD) to the geodesically convex case. The challenge lies in generalizing these adaptive schemes to a Riemannian setting, where intrinsic coordinate systems are not readily available, making notions like sparsity or coordinate-wise updates meaningless. The study focuses on adapting optimization algorithms to the Riemannian setting, proposing generalizations for algorithms in the case of a product of manifolds. The motivation behind this work was the learning of symbolic embeddings in non-Euclidean spaces, specifically for tasks like hyperbolic taxonomy embedding. The study explores optimizing Euclidean word embeddings with ADAGRAD, highlighting the need for Riemannian adaptive algorithms for competitive optimization-based Riemannian embedding methods. It also touches on differential geometry concepts and suggests potential benefits for embedding methods in hyperbolic spaces. A manifold M of dimension n is a space that can locally be approximated by a Euclidean space R n. The tangent space T x M is an n-dimensional vector space at each point x \u2208 M. A Riemannian metric \u03c1 is a collection of inner-products on the tangent space. A Riemannian metric \u03c1 is a collection of inner-products varying smoothly on a manifold M, defining local geometry. It induces a global distance function on M, where the distance between points x and y is the infimum of path lengths between them. Riemannian SGD updates on a Riemannian manifold involve using the exponential map or a retraction map to update along the shortest path in the relevant direction while staying within the manifold. The main algorithms of interest are ADAGRAD and ADAM. ADAGRAD rescales updates coordinate-wise based on past gradients, beneficial for sparse gradients or deep networks. ADAM's update rule involves momentum terms for adaptive learning rates. The ADAM update rule, introduced by BID9, includes momentum and adaptivity terms. When \u03b2 1 = 0, it is similar to RMSPROP BID24 but with an exponential moving average. This helps avoid the issue of learning stopping too early in ADAGRAD. The momentum term in ADAM has shown significant empirical improvements. AMSGrad, proposed as a modification to the ADAM algorithm, addresses a convergence proof mistake identified by BID18. It introduces AMSGrad and ADAMNC as solutions, with AMSGrad involving an increasing schedule for \u03b2 2. Coordinate-wise updates on a Riemannian manifold require a coordinate system, and a small \u03b5 is often added for numerical stability. On a Riemannian manifold, coordinate-wise updates require a coordinate system. A small \u03b5 is typically added for numerical stability. Different charts can be defined around each point, with quantities defined intrinsically to the manifold. The Riemannian gradient of a function can be defined intrinsically, but its Hessian is only intrinsically defined at critical points. The RSGD update on a Riemannian manifold is intrinsic, involving exp and grad. Expressing updates in a coordinate-free manner is unclear. One could use a canonical coordinate system for updates in the tangent space. In a general Riemannian manifold, parallel transport depends on the chosen path and curvature introduces a rotational component, breaking the sparsity of gradients and adaptivity benefits. The interpretation of adaptivity as optimizing different features at different speeds is lost in this context. The text discusses the loss of interpretability in optimizing different features at different speeds on a Riemannian manifold due to the dependence on the chosen path and curvature. The techniques used to prove the theorems do not apply to updates defined in a certain way, assuming additional structure on the manifold. In a Riemannian manifold, designing adaptive schemes for each component x i as a \"coordinate\" is proposed to address the difficulty of meaningful adaptive schemes. The adaptivity term involves rescaling the gradient using squared Riemannian norms. ADAM, ADAGRAD, and AMSGRAD were briefly discussed in section 2. ADAM is a combination of ADAGRAD with momentum, using exponential moving averages for past squared-gradients. AMSGRAD corrects ADAM's convergence proof. ADAMNC is ADAM with a non-constant schedule for momentum parameters \u03b21 and \u03b22. The schedule proposed by BID18 for \u03b22 in ADAMNC allows recovery of ADAGRAD's squared-gradients sum. ADAMNC without momentum yields ADAGRAD by recovering the sum of squared-gradients. Assumptions and notations involve geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0. Projection operators and maps are defined for feasible parameters on the product manifold. In the context of geodesically complete Riemannian manifolds with lower bounded sectional curvature, a family of geodesically convex functions is considered. The Riemannian AMSGRAD algorithm is presented alongside the standard AMSGRAD algorithm, with convergence guarantees on regret after T rounds. RADAM and ADAM algorithms are also discussed. The convergence guarantee for RAMSGRAD is presented in Theorem 1, with \u03b6 defined as a quantity. When (Mi, \u03c1i) = R for all i, convergence guarantees between RAMSGRAD and AMSGRAD coincide. In the Riemannian case, the curvature dependent quantity becomes equal to 1, recovering the convergence theorem of AMSGRAD. The convergence theorem of AMSGRAD is discussed, along with the speed at which the regret bound worsens with small non-zero curvature. Similar remarks apply to RADAMNC, with convergence guarantee shown in Theorem 2. The convergence proof for RADAGRAD is also mentioned. The role of convexity in regret bounds for convex objectives is crucial. Differentiable functions are convex or geodesically convex based on specific conditions. Regret bounds are typically obtained by bounding the difference between function values using a specific equation. Regret bounds for convex objectives are usually obtained by bounding the difference between function values using a specific equation. In the Riemannian case, this involves bounding each term g t , x t \u2212 x * using the cosine law. For an SGD update, this simplifies to a telescopic summation and requires a well-chosen decreasing schedule for \u03b1. In Riemannian manifolds, this step is generalized using lemma 6 introduced by Alexandrov. The generalized step using lemma 6 by Alexandrov in Riemannian manifolds allows for bounding the curvature dependent quantity \u03b6 to bound \u03c1. The bounds improve for sparse gradients, where only a few words are updated at a time on a manifold. The convergence theorems do not require specifying \u03d5 i, indicating adaptivity in the choice of \u03d5 i. The regret bounds could be enhanced by utilizing momentum/acceleration in the proofs for a specific \u03d5 i, as shown in lemma 3. Empirical assessment of RADAM, RAMSGRAD, and RADAGRAD algorithms compared to non-adaptive RSGD method is conducted by embedding the WordNet noun hierarchy in the n-dimensional Poincar\u00e9 model of hyperbolic geometry. Each word is embedded in a space of constant curvature \u22121. The Poincar\u00e9 model is chosen for its closed form expressions in algorithms. Riemannian gradients are rescaled Euclidean gradients. Various mathematical operations like exponential and logarithmic maps are utilized. Parallel transport along geodesics is also discussed. The transitive closure of the WordNet taxonomy graph consists of 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in Dn to minimize distances between connected words. Loss function similar to log-likelihood is minimized using negative word pairs sampling. The loss function used in the study focuses on word pairs in a graph, with a fixed number of pairs. The direction of edges is not considered in the loss calculation. Two settings are used for evaluation: reconstruction and link prediction. Link prediction involves sampling a validation set from transitive closure edges. The study specifically looks at 5-dimensional hyperbolic spaces for training. The study focuses on training in 5-dimensional hyperbolic spaces. Training details include a \"burn-in phase\" for 20 epochs with a fixed learning rate of 0.03 using RSGD with retraction. Negative words are sampled based on graph degree during this phase, improving metrics. Optimization methods favor RADAM over RAMS-GRAD, showing convergence to lower loss values. Results show that replacing the true exponential map with its first-order approximation leads to lower loss values in both RSGD and adaptive methods. Retraction-based methods are reported separately as they require fewer steps and smaller gradients to escape sub-optimal points on the ball border. Experiments with different learning rates {0.001, 0.003, 0.01, 0.03, 0.1} are shown in FIG2 for exponential and retraction-based methods. In experiments with different learning rates, the best setting for RSGD is shown in orange, with slower convergence in blue and faster overfitting in green. For RADAM and RAMSGRAD, only the best settings are shown. RADAM consistently achieves the lowest training loss and outperforms all other methods on the MAP metric for both reconstruction and link prediction settings. In the \"retraction\" setting, RADAM achieves the lowest training loss and is on par with RSGD on the MAP evaluation for both reconstruction and link prediction settings. RAMSGRAD converges faster in terms of MAP for the link prediction task, indicating better generalization capability. Various first-order Riemannian methods have emerged after the introduction of Riemannian SGD. New methods for convergence analysis in the geodesically convex case were introduced, including Riemannian accelerated gradient descent and averaged RSGD. Stochastic gradient Langevin dynamics was generalized to optimize on the probability simplex. Riemannian counterparts of SGD with momentum and RMSprop were proposed, but without convergence guarantees. Their algorithm compromises the possibility by performing coordinate-wise adaptive operations in the tangent space. The previous methods introduced new convergence analysis techniques in the geodesically convex case. In contrast, the proposed algorithm generalizes adaptive optimization tools for non-Euclidean embeddings without discussing adaptivity across manifolds or providing convergence analysis. In this study, the authors propose a method to generalize adaptive optimization tools for Cartesian products of Riemannian manifolds. They show that their approach outperforms non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. The convergence rates are similar to Euclidean models, and they use various inequalities to derive their results. The authors propose a method to generalize adaptive optimization tools for Cartesian products of Riemannian manifolds, outperforming non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. They use various inequalities to derive their results, including Young's inequalities and a user-friendly inequality developed to prove convergence of gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. The text discusses inequalities used to prove convergence of optimization algorithms for geodesically convex functions in Alexandrov spaces. Lemma 6 introduces the cosine inequality, while Lemma 7 presents an analogue of Cauchy-Schwarz inequality. Lemma 8, used in the convergence proof for ADAMNC, deals with non-negative real numbers. Lemma 8 (BID0) states that for non-negative real numbers y1, ..., yt, a certain condition holds."
}