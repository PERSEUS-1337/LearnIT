{
    "title": "HyeJf1HKvS",
    "content": "This work introduces a two-stage neural architecture for learning structural correspondences between graphs. It utilizes localized node embeddings from a graph neural network to rank soft correspondences initially and then employs synchronous message passing networks to iteratively refine the rankings for consensus in local neighborhoods. The message passing scheme computes a reliable measure of consensus for corresponding neighborhoods, guiding the re-ranking process effectively. The architecture is scalable to large, real-world inputs while being purely local and sparsity-aware. The purely local and sparsity-aware architecture effectively establishes structural correspondences between graphs, improving upon the current state-of-the-art in computer vision and entity alignment tasks. Graph matching, essential for many real-world applications, involves establishing meaningful correspondences between nodes in graphs by considering node and edge similarities. Graph matching is crucial for various real-world applications, such as comparing molecules in cheminformatics, matching protein networks in bioinformatics, linking user accounts in social network analysis, and recognizing actions in computer vision. The problem has been extensively studied in theory and practice, often using domain-agnostic distances like graph edit distance. Various neural architectures have been proposed to tackle the task of graph matching, as domain-agnostic distances like graph edit distance may not be tractable for large-scale instances. These approaches do not adapt to data distribution or consider continuous node embeddings, which provide crucial information about node semantics. Graph matching approaches based on neural architectures or graph similarity have limitations such as only computing similarity scores between whole graphs, relying on inefficient global matching procedures, and not generalizing to unseen graphs. These methods may also be prone to matching neighborhoods between graphs. Typically, graph matching is formulated as an edge-preserving, quadratic assignment problem. The assignment problem is formulated based on neighborhood consensus to prevent adjacent nodes in the source graph from being mapped to different regions in the target graph. It involves supervised and semi-supervised matching of graphs using the intuition of neighborhood. Our proposed deep graph matching architecture incorporates supervised and semi-supervised matching of graphs with a focus on neighborhood consensus as an inductive bias. The method involves two stages: local feature matching and a subsequent process. The proposed deep graph matching architecture involves two stages: local feature matching and iterative refinement using synchronous message passing networks. The feature matching step computes initial correspondence scores based on local node embeddings, while the refinement strategy aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The method is scalable to large, real-world inputs. The two-stage neighborhood consensus architecture involves local feature matching and iterative refinement using synchronous message passing networks. Correspondence scores are computed based on node embeddings, and neighborhood consensus is reached for correspondences using a differentiable validator for graph isomorphism. The method is scalable to large, real-world inputs. The method involves obtaining initial soft correspondences between nodes in source and target graphs based on node embeddings. Sinkhorn normalization is applied to ensure constraints are met, and the neural network is trained in a supervised manner against ground truth. The method involves training a Graph Neural Network (GNN) \u03a8 \u03b81 in a supervised manner against ground truth correspondences by minimizing negative log-likelihood. GNN updates node features using a neural message passing scheme, aggregating localized information to obtain node representations. The recent work in geometric deep learning and relational representation learning offers a variety of operators for precise feature control. However, the local nature of node embeddings can lead to false correspondences, violating neighborhood consensus criteria. Detecting these violations locally is necessary due to the NP-hard nature of finding a global optimum. The proposed algorithm aims to detect violations of criteria in local neighborhoods using graph neural networks and iterative refinement of correspondences. The soft correspondence matrix is utilized to pass node functions between domains, enabling the consensus method to work effectively. Our consensus method involves mapping node indicator functions between domains using a shared graph neural network and performing synchronous message passing to measure neighborhood consensus between node pairs. This measure is used to update correspondence scores iteratively. The consensus method involves using an MLP to improve consensus iteratively in node pairs. The objective combines feature matching and neighborhood consensus errors, optimized using stochastic gradient descent. The consensus stage resolves ambiguities and false matchings by distributing global node colorings with purely local operators. The two-stage approach is crucial as neighborhood consensus requires an initial matching. The importance of a two-stage approach is highlighted by two theorems showing the effectiveness of measuring local neighborhood matches between graphs. The theorems provide conditions for isomorphic and non-isomorphic graphs, emphasizing the need for a permutation equivariant GNN with injective functions for successful matching. A GNN that satisfies permutation equivariance and injectivity requirements provides equal node embeddings. Common GNN architectures are equivariant due to permutation invariant neighborhood aggregators. Injectivity can be achieved by using a powerful GNN like the Weisfeiler & Lehman heuristic. The proposed approach in graph structures involves using sum aggregation with MLPs on neighboring node features. It can be related to classical graph matching techniques like the graduated assignment algorithm. The softassign operator is implemented using sinkhorn normalization on rescaled inputs to encourage integer solutions. The approach resembles the approximation of the linear assignment problem via sinkhorn normalization. The gradient Q is related to a neighborhood consensus scheme for a non-trainable GNN instantiation. Correspondence scores are updated using trainable neural networks based on the difference between certain outputs. Our model utilizes trainable neural networks to update correspondence scores based on output differences, supporting continuous node and edge features in graph matching. This approach simplifies computation compared to traditional methods and is scalable to large input domains. The proposed algorithm optimizes initial correspondences by sparsifying them before neighborhood consensus, reducing memory footprint and time complexity. The proposed algorithm optimizes initial correspondences by sparsifying them before neighborhood consensus, reducing memory footprint and time complexity. This involves replacing node indicator functions with randomly drawn node functions to improve computational efficiency. The algorithm optimizes initial correspondences by sparsifying them before neighborhood consensus, replacing node indicator functions with randomly drawn node functions to improve efficiency. Theorem 1 still holds, while Theorem 2 may not hold, but the refinement strategy resolves ambiguities by re-sampling in each iteration. Softmax normalization is used, fulfilling requirements for doubly-stochastic solutions. The algorithm optimizes initial correspondences by sparsifying them before neighborhood consensus. Row-wise softmax normalization is proposed to relax constraints and resolve violations, leading to convergence to the correct solution. Experimentally, it is shown that this approach is sufficient for the algorithm to converge. The algorithm optimizes initial correspondences by sparsifying them before neighborhood consensus, leading to convergence to the correct solution. Varying the number of refinement iterations for training and testing speeds up runtime and encourages convergence with fewer steps. Decreasing the number of iterations during training does not affect convergence abilities during testing. The method is verified on three different tasks, showing benefits in an ablation study on synthetic graphs. Our method is implemented in PYTORCH using the PYTORCH GEOMETRIC and KEOPS libraries for efficient processing of sparse mini-batches with GPU acceleration. Optimization is done via ADAM with a fixed learning rate of 10^-3 for all experiments. In experiments, optimization is done via ADAM with a fixed learning rate of 10^-3. Similar architectures are used for \u03a8 \u03b81 and \u03a8 \u03b82, omitting dropout in \u03a8 \u03b82. Hits@k is reported to evaluate the model on synthetic graphs, aiming to learn a matching for pairs of graphs in a supervised fashion. Each pair consists of an undirected Erd\u0151s & R\u00e9nyi graph G s with varying nodes and edge probability, and a target graph G t constructed from G s by removing edges. In experiments, optimization is done via ADAM with a fixed learning rate of 10^-3. The graph neural network operators \u03a8 \u03b81 and \u03a8 \u03b82 are implemented with three layers of the GIN operator for expressiveness. Training and evaluation are conducted on 1,000 graphs for different edge removal probabilities. Additional experiments in Appendix E test the approach's robustness to node changes. Architecture parameters include two layers and a hidden dimensionality of 32 for all MLPs. The architecture parameters for the graph neural network include two layers with a hidden dimensionality of 32 for all MLPs. ReLU activation is applied after each layer, and input features are initialized with one-hot encodings of node degrees. The final node representations are computed using Jumping Knowledge style concatenation. Training and testing are done with 10 and 20 refinement iterations, respectively. The matching accuracy decreases with increasing structural noise, even with global sinkhorn normalization applied. Our proposed two-stage architecture can recover all correspondences, regardless of structural noise. This applies to both variants discussed, highlighting the benefits of matching consensus and scalability enhancements. The procedure is able to converge even when training does not reach convergence. Our proposed two-stage architecture can recover all  correspondences, even when training does not converge. Increasing the number of iterations during testing allows our procedure to converge. The refinement strategy performs well on sparsified top k correspondences, recovering correct matches with increasing k. This scalability enhancement makes it an excellent option for large graphs. The proposed two-stage architecture can recover all correspondences, even when training does not converge. The scalability enhancement makes it an excellent option for large graphs, as demonstrated in experiments on the PASCALVOC and WILLOW-OBJECTCLASS datasets. The PASCALVOC dataset is pre-filtered to exclude difficult, occluded, and truncated objects, resulting in 6,953 and 1,671 annotated images for training and testing, respectively. The PASCALVOC dataset has 6,953 training and 1,671 testing images with varying instances and keypoints. The WILLOW-OBJECTCLASS dataset has consistent orientations with 40 images per category and 10 keypoints per image. The model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 images per class. Graphs are constructed using Delaunay triangulation of keypoints for fair comparison with previous methods. The input features for keypoints are obtained from pre-trained VGG16 on IMAGENET. The architecture includes SPLINECNN as the graph neural network operator with trainable B-spline based kernel function. Results are evaluated for both isotropic and anisotropic cases. The input features for keypoints are obtained from pre-trained VGG16 on IMAGENET. The architecture includes SPLINECNN as the graph neural network operator with trainable B-spline based kernel function. Results are evaluated for both isotropic and anisotropic cases. The edge features consist of normalized relative distances and 2D Cartesian coordinates. For SPLINECNN, a kernel size of 5 in each dimension and a hidden dimensionality are used. Our SPLINECNN architecture uses a kernel size of 5 in each dimension, a hidden dimensionality of 256, and ReLU as the non-linearity function. The network consists of two convolutional layers, dropout with probability 0.5, and a final linear layer. Training involves forming pairs between training examples of the same category and evaluating the model with test graph pairs. The model is trained using negative log-likelihood for superior performance compared to displacement loss. Our SPLINECNN architecture utilizes a kernel size of 5 in each dimension, a hidden dimensionality of 256, and ReLU as the non-linearity function. It consists of two convolutional layers, dropout with probability 0.5, and a final linear layer. Training involves forming pairs between training examples of the same category and evaluating the model with test graph pairs using negative log-likelihood. In Zanfir & Sminchisescu (2018), the complete architecture is evaluated using isotropic and anisotropic GNNs for L \u2208 {0, 10, 20}, with ablation results from using \u03a8 \u03b81 = MLP for local node matching. Hits@1 results for PASCALVOC and WILLOW-OBJECTCLASS are shown in Table 1 and 2, respectively. The refinement strategy outperforms competing methods and non-refined baselines, reducing error by half on the WILLOW-OBJECTCLASS dataset. The second stage of our approach shows significant improvements, especially when starting from a weaker initial feature matching baseline. Task-specific isotropic or anisotropic GNNs are used for further enhancement. Our model is validated on the geometric feature matching problem without additional visual features, demonstrating generalization capabilities on the PASCALPF dataset. Our model is trained on a synthetic set of graph pairs with added noise and outliers, using an anisotropic keypoint architecture. The evaluation is done on the PASCALPF dataset, consisting of 1,351 examples. Our model, trained on synthetic graph pairs with added noise and outliers, is evaluated on the PASCALPF dataset with 1,351 image pairs across 20 classes. Results show improvement over the state-of-the-art, especially with our consensus architecture. Additionally, our method performs well even without visual information. The curr_chunk discusses linking entities from different knowledge graphs and obtaining entity input features using monolingual FASTTEXT embeddings. The text also mentions using graph neural architecture and parameters for the task. The text discusses using a graph neural network operator with specific architecture and parameters for linking entities from different knowledge graphs. Training is done in a semi-supervised fashion using negative log likelihood, with a three-layer GNN and dimensionality settings. In the refinement phase, the sparse top k correspondence matrix is updated multiple times for efficiency. Results show that the proposed model outperforms previous methods, with improvements of up to 9.38 percentage points across all categories. The proposed refinement strategy significantly improves Hits@1 of initial correspondences and achieves gains of up to 9.38 percentage points across all categories. The scalability of the approach allows for multiple refinement iterations while maintaining large hidden feature dimensionalities, effectively solving challenging real-world problems. The expressive power of GNNs is closely linked to the WL heuristic for graph isomorphism testing. The limitations of our method are inherited from the WL heuristic for graph isomorphism testing. One possible limitation is the failure to converge when two nodes are assigned the same color by WL, resulting in non-convergence due to equal neighborhood sets. Adding a small amount of noise may help resolve these ambiguities. In theory, adding noise may resolve ambiguities in graph isomorphism testing. Various related problems include maximum common subgraph, network alignment, graph edit distance, and graph matching. Graph neural networks have become a focus of research recently. Graph neural networks have become a focus of research, leading to proposed deep graph matching techniques. A two-stage neural architecture was presented for learning node correspondences between graphs in a supervised or semi-supervised fashion, aiming to reach a neighborhood consensus between matchings. The proposed architecture for graph neural networks aims to improve upon the state-of-the-art by scaling to large input domains and consistently enhancing performance on real-world datasets. The final optimized algorithm is detailed in Algorithm 1, showcasing the permutation equivariance of the model and its ability to map T-hop neighborhoods around nodes to the same vectorial representation. GNN \u03a8 \u03b82 maps T-hop neighborhoods around nodes to the same vectorial representation, distinguishing graph structures with the power to distinguish any graph structure from j. Isomorphism P \u2208 {0, 1} with identity matrix I |Vs| and permutation matrix SNT(i),NT(j) = P. If di,argmax Si,: = 0 for all i \u2208 Vs, S holds submatrices describing graph structures. The algorithm described in Section 3.3 is an extension of the graduated assignment algorithm (Gold & Rangarajan, 1996) with trainable parameters, allowing for a trainable refinement procedure. The experiments were replicated to evaluate the impact of this refinement procedure. The experiments replicated to evaluate the impact of a trainable refinement procedure showed that using trainable neural networks consistently improves results compared to fixed-function message passing schemes. This approach can learn to utilize node and edge features effectively, guiding the refinement procedure further and offering a variety of task-dependent GNN operators. To experimentally validate the robustness of our approach towards node addition or removal, we conducted synthetic experiments using Erd\u0151s & R\u00e9nyi graphs with varying numbers of nodes and edge probabilities. The target graph is constructed by adding noisy nodes to the source graph and generating edges between them. Our consensus stage is robust to node addition or removal, with the neural architecture detecting and decreasing false positive influence of unmatched nodes. The problem of identifying correspondences between nodes in graphs has been extensively studied in various domains. The combinatorial maximum common subgraph isomorphism problem, which seeks the largest common subgraph in two given graphs, is NP-hard and difficult to approximate. The consensus stage in our approach can detect and reduce false positive influence of unmatched nodes. In cheminformatics, exact polynomial-time algorithms are available for specific problem variants only. In bioinformatics and computer vision, network alignment or graph matching techniques are developed for large networks without specific structural properties. Graph matching involves minimizing a function for two graphs of order n with adjacency matrices A s and A t. In graph matching, the goal is to minimize a function for two graphs of order n with adjacency matrices A s and A t. The function to be minimized involves permutation matrices and squared Frobenius norm. Previous research has focused on minimizing this function using Frank-Wolfe type algorithms and projecting the solution to a set of permutation matrices. The applicability of relaxation and projection in graph matching is still poorly understood, with few theoretical results existing. The WL heuristic distinguishes two graphs if there is no fractional solution that satisfies the objective function. The Frank-Wolfe algorithm can be modified to obtain the WL partition, and the standard relaxation yields a correct solution for a specific class of asymmetric graphs. The standard relaxation provides a correct solution for asymmetric graphs characterized by spectral properties of their adjacency matrix. Various approaches to graph matching exist, including spectral relaxations and random walks. Graph matching is closely related to the quadratic assignment problem (QAP). The recent literature on graph matching considers a weighted version, leading to Lawler's QAP formulation. Zhou & De la Torre (2016) proposed factorizing the affinity matrix and incorporating global geometric constraints. Zhang et al. (2019c) studied kernelized graph matching, expressing the problem as Koopmans-Beckmann's QAP in a Hilbert space. Swoboda et al. (2017) explored Lagrangean decompositions of the graph matching problem using dual ascent algorithms. Functional representation for graph matching has been proposed to avoid constructing the affinity matrix, leading to state-of-the-art performance in graph matching tasks. Graph edit distance is a related concept studied in computer vision. The graph edit distance measures the minimum cost to transform one graph into another by adding, deleting, and substituting vertices and edges. It is NP-hard and related to the maximum common subgraph problem and the quadratic assignment problem. Recent exact algorithms have been proposed but are limited to small graphs. The problem of network alignment is defined similarly to the graph edit distance, with algorithms using heuristics based on the assignment problem to reduce running time. Linear time can be achieved for restricted cost functions. The problem of network alignment involves computing a similarity matrix from graph topology and solving the assignment problem to align graphs efficiently. ISORANK, proposed by Singh et al. (2008), uses the adjacency matrix of the product graph and PageRank algorithm. Kollias et al. (2012) suggested an approximation of ISORANK using decomposition techniques to avoid generating the product graph. The techniques mentioned aim to find optimal correspondences between vertices in network alignment, using various approaches like linearizing optimization problems and message passing algorithms. In practical applications, learning node and edge similarity functions for specific tasks, such as graph edit distance, has been proposed. Different approaches, including deep graph matching procedures, have been explored to refine local feature matchings and enforce neighborhood constraints. Refining local feature matchings and enforcing neighborhood consistency in graph matching has been a relevant topic for years. Deep graph matching approaches have been heavily investigated recently, with models developed by Zanfir & Sminchisescu (2018), Wang et al. (2019b), and Zhang & Lee (2019) using supervised networks based on displacement and combinatorial objectives. Zanfir & Sminchisescu (2018) also model graph matching affinity through a differentiable spectral solver. Our matching procedure is fully-learnable, unlike previous approaches that use node-wise features and dense node-to-node cross-graph affinities. While other methods involve mapping point coordinates into a high-dimensional space for matching, they do not naturally resolve inconsistent neighborhood assignments like we do. Our approach resolves inconsistent neighborhood assignments in graph matching by enhancing the optimal transport objective with node embeddings. It is fully-learnable and works in a supervised fashion for sets of graphs, allowing for generalization to unseen instances. The task of network alignment has been explored using various methods such as leveraging CYCLEGANs to align NODE2VEC embeddings, designing deep graph models for network topology preservation, utilizing local matching procedures based on node embedding similarity, and using shared graph neural networks to approximate graph edit distance. The concept of enhancing intra-graph node embeddings by inter-graph node embeddings has been heavily investigated in practice. Bai et al. (2018) proposed a method to order the correspondence matrix in a breadth-first-search fashion and process it with traditional CNNs. Wang et al. (2019b) enhanced the GNN operator by incorporating inter-graph node embeddings. Incorporating inter-graph node embeddings enhances the GNN operator, as shown by Wang et al. (2019b) and other researchers. Xu et al. (2019d) utilize alternating GNNs to propagate local features between graphs, while Wang & Solomon (2019) address rigid motion between point clouds using a point cloud matching approach with a differentiable SVD module. Inner product similarity scores are used for feature matching after passing node embeddings through a Transformer module. Methods for obtaining consistency in local neighborhoods for image matching have a long history in computer vision. These techniques significantly improve the results of local feature matching procedures. Recently, a deep neural network for neighborhood consensus using 4D convolution was proposed, but it cannot be efficiently transferred to the graph domain directly. Our algorithm infers errors for the product graph but performs computations on the original graphs, improving results of local feature matching procedures. The algorithm defines continuous maps between function spaces on manifolds and is commonly used for solving tasks."
}