{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, including humans, despite individual incentives conflicting with the common good. Understanding cooperative behavior in self-interested individuals is significant in multi-agent reinforcement learning and evolutionary theory. This study focuses on intertemporal social dilemmas, where the conflict between individual and group interests is pronounced. Through a combination of MARL and natural selection, the research demonstrates ways to address these dilemmas. By combining MARL with structured natural selection, individual biases for cooperation can be learned in a model-free way. An innovative modular architecture for deep reinforcement learning agents supports multi-level selection. Results in challenging environments are interpreted in the context of cultural and ecological evolution, showcasing cooperation in nature despite selfish interests. Altruism can be favored by selection when cooperating individuals interact with other cooperators, avoiding exploitation by defectors. Various possibilities exist, including kin selection, reciprocity, and group selection. Cooperation among self-interested agents is a key topic in multi-agent deep reinforcement learning, formalized as an intertemporal social dilemma. In the context of multi-agent deep reinforcement learning, the intertemporal social dilemma (ISD) extends social dilemmas to Markov settings. These dilemmas involve a trade-off between collective welfare and individual utility, where self-interested agents tend to converge to defecting strategies. The goal is to find training regimes that promote cooperation among individuals, with solutions falling into opponent modeling, long-term planning, and specific strategies. Evolution can be applied to remove hand-crafted approaches in deep reinforcement learning, including optimizing hyperparameters, black-box optimization, and evolving neuroarchitectures, regularization, loss functions, and behavioral diversity. Evolution can be applied to remove hand-crafted approaches in deep reinforcement learning, including optimizing hyperparameters, black-box optimization, and evolving neuroarchitectures, regularization, loss functions, and behavioral diversity. The principles mentioned are driven by single-agent search and optimization or competitive multi-agent tasks, with no guarantee of success in the ISD setting. Evolutionary simulations of predator-prey dynamics have used enforced subpopulations to evolve populations of neurons for neural networks. The proposed system distinguishes between optimization processes unfolding over two distinct time-scales in addressing the challenges of ISDs. Evolution can help mitigate intertemporal social dilemmas by bridging the fast time-scale of learning with the slow time-scale of evolution through intrinsic motivation modeled as an additional term in agent rewards using a neural network genotype. Evolutionary theory suggests that evolving intrinsic reward weights across a population does not lead to altruistic behavior. To address this, an evolutionary dynamics strategy called \"Greenbeard\" is implemented, where agents choose partners based on cooperativeness signals. This process, known as assortative matchmaking, is not a universal solution for cooperation in all species. The curr_chunk discusses the limitations of assortative matchmaking in explaining cooperation in all taxa and introduces a new modular training scheme called shared reward network evolution for multi-agent reinforcement learning. This approach involves training agents with two neural network modules: a policy network and a reward network. In multi-agent reinforcement learning, the policy network is trained using modified rewards from the reward network. The two modules evolve separately on a slow timescale, with each agent having a distinct policy network but sharing the same reward network. The fitness for the policy network is individual reward, while the fitness for the reward network is the collective return for the group. This modular approach prevents evolved reward networks from being influenced by individual agents. In multi-agent reinforcement learning, evolving policy and reward networks separately prevents overfitting to specific policies. Different parameters were explored, including environments, reward network features, matchmaking, and evolution methods. The study focuses on Markov games in a MARL setting, specifically addressing intertemporal social dilemmas. In a MARL setting, intertemporal social dilemmas are studied, where selfish actions benefit individuals in the short term but harm the group in the long term. Two dilemmas are considered in partially observable Markov games on a 2D grid. In the Cleanup game, agents collect apples based on field cleanliness. In the Harvest game, agents collect rewarding apples with spawn rate affected by aquifer cleanliness. Agents must balance collecting apples and cleaning to prevent waste buildup. In the Harvest game, agents collect apples with spawn rates based on nearby apples, leading to a dilemma between short-term harvesting and long-term depletion. The reward components in the model include total, extrinsic, and intrinsic rewards for player i. The reward components in the model for player i include extrinsic reward obtained from environment actions and an intrinsic reward calculated based on social preferences using a neural network with evolved parameters. The feature vector f i is a player-specific quantity that can be transformed into intrinsic reward via a reward network. Each agent has access to the same set of features, with the exception of its own demarcated feature. Features are based on received or expected future rewards and may not align in time in Markov games. In Markov games, rewards for different players may not align in time, so social preference models should focus on comparing temporally averaged reward estimates rather than instantaneous values. Two methods of aggregating rewards were considered. Agent A adjusts policy using off-policy importance weighted actor-critic. The architecture includes intrinsic and extrinsic value heads. The architecture for social preference models in Markov games includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. Two methods of deriving intrinsic reward are discussed: retrospective method based on past rewards and prospective method based on future rewards. The retrospective variant updates temporally decayed rewards at each timestep, while the prospective variant uses value estimates with a stop-gradient before the reward. The variant uses value estimates for f ij and has a stop-gradient before the reward network module to prevent gradients from flowing back into other agents. Training involved a population of 50 agents with policies {\u03c0 i}, sampled 5 players for each of 500 arenas running in parallel. Episode trajectories lasted 1000 steps. The training process involved a population of 50 agents with policies, sampled 5 players for each of 500 arenas running in parallel. Episode trajectories lasted 1000 steps and weights were updated using V-Trace. The set of weights evolved included learning rate, entropy cost weight, and reward network weights. Agents were allowed to observe their last actions, intrinsic rewards, and extrinsic rewards as input to the LSTM in the agent's neural network. The objective function comprised three components. The function presented in BID10 had three components: value function gradient, policy gradient, and entropy regularization, weighted by hyperparameters baseline cost and entropy cost. Evolution was based on a fitness measure calculated as a moving average of total episode return, determined by random or assortative matchmaking methods. The study used two methods for matchmaking in the game: random matchmaking and assortative matchmaking. Random matchmaking selected agents uniformly at random, while assortative matchmaking grouped agents based on recent cooperativeness. This ensured that highly cooperative agents played with each other, while defecting agents played with other defectors. Cooperativeness was calculated based on the agent's actions in the game. The study used cooperative metric-based matchmaking for Harvest, calculating cooperativeness based on agent's return compared to mean return of all players. This matchmaking was not used for multi-level selection model. Reward network was separately evolved within its own population to allow competition between like components. The study utilized cooperative metric-based matchmaking for Harvest, evolving reward networks separately to allow competition between like components and independent exploration of hyperparameters. Five policy networks were paired with a shared reward network in each episode, promoting generalization to a wide range of policies. The evolution of optimization-related hyperparameters was based on individual agent return, while the reward network parameters were evolved according to fitness based on total episode return across the group of co-players. This approach differs from previous work by focusing on social features rather than remapping environmental events, and the evolution of the reward network is aimed at addressing the tension in Inter-Subjective Dynamics (ISDs) for social cooperation. It is akin to evolving a form of communication for social cooperation rather than simply shaping rewards. In a social setting, shared reward networks play a critical role in evolving communication for social cooperation. Unlike hand-crafted aggregation methods, this biologically principled approach balances group fitness and individual rewards. Without an intrinsic reward network, performance is poor in games like Cleanup and Harvest. In experiments comparing random and assortative matchmaking with PBT and reward networks using retrospective social features, individual reward network agents perform similarly to PBT on Cleanup and slightly better on Harvest. Adding reward networks over social features has little benefit if players have separate networks evolved selfishly. Assortative matchmaking experiments show that without a reward network, performance is the same as the PBT baseline, but with individual reward networks, performance improves significantly. The performance of agents with shared reward networks was as good as assortative matchmaking and handcrafted intrinsic rewards, even with random matchmaking. This suggests that agents did not require immediate access to honest signals of cooperativeness to resolve the dilemma. The retrospective variant of reward network evolution outperforms the prospective variant, as it does not rely on agents learning value estimates before the reward networks become useful. Social outcome metrics are plotted to capture the complexities of agent behavior. The retrospective variant of reward network evolution outperforms the prospective variant in capturing the complexities of agent behavior. Sustainability is measured by the average time step agents receive positive rewards, showing more sustainable behavior with reward networks. Equality is calculated using the Gini coefficient, with the prospective variant leading to lower equality and the retrospective variant having high equality. Tagging measures the average number of times a player fined another player. The study compares the performance of different reward networks in capturing agent behavior complexities. Tagging frequency is higher with prospective or individual reward networks compared to a shared reward network. The weights of the final shared reward networks suggest different social preferences are needed for resolving each game. In Cleanup, the final layer weights evolved to be close to 0, while in Harvest they evolved to be of large magnitude but opposite sign. The biases showed a similar pattern. Cleanup required a less complex reward network, while Harvest needed a more complex one to prevent over-exploitation of resources. The first layer weights tended to take on arbitrary positive values due to random matchmaking. Intrinsic rewards based on features from other agents were examined, showing that natural selection via genetic algorithms did not lead to cooperation. Assortative matchmaking could generate cooperative behavior when honest signals were present. A new multi-level evolutionary paradigm based on shared goals was proposed. Evolutionary paradigm based on shared reward networks promotes cooperation by improving credit assignment and exposing social signals correlated with selfishness. The shared reward network evolution model promotes cooperation by utilizing mechanisms like competitive altruism, other-regarding preferences, and inequity aversion. Humans cooperate more readily when they can communicate, as shown in laboratory experiments. This model is inspired by multi-level selection but differs in that lower level units constantly swap with higher level units. This form of modularity is observed in nature. In nature, modularity can be seen in various ways. For instance, microorganisms form multi-cellular structures to solve adaptive problems, while prokaryotes can incorporate modules from their environment for cooperation. In humans, a reward network may represent a shared cultural norm based on accumulated cultural information. This allows the spread of norms independently. The spread of norms can occur independently of individual agents' success. Future work could explore alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity. Investigating different weights in a reward network may reveal the evolutionary origins of social biases. Studying an emergent assortative matchmaking model could add generality to the setup. Combining an evolutionary approach with these mechanisms could provide further insights. The text discusses the setup for a game where agents engage in cooperative behaviors through communication and punishment. The game involves episodes lasting 1000 steps in a playable area of specific sizes. Agents can only observe through a 15x15 RGB window and have actions like moving, rotating, and tagging each other with a reward cost. This setup allows for the possibility of punishing free-riders. The Cleanup game involves cleaning waste and punishing free-riders through cooperative behaviors. Training includes joint optimization of network parameters via SGD and hyperparameters/reward network parameters via evolution in the standard PBT setup. Gradient updates are applied for every trajectory up to a maximum length of 100 steps, using a batch size of 32. Optimization is done via RMSProp with specific parameters, and the learning rates are initially set to 4 \u00d7 10 \u22124. The Cleanup game involves joint optimization of network parameters through SGD and hyperparameters/reward network parameters through evolution using PBT. Evolution involves genetic algorithms to search over hyperparameter space, resulting in an adaptive schedule. Mutation rates and perturbations are used to evolve hyperparameters. A burn-in period of 4 \u00d7 10 6 agent steps is implemented before evolving network parameters. A burn-in period of 4 \u00d7 10 6 agent steps is implemented for evolution to assess fitness accurately before evolving network parameters."
}