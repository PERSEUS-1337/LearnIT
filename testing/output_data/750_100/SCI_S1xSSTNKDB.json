{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this bias, a new dataset with 108,501 images balanced on race was created, including White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino groups. Images were collected from YFCC-100M Flickr dataset and labeled with race, gender, and age. Evaluation was done on existing face attribute datasets. The study created a new dataset with 108,501 images balanced on race, gender, and age. Evaluations showed that the model trained on this dataset was more accurate on novel datasets and consistent across race and gender groups. Comparison with commercial computer vision APIs was also conducted. Automated face detection, alignment, recognition, generation, modification, and attribute classification systems have been proposed and applied in various fields such as security, medicine, education, and social sciences. These systems have shown success in research and development. Existing public face datasets are biased towards Caucasian faces, with lighter skin faces making up around 80% of large-scale databases. This bias can lead to inaccurate results for other racial groups and raises ethical concerns about fairness in automated systems. Automated systems in machine learning and AI face ethical concerns regarding fairness, with studies showing biases in commercial computer vision systems towards male and light faces. Biases in training data can lead to asymmetric accuracy across sub-demographics. Unwanted biases in image datasets can occur due to biased selection, capture, and negative sets. To address race bias in image datasets, a new face dataset with balanced race composition is proposed, containing 108,501 facial images from various sources. Seven race groups are defined: White, Black, Indian, etc. The dataset aims to mitigate biases in existing datasets collected primarily from platforms showing White people. Our dataset includes 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. It outperforms existing datasets on novel data, showing better generalization across racial groups. This dataset is the first large-scale face attribute dataset to include Latino and Middle Eastern individuals. The face attribute dataset includes Latino and Middle Eastern individuals, expanding the applicability of computer vision methods to analyze different demographics. The goal is to classify human attributes like gender, race, age, emotions, and expressions from facial appearance. Face attribute datasets, including our new dataset, are predominantly White and sourced online. These datasets are used for tasks like face verification and person re-identification. It is crucial for these systems to perform equally well across different gender and race groups to maintain reputations. The racial bias incidents in machine learning and computer vision research have led to the termination of services or features, such as dropping sensitive output categories. Most commercial providers have stopped offering a race classifier due to these incidents. Face attribute recognition is also impacted by these issues. Service providers have ceased offering a race classifier and face attribute recognition is affected by racial bias incidents. Social scientists use image analysis to infer demographic attributes and study behaviors, with the cost of unfair classification being significant. In the context of algorithmic fairness and dataset biases, research focuses on balanced accuracy to ensure attribute classification is independent of race and gender, aiming for fair outcomes such as loan approval. Algorithmic fairness is about producing fair outcomes regardless of protected attributes like race or gender. Research in this area involves auditing bias in datasets, improving datasets, and designing better algorithms. The paper discussed focuses on gender classification from facial images. The paper aims to address biases in gender classification from facial images, particularly in systems that are inaccurate for dark-skinned females due to biased datasets and associations between scene and race in images. The contribution is to mitigate, not entirely solve, these limitations and biases. The paper aims to address biases in gender classification from facial images by collecting more diverse face images from non-White race groups, improving generalization performance to novel datasets. The dataset includes Southeast Asian and Middle Eastern races, arguing that not having these major race groups in datasets is a form of discrimination. The dataset defines 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is based on physical traits while ethnicity is based on cultural similarities. The paper argues that not including Southeast Asian and Middle Eastern races in datasets is a form of discrimination. The paper defines 7 race groups based on physical traits: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. The study questions whether dataset bias should be measured based on skin color or race. Skin color is used as a proxy for racial or ethnic grouping, but it has limitations due to variations in lighting conditions and the one-dimensional nature of skin color compared to the multidimensional concept of race. The skin color is one dimensional and has limitations in differentiating race groups. Race is explicitly used and annotated by human annotators, complemented by skin color measured by ITA. Face datasets often source from public figures like politicians or celebrities. The selection bias in face datasets sourced from public figures like politicians or celebrities can lead to older or more attractive individuals being overrepresented. Some datasets collected via web search may prioritize stereotypical faces or celebrities, rather than diverse individuals from the general public. The goal is to minimize this bias. The dataset was created by detecting faces from a public image dataset without preselection to minimize selection bias and maximize diversity. It is smaller but more balanced on race compared to other datasets. The dataset size was incrementally increased by detecting and annotating 7,125 faces randomly sampled from the entire dataset. The dataset was created by randomly sampling faces from the YFCC100M dataset without considering image locations. Demographic compositions of each country were estimated, and the number of images for each country was adjusted to avoid dominance by the White race. Faces from the U.S. and European countries were excluded after sampling enough White faces. The minimum face size detected was set to 50 by 50 pixels to ensure recognizability of attributes. We used images with \"Attribution\" and \"Share Alike\" Creative Commons licenses for annotation of race, gender, and age group using Amazon Mechanical Turk. Three workers were assigned per image, with ground-truth values determined by agreement among workers. Annotations were refined through training. The annotations were refined by training a model from initial ground truth annotations and manually re-verifying them. The race composition of datasets was measured, with most datasets biased towards the White race. Race labels were annotated for 3,000 random samples from datasets without race annotations. The study found that most datasets, unlike race, are relatively balanced on gender. An identical model architecture was used to train from each dataset, with face detection done using dlib's CNN-based face detector. The experiment was conducted in PyTorch and compared with three other datasets: UTKFace, LFWA+, and CelebA. UTKFace, LFWA+, and CelebA datasets were used for comparison. UTKFace and LFWA+ have race annotations, while CelebA only has gender information. FairFace defines 7 race categories but only 4 were used for comparison. Cross-dataset classifications were performed using models trained from these datasets. FairFace is the only dataset with race annotations. The study compared classification results for race, gender, and age across different datasets using models trained on UTKFace, LFWA+, and CelebA datasets. FairFace, with 7 races, was merged with other datasets for compatibility. Results showed that models performed best on the dataset they were trained on, with highest accuracy on some variables in the LFWA+ dataset due to its bias and the diversity of the model. The study compared classification results for race, gender, and age across different datasets using models trained on UTKFace, LFWA+, and CelebA datasets. FairFace, with 7 races, was merged with other datasets for compatibility. Results showed that models performed best on the dataset they were trained on, with highest accuracy on some variables in the LFWA+ dataset due to its bias and the diversity of the model. This is partly because LFWA+ is the most biased dataset and ours is the most diverse, and thus more generalizable dataset. To test the generalization performance of the models, we consider three novel datasets collected from different sources than our data from Flickr. We chose test datasets that contain people in different locations, such as geo-tagged Tweets from four countries (France, Iraq, Philippines). The study collected data from various sources including Twitter, professional media outlets, and public image datasets. They randomly sampled faces from four countries and media photographs from known media accounts. The dataset also included a protest dataset. The authors collected data from various sources including Twitter, media outlets, and public image datasets. They sampled faces from different countries and media photographs. A protest dataset was also included, with 8,000 faces randomly sampled and annotated for gender, race, and age. Gender classification accuracy and model performance are presented in tables. The FairFace model outperforms other models in classification accuracy for race, gender, and age on novel datasets. Even with fewer training images, FairFace performs better than larger datasets like CelebA, indicating dataset size is not the sole factor for performance. The FairFace model shows improved performance in classification accuracy for race, gender, and age compared to other datasets. It also demonstrates more consistent results across different race groups, indicating that dataset size is not the only factor influencing performance. The model's fairness is measured by standard deviations of classification accuracy on various sub-populations. The FairFace model demonstrates improved classification accuracy for race, gender, and age compared to other datasets. It shows more consistent results across different race groups, indicating that dataset size is not the only factor influencing performance. The model's fairness is measured by standard deviations of classification accuracy on various sub-populations. Gender classification accuracy of different models on external validation datasets for each race and gender group is shown in Table 4. The FairFace model achieves the lowest maximum accuracy disparity, while the LFWA+ model exhibits the highest disparity biased toward the male category. The CelebA model tends to show bias toward the female category due to the dataset containing more female images. The FairFace model achieves less than 1% accuracy discrepancy between male and female categories, as well as White and non-White groups for gender classification. Other models exhibit a strong bias towards males, with lower accuracy on females and non-White individuals. The gender performance gap is most pronounced in the LFWA+ dataset, indicating unbalanced representation in training data may contribute to asymmetric gender biases in computer vision services. The dataset characteristics were investigated to measure data diversity, revealing unbalanced representation in training data as a likely cause. Faces in FairFace are well spread in 2D space, with race groups loosely separated, possibly due to the dominance of White faces in the training data. The dataset characteristics were investigated to measure data diversity, revealing unbalanced representation in training data as a likely cause. The embedding was trained from biased datasets, suggesting many non-typical examples. LFWA+ and UTKFace focus on local clusters, while FairFace has well-spread race groups. Pairwise distance distributions were examined to measure face diversity in the datasets. The CDF functions of 3 datasets show varying levels of face clustering. UTKFace has tightly clustered faces, while LFWA+ exhibits diverse faces despite a majority being white. This diversity is attributed to the training data, not actual appearance diversity. The study compared face classification accuracies across demographic groups using FairFace images and various online APIs. The dataset was diverse in race, age, expressions, head orientation, and photographic conditions, making it a better benchmark for bias measurement. 7,476 random samples from FairFace were used for the analysis. The study analyzed face classification accuracies using FairFace images and online APIs. 7,476 random samples were used, ensuring diversity in race, gender, and age groups. Children under 20 were excluded due to ambiguity in gender determination. Experiments were conducted in August 2019. The gender classification accuracies of tested APIs are shown in Table 6. These APIs detect faces in input images and classify gender, with Amazon Rekognition detecting all faces. Detection rates are reported in Table 8 in the Appendix. The detection rate is reported in Table 8 in the Appendix. Two sets of accuracies are presented, one including mis-detections as mis-classifications and the other excluding them. A model trained with their dataset serves as an upper bound for classification accuracy. Gender classifiers still favor males, with dark-skinned females showing higher error rates, although there are exceptions. This paper highlights that skin color alone is not a reliable indicator of model bias, as some APIs classified Indians more accurately than Whites despite darker skin tones. Gender bias in face detection was also noted, with Microsoft's model failing to detect many male faces. The paper proposes a new face image dataset balanced on race, gender, and age, achieving better results than existing datasets. Our dataset, derived from the Yahoo YFCC100m dataset, achieves better generalization performance for gender, race, and age on novel image datasets from Twitter, online newspapers, and web search. It produces balanced accuracy across race groups, unlike other datasets that lead to asymmetric accuracy. The dataset allows for both academic and commercial usage in training new models. The novel dataset proposed in this paper aims to address race and gender bias in computer vision systems, promoting algorithmic fairness. It can be used for training new models and verifying balanced accuracy, contributing to the transparency and acceptance of AI systems in society. The novel dataset aims to address race and gender bias in computer vision systems, promoting algorithmic fairness and transparency in AI systems. APPENDIX Figure 5 shows the distribution of different races based on skin color in the dataset."
}