{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on scenarios where source and target domains may not share the same categories. This paper introduces a method that combines Self-Ensembling with category-agnostic clusters in the target domain to address the challenge of aligning samples from unknown classes. The novel architecture SE-CC combines Self-Ensembling with category-agnostic clusters in the target domain to facilitate domain adaptation. Clustering is used to reveal the underlying data space structure specific to the target domain, aiding in generalization for closed-set and open-set scenarios. SE-CC utilizes clustering to preserve underlying structure in the learnt representation and enhances it with mutual information maximization. Superior results are achieved in open-set and closed-set domain adaptation on Office and VisDA datasets compared to state-of-the-art approaches. Unsupervised domain adaptation leverages labeled source samples and unlabeled target samples to generalize a target model, aiming to alleviate the performance drop on a new domain known as \"domain shift.\" Existing models often align data distributions between source and target domains, but a critical limitation exists in this approach. Existing models in unsupervised domain adaptation align data distributions between source and target domains, limiting their applicability to closed-set scenarios. This hinders generalization in open-set scenarios where distinguishing unknown target samples from known ones is challenging. The difficulty lies in classifying known target samples correctly while learning a hybrid network for both scenarios. One way to address the challenge of classifying known target samples correctly in unsupervised domain adaptation is by using a binary classifier to assign known/unknown labels to each target sample. Unknown samples are treated as outliers and discarded during the adaptation process. However, this approach may not fully exploit the data structure when target sample distributions are diverse or semantic labels are ambiguous. In unsupervised domain adaptation, when target sample distributions are diverse or semantic labels are ambiguous, binary classification performance is suboptimal. To address this, clustering is performed on all unlabeled target samples to explicitly model the diverse semantics of known and unknown classes. Target samples are decomposed into clusters, which convey discriminative knowledge of unknown and known classes specific to the target domain. By steering domain adaptation with category-agnostic clusters, the learned representations are expected to be domain-invariant for known classes and discriminative for unknown and known classes. In unsupervised domain adaptation, clustering is used to model diverse semantics of known and unknown classes in target samples. A new approach, Self-Ensembling with Category-agnostic Clusters (SE-CC), incorporates an additional clustering branch to refine representations and preserve the structure of the target domain. Target samples are decomposed into category-agnostic clusters to enhance domain invariance for known classes and discriminative ability for unknown and known classes. The Self-Ensembling with Category-agnostic Clusters (SE-CC) approach integrates a clustering branch to predict cluster assignment distribution of target samples. KL-divergence is used to minimize the mismatch between estimated and inherent cluster distributions, enforcing preservation of data structure in the target domain. The SE-CC framework enhances feature representation by maximizing mutual information among input features, output distribution, and cluster assignment in target samples. Unsupervised domain adaptation in CNNs aims to minimize domain discrepancy through Maximum Mean Discrepancy (MMD) to learn domain-invariant representation. Long et al. (2016) integrates MMD into CNNs for domain invariant representation and adds a residual transfer module. Another approach by Goodfellow et al. (2014) uses a domain discriminator to induce domain confusion for unsupervised domain adaptation. Tzeng et al. (2015) enforces domain invariance with a domain confusion loss in the discriminator. Ganin & Lempitsky (2015) treat domain confusion as binary classification using a gradient reversal algorithm. Open-Set Domain Adaptation extends traditional domain adaptation by addressing a scenario where the target domain includes samples from new and unknown classes. Panareda Busto & Gall (2017) and Saito et al. (2018b) use different methods to tackle this challenge, such as exploiting known/unknown class assignments and adversarial training. Saito et al. (2018b) and Baktashmotlagh et al. (2019) utilize adversarial training and factorization of source and target data to separate target samples of unknown class and model them with a private subspace tailored to the target domain. Conditional entropy and self-ensembling loss are applied to align classification predictions between teacher and student models. To further enhance classification predictions alignment, clustering is used to decompose unlabeled target samples into category-agnostic clusters. These clusters are integrated into Self-Ensembling to improve performance in closed-set and open-set scenarios. A clustering branch in the student model infers cluster assignments for each target sample, aligning them with the original cluster distribution through minimizing KL-divergence. This enforces the feature representation to preserve the data structure of the target domain. SE-CC utilizes unlabeled target samples for learning task-specific classifiers in the open-set scenario by leveraging category-agnostic clusters for representation. The feature representation of the student is enhanced by maximizing mutual information among its feature map, classification, and cluster assignment distributions. SE-CC leverages category-agnostic clusters for representation learning, preserving target data structure during domain adaption. This enables alignment of sample distributions within known and unknown classes, and discrimination between them. The preservation is used to enhance representation learning by maximizing mutual information among input feature, cluster, and class probability distributions. In this paper, the SE-CC model integrates category-agnostic clusters into domain adaptation for both closed-set and open-set scenarios. It leverages these clusters for representation learning, aligning sample distributions within known and unknown classes to enhance representation learning. The goal of open-set domain adaptation is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown target samples. Self-Ensembling, based on Mean Teacher, encourages this learning process. Self-Ensembling aims to ensure consistent classification predictions between teacher and student models with small perturbations of input images. It penalizes the difference in classification predictions by both models, encouraging domain-invariant representations and classifiers for recognizing known classes in the target domain. The Self-Ensembling method focuses on maintaining consistent classification predictions between teacher and student models through training. It utilizes gradient descent for the student and updates the teacher's weights as an exponential moving average. Additionally, it incorporates unsupervised conditional entropy loss to drive decision boundaries away from high-density regions in the target domain. Open-set domain adaptation is challenging as it requires classifying both inliers and outliers into known and unknown classes. A common approach is to use a binary classifier, but this oversimplifies the problem by assuming all unknown samples belong to one class. This may not be robust when unknown samples span multiple classes. To address the limitations of oversimplified binary classification in open-set domain adaptation, clustering is used to model diverse semantics in the target domain. This involves creating category-agnostic clusters integrated into Self-Ensembling for guiding domain adaptation. An additional clustering branch aligns cluster assignment distribution with inherent cluster distribution, enforcing domain-invariant feature representations for known classes and more discriminative features for unknown and known classes. In unsupervised machine learning, clustering is used to group unlabeled data. K-means is a popular method for decomposing target samples into clusters, revealing underlying structures in the target domain. The clusters are category-agnostic but tailored to the target domain, promoting local discrimination among samples with similar semantics. In unsupervised machine learning, clustering is used to group unlabeled data. K-means decomposes target samples into clusters for local discrimination. Target samples are represented as output features of CNNs pre-trained on ImageNet for clustering. The underlying structure of each target sample is encoded as joint relations with category-agnostic clusters. The clustering branch in the student model predicts the distribution over category-agnostic clusters for cluster assignment of target samples. It measures the cluster distribution through softmax over cosine similarities between samples and cluster centroids. The centroids are defined as the average of samples in each cluster. The clustering branch in the student model predicts cluster assignment distribution for target samples based on input features. It uses a modified softmax layer to infer cluster assignments and is trained with supervision from the inherent cluster distribution of each sample. The KL-divergence loss is used to measure the mismatch between estimated and inherent cluster distributions in the target sample. By minimizing this loss, the representation learns to preserve the data structure and inter-cluster relationships, making it more discriminative for both known and unknown classes. The KL-divergence loss with inter-cluster relationships constraint is used in SE-CC to ensure similarity between semantically similar clusters. Mutual Information Maximization is employed to strengthen target features in an unsupervised manner. The MIM module in the student model aims to estimate and maximize mutual information between input features, output distributions, and cluster assignments to enhance downstream tasks. Global Mutual Information is utilized to tune the feature's suitability for these tasks. The input target sample x is encoded into a global feature vector G(x) using convolutional and pooling layers. This feature vector is concatenated with classification and cluster assignment distributions before being fed into a global Mutual information discriminator for alignment assessment. The global Mutual information discriminator is implemented with three stacked fully-connected networks and a softplus function. The final output score represents the probability of discriminating real input features. Local Mutual Information is also exploited among local input features at every spatial location. The local Mutual information discriminator utilizes spatially replicated distributions to discriminate input local features based on classification and cluster assignment. It consists of three stacked convolutional layers and is concatenated with the input feature map. The local Mutual information discriminator is constructed with three stacked convolutional layers and measures the probability of discriminating real input local features. The final objective for the module combines local and global Mutual Information estimations with a tradeoff parameter \u03b1. The training objective of the SE-CC integrates cross-information estimation. The SE-CC training objective integrates cross entropy loss, unsupervised self-ensembling loss, conditional entropy loss, KL-divergence loss, and Mutual Information estimation on target data. Experiments were conducted on the Office Saenko et al. VisDA dataset for synthetic-real image transfer. The validation domain contains real images from COCO Lin et al. (2014) and the testing domain includes video frames in YTBB Real et al. (2017). The synthetic images in the training domain are used as the source, and the COCO images in the validation domain are used as the target for evaluation. For open-set adaptation, known classes are defined for the source and target domains, with background and other categories as unknown classes. Open-Set Domain Adaptation on Office involves unknown classes in the target domain with a 1:10 ratio of known-to-unknown samples. Three metrics - Knwn, Mean, and Overall - are used for evaluation. ResNet152 is used as the backbone for CNNs in both closed-set and open-set scenarios. The accuracy of all 12 classes is reported for closed-set adaptation. The performances of different models on Office for open-set adaptation are compared in Table 1. A variant of SE-CC (SE-CC \u2666) is included for fair comparison with AODA, which learns classifiers without unknown source samples. SE-CC \u2666 recognizes only N-1 known classes and target samples are labeled as unknown if the predicted probability is below a certain threshold. Results show that SE-CC outperforms other models consistently across two metrics. Our SE-CC model outperforms other state-of-the-art closed-set and open-set adaptation models on most transfer directions, especially on harder transfers like D \u2192 A and W \u2192 A. The key advantage lies in exploiting category-agnostic clusters for domain adaptation, making the feature representation domain-invariant for known classes while still discriminative for target samples. Open-set adaptation techniques like AODA, ATI-\u03bb, and FRODA outperform RTN and RevGrad by rejecting unknown target samples as outliers and aligning data distributions only for inliers. This confirms the effectiveness of excluding unknown target samples during domain adaptation in open-set scenarios. Our SE-CC outperforms AODA, ATI-\u03bb, and FRODA in domain adaptation by injecting category-agnostic clusters as a constraint for feature learning and alignment. Performance comparisons on Office and VisDA datasets show the effectiveness of SE-CC in closed-set domain adaptation. Our SE-CC outperforms other state-of-the-art closed-set adaptation techniques by exploiting the underlying data structure in the target domain via category-agnostic clusters. The ablation study investigates how each design in SE-CC influences overall performance, with Conditional Entropy incorporating unsupervised conditional entropy loss and KL-divergence Loss aligning the estimated cluster assignment distribution. The SE-CC model improves performance in open-set domain adaptation by using KL-divergence Loss and Mutual Information Maximization to refine features and enhance suitability for downstream tasks. Table 5 shows performance improvements on VisDA with different designs. CE is a general way to enhance classifier for target domain. The SE-CC model utilizes CE, KL, and MIM to enhance classifier performance in the target domain, resulting in a total performance boost of 4.2% in Mean metric. This approach exploits category-agnostic clusters for open-set adaptation, demonstrating the effectiveness of mutual information maximization. The study focuses on utilizing category-agnostic clusters for domain adaptation in open-set and closed-set scenarios. It involves decomposing target samples into clusters and integrating a clustering branch into the student model to align cluster assignments. This helps in preserving the underlying data structure in the target domain. The study integrates a clustering branch into the student model to align cluster assignments, preserving the data structure in the target domain. Experiments on Office and VisDA show performance improvements compared to state-of-the-art techniques. The SE-CC implementation is in PyTorch, optimized with SGD using a learning rate of 0.001 and a mini-batch size of 56. The learning rate and mini-batch size are set at 0.001 and 56 for all experiments. Training iterations are limited to 300 and 25 epochs on Office and VisDA. The global feature dimension is 128/1,024 in AlexNet/ResNet. Cluster number, tradeoff parameters, and tuning details are specified for open-set and closed-set adaptation tasks. The hyper-parameter search for each transfer is restricted in a specific range. The use of KL-divergence in the clustering branch is compared with L1 and L2 distance, showing that KL-divergence is a better measure of mismatch. Evaluation of Mutual Information Maximization in SE-CC involves assessing different MIM module variants by estimating mutual information between input features and various outputs. CLS, CLU, and CLS+CLU estimate local and global mutual information between input features and classification branch output, clustering branch output, and combined output of both branches. Compared to SE-CC without MIM module, CLS and CLU slightly improve performance by utilizing mutual information between input features and branch outputs. The results show the benefit of exploiting mutual information between input features and combined outputs of classification and clustering tasks in the MIM module. SE helps align source and target distributions for domain-invariant representation, but struggles with recognizing unknown samples. SE-CC separates unknown target samples from known target samples by preserving the underlying data structure, making them indistinguishable from known samples in two domains."
}