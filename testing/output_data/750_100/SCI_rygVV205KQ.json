{
    "title": "rygVV205KQ",
    "content": "High-dimensional sparse reward tasks in reinforcement learning are challenging. This work uses imitation learning to address challenges in learning a representation of the world from pixels and exploring efficiently with rare reward signals. Adversarial imitation can work well in high-dimensional observation spaces with a small adversary acting as the reward function, requiring only 128 parameters and basic GAN formulation for training. This approach overcomes limitations of contemporary imitation methods by not needing demonstrator actions, only video input. The proposed agent can solve a challenging robot manipulation task of block stacking from only video demonstrations and sparse reward, outperforming non-imitating agents. It learns faster than competing approaches with hand-crafted reward functions and GAIL baselines. Additionally, a new adversarial goal recognizer allows the agent to learn stacking purely from imitation in some cases. In this work, the GAIL agent can handle high-dimensional pixel observations with a single layer discriminator network. The efficiency is improved by using a D4PG agent, a state-of-the-art off-policy method that utilizes a replay buffer for past experiences. The method utilizes a replay buffer to store past experiences and successfully uses various types of features with a single-layer adversary. It modifies GAIL for off-policy D4PG agents with experience replay, demonstrating the ability to solve a challenging robotic task from pixels. Our approach successfully solves a challenging robotic block stacking task using only demonstrations and sparse rewards. It outperforms previous imitation methods by learning to stack faster with the same resources. The main contribution is a 6-DoF Jaco robot arm agent achieving a 94% success rate in simulation. The text discusses an agent that achieves a 94% success rate in a robotic block stacking task using sparse rewards and demonstrations. It also introduces an adversary-based early termination method for actor processes to improve task performance and learning speed. Additionally, an agent that learns without task rewards using an auxiliary goal recognizer adversary is mentioned, achieving 55% stacking success from video imitation only. Ablation experiments on Jaco stacking and a 2D planar walker benchmark are conducted to understand the reasons for improvement in the agent. Random projections with a linear discriminator and using value network features are effective in some cases. A Markov Decision Process (MDP) is defined by states, actions, reward function, transition distribution, and discount. The goal is to find a policy that maximizes expected rewards. DDPG BID23 is an actor-critic method using neural networks to represent the policy and action-value function. New transitions are added to a replay buffer for training, with target networks used to improve stability. The DDPG method uses neural networks for policy and action-value function representation, with target networks updated every K learning steps. Building on DDPG, the D4PG agent incorporates improvements and in GAIL, a reward function is learned using a discriminator network to distinguish between agent and expert transitions. GAIL is closely related to MaxEnt inverse reinforcement learning. A D4PG agent is used for off-policy training with experience replay. The reward function is jointly trained using a modified equation. The replay buffer changes the original expectation over the agent policy, requiring the discriminator to distinguish expert transitions from transitions produced by previous agents. The reward function in GAIL interpolates imitation reward and a sparse task reward, using the sigmoid of the logits to bound the imitation reward between 0 and 1. This allows for intuitive values for early termination of episodes in the actor process based on the discriminator score. The actor process includes early termination based on discriminator score to prevent agent drift from expert trajectories. Multiple CPU actor processes run in parallel with a single GPU learner process. The actor process includes early termination based on discriminator score to prevent agent drift from expert trajectories. In practice, setting \u03b2 = 0.1 helps avoid wasting computation time. A critical design choice in the agent is the type of network used in the discriminator, which affects the training dynamics. The discriminator's capacity must be balanced to effectively teach the agent how to solve the task. FIG1 illustrates the discriminator architectures studied in this work. Expert demonstrations are a valuable source of data for feature learning as they cover the necessary regions of the state space. Access to expert actions is not assumed, ruling out behavior cloning for feature learning. High-resolution images are used, so learning features by predicting in pixel space is not chosen. Contrastive predictive coding (CPC) is a representation learning technique that maps observations into a latent space for long-term predictions, using a probabilistic contrastive loss with negative sampling. This approach is preferred over pixel prediction for learning features in imitation learning. One way to improve training efficiency is to replace sparse rewards with a neural network goal recognizer trained on expert trajectories. However, there is a risk that the agent could exploit blind spots in the recognizer to receive imitation rewards without actually solving the task. This issue has been observed in practice. To improve training efficiency, sparse task rewards can be replaced with a secondary goal discriminator network that detects if an agent has reached a goal state. The goal state is defined as a state in the latter 1/M proportion of expert demonstrations, with M = 3 in experiments. The modified reward function includes the secondary goal discriminator network D goal, which operates on the same feature space as the primary discriminator D. Training D goal is similar to D, but expert states are only sampled from the latter 1/M portion of each demonstration. GAIL is typically seen as an imitation learning method, but training a second discriminator to recognize goal states allows agents to surpass demonstrators by learning to reach goals faster. Environments include a Kinova Jaco arm with 9 degrees of freedom. The Kinova Jaco arm has 9 degrees of freedom, controlled by policies setting joint velocity commands. Demonstrations were collected using a SpaceNavigator 3D motion controller, with 500 episodes per task. Another 500 trajectories were gathered for validation. The dataset includes 30 \"non-expert\" trajectories for CPC diagnostics, collected through various behaviors. A D4PG agent was trained using 200 expert demonstrations with 64x64 pixel observations. The imitation method is compared to a D4PG agent in this section. In section 7.5, the imitation method is compared to a D4PG agent on dense and sparse rewards, as well as GAIL agents with discriminator networks operating on pixels directly. The proposed method using a tiny adversary shows favorable results. The CPC models future observations well for expert sequences but not for non-expert ones. Conditioning on k-step predictions improves performance on stacking tasks when the discriminator also uses CPC embeddings. In stacking tasks, the performance of the method using CPC embeddings is compared to D4PG and GAIL agents. D4PG struggles with sparse rewards and slow learning pace, while imitation methods excel with sparse rewards. Agents using value network features outperform those using CPC features. GAIL from pixels performs poorly, but GAIL with tiny adversaries on random projections has limited success. The GAIL agent with CPC features outperforms the one with pixel features due to norm clipping in the critic optimizer. However, adding norm clipping to the pixel-based agent does not improve performance in Jaco or Walker2D environments. The GAIL agent with CPC features outperforms the one with pixel features due to norm clipping in the critic optimizer. Using CPC features for temporal predictions can improve performance in ablation experiments for Jaco stacking. Adding layers to the discriminator network does not improve performance, and early termination is crucial. In ablation experiments for Jaco stacking, it was found that a small discriminator on a meaningful representation is optimal for imitation learning. Increasing the power of the discriminator actually led to degraded performance. An early termination criterion was introduced in Section 3 to stop episodes. The early termination criterion introduced in Section 3 stops episodes when the discriminator score becomes too low, leading to faster learning. The average episode length decreases as the agent improves at imitating the expert, as shown in Figure 7. In a third ablation experiment, the proposed method's data efficiency is evaluated in terms of expert demonstrations. Results show that even with 60 demonstrations, good performance is achieved. An outlier was observed with 120 demos due to one random seed not performing well. Performance on the planar walker is visualized in Figure 7, where conventional GAIL methods did not learn, but the proposed method using value network features showed improvement. The proposed method using value network features and random projections learned to run, achieving a 55% success rate without any task rewards. Videos of the trained agent are available in the supplementary materials. The agent seed achieved a 55% success rate in stacking tasks, outperforming human teleoperators. Leveraging expert demonstrations to improve agent performance is a common practice in robotics. Previous work has shown that priming a Q-function on expert demonstrations can lead to improved agent performance after a single training episode. Imitation learning is a popular approach in robotics to improve agent performance using expert demonstrations. Deep learning has been successful in tasks like computer vision and can be extended to taking actions in environments. Supervised imitation is a simple yet effective method for this purpose. In imitation learning, supervised approaches like behavioral cloning and one-shot imitation are used to replicate desired behaviors from expert demonstrations. Different methods, such as using attention mechanisms or gradient-based meta learning, have been employed to achieve one-shot learning of observed behaviors in robotics tasks. Our approach aims for the agent to learn by interacting with the environment rather than supervised learning, avoiding cascading failures in very different states. This eliminates the need for a large number of demonstrations and access to demonstrator actions, which may not always be available. Instead of using behavior cloning, BID38 BID25 ; BID0. Instead of behavior cloning, BID38 BID25 ; BID0 propose inverse reinforcement learning (IRL) to learn a reward function from demonstrations and optimize it using reinforcement learning. DQfD and DPGfD develop methods to train agents with expert trajectories and agent experiences, showing success in tasks like peg insertion. Our major contribution is the use of minimal adversaries on top of learned features to solve a peg insertion task on a real robot without access to expert actions. Through the use of minimal adversaries and learned features, we can solve sparse reward tasks with high-dimensional input spaces. Different approaches focus on learning compact representations for imitation learning from expert observations. BID30 and BID3 learn self-supervised features from third person observations to bridge the domain gap. While they track a single expert trajectory, we use our features for learning tasks from all available expert trajectories through GAIL, aiming to generalize all possible trajectories. Utilizing static self-supervised features like contrastive predictive coding and dynamic value network features, we successfully train block stacking agents from sparse rewards on pixels. Supplementary videos of our learned agents can be viewed online. The behavior cloning model uses a residual network pixel encoder architecture followed by an LSTM and a linear layer for regressing expert actions from pixel observations. The stacking accuracy is around 15%. The model consists of an encoder and an autoregressive model optimizing the same loss function. The behavior cloning model utilizes a residual network pixel encoder architecture with an LSTM and a linear layer to regress expert actions from pixel observations, achieving a stacking accuracy of around 15%. The model includes an encoder and an autoregressive model that optimize the same loss function, focusing on maximizing mutual information between context and target variables for compact representations. Our proposed approach involves model learning through contrastive predictive coding (CPC) and training the agent using CPC future predictions. Reward functions are modified from BID37, with dense staged rewards defined in five stages and sparse rewards in two stages. Each episode lasts 500 time steps without early stopping. The approach involves model learning through contrastive predictive coding (CPC) and training the agent using CPC future predictions. Sparse rewards are defined in two stages, with no rewards for reaching, lifting, or releasing. Actor and critic share a residual network with convolutional layers and fully connected networks. Instead of using a scalar state-action value function, they use independent three-layer fully connected networks. In this paper, a categorical representation of Z is adopted for Distributional Q functions, with fixed atoms bounded between V min and V max. The bootstrap target is computed with N-step returns using a sub-sequence of states, actions, and rewards. The paper adopts a categorical representation of Z for Distributional Q functions with fixed atoms bounded between V min and V max. The loss function for training the distributional value functions involves cross entropy. Distributed prioritized experience replay is used to increase stability and learning efficiency."
}