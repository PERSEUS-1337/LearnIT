{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a reduction in parameters and improved performance. The use of branches also provides additional regularization. Tighter coupling of these branches by averaging their log-probabilities enhances the learning of better representations. Coupling branches in a neural network architecture, known as \"coupled ensembles\", improves representation learning. This approach is applicable to various neural network architectures. Using DenseNet-BC with a 25M parameter budget, error rates of 2.92%, 15.68%, and 1.50% are achieved on CIFAR-10, CIFAR-100, and SVHN tasks. For the same budget, DenseNet-BC alone has error rates of 3.46%, 17.18%, and 1.8%. Ensembles of coupled ensembles with 50M parameters yield error rates of 2.72%, 15.13%, and 1.42% on these tasks. The design of early convolutional architectures involved choices of hyper-parameters such as filter size and number of filters. Since the introduction of VGGNet, the design has moved towards a fixed filter size of 3x3 and N feature maps, down-sampling by maxpooling or strided convolutions, and doubling the computed feature maps after each down-sampling operation. This design philosophy is used by state-of-the-art models like ResNet and DenseNet. The proposed template extends the design philosophy of state-of-the-art models like ResNet and DenseNet by introducing \"coupled ensembling\" to split parameters among branches for improved performance with fewer parameters. The paper introduces the concept of coupled ensembles to split parameters among branches, improving performance with fewer parameters. By combining activations through an arithmetic mean, convolutional networks on CIFAR and SVHN datasets show significant improvement. Further ensembling of coupled ensembles leads to additional enhancements. The proposed approach involves coupled ensembles with branches sharing parameters, improving performance with fewer parameters compared to traditional ensembles. The network architecture is similar to existing models but differs in training a single model with branches instead of training each member separately. The proposed approach involves coupled ensembles with branches sharing parameters to improve performance with fewer parameters. Multi-branch architectures have been successful in vision applications, combining activations and using different preprocessing blocks for each branch. Proposed modifications in vision applications involve using grouped convolutions to factorize spatial and depth wise feature extraction. Unlike previous approaches that modify building blocks, we suggest a global model level modification by specifying a generic structure for CNNs. This includes replicating a specified \"element block\" in parallel. The \"element block\" is replicated in parallel branches to form the final composite model. Shake-Shake regularization proposes a stochastic mixture of branches for improved performance on CIFAR datasets, but requires more epochs for convergence and depends on batch size. In contrast, our method uses the same hyper-parameters as the base CNN model. BID26 explores connecting layers across parallel paths in a ResNet for information exchange. Our method involves a generic rearrangement of a given architecture's parameters, leading to efficient parameter usage. Ensembling neural networks can improve model performance by combining outputs from multiple trainings with different error distributions. Our proposed model architecture involves rearranging parameters to efficiently use them, similar to ensembling neural networks. This approach combines outputs from parallel branches to improve overall task performance, inspired by ResNet and Inception modules. Our approach involves rearranging parameters into parallel branches to improve performance, similar to ensembling neural networks. This method leads to a significant performance improvement, although it increases model size and prediction time. Our approach involves rearranging parameters into parallel branches to improve performance, similar to ensembling neural networks. The model comprises several branches, each using an element block like DenseNet-BC or ResNet. Fuse Layers combine the parallel branches. The model involves rearranging parameters into parallel branches, similar to ensembling neural networks. The Fuse Layer combines the branches by averaging their individual log probabilities. This approach is applied to classification tasks like CIFAR, SVHN, and ILSVRC, with potential generalization to other tasks like segmentation and object detection. The neural network models output a score vector of the same dimension as the target classes. Neural network models output a score vector of the same dimension as target classes, usually followed by a fully connected (FC) layer and SoftMax (SM) layer for probability distribution. Different network architectures for image classification have variations before the last FC layer. The internal setup complexity is not a concern during training. The internal setup complexity is not a concern during training. Fusion in ensemble models involves computing individual predictions separately for each model instance and averaging them, which is equivalent to predicting with a \"super-network\" including the instances as parallel branches with an AVG layer on top. Supernetworks are not implemented due to memory constraints on GPUs. The model consists of parallel branches producing score vectors for target categories. These vectors are fused through a \"fuse layer\" during training to create a single prediction. Three options are explored for combining score vectors during training and inference. The model utilizes parallel branches to produce score vectors for target categories, which are combined through averaging during training and inference. This method leads to improved performance with fewer parameters. The proposed architecture uses parallel branches to generate score vectors for target categories, combined through averaging. Parameters are concatenated from element blocks in the composite branched model. Evaluation is done on CIFAR-10, CIFAR-100, and SVHN datasets with varying numbers of training and testing images. The proposed architecture uses parallel branches to generate score vectors for target categories, combined through averaging. Parameters are concatenated from element blocks in the composite branched model. Evaluation is done on CIFAR-10, CIFAR-100, and SVHN datasets with varying numbers of training and testing images. All images are 32\u00d732 pixels in size and input image normalization is done for CIFAR-10, CIFAR-100, and SVHN datasets. Data augmentation is used during training on CIFAR datasets, while no data augmentation is used for SVHN. A dropout ratio of 0.2 is applied in the case of DenseNet when training on SVHN. In experiments with DenseNet on SVHN, a dropout ratio of 0.2 is applied. Testing involves input normalization similar to training. Error rates are presented as percentages averaged over the last 10 epochs. Execution times were measured using a single NVIDIA 1080Ti GPU. Experiments on CIFAR-100 with DenseNet-BC, depth L = 100, growth rate k = 12 were conducted. In experiments with DenseNet on CIFAR-100, a depth of L = 100 and growth rate k = 12 were used. Comparisons were made between a branched architecture and an ensemble of independent models, showing lower test error with the branched configuration. The experiments with DenseNet on CIFAR-100 compared a branched architecture with a single branch model, showing lower test error with the branched configuration (17.61 vs. 20.01). The effect of the branched configuration becomes more pronounced as the number of parameters increase. The experiments show that arranging parameters into parallel branches is more efficient than a single large branch or multiple independent models. The performance of the proposed branched model is compared for different choices of the \"fuse layer\". Experiments evaluate training and prediction fusion combinations for a branched model with e = 4, trained under various conditions. The study compares the performance of a branched model with different \"fuse layer\" combinations after the FC, LSM, or LL layers. Results are presented in Table 1, showing top-1 error rates on the CIFAR-100 test set for different architectures and training conditions. The study compares the performance of models with different \"fuse layer\" operations for inference. Table 1 shows the results for models with parameters obtained using different training methods. The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC (L = 250, k = 24) model BID11. The \"fuse layer\" in the study performs almost as well as a DenseNet-BC (L = 250, k = 24) model BID11 with significantly fewer parameters. Coupled ensembles with LSM fusion result in lower error rates for \"element blocks\" trained together, indicating improved learning of complementary features and better representations. Averaging log probabilities helps update all branches consistently, providing a stronger gradient signal during back-propagation. Ensemble combinations, except Avg. FC training, outperform single branch networks. With a parameter budget of 3.2M, using 4 branches reduces error rate to 17.61. Avg. FC training shows poor performance for individual branches due to unrelated FC instances. The Avg. FC prediction works better than Avg. SM prediction due to FC values transmitting more information. All experiments use Avg. LSM for training. In this section, the optimal number of branches e for a given model parameter budget is investigated using DenseNet-BC on CIFAR-100. Results are shown in table 3 for different configurations of branches e, depth L, and growth rate. The performance of DenseNet-BC on CIFAR-100 with different configurations of branches e, depth L, and growth rate k is analyzed in TAB2. Parameter counts are quantified based on L and k values, with L needing to be a multiple of 6 modulo 4. Model configurations with parameters just below the target are selected for fair comparison, with some models having slightly more parameters for interpolation. The optimal configuration for DenseNet-BC on CIFAR-100 with 800k parameters includes 3 branches, depth of 70, and growth rate of 9. This setup reduces the error rate from 22.87 to 21.10. Using 2 to 4 branches shows significant performance improvement over the single branch model. However, using 6 or 8 branches performs worse. Model performance remains stable with slight variations in parameters around the optimal values. The DenseNet-BC architecture shows robustness with variations in L, k, and e values. Coupled ensemble approach improves performance but increases training and prediction times. Evaluation against different models shows DenseNet-BC as the current state of the art. The evaluation of ResNet BID8 with pre-activation as the element block showed improved performance in coupled ensembles compared to single branch models. This approach outperformed current state-of-the-art models and involved training multiple models as a single global model. The DenseNet-BC architecture outperformed single branch models in performance with varying network sizes and parameters. The trade-off between depth and growth rate was not critical for a given parameter budget, and choosing between the number of branches, depth, and growth rate was flexible for fixed parameter budgets. The DenseNet-BC architecture outperformed single branch models with varying network sizes and parameters. Different configurations were tested, including single and multi-branch versions with varying numbers of branches. Error rates were compared with a Torch7 implementation, showing differences possibly due to conservative error rate measures and statistical variations. The coupled ensemble of DenseNet-BC models outperforms other network sizes, including DenseNet-BC's reported performance. Larger models show better error rates on CIFAR 10, CIFAR 100, and SVHN, comparable to state-of-the-art implementations. Comparison with meta-learning scenarios is also provided in the supplementary material. The coupled ensemble approach with DenseNet-BC models outperforms other network sizes on CIFAR 10, CIFAR 100, and SVHN datasets. The limitation lies in the network size fitting into GPU memory and training time constraints. The classical ensembling approach was used for further improvement beyond 25M parameters. The question of significant performance improvement was raised, especially since the classical approach tends to plateau after a few models. The coupled ensemble approach with DenseNet-BC models outperforms other network sizes on CIFAR 10, CIFAR 100, and SVHN datasets. The ensembled models show significant improvement from 1 to 3 models but not much from 3 to 16 models. By fusing two models, a significant gain was obtained, but further fusion did not yield much improvement. The ensembles of coupled ensemble networks outperform all state-of-the-art implementations. The proposed approach involves replacing a single deep convolutional network with multiple \"element blocks\" that function as standalone CNN models. These blocks are connected via a \"fuse layer\" that combines their output scores. During training, the log-probabilities of the targets are averaged across the blocks. The performance of branched models with 13M parameters outperforms single branch models with double the parameters on CIFAR-100, as shown in FIG1. The proposed approach involves using multiple \"element blocks\" connected via a \"fuse layer\" to improve performance over a single deep convolutional network. Log-probabilities of targets are averaged across blocks during training, leading to better performance with 13M parameters compared to double the parameters in single branch models on CIFAR-100. This improvement comes with a slight increase in training and prediction times. The proposed approach involves using multiple \"element blocks\" connected via a \"fuse layer\" to improve performance over a single deep convolutional network. The increase in training and prediction times is mainly due to sequential processing of branches during forward and backward passes. To address this, data parallelism can be extended to branches or spread over multiple GPUs. Preliminary experiments on ImageNet show that coupled ensembles have lower errors compared to single branch models for the same parameter budget. The test and train versions of networks use a common structure with an averaging layer placed after the last FC layer in the test version. The model instances do not need to share the same architecture. In the train version, the averaging layer can be placed after the last FC layer, LSM layer, or LL layer. \"Element blocks\" from other groups are reused for efficiency and meaningful comparisons. Each branch is defined by a parameter vector W e, while the global network is defined by a parameter vector W. When training in coupled mode, a script splits the global parameter vector W into individual parameter vectors W e for prediction. This allows for combining different training and prediction conditions, even if not all are equally efficient. The network architecture is determined by global hyper-parameters specifying train versus test mode, number of branches, and placement of AVG layer. Larger models may require data batches to be split into \"micro-batches\" for training. When training larger models, data batches may need to be split into micro-batches with b/m elements each. The gradient is accumulated over these micro-batches and averaged to approximate the equivalent gradient of processing data as a single batch, with BatchNorm layer affecting exact equivalence due to using micro-batch statistics. However, this difference is not significant in practice. When training larger models, data batches may need to be split into micro-batches with b/m elements each. The gradient is accumulated over these micro-batches and averaged to approximate the equivalent gradient of processing data as a single batch. Parameter updates are performed using gradient for a batch, while forward passes are done with micro-batches for optimal throughput. Memory requirement depends on network depth and mini-batch size, with the micro-batch \"trick\" adjusting memory needs while keeping mini-batch size constant. The multi-branch version does not require more memory if branch width is adjusted. In practice, for \"full-size\" experiments with 25M parameters, training was done within the 11GB memory of GTX 1080 Ti using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch ones. Splitting the network over two GPU boards allows for efficient training. Training with micro-batch sizes of 16 for single-branch versions and 8 for multi-branch ones on a GTX 1080 Ti with 11GB memory. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but does not significantly increase speed or improve performance. Comparing test time equivalence between FC average and logsoftmax for different branches. Using only two branches still provides a significant gain over a single branch architecture. Using only two branches still provides a significant gain over a single branch architecture of comparable size. The experiment evaluated the depth L and growth rate k for a fixed parameter count, showing stable performance. The best combination predicted for the test set was (L = 82, k = 8, e = 3), although (L = 70, k = 9, e = 3) appeared slightly better but not statistically significant. In comparing parameter usage and performance of branched coupled ensembles with model architectures recovered using meta learning techniques, the issue of reproducibility and statistical significance arises. Variations in performance measures include framework used (Torch7 and PyTorch) and random seed for network initialization. Experiments were conducted with Torch7 (lua) and PyTorch, noting non-determinism in CuDNN during training. Fluctuations in batch normalization were observed even with all hyper-parameters set to 0. The choice of model also played a significant role in the results. The choice of model instance from training epochs, whether the last epoch model or the best performing model, can impact evaluation measures due to random initialization variations. Different random seeds may lead to different local minima, but properly designed and trained neural networks should have similar performance. The dispersion in performance of neural networks obtained with proper design and training complicates comparisons between methods. Small differences may not be significant, making classical statistical tests less helpful. Experiments in this section estimate dispersion in a moderate scale model, focusing on DenseNet-BC with L = 100, k = 12 on CIFAR 100. Different effects were quantified using Torch7 and PyTorch with the same or different seeds. Performance was measured by the error rate of the model. In this configuration, the performance measure was based on the error rate of the model computed at the last epoch or the average of the error rate of the models computed at the last 10 epochs. Different cases were analyzed, presenting the minimum, median, maximum, and mean with standard deviation over 10 measures from 10 identical runs. Additionally, the root mean square of the standard deviation of fluctuations on the last 10 epochs was also presented. The study compared Torch7 and PyTorch implementations, finding no significant differences. Using the same seed or different seeds also showed no significant difference in results. The means over the last epoch and last 10 epochs were similar. The standard deviation of measures from 10 runs had slight variations. The standard deviation of measures computed on the last 10 epochs is consistently smaller, reducing fluctuations. Averaging measures over the last 10 epochs results in significantly lower mean compared to single last epoch. The standard deviation of measures computed on the last 10 epochs is consistently smaller, reducing fluctuations and resulting in significantly lower mean compared to single last epoch. To ensure reproducibility and fair comparisons, a method is proposed that avoids selecting the best model based on test data, as it can lead to tuning on the test set. In experiments, using the error rate at the last 10 iterations reduces fluctuations and provides a more stable estimation of performance. Tuning on the test set can introduce bias, so it is recommended to avoid selecting the best model based on test data. In CIFAR experiments, the average error rate of models from the last 10 epochs is used for more robust results. For SVHN experiments, the last 4 iterations are used due to fewer epochs. This approach leads to more stable performance estimation and should be applied generally. In this study, comparisons between single-branch and multi-branch architectures have shown an advantage for multi-branch networks at a constant parameter budget. However, the training time for multi-branch networks is currently longer than for single-branch networks. The study investigates if multi-branch architectures can still outperform single-branch ones at a constant training time budget. Ways to reduce training time include reducing the number of iterations. Reducing training time can be achieved by decreasing the number of iterations, reducing parameter count, or increasing width while decreasing depth. Results for these options are compared to a single branch DenseNet-BC L = 190, k = 40, e = 1, which takes about 80 hours to train. The corresponding multi-branch baseline, DenseNet-BC L = 106, k = 33, e = 4, has a training time 1.6 times longer for 300 epochs. Reducing training time can be achieved by decreasing the number of iterations, reducing parameter count, or increasing width while decreasing depth. Results for different options are compared to a single branch DenseNet-BC L = 190, k = 40, e = 1, which takes about 80 hours to train. The options include reducing training epochs to 188, reducing depth to 88, or matching parameter count and training time of the baseline. Despite performing slightly worse, these options still outperform the full multi-branch baseline. In comparison to the full multi-branch baseline, all three options perform better than the single-branch baseline. DenseNet-BC L = 88, k = 20, e = 4 outperforms the single-branch baseline with reduced parameters and training time. The performance of single branch models and coupled ensembles is compared in a low training data scenario using two datasets: STL-10 and a 10K balanced random subset of CIFAR-100. Results from experiments on STL-10 and a subset of CIFAR-100 show that coupled ensembles outperform single-branch models with a fixed parameter budget. Preliminary experiments on ILSVRC2012 were conducted using images of size 256x256 due to constraints. Data augmentation included random flips and crops of size 224x224. The augmentation involved random horizontal flips and crops of size 224x224. A DenseNet-169-k32-e1 single-branch model is compared with a coupled ensemble DenseNet-121-k30-e2. Despite not being state-of-the-art, the coupled ensemble approach shows a significant improvement over the baseline in experiments with full-sized images and increased data augmentation. Results are displayed in table 11, with updates planned after the deadline."
}