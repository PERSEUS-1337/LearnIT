{
    "title": "BJInEZsTb",
    "content": "In this paper, the authors explore representation learning and generative modeling using three-dimensional geometric data in the form of point clouds. They introduce a deep autoencoder network that outperforms existing methods in 3D recognition tasks and enables shape editing applications through algebraic manipulations. Additionally, the study includes an analysis of different generative models, such as GANs and Gaussian mixture models, showing improved performance when trained in the latent space of the autoencoder. The study explores representation learning and generative modeling using 3D geometric data. A deep autoencoder network outperforms existing methods in 3D recognition tasks and enables shape editing. GANs and Gaussian mixture models show improved performance when trained in the autoencoder's latent space. GMMs trained in the latent space of the autoencoder produce samples with the best fidelity and diversity. The study focuses on quantitative evaluation of generative models using 3D geometric data. Different encodings like view-based projections, volumetric grids, and graphs are used for representing real-life objects. These encodings, while effective in their domains, often lack semantics. Recent advances in deep learning offer a data-driven approach to designing new objects, eliminating the need for complex parametric models linking semantics to representations. This is particularly beneficial in domains with abundant data, where deep learning tools have revolutionized generative model design. Deep learning tools have revolutionized generative model design by eliminating the need for hand-crafting features and models in data-rich domains. Architectures like autoencoders and Generative Adversarial Networks are successful at learning complex data representations and generating realistic samples. Recently, deep learning architectures for various 3D modalities have emerged, with a focus on point clouds in this paper. In this paper, the focus is on point clouds as a 3D modality. Point clouds offer a compact representation of surface geometry, suitable for geometric operations and learning tasks. They are commonly generated by devices like the Kinect and iPhone for face identification. Existing deep architectures for 3D point clouds include PointNet BID17 for classification and segmentation, BID14 for pipeline integration, and Fan et al. (2016) for underlying applications. Generative models for point clouds have gained attention in the deep learning community, particularly with the introduction of GANs. However, training GAN-based generative pipelines is challenging and unstable. Evaluating generative models remains a challenge, as there is no universally accepted method to assess fidelity in how generated points resemble actual data. The text discusses the challenges of training and evaluating generative models for point clouds, focusing on fidelity and coverage. It introduces a new AE architecture inspired by recent classification architectures to address these issues. The text introduces generative models for point clouds that can generate data similar to the training and test datasets. It suggests training an autoencoder with a compact bottleneck layer first, followed by training a plain GAN in the fixed latent representation. This approach aims to improve classification and semantic operations without the need for joint learning. Training GANs in a compact, low-dimensional latent space is easier and leads to superior reconstruction and coverage. Multi-class GANs perform almost as well as dedicated GANs trained per object category when trained in the latent space. Various metrics are evaluated for their applicability as learning objectives. The paper evaluates different metrics for learning objectives and sample evaluation in generative models. Chamfer distance is found to be inadequate in distinguishing between good and pathological cases. New fidelity and coverage metrics are proposed for model evaluation. The paper introduces models for latent representations and point cloud generation, evaluating them quantitatively and qualitatively in Section 4. The code for all models is publicly available. Autoencoders aim to reproduce input data with a low-dimensional representation in a bottleneck layer. The Encoder (E) compresses data into latent representation z, which the Decoder (D) uses to reproduce the original data. Generative Adversarial Networks (GANs) involve a game between a generator (G) and a discriminator (D) to create realistic samples. The discriminator distinguishes between real and synthesized samples using specific loss functions. Losses for the discriminator and generator networks are defined by \u03b8 (D) and \u03b8 (G) parameters. Improved Wasserstein GAN is used for stability. Point clouds pose unique challenges due to lack of grid-like structure for convolution operations. Recent classification work on point clouds bypasses the lack of grid-like structure by circumventing 2D convolutions. Point clouds are unordered, making comparisons between two point sets challenging and requiring permutation-invariant features. Two permutation-invariant metrics have been developed for comparing unordered point sets. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature: Earth Mover's distance (EMD) and Chamfer (pseudo)-distance (CD). EMD transforms one set to the other through a transportation problem, while CD measures the squared distance between each point in one set to its nearest neighbor in the other set. EMD is differentiable almost everywhere, while CD is more computationally efficient. Evaluation Metrics for representations and generative models involve comparing point cloud distributions to ground truth counterparts to assess quality, faithfulness, diversity, and potential mode-collapse. This comparison is essential for measuring how well a representation model matches training or test sets. The Earth Mover's distance (EMD) and Chamfer distance (CD) are two metrics used for comparing unordered point sets, with EMD being differentiable and CD being more computationally efficient. To evaluate how well a point-cloud distribution matches a ground truth distribution, metrics like Coverage, COV-CD, COV-EMD, and Minimum Matching Distance (MMD) are used. Coverage measures the fraction of matched point-clouds in G to A, while MMD captures fidelity by finding the closest point cloud in A to each point cloud in G and reporting the average distance. The Minimum Matching Distance (MMD) measures the distances in pairwise matchings in point cloud data, correlating with the realism of the elements. It can be calculated using either structural distances, resulting in MMD-CD and MMD-EMD. Jensen-Shannon Divergence (JSD) calculates the difference between marginal distributions in a 3D space, assessing how point clouds occupy similar locations. In this section, the architectures of representation and generative models for point clouds are described, starting with an autoencoder design. A GAN architecture tailored to point-cloud data is introduced, followed by a more efficient pipeline that learns an AE and then trains a smaller GAN in the learned latent space, along with a simpler generative model based on Gaussian Mixtures. The input to the AE network is a point cloud with 2048 points, representing a 3D shape. The encoder architecture follows the principle of 1-D convolutional layers with kernel size. The encoder architecture for point clouds uses 5 1-D convolutional layers with increasing features, ending with a symmetric function. The output is a k-dimensional vector for the latent space. The decoder consists of 3 fully connected layers to produce a 2048 \u00d7 3 output. The encoder architecture for point clouds uses 5 1-D convolutional layers to generate a k-dimensional vector for the latent space, followed by a symmetric function. The decoder has 3 fully connected layers producing a 2048 \u00d7 3 output. Two distinct AE models, AE-EMD and AE-CD, were created using efficient EMD-distance approximation and Chamfer-Distance as structural losses. The optimal latent-space size was determined to be k = 128 through training 8 AEs with different bottleneck sizes and evaluating generalization error. The generative model operates on a 2048 \u00d7 3 point set input, with a discriminator architecture similar to the AE. The generator uses a 128-dimensional noise vector to produce a 2048 \u00d7 3 output. The generator in the Latent-space GAN (l-GAN) takes a 128-dimensional noise vector and maps it to a 2048 \u00d7 3 output through 5 FC-ReLU layers. The l-GAN operates on the bottleneck variable of a pre-trained autoencoder, trained separately for each object class. The architecture of the l-GAN is simpler compared to the r-GAN, with shallow designs for both the generator and discriminator. The study explores the effectiveness of shallow designs in the generator and discriminator of r-GANs and GMMs trained on latent spaces learned by AEs. GMMs with varying Gaussian components and covariance matrices are used as point-cloud generators. The study evaluates the performance of shallow designs in r-GANs and GMMs trained on latent spaces learned by AEs. Shapes from the ShapeNet repository are reconstructed using class-specific AEs. Training involves splitting models into training/testing/validation sets. Classification is used to assess the quality of unsupervised representation learning algorithms. The experiment evaluates the performance of latent features computed by an AE on ShapeNet models. The AE was trained on various shape categories, with a larger bottleneck of 512 and batch-norm applied to the decoder. Features for a 3D shape are obtained by feeding its point-cloud through the network to extract a 512-dimensional vector. The 512-dimensional bottleneck layer vector extracted from a 3D shape's point-cloud is processed by a linear classification SVM trained on ModelNet BID32. The feature is compared to the previous state of the art BID31, showing that the 512-dimensional feature is more intuitive and parsimonious. The decoupling of latent representation from generation allows flexibility in choosing the AE loss, affecting the learned feature. On ModelNet10, EMD and CD losses perform equivalently for larger objects with fewer categories, while increased variation within the collection impacts performance. When variation within the collection increases, CD produces better results due to its local and less smooth nature, understanding rough edges and high frequency details. The experiment also demonstrates the domain-robustness of learned features. Visual assessment of the representation quality is shown through reconstruction results using AEs on the ground truth dataset. Our learned representation can generalize to unseen shapes, enabling shape editing applications like interpolations, part editing, and analogies. The ability of our AEs to reconstruct unseen shapes is demonstrated visually and quantitatively in the results. The study compares five generative models trained on chair point-cloud data, including two AEs with 128-dimensional bottleneck trained with CD or EMD loss, referred to as AE-CD and AE-EMD. Additionally, l-GANs are trained in each AE's latent space, with further models trained in the AE-EMD space. The study compares five generative models trained on chair point-cloud data, including AE-EMD, l-GAN with Wasserstein objective, GMMs, and r-GAN. GANs are trained for 2000 epochs, selecting the best model based on synthetic results matching the ground-truth distribution. Distance between synthetic and validation sets is measured to select the final model. The study compares generative models trained on chair point-cloud data, including AE-EMD, l-GAN, GMMs, and r-GAN. Models are selected based on synthetic results matching the ground-truth distribution. The distance between synthetic and validation sets is measured using JSD or MMD-CD metrics for model selection. GMMs perform better with full covariance matrices, indicating strong correlations between latent dimensions. The study evaluates generative models trained on chair point-cloud data, comparing their performance based on synthetic sample generation. Strong correlations between latent dimensions were observed with GMMs using MMD-CD as the selection criterion. The optimal number of Gaussians was found to be 40. The models were compared based on their ability to generate synthetic samples, with an average classification score of 84.7% achieved by the ground-truth point clouds. The study evaluates generative models trained on chair point-cloud data and compares their performance in generating synthetic samples. The models are assessed based on how closely the distribution of the generated samples resembles the ground truth distribution. The average classification probability for samples being recognized as a chair is also measured. The experiment evaluates 5 generators on a test-split of chair dataset using minimal JSD on the validation-split. GMM-32-F, a GMM with 32 Gaussian components, yields the best results in fidelity and coverage. Training a Gaussian mixture model in the latent space of EMD-based AE shows promising results. The experiment evaluates 5 generators on a test-split of chair dataset using minimal JSD on the validation-split. GMM-32-F yields the best results in fidelity and coverage. The achieved fidelity and coverage by GMMs are comparable to the reconstruction baseline. Generalization ability of the models is established through comparable performance on training vs. testing splits. In the experiment, synthetic datasets are generated for train, test, and validation splits to reduce sampling bias when measuring MMD or Coverage statistics. The MMD-CD distance to the test set is relatively small for the r-GANs, as noted in TAB10. The MMD-CD distance to the test set is relatively small for the r-GANs, despite qualitative results showing otherwise. This discrepancy is attributed to the inadequacy of the chamfer distance in distinguishing pathological cases, as illustrated in Fig. 3 with examples of synthetic point clouds generated by r-GANs and l-GANs. The nearest neighbor in each synthetic set is found under the chamfer distance. The r-GAN results are of lesser quality due to clouds with many points concentrated in specific areas. This affects the CD values and implies a discrepancy in distinguishing pathological cases. The CD metric is \"blind\" to partial matches between shapes, leading to larger coverage metrics compared to EMD. EMD promotes one-to-one mapping, correlating more strongly to visual quality and heavily penalizing r-GAN results in terms of MMD and coverage. Extensive measurements were conducted during model training to understand their behavior. During training, measurements were taken to understand model behavior. The r-GAN struggled to provide good coverage of the test set, indicating the difficulty in training end-to-end GANs. The l-GAN (AE-CD) performed better in terms of visual quality of synthetic results. The CD distance favors r-GAN results due to high-density areas in point sets, but promotes unnatural topologies. Switching to an EMD-based AE (l-GAN, AE-EMD) improves coverage and fidelity. Both l-GANs suffer from mode collapse during training. The text discusses the use of GANs on point-cloud data, highlighting a catastrophic collapse in coverage during training. Switching to a latent WGAN helps eliminate this issue. Comparisons are made with voxel-based methods, showing improvements in coverage and fidelity. The models are also compared to a recent voxel-grid based approach in terms of JSD on the training set of the chair category. The authors convert voxel grid output into a point-set with 2048 points using farthest-point-sampling. They use an isovalue parameter of 0.1 and isolate the largest connected component. The r-GAN outperforms BID31 in diversity and realism, while l-GANs perform even better. The l-GANs outperform the r-GAN in terms of classification and diversity with fewer training epochs. The training time for l-GAN is significantly shorter due to its smaller architecture. The synthetic results from l-GANs and 32-component GMM show high quality outputs, highlighting the strength of the learned models. The high quality results from both models demonstrate the strength of the learned representation. The l-GAN produces clearer results compared to the r-GAN, showing the advantage of using a good structural loss. Synthetic point clouds generated by l-GAN and 32-component GMM trained on the latent space of an AE using the EMD loss are shown in Figure 5. The AE-EMD was trained on a mixed set of point clouds from 5 categories. Training and testing datasets were randomly selected with 2K models per class for training, 200 for testing, and 100 for validation. The multi-class AE had a bottleneck size of 128 and was trained for 1000 epochs, compared to class-specific AEs trained for 500 epochs. The best AE model was chosen based on minimal reconstruction loss on the validation set. The l-WGANs were trained on six AEs for 2K epochs and evaluated using MMD-CD. Results showed that multi-class AE-based l-WGANs performed similarly to class-specific ones in terms of fidelity/coverage. Visual quality comparison also indicated minimal sacrifice in using multi-class AE-EMD. The models were trained on latent spaces of dedicated and multi-class EMD-AEs. The average measurement is computed using weighted averages. Limitations include failure cases where rare geometries are not faithfully decoded, missing high-frequency details, and struggles to create realistic shapes for some classes. Designing more robust raw-GANs for point clouds is an interesting avenue for future work. Recent works have explored training Gaussian mixture models in the latent space of autoencoders, addressing issues with over-regularization in VAEs. Some methods gradually increase the weight of the regularizer to improve reconstruction quality. Innovative architectures for 3D point-cloud representation learning and generation were introduced, showing good generalization to new data and encoding meaningful semantics. The best-performing generative model was a GMM trained in the fixed latent space of an AE, suggesting a simple yet effective approach. The AE mentioned in Section 4.1 had an encoder with 128, 128, 256, and 512 filters, and a decoder with 1024, 2048, 2048 \u00d7 3 neurons. Batch normalization was used between layers, and online data augmentation included random rotations along the z-axis. The AE was trained for 1000 epochs. The AE was trained for 1000 epochs with CD loss and 1100 with EMD. Encoder had 64, 128, 128, 256 filters, while decoder had 3 FC-ReLU layers with 256, 256, 2048 \u00d7 3 neurons each. Different setups like denoising/regularised did not show advantages. Adding drop-out layers worsened reconstructions. Batch-norm on encoder only sped up training. The r-GAN model used a discriminator with 1D-convolutions and leaky-ReLU layers, followed by a featurewise max-pool. The generator consisted of FC-ReLU layers with varying numbers of neurons. The model was trained with Adam optimizer and a batch size of 50, using a noise vector. The discriminator in the current model has 2 FC-ReLU layers with {256, 512} neurons each and a final FC layer with a single sigmoid neuron. The generator has 2 FC-ReLUs with {128, k For the classification experiments, a one-versus-rest linear SVM classifier with an l2 norm penalty and balanced class weights was used. The optimization parameters can be found in Table 5, which includes the training parameters for SVMs on each dataset with each structural loss of the AE. The AE-EMD is used for shape editing applications across 55 object classes, showcasing its ability to encode features for different shapes. The reconstruction quality of the AEs is comparable between training and test datasets, indicating generalization ability. The AE-EMD is utilized for shape editing applications across various object classes, demonstrating its capability to encode features for different shapes. Using shape annotations as guidance, structural differences between object categories can be modeled in the latent representation. The latent representation can model structural differences between object categories. By transforming the latent representation, properties of objects can be changed, as shown in Figure 8. Interpolating between different shapes in the latent space allows for visualizing structural and topological differences. The latent representation allows for interpolating shapes and creating morph-like sequences between them. It also enables morphing between shapes of different appearances and classes. Additionally, shape analogies can be found in the latent space. The latent space demonstrates the euclidean nature by finding \"analogous\" shapes through linear manipulations and nearest-neighbor searching. Shape analogies have been of recent interest in the geometry processing community. Preliminary results of point-cloud generators working with voxel-based AEs are included. We used a full-GMM model with 32 centers on ShapeNet's chair class, comparing grid resolutions of 32^3 and 64^3. Generated voxel-grids were converted into 2048 points by extracting a mesh and sampling points uniformly. Comparisons were made with established point-cloud generators. Using a full-GMM model with 32 centers on ShapeNet's chair class, grid resolutions of 32^3 and 64^3 were compared. Voxel-grids were converted into 2048 points by extracting a mesh and sampling points uniformly. Comparisons were made with established point-cloud generators. The latent AE-based GMM models outperform Wu et al.'s voxel-based GAN architecture by a significant margin, showing the advantage of using latent representations for generation. The performance of the 64^3 voxel-based GMM is comparable to the one at 32^3 resolution, indicating that high-frequency details in the ground-truth data do not significantly affect fidelity. Point-cloud-based models outperform voxel-based models in fidelity, as measured by the MMD. The coverage boost of voxel-based latent-space models compared to the MMD is likely due to the way the coverage metric is computed. The coverage metric is artificially increased by voxel-based models matching even poor quality shapes to ground truth. The histogram in FIG0 shows distances between GMM-generated samples and their closest matches in ground truth, with voxel-based methods showing a heavier \"tail\" indicating poor quality matchings. The voxel-based output covered models not captured by point-cloud output, mainly poor quality shapes. Volumetric models used GMMs with full covariances and different resolutions, with mesh conversion done using marching cubes algorithm. The voxel-based AEs use fully-convolutional encoders and decoders with specific layer parameters. The last decoder layer has a stride of 4 and does not use non-linearity. The model encoder has batch normalization and specific filter sizes. The voxel-based autoencoders were trained for 100 epochs using Adam optimizer with binary cross-entropy loss. The learning rate was 0.001, \u03b21 was 0.9, and batch size was 64. The reconstruction quality of the dense voxel-based AE was compared to the state-of-the-art method BID28 on the ShapeNetCars dataset using a 0.5 occupancy threshold. Reconstruction quality was measured by the intersection-over-union between input and synthesized voxel grids. The GMM-generator is compared against a model that memorizes the training data of the chair class by evaluating metrics between the \"memorized\" sets and the point-clouds of the test split. The coverage/fidelity obtained by the generative models is slightly lower when memorizing the training set, producing good coverage/fidelity with respect to the test set. The advantage of using a learned representation is in learning the structure of the underlying space, enabling compact data representation and generating novel shapes. Despite some mode collapse in generative results, the achieved MMD is almost identical to memorization, indicating excellent fidelity. More details are provided in Tables 10, 11, 12. In Tables 10, 11, 12 comparisons with BID32 for major ShapeNet classes are provided. Table 10 shows JSD-based comparisons for two models, while Table 12 shows MMD/Coverage comparisons on the test split. l-GAN uses the same adversarial objective as BID31. Generalization error of various GAN models is shown in FIG0. The generalization error of various GAN models is evaluated using JSD and MMD-CD metrics in FIG0. GMM model selection is also discussed, showing that models with full covariance achieve smaller JSD compared to diagonal covariance models. The text discusses the evaluation of GAN models using JSD and MMD-CD metrics, highlighting the importance of full covariance models for achieving smaller JSD. The GMM model selection process is also mentioned, with 30 or more clusters being deemed sufficient for minimal JSD. The text also includes a comparison of five generators on chair data, showcasing consistent model quality regardless of the selection metric used. The text discusses evaluating GAN models using JSD and MMD-CD metrics, emphasizing the importance of full covariance models for smaller JSD. GMM model selection with 30 or more clusters is sufficient for minimal JSD. A comparison of five generators on chair data shows consistent model quality regardless of the selection metric used. GMM-40-F represents a GMM with 40 Gaussian components with full covariances. Synthetic point-clouds are sampled for each model, measuring how well they match the ground truth in terms of MMD-CD, complementing a previous evaluation measure in FIG2."
}