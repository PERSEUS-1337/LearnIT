{
    "title": "rygDeZqap7",
    "content": "Natural language understanding research has shifted towards complex Machine Learning and Deep Learning algorithms, which outperform simpler models but require large labeled datasets. To address this, a methodology for expanding training datasets and training data-hungry models using weak supervision is proposed. This approach is applied to biomedical relation extraction, a task crucial for drug discovery but challenging due to the time and cost of creating training datasets. The methodology proposed involves enhancing LSTM network performance through weak supervision in biomedical relation extraction for drug discovery. Extracting structured information from the increasing number of scientific papers in the biomedical field can greatly impact tasks like drug design. The focus is on automating semantic triple extraction from biomedical abstracts, specifically on Regulations and Chemically Induced Diseases. This work aims to enhance LSTM network performance in biomedical relation extraction for drug discovery. The curr_chunk discusses the importance of extracting semantic triples related to Regulations and Chemically Induced Diseases (CID) in biomedical research for drug design and discovery. It explains the process of identifying entities of interest and building classifiers to recognize target relationships in unstructured text. The curr_chunk focuses on proposing a new methodology for relation extraction in biomedical research using weak supervision, combining ideas from semi-supervised and ensemble learning. The curr_chunk proposes a methodology for relation extraction in biomedical research using weak supervision. It involves training base learners on a small labeled dataset, predicting labels for a larger unlabeled dataset, deriving weak labels using a denoiser, and training a meta-learner with weak supervision. Key contributions include a detailed methodology specific to relation extraction, demonstrating effectiveness in a controlled experiment, and investigating denoising methods' impact on system behavior. The curr_chunk discusses the release of code for investigating denoising methods' effects on system behavior and performance. It also covers literature on information extraction, relation extraction from biomedical text, and semi-supervised and ensemble learning methods. Unsupervised methods like Open Information Extraction BID4 are highlighted for unstructured information extraction. The curr_chunk discusses semi-supervised methods like bootstrapping algorithms for information extraction, utilizing both labeled and unlabeled data. Other approaches include contextual data augmentation for training samples. Distant supervision is a method that generates weak labels for relation extraction using Knowledge Bases instead of pre-trained classifiers. This approach benefits from large-scale datasets without requiring human annotation. Our work complements distant supervision for weakly-labeled datasets in biomedical relation extraction. BioCreative competitions drive research in this area, with recent advancements showing improved performance by extending training sets using distant supervision. The BioCreative VI -CPR task focused on identifying relations between Chemicals and Proteins (or Genes) on a sentence level. The best performing team used an ensemble of LSTM, CNN, and SVMs, while the second highest score was achieved with Support-Vector Machines. Overfitting issues were observed with approaches solely using Deep Neural Networks, highlighting the importance of training data scarcity in this domain and the effectiveness of ensemble methods for generalization. Our work aims to combine ensemble methods with semi-supervised learning to improve generalization in Machine Learning models. Ensemble learning reduces high variance by combining multiple learners, while semi-supervised learning utilizes unlabeled data for better generalization. This approach has not been investigated for the task of improving generalization with Deep Neural Networks. Ensembles can enhance semi-supervised learning by providing multiple views, allowing the system to perform better with less data. Co-training was the first system proposed for this combination, where two independent learning algorithms leverage unlabeled data. Recent research has shown that independent learning algorithms can succeed without complete independence, by expanding on the underlying data distribution. Expert-defined lexicons and co-training are used to reduce noise and improve accuracy in functional genomics annotation, outperforming supervised methods without manually labeled data. State-of-the-art supervised methods trained on tens of thousands of annotated examples. Tri-training is an extension of co-training to three learners, where two learners teach the third learner when they agree on the label of a new data point. Co-forest extends this to more learners, with an ensemble system making decisions on re-training using all learners. The key difference is that the base learners in the ensemble system are only used to generate weak labels, not for final predictions. In the context of state-of-the-art supervised methods, weak labels are generated by base learners without re-training or improvement. This approach allows for the use of all unlabeled data, unlike other paradigms that only incorporate a few high-confidence annotated examples for re-training. Additionally, weak supervision and data programming have heavily influenced this methodology. Weak supervision and data programming have heavily influenced the development of the methodology. Weak supervision involves training models using labels of questionable quality, while data programming focuses on creating training sets programmatically when no ground-truth labels are available. This process involves defining weak supervision sources and encoding them into Labeling Functions (LF) for each unlabeled data point. In weak supervision, Labeling Functions (LF) can provide labels or abstain from voting, using various sources. The objective is to derive weak labels from a vote matrix \u039b, aiming to be close to true labels. Data programming uses a Generative Model for denoising, considering agreements and disagreements among LFs. The Generative Model (GM) in data programming estimates correlations of Labeling Functions to train without ground truth. Predicted label distributions are used as weak labels, and a noise-aware discriminative model is trained using these labels for final prediction. Based on weak supervision and data programming, a methodology for semi-supervised learning is proposed to utilize multiple learners. It suggests augmenting a gold-labeled training set with lower quality data to increase dataset size for training a complex model. Instead of relying on heuristics or crowd-sourced labels, machine learning models of lower complexity are used as weak supervision sources to scale the dataset size. The methodology requires a labeled training set, an unlabeled dataset drawn from the same distribution, a validation set for hyperparameter tuning, and a held-out test set for evaluation. In ensemble learning, 162 base learners are trained on a dataset D_B to solve a task T. Various design choices and hyperparameters are used to create diverse learners. One important design choice is sentence pruning, where only relevant words between entities of interest are kept. In ensemble learning, base learners are trained on a dataset to solve a task. Sentence pruning involves keeping only relevant words between entities of interest. Different approaches like whole sentences, window size, and dependency path are investigated. Sequential features include up to tri-grams. Text representation involves converting the corpus to numerical form. In our approach, we use up to tri-grams for sequential features. Text representation involves converting the corpus to a numerical form using token occurrences or TF-IDF weights. Machine learning algorithms employed include Logistic Regression, Support Vector Machines, Random Forest Classifiers, Long-Short Term Memory Networks, and Convolutional Neural Networks. Note that some feature engineering steps are not applicable when using LSTMs & CNNs. Base learners are produced and a subset is selected due to computational cost. To maximize base learners' performance and diversity, a subset is selected by discarding classifiers below a performance threshold, ensuring computational efficiency and avoiding similar classifiers in the disagreement-based method. This approach simplifies the complex issue of maximizing individual base learner performance and diversity. To enhance base learners' performance and diversity, a subset is chosen by discarding classifiers below a performance threshold. A similarity-based clustering method is used to select the most diverse classifiers, with a K-means clustering approach employed on a similarity matrix of base learners' predictions. The base learners closest to the cluster centroids are selected as representatives of their cluster. To determine the appropriate number of clusters, the silhouette score coefficient BID32 is used. The labels of DU are predicted using selected base learners to create a binary prediction matrix. A denoiser is then used to reduce the vote matrix into weak labels. Hyperparameters are selected using the validation dataset, and simpler denoisers like Majority Vote and Average Vote are considered to unify the label matrix. In practice, when training the meta-learner with weak supervision, label quality is traded for quantity. High-capacity models like Deep Neural Networks are used as meta-learners to learn their own features and build a more accurate representation by relying on a larger, albeit noisy, training set. In experiments using Snorkel, the BioCreative CHEMPROT and CDR datasets are utilized for relation extraction. The methodology requires three gold-labeled datasets and a held-out test set, with the original test sets used as the held-out test set. The original training and test sets are merged and shuffled for the experiments. The methodology involves using three gold-labeled datasets and a held-out test set for relation extraction. The original training and test sets are merged and shuffled to create different datasets for training, validation, and unlabeled data. This ensures no bias in document selection and that all datasets are drawn from the same distribution. The dataset used for relation extraction involves three gold-labeled datasets and a held-out test set. All documents undergo the same pre-processing steps to control the effect of choices on algorithm results. The text pre-processing pipeline is mainly handled by SpaCy (v1.0), an open-source NLP library. The text pre-processing pipeline for relation extraction involves tasks such as sentence splitting, tokenization, and dependency parsing using SpaCy (v1.0). Named Entity Tags are manually annotated in both datasets for candidate extraction. Snorkel is used for mapping candidates to their labels, and a relationship classifier is crucial for understanding Natural Language. In the text pre-processing pipeline for relation extraction, named entities are replaced with tokens for prediction. A bi-directional Long-Short Term Memory network is used for Natural Language tasks with randomly initialized word embeddings and under-sampling for class balance. In the text pre-processing pipeline for relation extraction, named entities are replaced with tokens for prediction. A bi-directional Long-Short Term Memory network is used with randomly initialized word embeddings and under-sampling for class balance. Research questions are formed based on different hyperparameter settings and validation dataset D V, aiming to enhance biomedical relation extraction using Machine Learning classifiers as weak supervision sources. The optimal setting for weak supervision on this task is also explored. Theoretical literature suggests that adding weakly labeled data can enhance the performance of the meta-learner BID30 in certain settings. Increasing the amount of weakly labeled data is expected to improve the meta-learner's performance quasi-linearly compared to using ground-truth labels. Key requirements for effective weak supervision include sources with better-than-random accuracy, diverse views of the problem, and sufficient overlap and disagreement for accurate estimation. Machine Learning classifiers have not been used as weak supervision sources in this setting before. The diversity and size of base learners trained on the same dataset are critical for the methodology's usability. Experiments are conducted to evaluate weak supervision's impact on performance. In experiments, the meta-learner's performance is compared when trained on different modes: full-supervision on D B, weak-supervision on D U, and weak-supervision on D U combined with full-supervision on D B. The study also assesses if weak-supervision can achieve results comparable to full-supervision after training the meta-learner using all ground-truth labels (D U + D B). Selecting the optimal number of base learners is crucial, as adding more learners may sacrifice performance for diversity. The denoising component is crucial for training the final learner on weak labels. Three denoising methods are used to assess label quality, producing binary or marginal weak labels with varying distributions. Error analysis is conducted to investigate the results. The study investigates the effect of different denoising methods on training a meta-learner using weak labels. The research answers two key questions: whether supervised machine learning classifiers can be used as weak classifiers and what is the optimal setting for applying weak supervision. The selection of base learners is based on a specific strategy, and experiments are conducted with varying numbers of base learners to maximize silhouette scores. Performance results of base learners and weak labels/marginals are reported. Training the meta-learner with weak labels from the denoisers shows improved performance compared to using fewer gold labels. Including ground-truth labels further enhances performance, demonstrating successful augmentation of training data through weak supervision. Using weak supervision can achieve performance comparable to full supervision, sometimes even slightly better. Differences in results are minor and not statistically significant due to high variance in meta-learners' performance. Under-sampled training sets in weak supervision were larger, based on weak labels instead of real ones. The study shows that weak supervision, using weak labels instead of real ones, can achieve comparable performance to full supervision. Majority Vote often outperforms the meta-learner, but this does not diminish the significance of the results. Visualizing the learning curves of the meta-learner confirms that weak labels improve performance, with statistically significant results indicated by confidence intervals. The study indicates that weak supervision with weak labels can achieve similar performance to full supervision. Majority Vote sometimes performs better than the meta-learner. The F1 score on the training set is consistently higher than on the test set, suggesting overfitting. Additional training data is needed to improve the meta-learner's performance. The small dataset size and complexity of the problem limit definitive conclusions. The F1 score of weak Majority Vote labels for 5 learners is the lowest. The meta-learner performs better with more than 10 base learners. Using Generative model marginals, performance slightly improves with more base learners, except for two exceptions. In most cases, the metalearner achieves the best performance. The metalearner achieves the best performance with Average Marginals, and Generative Model marginals also improve its performance compared to Majority Vote weak labels, except for one case. GM marginals depend on hyperparameters chosen based on F1 score on a validation dataset, but this measure may not fully reflect performance under different weak label distributions. Denoisers can produce binary or non-binary weak labels, with marginal weak labels enhancing the meta-learner's performance over binary labels. The metalearner performs best with Average Marginals, and Generative Model marginals improve performance compared to Majority Vote weak labels. Marginals follow a U-shaped distribution, unlike average marginals. Error analysis shows similar misclassification rates for weak labels, but Average Vote labels are more effective. The F1 score is not suitable for evaluating marginal weak labels. Training loss and validation scores change as the LSTM is trained for more epochs, with higher error when marginal labels are used. Average weak marginals spread out, while binary prediction starts early. The LSTM quickly predicts binary training labels accurately despite a delay with noisy-labeled MV weak labels. Training with marginal labels is treated as a regression problem, penalizing the model for inaccurate predictions. The predicted logits become more spread out as training marginals become more uniform, resembling regression rather than classification. The methodology applied on the CPR task involved expanding datasets and training base learners using CHEMPROT documents. The addition of weakly labeled data led to a decrease in meta-learner performance, indicating issues with data quality. A class imbalance was observed in the outgoing citations dataset compared to the original, suggesting different dataset distributions. Validation using the t-SNE algorithm confirmed these findings. The t-SNE algorithm BID20 was used with features from the best-performing base-learner to visualize candidate samples from the original set versus samples from both sets. Most candidates from the new dataset were found in specific regions of the 2D space, indicating unsuitability for the use case. Weak supervision can enhance complex model performance by utilizing unlabeled data and multiple base learners. Further investigation into constructing appropriate unlabeled datasets is necessary. The proposed methodology utilizes unlabeled data and multiple base learners to model the problem space effectively. By shifting human effort from hand-labeling to feature engineering, diverse learners can be constructed to take advantage of additional data. It is crucial that the unlabeled data come from the same domain as the labeled data for the base learners to generalize and perform well. By utilizing unlabeled data and multiple base learners, the method aims to scale training datasets effectively while improving performance. It eliminates the need for hand-labeling large datasets and can be reused for similar tasks with appropriate datasets. Further exploration is needed to construct a large unlabelled dataset for improved experimentation. Further exploration is needed to construct a large unlabelled dataset for improved experimentation and to draw stronger conclusions on the research questions. Preliminary experiments show that collecting an appropriate unlabeled dataset is challenging, along with defining what is considered \"appropriate\". Further, it is crucial to find a more suitable metric than the F1 score for evaluating marginal weak labels. The absence of such a metric hinders drawing direct conclusions from weak labels without the need for an additional step like training the meta-learner. This could impact the selection of optimal hyperparameters for the Generative Model and overall performance. Additionally, exploring the use of a meta-learner and pre-trained word embeddings is suggested for further investigation. Further investigation could involve experimenting with the meta-learner and defining a more appropriate selection method for Base Learners. It would also be interesting to explore how the system would perform if Base Learners abstained from voting on uncertain examples. This could provide a modeling advantage for the Generative Model compared to unweighted methods like Majority Voting. The analysis related to weak supervision BID29 highlights the modeling advantage of a Generative Model over unweighted methods like Majority Voting."
}