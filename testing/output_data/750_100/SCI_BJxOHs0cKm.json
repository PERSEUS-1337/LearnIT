{
    "title": "BJxOHs0cKm",
    "content": "Model generalization is linked to local properties of the optima, described by the Hessian. The PAC-Bayes paradigm connects generalization ability to the Hessian, Lipschitz constant, and parameter scales. A metric is proposed to score generalization capability and an algorithm is suggested to optimize the model. Deep models are effective in applications like computer vision. Deep models, used in computer vision, speech recognition, and natural language processing, often have millions of parameters but still generalize well. Classical learning theory suggests that model generalization is related to the complexity of the hypothesis space, measured by parameters, Rademacher complexity, or VC-dimension. However, over-parameterized models have been observed to generalize well on test data, despite a complex hypothesis space. The generalization capability of deep models is related to the simplicity of the final solution learned from training data. Metrics like the spectrum of the Hessian matrix and sharpness of the solution are empirically linked to model generalization. Large eigenvalues of the Hessian matrix often lead to poor generalization. The sharpness metric and generalization are empirically linked. Hessian-based sharpness measures may not directly explain generalization. Re-parameterization can drastically modify the geometry of parameters in RELU-MLP. Bayesian analysis uses Taylor expansion to approximate the posterior, with the Hessian of the loss function evaluating model simplicity. BID34 penalizes sharp minima using the Occam factor and determines optimal batch size. BID4 connects these concepts. The PAC-Bayes bound is used to penalize sharp minima and determine optimal batch size. It connects to Bayesian analysis and provides an alternative perspective on Occam's razor. It has been used to analyze generalization behavior in deep models, showing that even though sharp minima approximate true labels better, they have complex structures in predicted labels compared to flat minima. In this paper, the authors explore the relationship between model generalization and the local \"smoothness\" of a solution from a PAC-Bayes perspective. They propose using the difference between perturbed loss and empirical loss as a sharpness metric and optimizing the PAC-Bayes bound for better model generalization. Fundamental questions regarding this relationship remain unanswered. The authors investigate the connection between model generalization and the Hessian of the loss function in a PAC-Bayes framework. They introduce a new metric for generalization and propose a perturbation-based algorithm that utilizes the estimation of the Hessian to enhance model generalization. The PAC-Bayes paradigm involves supervised learning with probability measures over a function class F. It aims to minimize expected loss by considering both random samples and functions drawn from the posterior distribution. The theory suggests a bound on the gap between expected and empirical loss. The PAC-Bayes paradigm involves minimizing the gap between expected and empirical loss by considering perturbations in the function class F. The bound is related to the KL divergence between distributions and connects generalization with local properties. The perturbation bound FORMULA4 connects generalization with local properties around the solution w through a small perturbation u. It is important to find an optimal perturbation level for u to minimize the bound. Researchers have found that the generalization ability of models is related to second-order information around local optima. In this section, the focus is on connecting the Hessian matrix with model generalization by introducing the local smoothness assumption and a main theorem. The paper defines a neighborhood set around a reference point, emphasizing a particular type of radius but noting that other types are also applicable. The goal is to control the perturbation level to optimize generalization around local optima. The paper introduces the concept of Hessian Lipschitz condition to model the smoothness of second-order gradients in numeric optimization. It assumes a convex function that is \u03c1-Hessian Lipschitz. The focus is on controlling perturbation levels to optimize generalization around local optima. Theorem 2 states that with careful selection of perturbation levels, the expected loss of a uniformly perturbed model can be controlled. The bound is influenced by the Hessian diagonal element, Lipschitz constant, neighborhood scales, number of parameters, and number of samples. The perturbation level is inversely related to \u2207 2 i,iL. The perturbation level is inversely related to \u2207 2 i,iL, suggesting the model be perturbed more along the coordinates that are \"flat\". The empirical loss function satisfies the local Hessian Lipschitz condition, bounding perturbations around a fixed point by terms up to the third-order. For perturbations with zero expectation, the linear term is zero. The linear term in (5) is simplified to E u [\u2207L(w) T u] = 0, and the second order term is also simplified. The \"posterior\" distribution of model parameters is uniform, with bounded perturbed parameters. The third order term in (6) is bounded as well. The linear term in (6) is bounded by DISPLAYFORM3. The bound in theorem 2 does not explain over-parameterization phenomenon. Lemma 3 states that the loss function is bounded and model weights are constrained. With probability at least 1 - \u03b4, for any w * \u2208 R m, DISPLAYFORM5 holds. The spectrum of \u2207 2L is not sufficient to determine generalization power for a multi-layer perceptron with RELU activation. Re-parameterizing the model and scaling the Hessian spectrum does not affect model prediction and generalization when using cross entropy. Theorem 2 details the optimization of \u03b7 as a hyper-parameter, with proof in Appendix C and D. The bound on model prediction and generalization is not dependent on cross entropy loss or assuming the model is RELU-MLP. The optimal perturbation levels scale inversely with parameters, leading to a logarithmic change in the bound. Lemma (3) shows that key factors are logarithmic, affecting RELU-MLP models. The text discusses heuristic-based approximations and empirical observations inspired by the bound on model prediction and generalization. It introduces a PAC-Bayes based Generalization metric called pacGen, which assumes local convexity in the metric. The text introduces a metric called pacGen, assuming local convexity. It discusses calculating the metric on real-world data by estimating diagonal Hessian elements and Lipschitz constant. The approach follows Adam (Kingma & Ba, 2014) for efficiency. The text discusses the use of a model without dropout from a PyTorch example, varying batch sizes for training, and observing the trend of the gap between test and training loss. It also introduces a metric \u03a8 \u03ba (L, w * ) and compares results with previous observations. Additionally, experiments are conducted by fixing batch size and varying learning rate, showing the generalization gap and \u03a8 \u03ba (L, w * ) as a function of epochs. The text discusses the generalization gap and \u03a8 \u03ba (L, w * ) trends with decreasing learning rate. It also mentions the success of adding noise for better generalization in models. The text introduces perturbing model weights based on the PAC-Bayes bound to optimize the perturbed empirical loss for better model generalization. The algorithm uses exponential smoothing to estimate the Hessian. In applications, the gradient \u2207L \u00b7 u may not be zero, especially with only 1 trial of perturbation. In Algorithm 1, parameters with small gradients below \u03b2 2 are perturbed using per-parameter \u03c1 i. Perturbation level decreases with epoch. Results on CIFAR-10, CIFAR-100 BID17, and Tiny ImageNet 8 are compared using Wide-ResNet BID36 model with depth 58 and widen-factor 3. The chosen model has a depth of 58 and a widen-factor of 3. Dropout layers are disabled. Adam is used with a learning rate of 10^-4 and a batch size of 128 for CIFAR-10 and CIFAR-100. Perturbation parameters include \u03b7 = 0.01, \u03b3 = 10, and =1e-5. For Tiny ImageNet, SGD is used with a learning rate of 10^-2 and a batch size of 200. Perturbed SGD parameters are set as \u03b7 = 100, \u03b3 = 1, \u03b2 1 = 0.999, \u03b2 2 = 0.1, =1e-5. The optimizer used for ImageNet is OPT(w t), while Adam is used for CIFAR with a learning rate of 10^-4 and SGD for Tiny ImageNet with a learning rate of 10^-2. Dropout rate is 0.1 for comparison. Perturbation effect is similar to regularization, decreasing training set accuracy but increasing validation set accuracy. PerturbedOPT performs better than dropout by applying different levels of perturbation. The perturbation effect in PerturbedOPT is better than dropout as it applies different levels of perturbation based on local smoothness structures, improving model generalization in the PAC-Bayes framework. The generalization power is linked to the Hessian, smoothness of the solution, parameter scales, and training samples. The best perturbation level scales inversely with the square root of the Hessian, mitigating scaling effects. This work integrates the Hessian in the model generalization bound, explaining the effect of re-parameterization. A new metric and perturbation algorithm are proposed based on the generalization bound. Empirical results show the algorithm acts as a regularizer, improving performance on unseen data. The toy example in FIG0 demonstrates a 2-dimensional sample set from 3 Gaussians, with binarized labels. A 5-layer MLP model with shared weights and no bias terms is used, trained on 100 samples with only two free parameters w1 and w2. The model is trained using 100 samples and the loss function is plotted with respect to the model variables w1 and w2. Many local optima are observed in this two-dimensional toy example, including a sharp one and a flat one. The colors on the loss surface represent the values of the generalization metric scores (pacGen), with smaller values indicating better generalization power. The metric score around the global optimum is high, suggesting possible poor generalization capability compared to the local optimum. The model trained with 100 samples shows various local optima, including a sharp and a flat one. The colors on the loss surface represent generalization metric scores. The local optimum indicated by the red bar has a similar overall bound compared to the \"sharp\" global optimum. The sharp minimum approximates the true label better but has some complexity. The sharp minimum approximates the true label better but has complex structures in its predicted labels. Truncation of the Gaussian distribution is necessary due to the bounded perturbation requirement. The coefficients are bounded and a prior \u03c0 as N(0, \u03c4 I) is chosen. After choosing the prior \u03c0 as N(0, \u03c4 I), the bound was approximated with \u03b7 = 39 using inequality (8). Truncation of the Gaussian distribution results in a smaller variance. When L(w) is convex around w * with \u2207 (w * ) \u2265 0, the best \u03c3 i can be solved for. For any \u03b4 > 0 and \u03b7, with probability at least 1 \u2212 \u03b4 over the draw of n samples, for any w * \u2208 R m such that assumption 1 holds, random variables are distributed as truncated Gaussian. After approximating the bound with \u03b7 = 39, the Gaussian distribution is truncated to reduce variance. When L(w) is convex around w * with \u2207 (w * ) \u2265 0, the optimal \u03c3 i can be determined. Random variables follow a truncated Gaussian distribution with probability at least 1 \u2212 \u03b4 over n samples, satisfying assumption 1. The proof involves optimizing \u03b7 as a hyper-parameter instead of over a grid. The proof involves optimizing \u03b7 as a hyper-parameter over a grid to build a form for a given value of i log \u03c4\u01d0 \u03c3i. The eigenvalues of \u2207 2L (w) are related to the generalization ability of the model, as observed in empirical studies. The eigenvalues of \u2207 2L (w) are related to the generalization ability of the model. Lemma 5 states that with probability at least 1 \u2212 \u03b4 over n samples, for any local optimal w * such that \u2207L(w * ) = 0, L(w) satisfies the local \u03c1-Hessian Lipschitz condition in N eigh \u03ba (w * ) for any random perturbation u. Lemma 5 states that at a local optimal point where \u2207L(w*) = 0, the first order term is zero even if E[u] = 0. The quadratic term in the inequality FORMULA10 is bounded by DISPLAYFORM4 due to extrema of the Rayleigh quotient. The section includes figures comparing dropout and a proposed perturbation algorithm, with dropout being a multiplicative perturbation using Bernoulli distribution. Results are presented using wide ResNet architectures with dropout layers turned on or off. Accuracy is reported for different dropout rates on CIFAR-10, CIFAR-100, and Tiny ImageNet. The chosen wide ResNet model has a depth of 58 and a widenfactor of 3. Different optimization algorithms and parameters are used for each dataset. The study used wide ResNet architectures with dropout layers on CIFAR-10, CIFAR-100, and Tiny ImageNet. Results showed that adding dropout improved validation/test accuracy. A dropout rate of 0.3 worked best for CIFAR-10, while 0.1 was better for CIFAR-100 and Tiny ImageNet, possibly due to the need for more regularization in datasets with fewer training samples. The perturbed algorithm outperforms dropout in experiments on validation/test data sets, possibly due to its ability to apply varying levels of perturbation based on local smoothness structures."
}