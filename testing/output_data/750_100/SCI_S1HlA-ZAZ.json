{
    "title": "S1HlA-ZAZ",
    "content": "An end-to-end trained memory system inspired by Kanerva's sparse distributed memory quickly adapts to new data and generates similar samples. It has a robust distributed reading and writing mechanism, analytically tractable memory, and is formulated as a hierarchical conditional generative model. The memory provides a rich data-dependent prior distribution, improving generative models trained on Omniglot and CIFAR datasets. Our memory model enhances generative models trained on Omniglot and CIFAR datasets, surpassing the capacity of Differentiable Neural Computers (DNC) and its variants. The challenge of efficiently utilizing memory in neural networks remains unresolved, with DNCs often collapsing reading and writing into single slots, hindering information sharing across memory. The challenge of efficiently utilizing memory in neural networks remains unresolved. Different memory strategies are used by various models, such as storing embeddings directly or summarizing datasets through averaging. Each approach has its trade-offs in terms of memory efficiency and information retention. Historically developed associative memory architectures like the Hopfield Net and Boltzmann Machine provide insight into efficient memory structures. The Hopfield Net stores patterns in low-energy states but has limited capacity due to recurrent connections. The Boltzmann Machine introduces latent variables to lift this constraint but requires slow reading and writing mechanisms. Kanerva's sparse distributed memory model BID15 offers fast reads and writes, addressing the issue of slow reading and writing mechanisms. A conditional generative memory model inspired by Kanerva's model introduces learnable addresses and reparametrised latent variables. The model solves the challenge of learning an effective memory writing operation by leveraging the analytic tractability of the memory model. Our proposal introduces a Bayesian memory update rule for efficient memory writing, utilizing a hierarchical generative model with a memory-dependent prior. This model quickly adapts to new data, providing top-down knowledge in addition to bottom-up perception. It enriches priors in VAE-like models through an adaptive memory system, offering effective online distributed writing for compression and storage. Our memory architecture extends the variational autoencoder (VAE) by using an adaptive memory store. The VAE has observable variable x and latent variable z, with generative model specified by prior distribution p \u03b8 (z) and conditional distribution p \u03b8 (x|z). Parameterised inference model q \u03c6 (z|x) approximates the intractable posterior p \u03b8 (z|x).\u03b8 represents generative model's parameters, and \u03c6 represents inference model's parameters, implemented as multivariate Gaussian distributions. The VAE uses multivariate Gaussian distributions for its parameters and aims to maximize log-likelihood by optimizing \u03b8 and \u03c6. The objective is to reconstruct x using its approximated posterior sample from q \u03c6 (z|x). The model introduces an exchangeable episode concept for training, focusing on expected conditional log-likelihood. It factors the joint distribution into marginal distribution and posterior for efficient computation. The model introduces an exchangeable episode concept for training, focusing on expected conditional log-likelihood. It factors the joint distribution into marginal distribution and posterior for efficient computation. This scenario is a way of formulating memory-based generative models, maximizing mutual information between memory and the episode to store. The latent variables corresponding to the observed episode are denoted as Y and Z. The joint distribution of the generative model can be factorized. The joint distribution of the generative model can be factorized using a matrix variate Gaussian distribution, with memory M represented by matrices R, U, and V. This distribution is equivalent to a multivariate Gaussian distribution of vectorized M, assuming independence between columns. The Kronecker product operator \u2297 is used to represent the independence between columns of matrix M. The addresses A are randomly initialized and optimized through back-propagation. The addressing variable yt computes weights for memory access, following an isotropic Gaussian distribution. VAEs use an isotropic Gaussian distribution as the prior for yt. A learned projection transforms yt into a key vector, and weights wt across rows of M are computed. The projection is implemented as a multi-layer perception to potentially address non-Gaussian distributions. The code zt generates samples of xt through a parametrised conditional distribution. zt has a memory dependent prior instead of an isotropic Gaussian prior. The memory-dependent prior for zt in the hierarchical model captures richer marginal distribution by incorporating memory and addressing variables. Global latent variable M captures statistics of the entire episode, while local latent variables yt and zt capture local statistics for data xt within an episode. Episodes of length T are generated by sampling M once, followed by sequential sampling of yt, zt, and xt for each sample. The approximated posterior distribution is factorised using conditional independence, refining the prior distribution with additional evidence. The parameterised posterior takes input from x t and the mean of the prior distribution. Constant variance is omitted, and the generative model is shared for all samples. The trade-off in updating memory is balancing old and new information optimally through Bayes' rule. Memory writing is seen as inference, computing the posterior distribution of memory. Batch and online inference methods are considered, with the posterior distribution of memory approximated using one sample of y t, x t. The posterior distribution of memory is computed by balancing old and new information through Bayes' rule. Memory writing is viewed as inference, with the posterior of addressing and code variables parameterized. The posterior of memory in the linear Gaussian model is analytically tractable, with parameters R and U being updatable. The posterior distribution of memory in the linear Gaussian model can be updated using Bayes' rule. The parameters R and U are updated based on the prediction error, cross-covariance matrix \u03a3 c, noise variance matrix \u03a3 \u03be, and covariance matrix \u03a3 z. The prior parameters R 0 and U 0 are trained through back-propagation to learn the general structure of the dataset, while the posterior adapts to features in the data subset. The update rule in the linear Gaussian model involves inverting \u03a3 z, with a complexity of O(T^3). Online updating can reduce the cost by using one sample at a time. Updating the entire episode at once is equivalent to iterative one-sample updates. Intermediate updates can be done using mini-batches with sizes between 1 and T. Storage and multiplication are also major costs in the update rule. The update rule in the linear Gaussian model involves inverting \u03a3 z with a complexity of O(T^3). Storage and multiplication of the memory's row-covariance matrix U is another major cost, with complexity O(K^2). Restricting the covariance to diagonal can reduce the cost to O(K), but experiments show it is useful for memory accessing coordination. Training the model involves optimizing a variational lower-bound of the conditional likelihood J. Future work may explore low-rank approximation of U for better cost-performance balance. To maximize the lower bound J, y t and z t are sampled from q \u03c6 (y t , z t |x t , M ). A mean-field approximation using the mean R is employed for computational efficiency in place of memory samples. The first term inside the bracket represents the VAE reconstruction error, while the first KL-divergence penalizes complexity. The Gaussian distribution's analytical tractability allows for distribution-based reading and writing operations. The first term in the current chunk is the VAE reconstruction error, with the first KL-divergence penalizing complexity. The memory-based prior penalizes deviation of the code z t. Kanerva's sparse distributed memory features an iterative reading mechanism that decreases errors through feedback iterations, converging to a stored memory. The iterative process in the model involves feeding back reconstructionx t, with Gibbs-like sampling following a loop in FIG0. Iterative reading improves denoising and sampling by using knowledge about memory. Training a parameterized model with the whole matrix M as input can be costly, but intractable posteriors can be addressed using q \u03c6 (y t |x t , M). The model involves iterative reading to improve denoising and sampling by utilizing memory knowledge. Intractable posteriors in non-tree graphs can be efficiently approximated using loopy belief-propagation. Future research will focus on understanding this process further. Model implementation details are provided in Appendix C, using simple encoder and decoder models for evaluating improvements. The study focuses on evaluating the benefits of an adaptive memory in encoder and decoder models. The experiments use the same model architecture with variations in filters, memory size, and code size. The Adam optimizer is used with minimal tuning. Results are reported as the variational lower bound divided by the episode length for comparison with existing models. The Omniglot dataset is initially used for testing the model. The study evaluates the benefits of an adaptive memory in encoder and decoder models using the Omniglot dataset. A 64 \u00d7 100 memory M and a 64 \u00d7 50 address matrix A are utilized. 32 images are randomly sampled to form an \"episode\" without class labels. The variational lower-bound is optimized using Adam with a mini-batch size of 16. For the CIFAR dataset, a mini-batch size of 16 is used with Adam optimization and a learning rate of 1 \u00d7 10 \u22124. The model is tested in an unsupervised setting without label information. Convolutional coders with 32 features at each layer, a code size of 200, and a 128 \u00d7 200 memory with a 128 \u00d7 50 address matrix are used to handle the increased complexity. The experiment starts with the 28 \u00d7 28 binary Omniglot dataset, following the same split of training and test examples. The training process of the Kanerva Machine model is compared with a baseline VAE model using the same encoder and decoder. There is a modest increase in parameters in the Kanerva Machine. The learning curves show that the model has learned to use memory effectively. The Kanerva Machine model outperformed the VAE on the Omniglot dataset, showing better reconstruction and KL-divergence. The model learned to use memory effectively, with the KL-divergence sharply decreasing around the 2000th step, indicating a more informative prior induced by memory. The rich prior from memory q \u03c6 (z t |y t , M ) provides key information for the code, with a lower KL-divergence compared to VAE. Training curves for CIFAR show similar patterns. Reduction in KL-divergence, rather than reconstruction loss, improves sample quality. VAE reached NLL of \u2264 112.7 at the end of training. The Kanerva Machine achieves a conditional NLL of 68.3, demonstrating the power of incorporating an adaptive memory into generative models. The results are not directly comparable to unconditional generative models due to the model's advantage of memory contents. The NLL improvement showcases the effectiveness of this approach. The Kanerva Machine demonstrates the effectiveness of incorporating an adaptive memory into generative models, achieving a conditional NLL of 68.3. The model's weights are well distributed over the memory, showing patterns superimposed on others. The reconstruction process involves denoising through iterative reading, with one-shot generation capabilities. The Kanerva Machine shows improved sample quality in consecutive iterations when tested on batches of images with many classes and samples. Initial samples are comparable to a VAE, but final samples reflect the conditioning patterns' statistics. The final samples from the Kanerva Machine show convergence after the 6th iteration, reflecting conditioning patterns' statistics. Unlike VAEs, the iterative sampling approach does not apply to VAEs as shown in Figure 3. Samples from CIFAR dataset contain a mix of classes and are compared with VAE samples which appear blurred. Samples from the Kanerva Machine show clear local structures after several iterations, even when input images are corrupted. The model can recover the original image through iterative reading, demonstrating generalization capabilities. The model's structure allows for interpretability of internal representations in memory. Linear interpolations between address weights are meaningful, as shown by examining interpolations between two weight vectors from random input images. Interpolating access weights produces meaningful results, as demonstrated in FIG2 in Appendix A. The training curves of DNC and Kanerva machine show differences in sensitivity to initialization and speed. Test variational lower-bounds of DNC and Kanerva Machine are compared with Differentiable Neural Computer (DNC) and Least Recently Used Architecture (LRUA). The DNC and Kanerva Machine were compared in a storage and retrieval task with Omniglot data. The DNC had a test loss close to 100 but was sensitive to hyper-parameters, while the Kanerva Machine was robust. The LRUA did not pass the loss level of 150 and was not included in the comparison. The Kanerva Machine showed robustness to hyper-parameters, performing well with batch sizes between 8 and 64 and learning rates between 3 \u00d7 10 \u22125 and 3 \u00d7 10 \u22124. It trained fastest with a batch size of 16 and learning rate of 1 \u00d7 10 \u22124, converging below 70 test loss with all configurations. This ease of training is attributed to principled reading and writing operations independent of model parameters. The Kanerva Machine, a novel memory model, demonstrates robustness to hyper-parameters and generalizes well to larger episodes, outperforming the DNC in terms of variational lower-bound. It can exploit redundancy in episodes with fewer classes, resulting in lower reconstruction losses. The Kanerva Machine is a memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model. It removes the assumption of a uniform data distribution by training a generative model to learn the observed data distribution. Memory is implemented as a generative model, allowing retrieval of unseen patterns through sampling, consistent with constructive memory neuroscience experiments. Probabilistic interpretations of Kanerva's model have been explored in previous works. The model discussed in the current chunk generalizes Kanerva's memory model to continuous, non-uniform data while integrating with deep neural networks. It is the first to do so while maintaining an analytic form of Bayesian inference. Other models have combined memory mechanisms with neural networks in a generative setting, such as using attention to retrieve information from trainable parameters. BID19 and BID5 are models that incorporate memory mechanisms with neural networks. BID19 uses attention to retrieve information from trainable parameters in a memory matrix, but the memory is not updated post-learning, limiting its adaptability. On the other hand, BID5 utilizes discrete random variables to address an external memory, storing images as raw pixels for fast adaptation, albeit at a high cost for large datasets. Our model efficiently stores information in a compressed form by leveraging statistical regularity in images through the encoder, learned addresses, and Bayes' rule for memory updates. It employs an exact Bayes' update-rule without compromising neural network flexibility. The model's performance and scalable architecture suggest a combination of classical statistical methods. The model efficiently stores information in a compressed form by leveraging statistical regularity in images through the encoder, learned addresses, and Bayes' rule for memory updates. Kanerva's sparse distributed memory model is characterized by distributed reading and writing operations, with fixed table of addresses A pointing to a modifiable memory M. Kanerva's sparse distributed memory model uses fixed addresses A sampled from {\u22121, 1} D to compare inputs y with Hamming distance. Addresses are selected based on threshold \u03c4, and patterns are stored in memory M using binary weight vector. The sparse distributed memory model by Kanerva uses fixed addresses to compare inputs with Hamming distance. Patterns are stored in memory by adding weighted vectors, allowing for correct retrieval even with over-written content. Additionally, corrupted queries can still be discovered from the memory. Kanerva's model allows for the discovery of corrupted queries from memory through iterative reading. However, its application is limited by the assumption of a uniform and binary data distribution, which is rarely true in real-world scenarios. This is because real-world data often lie on low-dimensional manifolds, making binary representation less efficient in high-level neural network implementations optimized for floating-point numbers. Our model architecture, as shown in FIG4, differs from a standard VAE by using a convolutional encoder for input image conversion in all experiments. The model architecture, shown in FIG4, deviates from a standard VAE by utilizing a convolutional encoder to convert input images into 2C embedding vectors. The encoder consists of 3 blocks with convolutional layers and a ResNet block, followed by linear projection to a 2C dimensional vector. The convolutional decoder mirrors this structure with transposed convolutional layers. The model architecture in FIG4 uses transposed convolutional layers. Adding noise to the input into q \u03c6 (y t |x t ) helps stabilize training. Gaussian noise with zero mean and standard deviation of 0.2 is used for all experiments. Bernoulli likelihood function is used for Omniglot dataset, and Gaussian likelihood function for CIFAR. Uniform noise U(0, 1 256 ) is added to CIFAR images during training to avoid Gaussian likelihood collapsing. The differentiable neural computer (DNC) is wrapped with the same interface as the Kanerva for fair comparison. The differentiable neural computer (DNC) is integrated with the Kanerva memory interface for a fair comparison. The DNC receives addressing variable y t and input z t during writing, with separated reading and writing stages in experiments. In experiments, a 2-layer MLP with 200 hidden neurons and ReLU nonlinearity is used as the controller in the DNC instead of LSTM to prevent interference with external memory. The issue of controllers bypassing memory output is addressed by ensuring the DNC only reads-out from its memory. To focus on memory performance, Figure 10 is removed. Test loss is compared between models using full covariance matrix and diagonal covariance matrix. Models with full covariance matrices were slower per-iteration but had quicker test loss decrease. The bottom-up stream in the model compensates for memory. The model compensates for memory by directly sampling z t from p \u03b8 (z t |y t , M ) during training, leading to quicker test loss decrease. The small difference in KL-divergence significantly impacts sample quality during CIFAR training. The Kanerva Machine shows increasing advantage over VAE in training. Linear Gaussian model defined by Eq. 6. Posterior distribution derived using conditional formula for Gaussian. Update rule in eq. 9 to 11 based on matrix variate Gaussian distribution properties. The Kanerva Machine outperforms VAE in training. The model described in the paper utilizes samples from q \u03c6 (z t |x t ) for writing to memory and mean-field approximation during reading. An alternative approach fully exploits the analytic tractability of the Gaussian distribution, using parameters \u03c8 = {R, U, V} for memory operations."
}