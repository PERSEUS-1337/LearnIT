{
    "title": "SJeQi1HKDH",
    "content": "Animals develop new skills through environmental interaction and social influence. This study incorporates social influence into reinforcement learning, allowing agents to learn from peers. A metric is defined to measure policy distance and uniqueness is quantitatively derived. The Interior Policy Differentiation (IPD) algorithm encourages agents to learn unique policies while solving the primary task. The Interior Policy Differentiation (IPD) algorithm improves performance by generating diverse policies to solve tasks. Reinforcement Learning (RL) involves learning through interaction with the environment to maximize rewards. Behavioral diversity is essential for species evolution and development of skills. Previous works have focused on promoting the emergence of diverse behaviors in RL. Diversity is a growing topic in RL. Two approaches to encourage behavioral diversity in RL are designing interactive environments with richness and diversity, and motivating agents to explore beyond maximizing rewards. In this work, the concept of policy differentiation in RL is addressed to improve the diversity of agents while maintaining their ability to solve tasks. Inspired by social influence in animal societies, a learning scheme is proposed where agents interact with the environment to maximize rewards. The target agent in Fig 1 learns to maximize rewards and differentiate actions from other agents using social uniqueness motivation. A policy distance metric is defined to compare agent similarity, and an optimization constraint is developed for immediate feedback in learning. Interior Policy Differentiation (IPD) is a novel method proposed for constrained policy optimization, bringing immediate feedback in the learning process. It encourages the target agent to perform well in the primal task while also taking different actions from other agents. This method has been benchmarked on locomotion tasks, showing the ability to learn diverse and well-behaved policies. The Variational Information Maximizing Exploration (VIME) method, along with curiosity-driven methods, adds intrinsic rewards to RL algorithms to encourage exploration and learn diverse policies. Random Network Distillation (RND) and Competitive Experience Replay (CER) are two methods that quantify intrinsic rewards in RL algorithms by leveraging prediction differences and state coincidence, respectively. These approaches aim to improve prediction accuracy when using previous state information. The Task-Novelty Bisector (TNB) learning method introduced by Zhang et al. (2019) aims to optimize external and intrinsic rewards by updating the policy in the direction of the angular bisector of the two gradients. However, the foundation of this joint optimization is not solid. The Distributed Proximal Policy Optimization (DPPO) method introduced by Heess et al. (2017) enables agents to learn complex locomotion skills in diverse environments. Despite the straightforward learning reward used, the skills learned by their policy are impressive and effective in navigating terrains and obstacles. The research by Such et al. (2018) demonstrates that different RL algorithms can lead to varying policies for the same task. Policy gradient algorithms tend to converge to a local optimum in Pitfall, while off-policy and value-based algorithms learn more sophisticated strategies. This paper focuses on learning different policies through a single algorithm. In the study by Schulman et al. (2015) and Kurutach et al. (2018), a single learning algorithm is used to learn various policies and avoid local optima. Model uncertainty is maintained through an ensemble of deep neural networks. A metric is defined to measure policy differences, laying the groundwork for the proposed algorithm. Learned policies are denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}, with \u03b8 i representing parameters and \u0398 the parameter space. The policies are simplified as \u03b8 i for convenience. A metric space is defined as an ordered pair (M, d) where d is a metric on M satisfying identity, symmetry, and triangle inequality properties. The Total Variance Divergence (D TV) is used to measure policy distance, with (\u0398, D TV) being a metric space. RL can be extended to continuous state and action spaces by replacing sums with integrals. The factor 1/2 in Schulman et al. (2015) is omitted for conciseness. The method aims to maximize the uniqueness of a new policy by calculating D \u03c1 TV(\u03b8i, \u03b8j) based on Monte Carlo estimation. Sampling from \u03c1(s) is necessary, and problems arise in continuous state spaces. In continuous state cases, efficiently obtaining enough samples for estimation after establishing ergodicity is challenging. To improve sample efficiency, it is proposed to approximate \u03c1(s) by considering the domain of possible states that are similar between different policies. This approximation requires that the domain of \u03c1(s) and \u03c1 \u03b8 (s) are divided, where \u03b8 is a fixed behavior policy. In continuous state cases, to improve sample efficiency, it is proposed to approximate \u03c1(s) by considering the domain of possible states that are similar between different policies. This approximation requires dividing the domain of \u03c1(s) and \u03c1 \u03b8 (s), where \u03b8 is a fixed behavior policy. The properties in Definition 1 still hold when Condition 1 is met, which can be ensured by adding noise on \u03b8. Sampling s from S \u03b8 \u222a S \u03b8j is necessary for more general cases to satisfy the properties in Definition 1. The objective function of policy differentiation involves terms related to the policy \u03b8 and the domain S \u03b8. The last term in the objective function is only related to the domain S \u03b8. With sufficient exploration and initialization of \u03b8, the last term can disappear. Proposition 1 states that estimating \u03c1 \u03b8 (s) using a single trajectory \u03c4 is unbiased. The next step is to develop an efficient learning algorithm for maximizing cumulative rewards in traditional RL. To improve behavioral diversity, the learning objective in reinforcement learning considers both the reward from the primal task and policy uniqueness. Previous approaches directly combine these rewards with a weight parameter \u03b1. The learning objective in reinforcement learning combines rewards from the primal task and policy uniqueness with a weight parameter \u03b1. The selection of \u03b1 is crucial as it affects the contribution of intrinsic reward. To address this, inspiration is drawn from social uniqueness, treating it as a constraint rather than an additional target. The multi-objective optimization problem is transformed into a constrained optimization problem with a threshold for minimal permitted uniqueness. The penalty method replaces the constrained optimization problem with a penalty term and coefficient, with the difficulty lying in the selection of the coefficient. Zhang et al. (2019) address this challenge with the Task Novel Bisector (TNB) in the form of Feasible Direction. In this work, the constrained optimization problem is solved using Interior Point Methods (IPMs) instead of Feasible Direction Methods (FDMs). The approach involves reformulating the problem into an unconstrained form with an additional barrier term in the objective. The solution of the problem is obtained by taking the limit of the reformulated equation. Refer to Appendix G for further discussion on this method. In our proposed RL paradigm, the learning process is influenced by peers, allowing for a more natural way to handle computationally challenging and numerically unstable situations when \u03b1 is small. By bounding collected transitions in the feasible region using previous trained policies, we implicitly ensure feasibility during training of new agents. During training, new agents are terminated if they step outside the feasible region, ensuring all collected samples are within it. This results in a new policy with sufficient uniqueness, eliminating the need to balance intrinsic and extrinsic rewards. The learning process becomes more robust without objective inconsistency. The Interior Policy Differentiation (IPD) method is demonstrated on the MuJoCo environment in OpenAI Gym. Experiments are conducted on locomotion environments like Hopper-v3, Walker2d-v3, and HalfCheetah-v3 with default parameters. The method ensures uniqueness beyond intrinsic stochasticity. The Interior Policy Differentiation (IPD) method is applied to locomotion environments in OpenAI Gym to enhance behavior diversity. Experiments show that different policies can be generated by selecting different random seeds before training. The method is demonstrated using PPO and compared with TNB and WSR approaches. The uniqueness metric is used to train new policies without reshaping. WSR, TNB, and the new method are compared in the same experimental settings. 10 policies are trained sequentially to be unique from each other. The study involves training policies sequentially to be unique from each other without social influence. Qualitative results are visualized to show the motion of agents over time, highlighting acceleration and motion patterns. The visualization starts at the beginning of each episode to provide a clear understanding of the process. The experimental results show the uniqueness and performance of policies, with the proposed method outperforming others in Hopper and HalfCheetah. In Walker2d, both WSR and the proposed method improve policy uniqueness, but none surpass the performance of PPO. Task-related rewards comparison is detailed in Table 1, with a box figure illustrating the performance of each policy and their reward curve. Figures in Appendix C show the performance and reward curves of trained policies. Success rate is used as a metric to compare different approaches, with policies surpassing the baseline considered successful. Our method consistently outperforms the baseline during training, ensuring reliable performance. Significant improvements were observed in the Hopper and HalfCheetah environments, with agents trained using PPO learning more effective policies. Our proposed method enhances traditional RL schemes by preventing policies from getting stuck in the same local minimum. It encourages exploration of different action patterns to improve performance, leading to reward growth. The environment of HalfCheetah promotes reward growth by preventing policies from getting stuck in local minima. Unlike other environments, HalfCheetah lacks explicit termination signals, leading to random actions and high control costs initially. Our learning scheme allows agents to receive termination signals from peers to avoid wasting effort. During the learning process, agents in our method first learn to terminate themselves early to avoid high control costs, then adapt to pursue higher rewards. This process acts as an implicit curriculum, with the challenge of finding unique policies increasing as more policies are learned with social influence. The results of our study show how performance changes with different levels of social influence. The ablation study in Fig. 4 demonstrates how performance varies under different scales of social influence. The decrease in performance is more pronounced in Hopper due to its limited 3-dimensional action space, restricting the discovery of diverse policies. This study introduces an efficient approach to encourage RL to learn diverse strategies inspired by social influence, defining policy uniqueness and treating the problem as a constrained optimization task. Our proposed method, Interior Policy Differentiation (IPD), draws insights from Interior Point Methods to address constrained optimization problems. Experimental results show that IPD can learn various policies, avoid local minima, and facilitate implicit curriculum learning. The uniqueness of policies in different environments is optimized by TNB and WSR. TNB and WSR optimize uniqueness directly, sometimes exceeding our method. Direct optimization can decrease task performance, so hyper-parameter tuning and reward shaping are crucial. Calculation of DTV involves removing Gaussian noise in PPO. Actor models in PPO use MLP with 2 hidden layers. In PPO, actor models use MLP with 2 hidden layers. The first hidden layer has 32 units fixed, while the second layer's unit number is varied in an ablation study. Different numbers of hidden units are used for different tasks based on success rate, performance, and computation expense considerations. Training timesteps are fixed for each task. The proposed method allows flexible control over policy uniqueness magnitude. In the proposed method, the constraint threshold r0 can be adjusted to control policy uniqueness. Different thresholds result in varying policy behaviors, with a larger threshold leading to more distinct actions by the agent. Constraints are not enforced on every single action, but rather on long-term differences. In the proposed method, constraints are applied to control policy uniqueness by adjusting the threshold value. The performance of agents under different thresholds is analyzed, with a focus on long-term differences rather than individual actions. Constraints can be implemented after a certain number of timesteps for considering similarities. The WSR, TNB, and IPD methods correspond to different approaches in constrained optimization. The optimization of policy is based on batches of trajectory samples and implemented with stochastic gradient descent. The Penalty Method considers constraints by putting them into a penalty term and solving the unconstrained problem iteratively. The WSR, TNB, and IPD methods are different approaches in constrained optimization. The Penalty Method involves putting constraints into a penalty term and solving the unconstrained problem iteratively. The TNB method selects a direction based on the bisector of gradients. The TNB method selects a direction based on the bisector of gradients and uses a barrier term of \u2212\u03b1 log g(\u03b8) to ensure success in optimization. The shape of g is crucial for the success of TNB, with the barrier term having minimal influence on the objective but increasing rapidly as \u03b8 approaches the barrier. The TNB method uses a barrier term to ensure optimization success, with the objective getting closer to the primal objective as \u03b1 decreases. However, directly applying this method can be computationally challenging and unstable, especially with small \u03b1 values. A more natural approach involves bounding collected transitions within the feasible region using previously trained policies. During training, termination signals are sent by trained policies to new agents, implicitly bounding the feasible region. Valid samples collected during training are within this region, ensuring uniqueness in the final policy. This eliminates the need to balance intrinsic and extrinsic rewards, leading to a more robust learning process without objective inconsistency. The process of our method is more robust and no longer suffers from objective inconsistency. Algorithm.1 shows the pseudo code of IPD based on PPO, with additions to the primal PPO algorithm highlighted in blue lines."
}