{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive and evaluates on multiple molecular optimization tasks. Our model significantly outperforms previous state-of-the-art baselines in molecular optimization tasks by translating input molecular graphs into better forms with improved biochemical properties. This task is challenging due to the vast space of potential candidates and the complex relationship between molecular properties and structural features. Graph generation for molecular structures is complex and computationally challenging due to dependencies in the joint distribution over nodes and edges. Previous work utilized valid chemical substructures to generate graphs but had limitations. The approach to graph generation for molecular structures involved separate tree and graph encoding, leading to non-autoregressive attachment predictions. To address these limitations, a multi-resolution, hierarchically coupled encoder-decoder for graph generation is proposed. The proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation involves an auto-regressive decoder that predicts substructure components and their attachments to the molecule being generated. The encoder represents molecules at different resolutions to match the decoding process, capturing essential information for each decoding step. The graph convolution supports prediction of attachments and substructures. Decoding process is efficient with hierarchy of smaller steps. Method handles conditional translation with input criteria. Model can generate invalid outputs. The model can generate invalid outputs at test time due to isolated tree and graph decoders. Interleaving the decoding steps can solve this issue. An autoregressive decoder is proposed to address inconsistent local substructure attachments during training. The new model is evaluated on multiple molecular optimization tasks, comparing with previous state-of-the-art graph generation methods. Our implemented atom-based translation model outperforms state-of-the-art graph generation methods in discovering molecules with desired properties, showing significant improvements on QED and DRD2 optimization tasks. The model runs 6.3 times faster during decoding and benefits from hierarchical decoding and multi-resolution encoding. Additionally, conditional translation can generalize even when trained on a small percentage of molecular pairs with the desired target property. Previous work in molecular graph generation has utilized various methods to generate molecules based on SMILES strings. Some models output adjacency matrices and node labels simultaneously, while others propose generative models for decoding. These approaches have shown success even when trained on a small percentage of molecular pairs with the desired target property combination. Previous work in molecular graph generation has utilized various methods to generate molecules based on SMILES strings. You et al. (2018b), Samanta et al. (2018), and Liu et al. (2018) proposed generative models decoding molecules sequentially node by node. Kajino (2018) developed a hypergraph grammar based method for molecule generation. Our work is most closely related to Jin et al. (2018) who generate molecules based on substructures using a two-stage procedure for realizing graphs. Graph neural networks have been extensively studied for graph encoding. Our method jointly predicts substructures and their attachments with an autoregressive decoder, unlike previous methods that introduce local independence assumptions and apply steps stage-wise during decoding. Our method represents molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees, closely related to previous work on learning to represent graphs in a hierarchical manner. Our approach involves representing molecules as hierarchical graphs with multiple layers capturing relevant information for decoding steps, focusing on graph generation rather than regression or classification tasks. Gao & Ji (2019) proposed learning graph hierarchy during the encoding process, while Ying et al. (2018) introduced substructures as subgraphs of molecules. Our focus is on graph generation, where a molecule is encoded into sets of vectors representing input at different resolutions. These vectors are aggregated by decoder attention modules in each generation step. The goal is to learn a function that maps a molecule into another with better chemical properties. The encoder-decoder with neural attention is illustrated in Figure 2, with the decoder adding substructures and determining their attachment to the current graph in each generation step. The attachment prediction in graph generation involves predicting attaching points in a new substructure and their corresponding attaching points in the current graph. To support hierarchical generation, a matching encoder is designed to represent molecules at multiple resolutions. A molecule is represented by a hierarchical graph with substructure, attachment, and atom layers. The model encodes nodes in the graph for decoding steps. The model encodes nodes in the graph into substructure vectors, attachment vectors, and atom vectors for decoding steps. The decoder utilizes a multi-layer neural network and bilinear attention for prediction. Substructures are defined as subgraphs of molecules, extracted from the molecule's structure. The paper discusses extracting substructures from molecules, including rings and bonds, to construct a substructure tree that characterizes their connections based on shared atoms. The vocabulary of substructures is limited to less than 500 with high coverage on test sets. The paper discusses constructing a tree by connecting substructures with shared atoms and applying tree decomposition. The graph decoder generates a molecule by expanding its substructure tree in a depth-first order, making predictions for new substructures and their attachments. The paper describes the process of predicting new substructures and their attachments in a molecule tree using a graph decoder. It involves topological prediction, substructure prediction, and attachment prediction based on hidden representations and probability distributions. The paper discusses predicting attachments between substructures in a molecule tree using a graph decoder. It involves predicting atom pairs in two steps: first predicting attaching atoms from a fixed graph, then classifying the correct configuration from a vocabulary. The paper discusses predicting attachments between substructures in a molecule tree using a graph decoder. The correct configuration is determined from the vocabulary based on predicted attaching points. The probability of a candidate attachment is computed using atom representations, leading to an autoregressive factorization of the distribution over the next substructure and its attachment. Teacher forcing is applied during training to improve predictions. The paper discusses predicting attachments between substructures in a molecule tree using a graph decoder. During training, teacher forcing is applied to improve predictions. The encoder represents a molecule with a hierarchical graph to support the decoding process. The attachment enumeration is tractable due to small substructure sizes. The average size of attachment vocabulary is less than 5, with fewer than 20 candidate attachments. The molecular graph of X shows atom connections with labels for atom type and bond type. The attachment layer represents attachment configurations of substructures in the molecule tree, aiding in attachment prediction. The attachment layer in the molecular graph illustrates attachment configurations of substructures, with nodes representing specific attachment configurations. The substructure layer provides crucial information for substructure prediction, with edges connecting atoms and substructures between layers for information propagation. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The MPN architecture from Jin et al. (2019) is used for encoding, denoted as MPN \u03c8 (\u00b7) with parameter \u03c8. The atom layer MPN encodes the atom layer of HX using embedding vectors of atoms and bonds in X. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The MPN architecture from Jin et al. (2019) is used for encoding, denoted as MPN \u03c8 (\u00b7) with parameter \u03c8. The atom layer MPN encodes the atom layer of HX using embedding vectors of atoms and bonds in X. The network propagates message vectors between atoms for T iterations and outputs atom representations h v for each atom v. The attachment layer in MPN utilizes input features for nodes and edges based on embeddings and relative ordering during decoding. The hierarchical graph HX for molecule X is encoded by a hierarchical message passing network (MPN) with three layers: atom, attachment, and substructure. The output of the hierarchical encoder is a set of vectors that represent a molecule X at multiple resolutions, which are then used as input to the decoder attention during decoding. The hierarchical MPN architecture is used to encode the hierarchical graph H G at each step, providing substructure vectors and atom vectors. Future nodes and edges are masked to ensure predictions depend on previously generated outputs. The training set includes molecular pairs (X, Y) allowing for multiple outputs Y for each compound X. To generate diverse outputs, a variational translation model F: (X, z) \u2192 Y is used. The method involves training a variational translation model F: (X, z) \u2192 Y, where z is a latent vector indicating the intended mode of translation. The model is trained using variational inference and encodes structural changes from molecule X to Y at both atom and substructure levels to compute and sample z. The method involves training a variational translation model F: (X, z) \u2192 Y, where z is a latent vector indicating the intended mode of translation. The latent code z is used with the input representation c X to reconstruct output Y. During testing, users cannot change the behavior of a trained model. Conditional translation is extended to handle desired criteria as input to the translation process. During variational inference, \u00b5 X,Y and \u03c3 X,Y are computed with an additional input g X,Y, augmenting the latent code as [z, g X,Y]. Users can specify criteria in g X,Y during testing to control the outcome. The translation model is evaluated on single-property optimization tasks following Jin et al. (2019). A novel conditional optimization task is constructed where desired criteria are fed as input. The text discusses a novel conditional optimization task where desired criteria are inputted to the translation process to ensure molecular similarity between input X and output Y. The model is trained on four different tasks under an unconditional setting. The model is trained under an unconditional setting. LogP Optimization measures compound solubility and synthetic accessibility. Two similarity thresholds are experimented with. Different scenarios are discussed where properties of the output need to be improved after translation. The model is trained under an unconditional setting for translation. Drug-likeness is the focus for improvement after translation, with criteria encoded as vector g. Evaluation metrics include translation accuracy and diversity, with a similarity constraint imposed. Test molecules are translated multiple times, with the final translation selected based on property improvement and similarity threshold. The HierG2G method is compared against baselines like GCPN, MMPA, Seq2Seq, and JTNN for translation tasks. Translation success rate and diversity are evaluated based on similarity and property constraints. Tanimoto distance is used to measure diversity, with successful translations meeting all constraints. The comparison includes Seq2Seq, JTNN, and CG-VAE for molecule generation. AtomG2G is developed as a baseline for atom-based translation, predicting completion of decoding process. AtomG2G predicts the completion of the decoding process by creating new atoms and predicting their types and bond types. The model achieves state-of-the-art results on translation tasks, outperforming JTNN in accuracy. Our decoder outperforms JTNN in translation accuracy and output diversity, running 6.3 times faster during decoding. It also surpasses AtomG2G on three datasets, showing a hierarchical model advantage. Comparing with Seq2Seq, JTNN, and AtomG2G, our model demonstrates superior performance. Our model outperforms other models in translation accuracy and output diversity, especially on criteria with strong constraints. Training on a small subset of examples resulted in lower success rates, highlighting the importance of conditional translation setup. Ablation studies were conducted to analyze the impact of different architecture choices on the QED and DRD2 tasks. In ablation studies on the QED and DRD2 tasks, replacing the hierarchical decoder with AtomG2G's atom-based decoder led to a decrease in model performance. The DRD2 task seemed to benefit more from structure-based decoding due to the importance of specific functional groups in biological target binding. Reducing the number of hierarchies in the encoder and decoder MPN did not significantly impact the hierarchical decoding process. When the top substructure layer is removed from the encoder and decoder MPN, translation accuracy drops slightly by 0.8% and 2.4%. Further removal of the attachment layer significantly degrades performance on both datasets as all substructure information is lost. Replacing LSTM MPN with GRU MPN resulted in a decrease in translation performance, but our method still outperforms JTNN by a wide margin. In this paper, a hierarchical graph-to-graph translation model is developed using the LSTM MPN architecture for HierG2G and AtomG2G baseline. The model generates molecular graphs using chemical substructures as building blocks, learning coherent multi-resolution representations. Experimental results show that the method outperforms previous models under various settings. The LSTM MPN with T message passing iterations is used in Algorithm 3 for HierG2G and AtomG2G models. An attention layer with a bilinear function is employed for node representations. The AtomG2G decoding process involves predicting new atoms and bond types sequentially. AtomG2G is an atom-based translation method that adds new atoms to the queue Q in |Q| steps for bond prediction. The training set and substructure vocabulary sizes for each dataset are listed in Table 3. The multi-property optimization dataset is constructed by combining the training sets of QED and DRD2 tasks. The training and test set for QED and DRD2 optimization tasks include 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters for HierG2G and AtomG2G models are set with specific dimensions and regularization weights. Both models are trained with Adam optimizer. For CG-VAE models, three were trained for logP, QED, and DRD2 optimization tasks using Adam optimizer. At test time, compounds are translated into latent representations and gradient ascent is performed to maximize property scores, resulting in multiple vectors for molecule decoding. At test time, compounds are translated into latent representations and gradient ascent is performed to maximize property scores, resulting in multiple vectors for molecule decoding. It is important to keep the KL regularization weight low (\u03bb KL = 0.005) for meaningful results. Ablation studies showed that modifying the decoder to an atom-based decoder improved results. In experiments, both atom and substructure vectors were used with hidden layer and embedding layer dimensions set to 300. The number of hierarchies in the encoder and decoder MPN was reduced in two-layer and one-layer models, adjusting the hidden layer dimension accordingly. Topological and substructure predictions were made based on different vectors in each model."
}