{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for image classification, latent features can be broadly divided into \"core\" features that remain consistent across domains and \"style\" features that can vary. Orthogonal features like position, rotation, and more complex attributes like hair color or posture fall into the latter category. When training deep neural networks for image classification, latent features can be categorized into \"core\" features that are consistent across domains and \"style\" features that can vary, such as position, rotation, hair color, or posture. To prevent adversarial domain shifts, the focus is on using \"conditionally invariant\" features for classification, assuming the domain as a latent variable. The distributional change of features across different domains is not directly observable. Data augmentation involves generating multiple images from an original image, with an ID variable linking them. Only a small fraction of images need to have this ID variable. In a causal framework, the ID variable is added to the model to treat the domain as a latent variable. Samples with the same class and identifier are considered counterfactuals under different style interventions. Regularizing the network using a graph Laplacian improves performance in settings with changing image qualities. Deep neural networks have shown exceptional performance in various prediction tasks, but issues can arise when the learned representations are not robust to changes in image quality, color, and other factors. Domain shifts can impact predictive performance, especially when deploying machine learning systems in real-world settings. This raises questions about interpretability, fairness, and transfer learning. When deploying machine learning systems in production, predictive performance may degrade due to hidden confounding factors like sampling biases. An example is the \"Russian tank legend\" where a system trained to distinguish tanks based on image quality would fail in practice due to biases in the training data. Hidden confounding factors, like those between image quality and tank origin, create indirect associations in deep learning. Large sample sizes are necessary to average out these effects, although not a guarantee. Sample size is also crucial for achieving invariance to factors like translation and rotation through data augmentation. Adversarial examples highlight the divergence between human and artificial cognition. Adversarial examples are intentionally perturbed inputs that mislead ML models but not humans. The goal is to mimic human ability to learn invariances from few instances of an object and align DNN features with human cognition. Fairness and discrimination considerations also play a role in controlling input data characteristics. Existing biases in datasets used for training ML algorithms tend to be replicated in the estimated models, leading to issues like Google's photo app tagging non-white people as \"gorillas\" due to biased training examples. To address this, counterfactual regularization is proposed to control input data characteristics and prevent undesired impacts on decision-making. Counterfactual regularization (CORE) is proposed to control latent features extracted by an estimator, categorizing them into 'conditionally invariant' (core) and 'orthogonal' (style) features. The goal is for a classifier to use only core features related to the target of interest, making it robust to adversarial domain shifts. CORE relies on observing \"counterfactuals\" in certain datasets, where the same object is seen under different conditions. It exploits knowledge about grouping instances related to the same object. The manuscript discusses how CORE reduces the need for data augmentation and improves predictive performance in small sample size settings. In \u00a74, counterfactual regularization (CORE) is formally introduced for logistic regression. In \u00a75, CORE's performance is evaluated in various experiments using the CelebA dataset BID26, which contains face images of celebrities. The task involves classifying whether a person wears glasses, with the constraint that all images of the same person must yield the same prediction. Additional instances of the same person are considered counterfactual observations. The standard approach is to pool all examples, but some observations can be grouped. Including 10 identities in the training set results in a total sample size of 321. Examples from the subsampled CelebA dataset and augmented MNIST dataset show counterfactual observations grouped by identity or original image. Exploiting grouping information in training reduces test error significantly compared to pooling all samples. The group structure reduces average test error from 24.76% to 16.89%, a 32% improvement. This approach also makes data augmentation more efficient by requiring fewer samples. Data augmentation aims to enhance efficiency by creating additional samples through modifications like rotation or flipping of images. By utilizing grouping information, invariance with respect to style features is enforced more strongly compared to traditional data augmentation methods. This approach reduces test error significantly and requires fewer samples for training. Using CORE for data augmentation, the average test error on rotated examples is reduced from 32.86% to 16.33%, with a total sample size of 10100. This approach enforces invariance with respect to style features more strongly compared to traditional methods, leading to significant error reduction. The approach proposed in BID13 is motivated by previous work and focuses on learning a representation without discriminative information about the input's origin. It involves an adversarial training procedure to maximize domain classification loss while minimizing target prediction loss. In contrast, the approach does not assume data from different domains but different realizations of the same object under different interventions. The data generating process in BID14 is similar to this approach. The BID14 model identifies conditionally independent features by adjusting variables to minimize MMD distance between distributions in different domains. A key difference from our approach is the explicit observation of the domain identifier in BID14, while it is latent in our model. We utilize an identifier variable to penalize the classifier for using latent features outside the set of conditionally independent features. Causal modeling aims to guard against adversarial domain shifts by ensuring predictions remain valid under interventions on predictor variables. However, transferring these results to adversarial domain changes in image classification faces challenges due to the anti-causal nature of the classification task. The challenges in anti-causal prediction involve guarding against style feature shifts in deep learning, which differs from standard causal inference methods. Various approaches have been proposed to address these challenges, but their goals and settings are different from the current context. Various approaches focus on cause-effect inference, with the Neural Causation Coefficient (NCC) proposed to estimate the probability of X causing Y. The NCC is used to distinguish between features of objects and features of the objects' contexts. Additionally, similarities between structural equation modeling and CGANs are noted, with CGANs fitted in both directions X \u2192 Y and Y \u2192 X. CGANs BID33 are fitted in both directions X \u2192 Y and Y \u2192 X to estimate causal direction. BID16 uses generative neural networks for cause-effect inference and identifying v-structures in a graph skeleton. Bahadori et al. (2017) propose a regularizer combining an L1 penalty with weights based on the probability of a feature being causal. Besserve et al. (2017) connect GANs with causal generative models using a group theoretic framework. Kocaoglu et al. (2017) introduce causal implicit generative models, known as CausalGAN, which sample from conditional and interventional distributions by incorporating the causal graph structure. BID29 utilize deep latent variable models and proxy variables to estimate individual treatment effects, while BID21 leverage causal reasoning to address fairness in machine learning by defining causal nondiscrimination criteria. The algorithms avoiding proxy discrimination require classifiers to be constant as a function of the proxy variables in the causal graph, bearing structural similarity to style features. Distinguishing between core and style features is akin to disentangling factors of variation, which has garnered interest in generative modeling. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce the dependence of latent representation on specified object transforms. The latent representation in Matsuo et al. (2017) aims to reduce dependence on specific object transforms, such as location, image quality, posture, brightness, background, and contextual information. The approach involves disentangling core and style features, with a focus on learning a latent representation that excludes certain style features like location. In a variational autoencoder framework, Bouchacourt et al. (2017) aim to separate style and content in grouped observations, assuming samples within a group share a common but unknown value for one factor of variation while the style can differ. They address a classification task without explicitly estimating latent factors as in a generative setting. In a classification task, a causal graph is developed to compare adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. The target of interest Y is typically R for regression or {1, . . . , K} for classification with K classes. The predictor X consists of p pixels, and the prediction y for y given X = x is represented by f \u03b8 (x) with parameters \u03b8 \u2208 R d corresponding to weights in a DNN. In regression, the goal is to minimize the expected loss by choosing weights \u03b8 that minimize the empirical loss L_n(\u03b8) on training data. This is achieved through penalized empirical risk minimization, where a penalty like ridge penalty is applied to the loss function. The penalty in regression can be a ridge penalty or exploit geometries like Laplacian regularized least squares. The structural model includes a latent domain variable D and an ID variable that can change based on class Y. The prediction is anti-causal, using predictors X that are non-ancestral to Y. The causal effect from the class label Y on the image X is mediated via core features X ci and style features X \u22a5. Interventions are possible on style features but not on core features. The distribution of style features X \u22a5 can change across domains, confounded by latent domain D. The latent domain D confounds features X and Y. Core features X ci are conditionally invariant, while style features X \u22a5 are context-dependent. The style intervention variable \u2206 influences both X \u22a5 and the image X. In this work, the focus is on guarding against adversarial domain shifts using causal graphs and potential outcome notation. The intervention variable \u2206 influences both style features X \u22a5 and the image X. The intervention variable \u2206 influences style features X \u22a5 and the image X, leading to additive effects. The goal is to devise a classification that minimizes adversarial loss by considering arbitrarily strong interventions on style features. The goal is to minimize adversarial loss by considering strong interventions on style features X \u22a5, which can only change certain aspects of the image. The term \"adversarial\" refers to interventions on style features, distinct from domain adversarial neural networks. The goal is to protect against shifts in the distribution(s) of test data by distinguishing between core and style features. Causal inference involves observing the outcome of taking a medicine or not, but never both simultaneously. The counterfactual scenario involves changing the treatment while keeping all other variables constant. Counterfactuals involve changing the treatment while keeping other variables constant. The style intervention \u2206 plays a similar role as treatment T in medical examples. This allows for observing the effect of changes in \u2206 on outcomes. In image analysis, the style intervention \u2206 serves a similar role as treatment T in medical examples, allowing for observing changes in outcomes under different conditions. Counterfactuals can be applied to image analysis by altering variables such as background, posture, viewing angle, and image quality to see the effect on the same object. The focus is not on the 'treatment effect' but on using \u2206 to analyze variations in images. The style intervention \u2206 is used to rule out parts of the feature space for classification, focusing on changes in classification under different style interventions while keeping class and identity constant. Notationally, samples are denoted by xi,j \u2208 Rp, with most samples having mi = 1. The pooled estimator treats all observations identically by summing over the loss with a ridge penalty. The estimator is always the ridge estimator with a cross-validated penalty parameter choice. The adversarial loss of the pooled estimator may be infinite. The pooled estimator works well in terms of adversarial loss if certain conditions are met. The first condition implies that if the estimator can extract certain information from an image, no further information in the image explains the outcome. The second condition is fulfilled if the relationships between variables are not deterministic. The estimator will perform well if certain edges in the model are absent. The optimal predictor in the invariant space I is determined by ensuring f \u03b8 (x(\u2206)) remains constant for all x \u2208 R p, where \u03b8 \u2208 I. This is achieved when f \u03b8 is solely a function of the core features X ci. The challenge lies in the fact that the core features are not directly accessible. The core features X ci are not directly observable, so we must infer the invariant space I from data. Empirical risk minimization is used to approximate the optimal invariant parameter vector, with an empirically invariant space I n defined by variance and regularization. Setting \u03c4 = 0 ensures identical predictions for class labels across all instances. The text discusses the use of empirical risk minimization to infer the invariant space I from data, with \u03c4 controlling the degree of variations in class label predictions. The matrix L ID is a graph Laplacian BID4, connecting samples with the same ID in the underlying graph. The graph Laplacian regularization penalizes variances \u03c3 2 i (\u03b8) and is formed in the sample space by the identifier variable ID. The outcome is not sensitive to the penalty \u03bb value, and defining the graph based on ID is crucial. The graph should be defined in terms of the identifier variable ID for effective regularization against adversarial domain shifts in binary classification using logistic regression. The pooled estimator shows infinite adversarial loss under certain assumptions. The pooled estimator has infinite adversarial loss under certain assumptions, while the CORE estimator's adversarial loss converges to the optimal value as n \u2192 \u221e. Various experiments are conducted to study how CORE handles confounded training data and changing style features in test distributions. Additional experiments involve classifying elephants and horses based on different attributes. In addition to experimental results in \u00a7C.2 and \u00a7C.3, a TensorFlow BID0 implementation of CORE will be provided along with necessary code for reproducing experiments. An open question remains on setting the tuning parameter \u03c4 or penalty \u03bb in Lagrangian form, with performance not highly sensitive to \u03bb choice. The example involves synthetically generated stickmen images, with the target Y being {adult, child} and X ci representing height, where Y is causal for height. The class Y is causal for height, a core feature that differentiates between adults and children. There is a dependence between age and movement, influenced by the hidden common cause of place of observation. The data generating process is illustrated in FIG0.9, showing how the model may fail when presented with images that do not align with the learned patterns. The model may fail when presented with images of dancing adults, as large movements are associated with children and small movements with adults. In test sets 2 and 3, the dependence between Y and X \u22a5 vanishes, leading to misclassification rates for CORE and the pooled estimator. CORE achieves good predictive performance on test sets 2 and 3 with as few as 50 counterfactual observations, while the pooled estimator fails with test errors > 40%. The learned representation of the pooled estimator uses movement as a predictor for age, unlike CORE which does not use this feature due to counterfactual regularization. Including more counterfactual examples would not improve the pooled estimator's performance. In the study, the CelebA dataset is used to classify whether a person in an image is wearing eyeglasses. The image quality is affected by the presence of glasses, with lower quality images for glasses wearers. Counterfactual observations are only available for images with glasses, with an intervention that samples new image quality from a Gaussian distribution. Counterfactual observations for images without glasses are not available. The study uses the CelebA dataset to classify images with glasses. A counterfactual observation is created by sampling image quality from a Gaussian distribution. Misclassification rates for different test sets are compared using CORE and pooled estimator. Test set 2 reverses the class of the quality intervention. The pooled estimator outperforms CORE on test set 1 by utilizing image quality information. However, it performs poorly on test sets 2-4 as it uses image quality as a predictor, unlike CORE whose performance is not significantly affected. In contrast to the pooled estimator, CORE's predictive performance is not affected by changing image quality distributions. Experimental details and results for quality interventions are provided, aiming to assess if CORE can exclude \"color\" from its learned representation. CORE's ability to exclude \"color\" from its learned representation is assessed by including counterfactual examples of different colors in the Animals with Attributes 2 dataset. Grayscale images are added for elephants, with a total sample size of 1850. Misclassification rates for CORE and the pooled estimator on different test sets are compared. The pooled estimator performs poorly on test sets 2 and 3 due to its reliance on the color \"gray\" in the training set, while CORE's ability to exclude color from its representation is highlighted by including counterfactual examples in the dataset. The CORE estimator's predictive performance is not affected by changing color distributions, as it demands invariance of predictions for instances of the same elephant. Adding grayscale images helps learn color invariance, while the pooled estimator's reliance on the color \"gray\" leads to poor performance on certain test sets. The proposed counterfactual regularization (CORE) ensures fairness by not including \"color\" in its learned representation, unlike the pooled estimator which uses \"color\" for decisions. CORE distinguishes core and style features in images to achieve robustness against interventions. The proposed counterfactual regularization (CORE) ensures fairness by achieving invariance of classification performance with respect to adversarial interventions on style features such as image quality, fashion type, color, or body posture. It works despite sampling biases in the data and can achieve the same classification performance as standard data augmentation approaches using fewer instances. The proposed counterfactual regularization (CORE) ensures fairness by achieving invariance of classification performance with respect to interventions on style features. Larger models like Inception or ResNet architectures have been suggested for further exploration. CORE can bring benefits for training Inception-style models by improving sample efficiency and generalization performance. Using video data could leverage temporal information for grouping and counterfactual regularization, potentially aiding in debiasing word embeddings. The structural equation for images can be linear in style features, and logistic regression can be used for class label prediction. Logistic regression is used to predict class labels in the context of interventions acting additively on style features. Core features are conditionally invariant, with observed variables including Y, X, and ID. Logistic regression is used for predicting Y from X in the presence of latent variables D, X ci, \u2206, X \u22a5, and noise variables. Training data with m samples is used to estimate \u03b8 with logistic loss for training and testing. Expected losses on test data include standard logistic loss and loss under adversarial style or domain interventions. The loss under adversarial style interventions allows large interventions on X \u22a5. The assumptions for Theorem 1 include sampling \u2206 from a distribution in R q, matrix W having full rank q, and having at least as many counterfactual examples as style variables. The sampling process involves collecting n independent samples from a distribution that satisfies certain constraints. For a subset of the samples, a new value of \u2206 is redrawn, resulting in m samples with n distinct values. The pooled estimator is guaranteed under Assumption 1. The pooled estimator, under Assumption 1, has infinite adversarial loss with probability 1. An equivalent result can be derived for misclassification loss. To show this, we need to demonstrate that W t\u03b8pool = 0 with probability 1, by showing that W t\u03b8pool = 0 with probability 1. This is done by assuming W t\u03b8pool = 0 and reaching a contradiction. The pooled estimator, under Assumption 1, has infinite adversarial loss with probability 1. To show this, we demonstrate that W t\u03b8pool = 0 by contradiction. If W t\u03b8pool = 0, then \u03b8 pool = \u03b8 * and the directional derivative of the training loss should vanish at the solution \u03b8 * for all training samples. The interventions only affect the column space of W in X, making the oracle estimator \u03b8 * the same under true and counterfactual training data. The derivative g(\u03b4) in FORMULA24 can be written as DISPLAYFORM10. The difference between FORMULA24 and FORMULA25 is taken, leading to DISPLAYFORM11. By model assumptions, x i,j \u2212 x i,j (0) = W \u2206 i,j, where \u03b4 = W u. From (A2), eigenvalues of W t W are positive, and r i (\u03b8 * ) is independent of interventions \u2206. The eigenvalues of W t W are all positive, and r i (\u03b8 * ) is not affected by interventions \u2206 i,j. The left hand side of (10) has a continuous distribution, and the probability of it not being identically 0 is 1. The proof of the first part is completed by contradiction. With probability 1, \u03b8 core = \u03b8 * in the second part. In the second part, it is shown that with probability 1, \u03b8 core = \u03b8 * in the linear subspace I. This is because the number of counterfactual examples exceeds the rank of W, ensuring that \u03b8 core = \u03b8 * . The estimator remains unchanged when using counterfactual data in the absence of interventions. The estimator remains unchanged when using data without interventions as training data. Comparing two formulas, we have c = m - n samples redrawn randomly from the empirical sample.\u03b8 core = \u03b8 * with probability 1 under (A3). The CelebA dataset is used to classify gender based on images, creating confounding by including mostly men wearing glasses. Counterfactuals are used by showing images of the same person without glasses for males and with glasses for females. Test set 1 follows the same distribution as the training set, while test set 2 explores the association between gender and glasses. In test set 2, the association between gender and glasses is reversed: women always wear glasses while men never wear glasses. The study compares training a four-layer CNN end-to-end with using Inception V3 features and retraining the softmax layer. Results show similar trends with increasing c leading to smaller performance differences between CORE and the pooled estimator. In a confounded setting with a hidden common cause, the pooled estimator performs worse on test set 2 as m increases, exploiting X \u22a5 more. The CelebA dataset is used to classify whether a person in an image wears eyeglasses, with a hidden common cause D indicating if the image was taken outdoors or indoors. The brightness of images is influenced by whether a person wears glasses or not. Test sets show variations in brightness interventions, with test set 2 reversing the sign of brightness for people with glasses. The pooled estimator's performance is affected by the hidden common cause in the dataset. The pooled estimator performs better than CORE on test set 1 by utilizing brightness information, but struggles on test sets 2 and 4 due to differences in brightness distribution. CORE's predictive performance is not affected by changing brightness distributions. Results for different counterfactual settings (\u03b2 \u2208 {5, 10, 20} and c \u2208 {200, 5000}) are shown in FIG0 .5. Using counterfactual setting 1 yielded the best results, while alternative settings 2 and 3 were also evaluated. In counterfactual setting 1, only brightness varies between examples, yielding the best results. Setting 2 presents challenges with varying factors, while grouping images of different persons still improves predictive performance. The optimal value for the tuning parameter \u03c4 or penalty \u03bb remains an open question. Misclassification rates of CORE on the AwA2 dataset vary with the penalty \u03bb. In the experiment, the performance of CORE on the AwA2 dataset is analyzed with varying penalty \u03bb. Results show that performance is not highly affected by the choice of \u03bb. The study includes varying the number of identities in the training data set, resulting in different sample sizes and misclassification rates for the test set. CORE improves predictive performance, especially when the number of identities is small. In an experiment analyzing the performance of CORE on the AwA2 dataset, varying penalty \u03bb did not highly affect performance. The study involved changing the number of identities in the training data set, leading to different sample sizes and misclassification rates for the test set. CORE showed improved predictive performance, particularly with small numbers of identities. Additionally, results for an experiment with varying numbers of augmented training examples and rotations are shown in FIG0 .8, demonstrating misclassification rates for rotated digits in test set 1. The misclassification rates of CORE are lower on test set 1, indicating more efficient data augmentation. Even for n = 1000, it benefits performance on test set 2. Results for different numbers of counterfactual examples show that the CORE estimator's performance is not sensitive to the number of examples. The pooled estimator fails to achieve good predictive performance on test sets 2 and 3, using \"movement\" as a predictor for \"age\". Experiments for counterfactual settings 1-3 show that setting 1 works best, with small differences between settings 2 and 3. A large performance difference is observed between \u00b5 = 40 and \u00b5 = 50. There is a large performance difference between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator. The latter command rotates the colors of the image in a cyclic manner. All images in test set 3 are changed to grayscale. The models are implemented in TensorFlow, using the same network architecture and training procedure for CORE and the pooled estimator. The loss function differs by the counterfactual regularization term. The Adam optimizer is used in all experiments. In experiments, the models are trained five times with the same data to assess variance. Training data is shuffled in each epoch, with mini batches containing counterfactual observations. Mini batch size is set to 120, making optimization more challenging for small c values."
}