{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models, allowing learning from uncertain supervision and inferring soft-inclusions among concepts. Building on the Box Lattice model, which uses high-dimensional hyperrectangles to model soft-inclusions, this approach maintains the geometric inductive bias of hierarchical embedding models. In this work, a novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. This approach improves optimization robustness in the disjoint case while preserving desirable properties of the original lattice measure. The novel hierarchical embedding model improves performance on various tasks, especially with sparse data. Embedding methods are crucial in machine learning, converting semantic problems into geometric ones. Recent years have seen an interest in structured or geometric representations, moving away from simple points in a vector space to more complex geometric structures like density functions, convex cones, and axis-aligned hyperrectangles. These objects better express ideas of asymmetry, entailment, ordering, and transitive relations. The Box Lattice model of BID22 focuses on modeling transitive relations with a probabilistic interpretation, replacing edges with conditional probabilities. Box embeddings are a generalization of order embeddings and probabilistic order embeddings, using overlapping boxes to model complex joint probability distributions. The Box Lattice model of BID22 focuses on modeling transitive relations with a probabilistic interpretation, using overlapping boxes to model complex joint probability distributions. However, the \"hard edges\" of boxes can cause difficulties for gradient-based optimization, especially with sparse data where some boxes should have zero overlap while others should have high overlap. This is a concern for tasks like market basket models and entailment tasks. The Box Lattice model of BID22 focuses on modeling transitive relations with overlapping boxes to represent joint probability distributions. They introduce an ad-hoc surrogate function to address the disjoint case, while we propose a new model based on smoothing the hard edges of boxes into density functions using Gaussian convolution. Our approach demonstrates superior performance on various tasks such as WordNet relations, Flickr caption entailment, and MovieLens market basket dataset, outperforming existing state-of-the-art results. The curr_chunk discusses related work on structured or geometric embeddings, specifically order embeddings and box embeddings models. It mentions the differences between probabilistic and hyperrectangle-based generalizations of order embeddings. The curr_chunk discusses models based on embedding points in hyperbolic space for learning hierarchical embeddings, similar to order embeddings and box embeddings. These nonprobabilistic models optimize an energy function and are biased towards learning tree structures due to the negative curvature of hyperbolic space. The curr_chunk discusses smoothing the energy landscape of models using Gaussian convolution, which is common in mollified optimization and machine learning models. It focuses on embedding orderings and transitive relations, a subset of knowledge graph embedding, with a probabilistic approach. The probabilistic approach discussed in the curr_chunk focuses on learning an embedding model that maps concepts to subsets of event space, tailored for transitive relations and fuzzy concepts of inclusion and entailment. It introduces methods for representing ontologies as geometric objects, including vector and box lattices. A poset is a pair P, where P is a set and is a binary relation. It generalizes the concept of a totally ordered set to allow some elements to be incomparable. Posets are useful for representing acyclic directed graph data with transitive relations. A lattice is a poset where any subset of elements has a unique least upper bound and greatest lower bound. In a bounded lattice, there are additional elements (top) and \u22a5 (bottom) representing the least upper bound and greatest lower bound of the set. Lattices have binary operations \u2228 (join) and. A lattice is equipped with binary operations \u2228 (join) and \u2227 (meet), satisfying properties like the lower bound of the set. Special cases include extended real numbers forming a bounded lattice under min and max operations, and sets partially ordered by inclusion with \u2229 and \u222a as operations. The dual lattice can be obtained by swapping \u2227 and \u2228 operations. In a lattice, the dual lattice is obtained by reversing the poset relation, resulting in a valid lattice with a sign change in real numbers. A vector lattice, also known as a Riesz space, is a vector space with a lattice structure. The standard partial order for the vector lattice R^n is the product order from the underlying real numbers. The Order Embeddings of BID20 embed partial orders as vectors using the reverse product order, corresponding to the dual lattice, and restrict the vectors to be positive. The vector of all zeroes represents, and embedded objects become \"more specific\" as they get farther away from the origin. FIG0 demonstrates a toy, two-dimensional example of the Order Embedding vector lattice representation of a simple ontology. Shading represents the probability measure assigned to this lattice in the probabilistic extension of BID9. The box lattice introduced by Vilnis et al. in the probabilistic extension of BID9 represents concepts in a knowledge graph with axis-aligned hyperrectangles. The lattice structure is defined by set inclusion between boxes, creating a natural partial order. The structure is represented by pairs of maximum and minimum coordinates for each axis, with operations for least upper bounds and greatest lower bounds. The box lattice in the probabilistic extension of BID9 represents concepts in a knowledge graph with axis-aligned hyperrectangles. The lattice meet is the largest box contained within both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are associated with the volume of boxes, their complements, and intersections under a suitable probability measure. The probability of an event x with interval boundaries (x m , x M ) under the uniform measure is given by n i (x M,i \u2212 x m,i ), constrained to the unit hypercube. The probability measure requires boxes to be constrained to the unit hypercube, with p(x) \u2264 1. p(\u22a5) is zero for an empty set. When using gradient-based optimization for learning box embeddings, a problem arises when two concepts are incorrectly labeled as disjoint, leading to zero gradient signal due to zero intersection volume. The authors propose a surrogate function to optimize when dealing with sparse lattices, where most boxes have little to no intersection. This can lead to issues when training makes two boxes disjoint, as there is no way to recover with the naive measure. They suggest developing alternate measures to avoid this problem, improving optimization. The authors propose a framework to develop alternate measures that improve optimization and final model quality by addressing issues with sparse lattices. They aim to relax the assumption of \"hard edges\" in standard box embeddings to reduce gradient sparsity and enhance optimization while preserving geometric intuition. In this section, the authors discuss representing boxes as products of intervals and their volumes under product measures. They introduce the concept of joint probability between intervals x = [a, b] and y = [c, d] as an integral of the product of indicator functions. By using functions of infinite support, specifically kernel smoothing, they aim to improve optimization and model quality by reducing gradient sparsity. The authors use kernel smoothing with Gaussian kernels to smooth indicator functions with infinite support, aiming to improve optimization and model quality by reducing gradient sparsity. They demonstrate this approach in one dimension and provide a closed form solution for evaluating smoothed indicators. The authors demonstrate the use of kernel smoothing with Gaussian kernels to improve optimization and model quality by reducing gradient sparsity. They provide a closed form solution for evaluating smoothed indicators using antiderivatives of the standard normal CDF and the softplus function. The authors show how kernel smoothing with Gaussian kernels can enhance optimization and model quality by reducing gradient sparsity. They offer a solution for evaluating smoothed indicators using antiderivatives of the standard normal CDF and the softplus function. However, the multiplication of Gaussian-smoothed indicators does not provide a valid meet operation on a function lattice, which complicates applications that train on conditional probabilities. By modifying equation 3, a function p can be obtained to simplify applications training on conditional probabilities. The identity holds true for the hinge function but not the softplus function, with a similar functional form applicable to both. In the zero-temperature limit, equations 3 and 7 are equivalent. Equation 7 is idempotent for overlapping intervals, while equation 3 is not. This leads to defining probabilities using equation 7. For one-dimensional intervals, a normalized version of equation 7 satisfies the idempotency requirement. Softplus upper-bounds the hinge function, allowing output values greater than 1, requiring normalization. Two approaches are used for normalization in experiments. For data with a small number of entities, boxes learn unconstrained and dimensions are divided by global minimum and maximum sizes. For data where repeated value computation is infeasible, projection onto the unit hypercube and normalization by m soft (1) is used. The final probability p(x) is the product over dimensions. The softplus function is compared to the logistic sigmoid link function and the standard uniform probability measure in a joint space of events. The softplus function shows better behavior for highly disjoint data. The softplus function outperforms the Gaussian model in handling highly disjoint boxes, maintaining the meet property. Experiments on the WordNet hypernym prediction task show the effectiveness of these improvements. The WordNet hypernym hierarchy has 837,888 edges. Positive examples are randomly selected from these edges, while negative examples are generated by swapping terms. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, requiring less hyper-parameter tuning. Further experiments are conducted to confirm its performance in a sparse regime with different numbers of positive and negative examples. In further experiments with different numbers of positive and negative examples from the WordNet mammal subset, the box lattice, smoothed approach, and order embeddings (OE) were compared. The training data is the transitive reduction of the mammal WordNet subset, with 1,176 positive examples. The dev/test sets contain 209 positive examples, with negative examples generated randomly. Balanced data shows that all models, including OE baseline, Box, and Smoothed Box, nearly match the full transitive closure. Smoothed Box models outperform OE and Box in all settings, especially on imbalanced data like in real-world entailment graph learning. Experiments were conducted on the WordNet mammal subset and the Flickr entailment dataset, which contains 45 million image caption pairs. The performance of the models was compared based on different levels of label imbalance. In experiments using the WordNet mammal subset and Flickr entailment dataset of 45 million image caption pairs, the Smoothed Box models showed superior performance compared to OE and Box models, particularly on imbalanced data. The dataset used for comparison was the same as in previous studies, with constraints on box dimensions and the application of the softplus function. Results showed a slight performance improvement, especially on unseen captions. In experiments using the WordNet mammal subset and Flickr entailment dataset, the Smoothed Box models outperformed OE and Box models, especially on imbalanced data. The dataset used for comparison had constraints on box dimensions and applied the softplus function, resulting in slight performance improvements, particularly on unseen captions. The current study focuses on a market-basket task using the MovieLens dataset to predict users' movie preferences, comparing different models including low-rank matrix factorization and complex bilinear factorization. The study compares different models for predicting users' movie preferences using the MovieLens dataset. The complex bilinear model includes an additional vector to capture the \"imply\" relation. Results show that the smoothed box embedding method outperforms all other baselines, especially the original box lattice model. The smoothed box embedding method outperforms all other baselines, especially in Spearman correlation, a key metric for recommendation tasks. This model is easier to train with fewer hyper-parameters and has achieved state-of-the-art results on various datasets, particularly effective with sparse data and robust in different conditions. The smoothed box embedding method is effective with sparse data and robust in various conditions. Research on geometrically-inspired embedding models is ongoing, with a focus on complex structures like unions of boxes. The study aims to explore function lattices and constraint-based learning approaches. The Gaussian kernel is normalized to have total integral equal to 1, preserving areas of boxes. The antiderivative of \u03c6 is the normal CDF, allowing for evaluation of the integral of interest. Fubini's theorem is applied to evaluate equation 8, leading to the conclusion with \u03c3 = \u03c4 \u22121. The MovieLens dataset, though not truly sparse, is large. The MovieLens dataset, with a large proportion of small probabilities, is suitable for optimization by the smoothed model. Additional experiments test the model's robustness to initialization, specifically with disjoint boxes. Control over initialization is possible, but not always over intermediate optimization results. Parametrizing the initial distribution of boxes with a minimum coordinate and a positive width, adjustments were made to ensure varying levels of disjoint boxes at initialization before learning on the MovieLens dataset. Results showed that the smoothed model was not affected by disjoint initialization, while the original box model's performance degraded significantly. This suggests that the strength of the smoothed box model lies in its ability to handle disjoint initialization. The smoothed box model's strength lies in its ability to smoothly optimize in the disjoint regime. Methodology and hyperparameter selection details are provided for each experiment, with code available for reproduction. WordNet experiments involve evaluating the model on the development set every epoch, with the best model used for the test set. Baseline models use BID22 parameters, while the smoothed model's hyperparameters are determined on the development set. The experimental setup for the smoothed model involves conducting 12 experiments with negative examples generated randomly. A parameter sweep is done for all models to choose the best result for each model. The model architecture is a single-layer LSTM that reads captions and produces box embeddings. The model is trained for a large number of epochs and tested. The LSTM model is trained for a fixed number of epochs and tested on development data. Hyperparameters are determined on the development set. The model is evaluated every 50 steps on the development set, and optimization stops if the best development set score does not improve after 200 steps. The best development model is used to score the test set."
}