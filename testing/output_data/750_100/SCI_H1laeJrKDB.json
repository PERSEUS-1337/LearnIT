{
    "title": "H1laeJrKDB",
    "content": "Recent deep generative models can produce realistic images and embeddings for computer vision and natural language processing tasks. Recent works have focused on studying the semantics of the latent space to improve control and understanding of the generative process. This paper introduces a new method to enhance interpretability by finding meaningful directions in the latent space for precise control. The method introduced in this paper focuses on finding directions in the latent space of generative models to control specific properties of generated images. It is suitable for simple transformations like translation, zoom, or color variations, and has been shown to be effective for both GANs and variational auto-encoders. Generative models are used in various applications like image in-painting and dataset-synthesis. However, control over generated images is often limited. Recent attempts have shown that modifying attributes of generated images is possible by adjusting latent codes or combining them. Recent studies have shown that generative models can be manipulated by adjusting or combining latent codes to control attributes of generated images. Understanding the latent space structure of these models provides insights into factors of variation in images, such as object presence, positions, and lighting. Factors of variations in images can be categorized as modal or continuous. Modal factors are discrete values like object categories, while continuous factors include size and position. Humans naturally describe images using these factors, which efficiently represent natural images. This way of characterizing images is also seen in the context of generative models and latent space structure. In this paper, a method is proposed to find meaningful directions in the latent space of generative models for precise control over specific continuous factors of variations. The current literature mainly focuses on semantic factors, but this approach aims to provide control over continuous factors without the need for labels or an encoder model. The method proposed in this paper aims to control specific continuous factors of variations in image generative models, such as vertical position, horizontal position, and scale. It does not require a labeled dataset or an encoder model and can be adapted to other variations like rotations, brightness, contrast, and color. The focus is on position and scale for quantitative evaluation. The method proposed in the paper aims to find interpretable directions in the latent space of generative models to control factors of variations in generated images. It demonstrates the ability to precisely control image properties by sampling latent representations along linear directions and introduces a novel reconstruction loss. The paper proposes a novel reconstruction loss for inverting generative models with gradient descent and discusses the difficulty of inverting generative models with optimization. It also explores the impact of disentanglement on controlling generative models, highlighting that modifying image properties is easier than obtaining labels for those properties. In the context of inverting generative models, the process involves computing the difference between transformed and original images to find the corresponding transformation direction in the latent space. This is achieved by finding a vector z that generates an image I, which closely resembles the transformed image T(I) after applying a specific transformation T. In the context of inverting generative models, the goal is to find a latent code z that approximates the transformation T(I) of an image I. By minimizing a reconstruction error L between I and the generated image \u00ce = G(\u1e91), the direction encoding the factor of variation described by T can be estimated. However, optimizing this process may lead to unrealistic reconstructions due to solutions being located in low likelihood regions of the training distribution. The choice of the reconstruction error L is crucial in optimizing generative models. Commonly used pixel-wise Mean Squared Error and cross-entropy can lead to blurry images. Alternative reconstruction errors have been proposed to address this issue, but they are computationally expensive. The poor performance of pixel-wise mean square error is attributed to favoring solutions that are unrealistic. The poor performance of pixel-wise mean square error favors unrealistic solutions. The limited capacity of the generator results in textures being reconstructed as uniform regions. This is due to the high-dimensional manifold of textures and the low dimension of the latent space. Reducing the weight of high frequencies in the loss function can lead to sharper results by allowing for a larger range of possibilities in the generated images. Using equation 2 allows for a larger range of possibilities in generated images, including more details and appropriate texture for realism. A quantitative comparison to other losses is done using the Learned Perceptual Image Patch Similarity (LPIPS). The optimization problem of finding z T such that G(z T ) \u2248 T T (I) can be solved by using a L 2 penalty on the norm of z to encode a centered Gaussian prior. Algorithm 1 involves creating a dataset of trajectories in the latent space corresponding to a transformation in the pixel space, parametrized by \u03b4t. This allows for a degree of transformation control. The algorithm involves creating trajectories in the latent space with a parameter \u03b4t controlling transformation. N = 10 with \u03b4t n distributed regularly on [0, T ]. Zhu et al. (2016) suggested using an auxiliary network to estimate z T for initialization, but training it is costly. In contrast to Zhu et al. (2016), our approach avoids the need for an auxiliary network to estimate z T for initialization, which is costly. By decomposing the transformation into smaller steps, we guide the optimization on the manifold to avoid slow convergence issues. Our approach, unlike Zhu et al. (2016), does not require additional training. We qualitatively compare our method to naive optimization in Appendix C. Ignoring undefined regions in transformed images and limitations of generative models are challenges we address. After addressing challenges with generative models, our method focuses on discarding outliers in latent codes to improve image generation. This process involves removing codes with high reconstruction errors, resulting in Algorithm 1 for generating trajectories in the latent space. Our method focuses on discarding outliers in latent codes to improve image generation by generating trajectories with Algorithm 1. The model posits that factors of variations can be predicted from the latent code, with a monotonic differentiable function g. This approach is exemplified using the dSprite dataset. The model uses a parametrized function g to predict factors of variation from the latent code, with trainable parameters \u03b8. Piece-wise linear functions are typically used for g \u03b8, but the distribution of the parameter t is generally unknown. The model estimates factors of variation using a parametrized function g with trainable parameters \u03b8. It solves the issue by modeling \u03b4t instead of t, estimating u and \u03b8 by minimizing MSE between \u03b4t and f(\u03b8,u), and can estimate the distribution of images generated by G. The knowledge of g \u03b8 allows for control over image sampling, transforming z \u223c N (0, 1) using an arbitrary distribution \u03c6. This control extends to both individual outputs and the overall distribution of generative models. Understanding these distributions can help identify biases in training datasets. Experiments were conducted on two datasets, including dSprites with binary 64 \u00d7 64 images of white shapes on a dark background. The first dataset, dSprites, consists of 737280 binary 64 \u00d7 64 images with white shapes on a dark background, allowing for the study of disentanglement. The second dataset is ILSVRC, containing 1.2M natural images from one thousand categories. Implementation details include the use of TensorFlow 2.0 and a BigGAN model for experiments. The BigGAN model uses a latent vector z and a one-hot vector to generate images from a specific category. The latent vector is split into six parts for different scale levels in the generator. Conditional Batch Normalization layers are used to modify the style of the generated image. Several \u03b2-VAEs were trained to study disentanglement in generation control. The method was trained on dSprites using an Adam optimizer for 1e5 steps with a batch size of 128 images and a learning rate of 5e\u22124. Evaluation focused on position and scale variations, with saliency detection used for natural images from BigGAN. The evaluation procedure involves extracting the barycenter from an image and using saliency detection with a model implemented in PyTorch. The scale is determined by the proportion of salient pixels, and the method involves sampling latent codes to generate images for evaluating the factor of variation. Jahanian et al. (2019) proposed an alternative method for quantitative evaluation. The proposed alternative method for quantitative evaluation involves using an object detector for assessing x and y shift as well as scale. Results from analyzing categories of objects from ILSVRC show precise control over object position and scale using latent space directions. The study demonstrates that latent space directions found are shared across all categories of interest, as shown in Figure 2. Qualitative results in Figure 3 further support this finding. Additionally, the analysis reveals how position and scale are encoded in the hierarchical latent code used by BigGAN. The study shows that latent space directions are consistent across categories, as seen in Figure 2. Figure 3 provides qualitative support. Position and scale are encoded in the hierarchical latent code of BigGAN, with spatial variations mainly in the first part of the code. Level 5 contributes more to y position than x position and scale. The study analyzed the results on ten categories of the ILSVRC dataset for training and ten other categories for validation, focusing on geometric transformations. The algorithm showed issues with large scales, likely due to poor performance when the object covers most of the image. This could be attributed to correlations between the object's vertical position and background introduced during processing. The study tested the effect of disentanglement on method performance by training \u03b2-VAE on dSprites with varying \u03b2 values. Results in Figure 5 demonstrate the ability to control object position in images by moving in the latent space. The method's effectiveness relies on the level of disentanglement achieved. The effectiveness of the method depends on the degree of disentanglement in the latent space, with better results seen with a larger \u03b2. Increasing \u03b2 leads to a decrease in standard deviation, allowing for more precise control over generated images. This highlights the importance of disentangled representations for controlling the generative process in GAN-like models and auto-encoders. Conditional GANs and VAEs offer ways to obtain latent representations of images, with conditional GANs allowing for user control over generated images based on chosen properties. VAEs face a trade-off between reconstruction accuracy and sample plausibility, with efforts to improve accuracy by identifying plausible regions in the latent space. Our method does not require labels and can find meaningful directions in generative models without changing the learning process. This approach could even be applied to InfoGAN and involves a priori knowledge of the factor of variation sought. Our work focuses on the latent space in generative models, specifically on finding the latent representation of an image without an encoder. Previous works have studied inverting the generator of a GAN to find the latent code of an image. Creswell & Bharath (2016) demonstrated the inversion process on simple datasets like MNIST and Omniglot by optimizing the latent code to minimize reconstruction error. Lipton & Tripathi (2017) improved results on CelebA but struggled with more complex datasets like ILSVRC. The reconstruction loss in Section 2.1.1 is tailored to this problem, enhancing reconstruction quality significantly. In the context of generative models, White (2016) suggests using spherical interpolation to reduce blurriness in reconstructions. A new algorithmic data augmentation technique called \"synthetic attribute\" is proposed to generate less blurry images with a VAE. Recent works on ArXiv emphasize the importance of finding interpretable directions in the latent space to control generative model outputs. The authors describe a method to find interpretable directions in the latent space of the BigGAN model. Their training procedure involves generating a dataset of trajectories before training the model, and they use a saliency model for evaluation instead of MobileNet-SSD v1. The authors use a saliency model instead of MobileNet-SSD v1 to measure performance on more categories. They provide insight on controlling auto-encoders and the impact of disentangled representations on BigGAN's latent space. Their model allows for more precise control over the generative process. In this context, a method is proposed to extract meaningful directions in the latent space of generative models like BigGAN to control properties of generated images. A linear subspace in the latent space can represent factors of variation such as translation and scale, enhancing understanding of the model's representations. The text discusses the use of a reconstruction loss in determining a generated image's fidelity to a target image in the Fourier space, focusing on the contribution of high frequency patterns and the uncertainty in modeling them. The \u03b2-VAE framework aims to discover interpretable latent representations for images without supervision. A convolutional VAE architecture is used to generate 64x64 images, with the optimization process favoring smoother images with fewer high frequencies. The contribution to the total loss directly depends on r^2, with the high frequency contributions being a key factor in the loss function. The decoder network for generating 64x64 images in the \u03b2-VAE framework consists of transposed convolutions and dense layers. The architecture includes convolutional layers with ReLU activation, followed by dense layers with ReLU and identity functions. The final output is obtained through transposed convolutions with sigmoid activation. The study compares different reconstruction methods for image generation, including Convolution + ReLU filters and Transposed Convolution + Sigmoid filters. Results show good reconstruction with certain parameter values, and a comparison with classical Mean Square Error and Structural dissimilarity methods. The study evaluates reconstruction methods for image generation using Mean Square Error (MSE) and Structural dissimilarity (DSSIM) proposed by Zhou Wang et al. (2004). Results demonstrate the accuracy of the reconstruction with an unconstrained latent code and the effectiveness of restricting z to a ball of radius \u221a d to avoid artifacts. Additionally, a quantitative evaluation using Learned Perceptual Image Patch Similarity (LPIPS) by Zhang et al. (2018) was performed on images from the ILSVRC dataset. The study compared reconstruction methods using MSE and DSSIM with LPIPS, showing that images reconstructed with the proposed error are closer to the target image. The curvature of the natural image manifold complicates the optimization problem, especially for non-linear factors like translation or rotation. The trajectory of images undergoing common transformations is curved in pixel space, including translation, rotation, and scaling. PCA analysis of the resulting trajectories shows that for large translations, the shortest path between images in pixel-space is nearly orthogonal. The shortest path between images in pixel-space is near orthogonal for large translations, rotations, and scaling. This causes issues during optimization of the latent code as the gradient of the reconstruction loss is tangent to this direction, resulting in small gradients for the error with respect to the latent code. When optimizing the latent code in generative models, slow-downs and convergence issues can occur when the direction of descent in pixel space is orthogonal to the manifold of natural images. This can result in small gradients for the error with respect to the latent code, especially for large translations, rotations, and scaling in the generated images. When optimizing latent codes in generative models, convergence issues can arise when the descent direction in pixel space is orthogonal to the natural image manifold. Moving an image to the right results in a zero reconstruction error if the circles do not intersect. Qualitative examples for various transformations and brightness levels are shown using the BigGAN model. Latent codes are sampled by adding a learned direction within a certain range. When optimizing latent codes in generative models, convergence issues can arise when the descent direction in pixel space is orthogonal to the natural image manifold. The categories chosen for position and scale produce interesting results, while brightness categories are affected by the absence of dark images in the training data. The direction is learned for position and scale on ten categories, and for brightness on the top five categories."
}