{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning involves evaluating and improving policies using historical data from a logging policy. This is important because on-policy evaluation is costly and has negative effects. A major challenge in off-policy learning is developing counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization with neural network policies show significant improvement over conventional baseline algorithms. Off-policy learning involves evaluating and improving a deterministic policy using historic data, which is crucial due to the high costs and risks associated with on-policy evaluation in real-world scenarios. Utilizing historic data for off-policy evaluation enables safe exploration of policy hypothesis space. Off-policy learning involves evaluating policies using historic data to enable safe exploration of policy hypothesis space. Various methods like Q learning, doubly robust estimator, and self-normalized techniques have been studied in reinforcement learning and contextual bandits. A new direction in off-policy learning involves using logged interaction data with limited feedback in the form of scalar rewards or losses for actions. The challenges of off-policy learning in bandit feedback include handling distribution mismatch between logging policy and new policy, and counterfactual inference difficulties in predicting user reactions to unchosen options. BID34 introduced a new counterfactual risk minimization framework to address the distribution mismatch between logging policy and a new policy. However, the linear stochastic models used for policy parametrization have limited representation power, and computing sample variance regularization is computationally intensive. Developing accurate and efficient training algorithms under this framework remains a challenge. Our contribution in this paper is three-fold: proposing a new learning principle for off-policy learning with bandit feedback by minimizing distribution divergence between the new policy and logging policy, enabling end-to-end training by parametrizing the policy as a neural network, and solving the divergence minimization problem using recent work on variational divergence minimization. Our experiment evaluation on benchmark datasets shows significant improvement in performance over conventional baselines, and case studies also corroborate the soundness of our theoretical proofs. The framework of off-policy learning with logged bandit feedback is reviewed, where a policy maps an input x to a structured output y. In the setting, the input trajectory of the agent determines the output action to take in the next time point. Stochastic policies define a distribution over the output space parametrized by \u03b8. Actions are taken by sampling from this distribution, with each action having a probability of being selected. Feedback on the action sampled is observed in online systems. In online systems, feedback \u03b4(x, y; y*) is observed for actions sampled from h(Y|x) compared to an underlying 'best' y*. The goal of off-policy learning is to minimize expected risk on test data by finding a policy with lower risk than the logging policy h0(Y|x). Improved policy h(Y|x) with lower expected risks R(h) < R(h0). Challenges include skewed distribution of logging policy and need for empirical estimation due to finite samples, leading to generalization error. The vanilla approach to solving the problem of skewed distribution and empirical estimation errors involves propensity scoring using importance sampling. This method addresses distribution mismatch by reweighting the expected risk. However, counterfactual risk minimization has been proposed to address flaws in the vanilla approach, such as lack of loss scaling invariance and high variance. The authors proposed a regularization term for sample variance to address large variance in the dataset. They approximated the regularization term using a first-order Taylor expansion to enable stochastic training. However, this approximation neglects non-linear terms and introduces errors. The authors introduced a regularization term to reduce sample variance, but the approximation neglects non-linear terms and introduces errors. They aim to derive a variance bound directly from the parametrized policy distribution. The authors introduced a regularization term to reduce sample variance and derive a variance bound directly from the parametrized policy distribution. The sampling weight w(z) = p(z) p0(z) is defined by two probability density functions, leading to an identity involving the R\u00e9nyi divergence. This allows for an upper bound on the second moment of the weighted loss. The authors introduced a regularization term to reduce sample variance and derive a variance bound directly from the parametrized policy distribution. The sampling weight w(z) = p(z) p0(z) is defined by two probability density functions, leading to an identity involving the R\u00e9nyi divergence. This allows for an upper bound on the second moment of the weighted loss. The bound is similar to Eq. (4) with the difference that we are now working with a joint distribution over x, y. Detailed proofs can be found in Appendix 1. Theorem 2 provides a generalization bound between the expected risk R(h) and empirical risk R(h) using the distribution divergence function. Let R h be the expected risk of the new policy on loss function \u03b4, and R h be the empirical risk. Additionally, assuming the divergence is bounded by DISPLAYFORM4, we can derive a bound. The proof of the theorem involves Bernstein inequality and the second moment bound, showing bias-variance trade-offs in empirical risk minimization problems. It suggests minimizing variance in bandit learning by not directly optimizing reweighed loss to avoid high variance in testing. In light of the recent success of distributionally robust learning, an alternative formulation of regularized ERM is explored. Instead of optimizing a 'loss + regularizer' objective function, a constrained optimization formulation is studied to minimize variance in testing. The new constrained optimization formulation for regularized ERM, with a regularization hyper-parameter \u03c1, provides a good surrogate for the true risk R(h). It eliminates the need to compute sample variance in existing bounds, but is still effective for parametrized distributions and finite samples. Estimating the divergence function for a parametrized distribution of h(y|x) with finite samples is challenging. Recent f-gan networks and Gumbel soft-max sampling can help minimize variational divergence. The need for stochasticity in the logging policy is emphasized for effective learning. The divergence term calculation in counterfactual learning is challenging due to unexplored regions, leading to an unbounded generalization bound. The objective requires minimizing the square root of a conditional integral, making learning difficult in this scenario. The divergence term calculation in counterfactual learning involves minimizing a convex function to reach a lower bound on the objective. This method draws a connection to the f-divergence measure and utilizes the f-GAN for variational divergence minimization. Applying Fenchel convex duality gives the dual formulation, allowing the swap of operators for tight bounds. The universal approximation theorem of neural networks states they can approximate continuous functions with any precision, making them suitable for the family of functions T in the inequality. The final objective is a saddle point of a function that maps input pairs to a scalar value, with the policy acting as a sampling distribution. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence. The true divergence is denoted by Df, and the empirical estimator is used to approximate it. The estimator used is based on the empirical distribution obtained by sampling from two distributions. The estimation error is decomposed into terms related to neural networks and empirical mean estimation. The universal approximation theorem states that the error is zero, and there exists a parameter in the family of neural networks that matches the true distribution. The difference between the empirical distribution and the underlying population is also considered. The difference between an empirical distribution and the underlying population distribution is verified using the strong law of large numbers. The ratio and f*(T0) are shown to be integrable, leading to the conclusion that the term approaches zero. The second term is also shown to approach zero. A generative-adversarial approach is then applied by representing the T function as a discriminator network and parametrizing the policy distribution h(y|x). The policy distribution h(y|x) is parametrized as a neural network h \u03b8 (y|x) for structured output problems. Gumbel soft-max sampling is used for differential sampling from the distribution h(y|x). The training procedure includes sampling from the logging policy h 0, an initial generator distribution h \u03b8 0 (y|x), and an initial discriminator function T w 0 (x, y) to optimize the generator h \u03b8 * (y|x). The text chunk discusses the optimization of a generator distribution h \u03b8 * (y|x) to minimize divergence from the initial distribution h 0. It involves sampling real and fake samples, updating the model, and training algorithms for counterfactual risk minimization from logged data. The algorithm presented in the text chunk optimizes a generator distribution to minimize divergence from the initial distribution. It involves sampling, updating the model, and training for counterfactual risk minimization from logged data. The algorithm works in two separate training steps: 1) update the policy parameters to minimize reweighed loss, and 2) update the generator and discriminator parameters to regulate variance for improved generalization. Exploiting historical data is crucial in multi-armed bandit problems and its variants like contextual bandit. Bandits problems, including multi-armed bandit and contextual bandit, have wide applications. Doubly robust estimators have been proposed for these problems, and recent theoretical studies have explored their minimax risk lower bound. Techniques from reinforcement learning, such as Q function learning and temporal difference learning, can also be applied to bandits problems. Off-policy learning in reinforcement learning (RL) involves methods like Q function learning and temporal difference learning, which account for the Markov property of the decision process. Recent works in deep RL have also addressed off-policy updates using techniques such as multi-step bootstrapping and off-policy training of Q functions. Learning from log traces involves applying propensity scores to evaluate candidate policies, similar to treatment effect estimation in statistics. Variance regularization in off-policy learning with bandit feedback aims to address distribution mismatch between training and testing data, known as covariate shift. This technique is derived from observational studies and aims to improve generalization performance in computational advertising. The text discusses variance regularization in supervised learning to address distribution mismatch, also known as covariate shift. It explores connections to distributionally robust optimization techniques and suggests applying divergence minimization techniques for domain adaptation problems. To evaluate proposed algorithms, a conversion from supervised learning to bandit feedback method is followed. A logging policy is constructed for a given dataset, and feedback is collected for each sample. Conditional random field is also used for benchmarks. The conversion from supervised learning to bandit feedback involves using a logging policy and conditional random field for benchmarks. Bandit feedback datasets are created with samples passed through the logging policy, actions recorded, and propensity scores calculated. Evaluation metrics for the probabilistic policy include expected loss as a direct measure of performance. The average hamming loss of maximum a posteriori probability (MAP) prediction is a faster way to generate predictions without sampling. However, a model with high MAP performance but low generalization performance might be overfitting. The comparison between different algorithms for improving performance in neural network policies is conducted, including Vanilla importance sampling algorithms, counterfactual risk minimization algorithm, and neural network policies without divergence regularization. Hyperparameters are selected based on validation set performance. In comparison to other algorithms for improving neural network policies, the effectiveness of variance regularization is verified using the \"NN-NoReg\" baseline. Four multi-label classification datasets from the UCI machine learning repo are used, with a three-layer feed-forward neural network for policy distribution and a two or three layer feed-forward neural network for divergence minimization. The separate training version 2 is used for benchmark comparison due to faster convergence and better performance. The networks are trained with Adam optimizer with different learning rates for reweighted loss and divergence minimization. PyTorch was used for implementation and training on Nvidia K80 GPU cards. Results from 10 experiment runs are reported with two evaluation metrics. Two Gumbel-softmax sampling schemes, NN-Soft and NN-Hard, are compared in the study. By introducing neural network parametrization of policies, test performance significantly improves compared to baseline CRF policies. Additional variance regularization further enhances testing and MAP prediction loss. No significant difference observed between Gumbel soft-max sampling schemes. Varying the maximum number of iterations quantitatively studies the effectiveness of variance regularization. In the study, variance regularization is quantitatively analyzed by varying the maximum number of iterations for divergence minimization. Results show that models with no regularization have higher loss and slower convergence rates, while increasing the maximum iterations leads to faster decrease in test loss. By increasing the maximum iterations for divergence minimization, the test loss decreases faster and the final test loss is lower. This indicates that adding a regularization term helps the learned policies generalize better to test sets. Stronger regularization through more divergence minimization steps leads to better test performance and faster convergence of the training algorithm. Theoretical bounds suggest that algorithm generalization improves with more training samples. Varying the number of passes of training data to sample an action also impacts the logging policy. When the number of training samples in the bandit dataset increases, models with and without regularization show improved test performance in expected loss, reaching a stable level. Regularized policies demonstrate better generalization performance compared to those without regularization. Stronger regularization enhances generalization ability, as indicated by MAP performance. In this section, experiments compare two training schemes: cotraining in Alg. 3 and an easier version Alg. 2. The figures suggest that blending weighted loss and distribution divergence performs slightly better than the model without regularization. The training scheme with regularization performs slightly better than the one without, but balancing the gradient is challenging. There is no significant difference between the two sampling schemes of Gumbel-softmax. The effect of logging policies on learning performance is discussed, emphasizing the importance of stochasticity. Additional visualizations of metrics can be found in Appendix 7. The stochasticity of the logging policy is tested by modifying the parameter h 0 with a temperature multiplier \u03b1. Increasing \u03b1 leads to a more deterministic policy. NN policies outperform logging policies in terms of test loss improvement. After testing the stochasticity of the logging policy by adjusting the parameter h 0 with a temperature multiplier \u03b1, it was found that NN policies perform better than logging policies when h 0 is sufficiently stochastic. However, as the temperature parameter exceeds 2/3, it becomes increasingly difficult, and sometimes impossible, to learn improved NN policies. The drop in performance ratios is mainly due to the decreased loss of the logging policy h 0. Additionally, NN policies with stronger regularization show slightly better performance compared to weaker ones, demonstrating the robustness of the learning principle. The regularization helps improve the NN policy's robustness and generalization performance as the stochasticity of h 0 decreases. Models with better h 0 constantly outperform baselines, but face increasing difficulty. The impact of logging policies on learned improved policies is discussed, with a focus on hamming loss and sampling biases. More visualizations of metrics can be found in the appendix. The study explores the impact of logging policies on policy accuracy and sampling biases. By varying the proportion of training data points used, improved policies outperform the logging policy, addressing sampling biases. The increasing ratios of test expected loss to h 0 performance indicate relative policy improvement. The study investigates the impact of logging policies on policy accuracy and sampling biases. It suggests that regularizing variance can enhance generalization performance in off-policy learning for logged bandit datasets. The proposed training principle combines importance reweighted loss with a regularization term measuring distribution divergence. Techniques like variational divergence minimization and Gumbel soft-max sampling are applied to achieve this. By applying variational divergence minimization and Gumbel soft-max sampling techniques, neural network policies are trained end-to-end to minimize the variance regularized objective. Evaluations on benchmark datasets confirmed the effectiveness of the learning principle and training algorithm. Limitations include the need for propensity scores, which may not always be available. Learning to estimate propensity scores and incorporating them into the training framework will enhance the applicability of the algorithms. The work focuses on off-policy learning from logged data and suggests directly learning importance weights for theoretical guarantees. The techniques and theorems can be extended to general supervised learning and reinforcement learning. Applying Lemma 1 to importance sampling weight function and loss function yields interesting results. The work focuses on off-policy learning from logged data and suggests directly learning importance weights for theoretical guarantees. By applying Lemma 1, the variance can be bounded using Reni divergence. Bernstein's concentration bounds are used to obtain bounds for importance sampling in bandit learning. The goal is to optimize a generator that minimizes R(w) using a regularization hyper-parameter \u03bb. The algorithm involves updating the discriminator and generator using mini-batches of real and fake samples. The generator gradient is estimated to minimize variance regularized risk in co-training. The dataset statistics are reported, and the effect of stochasticity on test loss is analyzed. The logging policy's impact on NN policies is analyzed in terms of expected loss and loss with MAP predictions. While NN policies can improve over the baseline h0, there is no clear trend in MAP prediction performance. Further investigation is needed to understand this phenomenon. Despite increases in logging policy, NN policies can still improve over h0 in expected loss. However, beating MAP predictions is challenging for NN policies if the logging policy has been exposed to full training data and trained in a supervised fashion."
}