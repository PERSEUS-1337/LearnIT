{
    "title": "Byl5NREFDr",
    "content": "We study model extraction in natural language processing, where an adversary reconstructs a victim model using only query access. The attacker can successfully mount the attack without real training data or using grammatically correct queries. Random word sequences with task-specific heuristics are effective for model extraction on various NLP tasks. Machine learning models represent valuable intellectual property, and attackers can extract a model that performs slightly worse than the victim model with a budget of a few hundred dollars. Defense strategies like membership classification and API watermarking can be circumvented by clever adversaries. Model stealing or model extraction involves malicious users locally reproducing existing models served through web APIs to avoid the high costs of model development. This can lead to theft of intellectual property and leakage of sensitive information about the training data. Contextualized pretrained representations like ELMo and BERT are popular for NLP APIs due to their performance boost and reduced sample complexity. These representations often require only a shallow task-specific network, such as a single layer in BERT. In this paper, it is demonstrated that NLP models fine-tuned from a pretrained BERT model can be extracted without access to the training data. Extraction attacks are possible with randomly sampled word sequences and simple heuristics, contrasting with prior work that requires access to semantically-coherent data for large-scale attacks. Extraction attacks on NLP models can be conducted without access to training data, using randomly sampled word sequences and simple heuristics. The attacker forms queries from sampled words, fine-tunes their own BERT model using victim outputs as labels, and the cost of these attacks is relatively low. The attacker conducts extraction attacks on NLP models using randomly sampled word sequences and victim outputs as labels to fine-tune their own BERT model. Despite the nonsensical nature of the queries, they are effective in extracting good models, with queries closer to the original data distribution yielding better results. The study explores how pretraining on the attacker's side can facilitate model extraction, and evaluates the effectiveness of defenses such as membership classification and API watermarking. While these defenses work against simple adversaries, they are ineffective against more sophisticated attacks. The research aims to inspire further investigation into stronger defenses against model extraction and a deeper understanding of the vulnerabilities in models and datasets. Model extraction attacks have been studied empirically and theoretically, mostly against image classification APIs. These attacks involve synthesizing queries close to the victim classifier's decision boundaries, which does not transfer well to text-based systems. Model extraction attacks have been studied against image classification APIs by synthesizing queries close to the victim classifier's decision boundaries. However, this method does not transfer well to text-based systems due to the discrete nature of the input space. Previous work by Pal et al. (2019) focused on pool-based active learning for natural sentence extraction from WikiText-2. In contrast, this study explores extraction on modern BERT-large models for tasks expecting pairwise inputs like question answering. This work is related to data-efficient distillation methods aiming to distill knowledge from larger models to smaller ones with limited input data. Unlike model extraction attacks on image classification APIs, which do not transfer well to text-based systems, this study focuses on extracting knowledge from modern BERT-large models for tasks like question answering. Previous work has explored pool-based active learning for natural sentence extraction, while this study aims to distill knowledge from larger models to smaller ones using rubbish inputs, which are randomly-generated examples that yield high-confidence predictions. BERT-large, a 24-layer transformer model, is studied for model extraction. Unnatural text inputs have been effective in training models for real NLP tasks without real examples. BERT-large, a 24-layer transformer model, known as BERT, revolutionized NLP by achieving state-of-the-art performance on various tasks with minimal supervision. The model converts word sequences into contextualized vector representations, learned through masked language modeling on unlabeled text data. NLP systems typically leverage BERT for fine-tuning with task-specific networks. The methodology involves using a task-specific network with parameters to construct a composite function, learned through fine-tuning. Description of extraction attacks involves reconstructing a local copy of a commercially available black-box API model using a task-specific query generator. The attacker uses a task-specific query generator to create nonsensical word sequences as queries to the victim model. The resulting dataset is used to train a new model. The extraction attacks involve reconstructing a black-box API model using a task-specific network. The curr_chunk discusses different tasks such as probability distribution, ternary NLI classification, extractive QA, and boolean QA. It also mentions two query generators, RANDOM and WIKI. The curr_chunk discusses two query generators, RANDOM and WIKI, used for tasks with complex interactions. Task-specific heuristics are applied for tasks like MNLI to improve model performance. The curr_chunk explains how query generation is done for different tasks like MNLI, SQuAD, and BoolQ. It involves replacing words in the premise to construct the hypothesis and sampling words from the passage to form a question. The process is evaluated in a controlled setting with examples provided in the appendices. In a controlled setting, attackers use the same number of queries as the original training dataset. Different query budgets are explored for each task, with commercial cost estimates provided using Google Cloud Platform's Natural Language API calculator. Evaluation metrics include accuracy of extracted models on the original development set and agreement between them. High accuracies are achieved even at low query budgets, with diminishing gains at higher budgets. In a controlled setting, attackers achieve high accuracy with extracted models on original development sets, even when trained with nonsensical inputs. Despite only seeing nonsensical questions during training, extracted SQuAD models recover 95% of original accuracy on WIKI. However, agreement between the outputs of the extracted model and the victim model is only slightly better than accuracy in most cases. Agreement is lower on held-out sets constructed using the WIKI and RANDOM sampling scheme. Extracted WIKI and RANDOM models have low agreements on SQuAD despite being trained on the same data. This indicates poor functional equivalence between the victim and extracted model. The study found that even when the API only provides argmax outputs, there was only a minimal drop in accuracy in model extraction experiments. This suggests that hiding the full probability distribution is not an effective defense strategy. Additionally, the effectiveness of extraction algorithms was measured with varying query budgets, showing that extraction can still be successful even with small query budgets. In model extraction experiments, even with small query budgets, extraction is often successful. Accuracy gains diminish with more queries. The cost of attacks can be estimated from the results. The effectiveness of nonsensical input queries for model extraction raises questions about their properties and the use of large pretrained language models. In model extraction experiments, even with small query budgets, extraction is often successful. Accuracy gains diminish with more queries. The cost of attacks can be estimated from the results. NLP models based on BERT are examined to understand their performance. Different victim models are tested with nonsensical queries to see if they produce the same answers. The RANDOM and WIKI extraction configurations for SQuAD are specifically analyzed. The study analyzed SQuAD models with varying random seeds, achieving F1 scores between 90 and 90.5. Models showed high agreement on SQuAD training and development set queries but dropped significantly on WIKI and RANDOM queries. The results suggest that victim models are brittle on nonsensical inputs, with high-agreement queries possibly closer to the original data distribution. High-agreement queries are more useful for model extraction as models tend to be brittle on nonsensical inputs. Sorting queries from RANDOM and WIKI datasets by agreement levels, high-agreement subsets consistently show large F1 improvements in model extraction compared to random and low-agreement subsets of the same size. This indicates that agreement between victim models serves as a good indicator of input-output pair quality for extraction. The study explores the potential of using high-agreement nonsensical queries for model extraction and the interpretability of such queries by humans. The researchers investigate if nonsensical textual inputs with high agreement among models have a human interpretation by involving human annotators in answering SQuAD questions. An experiment was conducted where human annotators answered SQuAD questions from different subsets. Annotators matched victim models' answers 23% of the time on the WIKI subset and 22% on RANDOM, but scored significantly higher on original SQuAD questions (77% exact match). Annotators used a word overlap heuristic to select answer spans, as revealed in interviews. In an experiment, human annotators matched victim models' answers using a word overlap heuristic. The extraction accuracy depends on the pretraining setup, especially when the attacker fine-tunes a different base model or extracts a QA model from scratch. The extraction accuracy heavily relies on the pretraining setup, with BERT coming in two sizes: BERT-large and BERT-base. Results show higher accuracy when the attacker starts from BERT-large, even if the victim was initialized with BERT-base. Matching models yield better accuracy when using the same model. This aligns with previous discussions on the topic. Training QANet model on SQuAD without pretraining shows high accuracy with original inputs but degrades significantly on nonsensical data. BERT-based models show a significant drop in F1 score when trained on nonsensical queries, highlighting the importance of better pretraining for language representation. The focus now shifts to investigating defense strategies that preserve API utility and remain undetectable to attackers without requiring re-training the victim model. The defense strategy discussed in the curr_chunk involves using membership inference to detect nonsensical or adversarial inputs and issue random outputs to eliminate extraction signals. The defense strategy involves using membership inference to detect nonsensical or adversarial inputs and issue random outputs to eliminate extraction signals. Membership inference is treated as a binary classification problem using datasets from MNLI and SQuAD labeled as real and fake examples. Logits and final layer representations of the victim model are used as input features for training the classifier, which transfers well to a balanced development set with the same distribution as the training data. The classifiers are robust to the query generation process. The defense strategy involves using membership inference to detect nonsensical or adversarial inputs and issue random outputs to eliminate extraction signals. Watermarking is another defense method where a fraction of queries are modified to return incorrect outputs, stored on the API side. This anticipates that extracted models will struggle with memorizing arbitrary information. Watermarking is a defense strategy to protect models from memorizing watermarked queries, making them vulnerable to detection. Evaluation on MNLI and SQuAD shows high accuracy in predicting watermarked outputs but low accuracy in predicting original labels. Watermarking is used to protect models from memorizing watermarked queries. Watermarked models have high WM Label Acc and low Victim Label Acc. Only 0.1% of queries are watermarked to minimize API performance drop. Non-watermarked models perform poorly on watermarked data, predicting victim model outputs. Training with more epochs exacerbates these differences. Watermarking is effective in protecting models from memorizing watermarked queries, with differences becoming more pronounced with increased training epochs. However, its limitations include the need for an attack to have already occurred and the assumption that the attacker will make the extracted model publicly available with query access. Attackers can evade detection by employing strategies like differentially private training, fine-tuning the model, or issuing random outputs on specific queries. Model extraction attacks against NLP APIs serving BERT-based models are effective even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, requiring further research for robust defenses against adaptive adversaries. Future directions following the results in this paper include leveraging nonsensical inputs to improve model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity. Cost estimates were obtained from Google Cloud Platform's Calculator. In this paper, cost estimates were obtained from Google Cloud Platform's Calculator for Natural Language APIs. Inputs up to 1000 characters per query are allowed, with costs calculated for datasets by counting instances over 1000 characters multiple times. Extrapolated costs for tasks not covered by Google Cloud APIs were based on entity analysis and sentiment analysis APIs. The models studied are single layers in addition to BERT-large. The model studied in this paper is a single layer in addition to BERT-large, requiring a similar number of FLOPs for similar input lengths. It is difficult to estimate the cost of issuing a certain number of queries, with some API providers offering a small budget of free queries. Attackers could exploit multiple accounts to collect data in a distributed manner. Most APIs are used on webpages and can be easily emulated to extract information at a large scale without cost. API costs can vary depending on computing infrastructure or revenue model. It is important to focus on low costs for extracting datasets rather than actual estimates. Tasks like machine translation and speech recognition are relatively inexpensive. For example, it costs $430.56 to extract a large conversational speech recognition dataset and $2000.00 for 1 million translations. The speech recognition dataset costs $2000.00 for 1 million translation queries of 100 characters each. Input generation algorithms for datasets include building a vocabulary from wikitext103 and sampling tokens uniformly. The top 10000 tokens from wikitext103 are kept, while the rest are discarded. A sentence is randomly selected from wikitext103 and words not in the top-10000 vocabulary are replaced with random words from this vocabulary. The process is repeated three times to create the final hypothesis by choosing and replacing words randomly. This process is similar to how premises are sampled in MNLI and SST2 datasets. The hypothesis is sampled in a manner identical to MNLI and SST2 datasets. A vocabulary is built using wikitext103, and paragraph tokens are randomly sampled to construct the final paragraph and question. The question is generated by sampling a paragraph from wikitext103 and appending it with a randomly chosen question starter word. The question is not appended with a ? symbol to match the format of BoolQ dataset. In this section, additional query generation heuristics are studied by comparing extraction datasets for SQuAD 1.1. Findings show that starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. A similar ablation study on MNLI also reveals the effectiveness of this approach. The study compares extraction datasets for SQuAD 1.1 and MNLI. Findings indicate that using common question starter words like \"what\" improves extraction, especially with RANDOM schemes. For MNLI, when there is low lexical overlap between premise and hypothesis, the model predicts neutral or contradiction, while high overlap leads to entailment prediction, resulting in an unbalanced dataset. Balanced datasets with edit-distance 3 or 4 words show strong extraction signals. Using frequent words aids extraction. The study compared extraction datasets for SQuAD 1.1 and MNLI, finding that using common question starter words like \"what\" improves extraction, especially with RANDOM schemes. For MNLI, low lexical overlap between premise and hypothesis leads to neutral or contradiction predictions, while high overlap results in entailment prediction, creating an unbalanced dataset. Balanced datasets with edit-distance 3 or 4 words show strong extraction signals. Frequent words aid extraction, as shown in Table 13 with examples provided. Human studies involved fifteen annotators who annotated five sets of twenty questions each, with different question sets including original SQuAD questions, WIKI questions with highest and lowest agreement among victim models, and RANDOM questions with highest and lowest agreement among victim models. In an ablation study on input features for the membership classifier, two input feature candidates are considered: the logits of the BERT classifier. The study shows that the average pairwise F1 follows the order original SQuAD >> WIKI, highest agreement > RANDOM, highest agreement \u223c WIKI, lowest agreement > RANDOM, lowest agreement. This ordering reflects the closeness to the actual input distribution. The ablation study compared two input feature candidates for the membership classifier: the logits of the BERT classifier and the last layer representations. Results show that the last layer representations are more effective in distinguishing between real and fake inputs, but the best results are achieved by using both feature sets. The last layer representations are more effective in classifying points as real or fake."
}