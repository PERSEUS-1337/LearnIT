{
    "title": "S1e-0kBYPB",
    "content": "In this work, the focus is on developing methods to explain the decisions of black-box AI models like neural networks. Two issues with current explanatory methods are identified: different perspectives on explanations lead to different instance-wise explanations, and post-hoc explainers have only been validated on simple models, not real-world neural networks. The text introduces a verification framework for explanatory methods in neural networks, focusing on feature selection. The framework is based on a complex neural network architecture trained on a real-world task, providing guarantees on its inner workings. The efficacy of the evaluation is validated by highlighting the failure modes of current explainers. The goal is to offer a readily available evaluation tool for feature selection in neural networks. A framework for evaluating post-hoc explanatory methods in black-box machine learning models, focusing on feature-selection perspective. Various methods have been developed to shed light on accurate models, with two main perspectives: feature-additivity and feature-selection. In the context of evaluating post-hoc explanatory methods in black-box machine learning models, two main perspectives are feature-additivity and feature-selection. Different methods have been developed for model explanation, with comparisons between feature-selection and feature-additivity explainers like L2X, LIME, and SHAP. In evaluating post-hoc explanatory methods in black-box machine learning models, two main perspectives are feature-additivity and feature-selection. Comparisons between these explainers like L2X, LIME, and SHAP highlight the strengths and limitations of each approach. The reliability of current explanatory methods in less dramatic bias scenarios remains an open question due to the unknown decision-making process of neural networks. The evaluation of explainers for neural networks involves assuming the target models behave reasonably, despite the unknown decision-making process. Assumptions about model behavior may be flawed due to surprising spurious correlations in human-annotated datasets. The framework proposed addresses the issue of spurious correlations in human-annotated datasets by generating evaluation tests for explanatory methods. The tests involve identifying tokens with zero contribution to the model's prediction on each instance in a dataset. The framework proposed aims to address spurious correlations in human-annotated datasets by generating evaluation tests for explanatory methods. It involves identifying tokens with zero contribution to the model's prediction and testing if explainers rank them higher than relevant tokens. The framework was applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis. The framework introduced generates evaluation tests for explainers to test for critical failures in target models. It provides guarantees on the behavior of the target model and penalizes explainers only when errors are guaranteed. This approach is the first to offer an automatic evaluation test without speculating on the target model's behavior. L2X, a feature-selection explainer, was evaluated using this framework. In a study comparing LIME and SHAP explainers to L2X, it was found that LIME and SHAP generally outperformed L2X. The reasons for this will be detailed in Section 5. The error rates of these explainers highlight potential failures in feature-selection methods. The study compares LIME and SHAP explainers to L2X, finding that LIME and SHAP generally outperform L2X. The explainers predict relevant tokens, even with zero contribution. A generic methodology for evaluation is proposed, applicable to other tasks. Feature-based explanatory methods are common, explaining predictions in terms of input unit-features. Feature-based explainers provide explanations by attributing signed weights to input features, either additively or selectively. Additive explainers assign weights to each feature based on their contribution to the model's prediction, while selective explainers identify a subset of features responsible for the prediction. Example-based explanations are also utilized in some cases. In this work, the focus is on verifying feature-based explainers, which attribute signed weights to input features for model prediction. It remains an open question how to validate their faithfulness to the target model effectively. The focus is on verifying feature-based explainers by evaluating their faithfulness to the target model through different methods like interpretable target models and synthetic setups. The faithfulness of feature-based explainers is evaluated through methods like interpretable target models and synthetic setups. For instance, L2X was tested on various synthetic tasks to control important features, potentially simplifying the job for explainers. Another approach involves assuming certain intuitive heuristics that high-performing target models are expected to follow, such as relying on specific linguistic patterns in sentiment analysis. The evaluation of feature-based explainers relies on adjectives and adverbs in agreement with predicted sentiment. Crowd-sourcing evaluation is used to assess if explainer features align with the model's prediction. However, neural networks may discover unexpected artifacts, making this evaluation unreliable. Explanations are tested to see if they help humans predict the model's behavior. The evaluation of feature-based explainers relies on adjectives and adverbs in agreement with predicted sentiment. Crowd-sourcing evaluation is used to assess if explainer features align with the model's prediction. However, neural networks may discover unexpected artifacts, making this evaluation unreliable. Our evaluation is fully automatic, focusing on a non-trivial neural network model. Our evaluation is automatic, focusing on a neural network model with guarantees on its inner-workings. It is more challenging than previous tests, requiring a strong fidelity of the explainer to the model. Explanatory methods adhere to two major perspectives: Feature-additivity for a model f and an instance x. Explanations of predictions in models like LIME are based on feature-additivity, where contributions from each feature approximate the prediction. Lundberg & Lee unified methods adhering to this perspective by verifying desired constraints. The Shapley values from game theory provide contributions that verify three desired constraints (local accuracy, missingness, and consistency). These values calculate the contribution of each feature in an instance by averaging its contributions over a neighborhood of the instance, masking out combinations of features. The choice of neighborhood is crucial for feature combinations in explanations of model predictions. Different perspectives exist, such as selecting a subset of features that lead to similar predictions as the original model. Various methods like L2X aim to find this subset by maximizing mutual information with the prediction. In 2018, S(x) is learned by maximizing mutual information with the prediction, assuming the number of important features per instance is known. However, this may not always be true, especially for sentiment analysis tasks. Instance-wise explanations are provided in Figure 1 to illustrate the differences between relying on a subset of features versus using all features. The sentiment analysis regression model assigns scores from 0 (most negative) to 1 (most positive), similar to real-world neural networks. Biases in datasets can heavily influence model behavior, as shown by Gururangan et al. (2018). For example, neural networks trained on SNLI may rely on specific tokens that are not always accurate indicators of the target class. This is illustrated in Figure 1, highlighting the differences between relying on a subset of features versus using all features. The feature-additive explanation in Figure 1 shows the relevance of \"nice\" and \"good\" in the sentiment analysis model. It explains how the model relies on different features to provide scores for instances. The feature-additive perspective aims to give an average explanation of the model on a neighborhood of the instance, while the feature-selective perspective focuses on the pointwise features used by the model on the instance. The feature-selective perspective provides insights into the model's behavior by ranking important features differently for different instances. This perspective aims to highlight specific features used by the model on individual instances, offering a more detailed explanation compared to the feature-additive perspective. In this paper, a verification framework is proposed for the feature-selection perspective of instance-wise explanations using the RCNN model. The dataset is pruned to identify irrelevant and relevant features for each datapoint, and metrics are introduced to evaluate explainers' performance in ranking these features accurately. The RCNN model consists of a generator and an encoder, both using recurrent convolutional neural networks. The generator selects a subset of tokens from the input text and passes it to the encoder for making predictions. There is no direct supervision on the subset selection process. The RCNN model consists of a generator and an encoder trained jointly with supervision only on the final prediction. Two regularizers are used to encourage the selection of a short sub-phrase and fewer tokens. Gradients for the generator are estimated using a REINFORCE-style procedure to handle non-differentiability. This facilitates the existence of tokens that do not contribute to the final prediction. The RCNN model aims to eliminate handshakes, where non-selected tokens contribute to the prediction. The goal is to create a dataset where non-selected tokens have zero impact on the model's output. The RCNN model aims to eliminate handshakes, where non-selected tokens have zero impact on the model's output. In an example, the model selects \"very\" with a score of 1, but selecting only \"very\" results in a score of 0.5, showing the presence of a handshake. The presence of S Sx = S x does not always indicate a handshake. The dataset is pruned to retain instances where S Sx = S x to eliminate handshakes, ensuring non-selected tokens do not impact the prediction. However, it is uncertain if all non-selected tokens are relevant to the prediction. The dataset is pruned to ensure that non-selected tokens do not impact the prediction. Some tokens may be noise but are selected to maintain a contiguous sequence. The dataset is further pruned to have at least one clearly relevant token for the prediction. The dataset is partitioned into clearly relevant tokens (SR x) and selected tokens that are not known to be relevant or noise (SDK x). The selected token s is considered relevant if the absolute change in prediction after its removal exceeds a threshold \u03c4. The procedure ensures that tokens changing the prediction by a high threshold are important and should be ranked higher. The dataset is pruned to retain instances with at least one clearly relevant token. The procedure does not provide an explainer with rankings or contribution weights. The procedure ensures that tokens changing the prediction by a high threshold are important and should be ranked higher. We evaluate explainers that provide a ranking over the features, with defined error metrics based on the importance of tokens in the instance. The evaluation of explainers providing rankings over features includes metrics to assess the accuracy of token importance. Metrics (A) highlights instances where the most important token is not selected, (B) identifies errors in explanations, and (C) measures the average number of non-selected tokens ranked higher than relevant ones. The RCNN model trained on the BeerAdvocate corpus evaluates explanations for errors and zero-contribution features. BeerAdvocate contains 100K human-generated beer reviews with aspects like appearance, aroma, and palate. The RCNN predicts ratings rescaled between 0 and 1. The RCNN model predicts ratings rescaled between 0 and 1 for three aspects independently. Three datasets are gathered, each with non-selected tokens having zero contribution to the prediction. A threshold of 0.1 is chosen for relevant tokens, considering a change in prediction of 0.1 as significant. The study provides statistics on dataset lengths and token selection criteria. They aim for strict thresholds to ensure high evaluation guarantees. Three popular explainers, LIME and SHAP, are tested for evaluation. In the study, three popular explainers, LIME, SHAP, and L2X, were tested. LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature-selection explainer. The study compared three explainers: LIME, SHAP, and L2X. L2X, a feature-selection explainer, underperformed on most metrics. One limitation of L2X is the need to know the number of important features per instance. To address this, the study used the average number of tokens highlighted by human annotators as a parameter for L2X. The study compared three explainers: LIME, SHAP, and L2X. L2X underperformed on most metrics, with an average K of 23, 18, and 13 for the three aspects. In Table 1, both LIME and L2X often ranked zero-contribution tokens higher than clearly relevant features, indicating ranking mistakes. SHAP performed better, placing fewer zero-contribution tokens ahead of relevant ones. L2X ranks around 3-4 zero-contribution tokens before a clearly relevant one for all three aspects, with an average K of 23, 18, and 13. The heatmap in Figure 6 shows the ranking of tokens by each explainer, with the intensity of color decreasing linearly with the ranking. Only the top 10 ranked tokens are shown for visibility. The heatmap displays the top 10 ranked tokens by each explainer, showing LIME and SHAP attributing importance to nonselected tokens like \"mouthfeel\" and \"lacing\". L2X ranks \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as most important, with \"gorgeous\" not making the top 13 tokens for L2X. In this work, the authors shed light on the distinction between two perspectives of explanations and introduce an evaluation test for post-hoc explanatory methods. They present error rates for three popular explanatory methods to raise awareness of their performance. The authors evaluate error rates for three popular explanatory methods to highlight potential failures in predictions. Their methodology can be applied to various tasks and areas, such as computer vision, by checking for zero contribution of non-selected elements. The authors evaluate error rates for explanatory methods, highlighting failures in predictions. Evaluation includes statistics on dataset instances, average review length, and token selection impact on prediction differences. The column provides statistics on dataset instances eliminated due to potential handshakes and further eliminated datapoints without selected tokens with significant effect on prediction. The model's prediction should differ when a non-selected token influencing the final prediction is removed. The model's prediction changes when a non-selected token is removed, indicating no handshake in the instance. This implies zero contribution from the eliminated tokens. The model's prediction changes when a non-selected token is removed, indicating no handshake in the instance. This implies zero contribution from the eliminated tokens. In order to satisfy the condition of Equation 11, it is sufficient to have S Sx = S x, finishing the proof. The beer LIME has a nice brown \"grolsch\" like bottle, pours a dark yellow color with a lot of head, fizzy, smells like fruit, tastes of fruit with a slight warming sensation. The beer LIME has a nice brown \"grolsch\" like bottle, pours a dark yellow color with a lot of head, fizzy, smells like fruit, tastes of fruit with a slight warming sensation. The taste is better than most American lagers, very smooth, with hints of apple and blueberry. The beer LIME has a nice brown \"grolsch\" like bottle, pours a dark yellow color with a lot of head, fizzy, smells like fruit, tastes of fruit with a slight warming sensation. It is better than most American lagers, very smooth, with hints of apple and blueberry. Figure 5 shows the rankings of explainers on an instance from the appearance aspect in the evaluation, with the top 5 features displayed on the right-hand side."
}