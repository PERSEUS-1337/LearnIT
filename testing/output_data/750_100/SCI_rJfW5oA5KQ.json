{
    "title": "rJfW5oA5KQ",
    "content": "Recent works have shown that Generative Adversarial Networks (GANs) suffer from lack of diversity or mode collapse, as powerful discriminators cause overfitting while weak discriminators cannot detect mode collapse (Arora et al., 2017a). In contrast to previous findings on GANs suffering from mode collapse, this paper demonstrates that GANs can effectively learn distributions in Wasserstein distance or KL-divergence with polynomial sample complexity by utilizing discriminators with strong distinguishing power tailored to specific generator classes. This approach involves designing discriminators, often neural networks, that induce Integral Probability Metric (IPM) to approximate Wasserstein distance and/or KL-divergence. The paper demonstrates that GANs can effectively learn distributions in Wasserstein distance or KL-divergence with polynomial sample complexity by utilizing discriminators with strong distinguishing power tailored to specific generator classes. The lack of diversity in GANs may be caused by sub-optimality in optimization rather than statistical inefficiency. Various ideas have been proposed to improve the quality of learned distributions and training stability in GANs, but there is still a lack of understanding. Recent work has highlighted concerns about mode collapse and lack of diversity in distributions learned by GANs. The paper discusses how mode collapse in GANs can be alleviated by designing discriminators with strong distinguishing power. It focuses on the Wasserstein GAN formulation and introduces the F-Integral Probability Metric for comparing distributions. WGAN aims to learn the data distribution p by setting up generators G and discriminators F. The Wasserstein-1 distance is used when F consists of all 1-Lipschitz functions. Parametric families like neural networks are used to approximate Lipschitz functions for optimization via gradient-based algorithms. The main concern with GANs is \"mode collapse,\" where the learned distribution generates high-quality but low-diversity examples due to the weakness of IPM compared to W 1. This problem arises from the fact that a mode-dropped distribution can fool IPM, leading to a scenario where W F (p, q) \u03b5 and W 1 (p, q) 1 simultaneously. The weakness of the discriminator in GANs can lead to mode collapse, where the learned distribution lacks diversity. Increasing the discriminator to larger families like all 1-Lipschitz functions is a natural solution, but it may not generalize well. The Wasserstein-1 distance used in optimization can be far from the population distance, even for typical distributions like a spherical Gaussian. The lack of diversity in GANs due to weak discriminators can lead to mode collapse. Increasing the discriminator to larger families like all 1-Lipschitz functions is a solution, but may not generalize well. The Wasserstein-1 distance used in optimization may not accurately represent the population distance, even for typical distributions like a spherical Gaussian. The dilemma arises from powerful discriminators causing overfitting, while weak discriminators result in diversity issues. The paper proposes a solution to the lack of diversity in GANs by designing a strong discriminator class F against a specific generator class G. The discriminator class F has restricted approximability with respect to G and the data distribution p, distinguishing them as well as all 1-Lipschitz functions can. The focus is on the functions \u03b3 L (t) = t \u03b1 with 1 \u2264 \u03b1. The paper focuses on designing a discriminator class F with restricted approximability against a generator class G to approximate the Wasserstein distance W 1 for data distribution p and any q \u2208 G. The framework allows for both realizable and non-realizable cases. The framework allows for non-realizable cases, resolving the dilemma by avoiding mode collapse and transitioning from population-level to empirical-level guarantees. Capacity bounds relate the Rademacher complexity of F to the Wasserstein distance between distributions. The complexity of F relates to the statistical properties of Wasserstein GANs, providing insights into diversity, generalization, and distance guarantees. This theoretical framework addresses the statistical theory of GANs with polynomial samples, offering techniques for designing. The paper focuses on developing techniques for designing discriminator class F with restricted approximability for various generator classes, including simple ones like mixtures of Gaussians and more complex ones like distributions generated by invertible neural networks. Properly chosen F provides diversity guarantees, especially for Gaussian distributions and exponential families. In Section 4, the paper explores distributions generated by invertible neural networks. Special neural network discriminators with an extra layer compared to the generator ensure restricted approximability, hiding polynomial dependencies on parameters. The non-linearities in these networks can lead to an exponentially large number of modes. The invertibility assumption in distributions generated by invertible neural networks leads to an exponentially large number of modes. However, this assumption only produces distributions supported on the entire space, which may not align with the low-dimensional manifold where natural images reside. The KL-divergence is crucial in the invertible case, but becomes infinite if the support of the estimated distribution does not coincide with the support of the original distribution. The technical part of the paper focuses on approximating Wasserstein distance by IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE in learning distributions with low-dimensional supports. The main proof technique involves developing tools for approximating the log-density of a distribution. The paper focuses on approximating the log-density of a smoothed neural network generator to measure diversity and quality of learned distributions. The IPM correlates with Wasserstein distance for low-dimensional distributions and with KL-divergence for invertible generator families. It suggests using IPM as an alternative measure in complex settings where KL-divergence or Wasserstein distance is not measurable. The paper discusses using the Inception Probability Metric (IPM) to evaluate the diversity and quality of learned distributions in GANs. It suggests that the lack of diversity in real experiments may be due to sub-optimal optimization rather than statistical inefficiency. Various empirical tests have been developed to assess diversity, memorization, and generalization in GANs, including interpolation between images, semantic combination of images, and classification tests. Results suggest that lack of diversity is a common issue, while memorization is not. Arora et al. (2017a; b) identified theoretical sources of mode collapse from a weak discriminator and proposed a \"birthday paradox\" to illustrate this phenomenon. Many solutions have been proposed to address mode collapse in GANs, with varying success. Feizi et al. demonstrated guarantees when training GANs with quadratic discriminators and Gaussian generators. However, there are no provable solutions for this issue in a broader context. Zhang et al.'s work showed that the IPM is a proper metric under certain conditions. Our work provides statistical guarantees in Wasserstein distance for distributions like injective neural network generators, addressing the issue of mode collapse in GANs. Liang (2017) discusses GANs in a non-parametric setup, highlighting that the sample complexity for learning GANs improves with the smoothness of the generator family. The sample complexity for learning GANs improves with the smoothness of the generator family. The rate derived is non-parametric and exponential in dimension unless the Fourier spectrum decays extremely fast. Invertible generator structure in Flow-GAN addresses issues with GAN training on real datasets. Successful GAN training implies learning in KL-divergence when data distribution can be generated by an invertible neural net. Our theory suggests that real data cannot be generated by an invertible neural network. If data can be generated by an injective neural network, we can bound the closeness between learned and true distribution in Wasserstein distance. The notion of IPM includes statistical distances like TV and Wasserstein-1 distance. When F is a class of neural networks, we refer to the F-IPM. The text discusses various distances between distributions, including the KL divergence and Wasserstein-2 distance. It also mentions the Rademacher complexity of a function class. The largest Rademacher complexity over p \u2208 G governs the generalization of the IPM for the Wasserstein GAN. The training IPM loss is defined as Eqn [W F (p n ,q n )]. Notation includes N(\u00b5, \u03a3) for a Gaussian distribution and a b denotes a \u2264 Cb for a universal constant C > 0. Neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees, considering bounded mean and well-conditioned covariance. The discriminators induce IPM W F with restricted approximability for various parameterized distributions. The IPM induced by discriminators has restricted approximability w.r.t. Gaussian distributions in the sense that the bounds differ by a factor of 1/ \u221a d. The 1/ \u221a d factor is not improvable without using more sophisticated functions than Lipschitz functions of one-dimensional projections of x. The text discusses the restricted approximability of discriminators with Gaussian distributions, showing that the bounds differ by a factor of 1/ \u221a d. It also explores the extension to mixture of Gaussians and exponential families, demonstrating that linear combinations of sufficient statistics can serve as discriminators with restricted approximability. The text discusses the discriminators' restricted approximability with Gaussian distributions, extending to mixtures of Gaussians and exponential families. It shows that linear combinations of sufficient statistics can serve as discriminators with restricted approximability, with specific conditions on the log partition function and curvature. The Fisher information matrix has bounds, with geometric assumptions needed for the Wasserstein distance in exponential families. The proof of eq. FORMULA15 is straightforward, while eq. (8) requires further development in Section 4. Discriminators with restricted approximability for neural net generators are designed in this section. In Section 4.1, invertible neural networks generators with proper densities are considered. Section 4.2 extends the results to injective neural networks generators, allowing latent variables of lower dimension than observable dimensions. Generators parameterized by invertible neural networks are discussed in this section. The curr_chunk discusses the use of non-spherical variances in neural networks to model data around a k-dimensional manifold. It focuses on invertible neural networks with standard feedforward nets and makes an assumption about invertible generators. The curr_chunk discusses the assumption of invertible generators in neural networks parameterized by \u03b8 = (W i , b i ) i\u2208[ ] with activation function \u03c3 and hidden factors satisfying \u03b3 i \u2208 [\u03b4, 1]. The neural net is invertible and its inverse is a feedforward net with activation \u03c3 \u22121. The smoothed version of Leaky ReLU meets the activation function conditions. The curr_chunk discusses the conditions on activation functions for neural networks. It is necessary to impose assumptions on generator networks to prevent pseudo-random functions. The function log p \u03b8 can be computed by a neural network with specific parameters and activation functions. The family of neural networks with certain activation functions contains all functions log p \u2212 log q. The proof involves the change-of-variable formula for log p \u03b8 (x) and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian requires computing the determinant of the weight matrices, which is a non-trivial task for a given G \u03b8. However, it is a constant that does not depend on the specific form of the parameterized family F. The computation for a given G \u03b8 is non-trivial, but it can be simplified by adding a bias on the final output layer. The proof of Lemma 4.1 is deferred to Section D.2. Theorem 4.2 states that the discriminator class F has restricted approximability with respect to G. The proof of Theorem 4.2 uses a lemma that relates the KL divergence. The proof of Theorem 4.2 utilizes a lemma connecting KL divergence to IPM when log densities exist in the discriminator family. Lemma 4.3 states conditions for F and outlines the proof sketch, deferring the full proof. The discriminator class chosen in Lemma 4.1 implements log p - log q for any p, q in G, aiming to bound the Wasserstein distance. The proof establishes lower and upper bounds on the Wasserstein distance. The lower bound is proven using transportation inequalities, while the upper bound relies on functions in F. The upper bound on the Wasserstein distance is established by using functions in F. Two workarounds are provided to address the Lipschitz global issue, leading to a W 1 or W 2 bound. The training success with small expected IPM ensures the estimated distribution q is close to the true distribution p. The training success with small expected IPM ensures that the estimated distribution q is close to the true distribution p in Wasserstein distance. It is important to design efficient algorithms to achieve a small training error based on this definition, which is a topic for future work. In this section, injective neural network generators are discussed for generating distributions on a low dimensional manifold, which is more realistic for modeling real images. A novel divergence between distributions is designed that can be optimized as IPM, despite technical challenges. The key idea is to design a variant of the IPM that is invertible only on the image of G \u03b8, a k-dimensional manifold in R d. The key idea is to design a variant of the IPM that approximates the Wasserstein distance by convoluting distributions with a Gaussian distribution. This smoothed F-IPM can be optimized with an additional variable \u03b2. Certain discriminator classes can approximate the Wasserstein distance for distributions generated by neural nets in G. The theorem states that if the discriminator family F has restricted approximability with respect to the generator family G, mode collapse will not occur. The IPM W F (p, q) is upper and lower bounded by certain conditions. In GAN training, the IPM W F (p, q) is bounded by the Wasserstein distance W 1 (p, q) when the discriminator family F has restricted approximability with respect to the generator family G. Synthetic experiments confirm this theory, showing correlation between IPM and Wasserstein / KL. In GAN training, the IPM is well correlated with the Wasserstein distance, indicating that optimization difficulty, rather than statistical inefficiency, may be the main challenge. Experiments with synthetic datasets and neural net generators support this correlation. In synthetic experiments with WGANs, invertible neural net generators are trained to learn various curves in two dimensions, such as the unit circle and a \"swiss roll\" curve. The IPM is shown to be well-correlated with the KL divergence, indicating optimization difficulty as the main challenge in GAN training. The Wasserstein distance is used to measure the quality of learned generators in WGANs, which are shown to effectively learn distributions like the unit circle and Swiss roll curve. Standard two-hidden-layer ReLU nets are used as generators and discriminators in these experiments. The study utilizes two-hidden-layer ReLU nets for both the generator and discriminator classes. The generator has an architecture of 2-50-50-2, while the discriminator has an architecture of 2-50-50-1. The RMSProp optimizer is used with learning rates of 10^-4 for both the generator and discriminator. Two metrics are compared between the ground truth distribution p and the learned distribution q during training: the neural net IPM WF(p, q) and the Wasserstein distance W1(p, q). The Wasserstein distance W1(p, q) is computed on fresh batches using the POT package, providing a good proxy of the true distance. Results show that the learned generator closely matches the ground truth distribution at iteration 10000, with correlation between neural net IPM and Wasserstein distance. However, at iteration 500, the generators have not fully learned the true distributions yet. The first polynomial-in-dimension sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence are presented. The analysis technique involves designing discriminators tailored to the generator class to avoid mode collapse. The techniques presented aim to extend to other distribution families with tighter sample complexity bounds by designing discriminators with better approximability bounds. The goal is to explore and generalize approximation theory results in the context of GANs, hoping for rich and satisfying outcomes similar to vanilla functional approximation settings. The text discusses symmetrization and the proof of Theorem 3.1 in the context of discriminator families and approximability bounds in GANs. It highlights the upper and lower bounds of the discriminator functions and the mean distance calculation using linear discriminators. The text discusses the computation of neuron distance between two Gaussians using covariance distance and the function R. It shows that R is strictly increasing and provides bounds for the quantity in the supremum. The text discusses bounding the quantity in the supremum using perturbation bounds and the W 2 distance between two Gaussians. It bridges the KL and F-distance using the machinery developed in Section D. The text discusses bounding the growth of \u2207 log p 1 (x) 2 for distributions with parameters \u03b8 i = (\u00b5 i , \u03a3 i ) \u2208 \u0398. By using perturbation bounds and the W 2 distance between two Gaussians, it bridges the KL and F-distance. The Rademacher contraction inequality is applied to bound the right hand side. Proof of Theorem 3.2 shows KL bounds for exponential family and Wasserstein bounds. The exponential family property is used to derive the bounds, and the Rademacher complexity is computed for generalization. The Rademacher complexity is computed for generalization in the context of learning mixture of k Gaussians using a neural network. The Gaussian concentration result is utilized in the proofs for convenience. The Rademacher complexity is used to show the upper and lower bounds for approximability in learning mixture of k Gaussians with a neural network. The upper bound is proven by demonstrating that each discriminator is D-Lipschitz, while the lower bound is established by considering the regularity properties of the distributions in the Bobkov-Gotze sense. The Gaussian concentration lemma is applied to show that each component is 1-sub-Gaussian, leading to the conclusion that f(X) is at most something. The Rademacher complexity is used to bound the Rademacher process for a one-hidden-layer neural network. By showing that each f(Xj) is 1-sub-Gaussian, it is concluded that f(X) is at most (D^2 + 1)-sub-Gaussian, satisfying the Bobkov-Gozlan condition with \u03c3^2 = D^2 + 1. The Rademacher complexity of f\u03b8 for \u03b8 is bounded, and Y\u03b8 is shown to be Lipschitz in \u03b8, leading to an \u03b5-covering set N(\u0398, \u03c1, \u03b5). The text discusses bounding the covering number of a set under a certain condition. It also mentions the analysis of a term involving log-sum-exp functions and its relation to sub-Gaussian maxima bounds. The text discusses upper bounding f-contrast by Wasserstein, with conditions on distributions and their densities. Truncated W1 and W2 bounds are derived based on certain assumptions on the gradient of f. The proof involves a truncation argument and uses Cauchy-Schwarz inequality. It extends a previous proposition by Polyanskiy & Wu (2016) and shows the coupling for W2 distance. The W2 distance involves a coupling (X, Y) \u223c \u03c0 such that X \u223c P, Y \u223c Q. By taking expectations, we can bound the distance. The inverse of x = G\u03b8(z) can be computed using a feedforward net with activation \u03c3\u22121. The problem of representing log p\u03b8(x) by a neural network is considered, with the density of Z \u223c N(0, diag(\u03b32)) being \u03c6\u03b3. The inverse network G\u22121\u03b8 has layers. The inverse network G\u22121\u03b8 can be implemented with layers and parameters in each layer, using \u03c3\u22121 as the activation function. By adding branches to the network, the log determinant of the Jacobian can also be computed recursively. By adding branches to the hidden layers of the inverse network with log \u03c3 \u22121 activation, we can compute log p \u03b8 (x) with no more than + 1 layers and O( d 2 ) parameters. This is achieved by combining the output of the density branch and the log determinant branch. The text discusses the Gozlan condition being satisfied for a random variable and the network G \u03b8. By applying Theorem D.1, it is shown that p \u03b8 satisfies the condition for any \u03b8 \u2208 \u0398. The random variable is proven to be L 2 -sub-Gaussian, satisfying the Gozlan condition with \u03c3 2 = L 2. Additionally, applying Theorem D.1(b) results in obtaining certain inequalities related to p \u03b81 and p \u03b82. The text discusses the Gozlan condition being satisfied for a random variable and the network G \u03b8. By applying Theorem D.1, it is shown that p \u03b8 satisfies the condition for any \u03b8 \u2208 \u0398. The random variable is proven to be L 2 -sub-Gaussian, satisfying the Gozlan condition with \u03c3 2 = L 2. Additionally, applying Theorem D.1(b) results in obtaining certain inequalities related to p \u03b81 and p \u03b82. p \u03b81 and p \u03b82 swapped, leading to an upper bound on W F by Wasserstein distances. The Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 is discussed, with bounds on W 2 and terms I and II analyzed. The text discusses the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 and analyzes bounds on W 2 and terms I and II. The bound eq. (22) implies a tail bound for X 2 \u2264 D, with a choice of D = K \u221a d. By the bound X 2 \u2264 C( Z 2 + 1), the tail bound P( X 2 \u2265 D) \u2264 exp(\u221220d) is obtained. Additionally, under either p \u03b81 or p \u03b82, certain inequalities are derived, allowing for the substitution of terms. The text discusses reparametrization of log-density neural networks to simplify the form of the network. By reparametrizing, the network belongs to a certain set, and the Rademacher complexity is bounded. An additional re-parametrization is done to simplify the log-density network further. The constant C(\u03b8) is the sum of the normalizing constant for Gaussian density and the sum of log det(W i ). A parameter K = K(\u03b8) \u2208 [0, d R W ] is created for this term. The metric is defined for any reparametrized \u03b8. The one-step discretization bound is discussed in the following lemmas. The constant C(\u03b8) is determined by the normalizing constant for Gaussian density and the sum of log det(W i). A parameter K = K(\u03b8) \u2208 [0, d R W] is defined for this term. The metric is applicable to any reparametrized \u03b8. Lemmas discuss the one-step discretization bound, with a constant C = C(R W, R b) for all \u03b8 in \u0398 such that \u03c1(\u03b8, \u03b8) \u2264 \u03b5. The bound holds for \u03b5 \u2264 min{R W, R b} and \u03bb \u2264 \u03bb 0 \u03b4 2 n, with the term dominating the generalization error. The text discusses the Lipschitzness of hidden layers in a neural network. It shows bounds for the k-th hidden layer and the inverse network G^-1. The claim is proven through induction on k, with specific equations and notations used to demonstrate the bounds. The text discusses verifying results for the k-th layer in a neural network, showing Lipschitzness and bounds for the inverse network G^-1. It demonstrates the claim through induction on k, with specific equations and notations used for the bounds. The text discusses the Lipschitzness and bounds for the inverse network G^-1 in the k-th layer of a neural network. It demonstrates the claim through induction on k, with specific equations and notations used for the bounds. The terms in the equations are shown to be sub-Gaussian and subexponential, with their means and parameters bounded accordingly. The text discusses the Lipschitzness and bounds for the inverse network G^-1 in the k-th layer of a neural network, with specific equations and notations used for the bounds. The terms in the equations are shown to be sub-Gaussian and subexponential, with their means and parameters bounded accordingly. The covering number of \u0398 is bounded by the product of independent covering numbers, further by the volume argument. Jensen's inequality is used to show that for any \u03bb \u2264 \u03bb 0 \u03b4 2, Y \u03b8 is mean-zero sub-exponential with the MGF bound. The text discusses the Lipschitzness and bounds for the inverse network G^-1 in the k-th layer of a neural network, with specific equations and notations used for the bounds. Using Jensen's inequality and applying the bound appendix D.6.2, it is shown that for any \u03bb \u2264 \u03bb 0 \u03b4 2 n, the distribution obtained by adding Gaussian noise with variance \u03b2 2 to a sample from G \u03b8 is truncated to a region in the latent space. The text introduces regularity conditions for the family of generators G, including bounds on partial derivatives of f and specific notations for the bounds. It also discusses truncating the distribution by adding Gaussian noise with variance \u03b2 2 to a sample from G \u03b8, focusing on a high-probability region in the latent variable and observable domain. The text introduces regularity conditions for the family of generators G and discusses truncating the distribution by adding Gaussian noise. It denotes functions and upper bounds, assuming Lipschitz activation functions. The main theorem states that for certain F, d, F approximates the Wasserstein distance. The text discusses a family of functions F and their relationship with generators G. It highlights the existence of a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. The main theorem shows the existence of a family of neural networks F that approximates log p for typical x. The text discusses approximating log density of p \u03b2 using a family of neural networks F for typical x, with a method based on Laplace's integration. The text discusses approximating log density of p \u03b2 using a family of neural networks F for typical x, with a method based on Laplace's integration. A greedy \"inversion\" procedure is used to calculate a lower bound for atypical x. Theorem E.1 is proven assuming the correctness of Theorem E.2, showing the existence of neural networks N1 and N2 in F that approximate log p \u03b2 and log q \u03b2. The proof involves equations and the Bobkov-G\u00f6tze theorem. The text discusses approximating log density of p \u03b2 using neural networks F. The lower bound is proven using the Bobkov-G\u00f6tze theorem. The upper bound is obtained by setting \u03b2 = W 1/6. The claim follows by considering the optimal coupling C of p, q and the induced coupling C z on the latent variable z in p, q. The text discusses approximating log density of p \u03b2 using neural networks F. The lower bound is proven using the Bobkov-G\u00f6tze theorem. The upper bound is obtained by setting \u03b2 = W 1/6. The claim follows by considering the optimal coupling C of p, q and the induced coupling C z on the latent variable z in p, q. The generalization claim follows analogously to Lemma D.5, using the Lipschitzness bound of the generators in Theorem E.2. The rest of the section is dedicated to the proof of Theorem E.2. By induction, we prove DISPLAYFORM5 for i = 0. Assuming it holds for i, we show DISPLAYFORM6. The Lipschitzness of \u03c3 \u22121 leads to DISPLAYFORM7 and DISPLAYFORM8. This implies DISPLAYFORM9, then DISPLAYFORM10, completing the claim. The neural network's Lipschitz constant is determined by \u0125 i = \u03c3 \u22121 (W. The integral is the cdf of a Gaussian with covariance DISPLAYFORM11, with smallest eigenvalue bounded by DISPLAYFORM12. The algorithm presented approximates the integral and can be implemented by a small, Lipschitz network. Parameters are set, and a discriminator family with restricted approximability for a degenerate manifold is defined. The algorithm utilizes a trivial \u03b2 2 -net of matrices with bounded spectral norm. The algorithm approximates the integral using a small, Lipschitz network and a discriminator family with restricted approximability for a degenerate manifold. It utilizes a trivial \u03b2 2 -net of matrices with bounded spectral norm to calculate gradients and approximate eigenvectors/eigenvalues. The algorithm uses an approximate version of Lemma E.8 with a different integral division. It involves an \"invertor\" circuit output, a set B, and matrices E i chosen from a \u03b2 2 -net. The claim is that there exist matrices E 1, E 2, ..., E r with eigenvalues separated by \u2126(\u03b2) and E i 2 bounded by 1 - O(\u03b2 log(1/\u03b2)). The algorithm utilizes an approximate version of Lemma E.8 with a different integral division, involving an \"invertor\" circuit output, a set B, and matrices E i chosen from a \u03b2 2 -net. It claims the existence of matrices E 1, E 2, ..., E r with eigenvalues separated by \u2126(\u03b2) and E i 2 bounded by 1 - O(\u03b2 log(1/\u03b2). The approximation in Theorem E.9 serves the purpose by Taylor expanding, leading to the evaluation of integrals and synthetic WGAN experiments with invertible neural net generators and discriminators designed with restricted approximability. The empirical IPM W F (p, q) is well correlated with the KL-divergence between p and q on synthetic data. Data is generated from a ground-truth invertible neural net generator, using Leaky ReLU activation function. The weight matrices of the layers are well-conditioned with singular values between 0.5 to 2. The discriminator architecture is chosen based on restricted approximability guarantee. The activation function is modeled as a trainable one-hidden-layer neural network. Constraints are added to all parameters according to Assumption 1 for training the generator and discriminator networks with stochastic batches. The study uses stochastic batches for both the ground-truth and trained generators in the Wasserstein GAN formulation. The discriminator is updated 10 times between each generator step with various regularization methods. The RMSProp optimizer is used for updates. Evaluation includes computing KL divergence between the true and learned generators. The study evaluates the difference in log densities between the true and learned generators using KL divergence as a criterion for distributional closeness. It also considers the training loss (IPM W F train) and the neural net IPM (W F eval) to assess the performance of the Wasserstein GAN formulation. The study evaluates the performance of the Wasserstein GAN formulation by optimizing the WGAN loss with a fixed generator and training the discriminator from scratch. The discriminator is trained in norm balls without additional regularization to maximize contrast. The theory shows that WGAN can learn the true generator in KL divergence, and the F-IPM is indicative of the KL divergence. In experiments, a two-layer net in 10 dimensions is used for G. In the first experiment, a two-layer neural net in 10 dimensions is used as the generator. The discriminator is trained with either Vanilla WGAN or WGAN-GP, with results shown in FIG5. WGAN training with a discriminator of restricted approximability can learn the true distribution in KL divergence. The KL divergence in GANs starts around 10-30 and the best run achieves a KL lower than 1, indicating that mode collapse is not happening. The W F (eval) and KL divergence are highly correlated, with adding gradient penalty improving optimization. W F can be a good metric for monitoring convergence and is better than the training loss curve. The experiment tested the necessity of a specific form of the discriminator by using vanilla fully-connected discriminator nets. Results showed that IPM with vanilla discriminators correlated well with KL-divergence. The left-most figure displayed KL-divergence between true and learned distributions, the middle showed estimated IPM, and the right depicted training loss. Estimated IPM in evaluation correlated well with KL-divergence. The inferior performance of the WGAN-Vanilla algorithm in terms of KL-divergence does not stem from the statistical properties of GANs but rather from the convergence of the IPM during training. This phenomenon is likely to occur in training GANs with real-life data as well. In this section, the correlation between perturbations of p and its effects is directly tested by comparing KL divergence and neural net IPM on pairs of perturbed generators. The process involves generating pairs of generators with small Gaussian noise perturbations, computing the KL divergence and neural net IPM between them. To stabilize the training process for the neural net IPM, the discriminator is optimized from 5 random initializations. The results show a clear positive correlation between the KL divergence and the perturbations. In FIG6, a positive correlation is observed between KL divergence and neural net IPM. Points mostly align around the line W F = 100D kl, indicating linear scaling in KL divergence. Outliers with large KL are attributed to perturbations causing poor conditioning of weight matrices. Experiments with vanilla fully-connected discriminator nets are redone using a three-layer net with hidden dimensions 50-10. The three-layer net with hidden dimensions 50-10 has more parameters than the architecture with restricted approximability. Results are shown in FIG7, indicating that vanilla discriminator structures may be sufficient for a good generator, although specific designs could enhance the quality of the distance W F. The KL divergence between the true distribution p and learned distribution q is plotted at different training steps, showing convergence but slightly weaker correlation compared to the setting with restricted approximability. The estimated IPM in evaluation correlates well with the KL-divergence between the true distribution p and learned distribution q. Correlation between KL and neural net IPM is computed with vanilla fully-connected discriminators, showing a correlation of 0.7489, similar to discriminators with restricted approximability (0.7315)."
}