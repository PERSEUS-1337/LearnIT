{
    "title": "BWlCpme3TS",
    "content": "We investigate the suitability of self-attention models for character-level neural machine translation, testing a novel transformer variant that outperforms the standard model. Extensive experiments on WMT and UN datasets show improved performance and faster convergence, with robust character-level alignments. Most existing NMT models operate on word or subword-level. Character-level models in Neural Machine Translation (NMT) offer a more memory-efficient and compact language representation compared to word or subword-level models. They mitigate out-of-vocabulary problems and are suitable for multilingual translation, eliminating the need for separate models for each language pair. Multilingual training can improve overall performance without increasing model complexity. In this work, the suitability of self-attention models for character-level translation is investigated. Two models are considered: the standard transformer and a novel variant called the convtransformer. The study evaluates self-attention models for character-level translation, comparing a standard transformer with a new convtransformer variant. Models are tested on bilingual and multilingual translation to English using different input languages. Results show that self-attention models perform well for character-level translation, requiring fewer parameters than subword-level models. The convtransformer model outperforms the standard transformer at the character-level, converging faster and producing more robust alignments. Lee et al. (2017) introduced a fully character-level translation approach using a recurrent encoder-decoder model with convolutional layers, max pooling, and highway layers for intermediate representations. The decoder generates output translation character by character with attention on encoded representations. Lee et al. (2017) introduced a character-level translation approach using a recurrent encoder-decoder model with convolutional layers, max pooling, and highway layers. Multilingual training of character-level models showed promising results, even for distant languages like Russian or Chinese. Cherry et al. (2018) conducted a detailed comparison. The transformer model, introduced by Vaswani et al. in 2017, has achieved state-of-the-art performance in NLP tasks. It uses attention mechanisms instead of recurrence, with six stacked encoder layers processing the input. Cherry et al. (2018) found that character-level models can outperform subword-level models due to their flexibility in processing sequences. The transformer architecture consists of six stacked encoder layers using self-attention and decoder layers for generating output. Recent work has shown attention can effectively model characters, prompting investigation into character-level bilingual and multilingual translation with the transformer. The convtransformer modifies the standard transformer architecture by adding a sub-block to each encoder block, consisting of three parallel 1D convolutional layers with different context window sizes. This allows for character interactions at different levels of granularity, similar to subword or word-level interactions. The representations are fused using an additional convolutional layer, resulting in an output with increased dimensionality. In contrast to previous work, the convtransformer model maintains input resolution and adds a residual connection for flexibility. Experiments are conducted on the WMT15 DE\u2192EN dataset to compare model configurations for character-level translation. The convtransformer model maintains input resolution and adds a residual connection for flexibility. Experiments are conducted on the WMT15 DE\u2192EN dataset to compare model configurations for character-level translation. Previous work on character-level translation is followed, using the newstest-2014 dataset for testing. Main experiments are conducted using the United Nations Parallel Corporus (UN) for multilingual experiments with sentences from six languages in the same domain. Training corpora are constructed by sampling one million sentence pairs from the FR, ES, and ZH parts of the UN dataset for translation to English. Table 2 shows BLEU scores on the UN dataset for different input training languages evaluated on three test sets. Bilingual datasets are combined and shuffled, with Chinese data latinized using Wubi encoding. Experiments are designed for a bilingual scenario. In a bilingual scenario, models are trained with a single input language, while in a multilingual scenario, two or three languages are inputted simultaneously without language identifiers. The BLEU performance of different character-level architectures trained on the WMT dataset is compared in Table 1, including recurrent character-level models and transformers trained on the subword level. The study compares character-level and subword-level transformers trained on the WMT dataset, finding character-level training to be slower but still achieving strong performance. The convtransformer variant performs similarly to the standard transformer. The convtransformer performs on par with the standard transformer on the UN dataset, outperforming it by up to 2.6 BLEU on multilingual translation. Training multilingual models on similar input languages leads to improved performance for both languages. The convtransformer is slower to train than the transformer but reaches comparable performance in fewer epochs, leading to a training speedup. Distant-language training is effective when the input language is closer to the target translation language. The multilingual models are analyzed for their learned character alignments through model attention probabilities. Bilingual models are seen to have greater flexibility in learning high-quality alignments compared to multilingual models, which may struggle due to distractions from other input languages. The study analyzes multilingual models' character alignments using attention probabilities. Bilingual models show better alignment flexibility compared to multilingual models, which may struggle due to distractions from other languages. The alignments are quantified using canonical correlation analysis (CCA) on encoder-decoder attention matrices from different languages. The study conducted analysis on transformer and convtransformer models separately, showing strong positive correlation for bilingual models with similar languages. However, introducing a distant language resulted in a drop in correlation, especially for ZH. The convtransformer was more robust to distant languages compared to the transformer. The study compared transformer and convtransformer models for character-level translation, finding that self-attention models perform well with fewer parameters. Training on multiple input languages is effective, especially for similar languages, but less so for different languages. In future work, the analysis will be extended to include additional languages from different language families, such as more Asian languages. Efforts will also be made to improve the training efficiency of character-level models, a main bottleneck. Model outputs and alignments from bilingual and multilingual models trained on UN datasets are presented in Tables 3, 4, and 5 and Figures 4, 5, 6, and 7. The bilingual and multilingual models trained on UN datasets show different patterns in translation from FR to EN. The convtransformer model has sharper weight distribution on matching characters and words for bilingual translation. For multilingual translation of close languages, both transformer and convtransformer preserve word alignments, with convtransformer producing slightly less noisy alignments. The convtransformer model shows better word alignments for multilingual translation of distant languages compared to the transformer model. This indicates that the convtransformer is more robust for such translations. The convtransformer model demonstrates improved word alignments for multilingual translation of distant languages compared to the transformer model, indicating its robustness for such translations. To ensure the effectiveness of an institutional framework in sustainable development governance, addressing regulatory and implementation gaps is crucial. To ensure the effectiveness of an institutional framework in sustainable development governance, addressing regulatory and implementation gaps is crucial. To ensure the effectiveness of the institutional framework in sustainable development governance, addressing regulatory and implementation gaps is crucial. The framework must tackle the gaps that have characterized governance in sustainable development so far. The institutional framework in sustainable development governance must address gaps in regulatory and implementation to be effective. Acknowledging the past will strengthen the future of humanity in security, peaceful coexistence, tolerance, and reconciliation among nations. The future of humanity in security, peaceful coexistence, tolerance, and reconciliation among nations will be strengthened by recognizing the facts of the past. The future of humanity will be reinforced by recognizing the facts of the past, leading to safety, peaceful coexistence, tolerance, and reconciliation among nations. The use of expert management on farms is important for maximizing productivity and efficiency in irrigation water usage. The use of expert management on farms is crucial for maximizing productivity and efficiency in irrigation water use. It is important to have experts managing farms to achieve optimal results in productivity and water usage. Expert management of farms is crucial for maximizing productivity and efficiency in irrigation water use. It is important to have experts managing farms to achieve optimal results."
}