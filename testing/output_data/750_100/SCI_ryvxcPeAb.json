{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, adversarial perturbations are decomposed into model-specific and data-dependent components, with the latter contributing significantly to transferability. The proposed noise reduced gradient (NRG) method targets the data-dependent component to craft adversarial examples, leading to enhanced transferability across various ImageNet classification models. Low-capacity models exhibit stronger attack capabilities compared to high-capacity models with similar test performance. These findings offer a principled approach to improving adversarial attacks. Recent works have shown that adversaries can manipulate inputs to fool neural network models, creating adversarial examples. Understanding this phenomenon can guide the design of effective defense approaches against black-box attacks. Large neural network models used in real-world applications like speech recognition and computer vision are vulnerable to such attacks, with low-capacity models being more susceptible despite comparable test performance. Adversarial examples can transfer across different models, posing a challenge for defending against them effectively. Transferability allows attacks on black-box systems. The vulnerability to adversarial examples was first studied as an optimization problem using box-constraint L-BFGS. The presence of adversarial examples in deep neural networks is attributed to strong nonlinearity. Different methods like FGSM and DeepFool have been proposed to address adversarial instability. Transferability of adversarial examples has been analyzed, leading to ensemble-based approaches. The transferability of adversarial examples was discussed, with ensemble-based approaches proposed for effective black-box attacks. Various defense mechanisms were introduced, such as defensive distillation, adversarial training, and image transformation to mitigate the impact of adversarial perturbations. In this work, the transferability of adversarial examples is explained, leading to enhanced black-box attacks. Adversarial perturbations consist of model-specific and data-dependent components, with the former being noisy and representing behavior off the data manifold. The transferability of adversarial perturbations is mainly due to the data-dependent component, which approximates the ground truth on the data manifold. A noise-reduced gradient (NRG) method is proposed to construct adversarial examples by utilizing this component. Benchmark results on the ImageNet validation set show the effectiveness of the NRG method when combined with other known methods. The proposed noise reduced gradient, when combined with other methods, significantly improves the success rate of black-box attacks on ImageNet validation set. Model-specific factors like capacity and accuracy influence the success rate, with higher accuracy and lower capacity models being more effective. This can be attributed to transferability and provides insights for attacking unseen models. In the context of deep neural networks, the high dimensionality of the model makes it vulnerable to adversarial perturbations. These perturbations are small and nearly imperceptible changes to input data, resulting in adversarial examples. Adversarial examples exist in various models like support vector machines and decision trees. Non-targeted attacks aim to misclassify input data, while targeted attacks aim to produce a specific wrong label. In a black-box attack, the adversary has no knowledge of the target model and cannot query it. Crafting adversarial examples in a black-box attack involves constructing adversarial perturbations on a local model and deploying them to fool the target model. This process is based on optimizing a loss function to measure the prediction-ground truth discrepancy and quantify the perturbation magnitude. The perturbation magnitude in crafting adversarial examples is quantified using the truth metric. For image data, x adv is constrained within [0, 255] d. The commonly chosen loss function is cross entropy, but BID2 introduced a loss function that manipulates output logit directly. Distortion is measured using \u221e and 2 norms, with human eyes being the ideal metric. Ensemble-based approaches, like using a large ensemble of models, are suggested by BID8 to strengthen adversarial examples. Ensemble-based approaches, like using a large ensemble of models, are suggested by BID8 to strengthen adversarial examples. The corresponding objective involves averaging predicted probabilities of source models with ensemble weights. Various optimizers, including the normalized-gradient based optimizer, can be used to solve the problem. The Fast Gradient Based Method attempts to solve the objective by performing a single step iteration. The Fast Gradient Based Method (FGBM) aims to solve the objective with a single step iteration using a normalized gradient vector. It is shown to be fast and has good transferability, making it a simple yet effective optimizer. The Iterative Gradient Method performs a projected normalized-gradient ascent for k steps, with a projection operator to enforce constraints and a step size alpha. The Iterative Gradient Method uses a normalized gradient vector for adversarial attacks, with different variations like IGSM and g q (x). Understanding why adversarial examples transfer between models is crucial for black-box attacks and defenses. Transferability is linked to the similarity of decision boundaries between source and target models. The transferability of adversarial examples between different models is linked to the similarity of decision boundaries. Models with high performance on the same dataset learn a similar function on the data manifold, but their behavior off the manifold can differ due to architectural differences and random initializations. The transferability of adversarial examples between different models is influenced by the architectures and random initializations, leading to data-dependent and model-specific components. The data-dependent component mainly contributes to transferability, capturing shared information between models on the data manifold. The model-specific component has little impact due to different behaviors off the data manifold. The transferability of adversarial examples between different models is influenced by architectures and random initializations. The adversarial perturbation crafted from model A can mislead both model A and B, with a data-dependent component attacking model B easily. The model-specific component contributes little to the transfer between models. The NRG method aims to increase success rates of black-box adversarial attacks by reducing model-specific noise in the gradient, enhancing the data-dependent component for better transferability between models. The NRG method reduces model-specific noise in the gradient to enhance the data-dependent component for improved transferability between models. This is achieved by applying local average to remove noisy information, resulting in a smoother and more data-dependent gradient. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) uses \u2207f in Eq.(8) to drive the optimizer towards more data-dependent solutions, reducing model-specific noise for improved transferability between models. The special case k = 1, \u03b1 = \u03b5 is known as noise-reduced fast gradient sign method (nr-FGSM). The noise-reduced fast gradient sign method (nr-FGSM) is a version of the nr-IGSM that aims to reduce model-specific noise for improved transferability between models. It can be applied to various optimizers and has been tested on ImageNet dataset with 50,000 samples. For targeted attack experiments, random wrong labels are assigned to each image. Pre-trained models from PyTorch like resnet and vgg are used. Different models are chosen for experiments to save time. In targeted attack experiments, random wrong labels are assigned to images. The Top-1 success rate is used to evaluate the performance of the attack. The cross entropy 2 is the chosen loss function, with distortion measured by \u221e norm and scaled 2 norm. In targeted attack experiments, random wrong labels are assigned to images. The Top-1 success rate is used to evaluate the performance of the attack. The chosen loss function is cross entropy, with distortion measured by \u221e norm and scaled 2 norm. Distances are evaluated using RMSD. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with FGSM and IGSM optimizers. nr-FGSM performs better than original FGSM consistently and dramatically in various blackbox attacks. The noise-reduced gradient technique improves transferability in adversarial attacks compared to vanilla gradient methods, even in white-box scenarios. When comparing nr-IGSM and IGSM under the same number of gradient calculations, nr-IGSM generates more easily transferable adversarial examples. The noise-reduced gradient (NRG) in nr-IGSM guides the optimizer to explore data-dependent solutions, improving transferability in adversarial attacks. Large models like resnet152 are more robust to adversarial transfer than small models. Model-specific components also contribute to transfer across models with similar architectures. IGSM generally generates stronger adversarial examples than FGSM, except in attacks against alexnet. Our observation contradicts claims in previous studies regarding the transferability of adversarial examples generated by FGSM and IGSM. The choice of hyperparameters, such as \u03b1 = 1 and k = min(\u03b5 + 4, 1.24\u03b5), may lead to underfitting and hinder transferability. Alexnet is significantly different from other models when targeted in attacks. The Alexnet model is different from other models when targeted in attacks, leading to lower fooling rates with IGSM compared to FGSM. This suggests caution in trusting the objective completely, as it may cause overfitting to source model-specific information. A noise-reduced gradient technique is proposed to improve cross-model generalization capability. In this study, the NRG method is applied to ensemble-based approaches for improved cross-model generalization. Evaluation is done on a reduced set of 1,000 images for non-targeted attacks using FGSM and IGSM. Results show saturated Top-1 success rates for IGSM attacks, with improvements demonstrated in Top-5 rates. For targeted attacks, generating adversarial examples predicted by unseen target models is a focus. Generating targeted adversarial examples predicted by unseen target models is challenging and requires a large step size for optimization. Single-model based approaches are ineffective for this task, as demonstrated by BID8. The optimization procedure sensitivity to step size is discussed in more detail in Appendix A. The Top-5 success rates of ensemble-based approaches show that NRG methods outperform normal methods by a large margin in both targeted and non-targeted attacks. The success rates are reported in Table 3, indicating significant improvements with noise-reduced counterparts. In this section, the sensitivity of hyper parameters m and \u03c3 in NRG methods for black-box attacks is explored using the nr-FGSM approach. Larger m leads to higher fooling rates, while an optimal value of \u03c3 results in the best performance. Overly large \u03c3 can introduce a large. The optimal value of \u03c3 influences performance in NRG methods for black-box attacks. Large \u03c3 introduces bias, while small \u03c3 struggles to remove noisy information effectively. The optimal \u03c3 varies for different source models, around 15 for resnet18 and 20 for densenet161. The robustness of adversarial perturbations to image transformations is explored, crucial for real-world survival of adversarial examples. The influence of image transformations on adversarial examples' survival in the physical world is quantified using the destruction rate. Densenet121 and resnet34 are chosen as the source and target models, with four transformations considered: rotation, Gaussian noise, Gaussian blur, and JPEG. The destruction rate describes the fraction of adversarial images no longer misclassified after the transformation. The study analyzes the robustness of adversarial examples generated by NRG methods compared to vanilla methods. Decision boundaries of different models are examined using Resnet34 as the source model and nine target models. The image perturbation is estimated in a 2-D plane to understand the performance of NRG-based methods. The study examines the robustness of adversarial examples generated by NRG methods compared to vanilla methods using Resnet34 as the source model and nine target models. Image perturbation in a 2-D plane is analyzed to understand the performance of NRG-based methods. The direction of sign \u2207f and sign (\u2207f \u22a5 ) is found to be sensitive for different models, with sign \u2207f being more sensitive for most target models. The study explores the sensitivity of gradients along the sign \u2207f and sign (\u2207f \u22a5 ) for different models, showing that penalizing the optimizer along the model-specific direction can prevent overfitting. It also notes that the minimal distance u to produce adversarial transfer varies among models, with larger distances for complex models like resnet152 compared to smaller models like resnet50, providing insight into why big models are more robust. In experiments, it was found that different models exhibit varying performances in generating adversarial examples. For instance, attacks from densenet121 consistently perform well across target models, while adversarial examples crafted from alexnet generalize poorly. This suggests that the choice of a local model can impact the effectiveness of attacks. The study explores the impact of choosing a local model on generating adversarial examples for attacking a remote black-box system. Different target models like vgg19 bn and resnet152 were tested with FGSM and IGSM attacks, showing that models with smaller test errors and fewer parameters have more powerful attack capabilities. The results are summarized in FIG11, indicating that models with large test errors and parameters have low fooling rates. The study found that models with smaller test errors and fewer parameters have stronger attack capabilities. This is due to the model's lower bias and less complex nature, allowing for easier transfer of adversarial examples. The study focused on the transferability of adversarial examples, showing that adversarial perturbations can be broken down into model-specific and data-dependent components. The noise-reduced gradient (NRG) based methods proposed in the paper are more effective in crafting adversarial examples compared to previous techniques. The study introduced NRG-based methods for crafting adversarial examples, showing their effectiveness. Models with lower capacity and higher test accuracy are better at black-box attacks. Future research will explore combining NRG-based methods with adversarial training for defense. Transferability in attacks is data-dependent and low-dimensional, making black-box attacks defensible. White-box attacks, originating from high-dimensional space, are harder to defend against. Future research will focus on learning stable features. Future research will focus on learning stable features beneficial for transfer learning by incorporating the NRG strategy to reduce model-specific noise. The success rates of targeted black-box attacks using IGSM are evaluated on 1,000 randomly selected images with ResNet152 and VGG16 BN as target models. The performance is measured by the average Top-5 success rate over three ensembles. In targeted black-box attacks using IGSM, the optimal step size \u03b1 plays a crucial role in performance. A very large \u03b1, such as 15 in this experiment, compared to the allowed distortion \u03b5 = 20, is beneficial. Both too large and too small step sizes can harm attack performances. Interestingly, a small step size \u03b1 = 5 with a large number of iterations performs worse than a small number of iterations. This could be due to overfitting solutions with more iterations. A large step size prevents overfitting and encourages exploration of model-independent areas, making more iterations better. The experiment on MNIST dataset tested models of different depths (1, 3, 5, 9) with varying attack capabilities. Results show that low-capacity models have stronger attack capabilities compared to large-capacity models. The experiment on MNIST dataset tested models of different depths with varying attack capabilities. Results show that low-capacity models have stronger attack capabilities compared to large-capacity models. In Section 6.4, Top-1 success rates of FGSM and IGSM attacks against resnet152 were analyzed. The distortion chosen was \u03b5 = 15, and the percentage of adversarial examples that can transfer to the resnet152 was noted."
}