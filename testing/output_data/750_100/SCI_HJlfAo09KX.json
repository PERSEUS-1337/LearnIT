{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer fully-connected neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function exhibits strong convexity and smoothness, allowing gradient descent to converge linearly to a critical point close to the ground truth. This can be achieved via the tensor method without needing fresh samples at each iteration. Neural networks have gained research interest for their success in practical domains like computer vision and artificial intelligence. Efforts have been made to understand the theoretical basis behind this success. Global convergence guarantee has been established for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, with near-optimal sample and computational complexity. Efforts have been made to understand the theoretical underpinnings of the success of deep neural networks, focusing on functions representation, optimization with gradient descent, and generalization. Research has also explored model recovery to recover the underlying model parameter for better generalization. Efforts have been made to understand the theoretical underpinnings of deep neural networks' success, focusing on functions representation, optimization with gradient descent, and generalization. Previous studies have explored model recovery to understand the underlying model parameter W for better generalization. Studies have focused on regression and classification problems, with research on single-neuron models, one-hidden-layer multi-neuron networks, and two-layer feedforward networks with ReLU activations. For regression and classification problems, previous studies focused on recovering neural network parameters using gradient descent with squared loss. Statistical guarantees were provided for model recovery in both settings. BID38 demonstrated statistical guarantees for model recovery using gradient descent with squared loss. The Hessian of the empirical loss function is positive definite in the local neighborhood of the ground truth, requiring fresh samples at each iteration for convergence. On the other hand, studies like BID21 and BID28 establish uniform geometry conditions for linear convergence without the need for resampling per iteration. In this paper, the study aims to provide a strong statistical guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function, which is more practical than the squared loss for classification problems. This is the first performance guarantee for this scenario, with contributions summarized for multi-neuron classification problems. For multi-neuron classification with sigmoid activations, if the input is Gaussian, the empirical risk function based on cross entropy loss is uniformly strongly convex in a local neighborhood of the ground truth. Gradient descent converges linearly to a critical point with near-optimal sample complexity. The tensor method proposed in BID38 provides an initialization near the ground truth for multi-neuron classification with sigmoid activations. The convergence guarantee for W is up to certain statistical accuracy, with a computational complexity of O(ndK 2 log(1/ )). The proof introduces a new condition on activation function curvature, allowing for analysis of the cross-entropy loss function. Various techniques are developed to utilize statistical information of geometric curvatures, ensuring uniform concentrations. Similar guarantees are also provided for classification using squared loss. The focus is on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization. The parameter recovery viewpoint is relevant for success in signal processing problems. The statistical model for data generation removes worst-case instances, allowing for a focus on average-case performance with benign geometric properties. The studies focus on the landscape analysis and model recovery of one-hidden-layer network models. It is known that in the optimization landscape, there are no spurious local minima if the network size is large enough compared to the data input. Spurious bad local minima exist in the population squared loss surface with ReLU activations when multiple neurons are present in the under-parameterized setting. In the optimization landscape, spurious bad local minima exist in the population squared loss surface with ReLU activations. Zhong et. al. provided characterizations for the local Hessian in regression problems with various activation functions. BID28 demonstrated linear convergence of gradient descent with ReLU activation and Gaussian input when the number of neurons is smaller than the input dimension. The sample complexity is O(d) for the regression problem, while BID21 showed that gradient descent converges linearly with bounded derivatives of \u03c6(\u00b7) and O(d log 2 d) sample complexity for classification with sub-Gaussian inputs. Our study differs by analyzing the cross entropy loss function, focusing on model recovery in classification problems. The paper discusses model recovery in classification problems with multi-neuron cases, which is a new area of study. Previous research focused on one-hidden-layer or two-layer neural networks with different structures under Gaussian input. The results presented are not directly comparable due to differences in networks and loss functions. The paper discusses model recovery in classification problems with multi-neuron cases, focusing on local geometry and convergence of gradient descent. It also covers the initialization method and includes numerical examples. Boldface letters denote vectors and matrices, with specific notations for transpose, spectral norm, and Frobenius norm. Gradient and Hessian of a function are denoted by \u2207f (W ) and \u2207 2 f (W ), respectively. The paper discusses model recovery in classification problems with multi-neuron cases, focusing on local geometry and convergence of gradient descent. It covers the generative model for training data and the gradient descent algorithm for learning network weights. The paper addresses model recovery in classification with multi-neuron cases, focusing on local geometry and gradient descent convergence. It discusses the generative model for training data and the algorithm for learning network weights using cross entropy loss. The paper discusses model recovery in classification with multi-neuron cases, focusing on local geometry and gradient descent convergence. It addresses the gradient descent algorithm with a well-designed initialization scheme to estimate W, emphasizing the importance of avoiding local minima. The algorithm uses a standard set of training samples throughout its execution. The paper introduces a new approach for gradient descent in model recovery, emphasizing the importance of avoiding local minima. It contrasts with previous work like BID38, which uses resampling. The main results are based on a quantity related to \u03c6(z) that captures the geometric properties of the loss function. The paper discusses the local strong convexity of the activation function and characterizes the Hessian of the empirical risk function in a neighborhood of the ground truth W. The condition number and theorem guarantee positive definiteness of the Hessian with high probability. Theorem 1 guarantees that the Hessian of the empirical cross-entropy loss function is positive definite in a neighborhood of the ground truth W, as long as certain conditions are met. The Hessian of the empirical cross-entropy loss function is positive definite near the ground truth W under certain conditions, with bounds depending on network parameters and activation functions. For orthonormal W, the sample complexity is near-optimal as n = \u2126(dK 5 log 2 d). The classification problem with quantized labels has dK unknown parameters. The empirical risk function is strongly convex near W, with at most one critical point in the local neighborhood. Theorem 2 proves the existence of a critical point Wn close to the ground truth W, where gradient descent converges linearly. Theorem 2 states that there are constants C, C1 > 0 such that if the sample size n \u2265 C \u00b7 dK, then with high probability there exists a unique critical point Wn in a local neighborhood of W. Gradient descent converges linearly to Wn at a rate of O(K9/4 d log n/n), ensuring consistent recovery of W as n increases. The tensor method proposed in BID38 is used for initialization in gradient descent, ensuring linear convergence to Wn with a computational complexity of O(ndK^2 log(1/\u03b5)). This method is briefly described in this section, with performance guarantees discussed. The tensor method proposed in BID38 is utilized for initialization in gradient descent, ensuring linear convergence to Wn with a computational complexity of O(ndK^2 log(1/\u03b5). The method involves defining a product \u2297, estimating the direction of each column of W, and reducing a third-order tensor to a lower-dimension tensor. The tensor method proposed in BID38 is utilized for initialization in gradient descent, ensuring linear convergence to Wn with a computational complexity of O(ndK^2 log(1/\u03b5). The method involves reducing a third-order tensor to a lower-dimension tensor and applying non-orthogonal tensor decomposition to output the estimate s i V w i. For the classification problem, technical assumptions are made regarding the activation function \u03c6(z) and the non-zero values of M3 and M4. The homogeneous assumption in BID38 is restrictive, so a new condition on the curvature of the activation function is assumed. This condition allows for a larger class of activation functions like sigmoid and tanh. The performance guarantee for the initialization algorithm is presented in Theorem 3. The proof of Theorem 3 provides a performance guarantee for the initialization algorithm in the classification model. It ensures accurate estimation of the direction and norm of W, with details in the supplementary materials. In this section, gradient descent is used to verify the strong convexity of the empirical risk function around W. Multiple random initializations in the local region lead to convergence to the same critical point Wn. The variance of the output of gradient descent is calculated by initializing multiple times. The standard deviation of the estimator Wn is quantified under different conditions. The experiment in BID21 quantifies the standard deviation of the estimator Wn under various initializations with the same training samples. Successful convergence of gradient descent is observed when SDn \u2264 10 \u22122. With K = 3 and d = 15, 20, 25, gradient descent converges to the same local minima with high probability as long as sample complexity is sufficient. Statistical accuracy of the local minimizer is shown for gradient descent when initialized close to the ground truth. The experiment in BID21 shows the statistical accuracy of the local minimizer for gradient descent when initialized close to the ground truth. Average estimation error decreases as sample size increases, matching theoretical predictions. Performance of gradient descent on cross entropy and squared loss is compared in FIG1 for K = 3 and d = 20. In a comparison between cross entropy loss and squared loss, cross entropy loss with gradient descent achieves lower error in a classification problem. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem, characterizing sample complexity for local strong convexity near the ground truth. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. It aims to extend the analysis to more general activation functions and network structures. The proof of Theorem 1 involves showing the smoothness of the Hessian of the population loss function. The study extends the analysis to more general activation functions and network structures, focusing on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. The smoothness of the Hessian of the population loss function is crucial in proving Theorem 1. The Hessian of the population risk is shown to be smooth around W in Lemma 1, followed by demonstrating local strong convexity and smoothness of \u2207 2 f n (W) in a neighborhood of W in Lemmas 2 and 3. The smoothness of the Hessian of the population loss function around W is demonstrated in Lemma 1 for sigmoid activations. Lemma 2 shows local strong convexity and smoothness of the population loss function in a neighborhood of W. The Hessian of the empirical loss function is shown to be close to the Hessian of the population loss function for sigmoid activations. With a sample size n \u2265 C \u00b7 dK log dK, the result holds with high probability. Combining Lemma 3 and Lemma 1 yields Theorem 1, establishing the relationship between the two Hessians. The proof of Theorem 2 shows that the gradient of f n (W) concentrates around the gradient of f(W) in B(W, r), ensuring the existence of a critical point W n in B(W, r). Additionally, it is demonstrated that W n is close to W and gradient descent converges linearly to W n with a properly chosen step size. Lemma 4 states that for sigmoid activation function with certain conditions, there exists a unique critical point W n in a specific region. This critical point is close to W and gradient descent converges linearly to it with the right step size. The critical point W n is established in a specific region, and gradient descent converges linearly towards it with the correct step size. The update rule for W t at the t-th iteration is given, and the local linear convergence of gradient descent is proven. The proof of gradient descent convergence to the local minimizer W n is divided into two parts. Part (a) shows the accuracy of estimating the direction of W, similar to arguments in BID38. Part (b) does not require a homogeneous activation function and is based on a mild condition in Assumption 2. The proof details a tensor operation for a tensor T \u2208 R n1\u00d7n2\u00d7n3 and three matrices A \u2208 R. The proof involves defining a tensor operation for a tensor T and three matrices A, B, C. It shows the accuracy of estimating the direction of each wi for regression and classification problems. The main idea is to bound the estimation error of P2 and R3 using Bernstein inequality. For the classification problem, Bernstein inequality is applied to all neurons together, with bounded label y i. In regression, Bernstein inequality is applied to each neuron individually, with output y i needing upper bounding. Different proof for estimating w i is provided here. The proof for estimating w i without the homogeneous condition on the activation function is provided, using a relaxed condition in Assumption 2. Quantity Q 1 is defined based on the first non-zero index, allowing estimation of w i through an optimization problem. The estimation of w i is achieved by substituting estimated values into an equation, allowing correct estimation of s i based on the sign of \u03b2 i. The inverse function of m 3,1 has an upper-bounded derivative, ensuring accurate estimation within a specified interval. The sub-gaussian and sub-exponential norms of random variables are defined as X \u03c82 and X, respectively. These definitions are used in the proofs to show that the sample size can be large enough for certain variables to be arbitrarily close. The sub-gaussian random variable X satisfies certain conditions based on its upper bound. The gradient and Hessian of E[(W; ) are calculated, with a concise representation of the hessian block provided. The mean value theorem is used to express g j,l (W) in terms of W. The gradient of g j,l (W) is calculated using the mean value theorem, and an upper bound for E T 2 j,l,k is derived using Cauchy-Schwarz inequality and Lemma 5. The second inequality follows from Lemma 5. For the sigmoid activation function \u03c6(x) = 1/(1+e^(-x)), a certain condition holds for a large constant C. By plugging in certain formulas, we can derive further results. The proof involves presenting upper and lower bounds of the Hessian of the population risk at ground truth and applying Lemma 1 to obtain a uniform bound in the neighborhood of W. In the proof, upper and lower bounds of the Hessian of the population risk at ground truth are presented to derive further results. Lemma 1 is applied to obtain a uniform bound in the neighborhood of W. Lemma 3 provides a proof by adapting analysis from a previous setting. It introduces the concept of a covering number and bounds the terms P(A t), P(B t), and P(C t) separately. In this sequel, we will bound the terms P(A t), P(B t), and P(C t) separately. A technical lemma from BID21 states that G i = v, \u2207 2 (W; x i) \u2212 E \u2207 2 (W; x) v where E[G i] = 0. Lemma 7 shows that G i \u03c81 is upper bounded by a constant C."
}