{
    "title": "HkxStoC5F7",
    "content": "The paper introduces a new framework called ML-PIP for data efficient and versatile learning, extending existing probabilistic interpretations of meta-learning to cover a broad class of methods. \\Versa{} is a framework that uses an amortization network to handle few-shot learning datasets, generating task-specific parameters in a single forward pass, eliminating the need for optimization at test time. \\Versa{} is a framework for few-shot learning that achieves state-of-the-art results on benchmark datasets, handling varying numbers of shots and classes at train and test time. It demonstrates its power through a challenging few-shot ShapeNet view reconstruction task, showcasing rapid adaptation to new datasets at test time. In this paper, a framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) is developed to improve data-efficient learning. The framework reframes and extends existing point-estimate probabilistic interpretations of meta-learning to cover a broader range of tasks. The framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) extends existing interpretations to include gradient-based, metric-based, amortized MAP inference, and conditional probability modeling methods. It leverages shared statistical structure between tasks, shares information on learning and inference, and enables fast learning in the presence of uncertainty in small datasets. The procedure for metalearning probabilistic inference enables fast learning through amortization. A new method called VERSA substitutes optimization procedures with forward passes through inference networks at test time, resulting in faster performance and eliminating the need for second derivatives during training. VERSA uses a flexible amortization network to output task-specific parameters in a single forward pass, accommodating arbitrary numbers of shots and tasks. The framework presented consists of a multi-task probabilistic model and a method for meta-learning probabilistic inference. It focuses on using discriminative models for predictive performance and leveraging shared statistical structure. The network can handle arbitrary numbers of shots and classes for classification tasks. Evaluation on various benchmarks shows state-of-the-art results, including settings with different test conditions and a challenging one-shot view reconstruction task. The standard multi-task directed graphical model in FIG1 utilizes shared parameters \u03b8 and task-specific parameters {\u03c8 to meet criteria for supervised learning tasks. The goal is to meta-learn fast and accurate approximations to the posterior predictive distribution for unseen tasks. The framework for meta-learning approximate inference involves using point estimates for shared parameters \u03b8 and distributional estimates for task-specific parameters {\u03c8. The probabilistic solution for few-shot learning includes forming the posterior distribution over task-specific parameters and computing the posterior predictive. The framework for meta-learning approximate inference involves using point estimates for shared parameters \u03b8 and distributional estimates for task-specific parameters {\u03c8. The probabilistic solution for few-shot learning includes forming the posterior distribution over task-specific parameters and computing the posterior predictive. To quickly perform this at test time, we approximate the posterior predictive distribution with an amortized distribution q \u03c6 (\u1ef9|D), using a feed-forward inference network with parameters \u03c6. In meta-learning approximate inference, a feed-forward inference network with parameters \u03c6 is used to learn the predictive distribution over test output \u1ef9 (t) given a training dataset D (t) and test input x. The approximate posterior predictive distribution is formed by amortizing the approximate posterior q \u03c6 (\u03c8|D) using a factorized Gaussian distribution. This enables fast predictions at test time, similar to amortized variational inference. Additional approximations like Monte Carlo sampling may be necessary. The training method for meta-learning involves optimizing the approximate posterior predictive distribution using a Gaussian distribution with means and variances set by the amortization network. The goal is to minimize the KL-divergence between the true and approximate posterior predictive distribution, returning parameters that best approximate the posterior predictive distribution in an average KL sense. The amortized procedure meta-learns approximate inference that supports accurate prediction by optimizing the approximate posterior q \u03c6 (\u03c8|D) to recover the true posterior p(\u03c8|D). Training involves selecting a task, sampling training data, forming the posterior predictive, and computing log-density. Repeating this process provides an unbiased estimate of the objective. The training procedure involves meta-learning approximate inference to optimize the posterior predictive distribution, focusing on minimizing KL divergence. This differs from standard variational inference and utilizes an inference network to approximate posterior distributions. The process includes end-to-end stochastic training to achieve accurate prediction. The training procedure involves end-to-end stochastic training with shared parameters \u03b8 and objective optimization for predictive performance. It includes individual feature extraction, instance pooling, regression onto weights, and data distribution sampling tasks. The training procedure involves end-to-end stochastic training with shared parameters \u03b8 for predictive performance. Meta-Learning Probabilistic Inference for Prediction (ML-PIP) uses episodic train/test splits and approximates the integral over \u03c8 using Monte Carlo samples. The learning objective does not require an explicit prior distribution over parameters, learning it implicitly through q \u03c6 (\u03c8|D, \u03b8) instead. The ML-PIP framework discussed in Section 4 unifies existing approaches and supports versatile learning, enabling rapid and flexible inferences without the need for retraining. This is achieved through the use of a deep neural network to amortize the approximate posterior distribution q. The text discusses design choices for flexibility in amortized inference using deep neural networks. It focuses on processing sets of variable size with permutation-invariant instance-pooling operations. This approach is applied to Few-Shot Image Classification using VERSA parameterization. Our parameterization for few-shot image classification is inspired by early work and recent extensions to deep learning. A shared feature extractor feeds into task-specific linear classifiers. Amortized inference requires modeling the distribution over weight matrices and biases, limiting inference to a specified number of classes. Our proposed method for few-shot image classification involves specifying weight vectors in a context-independent manner, allowing for amortization of individual weight vectors associated with specific feature extraction instances. This approach reduces the output dimensionality and enables efficient inference for a specified number of classes. The proposed method for few-shot image classification involves specifying weight vectors in a context-independent manner, allowing for amortization of individual weight vectors associated with specific feature extraction instances. This reduces output dimensionality and enables efficient inference for a specified number of classes. The amortization network operates directly on extracted features to reduce the number of learned parameters. End-to-end training is employed, backpropagating to \u03b8 through the inference network. The classification matrix is constructed by performing feed-forward passes through the inference network. The proposed method for few-shot image classification involves context-independent inference through an amortization network, reducing the number of parameters needed for efficient classification. The approximation addresses limitations of naive amortization by using insights from Density Ratio Estimation and demonstrating the closeness of approximate posterior distributions to their context-independent counterparts. VERSATILE Few-Shot Image Reconstruction: The amortization network can be meta-trained with varying numbers of classes and adapt to different class numbers at test-time. This task involves view reconstruction in a high-dimensional output space, where the model infers object appearance from different angles based on limited views. The approach is akin to a GAN generator or decoder in a multi-output regression setting. Our generative model, similar to a GAN generator or VAE decoder, uses a latent vector and angle representation to produce images at specified orientations. The generator network parameters are global, while the latent inputs are task-specific. A Gaussian likelihood in pixel space is used for generator outputs, with a sigmoid activation to ensure output means are between zero and one. The sigmoid activation ensures output means are between zero and one. An amortization network processes image representations and view orientations before instance-pooling. ML-PIP unifies various meta-learning approaches as approximate inference in hierarchical models. The text discusses various approaches to meta-learning, including gradient and metric-based variants, amortized MAP inference, and conditional modeling. It compares these approaches to VERSA.Gradient-Based Meta-Learning, focusing on task-specific parameters and semi-amortized inference. The text discusses the importance of shared inference parameters like initialization and learning rate for optimization in meta-learning tasks. It presents a perspective on Model-agnostic meta-learning as semi-amortized ML-PIP, contrasting with previous work on one-step gradient parameter updates. The update choice is viewed as an amortization process trained using predictive KL, which naturally recovers test-train splits. Multiple gradient steps could be incorporated into an RNN to compute optimal parameters. VERSATILE (VERSA) is a method that simplifies inference by treating both local and global parameters without the need for back-propagation during training or gradient computation at test time. It contrasts with other methods by being distributional over \u03c8 and enabling metric-based few-shot learning using task-specific and shared parameters in neural networks. The predictive distribution for estimating parameters involves averaging top-layer activations for each class. VERSA uses a flexible amortization function for inference, while BID43 proposed predicting class weights from pre-trained network activations to support online learning and transfer tasks. The VERSA framework utilizes a flexible amortization function for inference, while BID43 predicts class weights from pre-trained network activations to support online learning and transfer tasks. This involves incrementally adding new few-shot classes and transferring from high-shot to low-shot classification tasks. The ML-PIP framework pre-trains \u03b8 and performs MAP inference for \u03c8, supporting full multi-task learning by sharing information between tasks. The ML-PIP framework involves training a conditional model for multi-task learning via maximum likelihood estimation, establishing a connection to neural processes. It differs from amortized VI in optimizing free-energy w.r.t. parameters. In VERSA, the objective differs from ML-PIP by not using meta train/test splits and includes KL for regularization. It significantly improves few-shot classification and is compared to VI/meta-learning hybrids. VERSA is evaluated on various few-shot learning tasks, showing high performance in amortized posterior inference and classification on Omniglot and miniImageNet datasets. In Section 5.2, VERSA's high accuracy is demonstrated on Omniglot and miniImageNet datasets with varying shot and way at test time. Section 5.3 examines VERSA's performance on a one-shot view reconstruction task with ShapeNet objects. An experiment is conducted to investigate the approximate inference in the training procedure, generating data from a Gaussian distribution with varying mean across tasks. The learnable parameters are trained with the objective function using mini-batches of tasks. The model is trained to convergence with Adam, and a separate set of tasks is generated for inference. The true posterior over \u03c8 is Gaussian and can be computed analytically. The approximate posterior distributions inferred by the trained networks show accurate recovery of posterior distributions for unseen test sets. VERSAs inference procedure accurately recovers posterior distributions over \u03c8, despite minimizing predictive KL divergence. Evaluation on few-shot classification tasks using Omniglot and miniImageNet datasets shows VERSA's effectiveness. The experimental protocol follows established methods, with true and approximate posteriors compared for unseen test sets. In the experiment, VERSA accurately recovers posterior distributions over \u03c8 for unseen test sets. Training is done episodically with k c examples per task. Results for few-shot classification performance are provided in TAB1 for VERSA and competitive approaches. VERS achieves new state-of-the-art results on 5-way -5-shot classification on miniImageNet (67.37%) and on 20-way -1 shot Omniglot benchmark (97.66%). The comparison excludes approaches using pre-training or residual networks to assess learning algorithm quality separately from the model's power. VERS achieves state-of-the-art results on various benchmarks, including 5-way -5-shot miniImageNet and 20-way -1 shot Omniglot. VERSA adapts only the weights of the top-level classifier, outperforming other methods that adapt all learned parameters for new tasks. Our inference procedure, VERSA, outperforms amortized VI and non-amortized VI in terms of log-likelihood and accuracy. The performance improvement is due to VI's tendency to under-fit, especially for small datasets, which is compounded when using inference networks. Non-amortized VI improves performance but does not reach the level of VERSA. Further experimental details are provided in Appendix C. The performance of VERSA surpasses amortized VI and non-amortized VI in terms of log-likelihood and accuracy. VERSA allows for varying the number of classes and shots between training and testing, resulting in high accuracy across different conditions. At test-time, VERSA demonstrates flexibility and robustness to varying conditions, achieving 94% accuracy in a 100-way scenario. It outperforms MAML in speed, taking only 53.5 seconds compared to MAML's 302.9 seconds for evaluating 1000 test tasks. In experiments using ShapeNetCore v2 BID5 dataset, VERSA shows over 5\u00d7 speed advantage compared to MAML with 4.26% higher accuracy. The dataset consists of 37,108 objects from 12 object categories, with 36 views generated for each object. Training, validation, and testing sets are split as 70%, 10%, and 20% respectively. In experiments with ShapeNetCore v2 BID5 dataset, VERSA outperforms MAML in speed by over 5\u00d7 and accuracy by 4.26%. The dataset includes 37,108 objects from 12 categories, with 36 views per object. Training, validation, and testing sets are split as 70%, 10%, and 20% respectively. VERSA is evaluated by comparing it to a C-VAE with view angles as labels BID38. Both models are trained differently, with VERSA trained episodically and C-VAE in batch-mode on all 12 object classes at once. VERSA produces images with more detail and sharper visuals compared to C-VAE. VERSAs images are more detailed and visually sharper than C-VAE images. Despite occlusion in single shots, VERSA can accurately fill in missing information. Quantitative comparison in Table 2 shows VERSA's superiority over C-VAE, with improved results as the number of shots increases. ML-PIP is a probabilistic framework for meta-learning that unifies various methods and suggests alternative approaches. Using ML-PIP as a foundation, VERSA is a few-shot learning algorithm that achieves state-of-the-art performance without gradient-based optimization at test time. It demonstrated impressive results on a challenging 1-shot view reconstruction task. Prototypical Networks show better performance when trained on a higher \"way\" than that of testing. For example, when trained on 20-way classification and tested on 5-way, the model achieves 68.20 \u00b1 0.66%. The new inference framework in Section 2 is based on Bayesian decision theory (BDT), providing a method for making predictions for unknown test variables by combining information from observed training data. The text discusses Bayesian decision theory (BDT) as a method for predicting unknown test variables by combining information from observed training data. It presents a derivation of a stochastic variational objective for meta-learning probabilistic inference. The text introduces a simple stochastic variational objective for meta-learning probabilistic inference based on Bayesian decision theory. It extends BDT to predict a full distribution over unknown test variables, quantifying the quality of the prediction through a distributional loss function. The optimal predictive distribution is found by optimizing the expected distributional loss within a constrained distributional family. Amortized variational training is used to amortize the predictive distribution. Amortized variational training involves forming quick predictions at test time by learning parameters that minimize average expected loss over tasks. Shared variational parameters \u03c6 are used to approximate the predictive distribution, which can take any training dataset as an argument for direct prediction. Optimal variational parameters are found by minimizing expected distributional loss across tasks. The procedure involves stochastically approximating Eq. (A.3) by sampling tasks and partitioning data into training and test sets, enabling episodic minibatch training. The log-loss function is employed, with Eq. (A.4) emphasizing the optimal predictive distribution q \u03c6. The optimal predictive distribution q \u03c6 is determined by the entropy of p, with Eq. (A.4) highlighting the closest member of Q to the true predictive distribution. This approximation replaces the true posterior with an approximate predictive distribution, with theoretical and empirical justifications provided. Exploration of alternative scoring rules and task-specific losses is left for future work. Theoretical and empirical justifications for the context-independent approximation are provided through density ratio estimation. The optimal softmax classifier is expressed in terms of conditional densities, constructing estimators for each class. This approximation focuses on constructing estimators for the conditional density for each class. The context-independent approximation is justified through density ratio estimation, constructing estimators for each class independently. This mirrors the optimal form using a specific formula for each class in a given task. An experiment is detailed to evaluate the validity of this assumption without imposing it on the amortization network. Without imposing the assumption on the amortization network, free-form variational inference is performed on weights for tasks using a Gaussian distribution. The distribution of weights for a specific class is expected to be similar regardless of additional classes in the task. An experiment on 5-way classification in the MNIST dataset is conducted by randomly sampling and fixing fifty tasks. The model is trained twice with a fixed feature extraction network and d \u03b8 set to 16. The model is trained in an episodic manner for few-shot classification experiments, achieving 99% accuracy on test examples. The optimized weights cluster by class in 2-dimensional space, with some overlap and outliers. The model achieves 99% accuracy on few-shot classification experiments. Class weights are typically independent of the task, but may shift if the model lacks capacity. A VI-based objective is derived for the probabilistic model. The text discusses the use of amortized and non-amortized variational inference in a probabilistic model. The objective function remains the same for both approaches, with the derivation of an evidence lower bound (ELBO) for a single task. Stochastic estimators are used to optimize the objective function by sampling tasks and performing Monte Carlo integration. In few-shot classification experiments, Monte Carlo integration is used to optimize the objective function, which differs from Eq. (4) in two key ways. The dataset used is Omniglot BID32, containing 1623 handwritten characters from 50 alphabets. The training procedure follows a pre-processing method similar to BID54. The images are resized to 28 \u00d7 28 pixels and character classes are augmented with rotations of 90 degrees. The training, validation, and test sets consist of 1100, 100, and 423 characters respectively. Training proceeds in an episodic manner with C classes selected at random for each task. During training, 15 character instances are used for testing, with a validation set to monitor progress. The final evaluation is done on 600 randomly selected tasks. The Adam BID25 optimizer with a learning rate of 0.0001 is used, with 16 tasks per batch. Models are trained for different iterations based on the shot and way configurations. For training, different models are trained for varying iterations based on shot and way configurations. The miniImageNet dataset consists of 60,000 color images divided into 100 classes. Training follows an episodic manner similar to Omniglot, using the Adam optimizer with a Gaussian form for q and 10 \u03c8 samples. The 5-way -5-shot model is trained for 100,000 iterations with 4 tasks per batch. The neural network architectures for the feature extractor \u03b8, amortization network \u03c6, and linear classifier \u03c8 are detailed. The amortization network provides Gaussian parameters for the weight distributions of the linear classifier \u03c8. Sampling from the weight distributions utilizes the local-reparameterization trick. The weight distributions are sampled using the local-reparameterization trick, sharing the feature extraction network \u03b8 with the amortization network \u03c8. The network architecture includes conv2d layers with dropout and pooling. The network architecture for miniImageNet few-shot learning includes conv2d layers with dropout and pooling, using Batch Normalization and dropout with a keep probability of 0.5 throughout. The feature extraction network consists of multiple conv2d layers with different sizes and strides, followed by dropout and pooling operations. The curr_chunk discusses the ShapeNetCore v2 database used for experimentation, which includes 3D object categories and a dataset of 37,108 objects obtained by combining 12 object categories. The dataset used for experimentation consists of 37,108 objects from the ShapeNetCore v2 database. Objects are divided into training (70%), validation (10%), and testing (20%) sets. Each object has 36 gray-scale images at 32x32 pixels, spaced evenly every 10 degrees in azimuth. Training is done episodically, with each iteration focusing on a random object and a single image view. The dataset consists of 37,108 objects from ShapeNetCore v2 database, divided into training, validation, and testing sets. Each object has 36 views, with one view used for evaluation and the rest for generating views using an amortization network. The network architectures for encoder, amortization, and generator networks are described in Tables E.2 to E.4. Training is done with Adam BID25 optimizer, a learning rate of 0.0001, 24 tasks per batch, and 500,000 iterations.\u03c6 = 256, d \u03c8 = 256, and one \u03c8 sample is used. The ShapeNet Encoder Network (\u03c6) has an output size of 32 \u00d7 32 \u00d7 1 with multiple layers of conv2d and pooling operations, ending with a fully connected layer with RELU activation. The parameters are set to d \u03c6 = 256, d \u03c8 = 256, and one sample for \u03c8."
}