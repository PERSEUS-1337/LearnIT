{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences in language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance. The codes capture the coarse structure of the target sentence by predicting simplified part-of-speech tags. Planning ahead enhances translation quality and allows for different sentence structures. Planning ahead is essential for improving translations, as different structures can be achieved by manipulating planner codes. Evidence from speech errors and behaviors suggests that speakers plan ahead, whether at the discourse or sentence level. In contrast, neural machine translation models do not have a planning phase when generating sentences, although some argue that planning may occur in hidden layers. This lack of structural information in NMT models contrasts with the importance of planning in human speech. In contrast to human speech planning, neural machine translation models lack structural information during sentence generation. To address this, researchers propose inserting planner codes at the beginning of output sentences to help the model plan the sentence structure before decoding actual words. Neural machine translation models lack structural information during sentence generation. Researchers propose inserting planner codes at the beginning of output sentences to help the model plan the sentence structure before decoding actual words. The NMT model takes an input sentence X and produces a translation Y, with planner codes governing the sentence structure. The input sentence provides rich information about the target-side structure, such as the presence of a noun, pronoun, and verb. Uncertainty in the order of words can be disambiguated by learning a set of planner codes. In this work, planner codes are used to disambiguate uncertain information about sentence structure in neural machine translation models. The codes are learned through a discretization bottleneck in an end-to-end network that reconstructs the structure. Experiments show improved translation performance with structural planning, without requiring modifications to the NMT model. In this work, structural planning is enhanced by manipulating planner codes to control output sentence structure. The process involves simplifying POS tags to extract structural annotations and using a code learning model to obtain planner codes. The aim is to reduce uncertainty in sentence decoding by providing a \"big picture\" structural annotation. This approach efficiently addresses local structure uncertainties through beam search or the NMT model itself. In this work, coarse structural annotations are extracted through a two-step process simplifying POS tags of the target sentence. Planner codes are then learned to remove uncertainty in sentence structure during translation. The code learning model architecture involves computing discrete codes based on simplified POS tags, which are then discretized into approximated one-hot vectors using the Gumbel-Softmax trick. The information from these codes is combined with input data to initialize a decoder LSTM. The code learning model architecture involves using the Gumbel-Softmax trick to compute discrete codes from simplified POS tags. These codes are then combined with input data to initialize a decoder LSTM for sequential prediction of target sentences. The model is optimized with crossentropy loss to obtain planner codes for all training data sentences. The model is trained to obtain planner codes for target sentences in the training data. The training data consists of sentence pairs (X, Y). Planner codes are connected to target sentences with an \"eoc\" token and used during decoding with beam search. Codes are removed from translation results during evaluation. Recent methods aim to improve syntactic correctness of translations. Recent methods aim to improve the syntactic correctness of translations by proposing various approaches. BID19 and BID2 restrict the search space of NMT decoders using different techniques, while other works incorporate target-side syntactic structures explicitly. Aharoni and Goldberg (2017) train NMT models to generate linearized constituent parse trees, and BID20 proposes a model to generate words and parse actions simultaneously. Some works aim to improve translation accuracy by incorporating syntactic structures. BID20 proposes generating words and parse actions simultaneously, with word prediction and action prediction conditioned on each other. Evaluation is done on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks using Kytea for tokenization. In the code learning model, the model is trained using Nesterov's accelerated gradient for maximum 50 epochs with a learning rate of 0.25. Different settings of code length N and the number of code types K are tested, with the information capacity of the codes being N log K bits. Evaluation of the learned codes for different settings is done in TAB1, assessing accuracy in reconstructing source sentences and guessing the correct code. The study evaluates the trade-off between source sentence accuracy and code accuracy in a code learning model. A balanced trade-off is found with N=2, K=4 settings. The NMT model uses 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with different hidden units for tasks. Key-Value Attention is applied in the first decoder layer, and a residual connection (BID3) is used for combination. The NMT model utilizes residual connections and dropout for training, with an NAG optimizer and specific learning rate. Conditioning word prediction on planner codes improves translation performance by regulating the search space. The NMT model uses residual connections, dropout, and NAG optimizer with a specific learning rate for training. Conditioning word prediction on planner codes improves translation performance by regulating the search space. However, applying greedy search on JaEn dataset resulted in a lower BLEU score compared to the baseline. Beam search was also attempted but did not significantly change the results. It is hypothesized that exploring multiple candidates with different structures simultaneously is crucial for improving beam search performance. This aligns with a recent study showing that beam search performance depends on candidate diversity. The NMT model uses residual connections, dropout, and NAG optimizer with a specific learning rate for training. Conditioning word prediction on planner codes improves translation performance by regulating the search space. Table 3 shows candidate translations produced by the model when conditioning on different planner codes in Ja-En task. The proposed method improves translation performance by conditioning word prediction on planner codes, resulting in diverse translations with different structures. The distribution of assigned planner codes for English sentences in the ASPEC Ja-En dataset is shown in Figure 3, indicating high diversity in paraphrased translations. The skewed distribution of codes highlights the effectiveness of the method in generating varied translations. In this paper, a planning phase is added to neural machine translation to generate planner codes that control the structure of the output sentence. This approach aims to improve translation performance by diversifying translations with different structures. The distribution of assigned planner codes for English sentences in the ASPEC Ja-En dataset shows high diversity in paraphrased translations. The skewed distribution suggests the method's effectiveness in generating varied translations. The paper introduces a planning phase in neural machine translation to generate planner codes that influence the sentence structure. This method enhances translation performance by producing diverse translations. Experiments demonstrate the effectiveness of the planner codes in generating varied sentence structures, improving translation quality. The framework can be extended to plan other latent factors like sentiment or topic."
}