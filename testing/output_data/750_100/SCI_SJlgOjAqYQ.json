{
    "title": "SJlgOjAqYQ",
    "content": "We conducted experiments to test global translation-invariance in deep learning models trained on the MNIST dataset. Both convolutional and capsules neural networks showed poor performance in this aspect, but data augmentation improved their performance. While the capsule network performed better on the MNIST testing dataset, the convolutional neural network generally had better translation-invariance performance. The success of convolutional neural networks in computer vision tasks is attributed to two key features. The success of CNN is attributed to reduced computation cost with weight sharing in convolutional layers and generalization with local invariance in subsampling layers. Capsule networks are robust in dealing with different viewpoints by using capsules, which are a group of neurons including pose and color information. Capsule networks aim for 'rate-coded' equivariance by encoding viewpoint-invariant knowledge in weights, not neural activities. Viewpoint changes in capsule networks have linear effects on pose matrices between different layers, but their ability to generalize for global translation invariance is still unclear. Visualizing and quantifying translation-invariance in deep learning models is crucial for understanding architectural choices and developing generalization models invariant to viewpoint. In this paper, a simple method is introduced to test the performance of global translation-invariance in convolutional and capsule neural network models trained on the MNIST dataset. The method involves using a testing dataset with images generated by shifting the center of mass of a Helvetica font digit one pixel at a time. This testing helps evaluate the global translational invariance (GTI) of deep learning models trained on the MNIST data. The testing dataset for evaluating global translational invariance involves shifting the center of mass of digits one pixel at a time. The dataset consists of 2520 images covering all possible translational translations. Deep learning models are trained on the MNIST dataset with 60000 samples and tested on both MNIST and GTI datasets. MNIST images are centered, while GTI images are uniformly distributed on the canvas. Our method for evaluating global translational invariance is robust to noise and small training datasets. The GTI dataset allows for capturing tiny differences in models and quantifying global invariance. The CNN model used has nine layers. The CNN model used for evaluating global translational invariance has nine layers with specific configurations for each layer. The total number of parameters is 361578, much smaller than Capsule networks. ReLU activation function is used in all layers except the last one, which uses softmax. The optimizer is Adam with default parameters in Keras. The CNN model, using ReLU in most layers, has Adam optimizer and cross entropy loss. Achieving high accuracy on MNIST data, it struggles with global translational invariance. Images with digits near the center are predicted correctly, while those at the corners are often misclassified. The CNN model struggles with global translational invariance, misclassifying images at the corners. Training with data augmentation by shifting images from the center improves performance on the GTI dataset. Data augmentation by shifting images from the center improves accuracy on the GTI testing dataset to 98.05%. CapsNet with 8.2M parameters outperforms CNN on the same dataset. The model uses Adam optimizer with exponential decay of learning rate (decay parameter 0.9) and margin loss in BID9. Reconstruction loss is added but scaled down by 0.0005. Capsule network shows robustness in viewpoint invariance for object recognition, but struggles with global invariance. Training on MNIST without data augmentation leads to incorrect predictions and images near the edge. Data augmentation improves CapsNet accuracy on the GTI dataset. The Capsule network shows robustness in viewpoint invariance for object recognition but struggles with global invariance. Training on MNIST with 20% shifting results in nearly all images being predicted correctly except those close to the edge. CNN outperforms CapsNet on the GTI dataset, even with wider receptive fields in CapsNet's convolutional layers. The removal of max-pooling layers in CapsNet may contribute to its lower performance on the GTI dataset. The Capsule network struggles with global translational invariance, leading to lower performance on the GTI dataset compared to CNN. Despite this, CapsNet architecture has the potential to handle global translational invariance better than CNN due to capsules' ability to learn all viewpoints. Capsules in CapsNet can learn all viewpoints, making them suitable for computer vision tasks. Testing on the GTI dataset shows CapsNet's potential, but it struggles with global translational invariance compared to CNN. The testing method involves applying random shifting to MNIST training data to cover all possible cases."
}