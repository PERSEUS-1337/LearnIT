{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map observed input-output pairs to a distribution over regression functions efficiently. However, NPs suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data they condition on. This issue is addressed by incorporating attention. Incorporating attention into Neural Processes (NPs) improves prediction accuracy, speeds up training, and expands the model's capabilities. Regression tasks involve modeling the distribution of output given input using a deterministic function like a neural network. Training is done on input-output pairs, with predictions being independent given the inputs. Incorporating attention into Neural Processes (NPs) enhances prediction accuracy, accelerates training, and broadens the model's capabilities by efficiently modeling a distribution over regression functions. Neural Processes (NPs) can predict the distribution of an output based on input-output pairs, with prediction complexity linear in the context set size. NPs can model data generated from a stochastic process, unlike Gaussian Processes (GPs) which are trained on observations from a single function. Comparing NPs and GPs directly is usually not feasible due to their different training regimes. Neural Processes (NPs) have appealing properties but tend to underfit the context set, leading to inaccurate predictions. The encoder in an NP aggregates the context set to a fixed-length latent summary using a permutation invariant function. The encoder in Neural Processes aggregates the context set to a fixed-length latent summary, leading to underfitting behavior. The mean-aggregation step acts as a bottleneck, making it difficult for the decoder to learn relevant context points for target predictions. Increasing dimensionality of the representation is not sufficient to address this issue. Inspiration is drawn from Gaussian Processes to improve conditional modeling. To address underfitting in Neural Processes, inspiration is drawn from Gaussian Processes. The kernel in GPs measures similarity between points, ensuring accurate predictions. This mechanism is implemented in Attentive Neural Processes (ANPs) using differentiable attention to focus on relevant contexts for target predictions while maintaining permutation invariance. ANPs are evaluated on 1D function tasks. Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) in terms of context reconstruction and training speed for 1D function regression and 2D image regression. ANPs demonstrate enhanced expressiveness and can model a wider range of functions by conditioning on observed contexts to predict targets. The model uses observed contexts to model an arbitrary number of targets in a way that is invariant to ordering. It aggregates context pairs using a deterministic function and an MLP, then models the likelihood of targets based on the aggregated representation. The NP model aggregates context pairs using a deterministic function and an MLP to form r C, then models the likelihood p(y T |x T , r C) with a Gaussian factorised across targets. The latent variable version includes a global latent z to account for uncertainty in predictions, modelled by a factorised Gaussian parametrised by s C. The encoder-decoder model includes a global latent variable z modeled by s C, with the likelihood as the decoder. The model can be defined using the deterministic path, latent path, or both. Using both paths allows for attention incorporation. The encoder-decoder model incorporates attention by maximizing the ELBO for a subset of contexts and targets. The NP learns to reconstruct targets with a KL term that encourages similarity between context and target summaries. The number of contexts and targets are randomly chosen at each training iteration. At each training iteration, the number of contexts and targets are randomly chosen for the NP to learn a wide family of conditional distributions. NPs have desirable properties like scalability, flexibility, and permutation invariance. However, they do not satisfy consistency in the contexts. The NP approximates the conditionals of the consistent data-generating stochastic process by minimizing KL divergence. An attention mechanism computes weights for key-value pairs in a query context. The attention mechanism computes weights for key-value pairs in a query context, allowing the query to attend to the key-value pairs. This permutation invariance property of attention is essential for its application to NPs. The idea of using a differentiable addressing mechanism has been successfully applied in various areas of Deep Learning. Attention mechanisms compute weights for key-value pairs in a query context, allowing the query to attend to the key-value pairs. Differentiable addressing mechanisms have been successfully applied in various areas of Deep Learning. Self-attention has been used for expressive sequence-to-sequence mappings in natural language processing and image modeling. Various forms of attention mechanisms, such as locality-based and dot-product attention, are discussed in the paper. Multihead attention is a parametrised extension of dot-product attention, where keys, values, and queries are linearly transformed for each head. This allows the query to attend to multiple head-specific values, which are then concatenated and linearly transformed to produce the final values. The multihead architecture in attentive neural processes allows queries to attend to different keys for each head, resulting in smoother query-values than dot-product attention. Self-attention is applied to context points to compute representations of each (x, y) pair, with the target input using cross-attention to predict the target output. The self-attention mechanism models interactions between context points before mean-aggregation. The self-attention mechanism in attentive neural processes models interactions between context points to obtain richer representations that encode relations between them. This is achieved by stacking self-attention and using cross-attention to produce query-specific representations in the deterministic path. The model allows each query to attend closely to relevant context points for prediction, preserving global latent dependencies. The latent path captures correlations in target predictions' distribution, while the deterministic path models local structure. The NP with attention preserves global dependencies by allowing queries to attend to relevant context points for prediction. The addition of attention increases expressivity and accuracy but also raises computational complexity. The addition of attention in NPs raises computational complexity to O(n(n + m)), but most computation is done in parallel. ANPs learn faster than NPs during training despite being slower at prediction time. The (A)NP learns a stochastic process and should be trained on multiple functions. The (A)NP learns a stochastic process and should be trained on multiple functions that are realisations of the process. During training, a batch of realisations is drawn from the data generating process, and random points are selected as targets to optimize the loss. The decoder architecture is consistent across experiments, with 8 heads for multihead. The (A)NP does not need to be trained on GP data or data from a known stochastic process. Two settings are explored: fixed hyperparameters for the kernel throughout training, and randomly varying hyperparameters at each iteration. The number of contexts and targets are randomly chosen at each iteration. X-values are drawn uniformly at random in the range of -2 to 2. Self-attention is not used for this 1D data, only cross-attention in the deterministic path. The encoder/decoder architecture is the same for NP and ANP, except for cross-attention. The encoder/decoder architecture is the same for NP and ANP, except for cross-attention. ANP shows faster learning and lower reconstruction error compared to NP, especially with dot product and multihead attention. Despite the added computational cost, ANP's convergence is quicker both in training iteration and wall clock time. Computation times of Laplace and dot-product ANP are similar to NP for the same value of d. The size of the bottleneck (d) in the deterministic and latent paths of the NP affects underfitting behavior. Increasing d improves reconstructions but has a limit. Beyond a certain value, learning becomes slow, and reconstruction error is still higher than multihead ANP with less time. Using ANPs offers significant benefits over increasing bottleneck size in NPs. Using ANPs provides significant benefits over increasing bottleneck size in NPs. In a qualitative comparison of attention mechanisms, dot-product attention accurately predicts almost all context points, while Laplace attention underfits the context. Dot-product attention has parameterized keys and queries, unlike Laplace attention which uses x-coordinates. The use of dot-product attention with parameterized keys and queries outperforms Laplace attention, which computes similarities based on L1 distance in the x-coordinate domain. Dot-product attention shows non-smooth predictions but multiple heads in multihead attention help smooth out interpolations for better context reconstruction. The (A)NP model shows good reconstruction of contexts and prediction of targets, with increased uncertainty away from contexts. It outperforms the NP model by being more expressive and capable of learning a wider range of functions. The (A)NPs are used to solve a toy Bayesian Optimization problem to find the minimum of test functions. Bayesian Optimisation (BO) problem involves finding the minimum of test functions from a GP prior. The (A)NP model can sample entire functions and accurately reconstruct contexts. Image data regression involves predicting pixel values from a stochastic process. See Appendix C for detailed analysis of results. We train the ANP on MNIST and CelebA datasets using self-attentional layers in the encoder. Three different models are compared: NP, Multihead ANP, and Stacked Multihead ANP. The ANP is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Comparing three models: NP, Multihead ANP, and Stacked Multihead ANP, the Stacked Multihead ANP shows accurate reconstructions of the whole image with the use of attention, achieving crisper inpaintings. The Stacked Multihead ANP, trained on MNIST and CelebA datasets, achieves accurate reconstructions of images using attention. It shows crisper inpaintings and can model global image structure with different z values. The model generalizes well even with limited context points during training. The Stacked Multihead ANP, trained on MNIST and CelebA datasets, achieves accurate reconstructions of images using attention. It shows improved context reconstruction error and NLL for target points with multihead crossattention. Stacked self-attention provides noticeable gains in crispness and global coherence qualitatively. Visualizing each head of Multihead ANP for CelebA shows where attention focuses on different pixels. The Multihead ANP, trained on MNIST and CelebA datasets, achieves accurate image reconstructions using attention. Different heads focus on various regions of the image, with each head having a unique role. The attention weights are color-coded for visualization, showing consistent behavior across different target pixels. The Multihead ANP achieves accurate image reconstructions using attention, with different heads focusing on various regions of the image. One application is mapping images from one resolution to another by predicting pixel intensities in a continuous space. This process can be problematic for NPs with inaccurate reconstructions. The Multihead ANP achieves accurate image reconstructions using attention, with different heads focusing on various regions of the image. It can map images from one resolution to another by predicting pixel intensities in a continuous space. The reconstructions of ANPs may be accurate enough to give reliable mappings between different resolutions, as shown in FIG6. The ANP is capable of mapping low resolutions to realistic target outputs with some diversity for different values of z. This performance is expected since the model has been trained on data with 32 \u00d7 32 resolution. The model, trained on 32 \u00d7 32 resolution data, can map images to higher resolutions like 256 \u00d7 256. It produces realistic high-resolution images with sharper edges and learns internal representations of features like eyes, even when the original image is coarse. The model can map images to higher resolutions like 256 \u00d7 256, producing realistic high-resolution images with sharper edges and learning internal representations of features like eyes. The ANP is not a replacement for state-of-the-art algorithms in image inpainting or super-resolution, but it shows flexibility in modeling a wide range of image applications. The ANP demonstrates flexibility in modeling various image applications by utilizing attention mechanisms similar to Gaussian Processes, which measure similarity between points in the same domain. This approach is related to Deep Kernel Learning, where a GP is used in an embedding space. The embedding space explored is related to Deep Kernel Learning, where a GP is applied to learned data representations. Comparing GPs and NPs training regimes is challenging, but one possibility is to update the GP kernel hyperparameters using NPs training regime. However, this approach still incurs a high computational cost and may require kernel approximations. The predictive uncertainties of Gaussian Processes (GPs) heavily depend on the kernel choice, while Neural Processes (NPs) learn uncertainties directly from data. GPs have the advantage of being consistent stochastic processes with exact closed-form expressions for covariance and marginal variance, a feature lacking in current NP formulations. Variational Implicit Processes (VIP) are related to NPs, defining a stochastic process with a finite dimensional z. In this work, the process and its posterior given observed data are approximated by a GP and learned via a generalisation of the Wake-Sleep algorithm. Meta-Learning (A)NPs can do few-shot learning, with attention being used in works like Vinyals et al. (2016) and Snell et al. (2017) for locating relevant observed image/prototype. Attention has been used in Meta-RL tasks such as continuous control and visual navigation. Few-shot density estimation using attention has been explored in various works. The Neural Statistician and the Variational Homoencoder have similar permutation invariant encoders. For ANPs, regression setting is less explored. Vfunc explores regression on a toy 1D domain using a setup similar to NPs. Regression on a toy 1D domain is explored without attention mechanisms, optimizing an approximation to the entropy of the latent function. Multitask learning in Gaussian Processes literature has been addressed by various works. Generative Query Networks are models for spatial prediction, with a special case similar to Neural Processes. Rosenbaum et al. (2018) apply GQN to 3D localization with an attention mechanism focusing on context frames. In this work, ANPs are proposed to augment NPs with attention, improving prediction accuracy, training speed, and model functionality. Future work includes incorporating cross-attention and global latent paths in the model architecture. The ANPs proposed in this work augment NPs with attention for improved prediction accuracy and training speed. Future work may involve incorporating cross-attention and global latent paths in the model architecture. Additionally, there is potential to train ANPs on text data to fill in blanks in a stochastic manner, and connections with the Image Transformer (ImT) BID21 suggest a model closely resembling an ImT defined on arbitrary orderings of pixels. The proposed Attention Neural Processes (ANPs) enhance Neural Processes (NPs) with self-attention in the decoder to improve expressiveness. The ordering and grouping of targets become important in this setup. Architectural details of NP and Multihead ANP models for regression experiments are shown in Figure 8. The experiments in Figure 8 show MLPs with relu non-linearities in the latent path outputs \u00b5 z , \u03c9 z and decoder outputs \u00b5 y , \u03c9 y. The 1D regression uses multihead cross-attention (M ultihead 1), while the 2D regression uses a form of multihead cross-attention from the Image Transformer BID21 without dropout. The self-attention module in the Stacked Multihead ANP for 2D Image regression experiments does not use dropout to limit stochasticity. It uses the same architecture as cross-attention but with specific parameters to output representations. Stacking more layers did not significantly improve results. The experiments also used specific parameters for the squared exponential kernel in the data generating GP. For fixed kernel hyperparameter experiments, l = 0.6 and \u03c3 2 f = 1, while for random kernel hyperparameter case, l \u223c U [0.1, 0.6], \u03c3 f \u223c U [0.1, 1]. Likelihood noise is \u03c3 n = 0.02. Batch size is 16, using Adam Optimiser BID14 with learning rate of 5e-5. One sample of q(z|s C ) is used for MC estimate of loss. Trained (A)NP models are compared in Figure 9. The MC estimate of loss in Equation FORMULA2 is compared in Figure 9 between the trained (A)NP models and the oracle GP. The Multihead ANP is closer to the oracle GP than the NP but still underestimates predictive variance due to variational inference. Investigating how to address this issue would be interesting. The conditional distributions for fixed kernel hyperparameters show non-smooth behavior for dot-product attention. The conditional distributions for fixed kernel hyperparameters exhibit non-smooth behavior in dot-product attention, leading to poor interpolations between context points. The KL term in NP loss differs between training on fixed and random kernel hyperparameters. In the fixed kernel hyperparameter case, the KL for multihead ANP quickly goes to 0, indicating the model finds the deterministic path sufficient for accurate predictions. However, in the random hyperparameter case, there is added variation in the data, leading to a non-zero KL where the model uses latents to model uncertainty in the stochastic process given context points. Using (A)NPs trained on 1D GP data, the study tackles the Bayesian optimization problem by modeling variation in stochastic processes with latents. ANPs consider all previous function evaluations as context points to create an informed surrogate of the target function. Results show that ANPs with different attention mechanisms outperform an oracle GP with true kernel hyperparameters in finding the minimum of test functions drawn from a GP prior. Thompson sampling is used to act based on the minimal predicted value of the surrogate function. The simple regret is consistently smallest for a NP with multihead attention, approaching the oracle GP. The cumulative regret decreases most rapidly for multihead, utilizing previous function evaluations effectively for predicting the function minimum. The initial lower cumulative regret compared to the oracle GP is due to under-exploration caused by uncertainties of ANP away from the context. The stacked self-attention architecture is used with a batch size of 16 for both MNIST and CelebA datasets, with specific learning rates and optimizer settings. The x and y values are rescaled for training, with random selection of pixels as targets and contexts. The stacked self-attention architecture is used with specific hyperparameters for both MNIST and CelebA datasets. Dropout is not used to restrict model stochasticity, and positional embeddings of pixels are not utilized. Little tuning has been done regarding architectural hyperparameters. One sample of q(z|s C ) is used to form a MC estimate of the loss. The NP overestimates predictive variance visually, as shown in the plot of standard deviation. The Stacked Multihead ANP improves results significantly over Multihead ANP, providing sharper images with better global coherence even when the face isn't axis-aligned. The uncertainty in the original NP is reduced with attention as the number of contexts increases, almost disappearing for the full context. The contexts in the figures contain the target, where relying on the cyan head alone can give an accurate prediction, but the different roles of the heads also apply when the target is disjoint from the context. In the case where the target is disjoint from the context, all heads become useful for target prediction as shown in FIG0. Each head visualizes pixels attended to in multihead attention in the NP, with different colors assigned to each head."
}