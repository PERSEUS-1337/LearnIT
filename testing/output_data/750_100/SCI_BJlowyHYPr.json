{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model for forecasting data streams from geospatial point-cloud sources. It uses a Dynamic Point-cloud Convolution (D-Conv) operator to perform convolution directly over point-clouds, extracting local spatial features from neighboring points. This operator maintains permutation invariance and represents neighboring correlations, crucial for spatiotemporal predictive learning. The D-Conv operator integrates grid-structural data into spatiotemporal forecasting models, compatible with LSTM architectures and attention mechanisms. CloudLSTM architecture is applied to point-cloud stream forecasting for mobile service traffic and air quality indicator forecasting. Real-world datasets show accurate long-term predictions, outperforming other neural network models. The forecasting aims to predict future values and locations of data streams from geospatial point-clouds. Antennas and sensors monitor city-scale mobile services and air quality. Point-cloud stream forecasting operates on irregular sets of points with complex spatial correlations. Vanilla LSTMs have limited spatial feature capabilities. Convolution-based recurrent neural network models like ConvLSTM and PredRNN++ are not suitable for handling scattered point-cloud data. Different approaches to geospatial data stream forecasting include predicting over grid-structured data streams and forecasting directly over point-cloud data streams using historical information. The proposed PointCNN leverages spatial-local correlations of point clouds without pre-processing, while ConvLSTM and PredRNN++ are not suitable for scattered point-cloud data. CloudLSTM introduces the DConv operator for forecasting over point cloud-streams. CloudLSTM introduces the DConv operator for precise forecasting over point-cloud streams, defining a point-cloud as a set of N points with value features and coordinates. An ideal point-cloud stream forecasting model should have five key properties: (i) Order invariance, where the arrangement of points does not affect the output; (ii) Information intactness, maintaining the same number of points without losing any information. An ideal point-cloud stream forecasting model should have five key properties: Order invariance, information intactness, interaction among points, robustness to transformations, and location variance. The model should capture local dependencies, allow interactions, be robust to transformations, and revise dynamic correlations during training. The Dynamic Point Cloud Convolution (DConv) operator is introduced to address these requirements. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM to address key properties required in a point-cloud stream forecasting model. DConv generalizes convolution on point-clouds, allowing for interactions, robustness to transformations, and dynamic correlations during training. The DConv operator in CloudLSTM processes point-cloud data by taking input channels and producing output channels while maintaining information integrity. It defines subsets of points within the input set and includes the nearest points to a specific point in Euclidean space. Each point in the set contains value and coordinate features. The DConv operator in CloudLSTM processes point-cloud data by defining subsets of points with value and coordinate features. It uses shared weights to sum element-wise products for dynamic spatial correlations, aggregating coordinate features for better exploitation. Learnable weights are 5D tensors shared across anchor points in the input map. The DConv operator in CloudLSTM uses shared weights as 5D tensors for dynamic spatial correlations in processing point-cloud data. Each element represents a scalar weight for input and output channels, nearest neighbors, and coordinate features. Bias terms are defined for output maps, and the sigmoid function limits predicted coordinates to (0, 1) to prevent outliers. The DConv operator in CloudLSTM uses shared weights for dynamic spatial correlations in processing point-cloud data. Raw point-cloud coordinates are normalized to (0, 1) before input to the model for improved transformation robustness. The number of nearest points can vary for each channel, reflecting different types of measurements in the dataset. Spatial correlations differ between measurements due to the nature of the data. The CloudLSTM's DConv operator utilizes shared weights for dynamic spatial correlations in processing point-cloud data, allowing each channel to find the best neighbor set for improved forecasting performance. This approach ensures learnable spatial correlations that vary between measurements due to human mobility and different data consumption patterns. The DConv operator in CloudLSTM utilizes shared weights for dynamic spatial correlations in processing point-cloud data. It weights its K nearest neighbors across all features for each point, producing values and coordinates in the next layer. DConv is a symmetric function that does not depend on input order, satisfying discussed properties. It operates on every point in a set, producing the same number of features and points for its output. The DConv operator in CloudLSTM utilizes shared weights for dynamic spatial correlations in processing point-cloud data. It captures local dependencies and improves robustness to global transformations. Normalization over coordinate features further enhances robustness. DConv learns the layout and topology of the cloud-point for the next layer, enabling dynamic positioning tailored to each channel and time step. The DConv operator in CloudLSTM utilizes shared weights for dynamic spatial correlations in processing point-cloud data, capturing local dependencies and improving robustness to global transformations. It enables dynamic positioning tailored to each channel and time step, efficiently implemented using simple 2D convolution. The DConv operator in CloudLSTM uses shared weights for dynamic spatial correlations in processing point-cloud data, capturing local dependencies and improving robustness to global transformations. It enables dynamic positioning tailored to each channel and time step, implemented efficiently with simple 2D convolution. PointCNN introduces variations to deformable convolution neural networks tailored to pointcloud structural data, employing the X-transformation to learn weight and permutation on a local point set using multilayer perceptrons. The DConv operator maintains permutation by aligning the weight of distance rankings between points, ensuring order invariance without extra parameters. The DConv operator in CloudLSTM ensures order invariance without extra complexity by deforming input maps and selecting neighboring points for operations. It can be plugged into LSTMs to learn spatial and temporal correlations over point-clouds. The Convolutional Point-cloud LSTM (CloudLSTM) is formulated similarly to ConvLSTM, with input, forget, and output gates (i_t, f_t, o_t), memory cell (C_t), and hidden states (H_t) all represented as point cloud data. Weight and bias tensors (W and b) are learnable parameters. The DConv operator, used in CloudLSTM, ensures order invariance and operates over gates computation without the sigmoid function. The CloudLSTM cell operates over gates computation without sigmoid functions. It is combined with Seq2seq learning and soft attention mechanism for forecasting. The overall Seq2seq CloudLSTM architecture includes an encoder and decoder with different stacks of CloudLSTMs. The encoder encodes historical information. The CloudLSTM cell, combined with Seq2seq learning and soft attention mechanism, encodes historical information into a tensor and decodes it into predictions. Point Cloud Convolutional layers process the data before generating final forecasts, similar to word embedding in NLP tasks. The model employs a two-stack encoder-decoder architecture. In this study, a two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell is employed. The DConv is also explored in vanilla RNN and Convolutional GRU, leading to CloudRNN and CloudGRU. These new architectures share a similar Seq2seq structure with CloudLSTM but do not use the attention mechanism. Performance comparison is conducted in the following section. To evaluate the performance of CloudLSTM and other architectures, datasets from 38 mobile services and 6 air quality indicators are used. CloudLSTM is utilized for forecasting future demands and air quality. A comparison with 12 baseline deep learning models is conducted using TensorFlow and TensorLayer libraries on a computing cluster with two NVIDIA Tesla K40M GPUs. In this study, various architectures are trained using a computing cluster with NVIDIA Tesla K40M GPUs. Models are optimized using the Adam optimizer and mean square error. The datasets and baseline models are introduced before discussing experimental results on spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments. Coordinate features are omitted due to fixed data source locations. In mobile traffic forecasting, large-scale multi-service datasets from two European metropolitan areas are analyzed. The data includes traffic volume generated by devices connected to non-uniformly distributed antennas in urban regions, resembling 2D point clouds. Coordinate features are excluded in the analysis due to fixed data source locations. The dataset consists of 24,482 traffic snapshots from 38 mobile services, each aggregated over 5-minute intervals. The dataset includes popular apps for video streaming, gaming, messaging, cloud services, and social networking. Air quality forecasting is done using a public dataset with six air quality indicators collected by 437 monitoring stations in China. The dataset includes air quality indicators (PM10, NO2, CO, O3, SO2) collected by 437 monitoring stations in China over a year. The stations are divided into two city clusters, with data measured hourly. Each cluster has 8,760 snapshots. Missing data is filled using linear interpolation. Measurements are transformed into input channels for the point-cloud S model after normalization. Refer to Appendix G for more details. The dataset includes air quality indicators collected by monitoring stations in China. The data is normalized and transformed into input channels for the point-cloud S model. Coordinate features are normalized to the (0, 1) range. Point-clouds are transformed into grids using the Hungarian algorithm. The ratio of training plus validation, and test sets is 8:2. CloudLSTM is compared with baseline models like PointCNN and CloudCNN for feature extraction from point clouds. The study compares CloudLSTM with various baseline models like PointCNN and CloudCNN for feature extraction from point clouds. Other models such as MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++ are also discussed in detail in the appendix. The accuracy of CloudLSTM for mobile traffic prediction is evaluated using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are also used to measure forecast fidelity and similarity with the ground truth. Neural networks are used to forecast city-scale mobile traffic consumption over a time horizon of 30 minutes with 6 consecutive past measurements. RNN-based models like LSTM, ConvLSTM, PredRNN++, CloudLSTM, CloudRNN, and CloudGRU are evaluated for long-term performance with 3 hours prediction steps. In the air quality forecasting use case, models receive 12 measurements as input and forecast indicators for the following 12 hours. Prediction steps are extended to 3 days for all RNN-based models. RNN-based models, including CloudLSTM, CloudRNN, and CloudGRU, outperform CNN-based models and MLP in forecasting urban scenarios. Different numbers of neighboring points and the attention mechanism are investigated, with RNN-based architectures showing superior performance in terms of lower MAE/RMSE and higher PSNR/SSIM. The DConv operator effectively learns features over geospatial point-clouds. The CloudLSTM model performs better than CloudGRU and CloudRNN in forecasting urban scenarios. The forecasting performance of CloudLSTM is not significantly affected by the number of neighbors, suggesting the use of a small K to reduce model complexity. The attention mechanism improves forecasting performance by capturing better dependencies between input sequences and vectors in decoders. In long-term forecasting performance evaluation, the MAE evolution of RNN-based architectures is extended up to 36 time steps. Models show reliability in city 1 with consistent MAE, while low K may impact CloudLSTM performance in city 2. This offers guidance on selecting K for different scenarios. In long-term forecasting, CloudLSTMs outperform ConvLSTMs by up to 12.2% in MAE and 8.8% in RMSE for 12-step air quality forecasting. Lower K values lead to better predictions, with CloudCNNs proving superior to PointCNNs as feature extractors. The effectiveness of CloudLSTM models for spatiotemporal point-cloud stream data is demonstrated, with CloudCNNs outperforming PointCNNs as feature extractors. Performance evaluations for long-term forecasting up to 72 time steps are conducted on RNN-based models. The experiments follow a strict variable-controlling methodology to study the effect of each factor. CloudLSTM, a dedicated neural model for spatiotemporal forecasting tailored to pointcloud data streams, outperforms other models like LSTM, ConvLSTM, PredRNN++, PointLSTM, and CloudRNN. D-Conv significantly contributes to performance improvements, while the attention mechanism in models like CloudLSTM does not have a significant impact. Core operator > RNN structure > attention are ranked by their contribution to performance. The CloudLSTM model, utilizing the DConv operator for convolution over point-clouds, predicts values and coordinates of each point to adapt to changing spatial correlations in data streams. It can be combined with various RNN models, Seq2seq learning, and attention mechanisms efficiently. The DConv operator transforms input tensors into 4D tensors for convolution. The output tensor is reshaped to (N, (H + L), U out) for efficient implementation. The DConv operator reshapes the output tensor to (N, (H + L), U out) for efficient implementation by applying the sigmoid function to the coordinates feature in S out. The complexity of DConv is analyzed by separating the operation into finding neighboring sets and performing weighting computations. The complexity of the DConv operator is analyzed by finding neighboring sets and performing weighting computations. The complexity of finding K nearest neighbors is O(K \u00b7 L log N) using KD trees. The overall complexity of computing features of the output point set is O((H + L) \u00b7 K), similar to a vanilla convolution operator. DConv introduces extra complexity compared to a convolution operator. By normalizing the coordinates features, the model achieves transformation invariance to shifting and scaling. This is represented by positive scaling coefficient A and offset B. The proposed CloudLSTM is combined with an attention mechanism for improved performance. The proposed CloudLSTM is combined with an attention mechanism for improved performance. The context tensor for state i at the encoder is represented with a score function e i,j. The proposal is compared against baseline models like MLP, CNN, and 3D-CNN, commonly used in mobile traffic forecasting. DefCNN learns convolutional filter shapes and is similar to the DConv operator in this study. The PredRNN++ is a state-of-the-art architecture for spatiotemporal forecasting on grid-structural data, outperforming models like ConvLSTM. CloudRNN and CloudGRU share a Seq2seq architecture with CloudLSTM but do not use the attention mechanism. In Table 3, the detailed configuration and parameters for each model in the study are shown. ConvLSTM, PredRNN++, and PointLSTM used 2 layers like CloudLSTMs, with 3x3 filters for a receptive field of 9. PredRNN++ has a unique structure compared to other Seq2seq models. The study compares different Seq2seq models with varying structures and parameters, including 2-stack Seq2seq CloudLSTM with different values of K, and Attention CloudLSTM. The models are optimized using the MSE loss function and evaluated using MAE, RMSE, PSNR, and SSIM metrics. The study evaluates models using MAE, RMSE, PSNR, and SSIM metrics. PSNR is defined with coefficients c1 and c2 to stabilize the fraction. Anonymized locations of antenna sets in two cities are shown in Figure 5. The measurement data is collected via traditional flow-level deep packet inspection at the packet gateway (P-GW) using proprietary traffic classifiers to associate flows to specific services. Anonymized locations of antenna sets in both cities are shown in Figure 5. Data collection was supervised by the national privacy agency to ensure compliance with data protection and confidentiality constraints. The dataset used for the study is fully anonymized and does not contain personal information. The raw data cannot be made public due to a confidentiality agreement. The analysis includes 38 different services, each showing the fraction of total traffic consumed. The analysis includes 38 different services, showing the fraction of total traffic consumed. Streaming is the dominant type of traffic, with web, cloud, social media, and chat services also consuming large fractions of mobile traffic. Gaming only accounts for 0.5% of the demand. The air quality dataset from 43 cities in China, collected by Microsoft Research, consists of 2,891,393 records from 437 monitoring stations over a year. Stations are divided into two clusters, A with 274 stations and B with 163. Missing data was filled using linear interpolation. The dataset can be accessed at https://www.microsoft.com/en-us/research/project/urban-air/. The proposed Attention CloudLSTMs are evaluated for forecasting accuracy on individual mobile services, showing similar performance across cities. Services with higher traffic volume, like streaming and cloud, have higher prediction errors due to frequent fluctuations in traffic evolution. The MAE evolution of RNN-based models for air quality forecasting on city clusters shows that error increases with time, especially for CloudLSTM with larger K values. This improvement in robustness is consistent with findings in mobile traffic forecasting. The evaluation of the mobile traffic forecasting task includes visualizing hidden features of CloudLSTM. Scatter distributions of hidden states in H t of CloudLSTM and Attention CloudLSTM are shown in Fig. 10, with examples of input snapshots from City 2. Each H t has 1 value feature and 2 coordinate features for each point. In Fig. 10, scatter subplots show hidden features of CloudLSTM and Attention CloudLSTM in City 2. Fig. 11 and 12 display NO2 forecasting examples in City Cluster A and B generated by RNN-based models, comparing performance visually. Point-clouds are converted into heat maps for better prediction by Attention CloudLSTMs. The proposed architectures, particularly Attention CloudLSTMs, show better prediction capabilities in capturing trends in point-cloud streams and maintaining high long-term visual fidelity. In contrast, other architectures degrade rapidly over time. The DConv method utilizes Sigmoid functions to regulate coordinate features, bringing distant points closer together for improved computation. Multiple DConv layers are stacked for enhanced performance. The CloudLSTM model, utilizing stacked DConv layers, enhances representability by refining input point positions at each time step. Outlier points are identified using DBSCAN, showing improved forecasting performance even with outlier points. The DBSCAN algorithm identifies 16 outlier points in the point-cloud data. The CloudLSTM model still achieves the lowest prediction error compared to other models. The CloudCNN model, utilizing the DConv operator, shows the best forecasting performance among CNN-based models. Further experiments are conducted to test the robustness of CloudLSTM to outliers. In a toy dataset, 10 outliers were randomly selected from 50 weather stations in each city cluster and moved away from the center by different distances on both axes. The positions of the remaining 40 weather stations were unchanged. The CloudLSTM and PointLSTM models were retrained under the same settings after the outliers were moved. The proposed CloudLSTM model performs well in forecasting outliers and inliers, regardless of their distance. It outperforms the PointLSTM model significantly. Comparisons with simple baselines like MLPs and LSTMs show the robustness of the CloudLSTM model to outliers. The CloudLSTM model outperforms MLPs and LSTMs by utilizing the K nearest neighbors' data for prediction. The number of neighbors K impacts the model's receptive field, with a small K focusing on local spatial dependencies and a large K considering global spatial correlations. The CloudLSTM model outperforms MLPs and LSTMs by utilizing K nearest neighbors' data for prediction, focusing on local spatial dependencies with small K and global spatial correlations with large K. The results show that K does not significantly affect baseline performance, while seasonal information in mobile traffic series can be leveraged for forecasting, although feeding the model with data spanning multiple days is impractical. To efficiently capture seasonal information in mobile traffic data, the study concatenates 30-minute sequences with a sub-sampled 7-day window to create an input with a manageable length of 90 data points. This approach avoids the impracticality of using data spanning multiple days, which would result in an extremely large forecasting model. By incorporating seasonal information in mobile traffic data forecasting, the performance of most models is improved, indicating that the model learns periodic patterns to reduce prediction errors. However, this approach increases input length and model complexity. Future work will focus on a more efficient way to integrate seasonal information with minimal complexity increase."
}