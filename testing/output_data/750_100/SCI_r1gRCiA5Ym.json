{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization performance and prevent overfitting in deep neural networks. This paper discusses three novel observations about dropout in DNNs with ReLU activations: 1) dropout encourages training each local linear model on nearby data points, 2) a constant dropout rate can result in different neural-deactivation rates for layers with varying fractions of activated neurons, and 3) the rescaling factor of dropout causes inconsistencies between training and testing normalization. The proposed method \"Jumpout\" addresses the inconsistency in normalization between training and testing conditions when using batch normalization. Jumpout samples the dropout rate using a decreasing distribution, trains local linear models on nearby data points, and adaptively normalizes the dropout rate at each layer and training sample. Jumpout addresses normalization inconsistency between training and testing with batch normalization by rescaling outputs for a better trade-off. It shows improved performance on various datasets with minimal additional costs. Dropout is a technique used in deep neural networks to prevent overfitting by randomly setting hidden neuron activations to 0. It helps reduce co-adaptation amongst neurons without adding significant computational overhead. However, tuning dropout rates can be a drawback as it requires extra hyper-parameters at each layer. Dropout rates need to be tuned separately for each layer and during various training stages to optimize performance. In practice, a single dropout rate is often used for all layers to reduce computation. Dropout acts as a perturbation on training samples, helping the DNN generalize to noisy data with a specific expected amount of perturbation. The fixed dropout rate causes perturbation with high probability, ruling out samples with less perturbation that could improve generalization. Different fractions of activated neurons lead to varying effective dropout rates, causing too much or too little perturbation. Dropout is incompatible with batch normalization. Dropout is incompatible with batch normalization (BN) due to the need for rescaling undropped neurons, which breaks the consistency of normalization parameters between training and test phases. This conflict often leads to dropout being omitted in favor of BN in modern DNN architectures. Dropout is often omitted in modern DNN architectures in favor of batch normalization. Three modifications are proposed to improve dropout, resulting in \"jumpout.\" The approach is motivated by observations on how dropout enhances generalization performance for DNNs with ReLU activations. Dropout enhances generalization performance for DNNs with ReLU activations by changing the polyhedral structure and linear models. Each model is trained to work for data points in nearby polyhedra, explaining why dropout improves performance. The typical number of units dropped out is np with a fixed dropout rate, leading to improved generalization. In jumpout, the dropout rate is a random variable sampled from a decreasing distribution, achieving local smoothness by allowing for different numbers of units to be dropped out. The dropout rate in jumpout is sampled from a decreasing distribution, ensuring local smoothness by allowing for varying numbers of units to be dropped out. This results in a higher probability of choosing a smaller dropout rate, decreasing the probability of smoothing polyhedra as points move farther away. Additionally, the fraction of activated neurons can differ across layers, samples, and training stages, leading to varying effective dropout rates. In jumpout, the dropout rate is adaptively normalized for consistent neural deactivation across layers and samples during training. The outputs are rescaled to maintain variance, allowing for compatibility between dropout and BN layers in DNN training. Jumpout, similar to dropout, randomly generates a 0/1 mask over hidden neurons without requiring extra training. It can be easily implemented and incorporated into existing architectures with minor modifications. In experiments on various benchmark datasets, jumpout shows similar memory and computation costs as dropout but consistently outperforms it on different tasks. This approach addresses the fixed dropout rate problem and allows for consistent neural deactivation across layers and samples during training. Recent work has proposed different methods to generate adaptive dropout rates, such as \"standout\" and models that learn adaptive dropout rates for different neurons or groups. BID23 also suggested adapting dropout rates based on the Rademacher complexity of the network. Jumpout adjusts dropout rates based on ReLU activation patterns without relying on additional trained models. It introduces minimal computation and memory overhead and can easily be integrated into existing model architectures. BID19 introduced Gaussian dropout as a Gaussian approximation of dropout and proposed optimizing it directly for faster convergence. Variational methods in dropout techniques have been extended to improve gradient estimator variance and achieve sparse dropout rates. Recent variants like Swapout combine dropout with random skipping connections, while Fraternal Dropout trains two identical DNNs with different dropout masks to reduce the gap between training and test phases. Jumpout is a dropout variant that aims to reduce the gap between training and test phases without additional training costs or parameters. It can be used in conjunction with other dropout variants and targets various dropout-related issues. The DNN architecture involves hidden nodes, activation functions, and bias terms, allowing for generalization of various DNN architectures. The network's output prediction is denoted by \u0177(x) in d dimensions. The convolution operator in DNN involves applying filters to input data, resulting in a sparse weight matrix with tied parameters. Average-pooling and max-pooling can be represented as matrix multiplications and activation functions, respectively. Residual network blocks can be represented by appending an identity matrix to retain input values. The DNN with short-cut connections can be written as a piecewise linear function using ReLU activation. The equivalent weight matrix combines the activation pattern with the original weight matrix. The DNN with short-cut connections can be represented as a piecewise linear function using ReLU activation. The weight matrix is modified based on the activation patterns, eventually leading to a linear model without ReLU functions. The gradient \u2202x corresponds to the weight vector of the linear model associated with the activation patterns on all layers for a given input x. The DNN with ReLU activations is represented as a piecewise linear function with activation patterns on all layers for a given input x. ReLU units are computationally efficient and widely used, with dropout improving generalization by considering local linear models and nearby convex polyhedra. Dropout improves the performance of DNNs by preventing co-adaptation of neurons and training a large number of smaller networks through randomly dropping neurons. This is inspired by local linear models and nearby convex polyhedra. Dropout improves generalization performance by training a large number of smaller networks through randomly dropping neurons. This approach treats the network prediction as an ensemble of outputs from these smaller networks, reducing variance. Additionally, dropout smooths each local linear model in a DNN with ReLUs by dividing the input space into convex polyhedra, where the DNN behaves like a linear model within each polyhedron. When using Dropout in large DNNs, the network prediction is treated as an ensemble of outputs from smaller networks created by randomly dropping neurons. This helps in reducing variance and improving generalization performance. The input space is divided into convex polyhedra, where the DNN behaves like a linear model within each polyhedron, especially when the number of neurons is large. Training samples are dispersed among different polyhedra, leading to each data point having its own distinct local linear model. Nearby polyhedra may correspond to different linear models due to consecutively multiplying weight matrices. Given the problems of dropout mentioned in Section 1.1, the proposal is to sample a dropout rate from a truncated half-normal distribution to address issues with the activation patterns of weight matrices in DNNs. This aims to improve the generalization ability of the network and prevent instability on new data. To address issues with dropout in DNNs, a dropout rate is sampled from a truncated half-normal distribution to ensure it is not too small or too large. This helps improve generalization ability and prevent instability on new data. The dropout rate is sampled from a Gaussian distribution and then truncated to specific limits. By utilizing a Gaussian-based dropout rate distribution, smaller dropout rates are sampled with higher probabilities to enhance generalization performance. This method encourages smoothness in the performance of local linear models, ensuring effectiveness on points within closer polyhedra. The dropout rate for each layer is a hyper-parameter that controls smoothness among nearby local linear models. Tuning dropout rates separately for different layers can improve network performance, but it is often computationally expensive. One common approach is to set a global dropout rate for all layers, although this may not be optimal due to variations in the proportion of active neurons. Setting a single global dropout rate for all layers may not be optimal as the proportion of active neurons in each layer varies significantly. To better control the behavior of dropout, it is suggested to adjust the dropout rate for each layer based on the fraction of active neurons. To optimize dropout behavior for different layers and training stages, the dropout rate is normalized by q + j and set as p j = p j /q + j. This approach ensures a consistent hamming distance between activation patterns, allowing for precise tuning of dropout as a single hyper-parameter. In standard dropout, neurons are scaled by 1/p during training and remain unchanged during testing. This scaling maintains consistent neuron means between training and inference phases. The scaling factor 1/p in dropout keeps neuron means consistent between training and test phases, causing incompatibility with batch normalization (BN) due to variance differences. Combining dropout with BN can lead to unpredictable DNN behavior as BN layers cannot adapt to variance changes. The curr_chunk discusses the process of neuron activation in a neural network, including the use of ReLU activation, dropout layers, and weight matrices. It also mentions the impact of dropout on neuron values and their contribution to the next layer. The terms x, w, and y are introduced for simplicity in calculations. The curr_chunk explains how applying a dropout layer affects the mean and variance of neurons during training, leading to inconsistency with the test phase in a neural network. To address this, rescaling the output y is suggested to counteract the impact of dropout on the mean and variance scales. To recover the original scale of the mean and variance after applying dropout in a neural network, rescaling factors need to be considered. The rescaling factor for the mean should be (1 \u2212 p j ) \u22121, while for the variance, it should be (1 \u2212 p j ) \u22120.5 if E(y j ) is small. Taking into account the value of E[w j ] can help scale the undropped neurons, but this requires additional computation and memory cost. The correct scaling factor for the variance of y j is (1 \u2212 p j ) \u22120.5, while for the mean consistency, (1 \u2212 p j ) \u22121 should be used. The rescaling factor for dropout in neural networks should be (1 \u2212 p) \u22120.75 for a good balance between mean and variance adjustments. This factor helps address the shift in both mean and variance, as shown in empirical plots for CIFAR10(s) network. The rescaling factor for dropout in neural networks should be (1 \u2212 p) \u22120.75 to balance mean and variance adjustments efficiently. This factor addresses the shift in both mean and variance, as demonstrated in empirical plots for CIFAR10(s) network. The rescaling factor for dropout in neural networks should be (1 \u2212 p) \u22120.75 to balance mean and variance adjustments efficiently. This factor is demonstrated in empirical plots for CIFAR10(s) network, showing consistency in performance with and without dropout. The rescaling factor for dropout in neural networks should be (1 \u2212 p) \u22120.75 to balance mean and variance adjustments efficiently. Using dropout with batch normalization (BN) can potentially improve performance, with larger dropout leading to more improvement. However, using original dropout with BN results in decreased accuracy when dropout rate exceeds 0.15. In contrast, using rescaled dropout shows continuous improvement with increasing dropout rate (up to 0.25), outperforming other configurations. Proposed improved dropout combines three modifications to address original dropout drawbacks. The proposed improved dropout, called \"Jumpout,\" addresses original dropout drawbacks by sampling from a decreasing distribution for a random dropout rate and normalizing it adaptively based on active neurons. This ensures consistent regularization and generalization effects across layers, training stages, and samples. Jumpout further scales outputs by (1 \u2212 p) \u22120.75 during training to balance mean and variance shifts with batchnorm operations. It requires three hyperparameters: \u03c3 controls the standard deviation, and (p min , p max ) bound samples from the half-normal distribution. Setting p min = 0.01 and p max = 0.6 has shown consistent performance across datasets and models. Jumpout has three hyperparameters, with \u03c3 being the key parameter tuned for good performance. The input hj is considered as the features of layer j for one data point. When using a mini-batch of data points, the average q+j is estimated for the batch to reduce computation and memory usage. Jumpout has similar memory costs as original dropout, with an additional 0/1 drop mask. In this section, dropout and jumpout are applied to various DNN architectures, comparing their performance on benchmark datasets. DNN architectures include a small CNN with four convolutional layers applied to CIFAR10, WideResNet-28-10 applied to CIFAR10 and CIFAR100, and the \"pre-activation\" version of ResNet-20. In 2016, different DNN architectures were applied to various datasets such as CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet. Standard settings, data preprocessing/augmentation, and hyperparameters were followed for CIFAR and Fashion-MNIST experiments. For ImageNet, pre-trained ResNet18 models were used with dropout and jumpout techniques. Starting from pre-trained model weights, jumpout consistently outperforms dropout on all datasets and DNNs tested, even on datasets with high test accuracy like Fashion-MNIST and CIFAR10. Jumpout consistently outperforms dropout on various datasets and DNNs, including Fashion-MNIST, CIFAR10, CIFAR100, and ImageNet. It shows significant improvements even on datasets with high test accuracy. A thorough ablation study confirms the effectiveness of each proposed modification in enhancing the vanilla dropout. Jumpout, with three modifications, outperforms vanilla dropout on different datasets and DNNs. It shows significant improvements, especially in early learning stages, reaching good accuracy faster. Further enhancements are possible with a better learning rate schedule specifically for jumpout. The study compares dropout and jumpout techniques on CIFAR10(s) dataset. Jumpout outperforms dropout in early learning stages, reaching final performance faster. The rescaling factor (1 \u2212 p) \u22120.75 shows a good balance between mean and variance rescaling."
}