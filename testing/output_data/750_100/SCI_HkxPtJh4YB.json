{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for exponential family inference over permutation matrices. This method is validated by the Sinkhorn approximation of the permanent, addressing the intractability issue as permutation size increases. The effectiveness of this approach is demonstrated in the probabilistic identification of neurons in C.elegans. The distribution over permutation matrices is defined using the Frobenius matrix inner product. Marginal inference, computing the matrix of expectations, is known to be intractable due to the computation of the permanent. Sinkhorn variational marginal inference offers an efficient solution to approximate the matrix of expectations. The Sinkhorn operator approximates \u03c1 as S(L), a doubly stochastic matrix. In section 3, the Sinkhorn approximation is shown to produce the best results for probabilistic inference of neural identity in C.elegans. The Sinkhorn operator approximates \u03c1 as S(L), a doubly stochastic matrix, linking marginal inference of \u03c1 L and computation of the permanent Z L through an optimization problem. The Sinkhorn operator approximates \u03c1 as S(L), a doubly stochastic matrix, by replacing the intractable dual function with a component-wise entropy. This approximation is based on a variational representation that links marginal inference of \u03c1 L and computation of the permanent Z L through an optimization problem. The Sinkhorn permanent approximation provides bounds for the normalizing constant. It is a theoretical framework that offers a more structured approach compared to heuristic methods. The Bethe variational inference method is a general rationale for obtaining variational approximations in graphical models. The Sinkhorn approximation provides bounds for the normalizing constant in a structured manner, with better theoretical guarantees than heuristic methods. It has been successfully applied to permutations through belief propagation, offering computational advantages over the Bethe approximation. The Sinkhorn approximation offers structured bounds for the normalizing constant, with theoretical advantages over heuristic methods. Sinkhorn and Bethe iterations differ in computational complexity, with the Bethe approximation showing better permanent approximations in practice. Formulae for these iterations are available in Appendix C, with comparisons to ground truth possible in simple cases. Sinkhorn approximations often outperform Bethe approximations, as shown in figures in the Appendix. The Sinkhorn approximation generally outperforms the Bethe approximation, producing better marginals with more mass on non-zero entries. Sinkhorn also scales better for moderate n, with faster iteration times compared to Bethe. Comparison of the two approximations using submatrices from the C.elegans dataset shows the superiority of Sinkhorn. Examples of a true marginal matrix \u03c1 with Sinkhorn and Bethe approximation. Histogram of log permanent values. Differences between approximate and true log permanent, mean absolute errors of log marginals. Sampling-based methods for marginal inference, with limitations of sophisticated samplers for permanent approximability. Recent advances in neurotechnology have enabled whole brain imaging of the worm C.elegans, a species with a stereotypical nervous system. The number of neurons (roughly 300) and connections between them remain unchanged from animal to animal. However, a technical problem must be solved before studying how brain activity relates to behavior: producing sensible marginal inferences from volumetric images of worm neurons. Our methodology aims to assign canonical labels to neurons in volumetric images of C.elegans, specifically in the context of NeuroPAL. The goal is to estimate probabilities of neuron identification with canonical identities, providing uncertainty estimates for the model. Our methodology assigns canonical labels to neurons in C.elegans images, estimating probabilities of neuron identification with uncertainty estimates. A gaussian model is used for each canonical neuron, with parameters inferred from annotated worms. The likelihood of observing data is calculated, inducing a posterior over P with a flat prior assumption. NeuroPAL assigns canonical labels to C.elegans neurons with uncertainty estimates. A human labels neurons with uncertain model estimates, resolving uncertainty and improving identification accuracy. The model update led to increased identification accuracy for neurons with fewer annotations needed. Comparison with simple baselines showed faster accuracy improvement. Various alternatives were considered, including Sinkhorn and Bethe approximations, MCMC, and different baseline methods for uncertainty estimation. Further details can be found in Fig 3 and the Appendix. The Sinkhorn and Bethe approximations show similar results, with Sinkhorn slightly outperforming Bethe. Both are significantly better than any baseline except for the oracle. In contrast, MCMC does not perform better than the naive baseline, indicating a lack of convergence for chain lengths. The Sinkhorn approximation for marginal inference is a faster and more accurate alternative to sampling compared to the Bethe approximation. Future work will analyze the relationship between permanent approximation quality and corresponding marginals. Additionally, S(L) = diag(x)Ldiag(y), where diag(x) and diag(y) are positive vectors turned into diagonal matrices. The (log) Sinkhorn approximation of the permanent of L, perm S (L), is obtained by evaluating S(L) in the problem it solves. A dataset of ten NeuroPAL worm heads with human labels and a log-likelihood matrix L is used for analysis. The Sinkhorn approximation is compared to the Bethe approximation for marginal inference. The Sinkhorn and Bethe approximations were compared using 200 iterations each, with results showing convergence. The MCMC sampler method by Diaconis (2009) was used with 100 chains of length 1000. Results were obtained on a desktop computer with an Intel Xeon W-2125 processor. The message passing algorithm described in (Vontobel, 2013, Lemma 29) was implemented in log-space with the Bethe approximation. The parameter eps was introduced for numerical stability. 1000 submatrices of size n were randomly drawn from ten available log likelihood C.elegans matrices. Error bars were too small to be noticed."
}