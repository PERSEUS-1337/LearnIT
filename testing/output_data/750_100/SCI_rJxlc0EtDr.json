{
    "title": "rJxlc0EtDr",
    "content": "Recent research on neural network architectures with external memory has focused on the bAbI question and answering dataset, which presents challenging tasks requiring reasoning. To further investigate the reasoning capacity of these architectures, a classic associative inference task from human neuroscience literature was employed. This task aims to test the ability to understand distant relationships among elements distributed across multiple facts or memories. However, current architectures struggle to reason over long distance associations, as shown in the study. MEMO is a novel architecture designed to reason over longer distances by introducing a separation between memories and facts stored in external memory. It also utilizes an adaptive retrieval mechanism, allowing for a variable number of 'memory hops' before producing an answer. MEMO can solve novel reasoning tasks and all 20 tasks in bAbI. MEMO is an architecture that can reason over longer distances by separating memories and facts in external memory. It uses an adaptive retrieval mechanism for multiple 'memory hops' before providing an answer. This allows for solving novel reasoning tasks and all 20 tasks in bAbI. In everyday life, we make judgments by connecting facts acquired at different times, known as inferential reasoning. Inferential reasoning involves using single experiences to infer relationships, supported by the hippocampus. Memories are stored independently through pattern separation to minimize interference and allow for recall of specific events. This conflicts with the hippocampus's role in generalization. The hippocampus plays a role in generalization by integrating separated memories at retrieval, allowing for inference. Research shows that this integration occurs through a recurrent mechanism, enabling interaction between pattern separated codes to support inference. This paper explores how neuroscience models can enhance inferential reasoning in neural networks, particularly those augmented with external memory like the Differential Neural Computer. Neural networks with external memory, such as the Differential Neural Computer and end-to-end memory networks, have shown impressive abilities in handling complex computational tasks. Recent advancements in attention mechanisms and context utilization have enabled traditional neural networks to tackle similar tasks. However, some tasks like bAbI present challenges with repetitive patterns that neural networks can exploit for degenerate solutions. To address this issue, a new task called Paired was introduced. To address challenges with degenerate solutions, a new task called Paired Associative Inference (PAI) was introduced, derived from neuroscientific literature. PAI aims to capture inferential reasoning by forcing neural networks to learn abstractions to solve unseen associations. This task is followed by finding the shortest path and bAbi to investigate effective memory representations. MEMO is a new approach that retains all facts in memory and utilizes a linear projection with a recurrent attention mechanism for greater flexibility in memory usage. This contrasts with other models that use fixed memory representations based on word embeddings and positional encoding. MEMO is based on the external memory structure presented in EMN but with new components for flexible weighting of individual elements in memory. It addresses the issue of prohibitive computation time in neural networks by adjusting input values for efficient computation. In addressing computation time issues in neural networks, Graves et al. (2016) propose adapting compute time based on task complexity. They draw inspiration from REMERGE, a model of human associative memory, to recirculate retrieved content as new queries and determine network convergence. The network uses a process to determine if it has reached a fixed point, inspired by adaptive computation time techniques. It outputs a halting policy to decide whether to continue computing or terminate, learning the termination criteria through reinforcement learning. Our network uses reinforcement learning to adjust weights based on the optimal number of computation steps. By adding an extra term to the REINFORCE loss, we minimize the expected number of computation steps, encouraging the network to prefer representations that minimize computation. Our contributions include a new task emphasizing reasoning, an investigation into memory representations for inferential reasoning, extensions to memory architectures, and a REINFORCE loss component for learning optimal iterations. Empirical results show effectiveness on three tasks. The curr_chunk discusses the tasks of paired associative inference, shortest path finding, and bAbI, focusing on the architecture of End-to-End Memory Networks and the setup for predicting answers based on knowledge inputs and queries. The curr_chunk explains the architecture of End-to-End Memory Networks (EMN) for tasks like paired associative inference and shortest path finding. It involves encoding input sequences, word embeddings, and positional encodings to predict answers based on knowledge inputs and queries. End-to-End Memory Networks (EMN) involve element wise multiplication to calculate weights over memory elements and produce outputs. EMN is trained via cross entropy loss on the final step. On the other hand, MEMO embeds input differently by deriving a common embedding for each input matrix. MEMO derives a common embedding for each input matrix, adapting them to be keys or values without using hand-coded positional embeddings. Multiple heads are used to attend to the memory, allowing flexibility in capturing different parts of the input sentence. MEMO uses multiple heads to attend to memory, adapting input matrices to keys or values without hand-coded positional embeddings. Each head has a different view of common inputs, allowing for flexibility in capturing different parts of the input sentence. The attention mechanism used by MEMO in multiple heads differs from EMN, utilizing DropOut and LayerNorm for generalization and learning dynamics. Matrices K(h), V(h), and Qt are transformed by matrices Wh and Wq for the attention mechanism. The attention mechanism in MEMO uses matrices for transforming logits and queries, with an output MLP producing the answer. It differs from Vaswani et al. (2017) by preserving queries separate from keys and values, leading to linear computational complexity. MEMO uses an attention mechanism with linear computational complexity, unlike methods with self-attention which have quadratic complexity. The number of computational steps required to answer a query is learned by collecting information at each step, processing it with GRUs and an MLP, and creating an observation s t. The network input s t is formed by the Bhattacharyya distance. The network input to MEMO is formed by the Bhattacharyya distance between attention weights of current and previous time steps, along with the number of steps taken so far as a one-hot vector. The network is trained using REINFORCE and adjusts parameters using n-step look ahead values. The network input to MEMO includes the Bhattacharyya distance between attention weights of current and previous time steps, along with the number of steps taken as a one-hot vector. Parameters are adjusted using n-step look ahead values, with the objective function aiming to minimize the expected number of hops by introducing a new term in the loss function, L Hop. The reward structure in training discrete random variables is defined by the target answer a and the prediction \u00e2 from the network. The variance for a binary halting random variable is bounded by 1/4, which is not too large for successful learning. The final layer of M LP R was initialized with bias init to increase the chances of producing a probability of 1. A maximum number of hops, N, was set for the network, and if reached, no additional hops were performed. There was no gradient sharing between the hop network and the main MEMO network. The Differential Neural Computer (DNC) is an influential model that operates sequentially on inputs, learning to read and write to a memory store. It has proven capable of solving algorithmic problems but has difficulty scaling to higher-dimensional domains. An extension incorporating sparsity has allowed the DNC to perform well on larger tasks like the bAbI task suite. Several memory-augmented architectures have been developed as alternatives to the Differential Neural Computer (DNC), including the Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet. These models share similarities with the DNC but operate on sequential inputs, use a parallel architecture for simultaneous updates across memory locations, incorporate a working memory buffer, and include additional components for improved performance. The RelationNet enables relational reasoning over memory contents and has shown good performance on tasks like the bAbI task suite. Adaptive Computation Time (ACT) adjusts computational budget based on task complexity by learning a halting probability called \"ponder time\". The \"ponder time\" in Adaptive Computation Time (ACT) dynamically modulates computational steps. Other approaches include Adaptive Early Exit Networks and using REINFORCE to adjust computation steps in recurrent neural networks. The REINFORCE algorithm has been used to optimize the number of steps in sequence processing. This technique has also been applied to recurrent neural networks and neural networks with external memory. Our method introduces the idea of using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory. Graph Neural Networks (GNNs) involve iterative message passing to propagate embeddings in a graph for various learning tasks. The process is similar to attention mechanisms and aims to determine if more information can be retrieved from memory. Our work differs from Graph Neural Networks (GNNs) in two fundamental ways: GNNs involve iterative message passing, while our method performs adaptive computation to modulate the number of message passing steps. Our model does not require message passing between memories - input queries attend directly to the memory slots. One contribution of this paper is to introduce a task, derived from neuroscience, to probe the reasoning capacity of neural networks by formalizing a prototypical task called paired associative inference (PAI). This task aims to capture the essence of reasoning by appreciating distant relationships among elements distributed across multiple facts or memories. The PAI task involves associating images to study hippocampus role in generalization. Two images are paired together, and later a new image is introduced to test memory recall. The PAI task involves testing episodic memory with direct and indirect queries. Direct queries rely on retrieving a specific episode, while indirect queries require inference across multiple episodes. The network is presented with a cue, image A, and two choices: the match, originally paired with B, or the lure, forming a different triplet A-B-C. The correct answer requires linking A and C because they were both paired with B. The study compared MEMO with other memory-augmented architectures like EMN, DNC, and UT. Results are summarized in Table 1, with details in the appendix. The task involves linking A and C, as they were both paired with B. MEMO and other memory-augmented architectures were compared on the hardest inference queries for PAI tasks. MEMO and DNC achieved the highest accuracy on the smaller set, while EMN and UT struggled. MEMO was the only architecture successful on longer sequences. Further analysis on a length 3 task showed DNC required 10 pondering steps to solve it. MEMO and DNC were compared on a length 3 PAI task. DNC needed 10 pondering steps for accuracy, while MEMO converged to 3 hops. Analysis of MEMO's approach included attention weights for an inference query to associate CUE with MATCH and avoid LURE interference. The original sequence A-B-C had class IDs 611-191-840. In the memory retrieval task, MEMO successfully activated slots containing associations A-B and B-C in multiple hops, assigning appropriate probability masses to support correct inference decisions. The activation sequence in MEMO for memory retrieval tasks is similar to computational models of the hippocampus and observed in neural data. The number of hops taken by the network influences the algorithm used for solving inference problems. This could be linked to knowledge distillation in neural networks. The number of hops in neural networks influences the algorithm for solving inference problems. Ablation experiments on MEMO confirmed that specific memory representations and recurrent attention mechanism are crucial for successful inference. This conclusion only applies to inference queries, not direct queries. The number of hops in neural networks affects solving inference problems. Ablation experiments on MEMO showed the importance of memory representations and attention for successful inference, not for direct queries. Our adaptive computation mechanism outperformed ACT in data efficiency for this task. The lure is an image presented in the same memory store, associated with a different sequence. The weights of the 3 hops used by the network are reported. Synthetic reasoning experiments on randomly generated graphs show the accuracy of models in finding the shortest path between nodes. DNC, Universal Transformer, and MEMO had perfect accuracy on a small graph with 10 nodes and path length of 2. MEMO outperformed EMN and DNC in predicting nodes on more complicated graphs with high connectivity, showing great scalability by considering more paths as the number of hops increases. Universal Transformer had different performance in predicting the first node. The Universal Transformer showed varied performance in predicting nodes on different paths, achieving slightly lower results than MEMO for the second node of the shortest path. Test results for MEMO's best hyper-parameters are reported, along with results from the best run for EMN, UT, and DNC. Additionally, the study includes training details on the bAbI question answering dataset. In the 10k training regime, MEMO achieved high accuracy on all tasks, outperforming other baselines with lower error rates. Ablation experiments were conducted to analyze the impact of different architectural components on performance. Refer to the appendices for detailed training specifics and results. In an in-depth investigation, the combination of memory representations and recurrent attention was crucial for achieving state-of-the-art performance on the bAbI task. The use of layernorm in the recurrent attention mechanism led to a more stable training regime and improved performance. Test results for the best run on the bAbI task are compared with DNC and Universal Transformer results. In an in-depth investigation, MEMO, an extension to existing memory architectures, showed promising results on inferential reasoning tasks. It outperformed other architectures on paired associative inference and graph traversal tasks, as well as matched the performance of state-of-the-art results on the bAbI dataset. Our analysis supported the hypothesis that flexible weighting of individual elements in memory, combined with a recurrent attention mechanism, achieved state-of-the-art results. The task involved creating three distinct datasets from the ImageNet dataset, embedding images using a pre-trained ResNet, and generating sequences of length three and four. Three distinct datasets were generated with sequences of length three, four, and five items. Each dataset contained 1e6 training images, 1e5 evaluation images, and 2e5 testing images. Batches were built with memory, query, and target entries by selecting N sequences from the pool, with N = 16. Memory content was created with pair wise associations between items in the sequence. Memory was created with pair wise associations between items in the sequence, resulting in 32 rows for S = 3. Queries consisted of 3 images: cue, match, and lure, with two types - 'direct' and 'indirect'. In 'direct' queries, cue and match are in the same memory slot, no inference required. The network is tested with direct and indirect queries, where direct queries test episodic memory by retrieving a specific episode, while indirect queries require inference across multiple episodes. The queries consist of three image embedding vectors (cue, match, lure) presented in a randomized order to avoid degenerate solutions. The position of the match and lure images is randomized to avoid degenerate solutions. The lure image always has the same position in the sequence but is drawn from a different sequence in memory. The task requires appreciating the correct connection between images and avoiding interference from other memory items. All possible queries supported by the current memory store are generated for each batch entry, with one selected at random. The batch is balanced with half direct queries and half indirect queries. The batch is balanced with half direct queries and half indirect queries. Longer sequences provide more 'direct' queries and multiple 'indirect' queries that require different levels of inference. The targets for prediction are the class of the matches, with some trials requiring more inference steps to appreciate overlapping images. The evaluation set contains 600 items with images of the entire sequence. Different models use memory and query inputs in their architecture, with DNC and UT embedding stories and query similarly to MEMO. The output of the UT model is used as the final prediction. Graph generation is also mentioned in the context. Graph generation for shortest path experiments involves generating graphs by sampling two-dimensional points from a unit square to represent nodes. Each node has K nearest neighbors as outbound connections. The task is represented by a graph description, query, and target in the evaluation set of 600 items at the end of training. The graph description includes tuples of integers representing connections between nodes, with source and destination nodes. Queries and target paths are also represented as tuples of integers. During training, a mini-batch of 64 graphs, queries, and target paths are sampled. Queries are a matrix of size 64 \u00d7 2, targets are of size 64 \u00d7 (L \u2212 1), and graph descriptions are of size 64 \u00d7 M \u00d7 2, where L is the length of the shortest path. In experiments, the graph description size is 64 \u00d7 M \u00d7 2, where M is the maximum number of nodes allowed. Networks are trained for 2e4 epochs with 100 batch updates each. For EMN and MEMO, the graph description is the memory contents, and the query is used as input. Keys are kept fixed to answer the target sequence of nodes with varying numbers of hops predicted by the model. The model predicts answers for nodes sequentially, using the predicted answer for the first node as the query for the second node in MEMO, while using the ground truth answer for EMN. The weights for each answer are not shared, and for the Universal Transformer, the query and graph description are embedded as in EMN. For the Universal Transformer and DNC models, the query and graph description are embedded similarly to EMN and MEMO. The encoder of the UT architecture is used to process the concatenated embeddings for the answer. In DNC, the graph description tuples are presented before the query tuple due to its sequential nature. The weights for each answer are not shared in both models. The models are trained using Adam with cross-entropy loss. Evaluation is done on a batch of 600 graph descriptions, queries, and targets to calculate mean accuracy. DNC and UT have a 'global view' on the problem during training. During training, DNC and UT have a 'global view' on the problem, allowing them to reason and work backwards from the end node to achieve better performance. In contrast, MEMO has a 'local view' where the answer to the second node depends on the answer to the first node, making it unable to perform better than chance if the first node's answer is incorrect. When comparing MEMO and EMN models, MEMO performs better when the ground truth answer of the first node is used as the query for the second node, compared to using the predicted answer. In the case of 20 Nodes with 5 outbound edges, MEMO's performance increases to 85.38% when using the ground truth, while EMN's performance remains at 69.20%. When using the same training regime as MEMO, EMN performs at chance level (22.30%). Results are consistent in a simpler scenario with 20 nodes and 3 outbound edges using the English Question Answer dataset. Text is pre-processed by converting to lowercase, ignoring punctuation, and treating commas as word separators. At training time, a mini-batch of 128 queries and corresponding stories are sampled from the dataset. Queries are a matrix of 128 \u00d7 11 tokens, and sentences are of size 128 \u00d7 320 \u00d7 11. Each answer has its own independent label, and all questions are separated from the text. At training time, a mini-batch of 128 queries and stories are used. The batch size is 128, with a max of 320 stories and 11 sentences. Padding with zeros is applied to queries and stories that do not reach the max size. Different models like EMN, MEMO, DNC, and UT handle stories and queries in various ways within their architectures. For training, a mini-batch of 128 queries and stories is used with padding applied to reach the max size. Different models like EMN, MEMO, DNC, and UT handle stories and queries in various ways within their architectures. The output of the model is used after sampling a mini-batch, followed by optimization using Adam with specific hyperparameters. A time encoding column vector is added to the memory store to account for temporal context. Networks are trained for 2e4 epochs with 100 batch updates each. Evaluation involves sampling a batch of 10,000 elements from the dataset and computing the forward pass. During training, various models like EMN, MEMO, DNC, and UT handle stories and queries differently within their architectures. The output is optimized using Adam with specific hyperparameters. Evaluation involves sampling a batch of 10,000 elements from the dataset and computing the forward pass to compute mean accuracy and accuracy per task for each of the 20 tasks of bAbI. The models are trained with a cross-entropy loss function to predict class ID, node ID, or word ID depending on the task. The halting policy network parameters were updated using RMSProp with learning rate l halt. MEMO has a temporal complexity of O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where n s is the number of samples processed, A is the number of answers, N is the number of hops, H is the number of heads used, I is the number of stories, and S is the number of words in each sentence. MEMO has linear temporal complexity and spatial complexity of O(I \u00b7 S \u00b7 d) with fixed parameters for all experiments. In contrast, the Universal Transformer has quadratic complexity. The spatial complexity of MEMO is dependent on the size of memory used for context information. In all experiments, the spatial complexity of MEMO is O(I \u00b7 S \u00b7 d) with fixed parameters. The implementation of ACT follows Graves (2016) and defines the halting unit h differently from the original ACT, using non-linearities for fairness in comparison. The implementation of ACT in MEMO+ACT uses non-linearities for fairness in comparison with the original ACT. The halting probability is defined, and the architecture follows Graves et al. (2016) with specific layer sizes detailed in Table 13. The architecture in MEMO+ACT uses non-linearities and follows Graves et al. (2016) with specific layer sizes detailed in Table 13. The hyperparameters used are based on the 'universal_transformer_small' implementation available at https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py, with details in Table 15. Hyperparameters were also searched for training on specific tasks."
}