{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs is introduced using the Wasserstein-2 metric proximal on the generators. The approach utilizes the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experimental results show improved speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. Generative Adversarial Networks (GANs) involve a discriminator distinguishing real and generated data, while the generator aims to deceive the discriminator. The adversarial game in Generative Adversarial Networks (GANs) involves a generator trying to fool a discriminator by recreating the density distribution from the real source through an implicit generative model. The problem of matching a target density is minimized using a discrepancy measure like the Kullback-Leibler (KL) divergence, which can be challenging with low dimensional support sets. Optimal transport, specifically the Wasserstein distance, is an alternative approach to defining a discrepancy measure between densities in applications with structured data and high dimensional sample spaces. It has been used in defining loss functions for generative models like the Wasserstein GAN, which has gained significant interest. Additionally, optimal transport can introduce structures for optimization, such as the Wasserstein steepest descent flow in full probability space. In this paper, the Wasserstein steepest descent flow is derived for deep generative models in GANs using the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. The Fisher-Rao natural gradient, induced by the KL divergence, is often advantageous in learning problems compared to the Euclidean gradient. However, in GANs, the low dimensional support sets and difficulties with the KL divergence make the Fisher-Rao natural gradient problematic. The proposed solution is to use the gradient. The Fisher-Rao natural gradient is problematic, so we propose using the gradient operator induced by the Wasserstein-2 metric BID21. The proximal operator for GAN generators is computed with regularization as the squared constrained Wasserstein-2 distance, which can be approximated by a neural network. The constrained Wasserstein-2 metric in implicit generative models has a simple structure. We introduce a relaxed proximal operator for generators to simplify computation, involving only the difference of outputs. The paper introduces a Wasserstein proximal method for GAN generators, with simple parameter updates. It demonstrates the method's effectiveness in various GAN experiments and reviews related work on optimal transport and its application in GAN optimization. The paper focuses on the Wasserstein-2 distance in GAN optimization, defining it as a trajectory minimizing kinetic energy to transport probability densities. The classic theory of minimal kinetic energy is extended to cover parameterized density models, where the density path is constrained within a parametrized model. The Wasserstein-2 metric function constrained to the parameter space is defined, involving feasible Borel potential functions. The infimum is found among feasible Borel potential functions and continuous parameter paths. The constrained metric on parameter space differs from the Wasserstein-2 distance on the full density set. The metric d W is used for steepest descent optimization, formulated in terms of Riemannian structure. The constrained Wasserstein-2 metric allows for a Riemannian metric structure to be obtained. The Wasserstein natural gradient is obtained from a Riemannian metric structure. The gradient operator is defined for a loss function F on parameter space \u0398. The steepest descent flow is determined by the natural gradient operator with respect to the constrained Wasserstein metric. The steepest descent flow in the Wasserstein Riemannian metric is given by a gradient descent iteration with a step size h. The computation of matrix G(\u03b8) \u22121 can be challenging, leading to the use of the proximal operator as a numerical scheme. The backward Euler method, also known as the Jordan-Kinderlehrer-Otto (JKO) scheme, involves a regularization of the original loss function with a distance parameter update. The approximation of the d W distance locally by a second order method is often used. The Semi-Backward Euler method for the gradient flow of loss function F in the parameterized setting allows for easier approximation without computing and inverting G(\u03b8). The Semi-Backward Euler method simplifies the gradient flow computation by avoiding the need to compute and invert G(\u03b8). It is implemented in implicit generative models by mapping noise prior Z to output samples with density X. This method is more tractable than the backward Euler method due to the constrained optimization over \u03a6. In this case, the update in Proposition 3 leads to a simpler formulation of the constrained Wasserstein-2 metric in implicit generative models. This reformulation defines the relaxed Wasserstein metric and introduces a simple algorithm for the proximal operator on generators. Proposition 4 states the requirements for the constrained Wasserstein-2 metric in implicit generative models. The constrained Wasserstein metric requires the generator's derivative to be a gradient vector field of another function. The gradient constraint is satisfied when the sample space is 1 dimensional, but not in general. Finding this function involves computational difficulties, making it an open problem for Wasserstein proximal operator computations. To simplify computations, a relaxed Wasserstein metric on the parameter space is considered. The relaxed Wasserstein proximal operator approximates the Wasserstein proximal based on a new metric, regularizing the generator in high-dimensional sample spaces. The algorithm requires a parameterized function to minimize, a generator, proximal step-size, batch size, and optimizers. The effectiveness of Wasserstein proximal operator in GANs is illustrated using a toy example with a family of distributions. The proximal regularization for a loss function F is defined, and various statistical distance functions between parameters are checked. The effectiveness of Wasserstein proximal operator in GANs is demonstrated with a toy example using different statistical distance functions between parameters. The Euclidean and Wasserstein-2 distances are effective, while the KL divergence and L2-distance are not suitable for measuring the difference in probability models. The Euclidean distance is model-independent, unlike the constrained Wasserstein-2 metric. The Wasserstein proximal operator outperforms the Euclidean proximal in decreasing the objective function. Numerical experiments show that the Relaxed Wasserstein Proximal algorithm provides faster speed and stability in training GANs compared to the Semi-Backward Euler method. The Relaxed Wasserstein Proximal (RWP) algorithm improves speed and convergence of GAN training by applying regularization on the generator. It introduces two hyperparameters: the proximal step-size h and the number of iterations. This approach is novel as most GAN training focuses on regularizing the discriminator. The Relaxed Wasserstein Proximal (RWP) algorithm introduces hyperparameters for GAN training: the proximal step-size and number of iterations. It tests this regularization on Standard GANs, WGAN-GP, and DRAGAN using CIFAR-10 and CelebA datasets with DCGAN architecture. Quality is measured using Fr\u00e9chet Inception Distance (FID). The Relaxed Wasserstein Proximal (RWP) algorithm introduces hyperparameters for GAN training to improve speed and stability of convergence. Quality is measured using Fr\u00e9chet Inception Distance (FID) with 10,000 generated images for CIFAR-10 and CelebA datasets. Hyperparameter choices for training are provided in Appendix C. Proximal regularization improves convergence speed and stability, aligning comparisons based on wallclock time. Results show lower FID for all GAN types, with a 20% improvement in sample quality for DRAGAN. Similar results are observed for the CelebA dataset. Multiple generator iterations may hinder Standard GANs on CelebA initially, requiring algorithm restarts. The study focuses on the challenges faced by Standard GANs on CelebA dataset, with multiple generator updates hindering initial learning. The potential solution lies in using a more stable loss function like WGAN-GP or adjusting the parameters. Using the stable WGAN-GP type, omitting regularization leads to high FID variance. However, with RWP regularization, FID converges more stably and achieves a lower FID. Latent space walks show RWP does not cause memorization. RWP improves speed and lowers FID, as seen in the provided samples. The experiment demonstrates the impact of 10 generator iterations per outer-iteration with and without RWP regularization. With RWP, convergence and lower FID are achieved, while without it, training is highly variable with FID increasing towards the end. The experiment compares training with and without RWP regularization on the CIFAR-10 dataset using the Semi-Backward Euler method. The training is comparable to standard WGAN-GP loss training. The algorithm and hyperparameter settings are detailed in the appendix. Optimization over three networks is presented in FIG3. In Section G, optimization over three networks in FIG3 is discussed. The Semi-Backward Euler method is compared to norm WGAN-GP, with similar generator iterations. The use of Wasserstein distance as a loss function in optimal transport and GANs is highlighted in the literature. Further investigation into the Semi-Backward Euler method is suggested for future research. The Wasserstein distance is a statistical metric used for comparing probability distributions supported on lower dimensional sets. It is leveraged in Wasserstein GANs, where the loss function is the Wasserstein-1 distance. The discriminator must satisfy the 1-Lipschitz condition in its computations. The Kantorovich dual variable must satisfy the 1-Lipschitz condition. Regularization of the discriminator is done to meet this requirement. The Wasserstein-2 metric creates a metric tensor structure, forming an infinite dimensional Riemannian manifold called the density manifold. The gradient flow in the density manifold is linked to transport-related partial differential equations, such as the Fokker-Planck equation. The gradient flow of the KL divergence function is studied in learning communities. Different approaches include leveraging gradient flow structure in probability space and exploring nonparametric models like the Stein gradient descent method. Approximate inference methods for computing Wasserstein gradient flow are also considered, introducing an approximation towards Kantorovich dual variables. In this work, the constrained Wasserstein gradient and its relaxations are applied to implicit generative models, focusing on general implicit generative models rather than specific Gaussian families or elliptical distributions. The focus of this work is on regularizing the generator in Wasserstein GANs, specifically computing the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space. The proposed method leads to better minimization results in terms of FID and faster convergence speeds. The metric function W2 is considered in the full probability set, involving infimum among feasible Borel potential functions and continuous density paths satisfying the continuity equation. The variational formulation introduces a Riemannian structure in density space, with smooth and strictly positive probability densities. The tangent space of the set of densities is defined, and the elliptic operator identifies functions modulo additive constants. The inner product endows the space with a Riemannian metric tensor. The Wasserstein gradient operator in (P + , g W ) is defined for a loss function F : P + \u2192 R. The gradient flow satisfies analytical results provided in BID3. The Wasserstein-2 metric and gradient operator are then constrained on statistical models defined by a triplet (\u0398, R n , \u03c1), where \u0398 \u2282 R d and \u03c1 : \u0398 \u2192 P(R n ). The parameterization function \u03c1(\u0398) \u2282 P(R n ) is assumed to be locally injective and under suitable regularities. A Riemannian metric g is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The Wasserstein statistical manifold is defined for \u03b8 \u2208 \u0398 and \u03c3 i \u2208 T \u03b8 \u0398. The metric tensor G(\u03b8) is smooth and positive definite, forming a smooth Riemannian manifold (\u0398, g \u03b8 ). The constrained Wasserstein gradient operator in parameter space is studied in Theorem 2. The constrained Wasserstein gradient operator in parameter space is studied in 2 studies. The distance d W can be expressed in the action function on the Wasserstein statistical manifold. The gradient operator on a Riemannian manifold is defined, and the derivation of the proposed semi-backward method is presented. The proof of a claim involving reparameterizing the time of a geodesic path is also provided. The proof of Proposition 4 involves reparameterizing the time of a geodesic path and deriving a consistent numerical method known as the Semi-backward method. This result is proven in a previous study and is presented here. Proof of Proposition 4 involves reparameterizing time of a geodesic path and deriving a consistent numerical method called the Semi-backward method. The implicit model is defined by a push-forward relation, with a gradient constraint. The probability density transition equation of g(\u03b8(t), z) satisfies the constrained continuity equation. The proof involves considering different equations and operators related to the push forward relation. The proof involves equations 9, 10, 11, 12, and 13, showing the explicit computation of the Wasserstein and Euclidean proximal operators. The push forward operator is defined, leading to the derivation of equation 10. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 include a batch size of 64 for all experiments. For CIFAR-10 with WGAN-GP, the Adam optimizer with specific parameters was used, while for CIFAR-10 with Standard and DRAGAN, different optimizer settings were applied. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 include a batch size of 64 for all experiments. For CIFAR-10 with WGAN-GP, the Adam optimizer with specific parameters was used, while for CIFAR-10 with Standard and DRAGAN, different optimizer settings were applied. The Adam optimizer with learning rate 0.0002, \u03b21 = 0.5, and \u03b22 = 0.999 was used for both the generator and discriminator in the experiments. A latent space dimension of 100, h = 0.2, and 5 generator iterations were also utilized. The Relaxed Wasserstein Proximal is designed as an easy-to-implement regularization method. The algorithm for training GANs with Relaxed Wasserstein Proximal involves updating the discriminator, performing Adam gradient descent, and repeating the process until a stopping condition is met. The key differences lie in the terms involving the generator and the number of generator iterations. In this paper, a Standard GAN with RWP regularization and WGAN-GP with RWP were trained on CelebA and CIFAR-10 datasets, respectively. FID scores were 17.105 and 38.3 for the generated images. Latent space walk analysis showed smooth transitions, indicating no memorization in GANs with RWP regularization. The hyperparameter settings for the Semi-Backward Euler (SBE) on WGAN-GP trained on CIFAR-10 included a batch size of 64, DCGAN architecture for discriminator and generator, Adam optimizer with specific parameters, latent space dimension of 100, and updates for discriminator, generator, and potential in each outer-iteration loop. The hyperparameters for training WGAN-GP on CIFAR-10 included a batch size of 64, DCGAN architecture, Adam optimizer, latent space dimension of 100, and updates for discriminator, generator, and potential in each outer-iteration loop. The generator and potential were updated 5 times each in an outer-iteration loop."
}