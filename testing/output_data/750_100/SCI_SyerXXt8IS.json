{
    "title": "SyerXXt8IS",
    "content": "Auto-generating enhanced input features for ML methods with limited training data. Biological neural nets (BNNs) excel at fast learning, with the insect olfactory network rapidly learning new odors through competitive inhibition, sparse connectivity, and Hebbian updates. MothNet, a computational model of the moth olfactory network, generates new features for use by standard ML classifiers, resulting in improved performance. The \"insect cyborgs\" combining BNN and ML methods outperform baseline ML methods on MNIST and Omniglot data sets, reducing test set errors by 20% to 55%. MothNet feature generator surpasses PCA, PLS, and NNs, showcasing the value of BNN-inspired feature generators in ML. Limited data is a common constraint for ML targets, hindering deployment and problem-solving capabilities. The curr_chunk discusses improving machine learning methods' ability to learn from limited data by automatically generating new class-separating features inspired by biological neural networks. It mentions the insect olfactory network as a simple BNN that can learn rapidly. The insect olfactory network, including the Antennal Lobe and Mushroom Body, can rapidly learn new odors with just a few exposures. It features competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism. The MothNet computational model demonstrated superior performance in learning vectorized MNIST digits with limited training samples per class. The model includes competitive inhibition in the Antennal Lobe (AL), sparsity in the Mushroom Body (MB), and weight updates only affecting MB\u2192Readout connections. Hebbian updates occur in the model. The MothNet architecture serves as a front-end feature generator for an ML classifier by combining it with a downstream ML module. The AL-MB model acts as an automatic feature generator for the ML module. The MothNet architecture acts as an automatic feature generator for ML classifiers, replacing the downstream processing in complex BNNs. In a non-spatial dataset, Mothnet Readouts significantly improved the accuracies of ML methods like NN, SVM, and Nearest Neighbors by encoding class-relevant information in a form accessible to the ML methods. The MothNet architecture significantly outperformed other methods in generating features for ML accuracy. vMNIST, created from downsampling and preprocessing MNIST data, provided advantages for baseline ML methods. The MothNet model outperformed other ML methods in feature generation. Full details and code for the MothNet experiments can be found in references [11] and [12]. The experiments involved comparing Cyborg vs baseline ML methods on vMNIST data with controlled training sample sizes. The MothNet model, trained on vMNIST samples using stochastic differential equation simulations, outperformed other ML methods in feature generation. Trained ML accuracies of baseline methods were compared with \"insect cyborgs\" that incorporated MothNet features, showing significant gains. In vMNIST experiments, feature generators like PCA, PLS, and NN were used as alternatives to MothNet. PLS was expected to outperform PCA due to its incorporation of class information. CNNs were not used as vMNIST lacks spatial content. In vMNIST experiments, feature generators like PCA, PLS, and NN were used as alternatives to MothNet. NN with weights initialized by training on an 85-feature vectorized Omniglot data set, then trained on vMNIST data as usual. Including two hidden layers did not improve baseline performance, showing that MothNet features were not equivalent to just adding an extra layer to a NN. MothNet readouts as features significantly improved accuracy of ML methods. MothNet architecture effectively captured new class-relevant features, outperforming PCA, PLS, and NN feature generators. MothNet features improved vMNIST ML baseline test set accuracies by 10% to 88%. MothNet features significantly increased accuracy across all ML models, with a relative reduction in test set error of 20% to 55%. NN models benefited the most, showing a 40% to 55% reduction in test error. Even when ML baseline accuracy exceeded MothNet's 75% ceiling, MothNet front-end still improved accuracy by providing clustering information. Gains were observed in almost all cases with N > 3. The cyborg framework using MothNet features significantly improved accuracy across all ML models, with gains observed in almost all cases with N > 3. MothNet features were more effective than PCA, PLS, NN, Nearest Neighbors, and SVM methods. The MothNet architecture, with a competitive inhibition layer (AL) and a high-dimensional sparse layer (MB), showed significant improvements in accuracy when using a pass-through AL. The gains were between 60% and 100% of those with normal ALs, indicating the importance of the trainable layer (MB) while also highlighting the value added by the competitive inhibition of the AL layer. The AL layer's competitive inhibition added up to 40% value in generating strong features, benefiting NNs the most. An automated feature generator based on a simple BNN with competitive inhibition, sparse projection, and Hebbian weight updates significantly improved learning abilities of standard ML methods on vMNIST and vOmniglot datasets. This bio-mimetic feature generator extracted class-relevant information not captured by ML methods alone. The MothNet's pre-processing made additional information in raw feature distributions accessible, outperforming standard methods like PCA, PLS, NNs, and pre-training. The competitive inhibition layer in the AL may enhance classification by creating attractor basins for inputs, pushing similar samples towards their respective class attractors, increasing effective distance between samples. The sparse connectivity from AL to MB functions additively. The sparse connectivity from the antennal lobe (AL) to the mushroom body (MB) has computational and anti-noise benefits. The MB is different from sparse autoencoders (SAs) as it does not seek to match the identity function and has a greater number of active neurons than the input dimension. Unlike Reservoir Networks, MB neurons do not have recurrent connections, and the Hebbian update mechanism is distinct. The MB requires very few samples to improve classification structure. The Hebbian update mechanism in the mushroom body (MB) is distinct from backpropagation, lacking an objective function or output-based loss. Weight updates in the MB occur locally based on a \"use it or lose it\" principle, contributing to increased total encoded information compared to traditional optimizers like MothNet and ML."
}