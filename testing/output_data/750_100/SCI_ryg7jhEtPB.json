{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) is a variational-inference method that achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. However, as the number of samples (K) is increased, the inference-network gradients break down, requiring the removal of high-variance score-function terms to circumvent this issue. In this work, the authors propose an adaptive-importance sampling framework called AISLE, which generalizes the RWS algorithm for optimizing the proposal distribution in importance sampling. This approach is argued to be more favorable than optimizing IWAE-type multi-sample objectives like IWAE-STL and IWAE-DREG. The AISLE framework generalizes the RWS algorithm and includes IWAE-STL and IWAE-DREG as special cases. It focuses on variational inference algorithms to learn the generative model and construct a tractable variational approximation. The AISLE framework generalizes variational inference algorithms to learn the generative model and construct a tractable variational approximation q \u03c6,x (z) of p \u03b8 (z|x). The setting involves a single latent representation-observation pair (z, x) with no shared parameters between the generative model and the variational approximation. The framework is general enough to cover amortised inference. The setting is general enough to cover amortised inference. Two main classes of stochastic gradient-ascent algorithms for optimising parameters have been proposed. IWAE-DREG removes problematic score-function terms from the IWAE gradient, inducing bias. RWS algorithm reweighted wake-sleep unbiasedly removes problematic score-function terms using a formal identity. The reweighted wake-sleep (RWS) algorithm optimizes two separate objectives for \u03b8 and \u03c6 using \u03c6-gradient and self-normalized importance sampling with K particles. RWS is an adaptive importance-sampling approach that iteratively improves its proposal distribution while optimizing \u03b8 via stochastic approximation. The RWS \u03c6-gradients do not degenerate as K \u2192 \u221e, unlike IWAE. In this work, it is debated whether the multi-sample objective approach of IWAE or the adaptive importance-sampling approach of RWS is preferable. The IWAE gradient lacks theoretical grounding but performs well, while RWS optimizes the proposal distribution directly and shows superior empirical performance. The IWAE gradient breakdown can be avoided by modifications like IWAE-STL and IWAE-DREG, which are justified through an adaptive importance-sampling view. This approach complements previous work by formalizing the argument that IWAE may be inferior to RWS for discrete latent variables. Our work generalizes the RWS algorithm to create a generic adaptive importance-sampling framework for variational inference. A generic adaptive importance-sampling framework for variational inference, termed AISLE, is introduced. AISLE encompasses RWS, IWAE-DREG, and IWAE-STL gradients as special cases. The framework in Section 3 allows for the derivation of various gradient estimators in a principled manner, ensuring non-degeneracy as K \u2192 \u221e. The IWAE-STL gradient can be obtained as a special case of AISLE through a novel application. The gradient recovery in AISLE is a novel application of the 'double-reparametrisation' identity. It suggests that the breakdown of RWS may not be due to its lack of a joint objective. AISLE also admits the IWAE-DREG gradient as a special case. The learning rate should be scaled as O(K) for the IWAE \u03c6-gradient. The learning rate for IWAE \u03c6-gradient should be scaled as O(K) unless gradients are normalized by popular optimizers like ADAM. AISLE does not require scaling up with K. It leads to a new family of gradient estimators for \u03b1-divergences and provides insights into the impact of self-normalization bias on importance-sampling based gradient approximations. The supplementary materials compare the main algorithms discussed in the work. The focus of the work is not to derive new algorithms or determine the best AISLE special case. Empirical comparisons of algorithms are referenced in other works. Notation shorthand is used for concise notation. The expectation of a test function can be estimated by the \u03c6, which are IID according to q \u03c6. Similarly, \u03c0 \u03b8 (f) can be approximated by self-normalised importance sampling estimators. The self-normalised estimate \u03c0 \u03b8 \u03c6, z (f) is typically not unbiased. The importance weighted autoencoder (IWAE) aims to maximize a lower bound on the log-marginal likelihood by optimizing the generative-model parameters \u03b8 and inference-network parameters \u03c6. The bound tightens with the number of samples K, and as K approaches infinity, the bound approaches the log partition function log Z \u03b8. Burda et al. (2016) show that as K \u2192 \u221e, the IWAE maximizes a lower bound on the log-marginal likelihood. For K = 1, it reduces to the VAE from Kingma & Welling (2014). However, for K > 1, the IWAE extends to another VAE on an augmented space using an auxiliary-variable construction. The gradient of the IWAE objective can be approximated using a Monte Carlo approach, but this approximation often has high variance, making it noisy and impractical. The IWAE maximizes a lower bound on the log-marginal likelihood as K increases, with a Monte Carlo approach used to approximate the gradient. The reparametrisation trick is employed to remove high-variance terms, with a lemma generalizing a well-known identity. The IWAE gradient relies on the reparametrisation trick to reduce high-variance terms. Drawbacks of the IWAE gradient include the need for reparametrisations and reliance on score-function terms in the gradient. The IWAE gradient relies on reparametrisation to reduce high-variance terms, but drawbacks include additional costs and vanishing signal-to-noise ratio. Two modifications have been proposed to avoid score-function terms and achieve stable signal-to-noise. The IWAE gradient relies on reparametrisation to reduce high-variance terms, but drawbacks include additional costs and vanishing signal-to-noise ratio. Two modifications have been proposed to avoid score-function terms and achieve stable signal-to-noise: IWAE-STL and IWAE-DREG. IWAE-STL heuristically ignores score function terms, introducing bias when K > 1, while IWAE-DREG removes score-function terms through Lemma 1. The reweighted wake-sleep (RWS) algorithm was proposed in Bornschein & Bengio (2015). The reweighted wake-sleep (RWS) algorithm, proposed in Bornschein & Bengio (2015), approximates gradients using self-normalised importance sampling. The optimization of both \u03b8 and \u03c6 is done simultaneously, but the lack of a joint objective is seen as a drawback. The lack of a joint objective in RWS is a drawback. The algorithm approximates gradients using self-normalised importance sampling, with optimization of both \u03b8 and \u03c6 done simultaneously. The function F(w) := w(1 \u2212 w) is used to transform self-normalised importance weights. In high-dimensional settings, self-normalised weights are mainly supported on the two particles with the largest weights. Re-using Monte Carlo samples for \u03b8-gradient approximation to approximate the \u03c6-gradient can reduce error in the \u03b8-gradient approximation. The proposal distribution q \u03c6 in importance-sampling schemes can be adapted without necessarily minimizing the KL-divergence. Other techniques, such as minimizing the \u03c7 2 -divergence, exist in the literature. Generalizing the RWS-objective involves maximizing log Z \u03b8 and minimizing D\u0192(\u03c0\u03b8 q \u03c6). The proposal distribution q \u03c6 in importance-sampling schemes can be adapted without necessarily minimizing the KL-divergence. The algorithm AISLE optimizes log Z \u03b8 and minimizes D\u0192(\u03c0\u03b8 q \u03c6) using a unified framework. This approach allows for robust \u03c6-gradient estimators that do not degenerate as K \u2192 \u221e, with optimization done via stochastic gradient-ascent. The \u03b8-gradient remains the same for all algorithms discussed in this work. The \u03b8-gradient is the same for all algorithms discussed, with different interpretations. Integrals of the form \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ) appear in derivations. Approximating the expectation and normalizing constant Z \u03b8 with Monte Carlo method shows bias and standard deviation orders. The \u0192-divergences used for variational inference in intractable models have a bias of order O(K \u22121 ) and a standard deviation of order O(K \u22121/2 ). Optimization of the \u0192-divergence as a function of \u03c6 can be done without knowledge of Z \u03b8, using self-importance sampling for approximation. The text discusses the reparametrised estimator in variational inference, specifically focusing on different cases such as AISLE-KL-NOREP/RWS and AISLE-KL. It demonstrates the derivation of IWAE-STL from AISLE without the need for a multi-sample objective. Proposition 1 shows that IWAE-STL can be derived from AISLE in a principled manner without relying on a multi-sample objective. This provides a theoretical basis for IWAE-STL, which was previously seen as biased and only heuristically justified. IWAE-STL exhibited good empirical performance even when RWS broke down, suggesting that this breakdown may not be due to RWS' lack of efficiency. The breakdown of RWS may not be due to lack of optimization for a joint objective. The AISLE-KL method can potentially reduce bias and variance by approximating the exact RWS gradient. The \u03b1-divergence between two distributions p and q is given by Z (p(z)/q(z)) \u03b1. The \u03b1-divergence between two distributions p and q is given by Z (p(z)/q(z)) \u03b1. This can be expressed as Z \u03ba \u03b8 Zf (w \u03c8 (z))q \u03c6 (z) dz with \u03ba = \u2212\u03b1 and f (y) = y \u03b1. Minimizing this divergence is important in importance sampling. AISLE-\u03b1-NOREP Equation (13) yields a special case proportional to the 'score gradient'. The \u03b1-divergence between two distributions p and q is given by Z (p(z)/q(z)) \u03b1. AISLE-\u03b1-NOREP Equation (13) yields a special case proportional to the 'score gradient' from Dieng et al. (2017, Appendix G). IWAE-DREG can be derived from AISLE in a principled manner without the need for a multi-sample objective. If gradients are normalized, AISLE-\u03c7 2 becomes equivalent to IWAE-DREG. The learning rate needs to be scaled as O(K) for IWAE or IWAE-DREG \u03c6-gradients. For the 'exclusive' KL-divergence, the approximation is a simple average over K independent replicates of the 'sticking-the-landing' estimator for VAEs. Optimizing this divergence can lead to faster convergence of \u03c6 than optimizing the 'inclusive' KL-divergence. The adaptive-importance sampling paradigm of reweighted wake-sleep (RWS) is preferred over the multi-sample objective paradigm of importance weighted autoencoders (IWAEs) due to its ability to achieve the same goals while avoiding drawbacks. The self-normalisation bias within RWS/AISLE plays a crucial role in learning. The self-normalisation bias in RWS/AISLE is important in learning. As the number of particles K increases, the approximation becomes more accurate. The estimators \u2207aisle-kl \u03c6 \u03b8, z and \u2207aisle-\u03c7 2 \u03c6 \u03b8, z also improve with higher K values. The self-normalisation bias in RWS/AISLE is crucial for learning. As K increases, the estimators become more accurate. The small-K self-normalisation bias of AISLE gradients favors minimizing the exclusive KL-divergence. The use of IWAEs aims to reduce bias in the gradient by minimizing the exclusive KL-divergence. Controlling the error of importance-sampling approximations involves ensuring q \u03c6 is close to \u03c0 \u03b8 in regions where \u03c0 \u03b8 has positive probability mass. A small 'inclusive' KL-divergence is key for well-behaved importance weights. In contrast, a small 'exclusive' KL-divergence is not enough for well-behaved importance weights as it only ensures closeness between q \u03c6 and \u03c0 \u03b8 in regions where q \u03c6 has positive probability mass. The family of proposal distributions Q can be flexible enough to contain a distribution q \u03c6 close to \u03c0 \u03b8, leading to well-behaved importance weights when minimizing the exclusive KL-divergence. In some cases, minimizing the exclusive KL-divergence may result in well-behaved importance weights, especially when the family of proposal distributions Q is sufficiently flexible to contain a distribution q \u03c6 close to \u03c0 \u03b8. However, if Q is not expressive enough, minimizing the exclusive KL-divergence could lead to poorly-behaved importance weights. The exclusive divergence algorithm may be preferred over gradient-descent for faster convergence in some cases. A smaller number of particles, K, could be better for \u03c6-gradients due to self-normalization bias. Simply setting K = 1 for \u03c6-gradients approximation is not always optimal. The \u03c6-gradients approximation may not be optimal even in ideal scenarios, as increasing K is desirable to reduce variance. Not utilizing all K particles and weights for gradient approximations is wasteful, and optimizing exclusive KL-divergence could lead to poorly behaved importance-sampling approximations when \u03c6 is far from optimal. The \u03c6-gradients approximation may not be optimal even in ideal scenarios, as increasing K is desirable to reduce variance. Utilizing different \u03c6-gradient estimators, such as AISLE-KL-NOREP and AISLE-\u03c7 2 -NOREP, can provide insights into gradient approximations without reparametrisation. The gradient for AISLE based on different divergences and reparametrization techniques does not achieve zero variance, even if q \u03c6 = \u03c0 \u03b8. The gradient for IWAE-DREG, proposed in Tucker et al. (2019), is proportional to the reparametrised IWAE gradient and addresses signal-to-noise ratio degeneration with K. The IWAE gradient from Tucker et al. (2019) is proportional to AISLE-\u03c7 2. The 'doubly-reparametrised' RWS \u03c6-gradient is also discussed, with a focus on the joint law and generative model parametrised by \u03b8. The proposal distributions are taken as fully factored Gaussians. The proposal distributions are fully factored Gaussians with parameters to optimize. The mean of the proposal coincides with the mean of the posterior if A = P and b. This model is similar to benchmarks in previous studies. The proposal distributions are fully factored Gaussians with parameters to optimize. The mean of the proposal coincides with the mean of the posterior if A = P and b. In a more realistic scenario, the latent vectors z can be correlated under the generative model, leading to uncertainty that may not be fully captured by the variational approximation. The only source of randomness in the expression is the multivariate normal random variable, affecting the variance of the A-and b-gradient portion of AISLE-KL/IWAE-STL. In Gaussian settings, the variance of the A-and b-gradient portion of AISLE-KL/IWAE-STL and AISLE-\u03c7 2 /IWAE-DREG goes to zero as the parameters approach their optimal values. The variance of the C-gradient portion also decreases to zero. Further analysis of reparametrisation-trick gradients in Gaussian settings is discussed in Xu et al. (2019). In Xu et al. (2019), algorithms are compared empirically for different numbers of particles and model dimensions. Each configuration is repeated 100 times with new synthetic data sets. The focus is on optimizing \u03c6 while fixing \u03b8 throughout. The generative model is specified via \u03a3 = I. The generative model is specified via \u03a3 = I and \u03a3 = (0.95 |d\u2212d |+1 ) (d,d )\u2208{1,...,D} 2. The fully-factored variational approximation cannot fully mimic the dependence structure of the latent variables under the generative model. Initial values \u03c6 0 of \u03c6 are drawn IID according to a standard normal distribution for the gradient-ascent algorithm. The initial values \u03c6 0 of \u03c6 are drawn IID according to a standard normal distribution for the gradient-ascent algorithm. Stochastic gradient-ascent and ADAM are used with default parameter values for a total of 10,000 iterations. The learning-rate parameters at each step follow i \u22121/2. The covariance matrix \u03a3 is not diagonal, with a logarithmic scaling on the second axis."
}