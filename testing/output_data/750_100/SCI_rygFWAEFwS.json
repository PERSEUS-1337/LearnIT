{
    "title": "rygFWAEFwS",
    "content": "We propose SWAP, an algorithm to speed up DNN training using large mini-batches and weight averaging of parallel models. The resulting models generalize well on CIFAR10, CIFAR100, and ImageNet datasets, demonstrating reduced training time. Training deep neural networks (DNNs) with large mini-batches and weight averaging can accelerate training by producing more precise gradient estimates and allowing for higher learning rates. In a distributed setting, multiple nodes can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates. Training with larger mini-batches requires fewer updates, thus fewer synchronization events, yielding good overall scaling behavior. However, there is a maximum batch size that can lead to worse generalization performance, limiting the usefulness of large-batch training strategies. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging the weights of models sampled from the final stages of training. Generating multiple independent SGD sequences and averaging models from each can achieve similar generalization performance. SWAP is a strategy to accelerate DNN training by better utilizing compute resources. It achieves generalization performance comparable to models trained with small-batches on image classification tasks. SWAP accelerates DNN training by utilizing compute resources efficiently, achieving performance comparable to small-batch models in less time. It outperforms state-of-the-art models on CIFAR10, training in 68% of the time. The impact of training batch size on generalization performance remains unclear. In (Keskar et al., 2016), authors discuss the sensitivity of sharp minima to data variations. In (Dinh et al., 2017), a flat minimizer is transformed into a sharp one without changing model behavior. In (Li et al., 2018), authors show the reverse behavior without weight-decay. In (McCandlish et al., 2018), authors predict that batch size can be increased without accuracy drop. The authors predict that increasing batch size up to a critical point does not affect accuracy. They argue that larger batch sizes lead to mini-batch gradients close to full gradients, with further increases not significantly improving signal to noise ratio. Using a larger batch size implies fewer model updates, impacting weight initialization distance. The authors argue that increasing batch size up to a critical point does not affect accuracy significantly. Larger batch sizes lead to mini-batch gradients close to full gradients, impacting weight initialization distance. Training with large-batches for longer times improves generalization performance but takes more time than the small-batch alternative. The batch size also affects the optimization process, with a critical batch size identified for convex functions in the over-parameterized setting. Local SGD is a distributed optimization algorithm that balances gradient precision with communication costs by allowing workers to adjust batch sizes. Various methods exist for adaptive batch sizes, but they often require specific datasets or extensive hyper-parameter tuning. Post-local SGD is a variant of the distributed optimization algorithm Local SGD, which refines large-batch training with local-SGD to improve model generalization and achieve speedups. Unlike Post-local SGD, SWAP averages models after multiple epochs instead of letting them diverge for a few iterations. Stochastic weight averaging (SWA) is a method where models are sampled from later stages of an SGD training run and averaged to improve generalization properties. This strategy has been effective in various domains such as deep reinforcement learning and semisupervised learning. In this work, SWA is adapted to accelerate DNN training. The algorithm involves three phases: large mini-batch updates with synchronization in the first phase, independent model refinement with smaller batch size and lower learning rate in the second phase, and different weight sets produced by workers. The SWA algorithm involves three phases: large mini-batch updates with synchronization in the first phase, independent model refinement with smaller batch size and lower learning rate in the second phase, and averaging weights of resulting models in the last phase to produce final output. Phase 1 stops before training loss reaches zero or accuracy reaches 100% to prevent optimization from getting stuck, with the optimal stopping accuracy being a tunable hyper-parameter. During phase 2 of the SWA algorithm, small-batch training is conducted independently by each worker, resulting in different models. The accuracies and learning-rate schedules are plotted in Figure 1, showing that during the large-batch phase, all workers share a common model with the same performance, while in the small-batch phase, the learning rates are the same but testing accuracies vary. In phase 2 of the SWA algorithm, small-batch training results in diverging models with different testing accuracies due to stochasticity. The averaged model outperforms individual models consistently. The SWAP algorithm visualizes the error of the test network by plotting it on a plane containing outputs of different algorithm phases. Weight vectors are generated to compute batch-norm statistics and evaluate accuracies. Training and testing errors for the CIFAR10 dataset are shown in Figure 2. In Figure 2, the training and testing error for the CIFAR10 dataset are plotted. The output of phase one ('LB') and one worker from phase two ('SGD') are shown on the training error landscape, with the final model ('SWAP') closer to the center. The test loss landscape also shows the variations in topology. In Figure 2, the training and testing error for the CIFAR10 dataset are plotted, showing the variations in the topology of the basin. The 'LB' and 'SGD' points fall in regions of higher error, while 'SWAP' is closer to the center and less affected by the change in topology. In Figure 3, the worker points lie in regions of higher testing errors than 'SWAP', which remains close to the center of both basins. In (Mandt et al., 2017), the authors suggest that in the later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process. By maintaining a constant learning rate, SGD iterates should reach a stationary distribution similar to a high-dimensional Gaussian, centered at the local minimum. The covariance grows with the learning rate and is inversely proportional to the batch size, with a shape dependent on the Hessian of the mean loss and gradient covariance. The authors argue that in a high-dimensional Gaussian distribution, the mass is concentrated near the ellipsoid's 'shell', making it unlikely for SGD to access the interior. Sampling weights from different SGD runs can lead to weights spread out on the ellipsoid's surface, with their average closer to the center. This justifies sampling from different runs as long as they start in the same basin of attraction. The authors argue that sampling from different SGD runs can lead to weights spread out on the ellipsoid's surface, with their average closer to the center. This justifies sampling from different runs as long as they start in the same basin of attraction. Additionally, the cosine similarity between the gradient descent direction and the direction towards the output of SWAP decreases as training progresses, indicating movement orthogonal to the basin. In this section, the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated. The best hyper-parameters were found using grid searches. Training was done using mini-batch SGD with Nesterov momentum and weight decay. Data augmentation techniques like cutout were employed, and a custom ResNet 9 model was used for training. The performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets was evaluated using a custom ResNet 9 model. Experiments were conducted on one machine with 8 NVIDIA Tesla V100 GPUs using Horovod for distributed computation. Different settings were used for training phases, including batch sizes and number of epochs. The experiments involved training a custom ResNet 9 model on CIFAR10, CIFAR100, and ImageNet datasets using SWAP. Different batch sizes and epochs were used, with 4096 samples per batch on 8 GPUs for 150 epochs, and 512 samples per batch on 2 GPUs for 100 epochs. Test accuracies and training times were compared for small-batch only, large-batch only, and SWAP models. Using SWAP with 8 Tesla V100 GPUs, significant improvements in test accuracies were observed for CIFAR10 and CIFAR100 datasets. SWAP achieved accuracies on par with small-batch training while terminating in time comparable to large-batch training. The current front-runner in the DAWNBench competition takes 37 seconds with 4 Tesla V100 GPUs to train CIFAR10 to 94% test accuracy. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch can achieve 94% test accuracy on CIFAR10 in 27 seconds. The code modifies learning rates and batch sizes throughout the optimization process. Small-batch experiments on ImageNet are run for 28 epochs with no modifications, while large-batch experiments double the batch size. Using SWAP with 16 Tesla V100 GPUs, the batch size and learning rates are doubled for large-batch experiments. SWAP phase 1 runs for 22 epochs, while phase 2 runs for 6 epochs with 8 GPUs per worker. Doubling the batch size reduces test accuracies, but SWAP recovers generalization performance with faster training times. Results are shown in Table 3, achieved with no tuning other than batch size adjustments. SWAP accelerations achieved with no tuning other than adjusting batch size. Double batch size, GPUs, and learning rate for larger batch experiments. Transition from modified to original schedule between phases. Comparing SWAP with SWA algorithm. Top1 and Top5 Accuracy, Training Time results compared. In this section, the SWA algorithm is compared with SWAP using the CIFAR100 dataset. Models are sampled with 10 epochs in-between for SWA and with 8 independent workers for 10 epochs each for SWAP. The goal is to see if SWA can recover test accuracy of small-batch training on a large-batch training run. Initial training cycle with cyclic learning rates is followed to sample 8 models. The study compares SWA and SWAP using the CIFAR100 dataset. SWA aims to recover test accuracy of small-batch training on a large-batch run. Large-batch training achieves lower accuracy, but SWA does not improve it. SWA after large-batch training reaches test accuracy of small-batch but takes three times longer. SWA aims to recover test accuracy of small-batch training on a large-batch run but takes more than three times longer than SWAP to compute the model. The learning rate schedule is illustrated in Figure 6b. Small-batch SWA and SWAP start from the best model found by small-batch training. The peak learning rate is selected using grid-search. The SWA schedule is then used in SWAP, starting phase two from the model generated in phase 1. SWAP achieves better accuracy than SWA by around 0.9% at 6.8x more training time. By relaxing constraints on SWAP, the resulting model achieved a test accuracy of 79.11% in 241 seconds or 3.5x less time compared to SWA. Weight Averaging in Parallel (SWAP) is an algorithm that improves model generalization performance by using large mini-batches for quick approximate solutions, then refining them through weight averaging of multiple models trained with small-batches. This approach results in a final model with good generalization performance trained in a shorter time. Weight Averaging in Parallel (SWAP) improves model generalization by refining large-batch runs with sequential or parallel model sampling. The resulting model performs as well as those trained with small-batches. Visualizations show that averaged weights are closer to the center of the training loss basin, indicating convergence to the same basin where refined models are found. Our method requires choosing a transition point between large-batch and small-batch training, determined through grid search. Future work will focus on a principled method for selecting this point and exploring SWAP with other optimization schemes. SWAP allows for the substitution of different optimization schemes such as mixed-precision training, post-local SGD, or NovoGrad. Parameters used in experiments were obtained through independent grid searches. Momentum and weight decay constants were kept at 0.9 and 5 \u00d7 10 \u22124 for all CIFAR experiments. Stopping accuracy of 100% indicates maximum epochs were used."
}