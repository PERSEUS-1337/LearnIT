{
    "title": "BygfghAcYX",
    "content": "Existing work on neural network generalization has focused on complexity measures like norms, margin, and sharpness, but these do not explain why networks generalize better with over-parametrization. This study introduces a novel complexity measure based on unit-wise capacities for two layer ReLU networks, showing a tighter generalization bound. The capacity bound aligns with test error behavior as network sizes increase, offering insight into the benefits of over-parametrization. Additionally, a lower bound for Rademacher complexity is presented, improving upon previous results. The curr_chunk discusses the improved lower bound for Rademacher complexity in neural networks, highlighting their success in various tasks and the pursuit of better performance through over-parametrization. Despite being able to fit random labels, these networks achieve smaller generalization error when trained with real labels. Increasing the model size in neural networks helps improve generalization error, even without explicit regularization like weight decay or early stopping. This contrasts traditional learning wisdom that suggests overfitting with larger models. Training on models with increasing hidden units reduces test error for image classification on MNIST and CIFAR-10. Empirical observations show improvement in generalization with over-parametrization across various architectural and hyper-parameter choices. The complexity of neural networks that captures this generalization phenomenon is not adequately explained by measures dependent on the total number of parameters, such as VC bounds. Existing works have proposed various measures to explain the generalization behavior of neural networks, which is not adequately captured by total parameter counts like VC bounds. Even when the network is large enough to fit the training data, test error continues to decrease for larger networks. In a feedforward network with a single hidden layer on CIFAR-10, similar phenomena as in the ResNet18 architecture are observed. Unit capacity and unit impact are crucial factors in the capacity bound, with both average unit capacity and impact decreasing faster than 1/ \u221a h, where h is the number of hidden units. Experimental settings can be found in Supplementary Section A. Previous studies have shown a margin-based generalization bound dependent on the spectral norm and 1,2 norm of the network layers. Existing complexity measures fail to explain why over-parametrization helps and increase with the size of the network, even for two layer networks. These measures depend on the number of hidden units explicitly or implicitly in their norms. In this study, the authors focus on the impact of hidden units on network norms. They simplify the architecture to two layer ReLU networks and prove a tighter generalization bound. Unlike existing bounds, their capacity bound decreases with an increase in hidden units and correlates with test error. The study focuses on the impact of hidden units on network norms in two-layer ReLU networks. The generalization bound decreases with an increase in hidden units and correlates with test error. The complexity at a unit level decreases faster than 1/ \u221a h for each hidden unit as the network size increases. The generalization bound depends on the Frobenius norm of the top layer and the difference of the hidden layer weights with the initialization, which decreases with increasing network size. In the over-parametrized setting, the impact of learned weights on initialization can be understood by considering the limiting case with a large number of hidden units. Training just the top layer of the network in this extreme setting results in minimizing the training error by selecting the right features that minimize the loss. As networks are over-parametrized, optimization algorithms need to focus on selecting the optimal features. In the over-parametrized setting, optimization algorithms require less tuning of weights as networks become more complex. Studies have shown the importance of initialization in training neural networks, with different approaches yielding varying results. Liang et al. (2017) proposed a Fisher-Rao method as an alternative approach. In the over-parametrized setting, optimization algorithms require less weight tuning as networks become more complex. Liang et al. (2017) proposed a Fisher-Rao method as an alternative approach for generalization behavior in larger networks. The contributions of this paper include empirical investigation on over-parametrization in neural networks on different datasets and proving tighter generalization bounds for two layer ReLU networks. Our proposed complexity measure for neural networks decreases with the increasing number of hidden units, potentially explaining the effect of over-parametrization on generalization. We provide a matching lower bound for the Rademacher complexity of two layer ReLU networks with a scalar output, significantly improving over previous bounds. The Lipschitz constant of the network class is determined by the two-layer fully connected ReLU networks with input dimension d, output dimension c, and the number of hidden units h. The margin operator \u00b5 is defined for c-class classification tasks to select the label with the maximum output score as the prediction. The ramp loss is defined as the difference between the score of the correct label and the maximum score among other labels. It is bounded between 0 and 1, with the expected margin loss of a predictor being calculated for any distribution and margin. The loss is denoted as L\u03b3(f) and can be used to estimate the training error and expected risk. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels, increasing with the complexity of the class. This complexity is bounded for neural networks to get a bound on the training error and expected risk. The Rademacher complexity of neural networks is bounded to get a bound on generalization error. Choosing the right function class is crucial to capture trained networks accurately and explain the decrease in error with increasing width. Experimenting with different measures of network layers helps understand the behavior with increasing hidden units. The experiments on the CIFAR-10 dataset show that the Frobenius norm of the learned layer increases with the number of hidden units, while the distance to initialization decreases. This suggests that the increase in weight norm in larger networks is due to the random initialization. In the experiments on the CIFAR-10 dataset, the Frobenius norm of the learned layer increases with the number of hidden units, while the distance to initialization decreases. This indicates that the weight norm in larger networks is influenced by random initialization. The per unit distance to initialization decreases with increasing h, and there is a shift in the distribution of angles between learned weights and initial weights in large networks. The unit capacity, defined as \u03b2 i = u i \u2212 u 0 i 2, is a key quantity in capacity bounds. In the second layer of trained networks, the Frobenius norm and distance to initialization decrease with increasing size, suggesting a limited role of initialization. The norm of outgoing weights from hidden units decreases faster than 1/ \u221a h as the size grows. This implies a decrease in the impact of each hidden unit as a linear separator in the ensemble of classifiers at the top layer. In the second layer of trained networks, the norm of outgoing weights from hidden units decreases faster than 1/ \u221a h as the size grows, implying a decrease in the impact of each hidden unit as a linear separator in the ensemble of classifiers at the top layer. The unit impact, defined as \u03b1 i = v i 2, plays an important role in two-layer neural networks, depending on the capacity and impact of hidden units. The hypothesis class of neural networks represented by parameters in set W has bounded unit capacity and impact. Studying the generalization behavior of this function class can provide a better understanding of these networks. A generalization bound for two-layer ReLU networks is proven by bounding the Rademacher complexity of the class F W in terms of the sum over hidden units of the product of unit capacity and impact. This, combined with a specific equation, gives us insights into the generalization properties of these networks. The proof in Section C introduces a new technique to decompose network complexity into that of hidden units, different from previous works that focused on layer complexity and Lipschitz property to bound generalization error. The proof introduces a new technique to decompose network complexity into hidden units, providing a tighter bound on the Rademacher complexity of two-layer neural networks. The generalization bound in Theorem 1 applies to any function in the defined function class with fixed \u03b1 and \u03b2. Theorem 2 presents a generalization bound for any two-layer ReLU network. The generalization error for any two-layer ReLU network is bounded with probability 1 - \u03b4 over the choice of the training set. The bound improves over existing bounds and decreases with increasing width for networks learned in practice. An explicit lower bound for the Rademacher complexity is also shown, matching the first term in the generalization bound. The additive factor in the bound is the result of taking the union bound over the cover of \u03b1 and \u03b2. The generalization error for two-layer ReLU networks is bounded with probability 1 - \u03b4 over the training set. The additive term in the bound is small and does not dominate the first term, leading to a decrease in capacity with over-parametrization. The generalization bound is extended to p norms in Appendix Section B, showing a finer tradeoff between the terms. Comparisons with previous work show similarities in the first term but differences in the key complexity term. The generalization error for two-layer ReLU networks is bounded with probability 1 - \u03b4 over the training set. The bound increases with h mainly due to the term U \u2212 U 0 1,2. Experimental comparison on CIFAR-10 and SVHN datasets shows training and test errors for networks of size h ranging from 2 6 to 2 15. The bound is small and does not dominate the first term, leading to a decrease in capacity with over-parametrization. In experiments on CIFAR-10 and SVHN datasets, networks of size 128 achieve zero training error, but larger networks show better generalization even without regularization. Unit capacity and unit impact decrease with network size, as shown in figures. Larger networks require fewer epochs to reach 0.01 cross-entropy loss. FIG6 compares different capacity behaviors. The effective capacity of function class is compared in FIG6 for networks of increasing sizes. The bound decreases with complexity and outperforms other norm-based bounds, even surpassing VC-dimension for networks larger than 1024. Numerical values are loose but provide insight into generalization behavior. Our capacity bound decreases with network size, outperforming other norm-based bounds and even surpassing VC-dimension for larger networks. This insight into generalization behavior can be improved with data-dependent techniques. In this section, a lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound. The behavior of the complexity measure is checked by comparing networks trained on real and random labels, showing correlation with generalization behavior. The lower bound on the Rademacher complexity of neural networks matches the dominant term in the upper bound of Theorem 1, extending it to a bigger function class. The parameter set DISPLAYFORM0 is defined, and the proof is provided in the supplementary Section C.3. The upper bound in Theorem 1 is tight, even with additional information like bounded spectral norm, indicating no improvement is possible. The lower bound for spectral norm bounded neural networks with scalar output and element-wise activation functions improves over previous capacity lower bounds, showing a gap between Lipschitz constant and network capacity. This non-trivial lower bound excludes neural networks with rank-1 weight matrices, revealing a capacity gap between ReLU and linear networks. The lower bound for neural networks with ReLU activations and linear networks does not hold for linear networks. By setting weight matrices in intermediate layers to be the Identity matrix, the construction can be extended to more layers. The function class defined by the parameter set has a Lipschitz bound. Choosing specific bounds for the weight matrices results in a stronger lower bound for this function class. This result improves upon previous work by Bartlett et al. (2017). Our result improves the lower bound in Bartlett et al. (2017) by a factor of \u221a h. Theorem 7 in Golowich et al. (2018) also gives a \u2126(s 1 s 2 \u221a c) lower bound for neural networks with bounded spectral norm. Our new capacity bound for neural networks decreases with the increasing number of hidden units, potentially explaining better generalization performance of larger networks. We focus on the role of then width in generalization behavior of two layer networks. The study focuses on the role of width in the generalization behavior of two-layer networks and the interplay between depth and width in controlling network capacity. A new capacity bound for neural networks decreases with an increasing number of hidden units, potentially explaining better generalization performance of larger networks. The study also provides a matching lower bound for network capacity, aiming for bounds with numerically smaller values. In an experiment, a pre-activation ResNet18 architecture was trained on the CIFAR-10 dataset, consisting of a convolution layer followed by 8 residual blocks. The study aims to understand the impact of different hyperparameter choices on the complexity of recovered solutions and the implicit regularization effects of optimization algorithms for neural networks. The pre-activation ResNet18 architecture consists of a convolution layer followed by 8 residual blocks with specific output channels and strides. Different hyperparameter choices were explored in training 11 architectures using SGD with mini-batch size 64 and specific learning rate adjustments. The study trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets using various architectures and data augmentation techniques. Different architectures were tested with increasing hidden units, trained using SGD with specific parameters, and stopped when loss reached 0.001 or after 1000 epochs. No weight decay was used in the experiments. The study trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets using various architectures and data augmentation techniques. Specific parameters were used for training, such as a fixed step size of 0.01 for MNIST and 0.001 for CIFAR-10 and SVHN. Weight decay, dropout, and batch normalization were not utilized. Training was stopped when the cross-entropy reached 0.01 or after 1000 epochs. Evaluation included calculating exact generalization bounds with specific margins set. BID2 and Neyshabur et al. (2015c) bounds were adjusted for binary classification. The study trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets using various architectures and data augmentation techniques. Specific parameters were used for training, such as a fixed step size of 0.01 for MNIST and 0.001 for CIFAR-10 and SVHN. Weight decay, dropout, and batch normalization were not utilized. Training was stopped when the cross-entropy reached 0.01 or after 1000 epochs. Evaluation included calculating exact generalization bounds with specific margins set. BID2 and Neyshabur et al. (2015c) bounds were adjusted for binary classification. The behavior of several measures on networks with different sizes trained on SVHN and MNIST datasets is shown in Figures 6 and 7. The left panel of FIG10 illustrates the over-parametrization phenomenon in the MNIST dataset, while the middle and right panels compare the generalization bound to others. In this section, Theorem 2 is generalized to the p norm using Lemma 11 to construct a cover for the p ball with entry-wise dominance. Theorem 5 provides a bound on generalization error for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1), and U 0 \u2208 R h\u00d7d. The error is bounded by a term involving the p norm of row 2 norms, improving on previous bounds. The upper bound for V F is of the same order if all rows of V have the same norm, giving a tighter bound that decreases with h for larger values. With probability 1 \u2212 \u03b4 over the choice of the training set S = {x i } m i=1, the generalization error for any function f(x) = V[Ux] + is bounded. A vector-contraction inequality for Rademacher complexities is used in the proof, along with a technical result from Maurer (2016). The Rademacher complexity of the class of networks can be decomposed to that of hidden units using a technical result from Maurer (2016). Lemma 9 provides a bound on the Rademacher complexity of the class F W defined in equations (5) and (4) with a training set S = {x i } m i=1 and \u03b3 > 0. The proof involves an induction on t to establish the inequality. Lemma 9 establishes an inequality using induction on t. The ramp loss is shown to be Lipschitz with a bound provided by Lemma 7. Lemma 10 states a contraction property for convex and increasing functions, which is used in the proof of Theorem 1. The proof involves applying Lemma 9 and utilizing the contraction property to derive an inequality. Lemma 11 introduces a covering lemma for a p ball, allowing the generalization bound in Theorem 5 to be proven without knowledge of network parameter norms. It shows how to cover a set dominating elements entry-wise, with bounds on the cover size. The generalization error is bounded by a lemma applying a union bound, with specific conditions on norms of matrices V and U. The lemma covers cases where certain norms are larger than a given value, ensuring the bound holds. The generalization error is bounded by a lemma for specific conditions on norms of matrices V and U. Lemma 14 provides specific results for the case p = 2, with bounds on the generalization error for a given function f(x). The proof of Theorem 2 directly follows from Lemma 14, which provides a generalization bound for p = 2 and specific conditions on norms of matrices V and U. Lemma 15 extends this bound for any p \u2265 2, with additional constants and logarithmic factors. The generalization error for a function f(x) with matrices V and U is bounded by a specific formula. The proof involves upper bounding the generalization bound and showing its validity for all values of \u00b5. The proof of Theorem 5 follows from Lemma 15 by using notation to hide constants and logarithmic factors. The proof of Theorem 3 starts with specific cases and involves dividing the dataset into groups with different elements in a standard orthonormal basis. The proof of Theorem 5 utilizes notation to conceal constants and logarithmic factors. It involves dividing the dataset into groups with different elements in a standard orthonormal basis, leading to the selection of U(\u03be) as Diag(\u03b2) \u00d7 F(\u03be), where F(\u03be) is orthonormal."
}