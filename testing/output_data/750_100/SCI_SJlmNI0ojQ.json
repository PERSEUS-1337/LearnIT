{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are popular for their ease of training, scalability, and lack of a lexicon requirement. This paper discusses constructing contextual acoustic word embeddings from a supervised sequence-to-sequence model, showing competitive performance on standard sentence evaluation tasks. Our embeddings show competitive performance on standard sentence evaluation tasks and spoken language understanding tasks, matching text-based embeddings in performance. Learning fixed-size representations for variable length data is a key focus in natural language processing research. Methods like word2vec, GLoVE, CoVe, and ELMo have gained popularity for natural language processing tasks. In speech recognition, the challenge lies in processing short-term audio features instead of words or characters, with variability in speakers, acoustics, and microphones. Previous approaches involved aligning speech and text or segmenting input speech into fixed-length segments. Our work focuses on constructing individual acoustic word embeddings grounded in utterance-level acoustics, unlike previous techniques that ignore specific audio context. We present various methods for obtaining these embeddings from an attention-based sequence-to-sequence model trained for direct speech recognition. Our work presents a model trained for direct Acoustic-to-Word (A2W) speech recognition, allowing for automatic segmentation and classification of input speech into individual words without the need for pre-defined boundaries. The model learns acoustic word embeddings in the context of their containing sentence, which proves useful in non-transcription downstream tasks. In this paper, the authors demonstrate the usability of attention for aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE). Their methods for constructing word representations directly from a speech recognition model are competitive with text-based word2vec embeddings. Additionally, they show the utility of CAWE in a speech-based downstream task of Spoken Language Understanding. Pretrained speech models can be used for transfer learning, similar to VGG in vision or CoVe in natural language understanding. A2W modeling is pursued using CTC and S2S models. Large amounts of training data are needed for these models, but progress has shown the possibility of training with smaller data and restricted vocabularies. The solutions for generating out-of-vocabulary words in A2W recognition models involve using smaller units like characters or sub-words. Recent advancements have focused on developing S2S models for large vocabulary A2W recognition, with some models achieving success with a vocabulary of around 30,000 words. Further improvements have been made in training these models for the large vocabulary task. The direct A2W model can learn word boundaries without supervision and is the best pure-word S2S model. BID4, BID5, BID7, BID25, and BID8 explore ways to learn acoustic word embeddings, with most using unsupervised methods except for BID6, which uses a supervised CNN model for speech recognition with short speech frames. BID4 proposes an unsupervised method to learn speech embeddings using a fixed context of words in the past and future, but it requires forced alignment between speech and words for training. Learning contextualized word embeddings is a rich area of research with established techniques like BID0 and BID1, progressing into contextual word embeddings like BID2 and BID3, useful for text-based downstream tasks. The curr_chunk discusses the use of contextual word embeddings in a speech recognition model, specifically focusing on the structure of the encoder network. It mentions the utilization of a pyramidal multi-layer bi-directional Long Short Term Memory (BLSTM) network for mapping input acoustic features to higher-level features. The decoder network in the speech recognition model is an LSTM network that uses an attention mechanism to generate target sequences. The attention mechanism enforces monotonicity in alignments by applying a convolution across time. Our model uses a peaky attention distribution for the current time step, following the experimental setup and hyper-parameters of our previous word-based models. We learn 300 dimensional acoustic feature vectors instead of 320 dimensional ones. Acoustic word embeddings are obtained from the end-to-end trained speech recognition system, utilizing hidden representations from the encoder and attention weights from the decoder. The method of constructing \"contextual\" acoustic word embeddings is similar to a specific method. Our method focuses on learning embeddings from a supervised task, addressing the alignment issue between input speech and output words using a location-aware attention mechanism. This mechanism helps segment continuous speech into words, allowing us to obtain word embeddings efficiently. The process involves constructing contextual acoustic word embeddings by assigning attention weights to acoustic frames' hidden representations based on their importance in classifying a word. This allows for the creation of word representations by weighing the hidden representations of acoustic frames according to their attention weights. The model assigns attention weights to acoustic frames' hidden representations to create word embeddings. It uses different methods to obtain acoustic word embeddings for a word. The model generates word embeddings by assigning attention weights to hidden representations of acoustic frames. It utilizes unweighted Average, attention weighted Average, and maximum attention techniques to create Contextual Acoustic Word Embeddings (CAWE). The Contextual Acoustic Word Embeddings (CAWE) are generated using attention scores over acoustic frames for a given word. Two datasets are used: the Switchboard corpus with telephonic conversations and the How2 dataset with instructional videos. The A2W model achieves a word error rate of 22.2%. The A2W model achieves a word error rate of 22.2% on Switchboard and 36.6% on CallHome set from the Switchboard Eval2000 test set, and 24.3% on dev5 test set of How2. The embeddings are evaluated on 16 benchmark sentence evaluation tasks covering Semantic Textual Similarity, classification, sentiment analysis, question type, Subjectivity/Objectivity, opinion polarity, entailment, semantic relatedness, and paraphrase detection using various datasets. The evaluation tasks include measuring correlation between embedding similarity and human scores for STS and SICK-R, while other tasks focus on classification accuracies using logistic regression. The SentEval toolkit BID26 is used for evaluation. The evaluation tasks involve measuring correlation between embedding similarity and human scores for STS and SICK-R, as well as classification accuracies using logistic regression. CAWE-M outperforms U-AVG and CAWE-W on Switchboard and How2 datasets for STS tasks, while showing comparable performance on classification tasks. The difference in performance between CAWE-M and CAWE-W could be due to noisy estimation of word embeddings in CAWE-W. The embeddings obtained from the speech recognition model outperform U-AVG and CAWE-W on STS tasks, with CAWE-M showing better performance. U-AVG performs worse due to its noisy construction process. The datasets used for downstream tasks are the same as described in Section 5.1. The speech recognition model used for word embeddings has a smaller vocabulary compared to word2vec CBOW. Despite this limitation, the performance of CAWE is competitive with word2vec CBOW. The performance of CAWE is competitive with word2vec CBOW. Evaluations show that acoustic embeddings combined with text embeddings outperform word2vec on 10 out of 16 tasks. The gains are more prominent in Switchboard dataset compared to How2, due to the characteristics of the speech data. The CAWE model is evaluated on the ATIS dataset for Spoken Language Understanding, which consists of spoken language queries for airline reservations. The model architecture includes an embedding layer and a single layer RNN-variant. Our architecture includes an embedding layer, a single layer RNN-variant (Simple RNN, GRU), a dense layer, and softmax. We train the model for 10 epochs with RMSProp (learning rate 0.001) and repeat the training 3 times with different seed values for average performance comparison. BID29 found that text-based word embeddings from large corpora perform well on the ATIS dataset. However, we show that speech-based word embeddings can achieve similar performance, highlighting the effectiveness of our speech-based embeddings. The study compares test scores of models initialized with different embeddings and fine-tuned for a task. Contextual acoustic word embeddings are learned from a speech recognition model, showing competitive performance with word2vec. Two variants of these embeddings outperform the simple average method by up to 34% on semantic textual tasks. The study demonstrates that contextual audio embeddings outperform simple average methods by up to 34% on semantic textual tasks and match the performance of text-based embeddings in spoken language understanding. The model shows potential for use in downstream speech-based tasks despite the complexity of noisy audio input. Future work will focus on scaling the model to larger corpora and vocabularies, and comparing with non-contextual acoustic word embedding methods. The curr_chunk discusses word embedding methods supported by the Center for Machine Learning and Health at Carnegie Mellon University and Facebook."
}