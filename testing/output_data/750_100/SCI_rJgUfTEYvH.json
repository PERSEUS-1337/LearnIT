{
    "title": "rJgUfTEYvH",
    "content": "Generative models for video prediction face the challenge of uncertain futures. Recent works have explored probabilistic models, but they are computationally expensive or do not optimize data likelihood directly. This work introduces multi-frame video prediction with normalizing flows, enabling direct optimization. This work introduces multi-frame video prediction with normalizing flows, allowing for direct optimization of data likelihood and producing high-quality stochastic predictions. Flow-based generative models offer a competitive approach to generative modeling of video, showcasing exponential progress in machine learning capabilities. The application of machine learning technology has been largely constrained to situations with large amounts of supervision or accurate simulations of the environment. An alternative to supervised learning is to use large unlabeled datasets with predictive generative models for effective prediction of future events. Predictive generative models can effectively predict future events by building an internal representation of the world, such as physical interactions in videos. These models can learn from large unlabeled datasets of real-world interactions, providing a rich understanding of the physical world without the need for labeled examples. Predictive generative models can learn representations for downstream tasks or direct applications like robotics by predicting uncertain futures. Recent works have focused on probabilistic models for uncertain futures, but they are computationally expensive or do not optimize likelihood directly. In this paper, the focus is on stochastic prediction, specifically conditional video prediction. A new class of video prediction models is proposed to generate diverse stochastic futures and synthesize realistic video frames by extending flow-based generative models. The approach extends flow-based generative models into conditional video prediction, addressing the challenges of modeling high-dimensional video sequences by learning a latent dynamical system model. This induces Markovian dynamics on the latent state, replacing the standard unconditional prior distribution. VideoFlow is a flow-based video prediction model inspired by the Glow model for image generation. It achieves competitive results in stochastic video prediction on the BAIR dataset, rivaling VAE-based models. VideoFlow produces high-quality qualitative results and avoids common artifacts seen in models using pixel-level mean-squared-error for training. VideoFlow is a flow-based video prediction model that achieves fast test-time image synthesis, making it practical for real-time applications like robotic control. It optimizes the likelihood of training videos directly, allowing for evaluation based on likelihood values. The research on deterministic models focuses on architectural changes and different generation objectives. The next challenge is to address modeling in various deterministic environments. Building models that can effectively reason over uncertain futures is the next key challenge after successfully modeling deterministic environments. Real-world videos are often stochastic due to random events or unobserved factors, leading deterministic models to either disregard potential futures or produce blurry predictions. Various methods have been developed to overcome this limitation. Building models to reason over uncertain futures is a key challenge after modeling deterministic environments. Methods like variational autoencoders, generative adversarial networks, and autoregressive models have been developed to incorporate stochasticity in predicting possible futures in real-world videos. Variational autoencoders, optimizing an evidence lower bound on log-likelihood, have been widely explored. The only prior class of video prediction models that directly maximize the log-likelihood of the data are auto-regressive models. However, synthesis with such models is typically inherently sequential, making synthesis substantially inefficient on modern parallel hardware. Prior work has aimed to speed up training and synthesis with these models, but the predictions are sharp but noisy. The proposed VAE model outperforms autoregressive models in producing high-quality long-term predictions with faster sampling. The VAE model utilizes a multi-scale architecture with stochastic variables, while autoregressive models encode input into multiple levels of stochastic variables sequentially. Flow-based generative models (Dinh et al., 2014) offer advantages like exact latent variable inference, log-likelihood evaluation, and parallel sampling. These models infer latent variables by transforming data through invertible functions, allowing for exact log-likelihood computation. Parameters are learned by maximizing log-likelihood. In a flow-based generative model, parameters of functions f1...fK are learned by maximizing log-likelihood over a training set. A generative flow for video uses a multi-scale flow architecture, breaking up the latent space into separate variables per timestep. The latent variable zt at each timestep is an invertible transformation of a video frame. In a flow-based generative model, parameters of functions f1...fK are learned by maximizing log-likelihood over a training set. The latent variable zt at each timestep is an invertible transformation of a video frame using a multi-scale architecture. The architecture includes multiple levels encoding information about the frame at different scales, with invertible transformations chosen for simplicity in computing the Jacobian determinant. In flow-based generative models, parameters of functions are learned to maximize log-likelihood. The architecture involves invertible transformations of video frames at different scales, with Jacobian determinants computed efficiently. Techniques like Actnorm, Coupling, SoftPermute, and Squeeze are applied to process input data effectively. The architecture of flow-based generative models involves invertible transformations of video frames at different scales using techniques like Actnorm, Coupling, SoftPermute, and Squeeze. The latent variables are inferred at different levels using a multi-scale architecture composed of flows operating on varying dimensions and scales. The multi-scale architecture described above is used to infer latent variables for each frame of the video. An autoregressive factorization is used for the latent prior, with conditional priors specified for previous timesteps and levels. The architecture uses a factorized Gaussian density with a deep 3-D residual network to predict mean and log-scale. The log-likelihood objective has two parts, with invertible transformations mapping the video frames. The architecture uses invertible transformations to map video frames, with log Jacobian determinants and latent dynamics model contributing to the objective. Parameters are jointly learned for the multi-scale architecture and latent dynamics model. Comparison of realism in generated trajectories is done with SAVP-VAE and SV2P using a real-vs-fake 2AFC Amazon Mechanical Turk. The VideoFlow model is conditioned with the frame at t = 1 to display generated trajectories at t = 2 and t = 3 for different shapes. Experimentation with 3-D convolutional flows was found to be computationally expensive compared to an autoregressive prior. Due to memory limits, only a small number of sequential frames per gradient step could be used during training. Using 2-D convolutions in VideoFlow with autoregressive priors allows for synthesizing long sequences without temporal artifacts. The generated videos display a blue border for conditioning frames and a red border for generated frames. The Stochastic Movement Dataset is modeled using VideoFlow, with each video starting with a shape on a gray background. More videos and results can be found on the provided website. The shape on a gray background randomly moves in one of eight directions with constant speed. A deterministic model averages out all eight possible directions in pixel space. By modeling the position of the shape at each step using only the previous position, VideoFlow maximizes the loglikelihood of the second frame given the first. The model predicts future trajectory of shape in eight random directions with low bits-per-pixel. Compared with SV2P and SAVP-VAE models using real vs fake test on Amazon Mechanical Turk. VideoFlow outperforms baselines in generating plausible \"real\" trajectories at a higher rate using an action-free dataset of a Sawyer robotic arm. Training models to generate 10 target frames conditioned on 3 inputs. VideoFlow, SAVP-VAE, SV2P, and SVG-LP are trained to generate 10 target frames based on 3 input frames. Random temporal patches of 4 frames are extracted for training VideoFlow to maximize log-likelihood. The average bits-per-pixel for the models is reported, ensuring they have seen a total of 13 frames during training. Variational bound of bits-per-pixel is estimated. Realism and diversity are measured using a 2AFC test and mean pairwise cosine distance in VGG perceptual space. VideoFlow outperforms SAVP-VAE and SV2P models in bits-per-pixel on the test set. The high values of bits-per-pixel in the baselines are attributed to their optimization objective, which does not directly optimize the variational bound on the log-likelihood. The study compares 100 videos from stochastic video generation models based on PSNR, SSIM, and VGG metrics. The BAIR robot-pushing dataset is stochastic with many possible futures. The best sample is chosen based on perceptual metrics. To evaluate the model, 100 videos are generated from stochastic models based on PSNR, SSIM, and VGG metrics. The closest video to the ground truth is determined using these metrics, helping to understand if the true future lies within the set of plausible futures. In prior work, researchers effectively remove pixel-level noise in their VideoFlow model to improve video quality by sampling at a lower temperature. This procedure results in higher quality videos at the cost of diversity. In prior work, researchers improved video quality in their VideoFlow model by sampling at a lower temperature, reducing pixel-level noise. The temperature T scales the standard deviation of the latent gaussian distribution P(z_t|z<t). Results with temperature 1.0 and optimal temperature tuned on the validation set using VGG similarity metrics are reported. Low-temperature sampling hurt performance for SV2P and SAVP-VAE. Hyperparameters with disappearing arms perform best for SAVP-VAE. Results for the best performing SAVP models are also reported. Our model with optimal temperature performs well on VGG-based similarity metrics, correlating with human perception and SSIM. It competes with state-of-the-art video generation models on these metrics. PSNR is a pixel-level metric, while VideoFlow models the conditional probability of frame distributions, underperforming on PSNR. Diversity and quality in generated samples are also considered. VideoFlow outperforms diversity values from prior work while being competitive in realism. At T = 0.6, it has the highest fooling rate and is competent with state-of-the-art models. VideoFlow at T = 0.6 has the highest fooling rate and competes with state-of-the-art VAE models in diversity. Lower temperatures result in less random arm behavior and clear background objects, leading to higher realism scores. Higher temperatures make the arm more stochastic, increasing diversity but decreasing realism. Interpolations between different shapes and frames are displayed in Figure 6. The VideoFlow encoder interpolates motion between initial and final frames in a cohesive manner. Different levels of latent representation allow for interpolating background objects and arm motion at varying scales. The VideoFlow encoder interpolates motion between initial and final frames at different scales. Two shapes with fixed type but different size and color are encoded into the latent space, showing smooth interpolation of shape size. Colors are sampled from a uniform distribution during training, with interpolated colors matching those in the training set. Generated videos show frames with and without occlusions using VideoFlow. Our VideoFlow model can detect temporal inconsistencies in future frames, generating 100 frames ahead while maintaining temporal consistency. In the presence of occlusions, background objects become noisier and blurrier, while the arm remains sharp. The latent state z t in our model cannot store information beyond what is present in the frame x t. In future work, the VideoFlow model aims to address the issue of forgetting objects when occluded by incorporating longer memory, such as using recurrent neural networks or memory-efficient backpropagation algorithms. The model is conditioned on 3 frames to detect the plausibility of temporally inconsistent frames in the immediate future. The VideoFlow model aims to address the issue of forgetting objects when occluded by incorporating longer memory. It is conditioned on 3 frames to detect the plausibility of temporally inconsistent frames in the immediate future. The model assigns a decreasing log-likelihood to frames further in the future, showing less likelihood to occur in the 4th time-step. VideoFlow is a flow-based video prediction model inspired by the Glow model for image generation. It introduces a latent dynamical system model for predicting future values, achieving competitive results with VAE models in stochastic video prediction. The model optimizes log-likelihood directly for faster synthesis, making it practical for real-world applications. In future work, VideoFlow plans to incorporate memory to model long-range dependencies and apply the model to challenging tasks using a dataset of i.i.d. observations of a random variable x with an unknown true distribution. The data consists of 8-bit videos with added uniform noise to match its discretization level. Applying low temperature to latent gaussian priors of SV2P and SAVP-VAE decreases VAE model performance. VideoFlow model benefits from low-temperature sampling by balancing noise removal from the background. The VAE models show a tradeoff between performance gain from noise removal and reduced stochasticity of the robot arm. Lowering temperature reduces arm motion stochasticity but hurts performance. A correlation between training progression and video quality is demonstrated. Videos generated by VideoFlow model show high-quality arm structure as bits-per-pixel decrease. Our VideoFlow model learns to model the arm structure and motion with high quality, resulting in high-quality video. Hyperparameters include a learning rate schedule, training for 300K steps with Adam optimizer, and tuning models using VGG cosine similarity metric. Different values of latent loss multiplier are used. Linear decay is applied for the SAVP-VAE model. GAN loss is tuned for SAVP-GAN. For the SAVP-GAN model, tuning involves adjusting the gan loss multiplier and learning rate on a logscale. Comparisons are made between P(X4 = Xt|X<4) and VGG cosine similarity for different time steps. Results show a weak correlation between VGG perceptual metrics and bits-per-pixel. VideoFlow model with 4x parameter reduction remains competitive with SVG-LP on VGG perceptual metrics."
}