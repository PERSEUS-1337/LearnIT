{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a major challenge for neural networks in dynamic data scenarios. A Differentiable Hebbian Consolidation model addresses this issue by incorporating a rapid learning plastic component to the fixed parameters of the softmax output layer. Our proposed model integrates task-specific synaptic consolidation methods to retain learned representations for a longer timescale, reducing forgetting in dynamic data scenarios. It outperforms comparable baselines on various benchmarks without requiring additional hyperparameters. Recent advances in machine learning have shown improvements in solving complex tasks through extensive training on large datasets. However, ML models used in real-world deployment face non-stationarity as data distributions change over time. This poses a challenge as models need to adapt to new data without forgetting previous learning. Catastrophic forgetting occurs in deep neural networks during continual learning, where performance degrades when models are trained with new data. This poses a challenge for DNNs to adapt to distributional changes without forgetting previously learned tasks. In real-world applications, machine learning systems face challenges with continual learning due to concept drift, imbalanced class distributions, and data scarcity, leading to the \"stability-plasticity dilemma\" for neural networks. The \"stability-plasticity dilemma\" is a challenge for machine learning systems, where balancing plasticity for new knowledge and stability for existing knowledge is crucial. Synaptic plasticity in biological neural networks plays a key role in learning and memory, with theories explaining human continual learning inspired by mammalian neocortex consolidation. The general idea for this approach is to consolidate and preserve synaptic parameters important for previously learned tasks, inspired by synaptic consolidation in the mammalian neocortex. The complementary learning system theory suggests that humans store high-level structural information in different brain areas while retaining episodic memories. Recent work on differentiable plasticity has shown that neural networks can achieve task-specific updates of synaptic weights. Recent work on differentiable plasticity has demonstrated that neural networks can be trained with \"fast weights\" that change quickly based on input representations, in addition to the standard \"slow weights\" used for long-term memory. This approach is inspired by synaptic consolidation in the mammalian neocortex and the theory of complementary learning systems in the human brain. Differentiable plasticity involves training neural networks with fast weights that change rapidly based on input, in addition to slow weights for long-term memory. Miconi et al. (2018) found that networks with learned plasticity outperform those with uniform plasticity. Recent approaches address catastrophic forgetting by adjusting synapse plasticity dynamically. This work extends differentiable plasticity to task-incremental continual learning. Differentiable plasticity involves training neural networks with fast weights that change rapidly based on input, in addition to slow weights for long-term memory. This work extends differentiable plasticity to task-incremental continual learning by developing a Differentiable Hebbian Consolidation model that adapts quickly to changing environments and consolidates previous knowledge. The model modifies the traditional softmax layer by augmenting the slow weights in the final fully-connected layer with plastic weights implemented using Differentiable Hebbian Plasticity. The Differentiable Hebbian Consolidation model combines task-specific synaptic consolidation approaches to overcome catastrophic forgetting. It unifies Hebbian plasticity, synaptic consolidation, and CLS theory for rapid adaptation to new data while leveraging compressed episodic memories to remember previous knowledge. The proposed method combines task-specific synaptic consolidation to address catastrophic forgetting in neural networks. It outperforms uniform plasticity networks on benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture. The Imbalanced Permuted MNIST problem is introduced, showing the effectiveness of plastic networks with task-specific consolidation methods. Hebbian learning theory suggests that continual learning in humans is attributed to weight plasticity, modifying the strength of existing synapses. The modification of synaptic strength through weight plasticity is based on Hebbian learning theory, where correlated activation of neurons strengthens connections. Incorporating fast weights in neural networks enables one-shot and few-shot learning. Incorporating fast weights in neural networks enables one-shot and few-shot learning. Munkhdalai & Trischler (2018) and Rae et al. (2018) proposed models using fast weights to improve learning, with different approaches such as Hebbian learning-based associative memory and a Hebbian Softmax layer. Miconi et al. (2018) introduced differentiable plasticity to optimize synaptic connection plasticity using SGD. Our work introduces a method that optimizes synaptic connection plasticity using SGD, incorporating fast weights in neural networks for one-shot and few-shot learning. This approach has been demonstrated on recurrent neural networks for pattern memorization and maze exploration tasks, focusing on meta-learning rather than continual learning challenges. Our work introduces a method that optimizes synaptic connection plasticity using SGD, incorporating fast weights in neural networks for one-shot and few-shot learning. This work leverages two strategies to overcome catastrophic forgetting: Task-specific Synaptic Consolidation and CLS Theory. The hippocampus performs rapid learning and individuated storage to memorize new instances or experiences. Task-specific synaptic consolidation inspired by notable works helps overcome catastrophic forgetting in continual learning. Regularization approaches estimate the importance of each parameter or synapse for retaining memories. When learning new tasks, a regularizer is added to adjust plasticity and prevent changes to important parameters of previously learned tasks. The regularization strategies focus on computing the importance of network parameters to retain memories. Regularization strategies focus on computing the importance of network parameters to retain memories. In Elastic Weight Consolidation (EWC), importance of each parameter is computed using an approximated Fisher information matrix. An online variant of EWC was proposed to improve scalability. Synaptic Intelligence (SI) is an online method for computing parameter importance based on cumulative changes in individual synapses. Our work is inspired by CLS theory, a computational framework for memory representation, and Memory Aware Synapses (MAS) measures sensitivity of the learned function to parameter perturbations. Various approaches based on CLS principles involve pseudo-rehearsal and episodic replay. In our work, we focus on neuroplasticity techniques inspired by CLS theory to address catastrophic forgetting. Previous methods include exact replay, episodic replay, and generative replay for mitigating forgetting in continual learning tasks. iCaRL utilizes rehearsal and regularization with an external memory for storing exemplar patterns from old task data. Neuroplasticity techniques inspired by CLS theory aim to alleviate catastrophic forgetting in continual learning. This involves utilizing slow and fast weights in synaptic connections to store long-term knowledge and temporary associative memory. Recent research has explored replacing soft attention mechanisms with fast weights in RNNs, incorporating Hebbian Softmax layers, and augmenting slow weights with fast weights matrices in neural networks. Neuroplasticity techniques inspired by CLS theory aim to alleviate catastrophic forgetting in continual learning. This involves utilizing fast weights in synaptic connections for rapid learning on simple tasks or meta-learning over a distribution of tasks. The Hebbian Softmax layer adjusts its parameters for one-shot and few-shot learning, switching between Hebbian and SGD updates based on task complexity. In continual learning setups, the fast weights memory storage effect diminishes as the network learns from numerous examples per class. The goal is to metalearn a local learning rule for continual learning. The model focuses on metalearning a local learning rule for fast weights using fixed weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights (\u03b8) and a Hebbian plastic component with a plasticity coefficient (\u03b1) and Hebbian trace. The \u03b1 adjusts the Hebb's magnitude, while the Hebbian traces accumulate mean hidden activations for each target label in the mini-batch. The final hidden layer h in the model computes post-synaptic activations of neurons using pre-synaptic activations. The softmax function is applied to obtain predicted probabilities \u0177. The parameter \u03b7 dynamically adjusts the learning rate for plastic connections and acts as a decay term for Hebb to prevent instability. The parameter \u03b7 acts as a decay term for the Hebb to prevent instability in the network. The network parameters are optimized by gradient descent during training on different tasks. In standard neural networks, weight connections have fixed weights, equivalent to setting plasticity coefficients \u03b1 = 0. The Hebbian weight update is calculated based on the activation levels of neurons. Our model utilizes Hebbian weight updates and hidden activations to make predictions during test time. The Hebbian traces are only updated during training, leading to better initial representations and continual learning of tasks. The model utilizes Hebbian weight updates and hidden activations for predictions during test time, leading to better initial representations and continual learning of tasks. Fast learning with a plastic weight component improves test accuracy, while selective consolidation into a stable component protects old memories, enabling the model to learn to remember over a range of timescales. The advantage of DHP Softmax over external memory is highlighted. The advantage of DHP Softmax is its simplicity in implementation, requiring no additional space or computation compared to external memory. This allows for easy scalability with increasing tasks. The Hebbian update visualization shows rapid learning and sparse parameter updates for memory storage. The plastic component in DHP Softmax quickly stores memory traces for recent experiences without interference, forming a compressed episodic memory. This method improves learning of rare classes and speeds up binding of class labels without additional hyperparameters. In Appendix B, a sample implementation of the DHP Softmax using PyTorch is provided. Hebbian Synaptic Consolidation is achieved by updating the synaptic importance parameters of the network in an online manner, following existing regularization strategies such as EWC, Online EWC, SI, and MAS. The network parameters represent the weights of connections between pre-and post-synaptic activities of neurons. The model adapts task-specific consolidation approaches by not computing synaptic importance parameters on the plastic component of the network. During training the first task, the synaptic importance parameter is set to 0 for all methods except for SI, which estimates it while training. The plastic component of the softmax layer can help reduce catastrophic forgetting. The plastic component in the softmax layer of the model helps alleviate catastrophic forgetting by adjusting the plasticity of connections. Comparing this approach to other methods like Online EWC, SI, and MAS, the model increases capacity by adding plastic weights and slow weights to the softmax output layer. This demonstrates that the mitigation of forgetting is not solely due to increased model capacity. The model's increased capacity from plastic weights helps mitigate forgetting during sequential task learning. Testing on various benchmarks, including Permuted MNIST and Split MNIST, evaluates the model's performance. Memory retention and flexibility are assessed by measuring test performance on the first and most recent tasks, as well as using the backward transfer metric. The study evaluates the model's performance on various benchmarks, measuring memory retention and flexibility through test performance on the first and most recent tasks. Consolidation methods like Online EWC, SI, and MAS are compared to establish a baseline for task-specific learning. In the benchmarks, the hyperparameters for consolidation methods (EWC, SI, MAS) remain consistent with or without DHP Softmax. The plastic components are not regularized, and MNIST pixels are permuted differently for each task. Input distribution changes between tasks, indicating concept drift. Multi-layered perceptron (MLP) networks with two hidden layers are used for Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The study utilized a network with two hidden layers and a cross-entropy loss. The plastic component had a \u03b7 value of 0.001. A comparison was made between the network with DHP Softmax and a fine-tuned vanilla MLP network, showing improved performance in alleviating catastrophic forgetting. The study compared the performance of a network with DHP Softmax to one without using task-specific consolidation methods. DHP Softmax maintained higher test accuracy during sequential task learning. An ablation study examined network structural parameters and Hebb traces for further interpretability. In the Permuted MNIST benchmark, synaptic plasticity increases rapidly in task T1, then decays after the 3rd task to prevent interference. Within each task from T4 to T10, synaptic plasticity follows a similar pattern. The Hebb trace maintains a memory of recent activity without runaway positive feedback. The Frobenius Norm of \u03b1 shows plasticity coefficients growing within each task, indicating the network leverages structure in the plastic component. Gradient descent and backpropagation are used for meta-learning to tune structural parameters. Introducing the Imbalanced Permuted MNIST problem, where tasks have imbalanced distributions with artificially removed training samples based on random probability. This benchmark addresses class imbalance and concept drift. The benchmark addressed class imbalance and concept drift by introducing the Imbalanced Permuted MNIST problem. DHP Softmax achieved 80.85% accuracy after learning 10 tasks with imbalanced class distributions, showing a 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces played a significant role in handling rare classes encountered infrequently. DHP Softmax with MAS achieves a 0.04 decrease in BWT, resulting in an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. The MNIST dataset was split into 5 binary classification tasks, with disjoint output spaces between tasks. The network used was an MLP with two hidden layers of 256 ReLU nonlinearities each. In experiments using an MLP network with two hidden layers of 256 ReLU nonlinearities, a cross-entropy loss, and an initial \u03b7 value of 0.001, DHP Softmax achieved a 7.80% improvement in test performance compared to a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and led to higher average test accuracy across all tasks, especially the most recent one, T5. Continual learning on 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 using a CNN architecture. MNIST, notMNIST, and FashionMNIST datasets are zero-padded to 32x32 size and replicated 3 times for grayscale images. Initial \u03b7 parameter set to 0.0001 for training the network with mini-batches. After training the network with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task, DHP Softmax plus MAS decreased BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. SI with DHP Softmax achieved the best performance with an average test accuracy of 81.75% and a BWT of -0.04 after learning all five tasks. The final average test performance after learning all tasks is summarized in Table 1. Adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. This allows new information to be learned without interference, aiding in generalization across experiences. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax automatically scales the magnitude of plastic connections, allowing for generalization across experiences. DHP Softmax showed significant improvement compared to traditional softmax layers, without introducing additional hyperparameters. The DHP Softmax model, combined with consolidation methods like EWC, SI, or MAS, improves the model's ability to prevent catastrophic forgetting when learning multiple tasks sequentially. This approach outperforms other consolidation methods on datasets like Split MNIST and 5-Vision Datasets Mixture. The initial \u03b7 value for DHP Softmax required minimal tuning effort. The combination of DHP Softmax and MAS outperforms other methods on Permuted MNIST and Imbalanced Permuted MNIST benchmarks by computing synaptic importance parameters layer by layer. The model shows lower negative BWT and higher test accuracy across all benchmarks, indicating superior performance compared to methods without DHP. Hebbian plasticity improves test accuracy in neural networks, reducing catastrophic forgetting in dynamic environments. Continual synaptic plasticity aids learning from limited data and scaling at long timescales, promoting continual learning in DNNs. This work suggests exploring Hebbian consolidation for gradient descent optimization in learning and memory tasks. In a continual learning setup, a neural network model is trained on a sequence of tasks T 1:nmax, each with its own task-specific loss to prevent catastrophic forgetting. The model learns data in sequential chunks enumerated by tasks, receiving input data (Xn) and corresponding label data (Yn) for each task. After training on a sequence of tasks with task-specific loss to prevent forgetting, the model learns an approximated mapping f to the true underlying function. The learned f maps new inputs to target outputs for all tasks learned so far, even if the classes in each task are different. Experiments were conducted using Nvidia Titan V or Nvidia RTX 2080 Ti, training with mini-batches of size 64 and plain SGD with a learning rate of 0.01. For training on a sequence of tasks, the model uses a mini-batch size of 64 and plain SGD with a learning rate of 0.01. Training continues for at least 10 epochs with early stopping if validation error does not improve for 5 epochs. Hyperparameters for Permuted MNIST experiments include regularization values for different consolidation methods. For synaptic consolidation methods, hyperparameters were optimized through grid search using specific values for each method. For Online EWC, \u03bb values ranged from 10 to 400, for SI -\u03bb from 0.01 to 1.0, and for MAS -\u03bb from 0.01 to 2.0. Training on Imbalanced Permuted MNIST involved removing samples from each class for each task. In the Imbalanced Permuted MNIST problem, training samples were artificially removed from each class in the original MNIST dataset based on random probabilities. Different removal probabilities were drawn for each class and task from a uniform distribution. The distribution of classes in each dataset for tasks 1 to 10 is shown in Table 2. For the Imbalanced Permuted MNIST experiments in Figure 5, regularization was applied. The total number of samples for each task ranged from 26012 to 35622. For the Imbalanced Permuted MNIST experiments in Figure 5, regularization hyperparameters were set for different task-specific consolidation methods: \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. Grid search was conducted to find the best hyperparameter combinations for each method. For Split MNIST experiments, regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 1.5 for MAS. Grid search was performed to determine the optimal hyperparameter combinations for each method. In a grid search for binary classification tasks, values of \u03bb were tested for Online EWC, SI -\u03bb, and MAS -\u03bb. The network was trained on a sequence of 5 tasks with mini-batches of size 64 using plain SGD. The Vision Datasets Mixture benchmark includes MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 datasets. The notMNIST dataset contains font glyphs for letters 'A' to 'J'. The FashionMNIST dataset includes grayscale images of clothing articles in 10 categories, with 60,000 images for training and 10,000 for testing. SVHN dataset consists of digits '0' to '9' from Google Street View images, with 73,257 training and 26,032 testing color images. CIFAR-10 dataset has 50,000 training and 10,000 testing color images from 10 categories. The CNN architecture for the dataset includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling operations. A multi-headed approach was used due to different class definitions between datasets. In this benchmark, the model has a trainable \u03b7 value for each connection in the final output layer, improving stability of optimization and convergence. Using separate \u03b7 parameters for each connection allows for modulation of plasticity rates. Using a single \u03b7 value across all connections led to instability in optimization on certain tasks. The regularization hyperparameter values for task-specific consolidation methods were set as \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. A random search was conducted to find the best hyperparameter combination for each method, with specific ranges tested for each method. Sensitivity analysis was performed on the Hebb decay term \u03b7. The sensitivity analysis on the Hebb decay term \u03b7 showed that setting it to low values led to the best performance in alleviating catastrophic forgetting during continual learning tasks. The sensitivity analysis on the \u03b7 parameter for the Split MNIST problem showed that low values led to better performance in preventing catastrophic forgetting during continual learning tasks. The average test accuracy for MNIST-variant benchmarks was presented in Table 4. The PyTorch implementation of the DHP Softmax model initializes fixed weights with He initialization and sets the learning rate of plastic connections. It adds a compressed episodic memory to the final output layer of a neural network. The DHP Softmax model, with compressed episodic memory in the final output layer, outperforms Finetune in class-incremental learning tasks on CIFAR-10 and CIFAR-100 datasets. The model was trained sequentially on tasks from CIFAR-100 after initial training on CIFAR-10. Test accuracies were reported after learning the final task in the sequence. In class-incremental learning tasks, DHP Softmax outperforms Finetune on CIFAR-10 and CIFAR-100 datasets. Test accuracies show DHP Softmax performs as well or better than training from scratch."
}