{
    "title": "B1xFhiC9Y7",
    "content": "Predicting structured outputs like semantic segmentation requires expensive per-pixel annotations for training convolutional neural networks. To address the challenge of generalizing to new domains without annotations, a domain adaptation method is proposed. This method involves learning discriminative feature representations of patches based on label histograms in the source domain and using an adversarial learning scheme to refine the features. By using adversarial learning, the framework refines feature representations in target patches to align with source distributions, improving semantic segmentation performance. The approach integrates global alignment with patch-level alignment and achieves state-of-the-art results on various benchmark datasets. Recent deep learning methods have advanced vision tasks like object recognition and semantic segmentation, relying on large-scale annotations for supervision. Domain adaptation methods have been developed to improve generalization of learned models for semantic segmentation tasks in different test domains. While numerous methods exist for image classification, there is still room for improvement in domain adaptation for pixel-level prediction tasks like semantic segmentation. This is crucial as annotating ground truth data for pixel-level predictions is costly. Domain adaptation is essential for pixel-level predictions due to the high cost of annotating ground truth data. Existing methods align distributions between source and target domains using adversarial learning, but global statistics may differ significantly between domains. One way to improve domain adaptation for pixel-level predictions is to align patch distributions through adversarial learning, considering patches that are more likely to be shared across domains regardless of their location. This approach addresses misalignment issues caused by differences in camera pose or field of view. To improve domain adaptation for pixel-level predictions, align patch distributions through adversarial learning. Consider patches likely shared across domains, regardless of location, to address misalignment caused by camera pose or field of view. Adopt a similar approach to learning discriminative representations for patches, relaxing high-variation problems among them. Use learned representations as a bridge for patch-level alignment against global alignment. We align patch distributions through adversarial learning to improve domain adaptation for pixel-level predictions. By utilizing two adversarial modules, we align global and patch-level distributions between domains. Pixel-level annotations are used to extract label histograms for patch-level representation, followed by K-means clustering to group patch representations into clusters. To improve domain adaptation for pixel-level predictions, patch representations are grouped into clusters using K-means clustering. An adversarial loss is used to align feature representations of target patches with the distribution of source patches in a clustered space. This representation learning is guided by label histograms, different from existing methods. In experiments, domain adaptation is performed for pixel-level road-scene image segmentation under various settings, including synthetic-to-real and cross-city scenarios. Extensive ablation studies validate the proposed framework, showing favorable performance in accuracy and visual quality through global and patch-level alignments. The proposed framework for structured output prediction shows favorable performance in accuracy and visual quality through global and patch-level adversarial learning modules. Additionally, a method to learn discriminative representations guided by label histograms of patches is developed, leading to improved patch-level alignment. The adaptation method outperforms various baselines in experiments on pixel-level road-scene image segmentation. In this work, the proposed adaptation method outperforms baselines and state-of-the-art methods on semantic segmentation. Domain adaptation methods for image classification and pixel-level prediction tasks are discussed, along with algorithms for learning disentangled representations. Domain adaptation approaches align feature distributions between domains using hand-crafted features or deep architectures. Recent algorithms utilize deep architectures to learn domain-invariant features, adopting adversarial learning schemes and minimizing Maximum Mean Discrepancy. Variants have been developed with different classifiers and loss functions. Some work focuses on enhancing feature representations through pixel-level transfer and domain separation. Domain adaptation for structured pixel-level predictions, particularly in semantic segmentation for road-scene images, is a less studied area compared to image classification tasks. To address domain adaptation in semantic segmentation for road-scene images, adversarial networks are used to align global feature representations across domains. A category-specific prior from the source domain is transferred to the target distribution. Instead of using inconsistent priors, the CDA method BID36 applies an SVM classifier to capture label distributions on superpixels for training the adapted model on the target domain. In contrast to previous domain adaptation methods for structured output, we propose learning discriminative representations for patches to aid in patch-level alignment, rather than global distribution alignment and class-specific priors. Additionally, an object prior from Google Street View is utilized to assist in aligning static objects. Our framework focuses on learning patch-level representations for alignment without the need for additional priors/annotations. It aims to aid in the alignment process by utilizing learned representations. Additionally, we emphasize learning a latent disentangled space for better understanding in tasks such as facial recognition, image generation, and view synthesis. The framework focuses on learning patch-level representations for alignment without additional priors/annotations, aiming to aid in the alignment process by utilizing learned representations. It emphasizes learning a latent disentangled space for better understanding in tasks like facial recognition, image generation, and view synthesis. Various methods have been proposed to learn interpretable representations of images based on pre-defined factors, such as pose and lighting, for rendering 3D images. These methods use generative adversarial networks (GANs) with auxiliary classifiers conditioned on factors like image labels and attributes. Our proposed domain adaptation framework focuses on learning discriminative representations for patches to aid in aligning distributions across domains without the need for pre-defined factors. The framework utilizes available label distributions as a disentangled factor, enhancing the alignment process for structured output prediction. Our goal is to align predicted output distribution of target data with source distribution by using supervised learning on source data and adversarial loss. We incorporate a classification loss to learn patch-level discriminative representations from the source output distribution. Another adversarial loss is used for aligning patch-level distributions of target data. To align the predicted output distribution of target data with the source distribution, adversarial loss is employed. This includes supervised learning on source data and utilizing global and patch-level adversarial loss functions. The goal is to push the target distribution closer to the source distribution. The baseline model includes supervised cross-entropy loss and an output space adaptation module using global alignment. The structured output is predicted by a fully-convolutional network with the loss summed over spatial map indices and categories. The structured output is predicted by a fully-convolutional network with the loss summed over spatial map indices and categories. DISPLAYFORM0 where O s = G(I s ) \u2208 (0, 1) is the predicted output distribution through the softmax function and is up-sampled to the size of the input image. For the adversarial loss L g adv , G and a discriminator D g optimize a min-max problem to distinguish source and target images. The text proposes performing patch-level domain alignment to find transferable structured output representations shared across source and target images from smaller patches. The network architecture consists of a generator G and a categorization module H for learning discriminative patch representations. Symbols denote source and target representations in a clustered space. The proposed method involves patch-level domain alignment to create transferable structured output representations shared between the source and target images. This is achieved by guiding patches from the target domain to adapt to a disentangled space of source patch representations through adversarial objectives. Learning discriminative representations is essential for this process, typically using class labels or pre-defined factors as supervisory signals. In this work, per-pixel annotations in the source domain are used to construct a semantically disentangled space of patch representations by utilizing label histograms for patches as the disentangled factor. Patches are randomly sampled from source images, spatial label histograms are extracted, and concatenated into a vector. The study utilizes per-pixel annotations in the source domain to create a semantically disentangled space of patch representations. Patches are sampled from source images, and spatial label histograms are generated and clustered using K-means. A classification module is added after the predicted output to simulate the label histogram construction process and learn a discriminative representation. The study utilizes per-pixel annotations to create a clustered space of patch representations using the softmax function. The learning process involves aligning target patches to the clustered space through adversarial loss. The goal is to align patches regardless of their location in the image. The study aligns target patches in an image using adversarial loss, reshaping data for classification by a discriminator. The process aims to align patches regardless of their location in the image. NETWORK OPTIMIZATION involves alternating optimization steps between updating the discriminators Dg and Dl, and updating the network G and H while keeping the discriminators fixed. The discriminator Dg is trained to distinguish between the source and target output distributions, while the discriminator Dl classifies feature representations from the source or target domain. The goal of updating the Network G and H is to align the target distribution with the source distribution using optimized discriminators Dg and Dl. This involves minimizing a combination of supervised and adversarial loss functions to enhance feature representations. During testing, only G is required, updating feature representations through back-propagation. The discriminator Dg uses a spatial map O as input with 5 convolution layers. Dl utilizes a K-dimensional vector with 3 fully-connected layers. The generator network G consists of 3 fully-connected layers with leaky ReLU activation and channel numbers {256, 512, 1}. A categorization module H is added to the output prediction O using an adaptive average pooling layer to generate a spatial map. This map is then fed into two convolution layers to produce a feature map F with channel number K. The proposed architecture includes a generator network with fully-connected layers and a categorization module. Implementation details involve using PyTorch on a Titan X GPU for training discriminators and generators with specific optimizers and learning rates. Ablation study results on GTA5-to-Cityscapes using the ResNet-101 network are also presented. The proposed framework for domain adaptation on semantic segmentation involves training the model with specific loss functions and hyperparameters. The architecture includes a generator network and a categorization module, with details on training using PyTorch on a Titan X GPU. Ablation study results on GTA5-to-Cityscapes using the ResNet-101 network are also discussed. The proposed framework for domain adaptation on semantic segmentation involves an extensive ablation study on the GTA5-toCityscapes scenario. The method outperforms state-of-the-art approaches on various benchmark datasets and settings, including synthetic-to-real and cross-city scenarios. Experiments involve adapting datasets like GTA5 BID27 and SYNTHIA BID28 to Cityscapes BID5, showcasing the effectiveness of the approach. To address domain gaps, Cityscapes images are adapted to the Oxford RobotCar BID23 dataset with rainy scenes. 10 sequences are manually selected, with 7 for training and 3 for testing. 895 training images and 271 annotated images for per-pixel semantic segmentation are used for evaluation. The annotated ground truth will be publicly available. In an ablation study on the GTA5-to-Cityscapes scenario, different loss functions and design choices in the proposed framework are evaluated using intersection-over-union (IoU) ratio as the metric. Adding disentanglement without alignments improves performance, demonstrating enhanced feature representation. The study evaluates the effectiveness of patch-level alignment in enhancing feature representation, showing that both Ld and Lladv losses are necessary for this process. Removing either results in a performance decrease of 1.9% and 1.5% respectively. The study emphasizes the importance of patch-level alignment for enhancing feature representation. The reshaping process in the clustered space allows for effective alignment of patches with similar representations, resulting in a 2.4% increase in IoU performance. Visualization of feature representations using t-SNE in FIG1 further supports this concept. In FIG1, t-SNE visualization BID34 shows patch-level features in clustered space with adaptation, resulting in well-overlapping representations. Comparison with state-of-the-art algorithms includes synthetic-to-real and cross-city scenarios, with experimental results adapting GTA5 to Cityscapes in TAB1 using VGG-16 architecture. The upper group uses VGG-16 architecture for feature and output space alignments, showing favorable performance against state-of-the-art methods. The bottom group utilizes ResNet-101 and achieves a 1.8% IoU improvement on 14 out of 19 categories. Results for adapting SYNTHIA to Cityscapes in TAB2 also show significant improvements compared to existing methods. Improvements are observed in comparison with state-of-the-art methods. Visual comparisons are shown in Figure 5, with more results in the appendix. Adapting between real images across different cities and conditions is highlighted, with a challenge case of adapting Cityscapes to Oxford RobotCar. The proposed method often generates segmentation with more details and less noisy regions. In this paper, a domain adaptation method for structured output is presented, combining global and patch-level alignments to improve segmentation results. The proposed method achieves a mean IoU of 63.6%, outperforming the authors' code by 1.4%. The approach involves output space adaptation for global alignment and learning discriminative representations of patches for patch-level alignment. An adversarial learning scheme is used to push the target patch towards the clustered space of source patches. The proposed method combines global and patch-level alignments to improve segmentation results by pushing target patch distributions closer to the source ones through an adversarial learning scheme. Extensive experiments validate its effectiveness under various challenges, showing favorable performance compared to existing algorithms. Training the model involves randomly sampling one image from each domain in a training iteration with a batch size of 1. Optimization strategy details are provided in Section 3.4, and TAB3 displays image and patch sizes during training and testing. The model incorporates an entropy regularization using BID21 to push target feature representation closer to source clusters. This approach achieves an IoU of 41.9%, lower than the patch-level adversarial alignment at 43.2%. Our model learns discriminative representations for target patches by aligning them closer to the source distribution in a clustered space guided by label histogram. Visualization in FIG3 demonstrates high similarity between source and target patches, showcasing the effectiveness of patch-level alignment. TAB4 presents complete results. In TAB4, the effectiveness of the proposed patch-level alignment is demonstrated by adapting Cityscapes to Oxford RobotCar. A comparison is made with models without adaptation and the output space adaptation approach BID31. Visual comparisons for different scenarios are provided in FIG4, 8, and 9 to 11, showing that the proposed method often yields better segmentation outputs with more details and less noisy regions. The proposed method demonstrates better segmentation outputs with more details and less noise in adapted segmentations for the GTA5-to-Cityscapes setting. Target images are shown before adaptation, output space adaptation BID31, and the proposed method. The proposed method shows improved segmentation results with fewer artifacts in adapted segmentations for the SYNTHIA-to-Cityscapes setting. Target images are displayed before adaptation, output space adaptation BID31, and the proposed method."
}