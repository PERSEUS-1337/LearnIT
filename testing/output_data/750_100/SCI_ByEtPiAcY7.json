{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complex nature of deep networks makes it impossible to explain hidden features in a way that humans can understand. However, this paper proposes a knowledge extraction method using \\textit{M-of-N} rules to address this issue. The proposed knowledge extraction method uses \\textit{M-of-N} rules to map the complexity/accuracy landscape of hidden features in a Convolutional Neural Network (CNN). Results show an optimal trade-off between comprehensibility and accuracy, with rules in the first and final layers being highly explainable, while those in the second and third layers are less so. This sheds light on the feasibility of rule extraction from deep networks. The results highlight the feasibility of rule extraction from deep networks and emphasize the value of decompositional knowledge extraction for explainability in Artificial Intelligence. There is a growing interest in explainable AI due to the lack of explainability in neural network models, particularly deep networks that rely on distributed representations. Knowledge extraction aims to increase the explainability of neural networks by revealing the implicit knowledge learned in their weights. This involves translating trained neural networks into symbolic rules or decision trees, similar to those in symbolic AI and machine learning. Unlike hand-picked features in traditional ML, distributed representations in neural networks may rely on weak statistical correlations that are not easily identifiable by humans. Rule extraction techniques have been developed over decades to translate trained neural networks into symbolic rules or decision trees, similar to those in symbolic AI and machine learning. The major issue with rule extraction is the complexity of the extracted rules, whether generated from network parameters or network behavior. Extractive summarization: Knowledge extraction from neural networks can be challenging due to the distributed representations found in the networks, where important concepts are represented by patterns of activity over many neurons. This distributed nature is believed to play a significant role in the capabilities of neural networks. The distributed nature of neural networks, identified as a fundamental property of connectionism, has led to the conclusion that explaining latent features using symbolic knowledge extraction is a dead end. Distillation methods are suggested instead for improving robustness, although their efficacy is questioned. Other practical approaches focus on guarantees of network behavior or visualizations to explain individual features. In this paper, a method is developed to empirically examine the explainability of latent variables in neural networks by using rule extraction to analyze the relationship between rule complexity and network behavior. The study examines the explainability of latent variables in neural networks through rule extraction. Different layers in a 4-layer CNN trained on fashion MNIST show varying accuracy in rule extraction. A 'critical point' on the rule extraction landscape reveals an ideal M-of-N rule for each latent variable. The complexity and accuracy of rules depend on the variable being described, with explainability trends differing between layers and architectures. The study explores rule extraction in neural networks, showing varying accuracy in different layers of a 4-layer CNN trained on fashion MNIST. Rules extracted from fully connected layers with complexities over 0.4 were more complex. Rules with near 0% error were found in the first and final layers, while the second and third layers had up to 15% error. Experimental results of the rule extraction process are presented in different sections of the study. The experimental results of rule extraction process for accuracy/complexity mapping in neural networks are discussed. Knowledge extraction methods like KBANN and algorithms generating binary trees for rule extraction are mentioned. The more recent algorithms for rule extraction in neural networks select an M-of-N rule based on maximum information gain with respect to the output. These methods treat the model as a black box and can be queried to generate data for rule extraction. Other extraction methods combine pedagogical and decompositional approaches. Some methods for rule extraction in neural networks combine pedagogical and decompositional approaches, while others use alternative visually oriented techniques. Most techniques focus on shallow networks or input/output relationships, with challenges in explaining deep network features due to multiple hidden layers. The use of decompositional techniques to explain deep network features may seem impractical due to complex rule hierarchies. However, some layers of a deep network can have explainable rules that clarify the network's behavior in terms of certain features. The possibility of rule extraction as a tool for explaining network models and understanding latent features is explored through logical rules in programming. A logical rule consists of an implication A \u2190 B, where A is the head and B is the body of the rule. Disjunctions in the body can be represented by multiple rules with the same head. When explaining neural networks using rules, literals represent neuron states. For binary neurons, X = True if x = 1, and X = False if x = 0. For continuous activation neurons, X = True if x > a, and X = False otherwise, where X is shorthand for x > a. Latent variables in neural networks are often poorly described by a single conjunctive rule. In neural networks, latent variables are often poorly described by a single conjunctive rule. M-of-N rules soften the constraint by requiring only M variables to be true for a specific value of M < N. This approach is commonly used in rule extraction. M-of-N rules offer a compact representation that reflects input/output dependencies in neural networks. They are a subset of propositional formulas and share structural similarity with neural networks, acting as 'weightless perceptrons'. M-of-N rules are represented by perceptrons with output neurons as the head and visible neurons as the body. Setting bias of output neuron to M and weights of input neurons to 1 or -1 encodes the rule in a neural network. These rules, forgotten in knowledge extraction, are brought back for explainability in neural networks with continuous activation values. To define literals for rule extraction in neural networks with continuous activation values, splitting values are chosen for each neuron based on information gain. The splitting values are selected to maximize the decrease in entropy of the network outputs on test examples. Input literals are then generated from the inputs to the target neuron. The input literals are generated from the inputs to the target neuron by choosing splits that maximize information gain. Each target literal in a layer will have its own set of input literals, corresponding to the same input neurons but with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches. Only the neuron with the optimal split that maximizes information gain with respect to the network output is tested, resulting in a single rule. In rule extraction, the focus is on accuracy and comprehensibility. Accuracy is defined in terms of the difference between rule predictions and network outputs. Neurons in a neural network determine the truth of literals, allowing for the computation of rules. In rule extraction, accuracy is measured by comparing rule predictions to network outputs. Neurons in a neural network determine the truth of literals, enabling rule computation. Comprehensibility is subjective, based on rule complexity. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF). For an M-of-N rule, complexity is M N M, normalized relative to a maximum complexity. The maximum complexity is calculated using the ceiling function with N possible input variables. To control for growth, the logarithm is taken. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), normalized relative to a maximum complexity. For an M-of-N rule, complexity is M N M. The maximum complexity is calculated using the ceiling function with N possible input variables. To control for growth, the logarithm is taken. As an example, a simple perceptron with a bias of 1 and two binary visible units with specific weights is considered. The rule h = 1 \u21d0\u21d2 1-of{x 1 = 1, \u00ac(x 2 = 1)} is analyzed, resulting in an error of 0.25. A 1 \u2212 of \u2212 2 rule is identified as the most complex rule possible for 2 variables. The loss function for a rule R is defined as a weighted sum with a parameter \u03b2 determining the trade-off between soundness and complexity. By using a brute force search procedure with various values of \u03b2, the relationship between the allowed complexity of a rule and its maximum accuracy can be explicitly determined. For \u03b2 = 0, the rule with the minimum loss will be the one with minimum error regardless of complexity, while for large \u03b2 values, the rule with the minimum loss will have 0 complexity, such as a 1-of-1 rule or trivial rules that always predict true or false. The search procedure involves generating splits for neurons based on input weights, searching through M-of-N rules to minimize the loss function L(R), and reordering variables by weight magnitude for optimization. The search procedure involves generating splits for neurons based on input weights to maximize information gain. Neurons with n input neurons have O(2^n) possible M-of-N rules, making exhaustive search intractable. The search procedure for generating splits for neurons based on input weights is intractable due to the large number of possible M-of-N rules. The assumption is made that the most accurate rules use the literals corresponding to neurons with the strongest weights. Ordering the literals by information gains instead of weights may not be necessary based on high accuracy results from experimental data. The algorithm for rule extraction was implemented in Spark and run on IBM cloud services to handle a large number of test examples and input neurons efficiently. The accuracy of the extracted rules was evaluated using examples from the training set, not the test set, to measure accuracy with respect to the network's output. By running the search in parallel, we can map the accuracy/complexity graph for about 50 hidden neurons in the second and third layer in several hours. Increasing the number of examples used in the accuracy calculation greatly increases the time taken and for this reason we only use 1000 examples. To demonstrate the procedure, we will examine the extraction process for the first hidden feature in the CNN trained on the fashion MNIST data set. In the CNN trained on the fashion MNIST dataset, the extraction process for the first hidden feature involves computing activations of neurons and predicted labels. Neurons in the first layer correspond to 5x5 patches of the input, with the optimal splitting value determined by information gain. Neuron 96, with an information gain of 0.015 at split value 0.0004, corresponds to a specific image patch. Neuron 96 corresponds to the image patch centered at (3, 12) with a split value of 0.0004. Input splits are chosen for maximum information gain with respect to variable H. Optimal M-of-N rules are determined for explaining H with different error/complexity tradeoffs. Three different rules are extracted as complexity penalty increases. The optimal M-of-N rules for explaining neuron 96 are visualized in Figure 1. The rules range from simple to complex, with a 5-of-13 rule having a 0.025 error rate. Adding a penalty to complexity changes the optimal rule. Training a feed forward network with a single hidden layer of 100 nodes on the DNA promoter dataset shows an exponential relationship between complexity and error, suggesting an ideal tradeoff. In training a feed forward network with a single hidden layer of 100 nodes on the DNA promoter dataset, an ideal complexity/error tradeoff is suggested. The output layer shows that a specific rule gives 100% fidelity to the network. The rules for the hidden layer are defined by information gain, with each literal in the rule described by an M-of-N rule extracted from the input layer. The network's output can be predicted by a set of hierarchical rules, with errors propagating through layers due to different splits chosen for the same layer. To replace the network with hierarchical rules, a single set of splits for each layer must be decided by moving down one layer at a time and selecting. To provide an idealized complexity/error curve for rule extraction with M-of-N rules, experiments are conducted layer by layer independently. This approach allows for examining the usefulness of M-of-N rule extraction and evaluating other extraction algorithms. Testing the rule extraction landscape of a neural network trained on a practical example helps in understanding the circumstances where M-of-N rule extraction might be beneficial. In a practical example, layerwise rule extraction was tested on a CNN trained on fashion MNIST in tensorflow. The CNN had a standard architecture with convolutional and max pooling layers, followed by a fully connected layer. Rules were extracted and tested against the network using random inputs from the training data. In the third layer, 50 features were randomly chosen for testing with a limit of 1000 literals. The final layer output was tested with 10 one-hot neurons, each undergoing a rule searching procedure. The search procedure was repeated for 5 different values of \u03b2, producing 5 sets of extracted rules with varying error/complexity trade-offs. The complexity/error trade-off for rules extracted from each layer varied. The first and final layers produced accurate rules with minimal errors, while the second and third layers showed a trade-off between accuracy and complexity. The third layer had the highest complexity with no minimum error achieved. The optimal accuracy/complexity tradeoff is not solely determined by the number of input nodes. Despite varying input node numbers, the third layer performs similarly to the second layer. The final layer provides more accurate rules with less complexity compared to the first layer. The results show a critical point where error increases rapidly as complexity penalty rises, indicating a natural set of rules. Current rule extraction algorithms do not consider complexity in optimization. This paper introduces rule complexity as a key factor in extraction algorithms, unlike current methods that do not consider complexity in optimization. Empirical evaluation is crucial for validating extraction algorithms, highlighting both limitations and potential. In some cases, like in CNN layers, simple explanations for features may not exist. The complexity of rules in CNN layers affects the accuracy of explanations. While simple rules may not explain all features, the final layer can be accurately explained with relatively simple rules. Selective use of decomposition algorithms depending on the layer can address the black box problem in neural networks. The black box problem of neural networks hinders their integration into society. Despite efforts to extract knowledge, most large neural networks remain difficult to interpret and explain due to their distributed nature. Critics argue that traditional knowledge extraction methods may not be effective in addressing this issue. The distributed nature of neural networks makes traditional decomposition rule extraction methods unfeasible. A novel search method for M-of-N rules was applied to explain latent features of a CNN, revealing an 'optimal' rule representing an error/complexity trade-off. Rule complexity was included in the search for extracted rules, showing a large trade-off difference between neurons in different layers and architectures. The distributed nature of neural networks makes traditional decomposition rule extraction methods unfeasible. Rule extraction may simplify explanations without reducing accuracy, making it a useful tool for understanding networks. Further research is needed to explore the effects of different transfer methods on accuracy and interpretability. The study will investigate the impact of using various transfer functions, datasets, architectures, and regularization techniques on the accuracy and interpretability of neural networks."
}