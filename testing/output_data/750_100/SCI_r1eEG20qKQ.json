{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization is a bilevel optimization problem where optimal parameters depend on hyperparameters. Regularization hyperparameters for neural networks can be adapted by fitting approximations to the best-response function. Scalable best-response approximations for neural networks can be constructed by modeling the best-response as a single network with gated hidden units. This approximation is justified by showing that the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. Our approach to hyperparameter optimization involves a gradient-based algorithm that adapts hyperparameters online, allowing for tuning of discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities without the need to differentiate the training loss. This method discovers hyperparameter schedules that outperform fixed values, showing empirical superiority. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training, outperforming other optimization methods on deep learning problems. Regularization hyperparameters like weight decay, data augmentation, and dropout are crucial for neural network generalization but challenging to tune. Popular optimization approaches include grid search, random search, and Bayesian optimization, which work well with low-dimensional spaces and ample resources. Hyperparameter optimization can be formulated as a bilevel optimization problem, considering structure for faster convergence. The best-response function w*(\u03bb) = arg min w LT(\u03bb, w) simplifies the problem to a single-level one. The best-response function w*(\u03bb) = arg min w LT(\u03bb, w) simplifies hyperparameter optimization to a single-level problem. To approximate w*, a parametric function \u0175 \u03c6 is proposed, optimizing \u03c6 and \u03bb jointly for faster convergence. Finding a scalable approximation \u0175 \u03c6 for weights w of a neural network is a challenge due to memory overhead. A compact approximation is constructed by modeling each row in a layer's weight matrix/bias as a rank-one affine transformation of hyperparameters. This approximation computes activations of a base network plus a correction term dependent on hyperparameters. The exact best-response for a shallow linear network with L2-regularized Jacobian follows a similar structure. Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering advantages over other optimization methods. They are easy to implement by replacing existing modules in deep learning libraries with \"hyper\" counterparts. This online adaption yields hyperparameter schedules that outperform fixed settings. The STN training algorithm does not require differentiating the training loss with respect to hyperparameters, allowing tuning of discrete hyperparameters. Empirical evaluation shows STNs outperform baseline methods on large-scale deep-learning problems. Bilevel optimization problems involve upper-level and lower-level sub-problems, with minimax problems being a common example. These programs have applications in various fields, including machine learning tasks like hyperparameter optimization and GAN training. Bilevel optimization problems are challenging, even with linear objectives and constraints. Most research focuses on restricted settings, but this study aims to find local solutions in nonconvex, differentiable, and unconstrained settings. The upper-and lower-level objectives are denoted by F and f, with parameters \u03bb and w. The goal is to solve the optimization problem subject to w. The study focuses on solving a bilevel optimization problem with upper-and lower-level parameters denoted by \u03bb and w. A gradient-based algorithm is desired for solving the problem, with simultaneous gradient descent being the simplest method but often giving incorrect solutions. A more principled approach involves using the best-response strategy. The study proposes using the best-response function to solve Problem 4 in a bilevel optimization setting. By substituting the best-response function, Problem 4 can be converted into a single-level problem, allowing for gradient descent optimization. Conditions for unique optima and differentiability of the best-response function are discussed, with sufficient conditions provided in a neighborhood of a given point. The study proposes using the best-response function to solve Problem 4 in a bilevel optimization setting. By substituting the best-response function, Problem 4 can be converted into a single-level problem, allowing for gradient descent optimization. Conditions for unique optima and differentiability of the best-response function are discussed, with sufficient conditions provided in a neighborhood of a given point. Lemma 1 states that if w 0 solves Problem 4b for \u03bb 0 and certain conditions are met, then there exists a continuously differentiable function w * that is the unique solution to Problem 4b for each \u03bb in a neighborhood of \u03bb 0. The gradient of F * decomposes into direct and response gradients, capturing the direct reliance and response of the upper-level objective on \u03bb. The gradient captures how the lower-level parameter responds to changes in the upper-level parameter, stabilizing optimization by converting the bilevel problem into a single-level one. Assuming uniqueness of a solution and differentiability of w * can yield fruitful algorithms in practice for gradient-based hyperparameter optimization methods. Gradient-based hyperparameter optimization methods aim to approximate the best-response w * or its Jacobian \u2202w * /\u2202\u03bb, but struggle with discrete and stochastic hyperparameters. Lorraine & Duvenaud (2018) proposed promising approaches to directly approximate w * using global approximation with parameters \u03c6. Duvenaud (2018) approximates w * as a differentiable function \u0175 \u03c6 with parameters \u03c6, representing neural net weights. The mapping \u0175 \u03c6 is a hypernetwork. Gradient descent with respect to \u03c6 minimizes a specific objective when the distribution p(\u03bb) is fixed. If support(p) is broad and \u0175 \u03c6 is flexible enough, it can be used as a proxy for w * in Problem 5. Lorraine & Duvenaud (2018) locally approximate w * in a neighborhood around the current upper-level parameter \u03bb using a factorized Gaussian noise. The approach involves perturbing the upper-level parameter \u03bb by a small amount to help the lower-level parameter learn how to respond. An alternating gradient descent scheme is used to update \u03c6 and \u03bb. This method has shown success with L2 regularization on MNIST but its applicability to different regularizers or larger problems is uncertain. The approach requires \u0175 \u03c6, which can be challenging for high-dimensional w, and setting \u03c3 remains unclear. In this section, a best-response approximation \u0175 \u03c6 is constructed for memory-efficient scaling to large neural networks. A method to adjust the neighborhood scale \u03c6 is described, along with an algorithm that easily handles discrete and stochastic hyperparameters. The resulting networks update their own hyperparameters online during training. Self-Tuning Networks (STNs) update their own hyperparameters online during training by approximating the best-response for a layer's weight matrix and bias as an affine transformation of the hyperparameters. This architecture adds a correction to the usual pre-activation to account for the hyperparameters. The best-response architecture for Self-Tuning Networks is tractable and memory-efficient, requiring specific parameters to represent weight matrix and bias. It allows for parallelism and improved sample efficiency by perturbing hyperparameters independently in a batch. The approximation can be easily implemented by replacing existing modules in deep learning libraries with \"hyper\" counterparts. In this section, a model is presented where the best-response function can be exactly represented using a linear network with Jacobian norm regularization. The best-response function takes the form of a network with hidden units modulated based on hyperparameters. The model presented involves using a 2-layer linear network with weights to predict targets from inputs. A squared-error loss regularized with an L2 penalty on the Jacobian is used, with a penalty weight mapped using exp. The theorem states that the network can be implemented with an additional sigmoidal gating. The model involves using a 2-layer linear network with weights to predict targets from inputs. Inspired by this, a similar gating of hidden units is used to approximate the best-response for deep, nonlinear networks. The sigmoidal gating can be simplified for a small range of hyperparameter values by replacing it with linear gating. Replacing sigmoidal gating with linear gating allows for weights to be affine in hyperparameters. The theorem shows that using an affine approximation to the best-response function for quadratic lower-level objectives yields the correct best-response Jacobian, ensuring convergence to a local optimum. The effect of the sampled neighborhood is discussed, highlighting the importance of the approximation matching the exact best-response. The scale of the hyperparameter distribution controls the flexibility of the approximation model. If the neighborhood is too wide, the approximation will not match the best-response gradient. The entries of \u03c3 must be large enough to capture the best-response but not too large to limit flexibility. Adjusting \u03c3 during training based on the sensitivity of the upper-level objective to the sampled hyperparameters is proposed to address issues related to the smoothness of the loss landscape. An entropy term weighted by \u03c4 \u2208 R + is included to enlarge the entries of \u03c3, resulting in an objective similar to variational inference. The objective function includes an entropy term weighted by \u03c4 to balance between optimization and inference, as noted in the literature. Minimizing the first term moves probability mass towards an optimum \u03bb * , requiring \u03c3 to balance shrinking and avoiding heavy entropy penalties. Performance evaluation involves F (\u03bb,\u0175 \u03c6. The algorithm's performance is evaluated by minimizing entropy penalties and balancing optimization with inference. The STN training algorithm can tune hyperparameters that other gradient-based algorithms cannot handle, such as discrete or stochastic hyperparameters. The hyperparameters are represented by an unconstrained parametrization \u03bb \u2208 R n, with a function r mapping \u03bb to the appropriate constrained space. Training and validation are denoted by L T and L V. The algorithm involves non-differentiable discretization for discrete hyperparameters. STNs are trained using a gradient descent scheme to minimize training and validation losses. The algorithm can handle non-differentiability of hyperparameters and parameters. The algorithm deals with non-differentiable discretization for discrete hyperparameters. It uses the reparametrization trick to estimate derivatives with respect to hyperparameters. There are two cases to consider when initializing parameters and learning rates. Case 1 involves regularization schemes where the gradient is through \u0175 \u03c6. Case 2 deals with cases where the loss function explicitly depends on a hyperparameter. The reparametrization gradient is utilized for estimating derivatives with respect to hyperparameters. In Case 2, the REINFORCE gradient estimator is used when the loss function explicitly depends on a hyperparameter. This approach is necessary for tuning hyperparameters that directly impact the validation loss. The method has been applied to convolutional networks and LSTMs, resulting in self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). Initial investigations focused on tuning a single hyperparameter to observe the behavior of self-tuning networks. Self-tuning networks (STNs) discovered hyperparameter schedules that outperformed fixed values in a simple setting. STNs, which optimize hypernetwork weights and hyperparameters jointly, adapt hyperparameters online, leading to better performance than fixed values. This behavior was examined in detail on the PTB corpus using an ST-LSTM to tune the output dropout rate. The schedule discovered by an ST-LSTM for output dropout outperforms the best fixed output dropout rate found by grid search, achieving 82.58 vs 85.83 validation perplexity. This improved performance is attributed to the schedule, not stochasticity introduced by sampling hyperparameters during training. The ST-LSTM discovered a schedule for output dropout that outperformed the best fixed rate found by grid search. This schedule improved performance significantly, with evidence showing that the schedule itself was the key factor rather than other aspects of the model. The STN discovered a schedule for output dropout that outperformed the best fixed rate found by grid search. Evidence showed that the schedule itself was responsible for the improvement, as demonstrated by training a standard LSTM from scratch using the final dropout value found by the STN. The STN consistently discovered the same schedule regardless of initial hyperparameter values, indicating the importance of the hyperparameter schedule. The STN schedule implements a curriculum by adjusting dropout rates during training, leading to better generalization. An ST-LSTM was evaluated on the PTB corpus with promising results. The experimental setup involved using a 2-layer LSTM with 650 hidden units and 650-dimensional word embeddings. 7 hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. The best results were achieved with a fixed perturbation scale of 1 for the hyperparameters. Additional details can be found in Appendix D. The experimental setup involved using a 2-layer LSTM with 650 hidden units and 650-dimensional word embeddings. Hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. STNs outperform other methods, achieving lower validation perplexity more quickly. The schedules the STN finds for each hyperparameter are nontrivial, with different forms of dropout used at various stages of training. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture and tuned 15 hyperparameters related to dropout, data augmentation, and noise levels. Comparisons were made between STNs and grid methods. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture and tuned 15 hyperparameters related to dropout, data augmentation, and noise levels. Comparisons were made between STNs and grid methods. STNs outperformed other methods in finding better hyperparameter configurations in less time. Bilevel Optimization was also discussed in the context of linear, quadratic, or convex objectives/constraints. In the context of linear, quadratic, or convex objectives/constraints, the approach involves replacing the lower-level problem with its KKT conditions as constraints for the upper-level problem. This method is similar to trust-region methods, where a simpler bilevel program is used to approximate the problem locally. Additionally, hypernetworks, which map to the weights of a neural net, have been explored in predicting weights in CNNs. Predicting weights in CNNs has been developed using hypernetworks to generate weights for modern CNNs and RNNs. Different approaches have been explored for gradient-based hyperparameter optimization, with one approach approximating the value of weights after gradient descent steps. The second approach for hyperparameter optimization uses the Implicit Function Theorem to derive \u2202w*/\u2202\u03bb(\u03bb0) under certain conditions. This method has been applied in various fields such as neural networks, log-linear models, kernel selection, and image reconstruction. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance metrics given hyperparameters and dataset. This approach avoids the expensive computation of Hessian by using Hessian-vector products with conjugate gradient solvers. Model-Based Hyperparameter Optimization uses Bayesian optimization to model the performance metrics given hyperparameters and dataset. The dataset is constructed iteratively, and the next hyperparameter to train on is chosen by maximizing an acquisition function. Training each model to completion can be avoided by making assumptions on learning curve behavior. However, these approaches may not hold in practice and do not take advantage of the network structure for hyperparameter optimization. Model-free hyperparameter optimization methods like random search and grid search do not scale well with the number of hyperparameters. Successive Halving and Hyperband improve random search by allocating resources adaptively. These methods do not consider problem structure but are easy to parallelize and perform well in practice. On the other hand, hyperparameter scheduling techniques like Population-based methods are not discussed in detail. Hyperparameter Scheduling involves Population Based Training (PBT) where a population of networks is trained in parallel. Under-performing networks have their weights replaced by better-performing ones, implementing a schedule for different hyperparameter settings. Single best-response approximation is used in STNs for tuning with gradients. Self-Tuning Networks (STNs) efficiently approximate best-response of parameters to hyperparameters using gradients. They can outperform fixed hyperparameters, achieve better generalization performance in less time, and offer a compelling path forward. Self-Tuning Networks (STNs) offer a compelling path towards automated hyperparameter tuning for neural networks. The parameters respond to hyperparameters through direct and response gradients, leading to improved generalization performance in less time. Support is acknowledged from various sources. Losses direct and response gradients in solving Problem 4b with \u03bb 0 and w 0. The Jacobian of \u2202f /\u2202w decomposes as a block matrix. Hessian \u2202 2 f /\u2202w 2 is positive definite. By the Implicit Function Theorem, a unique continuously differentiable function w * exists. The unique solution to Problem 4b for all \u03bb \u2208 U is w * (\u03bb), following from continuity and second-order optimality conditions. The data matrix X is decomposed using SVD, with orthogonal matrices U and V, and diagonal matrix D. The function y(x; w) is simplified by setting u = s Q, leading to standard L2-regularized least-squares linear regression. The optimal solutions for regularized and unregularized versions are given by u*(\u03bb) and u*, respectively. Q0 = V is defined as the change-of-basis matrix, and s0 solves the unregularized regression. The optimal solutions for regularized and unregularized versions of the function y(x; w) are defined by u*(\u03bb) and u*, respectively. Q0 = V is the change-of-basis matrix, and s0 solves the unregularized regression problem. The chosen functions Q*(\u03bb) and s*(\u03bb) meet the criteria of \"best-response functions\" in the context of Problem 13. By assuming f is quadratic, we have matrices A, B, C, vectors d, e. Derivative conditions lead to finding function f as U\u03bb + b. Simplifying further, we get equation 36 with linearity of expectation and Trace cyclic property. The text discusses simplifying expectations and finding derivatives using matrix-derivative equalities. It also mentions setting derivatives equal to 0 to find the best-response Jacobian. Substituting values into equations leads to the final result. The text discusses updating model parameters without updating hyperparameters. Training was terminated when the learning rate dropped below 0.0003. Variational dropout was tuned on input, hidden state, and output of the LSTM. Embedding dropout was also tuned, removing certain words from sequences. Hidden-to-hidden weight matrix was regularized using DropConnect. The hidden-to-hidden weight matrix was regularized using DropConnect, which operates directly on weights. Activation regularization (AR) penalizes large activations, while temporal activation regularization (TAR) is a slowness regularizer. Scaling coefficients \u03b1 and \u03b2 were tuned for AR and TAR. Hyperparameter ranges were defined for the baselines. For the CNN experiments, 20% of the training data was held out for validation. The baseline CNN was trained using SGD with initial learning rate 0.01 and momentum 0.9 on mini-batches of size 128. The learning rate was decayed by 10 if validation loss did not decrease for 60 epochs, and training ended if the learning rate fell below 10^-5 or validation loss did not decrease. The ST-CNN's hyperparameters were optimized using Adam with a learning rate of 0.003. Training ends if the learning rate drops below 10^-5 or validation loss doesn't decrease for 75 epochs. The search spaces for hyperparameters were defined for baselines like grid search and random search. The ST-CNN's hyperparameters were optimized using Adam with a learning rate of 0.003. Training ends if the learning rate drops below 10^-5 or validation loss doesn't decrease for 75 epochs. The model used an entropy weight of \u03c4 = 0.001 and had restrictions on cutout length and number of holes. Dropout rates and data augmentation parameters were initialized to 0.05. ST-CNN was found to be robust to initialization. The ST-CNN is relatively robust to hyperparameter initialization, with low regularization aiding optimization in the early epochs. Curriculum learning, a continuation method, involves optimizing a sequence of functions ordered by difficulty, starting with a simpler version of the problem. In this section, hyperparameter schedules are explored as a form of curriculum learning. Grid searches are used to analyze the effects of different hyperparameter settings during training, showing that greedy schedules can outperform fixed values. The text discusses how hyperparameter schedules, specifically greedy schedules, can outperform fixed values. A grid search was conducted over 20 values each for input and output dropout, showing that smaller dropout rates are better at the start of training, while larger rates perform better as training progresses. Additionally, a simple example illustrates the benefits of greedy hyperparameter schedules. The text discusses constructing a dropout schedule based on hyperparameter values that achieve the best validation perplexity at each epoch in training. Using small dropout values at the start and larger values later leads to better generalization and overall validation perplexity. FIG10 shows perturbed values for output dropout used for investigation. In this section, PyTorch code listings for constructing ST-LSTMs and ST-CNNs using HyperLinear and HyperConv2D classes are provided, along with optimization steps for the training and validation sets."
}