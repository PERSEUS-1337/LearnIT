{
    "title": "BJxpIJHKwB",
    "content": "Few shot image classification involves learning a classifier from limited labeled data. Generating classification weights is common in meta-learning for few shot image classification, but it's challenging to create exact weights for diverse query samples with very few training samples. The proposed Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) addresses this by generating different classification weights for each query sample, allowing them to attend to the entire support set. The proposed Attentive Weights Generation for few shot learning via Information Maximization (AWGIM) aims to generate adaptive weights for diverse query samples by maximizing mutual information between weights and query/support data. This approach unifies information maximization into few shot learning and achieves state-of-the-art performance on benchmark datasets. Few shot learning is proposed to enable deep models to learn from very few samples, as humans can learn from limited data. Meta learning is the most popular approach for few shot problems, where the model extracts high-level information. Meta learning methods for few shot learning involve extracting high-level knowledge across tasks to quickly adapt to new tasks. These methods include gradient-based and metric-based approaches, with weights generation showing effectiveness in generating classification weights for different tasks. AWGIM introduces Attentive Weights Generation for few shot learning to generate classification weights specifically for each query sample, addressing limitations of fixed weights for different tasks. Simple cross attention between query samples and support set is shown to be insufficient in fitting diverse query data. AWGIM proposes maximizing mutual information between generated weights and query, support data to address limitations of fixed weights in fitting diverse query data. It introduces Variational Information Maximization in few shot learning, minimizing computational overhead. AWGIM eliminates inner updates without performance compromise, achieving state-of-the-art results on benchmark datasets. AWGIM proposes maximizing mutual information between generated weights and query, support data to address limitations of fixed weights in fitting diverse query data. It introduces Variational Information Maximization in few shot learning, minimizing computational overhead. AWGIM achieves state-of-the-art results on benchmark datasets through detailed analysis validating the contribution of each component. In metric-based few-shot learning methods, a similarity metric is learned between query and support samples. Some works also consider spatial information or local image descriptors to compute richer similarities. Generating classification weights directly has been explored, with some approaches generating weights as linear combinations of base and novel class weights. In few-shot classification, various methods have been explored to generate classification weights, including using graph neural networks, generating \"fast weights\" from loss gradients, and utilizing generative models to generate more data. These approaches do not consider generating different weights for different query examples or maximizing mutual information. Additionally, some methods directly use closed-form solutions for few-shot classification. In few-shot classification, closed-form solutions are used directly. Label propagation on a transductive graph is integrated to predict query class labels. Attention mechanism, effective in computer vision and natural language processing, models interaction between queries and key-value pairs. Self attention and cross attention are used to encode task and query-task information. Similar work includes Attentive Neural Processes. In contrast to Attentive Neural Processes, our work focuses on few-shot image classification using attention to maximize mutual information. Mutual information measures uncertainty reduction between random variables x and y, defined as the Kullback-Leibler divergence between joint distribution and product of marginal distributions. Our work focuses on few-shot image classification using attention to maximize mutual information. Mutual information measures uncertainty reduction between random variables x and y. It has applications in Generative Adversarial Networks, self-supervised learning, and visual question generation. The attentive path equips the query sample with task knowledge through an attention mechanism. The proposed model aims to generate classification weights specific for x, maximizing mutual information. It involves using two networks to reconstruct inputs and force the generator to be sensitive to different query samples. The problem formulation, model description, objective function, and theoretical analysis are provided. This approach aligns with popular meta-learning methods for few-shot classification. The proposed model aims to generate classification weights specific for x, maximizing mutual information. It involves using two networks to reconstruct inputs and force the generator to be sensitive to different query samples. The problem formulation, model description, objective function, and theoretical analysis are provided, aligning with popular meta-learning methods for few-shot classification. Following episodic training paradigm, a N-way K-shot task includes support set S and query set Q. Support set S contains N K labeled samples, while query set Q includes x for which label \u0177 needs to be predicted based on S. Meta-loss is estimated on Q during meta-training to optimize the model, and the performance is evaluated on Q during meta-testing with labeled S. Classes used in meta-training and meta-testing are disjoint. The proposed approach aims to generate classification weights for different tasks in a meta-learning framework. It utilizes a feature extractor to output image embeddings and a meta-learner to generate task-specific weights. Latent Embedding Optimization (LEO) is a related method for weight generation. In LEO, the latent code z is generated by h conditioned on support set S. Classification weights w can be decoded from z with l. The objective function of LEO aims to minimize a certain value. LEO generates a lower-dimensional latent space to avoid updating high-dimensional weights in the inner loop. Unlike AWGIM, LEO does not require inner updates and generates fixed weights based on the support set for each task. The proposed method, LEO, can be seen as a special case of AWGIM under certain conditions. A feature extractor processes images in a sampled task, generating d-dimensional vectors. Two paths encode task context and query samples, concatenated for classification weight generation. The proposed method, LEO, is a special case of AWGIM. It involves a feature extractor processing images to generate d-dimensional vectors. Two paths encode task context and query samples for classification weight generation, focusing on maximizing mutual information between variables. The contextual path uses a multi-head self-attention network to learn representations for the support set. The proposed method, LEO, introduces attentive path for generating adaptive classification weights based on individual query examples attending to task context. This addresses the issue of sub-optimal weights generated solely from the support set. In the attentive path, a multi-head self-attention network is used to encode global task information for adaptive classification weights. The self-attention network in this path provides context for query samples to attend in cross attention, different from the contextual path which focuses on generating classification weights. Sharing the same self-attention networks between paths may limit the expressiveness of learned representations. The cross attention network is applied on query samples and task-aware support sets to produce comprehensive representations. Multi-head attention is used to learn from different subspaces, resulting in concatenated tensors for further processing. The cross attention network combines X cp and X ap to form X cp\u2295ap, which generates classification weights specific to each query sample. These weights are adaptive to the task-context and individual query samples. The weights are decoded by a generator g and sampled from a Gaussian distribution. The sampled weights are denoted as W and used for classification. The weights W are computed for each class to have W final. Query data prediction is done by XW finalT. Support data prediction is computed as XsW finalT. Two decoders r1 and r2 reconstruct Xcp and Xap respectively using the generated weights W. The outputs are Xcp re and Xap re. The outputs of r1 and r2, Xcp re and Xap re, are denoted as matrices. Reconstruction is used as auxiliary tasks for a reason discussed in Sec. 3.4. The analysis is performed on one query sample without classification weights. The weights generated from g are expected to be query-specific, but experiments show that a weight generator conditioned only on the support data performs better. The weights generated from two paths are not sensitive to different query samples, indicating that information from the attentive path is not well preserved. To address this, the proposal is to maximize mutual information between generated weights and support/query data. Directly computing this mutual information is challenging due to intractable true posterior distributions. To address the issue of intractable true posterior distributions, Variational Information Maximization is used to compute the lower bound of mutual information. By approximating the true posterior distributions with p \u03b8 (x|w), the entropy terms can be omitted to maximize the lower bound as a proxy for the true mutual information. The new objective function aims to maximize the log likelihood of labels for support and query data by minimizing cross entropy. Gaussian distributions are assumed for p \u03b8 (x|w) and p \u03b8 (x|w), with r1 and r2 approximating their means. The loss function involves reconstructing xcp and xap with L2 loss to train the network. The loss function for training the network involves reconstructing xcp and xap with L2 loss. Hyper-parameters \u03bb1, \u03bb2, \u03bb3 are used for trade-off. The generated classification weights carry information about support data and query samples. In LEO, the inner update loss is computed as cross entropy on support data. The encoding process in LEO involves contextual and attentive paths with computational complexity O((N K) 2 + |Q|(N K)). The few-shot learning problem makes (N K) 2 negligible, and cross attention can be parallelly implemented. AWGIM reduces computational overhead by avoiding inner updates, improving performance, and significantly decreasing training and inference time. Empirical evaluation on miniImageNet and tieredImageNet datasets shows its effectiveness compared to other methods. tieredImageNet dataset has 608 classes and 779,165 images selected from 34 higher level nodes in ImageNet hierarchy. 351 classes are used for meta-training, 97 for meta-validation, and 160 for meta-testing. Image features from LEO are utilized, represented by a 640 dimensional vector. For N-way K-shot experiments, classes are randomly sampled from the meta-training set with support and query sets. Models are trained on two datasets for 5-way 1-shot and 5-shot tasks. During meta-testing, tasks are sampled from the meta-testing set, and accuracy is reported using TensorFlow implementation. The dimension of feature embeddings is set to d = 640, with d h at 128. The number of heads h in the attention module is 4. Various models like Meta LSTM, Prototypical Nets, Relation Nets, SNAIL, TPN, and MTL have been compared based on their performance metrics. Table 2 shows accuracy comparison of different approaches on tieredImageNet, with results averaged on 600 tasks from meta-testing set. Best results are highlighted. Model Feature Extractor 5-way 1-shot 5-way 5-shot MAML (Finn et al., 2017) Conv-4 51.67 \u00b1 1.81% 70.30 \u00b1 1.75% Prototypical Nets. TPN Conv-4 achieves 53.31\u00b1 0.89% accuracy in 5-way 1-shot and 72.69 \u00b1 0.74% in 5-way 5-shot tasks. Relation Nets Conv-4 achieves 54.48 \u00b1 0.93% in 5-way 1-shot and 71.32 \u00b1 0.78% in 5-way 5-shot tasks. TPN Conv-4 outperforms other models with 59.91 \u00b1 0.96% accuracy in 5-way 1-shot and 72.85 \u00b1 0.74% in 5-way 5-shot tasks. MetaOptNet Resnets is used for network optimization with specific hyperparameters and training iterations. The approach AWGIM is evaluated on two datasets compared to state-of-the-art methods. Results of MAML, Prototypical Nets, and Relation Nets on tieredImageNet are assessed. Results of Dynamic on miniImageNet with WRN-28-10 as the feature extractor are also reported. The backbone network structure of the feature extractor used is included for reference. The results of the AWGIM approach are compared with other methods on miniImageNet and tieredImageNet datasets. AWGIM outperforms all methods in the top parts of the tables and shows competitive performance in the bottom part, being the best on tieredImageNet and close to the state-of-the-art. AWGIM outperforms LEO in all settings, with detailed analysis provided in Table 3. Classification weights are generated using WRN-28-10 as the backbone network for fair comparison. Random shuffling of weights shows optimality for different query samples. In Table 3, a detailed analysis of AWGIM is presented, with results compared to LEO by Rusu et al. (2019). Different generators are studied, including those conditioned on the support set and query set, showing that self-attention is as effective as inner updates in LEO. The study compares different generators in AWGIM to LEO, showing that self-attention is as effective as inner updates in modeling task-context. The effect of attention is further explored by replacing attention modules with MLPs in \"MLP encoding\", achieving accuracy close to LEO even without attention. In the study, the importance of maximizing information in MLP encoding is highlighted. Ablation analysis was conducted by varying \u03bb 1 , \u03bb 2, and \u03bb 3, showing that maximizing mutual information between weights and support is crucial for accuracy.\u03bb 1 = \u03bb 2 = 0 significantly degrades accuracy compared to \u03bb 3 = 0. The study emphasizes the importance of maximizing information in MLP encoding. Ablation analysis showed that maximizing mutual information between weights and support is crucial for accuracy, with \u03bb 1 = 0 noticeably affecting performance. The classification weights in AWGIM are tailored for each query sample, and shuffling them between samples within and across classes is done to study their adaptability. The study focuses on maximizing information in MLP encoding by maximizing mutual information between weights and support. Shuffling weights between samples within and across classes affects accuracy, with random shuffle between classes degrading accuracy in 5-way 1-shot experiments. This suggests that generated weights for query samples from the same class are similar when support data is limited. In this work, Attentive Weights Generation via Information Maximization (AWGIM) is introduced for few-shot image classification. The study shows that a larger support set leads to more diverse and specific classification weights for each query sample in a 5-way 5-shot setting. AWGIM learns to generate optimal classification weights for each query sample within the task using two encoding paths. AWGIM introduces a method to generate optimal classification weights for query samples in few-shot learning tasks. It maximizes mutual information between weights and support data. This approach achieves state-of-the-art performance on benchmark datasets. The multi-head attention mechanism is utilized to process support samples in N-way K-shot tasks. The attentive path in AWGIM uses attention to encode global task information to support samples. Cross attention between query and context-aware support samples is computed. Classification weights follow a Gaussian distribution with diagonal covariance and are sampled during meta-training. During meta-training, weights are sampled from a Gaussian distribution with diagonal covariance. Few shot regression tasks are modified by setting the number of classes to 1 and adapting cross entropy loss to mean square error. Data points are used as inputs to generate weight and bias parameters for a three-layer MLP with hidden dimension 40. Sinusoidal or linear regression tasks are constructed for the experiments. The study compares multi-head attention with single-head attention in 5-way 1-shot and 5-way 5-shot experiments on the miniImageNet dataset. Results show that multi-head attention improves performance, especially in scarce data scenarios. Comparison with LEO in terms of convergence speed is also conducted. The accuracy of metavalidation set during meta-training on 5-way 1-shot miniImageNet is plotted, showing that AWGIM converges faster than LEO and outperforms LEO except for the first few iterations. Inference time of AWGIM is measured to show minimal computational overhead compared to using \"MLP encoding\" in two paths. Experiments on miniImageNet with a batch size of 64 and 100 batches processed show that AWGIM has faster inference times. The usage of self-attention and cross attention in AWGIM incurs negligible overhead compared to MLP encoding due to small values of N, K, and |Q|. Visualization of classification weights by t-SNE shows 10,000 weight vectors from 400 tasks in the meta-validation set. Inputs to the generator g are also visualized for comparison. The visualization results in Figure 4 show the generated classification weights by the generator g, compared to the inputs. The decoded weights for each class are clustered closer together, indicating that g can generate adapted weights for different query samples. This aligns with the results in Table 3, which suggest that query samples from different classes have distinct classification weights. The visualization in Figure 4 displays distinct classification weights for query samples from different classes. Blue and red dots represent classification weights for two query samples in the same task."
}