{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, specifically Choco-SGD, enables data privacy, on-device learning, and efficient scaling to large compute clusters. The algorithm shows linear speedup with high compression ratios on non-convex functions and non-IID training data. Practical performance is demonstrated in training scenarios over decentralized user devices and in datacenters. Distributed machine learning through decentralized training enables efficient scaling and data privacy, leveraging the computational power of multiple devices while maintaining data locality. Recent theoretical results suggest decentralized schemes can be as effective as centralized approaches in terms of convergence of training loss. Gradient compression techniques have been proposed for distributed training to reduce data sent over communication links. Tang et al. (2018) introduced DCD and ECD algorithms for decentralized training of deep neural networks, allowing for communication compression. CHOCO-SGD is a new algorithm introduced for convex problems that allows for high compression ratios. It focuses on generalization performance on test sets, departing from previous works that mainly considered training performance on train sets. CHOCO-SGD shows speed-ups over decentralized baseline in challenging peer-to-peer training. It also improves time-to-accuracy on large tasks like ImageNet training in datacenter setting with better scalability. Decentralized algorithms struggle to match the performance of centralized schemes on large tasks like ImageNet training. These findings highlight the limitations of current decentralized training schemes and call for further research on scalable decentralized training methods. CHOCO-SGD converges at a rate of O(1/\u221anT + n/(\u03c1^4\u03b4^2T)) on non-convex smooth functions, showing a linear speedup in the number of workers. A version with momentum is analyzed for practical performance on on-device training and peer-to-peer social networks, reducing bandwidth requirements for joint training. Decentralized schemes and gradient compression methods have been proposed for training in communication restricted settings, aiming to reduce bandwidth requirements for joint training in deep learning models. The performance of decentralized schemes when scaling to a larger number of nodes is systematically investigated, highlighting shared difficulties encountered by current approaches. In this paper, the focus is on combining decentralized SGD schemes with gradient compression, specifically emphasizing approaches based on gossip averaging. This approach is advocated for learning over decentralized data, as studied in federated learning literature for centralized algorithms. Decentralized SGD approaches, such as gossip averaging, rely on the spectral gap of the mixing matrix for convergence rate. Lian et al. (2017) combine SGD with gossip averaging, showing convergence at a rate of O(1/\u221anT + n/(\u03c1^2T)). The spectral gap mainly affects smaller terms, with similar results seen in related schemes. Quantization for communication compression in deep learning has gained popularity, with theoretical guarantees established for unbiased and biased compression schemes. Error correction schemes show the best practical and theoretical performance. Proximal updates and variance reduction are also being studied in combination with quantized updates. Decentralized optimization with quantization has challenges with gossip averaging due to quantization noise. Reisizadeh et al. (2018) proposed an algorithm that can still converge, albeit slower. Adaptive schemes with higher compression accuracy converge at the expense of increased communication cost. In decentralized optimization, algorithms like DCD and ECD by Tang et al. converge at the same rate as centralized methods for constant compression ratio. CHOCO-SGD algorithm can handle high compression and has been analyzed for convex functions. For non-convex functions, a rate of convergence is shown with \u03b4 > 0 measuring compression quality. DeepSqueeze by Tang et al. also converges with arbitrary compression ratio. In decentralized optimization, algorithms like DCD and ECD by Tang et al. converge at the same rate as centralized methods for constant compression ratio. CHOCO-SGD algorithm can handle high compression and has been analyzed for convex functions. For non-convex functions, a rate of convergence is shown with \u03b4 > 0 measuring compression quality. DeepSqueeze by Tang et al. also converges with arbitrary compression ratio. CHOCO-SGD achieves higher test accuracy under the same tuning amount. The decentralized optimization problem, compression operators, and the gossip-based stochastic optimization algorithm CHOCO-SGD are formally introduced in this section. In decentralized optimization, algorithms like DCD and ECD by Tang et al. converge at the same rate as centralized methods for constant compression ratio. CHOCO-SGD algorithm can handle high compression and has been analyzed for convex functions. For non-convex functions, a rate of convergence is shown with \u03b4 > 0 measuring compression quality. DeepSqueeze by Tang et al. also converges with arbitrary compression ratio. CHOCO-SGD achieves higher test accuracy under the same tuning amount. Communication is limited to local neighbors in a weighted graph defined by the network topology. Positive weights are assigned to edges for message exchange, computed based on local node degrees. Compression operators aim to transmit compressed messages, not necessarily unbiased like quantization operators, supporting a wider range of options. Examples can be found in literature such as Koloskova et al. (2019). Compression operators aim to transmit compressed messages, supporting a wider range of options. CHOCO-SGD algorithm involves workers updating their private variables using stochastic gradient and gossip averaging steps to preserve averages of iterates despite quantization noise. Nodes communicate with neighbors and update variables using compressed updates. The CHOCO-SGD algorithm involves workers updating private variables using compressed updates. Nodes communicate with neighbors and update variables using compressed updates, with publicly available copies of private variables. Communication and gradient computation can be executed in parallel, with each node only needing to store 3 vectors at most. The CHOCO-SGD algorithm extends the analysis to non-convex problems, with bounded variance of stochastic gradients on each worker. The convergence rate is denoted by c, showing linear speed-up compared to SGD on a single node. The CHOCO-SGD algorithm shows linear speed-up compared to SGD on a single node. The experiments compare CHOCO-SGD to relevant baselines using commonly used compression operators and momentum in all algorithms. The momentum version of CHOCO-SGD is provided in Algorithm 2. Algorithm 1 introduces weight decay factor \u03bb, momentum factor \u03b2, and local momentum memory. It uses a ring topology with 8 nodes to train ResNet20 on the Cifar10 dataset. The training data is split between workers and shuffled after every epoch. DCD and ECD with momentum are implemented following the procedure in (Tang et al., 2018). The momentum factor is set to 0.9 without dampening. The initial learning rate is fine-tuned and gradually warmed up from a small value for the first 5 epochs. The learning rate is decayed twice and training stops at 300 epochs. The consensus learning rate is also tuned for CHOCO-SGD and DeepSqueeze. Hyper-parameter tuning details can be found in Appendix F. Compression schemes are evaluated on ResNet20 layers separately, with top-1 test accuracy reported for each node over the dataset. Two unbiased schemes include quantization and random sparsification, while biased schemes involve selecting weights based on magnitude. The text discusses various compression schemes for ResNet20 layers, including unbiased quantization and biased weight selection methods like top-k. The combination of biased and unbiased schemes is not supported by theory, but some schemes can be scaled down to meet specifications. Results show that ECD and DCD perform well with small compression ratios but may diverge with high ratios. DCD with biased top-k sparsification outperforms unbiased random selection. CHOCO-SGD demonstrates good generalization across scenarios. CHOCO-SGD demonstrates good generalization in all scenarios with minimal accuracy drop. Sign compression achieves state-of-the-art accuracy with significantly fewer bits per weight. The focus now shifts to decentralized real-world scenarios where centralized methods are inefficient due to local training data on each device. In decentralized scenarios, each device has limited access to local data, communication bandwidth is restricted, global network topology is unknown, and many devices are connected. Privacy is a key motivation for this setting, keeping training data private on each device. Training data is permanently split between nodes, never shuffled during training. In decentralized scenarios, each device has limited access to local data and communication bandwidth is restricted. Privacy is a key motivation, keeping training data private on each device. For decentralized deep learning, CHOCO-SGD with sign compression is compared to a centralized baseline where all nodes route their updates to a central coordinator for aggregation. In decentralized deep learning, CHOCO-SGD with sign compression is compared to decentralized SGD without compression and centralized SGD without compression. The scaling properties of CHOCO-SGD are studied on 4, 16, 36, and 64 nodes using different network topologies. Learning rates are tuned separately for all methods, with consensus learning rate tuned for CHOCO-SGD. The testing accuracy of CentralizedSGD outperforms CHOCO-SGD due to graph topology and communication compression effects, leading to slower convergence. Train and test performance are similar, indicating performance degradation is not a generalization issue. Increasing the number of epochs improves the performance of decentralized schemes, but even with 10 times more epochs, the gap between centralized and decentralized algorithms could not be fully closed. In a real decentralized scenario, the focus is on reducing communication to save on mobile data costs. The number of transmitted bits is fixed at 1000 MB for comparison, with CHOCO-SGD showing the best testing accuracy despite slight degradation with more nodes. In experiments on a real social network graph with 32 nodes, training models on user devices connected by the network showed that torus topology is beneficial for large numbers of nodes due to good mixing properties. Both Decentralized and Centralized SGD require significantly more bits for reasonable accuracy. In a study using the Davis Southern women social network with 32 nodes, ResNet20 and LSTM models were trained for image classification and language modeling tasks respectively. The results are presented in Figures 2-3 and Table 3. In image classification, the decentralized algorithm outperforms the centralized and quantized decentralized in training accuracy, but the centralized scheme has the highest test accuracy. CHOCO-SGD performs best in test accuracy with less transmitted data. For language modeling, both decentralized schemes face challenges. In large-scale training, CHOCO-SGD outperforms centralized SGD in test perplexity for Resnet-50 on ImageNet-1k. The benefits of CHOCO-SGD are more pronounced when scaling to more nodes in decentralized optimization methods. Decentralized optimization methods can address scaling issues even for well-connected devices like datacenters with fast InfiniBand or Ethernet connections. Recent studies have shown impressive speedups for training on multiple GPUs, but their algorithms differ from CHOCO-SGD with asynchronous updates and exact communication. The setup involves training ImageNet-1k with Resnet-50 on 8 machines with 4 Tesla P100 GPUs each, using decentralized communication with compressed communication in a ring topology. The setup involves training ImageNet-1k with Resnet-50 on 8 machines with 4 Tesla P100 GPUs each, using decentralized communication (sign-CHOCO-SGD) in a ring topology with a mini-batch size of 128 per GPU. CHOCO-SGD benefits from its decentralized and parallel structure, taking less time than all-reduce to perform the same number of epochs with only a slight 1.5% accuracy loss. Our study compares the test accuracy of CHOCO-SGD (76.37%) with another method (75.15%). Despite using different hardware, we achieved a 20% time gain over the common all-reduce baseline on commodity hardware. We suggest using CHOCO-SGD for decentralized deep learning in bandwidth-constrained environments, with theoretical convergence guarantees and linear speedup in the number of nodes. Our main contribution is enabling training in communication-restricted environments while respecting the locality of training data. The algorithm shows a linear speedup in the number of nodes and performs well in image classification and language modeling tasks. The algorithm demonstrates decentralized schemes for high communication compression, expanding applications of decentralized deep learning. The proof of Theorem 4.1 is presented, derived from a more general statement in Theorem A.2. The structure of the proof follows a previous study by Koloskova et al. (2019). Decentralized algorithms for high communication compression in deep learning are discussed, with a focus on stochastic gradient updates and averaging among nodes. Convergence of these algorithms is shown as long as the averaging scheme exhibits linear convergence. The specific averaging used in CHOCO-SGD has been proven to have linear convergence. Decentralized SGD with arbitrary averaging is outlined in Algorithm 3. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, where an averaging scheme is assumed to preserve iterates' average and converge linearly with a parameter 0 < c \u2264 1. Exact Averaging is an example that achieves consensus averaging at a rate of c = \u03c1, with \u03c1 being an eigengap of the mixing matrix W. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, where an averaging scheme is assumed to preserve iterates' average and converge linearly with a parameter 0 < c \u2264 1. Substituting \u03c1 as an eigengap of mixing matrix W into Algorithm 3 recovers D-PSGD algorithm. To recover CHOCO-SGD, CHOCO-GOSSIP is chosen as the consensus averaging scheme. The order of communication and gradient computation parts in Algorithm 1 is exchanged to illustrate their independence and parallel execution. Decentralized SGD with arbitrary averaging is presented in Algorithm 3, where an averaging scheme is assumed to preserve iterates' average and converge linearly with a parameter 0 < c \u2264 1. The iterates of Algorithm 3 with constant stepsize \u03b7 satisfy certain conditions, leading to a linear speed up compared to SGD on one node. The convergence rate of the underlying averaging scheme affects the overall performance. Decentralized SGD with arbitrary averaging in Algorithm 3 shows a linear speed up compared to SGD on one node. The convergence rate of the underlying averaging scheme affects the performance. CHOCO-SGD with CHOCO-GOSSIP averaging converges at a specific rate, with a dependence on the eigengap of the mixing matrix W. The theorem provides guarantees for the averaged vector of parameters x in decentralized settings, where averaging all parameters across machines is costly. Similar guarantees can be achieved on individual iterates x_i as shown in previous work. The text discusses guarantees on individual iterates x_i in decentralized settings, with a focus on convergence rates and step sizes in Algorithm 3. The results can be relaxed and hold for any T, but the convergence rate may be worse compared to previous theorems. The text discusses convergence rates and step sizes in Algorithm 3 for decentralized settings. The results hold for any T, but the convergence rate may be worse compared to previous theorems. Theorem A.4 is proven using L-smoothness, and Corollary A.5 shows convergence of local weights x (t) i under certain assumptions. Lemma B.1 and B.3 provide inequalities for vectors and matrices. The text discusses CHOCO-SGD algorithm for decentralized settings, demonstrating how to combine it with weight decay and momentum. Nesterov momentum can be adapted for this setting. CHOCO-SGD can be interpreted as an error feedback algorithm. The text discusses CHOCO-SGD algorithm for decentralized settings, demonstrating how to combine it with weight decay and momentum. Nesterov momentum can be adapted for this setting. CHOCO-SGD can be interpreted as an error feedback algorithm where quantization errors are saved into internal memory and added to the compressed value at the next iteration. In this section, the comparison is made between CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression for model training and hyper-parameter tuning. Two models are trained: ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. For the language modeling task on WikiText-2, a three-layer LSTM with hidden dimension of size 650 is used. The loss is averaged over all examples and timesteps, with a BPTT length set to 30. Gradient clipping (0.4) and dropout (0.4) are applied, and both ResNet20 and LSTM are trained for 300 epochs. The per node mini-batch size is 32, and the learning rate of CHOCO-SGD follows a linear scaling rule. The learning rate of CHOCO-SGD follows a linear scaling rule based on node degree. Momentum is only applied to ResNet20 training. The initial learning rate is gradually warmed up from 0.1 to the fine-tuned rate for the first 5 epochs. The learning rate is decayed by a factor of 10 at 50% and 75% of training epochs. The optimal learning rate per sample is determined by a linear scaling rule. The learning rate of CHOCO-SGD is determined by a linear scaling rule based on node degree. The optimal parameters are searched in a predefined grid to ensure the best performance. Fine-tuned hyperparameters for training ResNet-20 on Cifar10 and a social network topology are reported in tables. The hyperparameters for training ResNet-20/LSTM on a social network topology with 32 nodes are tuned for CHOCO-SGD. The training data is split between nodes without shuffling, with a mini-batch size of 32 per node and a maximum node degree of 14. The learning curve for the social network topology is also plotted. Each node can only access a subset of the dataset. The local mini-batch size is 32 for 32 nodes on a social network topology. Plots show training and test accuracy, as well as model consensus towards the end of optimization. Local models initially diverge from the averaged model before reaching consensus. The local models diverge from the averaged model as the stepsize decreases, a behavior also observed in a previous study by Assran et al. (2019)."
}