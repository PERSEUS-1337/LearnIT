{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. GANs can efficiently generate high-fidelity audio with log magnitudes and instantaneous frequencies, outperforming WaveNet baselines on various metrics. Neural audio synthesis faces challenges in modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on high quality but have slow sampling speeds. Generative Adversarial Networks (GANs) have been successful in generating high resolution images efficiently. They achieve global latent control by conditioning a stack of transposed waveforms one audio sample at a time. Research has focused on speeding up generation, but methods introduce significant overhead such as training secondary networks or writing customized low-level kernels. Large models operate at a fine timescale, limiting autoencoder variants to modeling local latent structure due to memory constraints. Efficient parallel sampling and global latent control in audio GANs through conditioning on a latent vector. Attempts to adapt image GAN architectures for audio waveform generation have not achieved the same level of fidelity. Frame-based techniques for audio waveforms focus on locally-coherent waves with periodicity. Frame-based techniques for audio waveforms, such as transposed convolutions or STFTs, struggle with aligning sinusoids due to differing periodicities. Transposed convolutional filters must cover all frequencies and phase alignments for phase coherence preservation. STFT allows unwrapping phase and deriving instantaneous radial frequency for better alignment. The instantaneous radial frequency expresses the constant relationship between audio frequency and frame frequency. GAN researchers have made rapid progress in image modeling by evaluating models on focused datasets with limited degrees of freedom, gradually advancing to less constrained domains. For example, the CelebA dataset is restricted to centered and cropped faces, providing a common reference for generating realistic texture and fine-scale features. Later models built on this foundation. The NSynth dataset was introduced with a focus on individual notes from musical instruments, aligned and cropped to emphasize fine-scale details like timbre and fidelity. Each note is accompanied by attribute labels for conditional generation. The dataset includes autoregressive WaveNet autoencoders and bottleneck models for audio generation. The original NSynth paper introduced autoregressive WaveNet and bottleneck spectrogram autoencoders. Follow-up work explored various approaches like frame-based regression models, inverse scattering networks, VAEs with perceptual priors, and adversarial regularization for domain transfer. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms. Convolutional filters trained for different tasks commonly learn to form frequency selective filter banks spanning human hearing range. Maintaining regularity of periodic signals over short to intermediate timescales is crucial for human perception. Alignment of frames with waveform periodicity is important, as there are typically many frequencies in a signal, posing a challenge for synthesis networks. Phase precession is a challenge for synthesis networks as they must learn frequency and phase combinations to produce coherent waveforms. It is similar to the phenomena observed in short-time Fourier transform (STFT) and occurs when filterbanks overlap. Another approach to generating coherent waveforms is inspired by the phase vocoder BID7, where a pure tone produces a precessing phase. The phase precesses and grows linearly by unwrapping it. The derivative of the unwrapped phase with respect to time is the instantaneous angular frequency, representing the true signal oscillation. The pure harmonic frequencies of a trumpet cause the wrapped phase spectra to oscillate at different rates. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Key findings include generating log-magnitude spectrograms and phases directly with GANs for more coherent waveforms, estimating IF spectra for even more coherence, and the importance of preventing harmonics from overlapping by increasing STFT frame size and switching to mel frequency scale. The study focuses on the NSynth dataset, where GANs outperform WaveNet in generating musical notes. Global conditioning on latent and pitch vectors allows GANs to create smooth timbre interpolation and consistent timbral identity across pitches. The NSynth dataset contains 300,000 musical notes from 1,000 instruments with labels for pitch, velocity, instrument, and acoustic qualities. Samples are four seconds long, sampled at 16kHz. Training was restricted to acoustic instruments and fundamental pitches MIDI 24-84 for natural sound. 70,379 examples were used for human evaluations on audio quality. We used 70,379 examples from instruments like strings, brass, woodwinds, and mallets. A new test/train 80/20 split was created from shuffled data. Inspired by image generation success, we adapted progressive training methods to generate audio spectra. The model samples a random vector from a spherical Gaussian and runs it through a stack of layers. The model generates output data by sampling a random vector from a spherical Gaussian and using transposed convolutions. A discriminator network estimates the difference between real and generated distributions. Gradient penalty and pixel normalization are used for Lipschitz continuity. Progressive and nonprogressive training variants show comparable quality, with slightly better convergence time and sample diversity for progressive training. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. An auxiliary classification BID24 loss is added to the discriminator to predict the pitch label. STFT magnitudes and phase angles are computed using TensorFlow's built-in implementation with specific parameters. In our method, we use an STFT with specific parameters to compute magnitudes and phase angles. The magnitudes are scaled to be between -1 and 1, while the phase angles are also scaled to match the tanh output nonlinearity of the generator network. Additionally, we have \"phase\" models and \"instantaneous frequency\" (\"IF\") models obtained by unwrapping the phase angle and taking the finite difference. In our method, we use an STFT with specific parameters to compute magnitudes and phase angles. The resulting models are \"instantaneous frequency\" (\"IF\") models. Performance is sensitive to having sufficient frequency resolution at the lower frequency range. By doubling the STFT frame size and stride with 75% overlap, we obtain spectral images with high frequency resolution. We also transform magnitudes and instantaneous frequencies to a mel frequency scale without dimensional compression, referred to as \"IF-Mel\" variants. In our method, we use an STFT with specific parameters to compute magnitudes and phase angles, resulting in \"instantaneous frequency\" models. Performance is sensitive to frequency resolution, which we achieve by doubling the STFT frame size and stride. We compare our models against WaveGAN, a state-of-the-art waveform generation GAN, and also train our own GANs with similar performance. WaveNet is currently the leading model in generative modeling. WaveNet is the state of the art in generative audio modeling. Strong WaveNet baselines are created by adapting the architecture to accept the same conditioning signal as GANs. The 8-bit mu law model outperforms the 16-bit model in stability and performance. Generative model evaluation is a challenging task. Generative models for audio generation face challenges in evaluation due to the difficulty in formalizing goals. Evaluation metrics are diverse to capture various aspects of model performance, with human evaluation considered the gold standard for audio quality assessment. Training networks to synthesize coherent waveforms is crucial, as human perception is highly sensitive to phase irregularities. Human perception is sensitive to phase irregularities, affecting listener experience. A comparison test using Amazon Mechanical Turk was conducted on models in TAB0, with participants rating audio quality on a Likert scale. 3600 ratings were collected, with each model involved in 800 comparisons. The diversity of generated examples was measured using the Number of Statistically-Different Bins (NDB) metric proposed by BID27. The diversity of generated examples is measured using the NDB metric proposed by BID27, which clusters training and generated examples into Voronoi cells in log-spectrogram space. Another metric, the Inception Score (IS), evaluates GANs by calculating the mean KL divergence between generated examples and a pretrained Inception classifier. The Inception Score (IS) measures the mean KL divergence between image-conditional output class probabilities and the marginal distribution. It penalizes models with examples that are not easily classified into a single class or belong to only a few classes. The metric is now called \"IS\" for consistency, but Inception features are replaced with features from a pitch classifier trained on spectrograms of the acoustic NSynth dataset. Additionally, Pitch Accuracy (PA) and Pitch Entropy (PE) are used to separately measure the accuracy of distinct pitches produced by models. The Fr\u00e9chet Inception Distance (FID) is a metric for evaluating GANs based on the 2-Wasserstein distance between multivariate Gaussians fit to features extracted from a pretrained Inception classifier. It correlates with perceptual quality and diversity on synthetic distributions. The accuracy of a pretrained pitch classifier on generated examples (PA) and the entropy of its output distribution (PE) are also measured separately. The audio quality of different model and representation variants was evaluated using human judgment and NDB score. IF-Mel was rated slightly inferior to real data, while WaveNet occasionally exhibited feedback issues. Sample diversity correlated with audio quality, and high frequency resolution improved NDB scores across model types. The WaveNet baseline model received the worst NDB score due to lack of diversity in autoregressive sampling. Histograms in the appendix show peaky distributions for different models. FID scores also reflect the impact of high frequency resolution on model performance. High-resolution models perform well on classifier metrics like IS, Pitch Accuracy, and Pitch Entropy, generating examples classified similarly to real data. However, phase models have high FID due to poor sample quality, even at high frequency resolution. Discriminative information is limited due to distribution discrepancies caused by mode collapse and other issues. The metrics provide a rough measure of model reliability in generating classifiable pitches, with low frequency models and baselines showing less reliability. Visualizing qualitative audio concepts is recommended, along with listening to accompanying audio examples for a better understanding. The WaveGAN and PhaseGAN models show phase irregularities in the waveform, creating a blurry web of lines, while the IFGAN model is more coherent with small variations. The Rainbowgrams display strong consistent colors for each harmonic in the real data and IF models, speckles in PhaseGAN, and irregularities in WaveGAN. Visualized examples from different GAN variants highlight the phase coherence differences. FIG1 visualizes the phase coherence of different GAN variants. Real data and IF models show consistent waveforms, while PhaseGAN has some discontinuities and WaveGAN is irregular. Rainbowgrams depict the log magnitude of frequencies, showing clear coherence in real data and IFGAN, while PhaseGAN shows noise and WaveGAN is largely incoherent. WaveGAN appears largely incoherent compared to other GAN variants. GANs allow conditioning on the same latent vector for the entire sequence, unlike autoregressive models like WaveNet. WaveNet autoencoders learn local latent codes controlling generation on a millisecond scale but have limited scope. Interpolating between examples in raw waveform, latent code, and global code of IF-Mel GAN is compared using a pretrained WaveNet autoencoder. Interpolating waveforms in WaveNet autoencoder and IF-Mel GAN global code results in mixing sounds, but WaveNet's linear interpolation fails due to complex latents, leading to oscillation and whistling. GAN model's spherical gaussian prior allows for smoother global interpolation. Interpolating waveforms in WaveNet autoencoder and IF-Mel GAN global code results in mixing sounds. WaveNet's linear interpolation fails due to complex latents, while GAN model's spherical gaussian prior allows for smoother global interpolation. The spherical interpolation in the IF-Mel GAN model allows for smooth timbre morphing between instruments while maintaining consistent pitch progression from Bach's Suite No. 1 in G major. The GAN model maintains consistent timbral identity across different pitches, creating a unique instrument character. Using upsampling convolutions allows for parallel processing during training and generation of audio samples. The IF-Mel GAN allows for parallel processing of training and generation on modern GPU hardware, significantly reducing latency for audio synthesis compared to WaveNet. This opens up the possibility for real-time neural network audio synthesis on devices. The work introduces the possibility of real-time neural network audio synthesis on devices, offering a wider range of expressive sounds compared to deep generative models focusing on speech synthesis. Adapting GANs for variable-length conditioning or recurrent generators is a potential future research direction. Our work proposes a modification to GANs for audio generation, improving training stability and architectural robustness. It builds on recent advances in GAN literature and aims to address the slow generation issue seen in autoregressive models for music synthesis. The NSynth dataset was introduced as a \"CelebA of audio\" and used WaveNet autoencoders for timbre interpolation. BID23 improved this by incorporating adversarial domain confusion loss for timbre transformations across various audio sources. BID5 achieved significant sampling speedups over WaveNet. BID5 achieved significant sampling speedups over WaveNet autoencoders by training a frame-based regression model to map pitch and instrument labels to raw waveforms. Their architecture, however, requires a large amount of channels, slowing down sample generation and training. High-quality audio generation with GANs on the NSynth dataset surpasses WaveNet baseline in fidelity and speed. Further research is needed to validate and expand to different types of natural sound. Potential for domain transfer and diverse applications of adversarial losses in audio generation. Issues like mode collapse and diversity in GANs also present in audio generation. In audio generation with GANs, combining adversarial losses with encoders or regression losses can capture the full data distribution. Different models were trained with the ADAM optimizer and varying learning rates and classifier loss weights. The best performance was seen with a learning rate of 8e-4 and classifier loss of 10. Progressive GAN techniques were used, including box upscaling/downscaling and pixel generators. The progressive GAN paper utilizes box upscaling/downscaling and pixel normalization in both networks. The discriminator includes the standard deviation of minibatch activations as a scalar channel. Real data is normalized before passing to the discriminator, with a Tanh output nonlinearity used for the generator. The log-magnitudes and phases are independently shifted and scaled to allow for outliers. The GAN variants are trained for 4.5 days on a single V100 GPU, with a batch size of 8. Progressive models train on 1.6M examples per stage (7 stages), with 800k during alpha blending and 800k after blending. The total examples for progressive models reach \u223c11M. The WaveNet baseline also uses a Tensorflow implementation with a decoder composed of 30 layers of dilated convolution. The decoder consists of 30 layers of dilated convolution with 512 channels and a receptive field of 3. It includes 3 stacks of 10 layers each, with increasing dilation from 20 to 29. The audio encoder stack is replaced with a conditioning stack operating on a one-hot pitch signal. This conditioning stack has 5 layers of dilated convolution, increasing to 25, and 3 layers of regular convolution, all with 512 channels. The conditioning signal is passed through a 1x1 convolution for each layer of the decoder and added to the output of each layer. For the 8-bit model, mulaw encoding and categorical loss are used, while the 16-bit model utilizes a quantized mixture of 10 logistics BID29. WaveNets reached convergence at 150k iterations in 2 days using 32 V100 GPUs with synchronous SGD training and a batch size of 1 per GPU."
}