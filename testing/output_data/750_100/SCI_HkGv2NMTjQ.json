{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with abstract super classes, serving as domain knowledge representation. However, ontology information is often overlooked in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, designed to preserve an ontological structure. The networks are trained and evaluated using common sound event classification datasets, showing improved performance by incorporating ontological information. Sounds in environments can be categorized into abstract classes like humans, emergency vehicles, and home, represented by ontologies. Ontologies, such as BID0, are used for sound event classification datasets. Neural networks are state-of-the-art for SEC but rarely consider ontologies. Ontologies provide structure to training data and network architecture through categories and relationships. Abstraction hierarchies are common in ontologies, where a super category represents subcategories. Hierarchies in sound event classifiers are defined by linguistics, with super categories representing subcategories. Taxonomies can be based on nouns or verbs, such as animal containing dog and cat. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Interactions between objects and materials, actions, and descriptors can also define taxonomies. Considering hierarchical relations in sound event classifiers offers multiple benefits. Hierarchies in sound event classifiers offer benefits such as allowing back-off to general categories, disambiguating acoustically similar classes, penalizing classification differently, and using domain knowledge to improve neural network models. Ontology-based network architectures have shown performance improvement in sound event classification. Authors in BID16 proposed an ontology-based deep restricted Boltzmann machine for textual topic classification, replicating a tree-like structure with intermediate layers. They demonstrated improved performance and reduced overfitting in training data. Another approach used perceptrons for each node in a hierarchy, showing enhanced performance through class disambiguation. Our proposed ontology-based networks aim to address class disambiguation using deep learning models. We introduce a framework for incorporating ontological information into deep learning architectures, including a Feed-forward model with an ontological layer to maintain consistency with the ontological structure. The framework extends the learning model to compute ontology-based embeddings using Siamese Neural Networks, considering ontologies with two levels. Training data consists of audio representations associated with labels from the ontology. The framework can be generalized to more levels. The ontology in the framework assigns labels to classes at different levels, with each class in a higher level mapped to one in the next level. For example, in a two-level ontology, classes like cat and dog are related to nature and music, respectively. This mapping helps in representing audio data with corresponding labels. The framework assigns labels to classes at different levels, with each class in a higher level mapped to one in the next level. This mapping helps in representing audio data with corresponding labels. In a given representation x \u2208 X, knowing the label y1 in C1 allows inferring its label in C2 using a probabilistic formulation. By estimating p(y2 |x) using a model, we can compute the estimation of p(y1 |x) and sum the values corresponding to the children of y2. This is valid for inference in the Feed-forward Network with Ontological Layer architecture. The text describes using a framework to assign labels to classes at different levels and how knowledge can improve model performance, especially for predicting classes at a higher level. The proposed framework is used to design ontology-based neural network architectures, including the introduction of an ontological layer. The Feed-forward Network (FFN) with Ontological Layer utilizes a base network to generate outputs for two ontology levels, C1 and C2. The base network learns weights for audio features input x to produce a vector z, which is then used to calculate probabilities for C1 and C2. The FFN can predict classes in C1 and C2 for any input after training. The ontological layer in the FFN model defines the weights for connecting super classes and sub classes in the ontology. Equation 3 represents this relationship as a directed graph using an incidence matrix M. These weights are not trainable but are essential for model training. In order to train the model, gradient-based methods are applied to minimize the loss function L, a combination of two categorical cross-entropy functions. The hyperparameter \u03bb \u2208 [0, 1] is tuned to balance the two functions. When \u03bb = 1, the model trains as a standard classifier using only information from the first level of the ontology. The goal is to create embeddings that preserve the ontological structure. The goal is to create embeddings that preserve the ontological structure using a Siamese neural network (SNN) architecture with a Feed-forward Network with Ontological Layer. The SNN enforces samples of the same class to be closer while separating samples of different classes, based on subclasses and superclasses. The Siamese neural network architecture enforces samples of the same class to be closer while separating samples of different classes based on subclasses and superclasses. The twin networks share weights and learn simultaneously, with ontological embeddings used to compute a Similarity metric. The distance between embeddings indicates the difference between samples, with specific distances assigned based on subclass and superclass relationships. The Feed-forward Model with Ontological layer using Ontology-based embeddings is trained with three types of audio example pairs to minimize the classification performance at different levels of the hierarchy in the sound event classification evaluation of ontological-based neural network architectures. The dataset for the Making Sense of Sounds Challenge 2 - MSoS aims to classify sound events. Challenge 2 - MSoS dataset for sound event classification consists of 1500 audio files from various sources, divided into 5 categories with different sound types. Evaluation dataset has 500 audio files, all in the same format. The dataset for sound event classification consists of 1500 audio files divided into 5 categories. Files are in single-channel 44.1 kHz, 16-bit .wav format. 80% of the set is for training, 10% for testing. Files are 5 seconds long with periods of silence. The evaluation set has 500 files in 5 classes. Urban Sounds -US8K dataset evaluates urban sound classification with a taxonomy adjusted to avoid redundant levels. The ontology for sound event classification includes 10 classes at level 1 and 4 classes at level 2. The dataset consists of 8,732 audio files in 10 subsets, with 9 folds for training and tuning parameters. Audio recordings are represented using state-of-the-art Walnet features BID1, with a 128-dimensional logmel-spectrogram vector computed for each audio file. The vector is transformed using a CNN trained on a balanced set of AudioSet. The architecture of the base network (Net) used in the experiment is a feed-forward multi-layer perceptron network with 4 layers: input layer (1024 dimensions), 2 dense layers (512 and 256 dimensions), and output layer (128 dimensions). The dense layers employ Batch Normalization, a dropout rate of 0.5, and ReLU activation function. Parameters in the Net box and those transforming vector z into p(y 1 |x) were tuned. The baseline models for different data sets did not include ontological information, consisting of the Base Network Architecture with an added output layer for level 1 or level 2. For level 1, this is similar to training the Feed-forward model with Ontological Layer using \u03bb = 1. The loss function for level 2 is not considered with \u03bb = 1. In contrast, the baseline model for level 2 does not have a layer for predicting y 1. The baseline model for level 2 does not include a layer for predicting y 1. Results of baseline models for MSoS and US8K data sets are shown in Table 1. The performance of the MSoS challenge was reported to be 0.81 for level 2. Training models with different values of \u03bb showed improved performance, especially with values other than 0 and 1. The ontological layer significantly impacts classification in both data sets. Using the ontological layer improves performance in both MSoS and US8K data sets. For MSoS, the best accuracy was achieved with \u03bb = 0.8, showing a 5.4% and 6% improvement in levels 1 and 2 respectively. In comparison, US8K saw a smaller improvement with the best accuracy at \u03bb = 0.7, resulting in a 2.5% and 0.2% improvement in levels 1 and 2. The architecture described in Section 2.3 evaluates the performance of ontology-based embeddings for sound event classification. t-SNE plots illustrate how the embeddings cluster at different levels, showing tighter and better defined clusters. The Walnet audio features were processed to train the Siamese neural network for ontology-based embeddings. The SNN was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for optimal performance. Testing different numbers of input pairs, 100,000 pairs yielded the best results. The experiment tested 100 to 1,000,000 pairs and found that 100,000 yielded the best performance. The loss function values were derived from previous experiments. Different lambda values were used for classifiers in level 1 and 2, affecting overall performance. Results showed accuracy performance for MSoS and US8K in level 1 and 2. The architecture outperformed the baseline but slightly underperformed without embeddings. The ontology-based embeddings showed better grouping with tighter and well-defined clusters compared to the method without embeddings. However, the performance on the US8K dataset was limited due to a similar number of sub classes and super classes. The study compared different approaches for sound event classification using hierarchical ontologies. The Feed-forward Network with Ontological Layer achieved 0.88 accuracy, while using ontological embeddings achieved 0.89, both outperforming the baseline of 0.80. The research proposed a framework for designing neural networks for sound event classification. In sound event classification, hierarchical ontologies were integrated into deep learning models using a Feed-forward Network with an ontological layer and a Siamese neural Network for ontology-based embeddings. Results showed improved performance over baselines, paving the way for further exploration of ontologies and relations in sound event classification. Further exploration of ontologies and relations is crucial for sound event classification due to the wide acoustic diversity and limited lexicalized terms to describe sounds."
}