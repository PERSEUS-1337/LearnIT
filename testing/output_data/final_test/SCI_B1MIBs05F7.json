{
    "title": "B1MIBs05F7",
    "content": "The application of stochastic variance reduction techniques to optimization has shown recent success, but their applicability to hard non-convex optimization problems in deep neural networks is uncertain. Naive application of SVRG and related approaches fail, as explored in this study. Stochastic variance reduction methods use control variates to reduce the variance of stochastic gradient descent estimates without introducing bias. Stochastic variance reduction techniques use control variates to reduce the variance of stochastic gradient descent estimates without bias. These methods achieve linear convergence rates for smooth strongly-convex optimization problems, improving upon the sub-linear rate of SGD. SVR methods exploit finite-sum structures through dual or primal approaches, and recent work has combined acceleration with variance reduction and extended SVR approaches to non-convex and saddle point problems. In this work, the behavior of variance reduction methods on non-convex problems, specifically a deep convolutional neural network for image classification, is studied. The challenges of applying variance reduction methods in practice are discussed, along with empirical studies on variance reduction in modern CNN architectures. Streaming variants of SVRG do not outperform regular SVRG, and properties of DNN problems give stochastic gradient descent an advantage over variance reduction techniques. In SVRG, training epochs are interlaced with snapshot points where a full gradient evaluation is performed. Snapshots can occur at any interval, although once per epoch is the most common frequency used in practice. The SGD step is augmented with the snapshot gradient using the control variate technique to form the SVRG step. The preference for recomputation or storage of the single-data point gradient depends on the computer architecture and its bottlenecks. In SVRG, training epochs are interlaced with snapshot points where a full gradient evaluation is performed. The preference for recomputation or storage of the single-data point gradient depends on the computer architecture and its bottlenecks. Following the control variate approach, the expected step conditioning on w k is just a gradient step, similar to SGD. Successive step directions are highly correlated due to the f(w) term appearing in every consecutive step between snapshots. Modern approaches to training deep neural networks deviate significantly from traditional SVR methods. Data augmentation is crucial for achieving state-of-the-art results in various domains. Transform functions are used, such as cropping, rotation, and flipping, before gradient calculation for a data-point. However, using random transforms can hinder variance reduction in standard SVRG. Different transforms applied during snapshot passes can disrupt the process. This issue arises when using standard libraries like PyTorch and TensorFlow, where transforms are automatically applied as part of the data pipeline. Data augmentation is essential for achieving top results in various fields. Transform locking is proposed to cache and reuse transforms during training epochs, improving performance. SVRG with transform locking shows zero variance at the start of an epoch, gradually increasing as expected. Without transform locking, variance is non-zero from the beginning and consistently worse. This addresses the handling of data augmentation in finite-sum methods. The handling of data augmentation in finite-sum methods has been previously considered for the MISO method BID4, which uses an exponential moving average to update stored gradients. Batch normalization BID14 breaks the finite-sum structure assumption by calculating mean and variance statistics within a minibatch for normalization. This interaction with SVRG depends on various factors. The interaction of Batch Normalization (BN) with Stochastic Variance Reduced Gradient (SVRG) depends on storage or recomputation of gradients. Catastrophic divergence occurs when gradients are recomputed naively. To apply BN at test time, mean and variance information must be stored during training. PyTorch updates the moving average of mean using mini-batch mean. At test time, the network switches to evaluation mode and uses stored running mean and variances. During training, using model.eval() switches to stored running mean and variances for normalization instead of mini-batch statistics. SVRG complicates gradient evaluations at current and snapshot iterates, leading to poor results and divergence. A BN reset approach temporarily stores normalization statistics before gradient evaluation to avoid modifying batch normalization library code. It is crucial to use train mode during the snapshot pass as well. During training, it is important to use train mode for both current and snapshot passes to ensure consistency in mini-batch statistics. Dropout can impact the finite-sum assumption by introducing sparsity in activations. Storing the dropout pattern during the snapshot pass and reusing it in the following epoch can help maintain correlation and variance reduction. Storing sparsity patterns directly is not practical due to the large number of patterns. During training, it is important to maintain consistency in mini-batch statistics by using train mode for both current and snapshot passes. Dropout introduces sparsity in activations, impacting the finite-sum assumption. Storing sparsity patterns directly is impractical due to the large number of patterns. Residual connection architectures benefit little from dropout when batch-norm is used, so it is not utilized in experiments. SVRG and SGD require computing the snapshot at an average or randomly chosen iterate from the epoch. Averaging is necessary for SGD in non-convex problems. Testing both methods with different tail averages showed faster initial convergence with a 10% tail average before the first step size reduction on the CIFAR10 test problem. SVRG achieved variance reduction on practical problems by comparing the variance of its gradient estimate to that of SGD. Transform locking and batch norm reset techniques were used to optimize SVRG performance. Ratios below one indicate variance reduction, while ratios around two suggest uncorrelated control variates increasing variance. Effective SVRG implementation requires careful consideration of these factors. SVRG achieved variance reduction by comparing gradient estimate variance to SGD. Model complexity affects variance reduction mid-epoch. Modern architectures benefit less from SVRG. Stochastic steps between snapshots impact SVRG performance. The SVRG method fails to reduce variance in modern high-capacity networks, even with adjustments to the snapshot interval. Reducing the duration can help keep SVRG variance reasonable compared to SGD. The SVRG method struggles to reduce variance in high-capacity networks, even with snapshot interval adjustments. For ResNet-110 and DenseNet models, SVRG step variance surpasses SGD early in the epoch, leading to increased wall-clock costs without improved performance. Performing snapshots less frequently also results in higher variance, hindering convergence. The gradient norm decays slowly during optimization for high-capacity networks, doubling the variance when added to the stochastic gradient. Increasing the learning rate can speed up convergence until it reaches a limit defined by the curvature. However, a ceiling is reached where the variance of the gradient estimate slows convergence, determining if variance reduction techniques like SVRG can allow for faster convergence with larger learning rates. Empirically, deep residual networks are initially constrained by curvature and later by variance. Increasing batch size to reduce variance allows for higher learning rates. SVRG may improve convergence by reducing variance, but for DenseNet and larger ResNet models, it does not show significant variance reduction. The variance of the SVRG estimator depends on the similarity of gradients between snapshot and current iterate. The gradient similarity between snapshot and current iterate affects variance reduction in optimization. Differences in movement speed through the landscape and gradient changes may explain variations. DenseNet model moves further from snapshot point compared to LeNet model. Curvature change is similar for both models, indicating similar Lipschitz smoothness constant. The curvature is highest at the beginning of an epoch due to the lack of smoothness of the objective. SVRG variants have shown promising results on small MNIST training problems, but may not be effective on larger problems. MNIST may not be a suitable baseline for optimization comparisons with small neural network architectures. Theoretical results for SVRG only apply to smooth objectives. The ELU activation function showed a small improvement in reducing variance, but no significant reduction was observed on the DenseNet model. One potential solution to improve variance reduction is to perform snapshots at finer intervals using streaming SVRG methods with mega-batches. The theory suggests taking a random number of SVRG inner loop steps, but often a fixed number of steps is used in practice. The Stochastically Controlled Stochastic Gradient (SCSG) method explores further variance reduction by sampling mini-batches from the mega-batch. Variance-overtime plots were produced to investigate the effectiveness of streaming SVRG methods, with mega-batches 10x larger than mini-batch size and 10 inner steps per snapshot. The data augmentation and batch norm reset techniques were used to reduce variance in the streaming SVRG method. Results showed variance reduction by 10x at the beginning of the 50th epoch, similar to non-streaming SVRG method. Convergence rates of SGD, SVRG, and SCSG were compared, with results shown in FIG6. The comparison of convergence rates between SGD, SVRG, and SCSG methods is shown in FIG6. SVRG shows variance reduction but introduces heavy correlation between steps, impacting convergence rates. The reduction in variance may not directly improve convergence rate as batch size or learning rate adjustments do. SVRG slightly outperforms SGD on the LeNet problem, while results on larger problems like ResNet and DenseNet on ImageNet are shown in FIG6. On larger problems like ResNet on ImageNet and DenseNet on CIFAR10, SVRG is slightly slower than SGD. The SCSG variant performs the worst in each comparison. SVRG's convergence rate is not dependent on the variance of the gradient, unlike SGD. SVRG may perform better for high gradient variance problems assuming comparable Lipschitz smoothness. Scaling the depth of ResNet architectures by 10 fold only increases gradient variance by approximately 25%, while scaling the width actually reduces the variance by about 10%. Deeper models with 100+ layers show worse test error at epoch 50 due to higher variance, leading to slower convergence with a fixed step size. Increasing model width while keeping the number of layers fixed results in lower gradient variance and test error at epoch 50. These findings suggest that lower gradient variance contributes to better performance. The results suggest that lower gradient variance contributes to the success of wider models like WRN BID28. While there is a test error gap between SVRG and SGD for deep models, wide models show no apparent gap. Despite negative results, stochastic variance reduction can still be explored further, potentially through adaptive application, meta-level adjustments to learning rates, scaling matrices, or hybrid methods with Adagrad BID9 and ADAM BID17."
}