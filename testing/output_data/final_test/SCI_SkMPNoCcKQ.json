{
    "title": "SkMPNoCcKQ",
    "content": "This work explores modeling non-linear visual processes using deep generative architectures to learn linear, Gaussian models of observed sequences. A joint learning framework is proposed, combining a multivariate autoregressive model and deep convolutional generative networks. The approach allows Variational Autoencoders and Generative Adversarial Networks to learn the non-linear observation and linear state-transition model simultaneously from observed frames. The method is demonstrated on toy examples and dynamic textures, addressing the challenges of unsupervised learning and generative modeling in deep learning. A generative model of visual processes enables the generation of video frames resembling the original training process without direct copying. This involves a probability model for individual frames and frame-to-frame transitions, often described as a multivariate autoregressive model. Linear transformations and Gaussian noise play a key role in this process, allowing for the analysis of long-term behavior. However, real-world visual processes typically involve non-linear frame transitions. Unsupervised learning has developed various approaches to fit MAR models to real-world processes, such as using linear low-rank or sparse approximations of frames. The success of GAN and VAE has sparked interest in deep generative learning for sequential processes. The approach involves linearization to simplify the model by learning latent representations of visual processes. The approach involves learning latent representations of visual processes to describe state-to-state transitions with an MAR model. A dynamic layer is introduced to jointly learn non-linear observations and linear state transitions, compatible with deep generative architectures like GANs and VAEs. The work focuses on representation learning and linearizing transformations under uncertainty using neural networks. The work involves learning latent representations of visual processes with an MAR model. It combines Linear Dynamic Systems with VAEs, focusing on control rather than synthesis. The model differs from previous works by modeling the transition distribution as a separate layer. The MAR model was learned separately from the GAN. The dynamic layer proposed in this work is similar to techniques used in image-to-image translation. Video synthesis discussed in the work focuses on finding a probabilistic model for spatial and temporal behavior, different from predicting future frames from previous ones. Video synthesis models need to consider long-term behavior due to their probabilistic nature. The core contribution of this work is combining neural networks with Markov processes. The Dynamic Texture (DT) model by BID8 has popularized LDSs in visual processes modeling, using an LDS with state transition matrix A, observation y, and observation matrix C. The observation matrix C \u2208 R d\u00d7n and vector y \u2208 R d represent an offset in the observation space. A more general dynamic model applicable to visual processes is defined, with assumptions of stationarity and the Markov property. The method can be extended to higher order Markov processes. The self map \u03d5 : M \u2192 M models the predicted frame transition. The self map \u03d5 : M \u2192 M models the predicted frame transition and describes the deterministic part of the dynamics. The function \u03c8 vt : M \u2192 M describes a displacement by v t in the tangent space T yt M of M at y t, followed by a retraction onto M. The displacement is sampled from i.i.d. zero-mean Gaussian noise. Eq. FORMULA0 is a special case of Eq. (2), where \u03d5(y) = CAC + (y \u2212\u0233) + \u0233 and \u03c8 v (y) = y + Cv. The model Eq. (2) describes a broader class of visual processes than Eq. (1), but the linear model Eq. (1) enables straightforward prediction, generation, and analysis of observations. It is of interest to find a model that linearizes real-world visual processes for a latent state space representation. The MAR model in Eq. (1) focuses on a non-linear dynamic system with a linear state transition and non-linear observation mapping. Transforming a non-linear model to this form simplifies video synthesis by sampling from autoregressive noise. The model is not unique with changes of basis in the state space, but implementing C via a neural network can account for this. Assumption 2 states that latent samples h t follow a standard normal distribution. If the state transition matrix A is given and the process is stationary, p(h t ) = p(h t+1 ). If the state transition matrix A is given and the process is stationary, Assumption 2 identifies the process noise model. To ensure that latent states h t remain standard Gaussian, the process noise should be zero-mean with covariance matrix I \u2212 AA. An algorithm is proposed to jointly learn C and A using a deep generative architecture. The model needs to transform the transition \u03d5 to a matrix multiplication and the perturbation \u03c8 vt to a superposition by zero-mean Gaussian noise. This can be achieved if the perturbation is small and C is a diffeomorphism. In control theory, linearization of dynamic systems is commonly done using a first-order Taylor polynomial around an equilibrium point. Assumption 3 states that the transition function \u03d5 must have at least one fixed point. To address the curse of dimensionality in real-world processes, a local linearizer function \u0393 can be used to map system observations to a lower-dimensional latent space before linearization. A matrix \u03a6 \u2208 R n\u00d7n allows for local linearization by moving the fixed point to the origin in a new coordinate system. If a differentiable self map \u03d5 on manifold M has a fixed point y * \u2208 M, and a diffeomorphism from M to R n exists, then \u03d5 can be approximated by a linear function near y * . Local linearizability is a concept used to analyze the feasibility of a problem by reparametrizing a local linearizer to match the shape of a manifold M. However, it does not address the differences in linearization properties on a global scale or provide guidance on finding an appropriate representation. Typically, learning the local linearizer and matrix separately is done by considering sampled observations. Global linearization problem is characterized by introducing a measure to consider the expected squared distance between a transformation \u03d5 and its linear approximation. The latent space is often used as a heuristic due to the lack of an analytic expression for the distribution on the data manifold M. Standard Gaussian distribution is assumed for the latent space, with \u0393 as a data representation mapping and \u03d5 as the transition function to be linearized. The expression includes a normalization factor and if \u0393(y) follows a standard Gaussian distribution, q(\u0393, \u03d5, \u03a6) < 1 indicates the linear prediction. If \u0393(y) has standard Gaussian distribution, q(\u0393, \u03d5, \u03a6) < 1 indicates that the linear prediction with \u03a6 is smaller than the expected distance of two independently drawn samples. Linearizing \u03d5 involves choosing a representation \u0393 with good linearization properties and inferring \u03a6 by minimizing Eq. (7). The proposed approach involves learning the representation and linear transition jointly to approximate the data, addressing the challenge of finding an appropriate model for \u0393 that can linearize the transformation \u03d5 effectively. To approximate the data distribution directly, the approach involves learning the representation and linear transition jointly. The goal is to find a model that approximates the given equation. The function C and matrix A need to be inferred from observed data to ensure that the probability distributions of succeeding frames coincide. The joint probability distribution of h t and h t+1 is zero-mean Gaussian. The aim is to find a function C and matrix A such that the random variable \u1ef9 has the same probability distribution as y 2. Estimation of probability distributions for high-dimensional data in deep learning involves approximating a function parameterized by a vector in euclidean space. Neural networks with trainable weights are commonly used in deep generative models like GAN and VAE. This work utilizes Wasserstein GAN and VAE to evaluate a proposed method. The techniques are assumed to be capable without further review. The VAE approximates a function mapping from low-dimensional Gaussian to high-dimensional data. Learning a matrix A and function C from a visual process involves generating frame pairs for training. The goal is to find an architecture to learn C and A such that the joint distribution of frames coincides with a given probability distribution. Using a deep generative architecture, a function f \u03b8 can transform Gaussian noise samples to high-dimensional data distribution samples. The matrices A and B are learned to fulfill a constraint, along with parameter \u03b7 defining the model. Training f \u03b8 can map Gaussian noise samples to s with desired probability distributions, fulfilling conditions for joint probability distribution of h 1 , h 2. The proposed neural network architecture for implementing f \u03b8 is depicted in a figure. The neural network architecture for implementing function f \u03b8 includes a dynamic layer with matrices A and B, split into upper and lower halves h1 and h2. The weights are trained with parameter \u03b7 using back-propagation, while ensuring the stationarity constraint is not violated by adding a regularizer to the loss function. The dynamic layer with matrices A and B, split into upper and lower halves h1 and h2, is used in combination with an affine layer at the input to implement the observer function C \u03b7. This layer serves multiple purposes, including accounting for changes of bases in the latent space, transforming the fixed point of the transition function to the origin, and reducing the latent dimension to facilitate the search for the transition matrix A. The training of C \u03b7 and A is done simultaneously using a dynamic layer, integrated as the decoder of a VAE and the generator of a Wasserstein GAN. The original discriminator of the DCGAN is used as the critic network of the Wasserstein GAN. For the Wasserstein GAN, the original discriminator of the DCGAN is used with doubled output channels to match training frame pairs. The VAE encoder uses the DCGAN discriminator with 2n output channels, where n is the latent dimension set to 10. Synthesis involves sampling from the MAR model and mapping latent states to the observation space. Latent states are sampled from Gaussian noise for Wasserstein GAN and estimated using the encoder for VAE. PyTorch code for experiments will be available online. The isotropic standard Gaussian model N(s; f\u03b8(x), \u03c3^2) is used for conditional data distribution. Adam optimizer with step size 2.5 * 10^-4 and regularizer constant \u03bb = 100 are employed. Experiments with MNIST dataset generate repeating sequences of hand written digits using \u03c3 = 4 and sequences of length 10000 for training. Higher \u03c3 values improve correct sequence synthesis probability but decrease digit shape variability. Small NORB dataset is used to investigate linearization of geometrical transformations. The Small NORB dataset is used to study linearization of geometrical transformations. The architecture is trained to synthesize sequences depicting azimuthal rotations of 20\u00b0 per frame, yielding recognizable rotation movements for animals. However, the algorithm confuses pairs of opposite poses, possibly due to the lack of a bijective mapping from the image manifold to the embedded space. The text discusses the use of a non-injective local diffeomorphism in dynamic texture synthesis experiments using the Wasserstein GAN. The method is tested on the UCLA-50 dataset, trained for 2000 epochs, and results are shown for classes candle and fountain-c-far. The method is evaluated on three RGB sequences using RMSPROP optimization for 900 epochs. Results for firepot, springwater, and waterfountain sequences from BID29 are shown in FIG5. Despite the simplicity of the model, the difference in quality compared to state-of-the-art results in BID29 is considered negligible. The approach involves learning embedded MAR models from image sequences, introducing the concept of local linearizability and utilizing deep generative models with a dynamic layer. Positive results are reported for low-resolution visual processes with a first-order Markov property assumed, aiming to explore the nature of linearization in future research. The text discusses linearization in visual processes, focusing on the Jacobian matrices of diffeomorphisms. It explores the conditions for fulfilling specific requirements related to linearizing representations. The goal is to improve theoretical understanding and applicability in non-stationary visual processes. The proposed method aims to linearize first-order Markov processes, with potential for generalization to higher orders. A procedure involving block matrix modeling and dynamic layers is presented, with the need for further investigation on feasibility and practical applicability. The method involves introducing regularizers to preserve the block Toeplitz structure of the covariance matrix and computing MAR parameters. The procedure is illustrated for the case m = 2, with a second-order MAR model and system of equations to infer the parameters."
}