{
    "title": "r1glDpNYwS",
    "content": "In this paper, the authors aim to design attacks that make classifiers generate wrong labels imperceptible to human observers. They propose an algorithm called LabelFool to achieve this by identifying a target label similar to the ground truth label and finding a perturbation for this target label. Experimental results show that the attack is less recognizable by humans in the label space. Deep neural networks achieve state-of-the-art performance in pattern recognition tasks, but can be fooled by adversarial samples. Attackers generate these samples to make classifiers fail. The ideal attacker should meet three requirements, including fooling networks by causing misclassification. The ideal attacker in adversarial attacks should meet three requirements: achieving a high attack rate, imperceptibility in the image space, and imperceptibility of the error made by the classifier in the label space. Various methods exist to generate imperceptible adversarial samples, including minimizing additive perturbations with lp norm and changing only a few pixels to cause classifier failures. Universal perturbations that are image-agnostic have also been demonstrated. The imperceptibility of the error made by the classifier in the label space is crucial in adversarial attacks. It involves misclassifying an image as a similar label to its ground truth to avoid detection by humans. This imperceptibility is necessary to lower observers' guard and allow the attack to succeed. In adversarial attacks, perturbations in the label space can cause confusion on the input terminal. Observers will notice misclassifications on the output terminal, leading to defensive measures. The imperceptibility in the label space is crucial for fooling classifiers. The proposed untargeted-attack algorithm, LabelFool, aims to misclassify images subtly to avoid detection. The proposed untargeted-attack algorithm, LabelFool, aims to generate adversarial samples that are less recognizable in the label space by human observers. Objective experiments on ImageNet show that these samples maintain imperceptibility in the image space while achieving a high attack rate in fooling classifiers. The proposed untargeted-attack algorithm, LabelFool, aims to generate adversarial samples that are less recognizable in the label space by human observers. Many researchers have studied how to generate adversarial samples, with methods like FGSM and CW attack. However, there has been no guide on choosing a target label that is difficult to notice when the network fails. Imperceptibility in the image space is also crucial, as highlighted by researchers. Researchers have explored various attacks like one-pixel attack, SparseFool, and DeepFool to generate adversarial samples with minimal image-level distortion and perturbing only a few pixels. DeepFool approximates multi-dimensional classification boundaries in two dimensions, potentially leading to errors in finding the nearest class. Researchers have explored attacks like FGSM, DeepFool, and SparseFool to create adversarial samples with minimal distortion. The method discussed in this paper focuses on choosing an undetectable target label and perturbing the input image to achieve misclassification. The approach is compared to existing attacks to highlight its advantages in imperceptibility within the label space. Symbols and notations used are summarized in Table 1. LabelFool, inspired by DeepFool, aims to choose a target label similar to the ground truth of an input image and perturb the image to be classified as this label. The method involves selecting a target label imperceptible to human observers and finding the most similar label to the input image's ground truth using a perceptual distance metric. We propose a weighted distance model to estimate the probability distribution of an input's ground truth label, allowing us to compute the distance between classes in a dataset and choose the nearest one as the target class. By using pre-trained image classification models to extract features, we can calculate the distance in the perceptual distance metric using cosine distance. The distance between classes in a dataset can be computed using the Hausdorff distance. This information is used to determine the target label for an input image by estimating the probability distribution of the ground truth label. Machine learning classifiers output a predicted label and a probability vector, which helps in finding the nearest label to the input image. The probability vector elements represent the likelihood of an input image belonging to a certain class. The distance function between an input image and classes in a dataset helps in determining the target label. Maximum Likelihood Estimation is used when the probability exceeds a threshold. Notations and symbols used in the paper are defined for clarity. The predicted class of an input image is determined by a distance function between classes and the ground truth label. Thresholds are used to sample labels for computing weighted distances, with some labels being abandoned based on probability thresholds. The predicted class of an input image is determined by a distance function between classes and the ground truth label. Thresholds are used to sample labels for computing weighted distances, with some labels being abandoned based on probability thresholds. The target label for an input image is chosen based on the minimum expected distance with all possible labels, computed using Importance Sampling. Once the target label is determined, the next step is to attack the input image to be mis-classified as the target label. The target label for an input image is chosen based on the minimum expected distance with all possible labels. The next step is to attack the input image to be mis-classified as the target label using targeted-attack algorithms like targeted-FGSM and targeted-CW. The method moves the input towards the boundary until it is classified as the target label. The method proposed aims to attack an input image to be mis-classified as the target label with tiny perturbations in the image space, inspired by DeepFool. The mathematical derivation is similar to DeepFool, with the main difference being the selection of a target label in the first step. The approach involves moving the input image towards the boundary until it is classified as the target label. The method proposed aims to attack an input image to be mis-classified as the target label with tiny perturbations in the image space. It involves moving the input image towards the boundary until it is classified as the target label. Experiments are conducted on ImageNet dataset for classification tasks. LabelFool is tested on the ImageNet-train split with validation and test images. The method is compared to DeepFool, FGSM, and SparseFool, showing its ability to deceive humans, maintain image quality, and fool neural networks. The study tested three attack methods: DeepFool, FGSM, and SparseFool on 600 randomly sampled source images from ImageNet-train split. Each image produced four adversarial images and a baseline image, totaling 3000 puzzles for human observers to determine the correctness of the labels. The experiment involved 10 observers to assess the mis-classified labels. In the study, 10 observers (3 females, 7 males aged 20-29) participated in a subjective experiment evaluating Performance Gain (PG) as an index. Human observers rated puzzles as \"True\" or \"False,\" with the Confusion Rate (CR) indicating the level of confusion. Performance Gain (PG) is a relative indicator comparing confusion rates between attack methods and baselines, normalizing the results. The formula for PG is shown in Eq. (5), where CR A is the attacker's confusion rate and CR B is the baseline's confusion rate. Results show a significant improvement in performance gain compared to FGSM and SparseFool, with a 25% and 30% increase respectively. Even compared to DeepFool, there is still a 3% improvement in performance gain. Attacks on animal images result in imperceptible label changes, making it difficult for humans to notice the attack taking place. In a subjective experiment, 247 out of 600 source images were animal images. The average performance gain of 10 observers in animal images showed nearly 90% improvement compared to FGSM and SparseFool, and about 50% improvement compared to DeepFool. The improvement in the label space did not result in a huge loss in the image space. Three metrics were used to demonstrate performance in the image space. In the label space, performance improvement did not lead to significant image quality loss. Three metrics were used to evaluate image space performance: perceptibility, perceptual similarity, and PieAPP. These metrics measure RGB intensities, perceptual distance, and perceptual error, respectively. Adversarial samples were generated and tested on 1000 randomly chosen ImageNet images, with the goal of minimizing the three metrics. In the experiment, four classifiers were tested on adversarial samples, showing LabelFool performing better than FGSM and SparseFool but slightly worse than DeepFool. Visual results suggest human observers cannot distinguish between LabelFool and DeepFool. Attack rates were evaluated in the last experiment, with results shown in Table 2. In experiments with four classifiers, LabelFool outperformed FGSM and SparseFool but was slightly behind DeepFool. LabelFool had the highest attack rate on all models, possibly due to its unique probability model for choosing target labels. This approach improves attack rates by avoiding mistakes when the classifier fails to give a correct classification. In this study, the importance of imperceptibility in the label space for adversarial samples is highlighted. A method called LabelFool is introduced to perturb input images to be mis-classified as a target label, making the misclassification less noticeable to human observers. By using Importance Sampling instead of MLE, the success rate of attacks is improved. In this paper, a feasible method to generate adversarial samples that confuse people in the label space is proposed. Suggestions for future research include optimizing perceptual features with a well-designed loss function and considering semantic distance in addition to perceptual distance. An interface for subjective experiments is also presented. The original data for Figure 3 and 4 is provided in Tables 3, 4, 5, and 6 for convenience in case of Figure 3 or 4 rewriting. An example is given to show why LabelFool has the highest attack rate compared to other methods like DeepFool. An instance is shown where the classifier fails to correctly classify an input image, leading DeepFool to perturb the image towards the correct class. LabelFool successfully attacks by choosing class 394 as the target class with the minimum expected distance from the top 3 classes. This highlights the importance of generating imperceptible adversarial examples in the label space, even for image classification tasks like face recognition systems. LabelFool aims to deceive face recognition systems by misclassifying individuals entering a gate. While untargeted attacks may result in misclassifying different individuals, it is harder to detect if the system misclassifies similar-looking individuals. This poses a significant security risk if a fake individual is allowed entry."
}