{
    "title": "rkvDssyRb",
    "content": "We distribute a single-agent RL problem to $n$ advisors who provide action values to an aggregator. Existing planning methods have flaws: \\textit{egocentric} overestimates values where advisors disagree, and \\textit{agnostic} is inefficient in danger zones. We propose a new approach called \\textit{empathic} and validate it on a fruit collection task. Seeking advice is crucial when facing complex problems. The individual seeks advice from various sources, including relatives, the internet, and specialists, to make an informed decision. Papers discuss breaking down a Reinforcement Learning task into simpler ones, with agents trained independently and then aggregated into a global policy. Multi-Advisor RL partitions a single-agent RL into a MultiAgent RL problem, utilizing a divide & conquer approach. The Multi-Advisor RL approach divides a single-agent RL problem into a MultiAgent RL problem, with advisors providing local Q-values for all actions. Advisors focus on different aspects like reward function, state space, and learning techniques. Different advisor planning methods include egocentric, agnostic, and empathic approaches. Egocentric planning leads to overestimation of state values where advisors disagree. The Multi-Advisor RL approach divides a single-agent RL problem into a MultiAgent RL problem, with advisors providing local Q-values for all actions. Advisors focus on different aspects like reward function, state space, and learning techniques. Egocentric planning leads to overestimation of state values where advisors disagree. Lowering the discount factor \u03b3 can avoid attractors in a navigation task. Agnostic planning BID30 is inefficient in dangerous environments due to fear of bad actions. Empathic planning converges to the global optimal Bellman equation. Distributed architecture speeds up learning and converges to better solutions. Egocentric planning gets stuck in attractors with high \u03b3 values. Our theoretical analysis shows that egocentric planning can get stuck in attractors with high \u03b3 values, while low \u03b3 values lead to high scores but instability with noise. Agnostic planning struggles to efficiently gather fruits near ghosts. Our novel empathic planning achieves high scores and is robust to noise despite lack of convergence guarantees with partial information. Task decomposition in literature includes distributing a single-agent RL problem among specialized advisors through methods like reward segmentation and state space reduction. Our focus is on these methods, but the findings are applicable to any advisor family. BID23 propose merging Markov decision Processes through value functions with strong assumptions like positive rewards and model-based RL. They accompany a classical RL algorithm by pruning suboptimal actions. BID24 suggest using a local SARSA online learning algorithm for training advisors, but face challenges with accurately estimating online policy with partial state space. BID21 delve into theoretical guarantees of convergence with local Q-learning and SARSA algorithms, but limit training advisors on local state space. BID30 relax this assumption at the expense of optimality guarantees. Van BID30 relaxes assumptions for optimality guarantees and beats Ms. Pac-Man by decomposing tasks into subtasks. MAd-RL is a generalization of Ensemble Learning for RL. BID25 uses a boosting algorithm on policies in RL. BID32 combines five online RL algorithms, showing mixture models perform better. BID6 focuses on explicit and deterministic transitions in planning methods. In Section 4, the planning method choice is crucial, with recommendations based on task definition. In BID11, advisors use reward shaping variants of the same function, akin to a bagging procedure. Advisors' recommendations are aggregated under the HORDE architecture BID28, with egocentric planning. Two aggregator functions tested are majority voting and ranked voting. BID14 opts for selecting the best advisor instead of aggregating recommendations, beneficial for staggered learning but not variance reduction. The UNREAL architecture BID12 and bootstrapped DQN architecture BID17 improve Atari and Labyrinth domains by training deep networks on auxiliary tasks in an unsupervised manner. They do not break tasks into smaller pieces, unlike other papers on the subject. Theoretical obstacles are identified in BID23 and BID21, but do not go beyond observing non-optimality. In this article, the pros and cons of different planning methods in Multi-Advisor RL are analyzed, with a focus on the fruit collection task in the Pac-Boy game. The game involves navigating a maze with 76 possible positions and 4 actions. The Pac-Boy game involves navigating a maze with 76 possible positions and 4 actions. The game consists of Pac-Boy, fruits, and ghosts, with 75 potential fruit positions that are randomly distributed. Pac-Boy gets a reward for eating fruit and a penalty for touching a ghost. The game lasts until all fruits are eaten or after 300 time steps, with approximately 10^28 states. The Reinforcement Learning framework is formalized as a Markov Decision Process (MDP) with state space X, action space A, transition function P, reward function R, and discount factor \u03b3. Trajectories in the MDP aim to maximize cumulative reward. The Multi-Advisor RL (MAd-RL) framework involves specialized advisors and an aggregator for solving single-agent RL problems. The Multi-Advisor RL framework involves specialized advisors and an aggregator for solving single-agent RL problems. The aggregator merges advisors' recommendations into a global policy using an f function. This function maps received Q-values from advisors into actions. Different aggregator functions can be implemented, such as voting schemes, Boltzmann policy mixtures, and linear value-function combinations. The focus is on analyzing how local Q-functions are computed for further analysis. The Multi-Advisor RL framework involves specialized advisors and an aggregator for solving single-agent RL problems. The aggregator merges advisors' recommendations into a global policy using an f function. This function maps received Q-values from advisors into actions. Different aggregator functions can be implemented, such as voting schemes, Boltzmann policy mixtures, and linear value-function combinations. The focus is on analyzing how local Q-functions are computed for further analysis. In the further analysis, the rewards are linearly decomposed, and the advisor's state representation is locally defined. The main theoretical result ensures that the advisors' training eventually converges under certain conditions. The Multi-Advisor RL framework involves specialized advisors and an aggregator for solving single-agent RL problems. The advisors use off-policy algorithms that converge for global convergence. The planning methods at the advisor's level include egocentric, agnostic, and empathic planning. The most common approach is egocentric planning, where the advisor evaluates the local greedy policy. Egocentric planning, used in Multi-Advisor RL, guarantees convergence to local optimal value functions for each advisor. However, it can overestimate state-action values when advisors disagree on optimal actions, leading to attractor situations. This flaw is illustrated with a simple MDP in FIG1. In a simple MDP depicted in FIG1, the system in initial state x0 has three possible actions: stay put (a0), perform advisor 1's goal (a1), or perform advisor 2's goal (a2). The Q-values for each action can be easily computed. If certain conditions are met, the egocentric planning may lead to sub-optimal decisions, similar to Buridan's ass paradox. The determinism of judgement is identified as the source of the issue in antic philosophy. An attractor x is a state where the optimal egocentric policy is to stay if possible, regardless of actions available. The system may be stuck in an attractor set, unable to achieve its goals, or escape due to lack of actions. The lack of actions can keep the system in an attractor set. An advisor is considered progressive if no action is worse than doing nothing. If all advisors are progressive, there cannot be any attractor. This condition is restrictive but applicable in some RL problems like resource scheduling. A MAd-RL setting without attractors does not guarantee optimality for egocentric planning. The lack of actions can lead to attractors in RL problems, but this does not ensure optimality for egocentric planning. Theorem 3 does not apply to RL problems with terminating states or navigation tasks. Navigation problem attractors involve moving towards one goal while moving further away from others. The expression for each action Q-value is given, leading to specific actions in certain scenarios. Theorem 4 is proven in a deterministic task setting. The condition on \u03b3 is a function of the size of the action set A. Theorem 4 in Section A of the appendix states that x \u2208 X is not an attractor if \u2200a \u2208 A, \u2203a -1x \u2208 A. Agnostic planning evaluates the random policy without making prior assumptions on future actions. The convergence of local optimization to its optimal value is guaranteed by Theorem 1. Local agnostic planning is equivalent to global agnostic planning, with no attractors present. However, acting greedily based on Q agn \u03a3 (x, a) may not always be optimal. The novel approach of empathic planning in MAd-RL involves locally predicting the aggregator's policy, leading to convergence to the global optimal policy. This method ensures that the advisors evaluate the current aggregator's greedy policy with respect to their local focus, ultimately reaching the optimal value function. The MAd-RL architecture utilizes state space reduction for faster learning, with no guarantee of convergence due to approximating the function locally. A fruit collection task is used to demonstrate the ease of learning the value function with a deep neural network. Different objective functions are considered, such as optimal number of turns and optimal return, to train the network. The DNN is trained on a fruit collection task using different objective functions like optimal return and egocentric planning MAd-RL. Results show that the TSP objective is the hardest to train on, while the RL objective follows as the second hardest. The egocentric planning MAd-RL objective is easier to train on. The egocentric planning MAd-RL objective is easier to train on, even without state space reduction or reward decomposition. Training is further accelerated when the target value is decomposed. MAd-RL performance decreases when \u03b3 approaches 1 due to attractors. Decomposing the objective function in the MAd-RL fashion simplifies training. Empirical validation in the Pac-Boy domain confirms these findings. MAd-RL settings involve one advisor per potential fruit location in a local state space of agent position and fruit existence. The curr_chunk discusses different settings in MAd-RL for agent positioning and fruit collection. It compares baselines like linear Q-learning and DQN-clipped with egocentric and empathic settings. The implementation details are available in the appendix. Video links show trajectories at the 50th epoch. Egocentric-\u03b3 = 0.4 shows a near-optimal policy for fruit collection. Ghost avoidance is better with egocentric and small \u03b3. The small-\u03b3 policy of moving towards the closest fruits is suboptimal but near optimal. The small \u03b3 setting provides an advantage in local optimization, ensuring perfect control near ghosts. In egocentric-\u03b3 = 0.9, an attractor phenomenon is observed where Pac-Boy moves to the center and avoids ghosts effectively. Empathic performs almost as well as egocentric-\u03b3 = 0.4, while agnostic struggles to finish fruits due to fear of ghosts. Dynamic normalization is used for agnostic planning based on remaining fruits. DQN-clipped also faces challenges in eating the last fruits, as confirmed by quantitative analysis in FIG6. Results from FIG6 confirm qualitative video-based impressions. egocentric-\u03b3 = 0.9 slightly outperforms linear Q-learning, DQN-clipped struggles with optimal performance and encounters ghosts, agnostic rarely eats all fruits, while egocentric-\u03b3 = 0.4 and empathic are near-optimal. egocentric-\u03b3 = 0.4 trains faster and finishes the game 20 turns sooner. Using a small \u03b3 may distort the objective function and diminish the reward signal exponentially based on distance. The experiment involved adding Gaussian centred white noise to the reward signal to test the performance of empathic planning compared to egocentric planning. Results showed that empathic planning outperformed egocentric planning even with 100 times larger noise variance. Egocentric planning was limited by high noise values, making it myopic and unable to perceive distant fruits. The article discusses MAd-RL, a framework for decomposing single-agent RL problems into simpler problems tackled by independent learners. It compares three types of local planning performed by advisors: egocentric, agnostic, and empathic. Egocentric planning has convergence guarantees but overestimates values in states of disagreement, leading to attractors. Empathic planning is shown to outperform egocentric planning even with high noise variance. In some domains, attractors can cause issues in MAd-RL systems, leading to a preference for no-op actions over progressing on subtasks. Certain domains like resource scheduling are attractor-free, while others like navigation require specific conditions to avoid attractors. An attractor-free setting ensures continuous progress towards goals, but doesn't guarantee optimal convergence. Agnostic planning offers convergence guarantees but may lead to suboptimal solutions, especially in dangerous environments where it favors avoiding risky situations like crossing bridges. The agnostic planning simplicity allows the use of general value functions, while empathic planning optimizes the system according to the global Bellman optimality equation. In experiments, convergence was always obtained, and a near optimal policy was learned in the Pac-Boy domain after 10 epochs. The approach can be applied to Ensemble RL tasks and ensures global convergence under certain conditions. Theorem 2 states that a state x is an attractor if the optimal egocentric policy is to stay in x if possible. The proof involves demonstrating two converse conditionals. The first condition shows that if x is an attractor, then staying in x is preferred. The second condition shows that if staying in x is preferred, then x is an attractor. Theorem 3 states that if all advisors are progressive, there cannot be any attractor. Theorem 4 guarantees that a state x is not an attractor if certain conditions are met. The proof involves defining the set of advisors for which an action is optimal in a state, and maximizing the perceived value of performing an action in that state. The condition for x not being an attractor is derived from comparing results with Definition 1. In the Taxi Domain, fruit and agent positions are represented in a 50-dimensional bit vector. A DNN with hidden layers is used to calculate state-values. Training is done for each discount factor setting to assess the complexity of the value function. The DNNs are trained over 500 epochs using the Adam optimizer. Four objective function targets are considered: TSP, RL, and summed egocentric planning MAd-RL. The TSP objective function is based on the Travelling Salesman Problem, the RL objective function depends on the discount factor \u03b3, and the MAd-RL objective function does not involve attractors. The summed egocentric planning MAd-RL objective function target simplifies the search process by not involving permutations. Each advisor is assigned to a specific source of reward or penalty, such as fruit locations or ghosts, with distinct state spaces for each. The learning process involves training fruit advisors through Temporal Difference updates in a small state space using tabular representation. Off-policy learning with Bellman residuals and a constant \u03b1 = 0.1 parameter is utilized. Q-values are aggregated for action selection, and ghost agents benefit from direct knowledge transfer by sharing Q-tables. In the Pac-Boy domain, Theorem 4 provides conditions for attractors in the MDP. The \u03b3 condition is necessary and sufficient for attractors, with stability dependent on available actions. A relaxed condition for not being stuck in an attractor set is \u03b3 \u2264 1/(|A|\u22122). The DQN algorithm with reward clipping is a baseline for comparison. The MAd-RL model uses a 4-channel binary image input with features like walls, ghosts, fruits, and Pac-Boy. Linear Q-learning is used with a sparse binary feature vector. Training deep architectures from these features was unsuccessful. The experimental setting includes 50 epochs with 20,000 transitions each and evaluation phase for 80 games. The maximum expected score is 37.5. Videos were linked using www.streamable.com for anonymity. Videos were linked using www.streamable.com for anonymity, including egocentric-\u03b3 = 0.4, egocentric-\u03b3 = 0.9, empathic, agnostic, and DQN-clipped videos."
}