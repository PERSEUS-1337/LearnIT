{
    "title": "Byl8BnRcYm",
    "content": "Node embeddings learned from Graph Neural Networks (GNNs) have been successfully used in various node-based applications, achieving state-of-the-art performance. However, when these embeddings are applied to generate graph embeddings, the scalar node representation may not efficiently preserve node/graph properties, leading to sub-optimal results. The Capsule Graph Neural Network (CapsGNN) improves upon existing GNN-based graph embeddings by using capsules to extract node features and a routing mechanism to capture important graph information. This model generates multiple embeddings for each graph, incorporating an attention module to focus on critical parts and handle graphs of various sizes. CapsGNN outperforms other techniques on graph classification tasks by capturing macroscopic properties of the whole graph through data-driven mechanisms. It focuses on Convolution-based Graph Neural Networks, which incorporate convolution operations from spectral or spatial perspectives. The method defined in BID1 is computationally expensive and non-spatially localized. BID methods introduce various techniques to improve graph neural networks, such as Chebyshev expansion and 1-step neighbor node convolution to reduce complexity. These methods focus on defining node receptive-fields and performing convolutions within them to generate new node representations. GNN's success in node representation learning has led to the development of deep-learning approaches that leverage node embeddings for graph-based applications. When generating graph embeddings, existing methods treat node representations as scalar features instead of vectors, potentially limiting the preservation of node and graph properties. To create high-quality graph embeddings, it is crucial to capture detailed properties like position and connection efficiently, which encoding as scalars may not achieve effectively. When applying GNN to graph representation learning, extending scalar to vector is proposed to preserve node/graph properties more efficiently. This technique, introduced in BID5 and improved in BID16 and BID6, involves extracting features as vectors known as capsules. Using capsules allows for a routing mechanism to generate high-level features, which is believed to be a more efficient way for feature encoding compared to max-pooling in CNN. Capsule Graph Neural Network (CapsGNN) is a novel deep learning architecture inspired by CapsNet. It uses node features from GNN to create high-quality graph embeddings. Each graph is represented with multiple embeddings, reflecting different properties. Routing preserves all information from low-level capsules, making it more representative than scalar-based approaches. CapsGNN utilizes GNN and routing mechanism to generate high-level graph capsules and class capsules. An Attention Module is applied to handle graphs of various sizes by assigning different weights to each capsule of each node. The model achieves state-of-the-art performance on 6 out of 10 benchmark datasets and comparable results on the rest. T-SNE visualization shows that different graph capsules capture distinct information. The text also introduces Graph Convolutional Networks (GCNs), routing mechanism in CapsNet, and Attention mechanism used in CapsGNN. CapsGNN utilizes GNN architecture, specifically GCN, to represent weighted directed graphs. The convolution operation is applied to each node and its neighbors at each layer, computing a new node representation through an activation function. This process involves trainable weights matrices, nonlinear activation functions, and an information transform matrix. The architecture proposed by BID8 in CapsGNN utilizes a nonlinear activation function and an information transform matrix calculated from the adjacency matrix A. It stacks L layers to generate final node embeddings ZL, with each layer considering adjacent nodes within l steps. The concept of capsules, inspired by Hinton's team, is used to generate graph capsules, representing features with vectors instead of scalar values like in traditional CNNs. The CapsNet architecture represents features with capsules (vectors) based on scalar values in feature maps. Capsules in Caps-GNN generate graph capsules and class capsules using Dynamic Routing mechanism to propagate properties from node capsules to graph capsules. Each graph is modeled as multiple graph capsules and then as multiple class capsules. In CapsGNN, Attention mechanism is used to scale node capsules for comparability across different graphs and to focus on relevant parts of graphs. This helps generate high-quality graph capsules. CapsGNN utilizes Attention Module and Dynamic Routing to extract high-quality graph capsules for graph classification. The process involves three key blocks: Basic node capsules extraction, High level graph capsules extraction, and Graph classification. GNN is used to extract local vertices features and build primary node capsules, while Dynamic Routing is applied to generate class capsules for graph classification. CapsGNN utilizes multi-scale node features represented as capsules for graph classification. GNN extracts node embeddings to form primary capsules, followed by Attention Module and Dynamic Routing to generate graph capsules. Trainable weights matrix Wlij serves as channel filters between layers. Global routing mechanism is then applied to generate high-level graph capsules. After extracting local node capsules, a global routing mechanism is used to create graph capsules. Each set of node capsules contains N sets, with each set denoted as Sn = {s11, .., DISPLAYFORM0}. The output is a set of graph capsules H \u2208 RP \u00d7 d, reflecting different graph properties. An Attention Module scales node capsules before generating graph capsules. Primary capsules are extracted based on each node in CapsGNN, with the number depending on input graph size. Direct application of the routing mechanism affects the value of high-level capsules. The Attention Module is introduced to address the dependency of high-level capsules on the number of primary capsules in CapsGNN. It uses a two-layer neural network for attention measure and node-based normalization to scale node capsules. This ensures that the generated graph capsules are independent of graph size and focus on important parts. The Attention Module in CapsGNN uses a two-layer neural network for attention measure and node-based normalization to scale node capsules, ensuring independence from graph size. Coordinate addition module preserves position information during node capsule votes generation. The procedure for generating multiple graph capsules involves scaling primary capsules and calculating votes. The CapsGNN module generates graph capsules for graph classification using a dynamic routing mechanism. Votes are calculated and used to compute high-level graph capsules. The final class capsules are generated through dynamic routing for classification loss calculation. The CapsGNN module generates graph capsules for graph classification using a dynamic routing mechanism. \u03bb is used to prevent reducing the length of all class capsules, especially when K is large. Reconstruction loss is used as a regularization method, where all class capsules except the correct one are masked and decoded to reconstruct the input information. The architecture details describe the fusion of GNN and CapsNet in CapsGNN. The CapsGNN module combines GNN and CapsNet to generate graph capsules for classification tasks. Performance is evaluated against existing approaches on 10 benchmark datasets, with an analysis of the efficiency of encoding graph features using capsules. The study includes an examination of the generated graph/class capsules and a comparison experiment to assess the contribution of each module in CapsGNN. The goal of graph classification is to predict the classes of graphs based on their structure and node labels. The CapsGNN module combines GNN and CapsNet for graph classification by analyzing graph structures and node labels. Kernel-based methods like WL, GK, and RW decompose graphs into sub-components, while deep-learning-based methods like Graph2vec, DGK, and AWE require extracting substructures in advance. These algorithms aim to find a mapping f : G \u2192 Y for graph classification. The algorithms Graph2vec, DGK, and AWE require extracting substructures in advance and learn graph representations similar to Doc2vec. DGK uses Word2vec to learn sub-structure similarities for building a graph kernel. Kernel-based machine learning methods like SVM are then applied for graph classification. These algorithms, along with other kernel-based methods, require two stages for graph classification. In contrast, end-to-end and data-driven architectures like PATCHY-SAN (PSCN) focus on sorting nodes and defining receptive-field sizes for each. PATCHY-SAN (PSCN) BID15 sorts nodes and defines receptive-field sizes for each node before applying 1-D CNN for graph classification. Other GNN-based algorithms like GCAPS-CNN BID22, Dynamic Edge CNN (ECC) BID19, and Deep Graph CNN (DGCNN) extract features and generate graph embeddings using different methods. The study utilizes biological and social network datasets for experimental evaluation, employing 10-fold cross-validation to assess performance objectively. Training involves adjusting hyperparameters using one fold as validation, eight for architecture training, and one for testing. The final result is the average of 10 test accuracies, with baseline comparisons made when original results are unavailable. CapsGNN achieves state-of-the-art performance on social datasets, improving classification accuracy by 2.78% on RE-M5K and 5.30% on RE-M12K. Learning features in the form of capsules and modeling a graph to multiple embeddings capture macroscopic properties important for classifying social networks. CapsNet focuses on extracting important information from capsules by voting, while applying routing to the whole graph preserves all information at a graph level, not giving prominence to individual fine structures. The CapsGNN model, while not ideal for highlighting individual fine structures in biological datasets, still shows strong performance in graph classification tasks. The experiment aims to assess the efficiency of capsules in encoding graph features by comparing CapsGNN with scalar-based architectures in terms of training and testing accuracy. The focus is on representing more information with a similar number of neurons. In this experiment, scalar-based architectures replace CapsGNN's graph and class capsules with fully-connected layers. CapsGNN uses vectors for features and routing for information propagation, while scalar-based architectures use scalar values. Different CapsGNNs are constructed by adjusting node and graph capsule dimensions and the number of graph capsules. The size of fully-connected layers in scalar-based architectures is adjusted to match CapsGNNs. Other hyperparameters remain the same. The tested architectures for the NCI1 dataset are detailed in TAB3, with accuracy results shown in FIG2. The simplest setting (2-4-2) is used as an example, where CapsGNN outperforms scalar-based architectures in representing the dataset. CapsGNN outperforms scalar-based architectures in representing the dataset, even when the dimension of FC is slightly higher than the dimension of graph embeddings. Increasing the number of graph capsules in CapsGNN and enlarging the dimension of FC in scalar-based architectures can make their training accuracy closer. The training accuracy of scalar-based architectures is slightly higher than CapsGNNs when the dimension of FC is about 20% larger than the dimension of graph capsules. CapsGNN demonstrates higher training and testing accuracy compared to scalar-based architectures, showcasing its efficiency in feature encoding and generalization. It leverages capsules to capture complex graph information effectively. The extracted graph/class capsules are analyzed by plotting their distribution using t-SNE, with an example shown for REDDIT-M12K dataset. Different channels of capsules are used to depict the distribution of graphs from different categories. The capsules extracted from different channels of CapsGNN represent various aspects of graph properties, allowing for discrimination between different categories. While specific aspects focused on by these capsules are still unclear, utilizing multiple embeddings in modeling objects may lead to more interpretable embeddings in the future. This approach contrasts with scalar-based neural networks and shows promise for learning more meaningful representations. The capsules extracted from different channels of CapsGNN represent various aspects of graph properties, allowing for discrimination between different categories. Introducing capsules preserves classification-related properties of each graph, reflecting the classification performance. This differs from standard scalar-based architectures, where each graph is modeled with only one graph embedding. CapsGNN is a novel framework that incorporates capsules theory into GNN for more efficient graph representation learning. It introduces capsules to extract features in the form of vectors based on node features, resulting in multiple embeddings for each graph that capture different aspects of graph properties. The generated capsules preserve classification-related information and other graph properties, offering potential for future exploration. CapsGNN is a novel framework that incorporates capsules theory into GNN for efficient graph representation learning. It introduces capsules to extract features in vector form based on node features, resulting in multiple embeddings for each graph. The model has achieved better performance on graph classification tasks, especially on social datasets, compared to other algorithms. CapsGNN is more efficient in encoding features compared to scalar-based architectures, making it beneficial for processing large datasets. The specific routing procedure is detailed in Algorithm 1. Incorporating capsules theory into GNN, CapsGNN extracts features in vector form from node features, improving graph representation learning. The model outperforms other algorithms on graph classification tasks, especially on social datasets. CapsGNN is more efficient in encoding features compared to scalar-based architectures, making it beneficial for processing large datasets. The procedure for calculating votes with node position indicators is detailed in the algorithm. CapsGNN represents node features as capsules, preserving properties more efficiently and generating multiple graph embeddings. Unlike other approaches, CapsGNN captures graph information from different aspects and assigns varying importance to different parts of the graph. CapsGNN uses attention mechanism to assign different weights to nodes in input graphs, focusing on critical parts. Unlike GCAPS-CNN, CapsGNN extracts node features from multiple GNN layers to preserve more information. GCAPS-CNN generates capsules in a feature-based manner and performs dimension reduction between layers. CapsGNN utilizes an attention mechanism to assign weights to nodes in input graphs, focusing on critical parts. It extracts node features from multiple GNN layers to preserve more information, unlike GCAPS-CNN which compresses capsules back to scalar features. CapsGNN handles more complex situations with graphs compared to CapsNet, introducing an Attention Module to tackle graphs of varying sizes and preserve important parts efficiently. CapsGNN introduces multiple graph embeddings through a graph capsules layer, reflecting various graph properties. This approach of learning representations has potential for complex data analysis beyond graphs. Unlike CapsNet, CapsGNN uses a different explanation of linear transformation for modeling spatial relationships between object parts and the whole object. CapsGNN applies linear trainable transformation to compute prediction vectors from nodes-level to graph-level representations, focusing on the relationship between node and graph properties. The architecture settings are consistent across datasets for robust performance. The model includes 5 layers for node capsules extraction, with 2 channels per layer, 16 graph capsules, and 8 dimensions for all capsules. The Attention Module has a hidden layer with 1/16 of input units, and routing is done in 3 iterations during training. Loss c and Loss r are simultaneously reduced, with Loss r scaled by 0.1. In the training stage, Loss c and Loss r are reduced simultaneously, with Loss r scaled by 0.1 to focus on classification. \u03bb is set as 0.5 for multi-class and 1.0 for binary classification. Node attributes in large datasets are set the same to prevent over-fitting, while smaller datasets have node degree and attributes sent to CapsGNN as features with dropout rate of 0.3 applied. The CapsGNN architecture includes different variations such as CapsGNN-noRout, CapsGNN-noRecon, and CapsGNN-Avg-noRout. These variations involve replacing the Attention module with the Average module and fixing similarity coefficients between capsules. The validation accuracy of each architecture is compared, with CapsGNN-Coord showing better performance in certain cases. The performance of social datasets is consistent across different architectures, while biological datasets show sensitivity to introduced modules. CapsGNN achieves the highest accuracy on NCI1 and PROTEINS, indicating limited effectiveness of the Coordinate Addition Module. Routing mechanism improves accuracy on NCI1, PROTEINS, and D&D by 2.1%, 0.6%, and 0.51% respectively. The Attention Module also enhances accuracy on these datasets by 1.37%, 1.24%, and 1.35% respectively. CapsGNN is a general framework that combines capsule theory with GNN to improve graph representation learning. The accuracy of NCI1, PROTEINS, and D&D can be enhanced by up to 2.98%, 1.33%, and 2.23% when using the Attention Module and Routing mechanism together in the architecture. Adjusting the architecture and hyper-parameters based on practical situations is recommended as each module plays a different role in different datasets."
}