{
    "title": "HkGsHj05tQ",
    "content": "In recent years, Deep Neural Networks (DNNs) have thrived with Batch Normalization (BN) playing a crucial role. However, BN's costliness due to reduction operations has been noted. A proposal in this paper suggests reducing BN's cost by using a small fraction of data for mean & variance estimation at each iteration. The challenge lies in balancing normalization effectiveness and execution efficiency. Two approaches are proposed: \"Batch Sampling (BS)\" randomly selects few samples from each batch, and \"Feature Sampling (FS)\" randomly selects a small patch from each feature map. Another approach involves creating few uncorrelated data for statistics' estimation with certain constraints. The proposed methods aim to reduce data correlation in Deep Neural Networks (DNNs) for accurate estimation and optimize execution patterns for faster acceleration. These methods result in an overall training speedup of up to 21.7% on modern GPUs without specialized libraries, with minimal impact on model accuracy and convergence rate. Additionally, they effectively address the \"micro-batch normalization\" issue with tiny batch sizes. The success of DNNs relies on deep structures, with batch normalization (BN) helping to standardize activations and enable deeper networks. BN allows for higher learning rates and less focus on initialization, but it introduces training overhead due to costly reduction operations, slowing down training speed by over 45%. Several methods have been developed to accelerate training speed in deep models, such as Range Batch Normalization (RBN) and L1-norm BN (L1BN) which simplify forward and backward passes. Self-normalization BID15 offers a solution without BN operation using the SELU activation function. Despite these advancements, further improvements are needed as current methods have limitations. In this paper, the proposal aims to reduce BN's computational cost by using minimal data for mean and variance estimation at each iteration. The challenge is to balance normalization effectiveness and execution efficiency, with two approaches identified to achieve this goal effectively and efficiently. The proposal aims to reduce Batch Normalization's computational cost by using minimal data for mean and variance estimation. Two categories of approaches are suggested: sampling (Batch Sampling and Feature Sampling) and creating data (Virtual Dataset Normalization). Multi-way strategies are designed to reduce data correlation and optimize execution patterns for running acceleration. The approaches are evaluated on various DNN models with negligible loss of model accuracy and convergence rate. The proposal suggests reducing Batch Normalization's computational cost by using minimal data for mean and variance estimation, achieving a training speedup of up to 21.7% on modern GPUs. This approach does not require specialized libraries like network pruning or quantization, and it addresses the \"micro-batch normalization\" problem for tiny batch sizes. The proposal aims to reduce Batch Normalization's computational cost by using minimal data for mean and variance estimation, achieving a training speedup of up to 21.7% on modern GPUs. This approach does not require specialized libraries and addresses the \"micro-batch normalization\" problem for tiny batch sizes. Various benchmarks show practical acceleration with negligible accuracy loss. The organization of the paper is presented in FIG0. Activations in a layer for normalization are described by a d-dimensional activation feature. Batch Normalization uses statistics to normalize activation. trainable parameters are introduced for representation recovery. The detailed operations of a BN layer in the backward pass can be found in Appendix C. Batch Normalization (BN) involves optimization of reduction operations using adder trees and computational graphs for both forward and backward passes. BN significantly slows down training speed on ImageNet by 32%-43% due to costly reduction operations. The computational complexity of BN should be O(m) if reduction operations are not optimized. With the proposed parallel algorithm, the reduction operation is transformed to cascaded adders of depth log(m), but the computational cost remains high. The computational cost of Batch Normalization (BN) remains high even with optimized reduction operations using cascaded adders. To reduce BN inefficiency, the effective value of m can be decreased by using few data points for mean and variance estimation. This approach can help in accelerating BN by modifying the equation with a smaller value of s, where s is the actual number of data points and s/m is the Sampling Ratio. To reduce Batch Normalization (BN) inefficiency, using few data points for mean and variance estimation can accelerate the process. However, this approach presents a challenge in preserving normalization effectiveness while improving execution efficiency. Estimation errors may occur when using few data, leading to the need for contrast tests to identify unnormalized layers. As the network deepens, mean and variance will scale up exponentially. The degradation of BN's effectiveness due to the exponential scaling of mean and variance as the network deepens can be recovered by sampling less correlated data within each layer and using intermittent BN configuration to prevent statistics scaling up across layers. This approach helps constrain the statistics shift to a smaller range and improve the accuracy of the entire model. In this paper, the overhead of sampling can be reduced by using regular and static execution patterns to balance normalization effectiveness and execution efficiency. Careful designs are needed to achieve practical acceleration with little accuracy loss. Using few data for statistics' estimation can reduce computational cost of BN operations, but the effectiveness-efficiency trade-off should be balanced. In this section, the focus is on balancing the trade-off between effectiveness and efficiency in BN operations. Two approaches are proposed: sampling uncorrelated data for statistics' estimation and using multi-way strategies to balance data correlation and execution regularity. Sampling involves selecting a small fraction of data from activations at each layer, while hypothesis-testing is applied to address the complexity of activation correlation in deep networks. In this work, a hypothesis-testing approach is applied to address the complexity of activation correlation in deep networks. Two empirical assumptions are made regarding data correlation within and between layers. Based on these assumptions, two uncorrelated-sampling strategies, Batch Sampling (BS) and Feature Sampling (FS), are proposed to reduce inter-layer data correlation. Detailed algorithms for BS and FS can be found in the appendix. To reduce inter-layer data correlation, Feature Sampling (FS) randomly selects small patches from each feature map of all samples for statistics' estimation. This method aims to have lower correlation within each layer and across layers. Naive Sampling (NS) is proposed as a comparison baseline, where the sampling index is fixed across layers. Regular and Static Sampling is designed for practical acceleration on GPU, with a focus on a more regular and static sampling pattern. To improve estimation effectiveness and execution efficiency, sampling rules are carefully designed in BS and FS. In BS, samples are continuous with shared indexes across channels but independent across layers. In FS, rectangular patches with shared locations are used within each layer but vary across layers. Virtual Dataset Normalization (VDN) is proposed to create uncorrelated data for statistics' estimation, involving offline calculation of dataset statistics and generating virtual samples to concatenate with real inputs. Virtual Dataset Normalization (VDN) involves generating virtual samples to concatenate with real inputs for statistics' estimation. The synthesized data is more uncorrelated than real samples, leading to more accurate estimation. The algorithm for implementation can be found in Alg. 3, Appendix A. The sampling and creating approaches can be used singly or jointly, with a controlling variable \u03b2 determining the contribution of sampled data in statistics estimation. Virtual Dataset Normalization (VDN) involves generating virtual samples to concatenate with real inputs for more accurate statistics estimation. Different approaches like NS, BS, and FS are compared in terms of reducing inter-layer and intra-layer correlations. Combining BS/FS with VDN can achieve lower accuracy loss in image classification tasks using CIFAR-10, CIFAR-100, and ImageNet datasets. The study evaluates the effectiveness and efficiency of classification tasks using CIFAR-10, CIFAR-100, and ImageNet datasets with different deep network models. Virtual Dataset Normalization (VDN) involves generating virtual samples to improve statistics estimation. Various approaches like NS, BS, and FS are compared for reducing correlations. The experimental configurations and model details can be found in the appendix. The experimental results show that different approaches like FS, VDN, and BS outperform the NS baseline in approximating the accuracy of normal BN. FS performs the best with minimal accuracy loss at low sampling ratios. VDN excels at extremely small sampling ratios, while BS consistently outperforms NS. Additionally, BN sampling can sometimes achieve better accuracy. The BN sampling approaches FS, VDN, and BS outperform the NS baseline in accuracy approximation. FS shows minimal accuracy loss at low sampling ratios, while VDN excels at extremely small ratios. BS consistently outperforms NS, and BN sampling can achieve better accuracy in some cases. Combining FS-1/64 and VDN-2/128 results in the lowest accuracy loss (-0.2%), showing that VDN can be effective when combined with other sampling strategies. DenseNet-121 training is time-consuming, so results are reported with FS/BS-VDN joint use. Even with the deeper structure of DenseNet-121, \"FS-1/64 + VDN-2/64\" achieves low accuracy loss (-0.6%). Gradient explosion is observed with VDN alone on very deep networks, but can be mitigated by combining VDN with other sampling approaches like FS+VDN. Training curves are visualized in FIG5 for convergence analysis. The training curves show satisfactory convergence for most approaches except for fully random sampling (FRS), which is less stable and achieves lower accuracy. The error of statistical estimation is affected by data correlation, with estimation errors denoted at the lth layer. The analysis is conducted on ResNet-56 over CIFAR-10, focusing on mean & variance estimation errors of all layers. FIG6 shows FS has the least error, while VDN has similar error to BS and NS but with lower sampling ratio. FIG7 reveals BS has less inter-layer correlation than NS, indicating better convergence. After evaluating the estimation error and normalization effectiveness, the study focuses on execution efficiency. BS shows higher acceleration ratio compared to FS and VDN due to its lack of fine-grained sampling and additional calculations. FS fails to achieve speedup on CIFAR-10 due to small image size. The proposed approaches can achieve up to 2x BN acceleration and 21.8% improvement. The proposed approaches can achieve up to 2x BN acceleration and 21.8% overall training acceleration on ResNet-18 and DenseNet-121. The \"BS+VDN\" joint approach shows significant speedup on very deep networks, reaching up to 23.8%. The relationship between sampling ratio and training speedup is illustrated in FIG0, showing that both BS & FS can achieve considerable speedup with a moderate sampling ratio. The text illustrates that BS & FS can achieve speedup with a moderate sampling ratio, with BS showing more significant acceleration due to its regular execution pattern. Training speedup is achieved on modern GPUs without specialized library support. Batch Normalization (BN) has been widely used in DNN models for faster convergence and better generalization. Decorrelated Batch Normalization (DBN) further improves normalization by whitening activations with ZCA whitening. Our work proposes using few data to estimate mean and variance for training acceleration, aiming to balance normalization effectiveness with computational cost, especially on large-scale modern models and datasets. Our work proposes using few data to estimate mean and variance for training acceleration, aiming to balance normalization effectiveness with computational cost. Two categories of approaches are proposed: sampling (BS/FS) or creating (VDN) uncorrelated data. Multi-way strategies are designed to reduce data correlation and optimize execution patterns. Experimental results show up to 21.7% training acceleration with negligible accuracy loss. VDN can also be applied to micro-BN scenarios for advanced performance. This paper demonstrates the effectiveness of using Batch Normalization (BN) with few data for statistics' estimation, achieving training speedup on modern GPUs without the need for specialized libraries. Specialized kernel optimization is suggested for further performance improvements. The Conv layer is used as an example, and batched features are represented as a 4D tensor. The operations for calculating means and variances are denoted as \"E 0,1,2\" and \"V ar 0,1,2\" respectively. The experiments on CIFAR-10 & CIFAR-100 are conducted using Nvidia GPUs with specific settings for weight decay, epochs, and learning rate adjustments. For ImageNet, different GPUs are used with similar weight decay and epoch settings but with a different initial learning rate calculation. During training, input images are augmented and evaluated for validation error. Winograd BID17 is used to speed up training. Proposed approaches speed up forward pass by reducing accumulation operations. Theoretical compute speedup for forward pass can reach log s (m) times. Theoretical compute speedup for forward pass can reach log s (m) times. Memory access reduced by m/s times, contributing to overall speedup. Backward pass operations based on derivative chain rule. Experiments on ResNet-56 show best decay rate setting for validation accuracy. The best decay rate setting for validation accuracy is 0.7, with performance degrading for rates below 0.7. Researchers often overlook decay rate, but default value may not be optimal. A pure BN network is built to show reduction operations as bottleneck, training for 100 iterations with reduction operations removed for contrast test. The reduction operations in both forward and backward passes were removed to test the bottleneck in BN. Results from using different GPUs showed that reduction operations can take up to >60% of the operation time in BN, indicating they are the bottleneck. Micro-BN aims to address the issue of unreliable estimation of activation statistics in small data nodes. Previous approaches include Sync-BN and Local-BN, with the former introducing inter-GPU data dependency and the latter avoiding batch dimension in activation tensors. In Sync-BN, different strategies like FS, BS, and VDN are used to optimize inter-GPU data dependency. Local-BN faces challenges due to tiny data per GPU, but VDN can still be effective. Experiments show that normalization in Sync-BN is based on efficient preservation techniques. The normalization in Sync-BN is based on statistics from multiple nodes through synchronization. Results on Local-BN with VDN optimization show promise for training very large models effectively. Top-1 validation error rate for VDN is slightly better than Group Normalization (GN) in scenarios with tiny batch sizes. The paper discusses training very large models effectively with tiny batch sizes, ensuring correct training by accommodating only several samples per GPU node. The organization of the paper is illustrated in FIG0, showing the preservation of effectiveness by reducing inter-layer or intra-layer correlation. Regular and static execution patterns are considered in all approaches."
}