{
    "title": "SkgkJn05YX",
    "content": "The paper discusses the robustness of neural networks against adversarial examples, introducing a new CNN architecture with good robustness. They use a technique called Random Mask to enhance CNN structures, achieving state-of-the-art performance against black-box attacks without adversarial training. The study also explores adversarial examples that deceive CNNs with Random Mask, revealing that these examples can also deceive humans, prompting questions about defining adversarial examples and robustness in deep learning. The success of deep CNNs in various machine learning fields has been overshadowed by their vulnerability to adversarial examples, which are small perturbations that can cause incorrect output labels without being noticeable to humans. These examples can easily transfer among different CNN architectures, posing a significant challenge in maintaining model robustness. Adversarial examples can transfer among different CNN architectures, raising concerns about model robustness. Adversarial training has shown some success but tends to overfit to the method of generation. Defense methods introduce randomness and transformations to inputs, but some argue they only provide \"obfuscated gradient\" and can still be attacked. The Random Mask method enhances CNN robustness against black-box attacks, outperforming adversarial training. Adversarial examples fooling CNNs with Random Mask also deceive humans. Random Mask is a method to modify CNN structures by randomly selecting and removing neurons before training. This enhances robustness against black-box attacks and challenges traditional definitions of adversarial examples. The drop rate, or drop ratio, determines the sampling process for masking neurons in a selected layer. Random Mask is a method that modifies CNN structures by randomly selecting and removing neurons before training. This reduces computational cost by masking neurons in feature maps without changing the number of parameters in convolutional kernels. The success of this implementation is based on the assumption that useful features should be computed at different spatial positions. The original CNN structure is powerful for feature extraction and parameter sharing, but it may overlook feature distribution. Random Mask allows filters to extract features from specific positions, potentially capturing more diverse features. Random Mask in a network can help capture spatial structures of local features by allowing filters to extract features from specific positions. An experiment comparing CNNs with and without Random Mask showed that the accuracy of classifying shuffled images was consistently lower with Random Mask, indicating that CNNs without Random Mask focus more on feature existence while CNNs with Random Mask detect spatial structures. Random Mask in CNNs helps detect spatial structures and limit poorly-organized features. It defends against adversarial examples by preventing perturbed patches from forming correct spatial structures with other parts of the image. Random Mask in CNNs helps resist adversarial perturbations by detecting spatial structures and limiting feature extraction. Adversarial examples against CNNs with Random Mask contain well-organized features that can change classification results, unlike random noises generated against normal CNNs. This supports the idea that Random Mask helps extract organized features by imposing limitations. The Random Mask in CNNs helps resist adversarial perturbations by limiting feature extraction and detecting spatial structures. The experiments show that CNNs maintain high test accuracy with Random Mask, and adding convolutional filters can further increase accuracy. The structure is compatible with ensemble methods, making them more powerful. However, applying Random Mask to deep layers may not be appropriate as the distribution of features is meaningful only in certain layers. The Random Mask network structure is extensively analyzed in this section. The experiments focus on testing the robustness of Random Mask, examining adversarial examples, and exploring where and how to apply Random Mask through comparative experiments. Various target networks are used, including ResNet-18, ResNet-50, DenseNet-121, and SENet-18. The effects of Random Mask on ResNet-18, ResNet-50, DenseNet-121, SENet-18, and VGG-19 are consistent. The defense performance on ResNet-18 is shown, with the network structure divided into shallow and deep blocks. Random Mask with different drop ratios is applied to these blocks. The accuracy under black-box attack is a common criterion of robustness. Adversarial examples are generated using different attack methods against various neural networks. The defense performance against adversarial examples generated by using DenseNet-121 on CIFAR-10 dataset is shown, with more results in the Appendix. Different attack methods are denoted by FGSM, PGD, and CW with specific parameters. Random Mask is a powerful defense mechanism against black-box attacks, improving CNN performance by limiting feature extraction and reducing noisy features. It helps existing CNNs achieve state-of-the-art performance against attacks like PGD and CW. Mask can enhance CNN performance against black-box attacks, with Random Mask improving accuracy. It also improves defense performance under white-box attacks, even without obfuscated gradient or gradient masking. Adversarial images misclassified by the network often have vague edges, aligning with the idea that real adversarial examples may be inevitable. The Random Mask technique helps networks capture more information related to human perception, leading to small perturbations that can change the semantic meaning of images. This raises questions about defining adversarial examples and robustness. Properties of Random Mask are discussed, including appropriate application positions, breaking symmetry benefits, diversity from randomness, and extensibility through structure adjustment and ensemble methods. Random Mask technique enhances network performance by adjusting structure and using ensemble methods. Comparative experiments focus on black-box defense performance for robustness. Results of experiments are summarized in Table 2, showcasing properties of Random Mask. Different techniques like dropping channels, applying masks, increasing channel numbers, and ensemble models are explored. The success rates of defense under various settings are presented. Additionally, the intuition that deep layers in a network should not be masked is discussed. In Section 2, experiments on ResNet-18 show that masking shallow layers leads to lower adversarial attack success rates compared to masking deep layers. This suggests that shallow layers are more crucial in limiting feature extraction. Applying Random Mask only to shallow blocks outperforms masking both shallow and deep blocks, supporting the idea that dropping elements with large receptive fields is not beneficial for the network. Random Mask with independent random masks outperforms normal network in robustness. Channel Mask maintains symmetry but Random Mask shows greater robustness despite drop in accuracy. Random Mask's randomness in masks across channels and layers provides advantages. The randomness in generating masks in different channels and layers allows each convolutional filter to focus on different patterns of feature distribution. Applying the same mask to each channel decreases test accuracy due to limitations in expressivity. Using different masks per layer is essential for better performance. Increasing the number of channels in masked layers can help compensate for the loss of information caused by masking many neurons. Ensemble methods using different Random Masks can improve test accuracy and robustness of CNNs with Random Mask modification. Random Mask is a modification of CNNs that enhances information capture and improves robustness without sacrificing test accuracy. It outperforms in black-box defense settings and can even alter the semantic information of images to deceive humans. Additionally, a Random Shuffle experiment demonstrates the effectiveness of randomly dropping neurons in the neural network. In a Shuffle experiment, a CNN with Random Mask is compared to a normal CNN on randomly shuffled images to test feature extraction performance. The experiment involves training networks on ImageNet dataset, selecting correctly predicted images, resizing and shuffling them for analysis. After training on ImageNet dataset, images are resized to 224 \u00d7 224 and shuffled into small patches before being fed to networks for classification. Results show that the network with Random Mask has lower accuracy on shuffled images compared to the normal network, indicating a reliance on object positions and margins for classification. The experiment involves attacking a neural network by adding a small perturbation to the input image to create an adversarial image. The goal is to make the adversarial image visually similar to the original image while changing the predicted label. The attack is successful if the predicted label of the perturbed image is different from the original. In attacking neural networks, there are two types of methods: Targeted Attack changes the output label to a specific one, while Untargeted Attack changes the output label without specifying a specific one. Three attack approaches are used: Fast Gradient Sign Method (FGSM), Basic Iterative Method (PGD). The curr_chunk discusses the PGD iterative attack method and the CW Attack optimization problem for constructing adversarial examples. It also briefly introduces the network architectures used in experiments, including the application of Random Mask at shallow layers. The curr_chunk explains the application of Random Mask to different network architectures, including ResNet-18, ResNet-50, DenseNet-121, SENet-18, and VGG-19. Random Mask is applied to specific convolutional layers in these architectures to enhance their performance. The architecture used in the study includes sixteen 3 \u00d7 3 convolutional layers and one fully connected layer. Training process details involve data preprocessing steps and specific parameters for training on CIFAR-10 and MNIST datasets. When training models on MNIST, per-pixel mean is subtracted and random horizontal flip is applied to train data. SGD with momentum parameter 0.9, weight decay parameter 5 \u00d7 10 \u22124, and mini-batch size 128 are used for training for 50 epochs. The learning rate decreases by a factor of 10 at epochs 20 and 40. Train and test curves of different network structures on CIFAR-10 and MNIST show similar tendencies. Adversarial examples generated from CIFAR-10 against ResNet-18 with Random Mask are shown in Figure 12. In experiments on adversarial examples, ResNets are trained using PGD attack method with perturbation scale \u03b1 = 16 and \u03b1 = 32. Adversarial examples are generated and tested on models using FGSM and PGD with specific parameters. Black-box settings from Madry's paper are listed, showing the process of generating adversarial examples. In experiments on adversarial examples, ResNets are trained using PGD attack method with perturbation scale 8 and PGD runs for 7 gradient descent steps with step size 2. Random Mask is applied to shallow blocks with drop ratio 0.85 to balance robustness and generalization performance. The trade-off between defense rate against adversarial examples and test accuracy is shown in FIG0. Random Mask is applied to five popular network structures for black-box defense performance on CIFAR-10 and MNIST datasets. In experiments on adversarial examples, Random Mask is applied to shallow layers of network structures to improve defense performance. The defense performance is evaluated against adversarial examples generated by PGD, with results shown in Table 4 and Table 5. Different drop ratios are used for Random Mask application, and the performance is consistent across different settings. Random Mask with drop ratios of 0.5 and 0.7 is applied to shallow layers of network structures to improve black-box defense performance. Results show consistent improvement across different network structures in defending against adversarial examples. Random Mask with drop ratios of 0.5 and 0.7 is applied to shallow layers of network structures to improve black-box defense performance against adversarial examples. Results show consistent improvement across different network structures. The text discusses the gray-box defense ability of Random Mask and the transferability of adversarial examples generated against Random Mask on the CIFAR-10 dataset. Adversarial examples are generated against one trained neural network and tested on a network with the same structure but different initialization, as well as on a network with the same drop ratio but different Random Mask. The adversarial knows some information on the network structure but not the parameters. Adversarial examples are tested on DenseNet-121 and VGG-19 for transferability. Results on gray-box attacks and transferability are shown in Table 7. FGSM with step size 16 is used to generate adversarial examples on source networks and test them on target networks with different initialization values and random masks. Random Mask improves performance under gray-box attacks, and CNNs with Random Mask show similar performance on adversarial examples. Random Mask have similar performance on adversarial examples from different gray-box attacks, indicating that CNNs with Random Mask of the same ratios catch similar information. Experimental results on Random Mask using various adversarial examples, attack methods, and mask settings on ResNet-18 are presented. Different settings for FGSM and PGD attacks were explored, including different step sizes and perturbation scales. From the experimental results on different attack methods and settings, it was observed that larger perturbation scales result in stronger adversarial examples. Additionally, smaller step sizes lead to more successful attacks by carefully searching around the original image. Strong PGD attack results were shown for settings (1, 20, 16), (2, 10, 16), and (1, 40, 32), while weak PGD attacks also performed well. Different confidence parameters were tested for CW attacks, with large \u03ba making it difficult to find adversarial examples for some neural networks like VGG. For ResNet-18, \u03ba = 40 (CW 40) is chosen as a good parameter to compare models with normal ones. The step number for choosing parameter c is set to 30. Adversarial examples used can fool the original network. Experimental results are listed in TAB9, 9, 10, 11, and 12. Different techniques like DC, SM, and \u00d7n were used to modify the network. The study explores different techniques like DC, SM, and \u00d7n to modify the network. Adversarial examples generated against DenseNet-121 achieve high accuracy on the CIFAR-10 test set. Various defense strategies are evaluated, including dropping channels, applying masks, increasing channel numbers, and ensemble models with different masks. The study evaluates defense strategies like dropping channels, applying masks, increasing channel numbers, and ensemble models with different masks. Models trained on CIFAR-10 achieve high accuracy on the test set. The study evaluates defense strategies like dropping channels, applying masks, increasing channel numbers, and ensemble models with different masks. For every channel and ensemble, five models with different masks of the same ratio are used. The success rates of defense under different settings are shown. Images from the Tiny-ImageNet dataset are randomly sampled, generated using ResNet-18 with a Random Mask of ratio 0.9 on the 1st and 2nd blocks. The attack method is PGD with perturbation scale 64, step size 1, and step number 80. Each image shows the generated image against the network with Random Mask, the image against the normal ResNet-18, and the original image."
}