{
    "title": "r1xaVLUYuE",
    "content": "Posterior collapse in Variational Autoencoders (VAEs) occurs when the variational distribution closely matches the uninformative prior for some latent variables. This paper explains posterior collapse through the analysis of linear VAEs and their connection to Probabilistic PCA (pPCA). It shows how local maxima in the marginal log-likelihood of pPCA lead to similar local maxima in the evidence lower bound (ELBO) of VAEs. Training a linear VAE with variational inference can recover a global maximum corresponding to principal component directions. The presence of local maxima can cause posterior collapse in deep non-linear VAEs, leading to various heuristic approaches in the literature to mitigate the effect of the KL term in the ELBO. The generative process of deep latent variable models involves drawing latent factors from a prior and using a neural network to generate data points. Maximum likelihood estimation requires marginalizing out latent factors, which is intractable. Variational Autoencoders (VAEs) optimize a lower bound on likelihood, leading to interest in discovering latent factors. However, the quality and number of learned latent factors are influenced by posterior collapse. The generative model in deep latent variable models is affected by posterior collapse, where the model ignores certain latent variables. Existing research attributes this to the KL-divergence term in the ELBO objective, leading to various heuristic approaches to mitigate its impact. However, a new hypothesis suggests that posterior collapse is due to spurious local maxima in the training objective, even when exact marginal log-likelihood is used. Linear autoencoders have been extensively studied, but less attention has been given to their variational counterpart. Linear autoencoders and PCA are optimal solutions to the linear autoencoder problem, with decoder weight columns spanning the principal component subspace. Probabilistic PCA (pPCA) recovers the principal component subspace as the maximum likelihood solution of a Gaussian latent variable model. Linear variational autoencoders can exactly recover pPCA, with a diagonal covariance structure leading to an identifiable model where the principal components are the columns of the decoder. The study of linear VAEs provides insights into posterior collapse, showing that the variance of the observation model impacts the stability of local stationary points in pPCA. Our contributions include proving that linear VAEs can recover the true posterior of pPCA without adding spurious local maxima. We also show that posterior collapse may occur in optimization of marginal log-likelihood, even in high-capacity, deep, non-linear VAEs. By carefully learning observation noise, we can reduce posterior collapse, and existing approaches alleviate it by reducing the stability of spurious local maxima. The pPCA model is a special case of factor analysis that uses a full covariance matrix instead of a spherical one. It allows for tractable maximum likelihood estimates of W and \u03c3 2. Variational Autoencoders use amortized variational inference to approximate the posterior distribution, allowing for tractable inference. Variational Autoencoders (VAEs) use neural networks to parameterize the variational distribution q(z|x) and the conditional likelihood p(x|z). Posterior collapse is a common issue in VAE optimization, where the KL divergence between the variational distribution and the prior plays a key role. The marginal log-likelihood may also contribute to posterior collapse. Posterior collapse is a consistent issue in VAE optimization, where the variational distribution collapses towards the prior, reducing the generative model's capacity. Related work discusses the relationship between robust PCA methods and VAEs, showing a correspondence between linear VAEs and pPCA objectives. The covariance structure of the variational distribution may help smooth out the loss landscape. The interaction between distribution and loss landscape in VAE optimization is a key focus for future research. Previous studies have linked posterior collapse to the lagging of the inference network during training. While some research explores issues with approximate inference, our findings suggest that local maxima can exist even when the variational distribution matches the true posterior. Alemi et al. use an information theoretic framework to study VAE representational properties, showing solutions with equal ELBO and marginal log-likelihood can exhibit posterior collapse. In VAE optimization, posterior collapse can occur even with weak decoders. The common approach to address this is annealing a weight on the KL term during training. However, this method may lead to suboptimal log-likelihood optimization. Another proposed method is using \"free-bits\" to ignore the gradient of the KL term below a certain threshold, but this can negatively impact training stability. Delta-VAEs have been introduced as a potential solution. Delta-VAEs choose prior and variational distributions carefully to prevent exact recovery of the prior, allocating free-bits implicitly. Other papers have explored alternative VAE formulations to improve image fidelity under Gaussian observation models. BID29 highlights the encouragement of pursuing orthogonal representations in VAEs due to the diagonal covariance used in the variational distribution. Their approach involves linear VAEs without modifying the objective function. Linear VAEs focus on optimization challenges in the autoencoder setting, showing an equivalence between pPCA and Bayesian autoencoders. They highlight the importance of suitable regularization for exact recovery of principal components. Comparisons between pPCA and linear VAEs are analyzed, with Figure 1 illustrating stationary points of pPCA. The stability of stationary points in pPCA depends on \u03c3 2 capturing principal components. Maximum likelihood estimates of pPCA are discussed, and a linear VAE can recover the global optimum and identifiability of principal components. The loss landscape of the linear VAE shows ELBO does not introduce spurious maxima. The stationary points of pPCA are characterized, with the maximum likelihood estimate of \u00b5 as the data mean. W MLE and \u03c3 MLE can be computed, with U k representing the first k principal components and R as a rotation matrix for model identifiability. The MLE solution is the global optima, and the stability of W MLE is discussed. The stability of stationary points in pPCA is influenced by \u03c3 2, with higher values leading to more stable points. Linear VAEs can recover the globally optimal solution. Linear VAEs can recover the globally optimal solution to Probabilistic PCA by using a diagonal covariance matrix globally for all data points. The global maximum of the ELBO objective for the linear VAE is identical to the global maximum for the marginal log-likelihood of pPCA. At its global optimum, the linear VAE can recover the pPCA solution by enforcing orthogonality and using a diagonal covariance matrix. The ELBO objective for the VAE is maximized when the decoder has weights W = W MLE, leading to the identification of principal components. The global optimum solution of the VAE has scaled principal components as columns of the decoder network. Full identifiability can be achieved by setting D = I. The linear VAE can recover the pPCA solution by enforcing orthogonality of decoder weight columns. The ELBO objective does not introduce additional spurious local maxima for the linear VAE model. If the decoder network has orthogonal columns, the variational distribution captures the true posterior, recovering the marginal log-likelihood. The ELBO objective can be improved by rotating the decoder columns towards orthogonality to avoid posterior collapse in VAEs. The KL between the variational distribution and the prior is not solely responsible for this issue, as even the exact marginal log-likelihood can be affected. Theoretical results suggest that additional stationary points in the ELBO objective are likely saddle points. In the linear setting, the ELBO objective does not introduce spurious local maxima. Empirical evidence from studying linear and non-linear VAE models is presented, showing likelihood values for optimal solutions in the pPCA model trained on the MNIST dataset. The study presents empirical evidence on likelihood values for optimal solutions in the pPCA model trained on the MNIST dataset, exploring the impact of additional principal components on the model's performance. The analysis extends to deep non-linear models with VAEs using Gaussian observation models on the MNIST dataset. The study investigates the impact of different values of \u03c3 2 on the ELBO in VAE models trained on the MNIST dataset. By optimizing for \u03c3 2, it was confirmed that the difference in ELBO is due to differences in latent representations. The final model's performance is evaluated on the training set to validate the findings. The study explores the impact of tuning \u03c3 2 on the ELBO in VAE models trained on the MNIST dataset. KL-annealing with \u03b2 coefficient is considered as an alternative approach. Results show that while KL-annealing may temporarily reduce posterior collapse, ultimately tuning \u03c3 2 is crucial for recovering the default ELBO solution. After exploring KL-annealing with \u03b2 coefficient to prevent posterior collapse in VAE models trained on MNIST data, the study found that allowing \u03c3 2 to be learned was crucial for reducing posterior collapse. Training with KL-annealing resulted in a much smaller \u03c3 2 value and ultimately reduced posterior collapse, despite both models achieving similar final training ELBO. The study explored KL-annealing to prevent posterior collapse in VAE models trained on CelebA dataset. Training with KL-annealing enabled the VAE to learn a smaller \u03c3 2 value, leading to a better final ELBO and reduced posterior collapse. The analysis also linked posterior collapse in linear VAEs to spurious local maxima in the marginal log-likelihood. In future work, the analysis of BID32 on optimizing deep non-linear VAEs will be extended to other observation models. The conditions for stationary points of xi log p(x i ) are formulated, focusing on the more general solutions. By utilizing the singular value decomposition of W, the stationary points equation is recovered. The stationary points equation is derived by utilizing the singular value decomposition of W. The global optimum is achieved by matching the leading singular vectors and values of S. All solutions except the leading principal components correspond to saddle points in the optimization landscape. The analysis considers different values of \u03c3 2 in relation to stability of perturbed solutions. The sign of the dot-product with the likelihood gradient determines stability, with the top q principal components being stable if certain conditions are met. If \u03c3 2 = \u03c3 2 M LE, a perturbation will lead to instability due to the average of the smallest eigenvalues. When \u03c3 2 is not set to the maximum likelihood estimate, there may be no unstable perturbation directions, leading to local optima with zero-columns in W. This phenomenon is observed in non-linear VAEs, causing posterior collapse. Learning \u03c3 2 in non-degenerate cases transforms local maxima into saddle points, where \u03c3 2 decreases with its gradient. Even in non-linear cases, local maxima persist. Linear autoencoders lack identifiability, resulting in decoder columns spanning the principal component subspace instead of recovering it. Linear VAEs, however, can successfully recover the principal components. Linear VAEs can recover principal components up to scaling, with identifiable columns in the decoder matrix. By fixing certain parameters, complete identifiability can be achieved. The analysis focuses on stationary points of the ELBO objective, deriving closed-form solutions for the marginal log-likelihood components. The VAE model considered has a linear encoder and decoder, Gaussian prior, and observation model. The linear VAE model with a Gaussian prior and observation model can achieve complete identifiability by ensuring orthogonal columns in the decoder. The marginal log-likelihood components can be expressed in closed form, with stationary points computed by taking derivatives with respect to key parameters. The stationary points are computed using matrix derivative identities, yielding the expected solution for the variational distribution directly. The ELBO objective does not introduce additional local maxima to the pPCA model, with the marginal log-likelihood recovered exactly at all stationary points. The proof of Theorem 1 shows that orthogonal columns of W ensure exact recovery of posterior mean and covariance. Using the singular value decomposition, we can write W = ULR T, where U and R are orthogonal matrices. The choice of R affects the ELBO, with the term minimized when R = I. At a stationary point, the mean of the variational distribution matches the true posterior, simplifying the KL divergence. Applying a small rotation to W corresponds to a perturbation of parameters, with R chosen to minimize log det M. The text discusses the choice of an orthogonal matrix R to minimize a term in the ELBO, indicating that a saddle point exists. It explains how to construct a small rotation matrix by ensuring det(R) = 1 and using an upper-triangular matrix B to make the rotation close to the identity. The text discusses perturbations to W, D, and V to decrease posterior KL while keeping marginal log-likelihood constant. It aims to extend linear analysis to Bernoulli observation models to address posterior collapse issues. The model considered involves a continuous latent space with a Bernoulli observation model. The text discusses difficulties in reasoning about stationary points under a Bernoulli observation model with no closed form solutions for the marginal likelihood or posterior distribution. Numerical integration methods exist but may not provide a good gradient signal. The logit-Normal distribution is used to compute the marginal likelihood, but the expectation has no closed form solution, making it challenging to tractably compute. Similarly, under ELBO, the expected reconstruction error needs to be computed. The text discusses difficulties in computing the expected reconstruction error for pPCA and VAE models trained on MNIST and CelebA datasets. The experiments involve evaluating log-likelihood, optimizing VAE parameters, and training convolutional VAEs. The procedure for training convolutional VAEs on the CelebA dataset followed that of the MNIST VAEs. Reconstructions and interpolations in the latent space were visualized in figures 9 and 10, respectively."
}