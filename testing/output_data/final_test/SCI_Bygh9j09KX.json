{
    "title": "Bygh9j09KX",
    "content": "Convolutional Neural Networks (CNNs) are found to prioritize recognizing textures over shapes, in contrast to human observers. Training the same standard architecture on 'Stylized-ImageNet' allows the network to learn a shape-based representation, aligning better with human performance in psychophysical experiments. In a well-controlled psychophysical lab setting, experiments showed improved object detection performance and robustness towards image distortions with a shape-based representation. CNNs combine low-level features to recognize complex shapes for tasks like object recognition and semantic segmentation. The network acquires complex knowledge about shapes associated with each category, recognizing parts of familiar objects and detecting objects as combinations of these parts. This shape hypothesis is supported by empirical findings and visualisation techniques like Deconvolutional Networks often highlight object parts in high-level CNN features. CNNs are proposed as computational models of human shape perception. CNNs are considered as computational models of human shape perception, with studies showing that CNNs implicitly learn representations of shape similar to human perception. Research also suggests that object shape is more crucial than color for object classification, aligning with human object recognition where shape is the most important cue. Object textures play an important role in CNN object recognition. CNNs can classify texturised images well even if global shape structure is destroyed. However, standard CNNs struggle with recognizing object sketches where texture cues are missing. Studies suggest that local information like textures may be sufficient for ImageNet object recognition. Linear classifiers on CNN's texture representation achieve similar performance to the original network. CNNs with constrained receptive field sizes can achieve high accuracies on ImageNet by recognizing small local patches. The texture hypothesis suggests that object textures are more important than global object shapes for CNN object recognition. Resolving this debate is crucial for the deep learning, human vision, and neuroscience communities. This study aims to provide insights into this issue. In this study, experiments were conducted to compare texture and shape biases in humans and CNNs using style transfer. Results showed that CNNs exhibit a texture bias, perceiving a cat with an elephant texture as an elephant, while humans still see it as a cat. Additionally, the study found that the texture bias in CNNs can be changed towards a shape bias. The study found that CNNs can overcome their texture bias and develop a shape bias when trained on specific datasets. Networks with a higher shape bias are more robust to image distortions and perform better on classification tasks. Detailed information for replication is available in the Appendix, and psychophysical experiments were conducted to compare human and CNN classification performance. Participants were presented with stimulus images and asked to choose from 16 entry-level categories. The images were also fed to four CNNs pre-trained on standard ImageNet. The study presented results based on 48,560 psychophysical trials with 97 participants. Six major experiments and three control experiments were conducted to assess texture and shape biases in object recognition tasks using different image features. Greyscale and silhouette images were used alongside original color images, and four CNNs pre-trained on ImageNet were employed for analysis. The study used greyscale, silhouette, and original color images for object recognition tasks. Texture images consisted of natural textures like animal skin or fur. Only images correctly classified by all four networks were selected for analysis to ensure interpretability in the cue conflict experiment. In a cue conflict experiment, images with contradictory features were presented to human observers for classification. Instructions were neutral, and 1280 cue conflict images were generated using style transfer. The study defined \"silhouette\" as the object's 2D bounding contour. The definition of \"object shape\" includes contours describing the 3D form, not just the silhouette. \"Texture\" is defined as an image with spatially stationary statistics. A new data set, Stylized-ImageNet (SIN), was created by replacing original textures with painting styles through AdaIN style transfer. The style of a painting was altered using AdaIN style transfer with a stylization coefficient of \u03b1 = 1.0. Kaggle's Painter by Numbers data set was used as a style source. AdaIN fast style transfer was chosen over iterative stylization for two reasons: to ensure different stylization techniques for training and testing, and to enable stylizing entire ImageNet quickly. Most object and texture images were recognized correctly by both CNNs and humans. Greyscale versions of objects were also recognized well. The study compared human and CNN recognition accuracies for objects with different levels of texture information. Human observers performed better with images lacking texture details. A cue conflict experiment was conducted to assess reliance on shape or texture features for classification. Human observers and CNNs were tested on images with conflicting texture and shape evidence. The experiment results show that human observers have a bias towards shape category decisions, while CNNs exhibit a bias towards texture category decisions. ImageNet-trained CNNs show a strong texture bias, possibly due to the training task emphasizing local texture features over global shapes. Training a ResNet-50 on a Stylized-ImageNet dataset further supports this hypothesis. On Stylized-ImageNet (SIN), a ResNet-50 achieves 79.0% top-5 accuracy, compared to 92.9% on ImageNet (IN). SIN is a harder task as textures are no longer predictive. Features learned on SIN generalize well to IN, but not vice versa. BagNets were tested to see if local texture features are sufficient for SIN. BagNets with limited receptive field sizes struggle on SIN dataset, showing reduced performance compared to ImageNet. The SIN-trained ResNet-50 exhibits a stronger shape bias, indicating the removal of local texture cues and the integration of long-range spatial information. The increased shape bias in SIN-trained ResNet-50 affects CNN performance and robustness. Shape-ResNet model outperforms vanilla ResNet in ImageNet classification and transfer tasks, suggesting SIN as a beneficial data augmentation for improved model performance. The SIN-trained network shows improved performance and robustness in object detection compared to the IN-trained CNN, especially when images are distorted. The shape-based representation in SIN helps align with global object shapes, leading to better accuracy in most image manipulations. The SIN-trained ResNet-50 shows high distortion robustness, even without prior exposure to distortions during training. Using style transfer, conflicting shape and texture images were generated to study CNNs' classification tendencies based on local textures rather than object shapes. ImageNet-trained CNNs classify objects based on local textures rather than global shapes, highlighting the importance of texture cues in object recognition. This explains why CNNs match texture appearance for humans and have predictive power for neural responses along the human ventral stream. Texture-based generative modeling approaches like style transfer and single image super-resolution also rely on texture representations. Modelling approaches like style transfer, single image super-resolution, and texture synthesis using CNNs produce excellent results. CNNs can recognize images with scrambled shapes but struggle with objects lacking texture information. Image segmentation models trained on synthetic texture images transfer to natural images. ImageNet-trained CNNs and human observers show marked behavioral differences in classification strategies. To reduce texture bias in CNNs used for object recognition, a new data set called Stylized-ImageNet (SIN) was introduced. This data set removes local cues through style transfer, forcing networks to focus on object shape. Results showed that ResNet-50 can learn to recognize objects based on shape, indicating that the texture bias in CNNs is induced by ImageNet training data. Standard ImageNet-trained models may be taking a shortcut by focusing on local textures, but shape-based features trained on SIN generalize well to natural images. Our results show that shape-based features trained on Stylized-ImageNet generalize well to natural images, benefiting recognition tasks using pre-trained ImageNet CNNs. Additionally, our ResNet-50 trained on Stylized-ImageNet demonstrates human-level robustness against image distortions, surpassing networks trained on specific distortions. This highlights the importance of a shape-based representation, as object shapes remain stable despite various noise distortions. The study demonstrates the advantages of a shape-based representation for robust inference in machine recognition. The findings offer insights into CNN representations and biases, contribute to more plausible models of human visual object recognition, and provide a starting point for future research on shape-based representations. The study compared shape-based and texture-based representations for object recognition. Trials involved presenting stimuli followed by a response screen with 16 categories. Experiment durations varied based on the number of stimuli, with 1280 stimuli taking 47 minutes to complete. In experiments with 1280 trials, observers could take a brief break after every block of 256 trials. Participants were shown the response screen before the experiment to name all 16 categories. They had 1500 ms to click on the category they believed was presented, with the last entered response counted as the answer. A practice session was conducted before the real experiment to familiarize participants with the time course and response screen layout. Feedback was provided for incorrect answers with a short low beep. The practice session consisted of 320 trials with a break after 160 trials. Feedback was given for incorrect answers with a short low beep. Natural images from 16-class-ImageNet BID10 were used in the practice session. Observers viewed stimuli on a 22\", 120 Hz VIEWPixx LCD monitor. The study involved 97 human observers who participated in experiments with different incentives. Observers had normal or corrected-to-normal vision and were presented with stimuli on a 22\" VIEWPixx LCD monitor. To ensure reliable data, a payment motivation scheme was implemented for participants based on their performance. Observers could earn a bonus of up to \u20ac5 or course credit for accuracy above 95%. Steps taken to prevent low-quality data included using a controlled lab environment, displaying observer performance on screen, and splitting experiments into blocks. ResNet-50 architecture from PyTorch was utilized in the study. The study utilized a standard ResNet-50 architecture from PyTorch for training models on ImageNet and SIN datasets. Training on SIN involved 60 epochs with specific optimization parameters and a batch size of 256. The SIN-trained model was used for experiments and comparisons in the study. The training in the third row involved jointly training on SIN and IN datasets as one big dataset for 45 epochs with adjusted learning rates. The model weights were then used to initialize the model in the fourth row, which was fine-tuned on ImageNet for 60 epochs with improved results using pre-trained features. For Faster R-CNN, the model was implemented using ResNet-50 or ResNet-152 as the encoder with custom input whitening. Training was done for 7 epochs on Pascal VOC 2007 and on MS COCO for the 2017 train/val split. Images were resized to have a short edge of 600 pixels. For MS COCO, the model was trained on the 2017 train/val split for 6 epochs with a batch size of 16 on 8 GPUs. Images were resized to have a short edge of 800 pixels. Pre-trained AlexNet, GoogLeNet, and VGG-16 were used for evaluation. ResNet-101 pre-trained on Open Images V2 was used for comparing biases in ImageNet vs. OpenImages. The inference code and label mapping for predictions on 16 classes were obtained from the Open Images dataset. Different pre-trained models like ResNet-101, ResNet-152, DenseNet-121, and SqueezeNet1 were evaluated. Model biases were also assessed after training on the dataset. The model architectures were obtained from torchvision.models and trained under identical circumstances as ResNet-50 with varying learning rates. Nine experiments were conducted with images saved in png format, sized 224 \u00d7 224 pixels. The original experiment included 160 colored images with unmanipulated objects on a white background for style transfer. For style transfer, neutral background images were carefully selected using Google advanced image search with specific criteria. Images were modified to have a white background if needed and verified by pre-trained CNNs to ensure accuracy for cue conflict experiments. The experiment involved converting images to greyscale and silhouette versions for further analysis. Greyscale images were stacked three times along the color channel for CNNs, while silhouette images showed black objects on a white background after a series of transformations. The experiment involved manually checking and adapting images for edge and texture experiments. Edge stimuli were generated using the \"Canny\" edge extractor in MATLAB, while texture images were selected based on object characteristics. The experiment involved generating edge stimuli using the \"Canny\" edge extractor in MATLAB and selecting texture images based on object characteristics. For difficult cases like man-made objects, textures were created by using images with many objects of the same kind clustered together. Cue conflict experiment used images with texture-shape conflicts generated through style transfer between texture and content images. A subset of 1280 images (80 per category) was used for human observation in a single session. The procedure involved selecting style and content images for cue conflict experiments. Five cue conflict images were generated for each 16x16 combination of style and texture categories. Trials without cue conflict were excluded from the analysis. The study aimed to test a method of generating stimuli with texture-shape cue conflicts by cropping texture images with a shape mask. This method involved using silhouette images as masks for texture images to create cue conflict stimuli. The stimuli were visualized in Figure 7, showing silhouettes filled with rotated textures and style transfer stimuli. In a control experiment, the texture database was augmented by rotating textures with different angles to create a database of 480 images. Human accuracies were provided for reference in a robustness experiment with distorted images. CNNs were evaluated on image manipulations applied to natural images for comparability. In a study using image manipulations, human observers and ImageNet-trained networks were tested on stimuli with texture-shape cue conflicts. Human observers showed a shape bias even when instructed to ignore shapes, indicating a default reliance on shape information. The experiment conducted as a control to ensure differences between humans and CNNs in cue conflict images are not due to setup. Humans show a shape bias even when ignoring shapes, while CNNs have a less pronounced texture bias. Texture bias is not specific to ImageNet-trained networks and occurs in various network types. Comparison of texture-shape biases on cue conflict stimuli between ResNet-101 trained on ImageNet and ResNet-101 trained on Open Images Dataset V2, along with human data for comparison. Texture bias is present in various network architectures pre-trained on ImageNet."
}