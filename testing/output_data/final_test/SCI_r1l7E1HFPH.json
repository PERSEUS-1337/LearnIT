{
    "title": "r1l7E1HFPH",
    "content": "In this work, the benefits of multi-step greedy policies in model-free Reinforcement Learning are explored within the framework of multi-step Dynamic Programming. These algorithms solve short-horizon decision problems iteratively and improve model-free algorithms by mitigating instabilities. Results are demonstrated on both discrete and continuous control problems. The field of Reinforcement Learning (RL) has seen success with deep neural networks in solving decision-making problems. Model-free RL algorithms like Q-learning and policy gradient have high variance and struggle with function approximation, especially in long-horizon decision problems. Using smaller discount factors can lead to more stable algorithms. Using smaller values of \u03b3 in RL addresses \u03b3-dependent issues and leads to more stable algorithms, but may result in biased solutions. Efroni et al. (2018a) proposed a multi-step greedy approach to mitigate \u03b3-dependant instabilities in RL, introducing \u03ba-greedy policy and \u03ba-PI and \u03ba-VI algorithms that iteratively solve \u03b3\u03ba-discounted decision problems. In this work, model-free deep RL implementations of \u03ba-PI and \u03ba-VI algorithms are derived and empirically validated using DQN and TRPO. These algorithms solve \u03b3\u03ba-discounted decision problems and improve the performance of model-free algorithms by using them as solvers of multi-step greedy PI and VI schemes. Important implementation details are emphasized in the experiments conducted. In this paper, the agent's interaction with the environment is modeled as a \u03b3-discounted Markov Decision Process (MDP). The MDP is defined by state and action spaces, transition kernel, reward function, discount factor, and initial state distribution. A stationary Markovian policy \u03c0 maps states to a probability distribution over actions. The value and action-value functions of \u03c0 are defined in terms of expected rewards. The optimal policy \u03c0* maximizes rewards with a maximum value of V max = R max /(1 \u2212 \u03b3). The optimal policy \u03c0* maximizes rewards with a maximum value of V max = R max /(1 \u2212 \u03b3). It is obtained through Policy Iteration (PI) and Value Iteration (VI) algorithms based on DP. VI iteratively computes the optimal Bellman operator, while PI calculates a 1-step greedy policy. The algorithms converge to the optimal value function V* and the optimal policy \u03c0*. In practice, the state space is often large, leading to approximate PI (API) and VI (AVI) algorithms. Munos (2003; 2005) and Farahmand et al. (2010) showed that error in these algorithms can be controlled. After N steps, algorithms converge to a solution \u03c0 N with bounded difference from the optimal value.\u03b4 represents per-iteration error, C upper-bounds mismatch in sampling distribution, and the second term in Eq. 3 accounts for error due to initial values of policy/value. The error due to initial values of policy/value decays with the number of iterations N. Generalizing the optimal Bellman operator and 1-step greedy policy to multi-step involves using h-optimal Bellman operator and h-step greedy policy with a lookahead of horizon h. Efroni et al. (2018a) introduced \u03ba-optimal Bellman operator and \u03ba-greedy policy for \u03ba \u2208 [0, 1]. The \u03ba-greedy policy w.r.t. the value function V is the optimal policy w.r.t. a \u03ba-weighted geometric average of all future h-step returns. Solving these equations is equivalent to solving a surrogate \u03b3\u03ba-discounted MDP with the shaped reward r t (\u03ba, V ), denoted as M \u03b3\u03ba (V ). Efroni et al. (2018a) derived \u03ba-PI and \u03ba-VI algorithms based on \u03ba-optimal Bellman operator and \u03ba-greedy policy. Efroni et al. (2018a) derived \u03ba-PI and \u03ba-VI algorithms, which are generalizations of the seminal PI and VI algorithms. Both PI and VI converge to the optimal value with an exponential rate depending on the discount factor \u03b3. Efroni et al. (2018a) introduced \u03ba-PI and \u03ba-VI algorithms, which converge faster than PI and VI but are computationally more expensive. These algorithms have properties that affect their asymptotic performance, such as the dependence on \u03ba and the impact of soft updates on \u03ba-greedy policies. The \u03ba-greedy policy \u03c0\u03ba may not always outperform \u03c0 with soft updates, suggesting the use of 'hard' updates with \u03c0\u03ba as the new policy. Implementing \u03ba-PI and \u03ba-VI involves solving a discounted surrogate MDP iteratively, either with a model using DP algorithms or model-free RL algorithms. This paper focuses on efficient model-free RL implementations for solving the surrogate MDP in \u03ba-PI and \u03ba-VI. In this paper, DQN and TRPO are used as subroutines for estimating a \u03ba-greedy policy or an optimal value of the surrogate MDP in \u03ba-PI and \u03ba-VI algorithms. The value of N (\u03ba) is set for the total number of iterations, and the number of samples for each iteration is determined. The implementation involves initializing replay buffer D, Q-networks, and target networks, followed by policy improvement steps. In this paper, DQN and TRPO are used as subroutines for estimating a \u03ba-greedy policy or an optimal value of the surrogate MDP in \u03ba-PI and \u03ba-VI algorithms. The value of N (\u03ba) is set for the total number of iterations, and the number of samples for each iteration is determined. The implementation involves initializing replay buffer D, Q-networks, and target networks, followed by policy improvement steps. Act by an \u03b5-greedy policy w.r.t. Q \u03b8 (st, a), observe rt, st+1, and store (st, at, rt, st+1) in D. Update \u03b8 by DQN rule with sampled batch. Update \u03c6 by TD(0) off-policy rule. Set N (\u03ba) to satisfy a specific equality for performance optimization. The approach suggests reasonable choices for N (\u03ba), such as N (\u03ba = 0.99) 4 and N (\u03ba = 0.5) 115, for C F A = 0.1 and \u03b3 = 0.99. Increasing \u03ba leads to fewer iterations needed for \u03ba-PI and \u03ba-VI to converge to a good policy. The discount factor of the surrogate MDP solved by \u03ba-PI and \u03ba-VI at each iteration is \u03b3\u03ba, increasing the effective horizon of the surrogate MDP with \u03ba. The number of samples per iteration, denoted by T (\u03ba), is determined beforehand, with an equal allocation of samples per iteration. DQN and TRPO are implemented in \u03ba-PI and \u03ba-VI algorithms. In Sections 5.1 and 5.2, DQN and TRPO are implemented as \u03ba-greedy solvers in \u03ba-PI and \u03ba-VI algorithms. The resulting algorithms are \u03ba-PI-DQN, \u03ba-VI-DQN, \u03ba-PI-TRPO, and \u03ba-VI-TRPO. Experiments are conducted to study the impact of \u03ba and N on performance, focusing on whether there is a performance tradeoff with \u03ba and the necessity of a significant number of samples per iteration. In \u03ba-PI-DQN, the algorithm uses DQN to solve the \u03b3\u03ba-discounted surrogate MDP with a shaped reward. The output of the DQN approximates the optimal Q-function, leading to the \u03ba-greedy policy. Off-policy TD(0) is used to evaluate the Q-function of the current policy. The off-policy data in the replay buffer allows for easy evaluation of the Q-function using off-policy TD(0). A target network Q\u03b8 is recommended for accurate estimation of the value function, but for space complexity reasons, it is not used in the \u03ba-VI-DQN algorithm. The pseudo-code for \u03ba-VI-DQN is provided in Appendix A.1, where \u03ba-VI repeats V \u2190 T\u03baV to compute the optimal value of the surrogate MDP M\u03b3\u03ba(V). In \u03ba-VI-DQN, the optimal value of the surrogate MDP is repeatedly solved by DQN to shape the reward of the next iteration. Empirical analysis of \u03ba-PI-DQN and \u03ba-VI-DQN on Atari domains shows performance. Ablation tests on parameter CFA values are conducted on the Breakout domain. The value of CFA sets the number of samples per iteration and total number of iterations in DQN-based algorithms. The best performance was obtained with CFA = 0.05 in experiments on Atari domains. Training performance of \u03ba-PI-DQN and \u03ba-VI-DQN with CFA = 0.05 is shown in Table 1. The results are presented in Figure 1 and Table 1 for easier reproducibility using OpenAI Baselines codebase. The results in Fig. 1 and Table 1, along with those in Appendix A.2, demonstrate the performance improvement of \u03ba-PI-DQN and \u03ba-VI-DQN over DQN (\u03ba = 1). Setting N (\u03ba) = T leads to performance degradation in most domains except Enduro, where it performs better. Despite the degradation, N (\u03ba) = T still outperforms DQN. Algorithm 4 outlines the pseudo-code of \u03ba-PI-TRPO, which iteratively updates the policy using return and value function estimates. In this section, the \u03ba-PI-TRPO and \u03ba-VI-TRPO algorithms are empirically analyzed on MuJoCo domains: Walker2d-v2, Ant-v2, HalfCheetah-v2, HumanoidStandup-v2, and Swimmer-v2. The \u03ba-PI-TRPO algorithm eliminates the policy evaluation stage, converting it to \u03ba-VI-TRPO by copying \u03c6 \u2190\u03b8, where V \u03c6 \u2190 V\u03b8 = T \u03ba V \u03c6. The \u03ba-PI-TRPO and \u03ba-VI-TRPO algorithms were tested on MuJoCo domains including Walker2d-v2, Ant-v2, HalfCheetah-v2, HumanoidStandup-v2, and Swimmer-v2. Ablation tests were conducted on the parameter CFA = {0.001, 0.05, 0.2} on the Walker domain with a total of 2000 iterations and 1000 samples per iteration. The best performance was achieved with CFA = 0.2, which was then used in experiments with other MuJoCo domains. Table 2 displays the final training performance of \u03ba-PI-TRPO and \u03ba-VI-TRPO on MuJoCo domains with CFA = 0.2. Results indicate that both algorithms outperform TRPO (\u03ba = 1), with CFA = 0.2 showing better performance than N(\u03ba) = T. The relationship between \u03ba-PI and the GAE algorithm is discussed further in this section. Schulman et al. (2016) introduced the Generalized Advantage Estimation (GAE) algorithm, which updates the policy and value function concurrently. GAE is compared to \u03ba-PI-TRPO and \u03ba-VI-TRPO, with the latter two showing slightly better performance. The performance of GAE is slightly worse than \u03ba-PI-TRPO and \u03ba-VI-TRPO. A modification was made to the OpenAI baseline implementation of GAE, aligning it with Schulman et al. (2016). Testing different \u03b3 values showed improved performance for TRPO in the Ant domain. \u03ba-PI-TRPO and \u03ba-VI-TRPO outperformed TRPO in the Ant domain. The performance of DQN and TRPO on various domains did not improve with changes in the discount factor \u03b3. Performance in Mujoco domains improved with smaller \u03b3 values, while it degraded in Atari domains. \u03b3 and \u03ba are different parameters that can be optimized separately for algorithm performance. In this work, simple generalizations of DQN and TRPO, known as \u03ba-PI and \u03ba-VI algorithms, were empirically tested. \u03ba-PI outperformed \u03ba-VI in Atari domains, while \u03ba-VI performed slightly better in Gym domains. The discrepancy in performance between the domains may be due to the short horizon decision problems in Gym domains. In short horizon decision problems, smaller discount factors can be used without needing information on the policy's value. Non-trivial \u03ba values improve performance in most experiments, except for Swimmer and BeamRider domains. Careful hyperparameter tuning of \u03ba is not necessary, and choosing the number of iterations using Eq. 8 enhances performance on tested domains. In model-free DRL, multi-step DP can be used in model-based DRL with an approximate model. Setting \u03ba as a function of the model's quality may improve performance. Future work could explore the relation between \u03ba and the model. Studying algorithms with an adaptive \u03ba parameter could enhance methods. In model-free DRL, multi-step DP can be used in model-based DRL with an approximate model. Setting \u03ba as a function of the model's quality may improve performance. Future work could explore the relation between \u03ba and the model. Studying algorithms with an adaptive \u03ba parameter could enhance methods. The detailed pseudo-codes of the \u03ba-PI-DQN and \u03ba-VI-DQN algorithms are reported in this section. In this section, additional results of the application of \u03ba-PI-DQN and \u03ba-VI-DQN on the Atari domains are reported. The detailed pseudo-codes of the \u03ba-PI-TRPO and \u03ba-VI-TRPO algorithms are also provided. The detailed pseudo-codes of the \u03ba-PI-TRPO and \u03ba-VI-TRPO algorithms are presented in Section 5.2. Initialize networks and policy with random weights, simulate policy for M time-steps, update parameters using TRPO, and evaluate the \u03ba-greedy policy. In this section, additional results of \u03ba-PI-TRPO and \u03ba-VI-TRPO on MuJoCo domains are discussed. Results for \u03ba-PI TRPO in the CartPole environment show no significant difference with \u03ba values close to 1.0. Experiments use a single-layered value function network and a linear policy network, with each configuration run for 10 random seeds. Plots display a 50% confidence interval. The \u03ba values closer to 1.0 show a clear difference in performance, especially when the discount factor \u03b3 is lowered. Setting \u03b3 to 0.36 allows for a clearer trade-off between \u03ba values. \u03ba-PI and \u03ba-VI aim to solve simpler sub-problems at each time step, considering the discounting and weighting of the shaped reward. In the CartPole domain, the shaping term does not affect performance, while the discounting term does. This suggests no bias issues in the problem. The shaped term correction is unnecessary for CartPole but may be needed for more complex problems like Mountain Car. Lowering \u03ba values in CartPole degrades performance, unlike in Mountain Car. In the continuous Mountain Car domain, increasing the \u03ba value leads to performance deterioration. Using \u03ba values of 0 for discounting and 1 for shaping results in the best performance. In the Pendulum environment, a non-trivial best \u03ba value is observed due to the absence of a maximum return ceiling. Choosing the best \u03b3 value and running \u03ba-PI on it improves performance for all \u03ba values. In short horizon domains like Mujoco continuous control tasks, \u03ba-PI and VI with discounting show significant performance improvement over TRPO baselines. In long horizon domains like Atari, \u03ba-PI and VI with shaping outperform DQN baselines. Lowering the discount factor leads to performance deterioration in both cases."
}