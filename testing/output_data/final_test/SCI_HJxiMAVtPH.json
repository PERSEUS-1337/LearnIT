{
    "title": "HJxiMAVtPH",
    "content": "We present network embedding algorithms (AE and MUSAE) that capture node information from local attribute distributions. These algorithms are useful for diverse applications and outperform comparable models on various network datasets. The embeddings implicitly factorize matrices of node-feature mutual information. Figure 1 illustrates the design inspiration for the multi-scale attributed network embedding procedure. The text introduces Skip-gram style embedding algorithms (AE, MUSAE, AE-EGO, MUSAE-EGO) that consider attribute distributions over local neighborhoods. The embeddings approximately factorize PMI matrices based on adjacency matrix power and node-feature matrix. Popular network embedding methods like DeepWalk are also discussed. Efficient unsupervised learning of node embeddings for large networks has seen significant progress in recent years. The proposed Skip-gram style embedding algorithms (AE, MUSAE, AE-EGO, MUSAE-EGO) focus on attribute distributions over local neighborhoods and factorize PMI matrices using adjacency matrix power and node-feature matrix. Popular network embedding methods like DeepWalk and Walklets are shown to be special cases of AE and MUSAE. Empirical results demonstrate strong performance in regression, classification, and link prediction tasks for real-world networks like Wikipedia and Facebook, with computational scalability and transfer learning capabilities. Reference implementations and datasets are available for evaluation. Our work aims to learn similar latent representations for nodes with similar features in their neighborhoods. Neighborhood preserving node embedding methods place nodes with common neighbors close together in the embedding space. These methods were inspired by the Skip-gram model, which generates word embeddings by factorizing a PMI matrix. DeepWalk is a method that generates random walks over a graph to obtain a \"corpus\" for the Skip-gram model. DeepWalk generates random walks over a graph to create a \"corpus\" for the Skip-gram model, which generates node embeddings. It implicitly factorizes a PMI matrix and pools matrices based on a first-order Markov process. However, this pooling method may not be optimal as it treats neighbors over increasing path lengths equally. In contrast, multi-scale node embedding methods like LINE, GraRep, and Walklets learn lower-dimensional components from each adjacency matrix power and concatenate them for the full node embedding. Multi-scale node embedding methods learn lower-dimensional components from each adjacency matrix power and concatenate them for the full node representation, giving higher performance in downstream settings without increasing parameters. Attributed node embedding procedures incorporate node attributes to determine pairwise proximity in the node embedding space. TADW is an elemental procedure that decomposes a convex combination of normalized adjacency matrix powers into a matrix product. Several models, including SINE, ASNE, TENE, AANE, and BANE, factorize matrices to obtain attributed node embeddings. These methods use various approaches such as joint non-negative matrix factorization and network structure-based regularization. The curr_chunk discusses the relationship between different methods for obtaining attributed node embeddings, such as GCNNs and multi-scale GCNNs. Various graph convolutional layers are compared, highlighting the differences in pooling node attributes from neighborhoods. The focus is on learning node embeddings using nearby attributes. The curr_chunk introduces algorithms for learning node embeddings based on nearby attributes in an undirected graph. It aims to create similar embeddings for nodes with similar attributes and for attributes in similar node neighborhoods. The embeddings are represented by matrices G and H in a latent space. The curr_chunk describes the Attributed Embedding (AE) procedure for learning node embeddings in an undirected graph. It involves sampling nodes for random walks, creating node sequences, and generating node and feature embeddings using Skip-gram with negative sampling. The AE method involves sampling nodes for random walks, creating node sequences, and generating node and feature embeddings using Skip-gram with negative samples. The Multi-Scale Attributed Embedding (MUSAE) adapts the AE algorithm to provide multi-scale attributed node embeddings. The AE method involves sampling nodes for random walks and generating node and feature embeddings using Skip-gram with negative samples. Specifically, for a given window size, the algorithm iterates over node sequences to create sub-corpora based on word co-occurrence statistics. Levy & Goldberg (2014) demonstrated that Skip-gram with negative sampling minimizes the loss function by factorizing a matrix of pointwise mutual information. The text discusses word embeddings and matrix factorization techniques applied to node embedding models in graphs. It mentions the factorization of a shifted PMI matrix and how AE and MUSAE implicitly perform similar matrix factorizations. Notation for adjacency and degree matrices in graphs is also explained. The text discusses the stationary distribution and joint distribution in graphs, using binary attribute matrices and conditional distributions. It explains the Bernoulli parameter for feature observation probabilities and the minimization of the SGNS aspect in MUSAE. The MUSAE embeddings factorize the node-feature PMI matrix using empirical statistics from random walks, providing unbiased estimates of joint probabilities. The AE algorithm provides unbiased estimates of joint probabilities over different path lengths. DeepWalk and Walklets are corner cases of AE and MUSAE, respectively. Adding an identity matrix to the feature matrices results in algorithms that learn embeddings that factorize the node-feature PMI. The AE algorithm and MUSAE learn embeddings that factorize node-feature PMI matrices. The runtime complexity for corpus generation and optimization is O(nlt x/y) and O(bdnlt x/y) respectively. Truncated walks per node affect corpus generation complexity and model optimization runtime. The memory complexity of storing AE embeddings is O(yd) and MUSAE embeddings also use O(yd) memory. The quality of representations is evaluated through supervised tasks like node classification, transfer learning, regression, and link prediction. Experiments on social networks and web graphs from Facebook, Github, Twitch, and Wikipedia are conducted, along with citation networks for model evaluation. The impact of input size changes on runtime is also investigated. In experiments, hyperparameter settings were consistent across models. Node classification performance was evaluated in two scenarios: k-shot learning using attributed embeddings with logistic regression, and predictive performance under fixed train-test splits. The latter involved using attributed node embeddings to train a logistic regression model with l2 regularization. Results were obtained through 100 seeded splits for robustness. The average micro F1 scores were calculated to compare unsupervised node embedding methods, showing little gains in performance when additional data points are added to the training set. MUSAE-EGO and AE-EGO had a slight performance advantage, especially when a small amount of labeled data is available. In a series of experiments, performance was evaluated based on training samples per class using average micro F1 scores from 100 seeded train-test splits. Results were compared to various embedding and graph neural network methods, with consistent random seeds used for train-test splits. Results were presented for Facebook, Github, and Twitch Portugal graphs in Table 6 of Appendix G, showing the best performing unsupervised embedding model in red and the strongest supervised neural model in blue. Additional results for unsupervised methods on Cora, Citeseer, and Pubmed graphs were also provided in Table 5 of Appendix G. Based on the experiments conducted, supervised node embedding methods outperform unsupervised methods on various datasets. The relative advantage in terms of micro F1 score is significant, with the largest difference observed on the Facebook network. Several observations were made based on the results, including the effectiveness of multi-scale representations and the limited impact of adding nodes in ego augmented models when ample labeled training data is available. Based on experiments, supervised neural models do not always outperform unsupervised methods. Neighbourhood-based methods like DeepWalk are transductive, while vanilla MUSAE and AE are inductive and can easily map nodes to the embedding space if attributes are shared. Supervised models trained on the source graph's embedding are transferable, unlike attributed embedding methods like AANE or ASNE. In a transfer learning experiment using Twitch country level social networks, an embedding function was learned from one country's social network and used to predict whether users stream explicit content on another country's network. The performance was evaluated using the micro F1 score on datasets from Germany, England, and Spain. In a transfer learning experiment using Twitch country level social networks, embeddings from MUSAE and AE were found to be transferable across graphs with shared vertex features. Transfer between German and English user graphs showed effective micro F1 score improvement. However, transfer to smaller graphs was generally poor. No clear evidence was found on whether MUSAE or AE gives better results in this specific problem. In a study comparing MUSAE and AE embeddings on Wikipedia webgraphs, MUSAE outperformed all benchmark methods, with the strongest variant showing a 2.05% to 10.03% improvement in test R2. MUSAE also significantly outperformed AE by 2.49% to 21.64% in test R2. Additionally, using vertices as features improved embedding performance, but this benefit seems to be dataset-specific. The final experiments focused on link prediction using an attenuated graph embedding approach. 50% of edges were randomly removed without changing the graph's connectivity. Edge features were calculated using binary operators, and logistic regression was used to predict edge existence. The approach was compared to attributed and neighborhood-based methods. The study compared different graph embedding methods for link prediction, with Walklets outperforming others. Attributed embedding methods performed poorly compared to neighbourhood-based ones. Experiments on synthetic graphs showed the impact of changing the number of vertices and features per vertex. The study compared graph embedding methods for link prediction, with Walklets performing best. Increasing features per vertex doubled runtime of AE and MUSAE. Number of cores used did not affect runtime with large feature sets. Doubling input size resulted in doubled optimization runtime. Interpolating linearly suggests a network with 1 million nodes, 8 edges. The study investigated attributed node embedding algorithms AE and MUSAE, proving their efficiency with linear runtime. These algorithms factorize probability matrices of features in node neighborhoods. Results show that MUSAE outperforms other node embedding methods on various datasets. Our proposed embedding models encode feature information from higher order neighborhoods, differentiating them from other methods. The empirical statistics of node-feature pairs obtained from random walks provide unbiased estimates of joint probabilities. The Weak Law of Large Numbers guarantees convergence of sample mean to expectation for sequences with bounded variance and covariances. For a single sequence generated by a random walk, we compute the sample average of indicator functions for specific node-feature pairs. The covariance tends to zero as the separation between variables increases, leading to convergence in probability to the expected value. The method involves using random walks to obtain unbiased estimates of joint probabilities of observing features around nodes in graphs. The approach was tested on various social networks and web page-page graphs, with details on graph statistics provided in Table 1. The datasets used are available with the source code of MUSAE and AE. The webgraph is a page-page graph of verified Facebook sites, collected through the Facebook Graph API in November 2017. It includes official Facebook pages from 4 categories: politicians, governmental organizations, television shows, and companies. The task related to this dataset is multi-class node classification for the 4 site categories. The largest graph used for evaluation is a social network of GitHub developers collected from the public API in June 2019. The largest graph used for evaluation is a social network of GitHub developers collected from the public API in June 2019. Nodes are developers who have starred at least 10 repositories and edges are mutual follower relationships between them. The task is binary node classification to predict whether the GitHub user is a web or a machine learning developer. The datasets used for node level regression are Wikipedia page-page networks on specific topics: chameleons, crocodiles, and squirrels. The curr_chunk discusses node features, average monthly traffic, and network characteristics in various datasets, including Twitch user-user networks. These networks are used for node classification and transfer learning tasks, with vertex features extracted based on gaming habits and friendships. The datasets allow for transfer learning across networks and were collected in May 2018. The supervised task related to these networks is binary node classification. The supervised task related to these networks is binary node classification, predicting if a streamer uses explicit language. Models use parameters for evaluation, including Doc2Vec embedding of node features. Logistic and elastic net regression are used for node level classification, regression, and link prediction. Standard settings are used for evaluation, except for regularization and norm mixing parameters. The regularization and norm mixing parameters were used for fair evaluation compared to other node embedding procedures. Different hyperparameter settings were tried to match the expressive power of competing models. DeepWalk utilized negative sampling for calculations, making it a special case of Node2Vec. LINE 2 created 64-dimensional embeddings based on first and second-order proximity. Other hyperparameters were taken from the original work. Node2Vec, Walklets, and attributed node embedding methods like AANE, ASNE, BANE, TADW, and TENE all used specific hyperparameter settings for downstream tasks. Parameters were tuned with grid search, and the final embeddings for each method were set to be 128 dimensional. The final embeddings for each method used in downstream tasks are 128 dimensional. The models were optimized with the Adam optimizer and sparsity-aware modifications were made based on PyTorch Geometric. Hyperparameters for training and regularization are listed in Table 4, with most models using information up to 2-hop neighborhoods. Model-specific settings were adjusted as needed. The models in Table 4 utilized various graph convolutional techniques such as Classical GCN, GraphSAGE, GAT, MixHop, and ClusterGCN with specific parameter settings and methodologies. The models in Table 4 utilized different graph convolutional techniques like Classical GCN, GraphSAGE, GAT, MixHop, and ClusterGCN with specific parameter settings and methodologies. The graphs were divided into clusters equal to the number of node classes, and for training, various methods like APPNP and SGCONV were used with specific configurations."
}