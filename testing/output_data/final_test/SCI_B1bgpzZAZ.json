{
    "title": "B1bgpzZAZ",
    "content": "The task of Reading Comprehension with Multiple Choice Questions involves selecting the correct option after reading a passage and question. The current model focuses on option selection based on similarity with a query-aware representation of the passage. However, humans use a combination of elimination and selection strategies. We propose ElimiNet, a neural network model that mimics this human process by incorporating gates. ElimiNet is a neural network model that incorporates gates to mimic human elimination and selection strategies in Reading Comprehension with Multiple Choice Questions. It refines document representation by eliminating options and outperforms current state-of-the-art models on the RACE dataset. An ensemble of ElimiNet with a selection-based method shows a 7% improvement over the best reported performance on the dataset. Reading comprehension involves answering questions from a given passage, with AI agents capable of applications like financial report analysis, troubleshooting, and general knowledge retrieval. Different variants of this task have been studied, including matching answers in the passage, synthesizing answers, or selecting from multiple choices. Building AI agents with deep language understanding for tasks like Reading Comprehension with Multiple Choice Questions (RC-MCQ) is gaining interest. Recently, BID10 released a large dataset for RC-MCQ from Chinese high school and middle school English exams, comprising 28000 passages and 100000 questions. Humans use option elimination and selection when answering MCQs, which can also help in discarding irrelevant portions of the document. This process can be repeated multiple times. Our proposed model for RC-MCQ aims to mimic the human process of answering MCQs by iteratively eliminating options and refining the document based on these eliminations. This is in contrast to current state-of-the-art models that focus solely on option selection without an iterative process. Our proposed model for RC-MCQ involves computing a query-aware document representation, using an elimination gate to decide on removing options, and refining the document by discarding aligned portions with eliminated options through orthogonalization. This iterative process aims to make the document representation dissimilar to irrelevant information. ElimiNet is a model that refines document representation by eliminating irrelevant options and selecting the most similar one. It outperforms GAR on 7 out of 13 question types in the RACE dataset. The model learns to iteratively refine the document representation and improve accuracy by 7% when combined with GAR in an ensemble model. The availability of large scale datasets has renewed interest in Reading Comprehension tasks. Various datasets offer different variations of RC tasks, such as span prediction, cloze-style, and multiple choice questions. The success of deep learning in NLP tasks has led to the development of neural network models for RC. The models proposed in BID17 BID14 and BID0 BID8 BID2 BID3 address different variants of Reading Comprehension tasks. They use a similar framework with components for encoding passage and query, capturing interactions, and making multiple passes over the passage. The differences lie in the choice of encoder, decoder, interaction function, and iteration mechanism. Our model introduces components for eliminating irrelevant options and refining passage representation in multiple rounds for selecting the most relevant option in Reading Comprehension Multiple Choice Questions (RC-MCQ). This is the first model to incorporate option elimination for RC-MCQ tasks, using a neural network to encode passages, questions, and options, capture interactions, eliminate options, and select the correct answer. The curr_chunk introduces modules for interaction, elimination, and selection in Reading Comprehension Multiple Choice Questions (RC-MCQ). It describes the process of computing vectorial representations using a bidirectional recurrent neural network with Gated Recurrent Units (GRU). The curr_chunk explains how the GRU units compute hidden representations for each word in the query and options. Different GRU cells are used for the query, options, and passage words. An interaction module allows the passage words' representations to be refined based on the query words' representations. The curr_chunk discusses refining passage word representations based on interactions with query words using a multi-hop architecture. This process involves computing the importance of each dimension of the passage word representation and using it as a gate to scale up or down different dimensions. The refined passage word representations then interact bidirectionally with each other. The elimination module refines passage representations by focusing on relevant options and modifying the passage accordingly. It introduces an elimination gate to determine if an option can be eliminated and adjusts the passage representation based on this decision. The elimination module introduces a gate to enable soft elimination of options, computed separately for each option based on final states of bidirectional GRUs and refined document representation. Parameters W e , V e , U e are learned for this process. The module refines the document representation by computing orthogonal components x e i and x r i, with the elimination gate deciding how much to retain. If e i = 1 (eliminate), the document representation is adjusted. The document representation is adjusted based on the elimination gate, which decides how much to retain. Parameters W s, V s, U s are learned for this process, introducing a subtract gate to control the components used in the equations. The document representation is refined by independently computing representations for each option and combining them to obtain a single refined representation. This process is repeated for multiple hops, with each hop refining the representation further. The selection module then computes the similarity between the refined document representation and each option representation. The model is trained using cross entropy loss by normalizing scores with softmax. The evaluation dataset is the RACE dataset with multiple choice questions from Chinese high school and middle school exams. The dataset has separate portions for high school (RACE-H) and middle school (RACE-M) with specific numbers of questions for training, validation, and testing. The RACE-M dataset contains 18728, 1021, and 1045 questions for training, validation, and testing respectively. It includes a variety of questions of different complexities, such as identifying appropriate titles, defining terms, summarizing key ideas, and standard Wh-type questions. The questions are categorized into 13 categories to explore the effectiveness of an elimination module for specific question types. The RACE-M dataset contains various question types categorized into 13 categories. Training the model involved two approaches: training all modules together or training them separately with a focus on the elimination module. Pre-training did not significantly improve performance, so results are reported without it. Hyperparameters: The vocabulary is limited to the top 50K words in the dataset for passage, query, and options. The passage length is not restricted, and the entire passage is processed through a bi-directional GRU. Model tuning is based on validation set accuracy, word embeddings are initialized with 100 dimensional Glove embeddings, and both fine-tuning and not fine-tuning are experimented with. The hidden size for BiGRU remains consistent. The study experimented with different hyperparameters for the BiGRU model, including hidden sizes of 64, 128, and 256, dropout values of 0.2, 0.3, and 0.5, and optimizer choices between Adam and SGD. Adam was found to converge faster, and models were trained for a maximum of 50 epochs. Results were compared with the Gated Attention Reader BID3 model on the RACE dataset, showing competitive performance. The study compared the performance of ElimiNet and Gated Attention Reader (GAR) models on the RACE dataset for middle school, high school, and full test sets. ElimiNet outperformed GAR in various question categories across the different datasets, showing only slight overall improvement on the entire test set. The study compared ElimiNet and GAR models on the RACE dataset, with ElimiNet outperforming GAR in various question categories. However, on the entire test set, the model only showed slight improvement over GAR due to the dominance of fill in the blank style questions. An ensemble of GAR and ElimiNet models was considered for overall performance improvement. The study compared ElimiNet and GAR models on the RACE dataset, with ElimiNet outperforming GAR in various question categories. An ensemble of GAR-ensemble and ElimiNet-ensemble was tested, with ElimiNetensemble performing better. The final ensemble yielded the best results, with an accuracy of 47.2%, outperforming GAR by 7% and GAR-ensemble by 2.8%. The subtract gate was found to improve model learning, as accuracy dropped without it. The study found that the subtract gate improves model learning by helping the model align the document representation with the correct option, as shown by a shift in probability scores during multiple passes of elimination. The model proposed focuses on Reading Comprehension with Multiple Choice Questions, using elimination and selection to arrive at the correct option. An elimination module decides whether an option should be eliminated, modifying the document representation accordingly. Gating functions determine the amount of alignment or orthogonalization, iteratively refining the document representation. The model outperforms current state-of-the-art models on 7 out of 13 question types in the RACE dataset. The model focuses on Reading Comprehension with Multiple Choice Questions, using elimination and selection to improve performance on the RACE dataset. Future work includes exploring reinforcement learning techniques for hard elimination. Jackie, with more experience, initially intimidated the narrator. Despite fear, the narrator directed by suggestion, leading to a successful scene with Gleason. Gleason appreciated the collaborative problem-solving approach, hinting at more creative ideas to come. When a bone is broken, it can be different for everyone, with sharp pain often present. It is important to stay calm, make the person comfortable, and call a doctor. Moving the injured part should be avoided as it could worsen the injury. The doctor will need to take an X-ray to set the bone back in place, possibly using metal pins for larger or multiple breaks, followed by putting on a cast. After a bone is broken, it is important to stay calm, make the person comfortable, and call a doctor. The doctor will set the bone back in place, possibly using metal pins for larger breaks, followed by putting on a cast to keep the bone in place for healing. Broken bones heal by producing new cells and blood vessels to close up the break until the bone is strong again."
}