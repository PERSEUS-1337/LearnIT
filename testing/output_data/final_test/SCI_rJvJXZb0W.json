{
    "title": "rJvJXZb0W",
    "content": "In this work, a framework for learning sentence representations from unlabelled data is proposed. The problem of predicting the context in which a sentence appears is reformulated as a classification task. A classifier distinguishes context sentences from other contrastive sentences based on their vector representations, leading to efficient learning of encoding functions. The model produces high-quality sentence representations that outperform state-of-the-art methods on NLP tasks, with a significant speedup in training time. Methods for learning meaningful representations of data have gained attention, especially in transfer learning with limited labeled data. Unsupervised learning from large unlabelled corpora, including self-supervision techniques, has become popular for training visual feature representations. Tasks like predicting spatial relationships between image patches, inpainting, and solving image puzzles have been successful in this regard. In the language domain, the distributional hypothesis has been crucial for developing methods to obtain semantic word representations. Neural approaches have been successful in learning high-quality representations from text corpora. Recent methods focus on learning sentence representations using encoder-decoder models. However, there are modeling issues as different sentences can express the same idea in various forms. Existing models aim to reconstruct context sentences but struggle with capturing the ideal semantic representation that is insensitive to the sentence structure. Existing models for learning sentence representations struggle with reconstructing the surface form of a sentence, leading to irrelevant predictions and high computational costs. To address this, a new objective operating directly in the space of sentence embeddings is proposed, replacing the generation objective with a discriminative approximation. This approach aims to improve semantic representation while reducing the computational burden associated with traditional methods. The new approach, quick thoughts (QT), focuses on efficient learning of sentence representations by using a discriminative approximation to identify the correct target sentence embedding. It aims to improve semantic representation and reduce computational costs compared to traditional methods. The approach establishes a new state-of-the-art for unsupervised sentence representation learning across various tasks involving sentence semantics. The pre-trained encoders will be publicly available. The pre-trained encoders will be made publicly available. Prior approaches to learning sentence representations from labelled and unlabelled data are discussed. BID18 proposed the paragraph vector (PV) model for embedding variable-length text. Unlike most methods, sentences are considered as atomic units. Encoder-decoder models and skip-thought vectors model have been successful at learning semantic representations. BID8 explore the use of convolutional neural network (CNN) encoders. The base model uses a CNN encoder and reconstructs input sentences and neighboring sentences using an RNN. They also consider a hierarchical version that sequentially reconstructs sentences within a larger context. Autoencoder models have been explored for representation learning in various data domains. Recursive autoencoders encode input sentences using a recursive encoder, and a decoder reconstructs the hidden states. De-noising autoencoder models introduce noise in sentences for reconstruction. Generative models of sentences are proposed based on a variational autoencoder. BID3 proposed a generative model of sentences using a variational autoencoder. BID13 learned bag-of-words (BoW) representations of sentences by identifying context sentences from candidates. Hill et al. (2016) introduced the FastSent model using a BoW representation to predict words in context sentences. BID2 used a weighted BoW model and showed better performance than BoW models trained on paraphrase data. BID10 used paragraph level coherence as a learning signal to choose the next sentence in a paragraph. In contrast to previous models, BID19's local coherence model uses a binary classifier to identify coherent/incoherent sentence windows. They focus on making observed contexts more plausible than contrastive ones, improving representation learning. By combining encoder-decoder sequence models' effectiveness with the efficiency of bag-of-words models, they achieve flexibility in architecture and efficient training. Attempts have been made to learn sentence representations using labeled/structured data. Different methods include mapping words to dictionary definitions, using paraphrase data, minimizing inner product between paired sentences in different languages, and using machine translation for obtaining more paraphrase data. Another approach involves using Natural Language Inference (NLI) as a means of learning generic sentence representations by identifying relationships between two given sentences. The training strategy involves learning a classifier on top of sentence embeddings to determine relationships between two sentences - entailment, neutral, and contradiction. Sentence encoders trained for this task show strong performance on downstream transfer tasks. Different approaches have been used to operationalize the distributional hypothesis, such as encoding and decoding functions to generate target sentences. Variations on the decoder include autoencoder models and predicting properties of a window of words in the input sentence. The approach involves using the meaning of the current sentence to predict the meanings of adjacent sentences by encoding the input sentence and choosing the correct target sentence from a set of candidates. This discriminative approximation to the generation problem facilitates learning rich representations. The model in FIG0 can ignore irrelevant aspects of a sentence to construct a semantic embedding space. Loss functions in a feature space, rather than raw data space, are preferred for similar reasons. Parametrized functions f and g encode sentences into fixed-length vectors. S ctxt is the set of sentences in the context of a given sentence s, while S cand contains context and non-context sentences for classification. The modeling approach involves a scoring function to determine the probability of a candidate sentence being correct in the context. The training objective aims to identify the correct context sentences for each sentence in the training data. The approach is based on the Skip-gram model, treating words as sentences and maximizing similarity between source and target words. Alternatively, a binary classifier can classify sentence windows as plausible or not. The modeling approach involves a binary classifier for sentence windows, aiming to classify them as plausible or implausible context windows. Objective (2) is found to work better by requiring ground-truth contexts to be more plausible than contrastive contexts. This approach encourages the sentence encoders to learn useful representations by minimizing the parameters in the classifier. The modeling approach involves a binary classifier for sentence windows to classify them as plausible or implausible. Encoders are motivated to learn useful representations with different parameters. The sentence representation is the concatenation of outputs from two encoders. RNNs are used as encoders, with GRU BID4 cells. The final hidden state of the RNN is the sentence representation. The sentence representations are evaluated using downstream NLP tasks, focusing on semantics rather than just syntax. Models are trained on the BookCorpus dataset with 45M sentences and the UMBC corpus with 129M sentences, which is three times larger. The dataset used for training models includes 129M sentences, three times larger than BookCorpus. Different vocabularies were used for the two datasets. A simple scheme for selecting contrastive sentences was found to be effective. Hyperparameters were tuned based on prediction accuracies on the validation set. A context size of 3 was used, with a batch size of 400 and learning rate of 5e-4 for all experiments. The RNN-based models used a batch size of 400 and learning rate of 5e-4 with the Adam optimizer. GRU cells were employed with weights initialized using uniform Xavier initialization. Tasks evaluated included movie review sentiment, product reviews, subjectivity classification, opinion polarity, question type classification, paraphrase identification, and semantic relatedness on the SICK dataset. MPQA tasks are binary classification tasks evaluated using 10-fold cross validation. Other tasks have train/dev/test splits, with the dev set used for regularization parameter selection. Evaluation involves obtaining feature representations from trained encoders and training a logistic/softmax classifier on top of embeddings for each task. Comparison with prior methods using unlabelled data is done in Table 1, showing dimensionality of representations and training time. RNN-based encoder variations similar to skip-thought model are considered, including uni-QT and bi-QT models using uni-directional and bi-directional RNNs as sentence encoders. The bi-QT and combine-QT models use RNNs to process sentences in different directions. The FastSent model is efficient but performs poorly compared to other methods due to its bag-of-words encoder. The de-noising autoencoder (SDAE) excels in paraphrase detection tasks by preserving word identity and order information. The uni/bi/combine-QT variations outperform the skipthought model and CNN-based BID8 in various tasks, requiring less training time. The MultiChannel-QT model combines pre-trained word embeddings with tunable word embeddings, showing good word representations. The MC-QT model, inspired by BID14, uses GloVe vectors for word embeddings and outperforms previous methods on UMBC data. It shows improvements on tasks when trained on a larger dataset of documents, demonstrating computational efficiency in Tensorflow implementation. Our models, implemented in Tensorflow, show computational efficiency. The best BookCorpus model (MC-QT) trains in under 11hrs on a GTX Titan X GPU. Training time for skip-thoughts model is 2 weeks, but a recent Tensorflow implementation reports 9 days on a GTX 1080. On the augmented dataset, our models take about a day to train, with improvements in all tasks except the TREC task. Our framework allows training with larger vocabulary sizes and is memory efficient. Our RNN implementation for reconstruction objectives consumes heavy memory, with most of it used by word embeddings. Comparison against task-specific supervised models shows our approach using linear classifiers on pre-trained embeddings. Additionally, our models are compared to methods trained on labelled/structured data, including tasks like image-caption matching, dictionary word mapping, and machine translation. The Infersent model performs well on various tasks, but a multichannel model trained on (BookCorpus + UMBC) data outperforms it in most tasks, especially in SST and TREC. Infersent excels in the SICK task due to its training on near paraphrases and non-paraphrases, using difference and multiplicative features of input sentence pairs. Ensembling different types of encoders is considered to leverage their strengths efficiently. Ensembling different types of encoders to leverage their strengths efficiently, a subset of model variations are considered for image-caption retrieval. Models are combined using a weighted average of predicted log-probabilities, approaching the performance of the best supervised task-specific methods in 3 out of 8 tasks. Performance results are presented in tables 3 and 4, with the ensemble model outperforming purely supervised methods. The evaluation setting involves image-to-caption and caption-to-image retrieval tasks, where images and captions are represented as vectors. A scoring function is trained using a margin loss to determine compatibility between image-caption pairs. VGG-Net features are used for image representation, while sentences are represented as vectors using a learning method. The scoring function aims to match pairs with higher compatibility than mismatching pairs. The scoring function f (x, y) = (U x) T (V y) is used in training with fixed representations. The MSCOCO dataset BID20 is commonly used, with train/val/test splits proposed in BID12. Results show significant improvement over previous unsupervised pre-training methods, outperforming both annotation and search tasks. The model has similarities to the skip-thought model in objective functions. The model has similarities to the skip-thought model in objective functions. Nearest neighbor retrieval experiment compares embedding spaces using 1M sentences from Wikipedia. Results show better related retrievals to query sentences compared to skip-thought model. Model identifies similar meaning sentences even with different clause orders, aligning with the goal of less sensitivity to expression form. Seizures may occur as glucose levels fall further, especially during rapid entry into autorotation. Low brain glucose levels can lead to seizures. Evidence of visa provision under Republic Act No. was made public after inquiries. The American alligator is the only natural predator of the panther and their mascot. Several individuals died prematurely, including Carmen, Toms, Carlos, and Pablo. Several individuals, including Carmen, Toms, Carlos, and Pablo, died prematurely. Ahmed Sher died at the age of 13. Music from \"Expo 2068\" and \"Dialogue\" was performed at various events. Mohammad Ali Jinnah agreed to make Urdu Pakistan's official language. ST Georges Charachidz and QT Wali Mohammed Wali made significant contributions to their respective fields. The PCC and retrosplenial cortex form the retrosplenial gyrus. The PCC, along with the retrosplenial cortex, forms the retrosplenial gyrus. The Macro domain macroH2A1.1 binds an NAD metabolite. No treaties were signed in British Columbia until 1998, except for the Douglas Treaties negotiated by Sir James Douglas. The Natal Railway Company's assets were purchased by the Natal Colonial Government in 1876. Our approach learns richer representations efficiently from large unlabelled text corpora, outperforming prior methods with less training time. We set a new state-of-the-art for unsupervised sentence representation learning. Comparing our model to skip-thought vectors in reasoning analogies at the sentence level, we formulate it as a retrieval task using query vectors. Our approach sets a new state-of-the-art for unsupervised sentence representation learning by efficiently extracting richer representations from large unlabelled text corpora. In comparing our model to skip-thought vectors for reasoning analogies at the sentence level, we formulate it as a retrieval task using query vectors. Guu et al. (2017) utilize word analogy datasets to create sentence tuples with analogical relationships, mining sentence pairs from the Yelp dataset that differ by a single word. They construct sentence analogy tuples based on known word analogy tuples, resulting in a dataset of 1300 sentence tuples. Each tuple generates 4 questions, with the candidate pool for sentence retrieval comprising all sentences in the dataset and 1M additional sentences from Yelp. TAB7 showcases the retrieval performance of our representations and skip-thought vectors on this task. Our model outperforms skip-thoughts in word analogy datasets, showing good performance in family and verb transformation categories. Qualitative retrieval results are shown in TAB8, highlighting better linearity properties in our representations. The model sometimes fails by assuming A and B are identical in analogical questions. In this section, the representations learned by RNN and BoW encoders on semantic similarity tasks are evaluated using STS14 datasets. The models were trained from scratch on BookCorpus data, with hyperparameter choices made for embedding size, number of contrastive sentences, and context size. The BoW encoder training process is described, with a focus on assessing similarity scores between pairs of sentences. Training the model on the BookCorpus dataset takes 2 hours on a Titan X GPU. Our RNN-based encoder outperforms other sequence encoders, while our Bag-of-Words variation performs comparably to prior models. QT (RNN) and QT (BoW) are our models trained with RNN and BoW encoders, respectively. In an experiment comparing training efficiency, a single-layer GRU RNN with H=1000 and W=300 word embedding size was used. Both models were trained for 1 epoch on the same data with Adam optimizer, batch size 400. The model trained with the ST objective took 31 hrs, while the model trained with their objective took 6.5 hrs. The evaluation of the models trained with different objectives shows a significant speed benefit for the model trained with their objective, with a 4.8x training speedup observed. Further speedups can be achieved by training with smaller encoders and concatenating them, exploring the trade-off between training efficiency and representation quality. The study explores the trade-off between training efficiency and representation quality by varying the size of embeddings in the multi-channel model (MC-QT) trained on the BookCorpus dataset. Results show that reducing the embedding size allows for more efficient training, albeit with a slight decrease in performance on downstream tasks. The study investigates the impact of reducing embedding size on training efficiency and model performance. Lower-dimensional models achieve comparable mean accuracies to higher-dimensional models, suggesting that high-quality models can be trained more efficiently. Training time and performance for different embedding sizes are compared across classification benchmarks."
}