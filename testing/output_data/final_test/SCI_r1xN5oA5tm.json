{
    "title": "r1xN5oA5tm",
    "content": "Most state-of-the-art neural machine translation systems incorporate attention mechanisms. This paper introduces novel phrase-based attention methods for n-grams of tokens, integrated into the Transformer network. The approach shows improvements in BLEU scores for various language translation tasks. Neural Machine Translation (NMT) has become the standard approach for translation tasks, utilizing an encoder-decoder architecture with attention mechanisms. The Transformer network has achieved state-of-the-art performance by solely using attention modules. Phrasal interpretation is crucial for language processing tasks and forms the basis of Phrase-Based Machine Translation, allowing for various relations between source and target tokens. The concept of phrasal attentions has been overlooked in NMT models, which typically generate translations token-by-token. Existing NMT systems lack explicit modeling of phrasal alignments, relying solely on neural architectures for contextual information. However, incorporating an inductive bias for phrases and phrasal alignments is essential for NMT to leverage the strong correlation between source and target tokens. In this paper, phrase-based attention methods are proposed for NMT to improve phrasal alignments. Two novel phrase-based attentions, CONVKV and QUERYK, are introduced to assign attention scores directly to phrases in the source and compute phrase-level attention vector for the target. Three new attention structures are also presented for conducting phrasal alignments, including homogeneous and heterogeneous structures that handle token-to-token and token-to-phrase mappings. The effectiveness of the approach is demonstrated by applying the phrase-based attention methods to all multi-head attention layers of the Transformer, showing improvements in WMT'14 translation tasks. The experiments on WMT'14 translation tasks show significant improvements in BLEU points for various language pairs compared to the baseline Transformer network. Most NMT models use an encoder-decoder framework to transform input sequences into continuous representations for generating target sequences. Different seq2seq models have led to state-of-the-art results, with convolutional models addressing the sequential computation drawback of recurrent models. The Transformer network BID19 utilizes parallel computation to reduce training time by structuring the encoder and decoder with stacked self-attentions and cross-attentions. It employs multi-headed, scaled multiplicative attention for query, key, and value vectors, with attentions based on self-attention in the encoder and decoder, and cross-attention in the decoder. The Transformer network utilizes parallel computation to reduce training time with stacked self-attentions and cross-attentions. Attention mechanisms in NMT architectures are order invariant, but RNN-based models use recurrent encoder/decoder, CNN-based models use position embeddings, and Transformer uses positional encoding to address this issue. Attention methods in NMT models attend to tokens, similar to word alignment models in traditional SMT, but it is acknowledged that phrases are better translation units. The NMT models should have explicit biases towards phrases to exploit the correlation between source and target phrases, as phrases are better translation units than words. This is important for robust phrasal alignment and learning the required mappings. The proposed method by BID21, SWAN, embeds phrases into attention modules in the Transformer network, allowing information propagation throughout the encoder, decoder, and cross-attention. Attention weights and vectors are computed based on n-grams of queries, keys, and values, with comparisons of different methods discussed. This approach can be applied to various architectures beyond the Transformer network. In this subsection, two novel methods are presented to achieve phrasal attention by applying temporal convolutional operations to sequence vectors representing tokens. The convolution operation involves window size n and kernel weights to generate a latent representation for each token. The notation Conv n (X, W) is used to denote the convolution operation over an input sequence X with window size n and kernel weights W. The key-value convolution method is discussed for potential application in different architectures. The key-value convolution technique uses trainable kernel parameters to compute latent representations of n-gram sequences. It involves an attention function with softmax, kernel weights for Q, K, and V, and zero-padding for key and value sequences. The CONVKV method is an indirect query-key attention approach. The CONVKV method uses kernel weights to compute n-gram patterns for attention. The Query-as-Kernel method allows queries to influence word order dynamically. The attention output includes trainable weights and a scaling factor to handle convolution operations. These phrase-based attention methods are extended to the Transformer framework for multi-headed attention. The Transformer framework incorporates homogeneous n-gram attention, where attention heads focus on different n-gram types. Each head attends to a specific n-gram type, with hyperparameters determining the allocation of n-grams to heads. Homogeneous attention requires left-padding for consistent sequence length and learns mappings in a distributed manner. However, the homogeneity constraint may restrict the model's capabilities. The homogeneous n-gram attention in the Transformer framework limits interactions between different n-gram types. Heterogeneous n-gram attention allows queries to attend to all types of n-grams simultaneously by computing attention logits for each n-gram type separately and then concatenating them before passing through the softmax layer. The heterogeneous attention process in the Transformer framework allows queries to attend to all types of n-grams simultaneously by computing attention logits for each n-gram type separately and then concatenating them before passing through the softmax layer. This approach eliminates the need to pad input sequences before the convolution operation and ensures that key/value sequences shorter than the window size do not have any valid phrasal component to be attended. The methods presented focus on attention mappings from token-based queries to phrase-based key-value pairs, beneficial for modeling token-to-token and token-to-phrase structures in translation tasks. In this section, a novel approach to heterogeneous phrasal attention is presented, allowing phrasal queries to attend to tokens and phrases of keys and values. This is achieved through the QUERYK and CONVKV methods, which involve applying convolutions on the query sequence to generate n-gram hidden representations. These representations are then used to attend over key-values, enabling phrase-to-token and phrase-to-phrase mappings. The novel approach to heterogeneous phrasal attention involves using queries to attend over unigram and bigram key-values, generating attention vectors for unigram and bigram queries. These vectors are then interleaved to form an aligned sequence for encoder and decoder. The interleaved vector sequences for unigram and bigram queries in the encoder and decoder are formed as DISPLAYFORM4. I enc and I dec represent self-attention interleaved sequences, while I cross represents cross-attention interleaved sequence. I dec and I cross are computed using the same formula but operate over different input sequences. Right connections are masked out in I dec and I cross to prevent information flow from the future in the decoder. The interleaving operation aligns phrase-and token-based representations of a token next to each other before passing through a convolution layer. The interleaved vectors are passed through a convolution layer to compute the overall representation for each token, allowing the model to learn the query's correlation with neighboring tokens. The encoder uses a window size of 3 and stride of 2 for unigram and bigram queries, while the decoder uses a window size of 2 and stride of 2 to preserve the autoregressive property. This section presents training settings, experimental results, and analysis of the models. The training settings for the models involved using the Adam optimizer with specific parameters, training on a single GPU due to limited resources, conducting experiments with sentence pairs containing 4096 tokens, applying residual dropout and label smoothing, and implementing the models in the tensor2tensor library on top of the original Transformer codebase. We conducted experiments with our models and the original Transformer in a fair comparison. Models were trained on WMT'16 English-German and English-Russian datasets with 4.5 and 25 million sentence pairs. Evaluation was done using newstest2013 and newstest2014 sets. Byte-pair encoding was used with vocabularies of 37,000 and 40,000 sub-words. Checkpoints were averaged for evaluation, with a beam search size of 5 and length penalty of 0.6. BLEU scores on WMT'14 test sets were reported for both language pairs. All models were trained with 1 GPU. Our models achieved higher BLEU scores than the Transformer base for En-De and En-Ru translation tasks. The best homogeneous model for En\u2192De had a BLEU of 26.86, outperforming the Transformer base by 0.8 points. The best heterogeneous model yields a BLEU of 27.15, surpassing the Transformer base by 1.1 points. Interleaved heterogeneous models achieve up to 27.4 BLEU, showing a 1.33 BLEU improvement over the Transformer base. On De\u2192En, models achieve up to +0.38 BLEU improvements compared to the Transformer base. Interleaved models outperform the Transformer by about 0.5 points, achieving up to 30.3 BLEU. On the En\u2192Ru translation task, models with CONVKV outperform the Transformer by 0.5 points, with the QUERYK model achieving the highest performance at 37.39 BLEU. Homogeneous models perform slightly better on Ru\u2192En task, while heterogeneous models excel in performance. The CONVKV heterogeneous model outperforms the Transformer base by 1.35 BLEU points on the English-to-German translation task. Interleaved attention models do not show significant improvements, possibly due to limitations in leveraging the morphological richness of Russian. Homogeneous models perform slightly better on the Ru\u2192En task, while heterogeneous models excel in performance overall. The effectiveness of relaxing the 'same n-gram type' attention constraint in heterogeneous models allows them to attend to all n-gram types simultaneously, resulting in better performance compared to homogeneous models. Including 3-gram and 4-gram components benefits heterogeneous models significantly, offering improvements in BLEU scores compared to models using only uni-and bi-grams. Our heterogeneous models show improved performance by including 3-gram and 4-gram components, leading to better BLEU scores compared to models using only uni-and bi-grams. Our experiments were conducted on a single GPU, in a 'low-resource' setting, as opposed to the state-of-the-art results reported by others using 8 GPUs. Training Transformer networks on a single GPU with lower batch size may not yield similar results as in higher GPU settings. In a low-resource setting on a single GPU, training Transformer models with smaller batch sizes may not yield similar results as in higher GPU settings. The Transformer big model performs slightly better with 8 GPUs compared to 1 GPU, likely due to batch size limitations. Additional experiments show that a heterogeneous model with 6 layers outperforms the Transformer base with 10 layers and matches the performance of the Transformer big model with 4 layers. Our heterogeneous model outperforms the Transformer base with 10 layers and matches the Transformer big model with 4 layers, achieving a BLEU of 37.39 (+2.75). Phrasal attention allows for similar or better results with a wider but shallower network, increasing parallelizability and suitability for low-resource setups. The model interprets alignments through attention heatmaps, showing improvements in complex language pairs like English-Russian. In a 6-layer Transformer model, phrasal attentions are concentrated in mid-layers, while token-to-token attention is highest in the top layer. The distribution of attentions varies based on model initialization, with a focus on phrases. TAB4 displays the distributions of phrase-and token-based attentions across different layers for two random seeds. In a 6-layer Transformer model, phrasal attentions are concentrated in mid-layers, while token-to-token attention is highest in the top layer. The distribution of attentions varies based on model initialization, with a focus on phrases. For model trained with seed 100, percentage (%) of activations for different attention types in each layer of the Interleaved model for English-to-German translation task in newstest2014."
}