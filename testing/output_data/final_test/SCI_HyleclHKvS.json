{
    "title": "HyleclHKvS",
    "content": "Stochastic gradient descent (SGD) is the go-to optimization algorithm for large-scale machine learning problems, trading off noisy gradient updates for efficiency. Variance reduction algorithms like SVRG aim for faster convergence by introducing control variates. Despite their guarantees, SVRG-like algorithms are not widely used in deep learning. A non-asymptotic analysis of SVRG in noisy least squares regression is presented, focusing on comparing its loss to SGD at each iteration. Our analysis compares the loss of SVRG to SGD in different regimes of neural networks. SVRG outperforms SGD in underparameterized models but is surpassed by SGD in overparameterized models. Large-scale machine learning problems often use SGD due to the intractability of computing exact gradients over the entire training set. The variants of stochastic gradient descent (SGD) evaluate noisy gradient estimates from small mini-batches of randomly sampled training points at each iteration. Despite its simplicity, SGD works well in non-convex non-smooth deep learning problems. However, the optimization performance near local optima is limited by mini-batch sampling noise. Sampling variance and slow convergence of SGD have been extensively studied in the past. Machine learning practitioners adjust mini-batch size or learning rate for convergence. SVRG-like algorithms show limited success in training deep learning models. Traditional stochastic optimization results focus on asymptotic analysis, but deep neural networks are typically trained for hundreds of epochs. To bridge the gap between SVRG benefits and practical computational constraints in training deep learning models, a non-asymptotic study was conducted on SVRG algorithms for noisy least squares regression. The analysis considered both underparameterized and overparameterized regimes, with a focus on optimizing least squares regression to understand learning dynamics in deep learning models. The presence of label noise was accounted for, with the loss being lower bounded by label variance. The main contributions include showing the expected loss of SVRG and SGD during optimization, comparing their computational cost and learning rate schedule, and analyzing their performance with and without label noise. SGD outperforms SVRG under mild computational cost with noisy labels, but always converges faster without label noise. Numerical experiments validate the theoretical findings. Numerical experiments validate theoretical predictions on convergence speed of SGD and SVRG in neural networks. Stochastic variance reduction methods aim to minimize a collection of functions using SGD, with induced noise requiring decaying step sizes for convergence. Methods like SAG, SVRG, and SAGA can recover linear convergence rate. Stochastic variance reduction methods like SAG, SVRG, and SAGA can achieve linear convergence rate with asymptotic cost comparable to SGD. These methods require storing recent gradients or spending extra computation at snapshot intervals. Larger step sizes can be used with stochastic variance reduction methods compared to SGD, leading to faster convergence under certain smoothness conditions. Optimal mini-batch size and step size can be determined based on the smoothness constant of functions. Recent studies have explored applying variance reduction methods in deep learning, with authors suggesting ineffectiveness due to various elements. The authors suggest that the ineffectiveness of variance reduction methods in deep learning may be due to elements like data augmentation, batch normalization, and dropout, which can make gradients stale quickly. They propose removing these elements or updating gradients more frequently. Previous work has shown the dynamics of stochastic gradient descent with momentum on noisy quadratic models, highlighting issues like short horizon bias. This model has been found to capture essential characteristics of neural network training. The authors analyzed the effectiveness of momentum, preconditioning, and learning rate choices in training ResNets and Transformers using a noisy quadratic model. They found that previous quadratic models were inadequate for analyzing stochastic variance reduction methods like SVRG. Instead, they used a noisy least-squares regression formulation considering mini-batch sampling noise and label noise. Their analysis compared SGD to SVRG along the optimization trajectory for any finite-time horizon under limited computation cost. The analysis compared SGD to SVRG along the optimization trajectory for any finite-time horizon under limited computation cost. Overparameterization in deep learning models allows for overfitting but generalizes well when trained using SGD. Different behaviors are observed in underparameterized and overparameterized regimes. Neural networks in the NTK regime achieve global convergence by memorizing every training example. Our proposed noisy least-squares regression analysis captures underparameterization and overparameterization behavior by considering label noise presence. We compare minibatch versions of SGD and SVRG algorithms. SGD update is based on minibatch gradient, training iteration, and learning rate. SVRG algorithm reduces gradient variance caused by minibatch sampling with an inner-outer loop approach. In the noisy least squares regression model, parameters are updated in inner loops based on a large batch gradient evaluation. The reference point is the last iterate of the previous outer loop. The input data is d-dimensional with a linear teacher model generating output labels with additive noise. The goal is to train a student model that minimizes the squared error. The training data is pre-processed to train a student model that minimizes squared loss over the data distribution. The optimizer queries data points to form minibatch gradients using SGD method or large batch gradients using SVRG. The expected loss is a function of the second moment of the iterate, with dynamics derived when \u03a3 is diagonal. Functions and identities are defined, and the SGD update with mini-batch gradient for the noisy least squares model is presented. The SGD update with mini-batch gradient for the noisy least squares model is analyzed to derive dynamics for the second moment of the iterate. The presence of label noise leads to a lower bound on the expected loss, and when the second moment approaches zero, the variance of stochastic gradient also approaches zero. The dynamics and decay rate for SGD are further studied when \u03a3 is diagonal. The dynamics of the second moment of parameters under the SVRG update rule reveal its weakness, showing a decay rate in the iterate's second moment. The weakness of SVRG is illuminated by the decay rate of the second moment of parameters under its update rule. The conflict arises between reducing the main term quickly with a large learning rate and maintaining variance reduction with a small decay rate. The control variate's contribution adds extra variance, worsening the situation. The variance reduction in SVRG comes at the cost of slower gradient descent shrinkage, but it allows for convergence to a lower loss value compared to SGD. This raises the question of which algorithm to use based on computational cost. Research includes numerical simulations and experiments on real datasets to analyze the decay rate of SVRG. The dynamics of SVRG in noisy linear regression show linear convergence without label noise, similar to SGD. This setting falls under the \"interpolation regime\" for linear convergence, with practical implications for training overparameterized neural networks. The convergence rate of SGD and SVRG without label noise was investigated in experiments. SVRG faces a dilemma between variance reduction and gradient descent shrinkage, but converges to a lower loss than SGD. The total computational cost and per-iteration computational cost are defined, and the study compares which algorithm converges faster given a certain total computational cost in both underparameterized and overparameterized regimes. The investigation involved numerical simulations and experiments on real datasets to compare the convergence rates of SGD and SVRG. Different batch sizes were tested, and the performance of each algorithm was summarized by plotting the minimum loss at each time step. The study found that the phenomenon predicted by theory often matched observations in practice. In experiments comparing SGD and SVRG, a trade-off between computational cost and convergence speed was observed for underparameterized neural networks. SVRG outperformed SGD initially, but in overparameterized models resembling modern neural networks, SGD showed faster convergence throughout training. Numerical simulations were conducted with a data distribution of dimension d = 100 and a spectrum of \u03a3 ranging from 1 to 0.01. Learning rates were varied from 1.5 to 0.01, with snapshot intervals for SVRG at {256, 128, 64}. In simulations comparing SGD and SVRG, a trade-off between computational cost and convergence speed was observed. SGD achieved faster convergence initially but converged to a higher loss compared to SVRG. This indicates that one can trade more compute cost for faster convergence speed by choosing SGD over SVRG, and vice versa. In comparing SGD and SVRG, a trade-off between computational cost and convergence speed was observed. Despite different per-iteration costs, the crossing points in the plot occurred at around 5.5 epochs for both methods. In experiments on MNIST and CIFAR-10 datasets, SGD showed faster linear convergence compared to SVRG, demonstrating dominance in this regime. The study included underparameterized and overparameterized settings with different batch sizes and learning rates. In experiments comparing SGD and SVRG on MNIST and CIFAR-10 datasets, SGD showed faster convergence speed than SVRG for all computational costs. Different models were used, including underparameterized logistic regression and MLP for MNIST, and a convolutional neural network for CIFAR-10. The experiments compared SGD and SVRG on MNIST and CIFAR-10 datasets using different models. Results showed that SGD had faster convergence speed than SVRG for all computational costs. The models included underparameterized logistic regression and MLP for MNIST, and a convolutional neural network for CIFAR-10. Only two 8-channel convolutional layers and one 16-channel convolutional layer with an additional fully-connected layer were used. The filter size was 5. The lowest loss achieved over all hyperparameters for these models at each per-iteration computational cost is shown in Figure 3. SGD converged faster in the early phase, resulting in a crossing point between SGD and SVRG, showing a trade-off between computational cost and convergence speed. On CIFAR-10, SGD initially converged faster than SVRG but was surpassed by SVRG around 17-25 epochs, indicating a trade-off between compute and speed. Comparing SGD and SVRG on MNIST and CIFAR-10 with overparameterized models, both achieved close to zero training loss on MNIST. SGD showed an advantage over SVRG, consistent with previous findings on noisy linear regression models. In a study comparing SGD and SVRG in underparameterized and overparameterized settings, SGD outperformed SVRG in terms of convergence rate. The experiments on CIFAR-10 showed a trend towards zero loss for both algorithms, with SGD consistently outperforming SVRG. The research included non-asymptotic analysis and numerical simulations on MNIST and CIFAR-10 datasets. Our experiments on MNIST and CIFAR-10 datasets showed a trade-off between computational cost and convergence speed for underparameterized neural networks. SVRG outperformed SGD initially, but in overparameterized models, SGD consistently showed faster convergence for all computational costs. The conclusion from The Matrix Cookbook states that for any square matrix A, the expected second moment of \u03b8 can be expressed as a function of m(\u03b8). The dynamics of the second moment of the iterate are derived under the assumption of noisy linear regression objective function. The dynamics of the second moment of the iterate following SVRG update rule is analyzed, showing variance reduction from control variate. The expectation of certain equations is derived, leading to the conclusion that the covariance between certain parameters exponentially decays. The dynamics of the second moment of the iterate following SVRG update rule is analyzed for variance reduction from control variate. The formula for gradient descent shrinkage is discussed, along with the conclusion on variance reduction terms. The proof of Theorem 4 is presented for the noisy linear regression objective function under specific assumptions. The analysis focuses on the dynamics of the second moment of the iterate following SVRG update rule for variance reduction. It evaluates the sensitivity of the number of data points used in calculating the gradient to illustrate its impact on convergence speed. The study shows that the number of data points has little effect on SVRG's convergence speed but influences the constant term of label noise. The constant term of label noise in the model is determined by the number of data points used in calculating the gradient, which affects the final loss. Additionally, large batch SGD is compared to SVRG in Figure 5b."
}