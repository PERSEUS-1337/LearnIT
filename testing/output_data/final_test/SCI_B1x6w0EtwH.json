{
    "title": "B1x6w0EtwH",
    "content": "Interactive Fiction games are text-based simulations where an agent interacts through natural language. KG-A2C is an agent that uses a dynamic knowledge graph to explore and generate actions in large text-based action spaces. Results show KG-A2C outperforms current IF agents in various games. Interactive Fiction (IF) games are simulations where an agent interacts through natural language, facing challenges due to partial observability. The agent must generate coherent language actions to progress in complex, interconnected puzzles. Knowledge graphs are used to represent partially observable worlds in Interactive Fiction games, aiding in commonsense reasoning and addressing challenges in natural language generation. In order to solve a popular IF game like Zork1, it's necessary to generate actions of up to five words from a vocabulary of 697 words. By utilizing a knowledge graph-based state space and template-based action space, a novel agent is introduced and trained. An empirical study evaluates the agent across various IF games, showing its effectiveness and generalizability. The agent achieves state-of-the-art performance on a large proportion of games despite the exponential increase in action space size. Prior work includes LSTM-DQN for verb-noun actions, DRRN algorithm for choice-based games, and Word2Vec for extracting affordances in games. Zahavy et al. (2018) and C\u00f4t\u00e9 et al. (2018) introduce methods for reducing action space and generating parser-based games, respectively. Yuan et al. (2019) optimize interfaces for playing IF games and compare agent performance using Jericho tools. Knowledge graphs are useful for various natural language tasks. Knowledge graphs have been utilized in natural language generation and interactive fiction tasks. Researchers have used knowledge graph representations to enhance neural conversational models and story ending prediction models. Ammanabrolu et al. (2019) explore procedural content generation in text-adventure games using knowledge graphs. They also introduce the Knowledge Graph DQN (KG-DQN) approach, where a knowledge graph is used as a state representation for a deep reinforcement learning agent in text-game playing. This work explores transferring control policies in text-games using knowledge graphs to seed an agent with commonsense knowledge and transfer knowledge between games. IF games are represented as partially observable Markov decision processes with challenges of partial observability and combinatorial actions addressed using a knowledge graph-based state. Using a knowledge graph as a state representation, 3-tuples of subject, relation, object are extracted from observations to help the agent map the world and retain learned information. The text discusses building a knowledge graph for games in the Jericho framework, focusing on interactive objects that an agent can directly interact with in the environment. This approach aims to extract information from a diverse set of games in a general manner, relaxing rules from previous studies. The text discusses building a knowledge graph for games in the Jericho framework, focusing on interactive objects that an agent can directly interact with in the environment. The graph includes triples extracted by OpenIE, and templates are used for interpreting player actions in the game. These templates consist of interchangeable verb phrases and prepositional phrases, filled in with words from the game's vocabulary. The Jericho framework provides programmable access to templates and vocabulary words for every IF game. The KG-A2C is an on-policy reinforcement learning agent that combines the knowledge-graph state space with the template action space. Its architecture involves encoding a state representation and decoding an action using an input representation network divided into observation encoder, score encoder, and knowledge graph components. The KG-A2C is an on-policy reinforcement learning agent that combines the knowledge-graph state space with the template action space. It involves encoding a state representation and decoding an action using an input representation network divided into observation encoder, score encoder, and knowledge graph components. The components include room description, game feedback, inventory, previous action, and total score. The observation encoder processes each component using a separate GRU encoder and utilizes subword tokenization with a vocabulary size of 8000. The KG-A2C agent combines knowledge-graph state space with template action space. It uses GRUs to encode components and a linear layer to combine them into the final observation. Graph Attention networks are used to update the knowledge graph, with node features computed using subword embeddings. Self-attention is applied after a linear transformation to all node features. The final knowledge graph embedding vector is computed using attention coefficients after a linear transformation of node features. The state embedding vector includes a binary encoding of the game progress. Action decoding is based on predicting a template and selecting objects. The process involves constructing an action by predicting a template and selecting objects using Decoder GRUs. This results in a template policy and a policy for each object. The architecture includes a living room with various objects and a knowledge graph building process for action decoding in Zork1. The action is encoded and passed through an attention layer, conditioning every predicted object on all previously predicted objects and template. A graph mask is introduced to streamline the object decoding process by restricting the objects predicted to those in the knowledge graph. This allows for more efficient exploration of the action space in an IF game. To address cases where the agent encounters new objects, objects are randomly added to the action space with a certain probability. The Advantage Actor Critic method is adapted for training the network, with significant changes made. Valid actions are crucial as the template-action space contains millions of possible actions, but only a few are meaningful and cause changes in the game world. This approach helps in navigating the complex action space efficiently. The concept of valid actions is used to navigate the large combinatorial action space effectively. Valid actions are recognized through game feedback and a detection algorithm. Sets of valid templates and objects are defined to aid in action decoding. Cross-entropy loss terms are applied to the decoder GRU to improve the action decoding process. The decoder GRU T and O calculate object loss and cross-entropy loss. A2C training involves updating the actor and critic based on action advantage and reward. The critic's prediction is updated to reflect the true value of a state. Entropy loss is added to prevent the agent from taking invalid actions. The experiments include an entropy loss over valid actions to prevent premature convergence. Ablations test algorithm effectiveness, followed by testing on Jericho games with handicaps. Hyperparameters are in Appendix B. Ablation study on Zork1 is conducted to assess algorithm performance. Zork1 is an early IF game where players explore a labyrinth, fight enemies, and solve puzzles to collect treasures. KG-A2C's architecture components are analyzed by removing knowledge graph, template-action space, and valid-action loss. LSTM-A2C removes the knowledge graph, changes state embedding vector computation, and eliminates graph mask for action decoding. KG-A2C-seq introduces interactive objects for object masking, replaces template actions with word-by-word action decoding, and utilizes teacher-forcing for effective exploration. KG-A2C-unsupervised explores the template action space manually without access to valid actions, leading to a different training approach. Template DQN is also mentioned in the context of the discussion. Template DQN Baseline extends LSTM-DQN to template-based action spaces with three output heads for estimating Q-Values. Human players familiar with IF games were asked to play Zork1, with half reaching a game score of around 40 before dying to a troll due to neglecting to collect a weapon. The remaining players in Zork1 faced hidden traps, with scores ranging from 5 to 15. The final two players surpassed the troll, achieving scores around 70. The full KG-A2C outperformed all baselines and ablations on Zork1, indicating the importance of all algorithm components. The first two rewards in the game are 5 and 10, reachable in 4 steps with an optimal policy. The LSTM-A2C and KG-A2C-seq showed similar progress. The LSTM-A2C and KG-A2C-seq agents show different performance in collecting rewards, with the KG-A2C-seq performing worse due to its action space. The LSTM-A2C-masked progresses further in the game but lags behind KG-A2C likely due to the lack of a graph component in its state embedding. LSTM-A2C and TDQN struggle to progress without a knowledge graph. The KG-A2C agent relies on templates and a knowledge graph to achieve state-of-the-art performance in natural language generation. The valid action supervised loss is not as crucial as the choice of state and action spaces. KG-A2C demonstrates the feasibility of scaling reinforcement learning for exploring goal-driven, contextually aware language generation. KG-A2C is a learning agent that scales reinforcement learning for natural language actions with a knowledge graph-based state space and template-based action space. It efficiently explores large action spaces and outperforms existing reinforcement learning agents. Our agent, utilizing a combined state-action space, shows significant improvement over the current state-of-the-art template-based agent in a diverse set of 26 human-made IF games. Interactive objects are identified through part-of-speech tagging and navigational actions help infer relative positions of rooms. The agent infers room positions by selecting verbs and prepositions from templates based on human playthrough data. Agents are trained individually on each game and evaluated based on valid actions within 100 steps. The agent infers room positions by selecting verbs and prepositions from templates based on human playthrough data. Agents are trained individually on each game using data from 32 parallel environments. TDQN was trained on a single environment with hyperparameters tuned on the game of Zork1. Final reported scores are an average over 5 runs of each algorithm. Interactive objects include a tree, path, branches, and forest. Feedback: Up a Tree, 10 feet above ground in large branches, birds nest with precious jeweled egg, fragile and ornate with gold inlay, lapis lazuli, and mother of pearl, hinged and closed with delicate clasp."
}