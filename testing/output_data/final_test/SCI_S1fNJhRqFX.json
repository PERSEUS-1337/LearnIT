{
    "title": "S1fNJhRqFX",
    "content": "The relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration is established. In this paper, the density of the Q function estimated by Distributional RL is used for UCB estimation without requiring counting, making it suitable for Deep RL. The asymmetry of empirical densities estimated by algorithms like QR-DQN is highlighted, leading to the introduction of truncated variance as an alternative UCB estimator. A novel algorithm based on this approach shows improved performance in multi-armed bandits settings and Atari 2600 games compared to QR-DQN. In Reinforcement Learning, the multi-armed bandit problem is a long-standing issue. While traditional solutions work well for simpler settings, they are not practical for high-dimensional Deep RL due to the presence of function approximators. The gambler's goal in a multi-armed bandit scenario is to maximize cumulative reward by pulling arms with unknown expected rewards. One approach is to initialize estimated means optimistically and update them based on observed rewards. This iterative process helps identify the arm with the highest true mean over time. In the context of Reinforcement Learning, the multi-armed bandit problem involves maximizing cumulative reward by pulling arms with unknown expected rewards. One approach is to initialize estimated means optimistically and update them based on observed rewards, eventually discovering the best arm. Upper Confidence Bound (UCB) algorithms, like UCB-1, operate on the principle of 'optimism in the face of uncertainty', pulling the arm with the highest upper confidence bound in hopes of a better mean. Estimation of the arm's UCB is based on the number of times it was pulled, extending to tree search cases with UCT. The exploration ideas from multi-armed bandits do not easily generalize to Deep RL due to the requirement of counting state-action pairs. UCB-V, a variation introduced by BID0, estimates UCB using empirical variance, which also involves counting. The annealed epsilon greedy approach, popularized by BID13, is a common exploration approach in Deep RL but is not very efficient as it does not consider the environment's structure. Researchers are seeking more effective exploration strategies for Deep RL. Researchers are exploring more efficient ways of exploration in Deep RL, such as parametric noise and posterior sampling. Uncertainty Bellman Equation and UCB type approaches have also been developed. A new approach in estimating empirical distributions of Q function, distributional RL, shows promising results. Distributional RL in Deep RL environments like Atari 2600 achieves state-of-the-art performance. It uses non-parametric methods like C51 and QR-DQN, which do not assume a specific distribution shape. The issue of asymmetric distributions affecting UCB estimation sensitivity is overlooked in existing literature. Empirical evidence in Section 4 shows that symmetry is rare in Distributional RL. In Section 4, empirical evidence shows that symmetry is rare in Distributional RL. The paper extends the UCB approach to asymmetric distributions and high-dimensional settings, introducing a truncated variability measure that outperforms variance in bandit settings. The extension to visual environments like Atari 2600 is based on recent advances in Distributional RL. The statistical relation between the number of observations of a random variable and the tightness of mean estimates is formalized through Hoeffding's Inequality. Theorem 1 quantifies the relation between upper confidence bound for X and the number of realizations of X. UCB can be estimated directly if the estimate of the probability density function (PDF) is available. Bayes-UCB uses a restricted family of distributions for closed form Bayesian update, while Neural Networks can model P[X] in a more expressive way. Quantile Regression approach towards Distributional RL is explored in this paper, introducing QR-DQN based on Quantile Regression. The core idea behind QR-DQN is Quantile Regression introduced in supervised machine learning. It defines the \u03c4-th linear quantile regression loss as a weighted sum of residuals, with weights based on the counts of residual signs and order of the estimated quantile \u03c4. Dabney et al. extended QR to RL with a neural network design similar to DQN, but with the last linear layer outputting N quantiles {\u03b8 i } i instead of a single Q estimate. Bellman update for a transition (x, a, r, x') and discount factor \u03b3 is also discussed. The QR-DQN approach in DeepRL uses Quantile Regression for distribution estimation, eliminating the need for state-action pair counting. This method is more elegant than previous approaches, such as using Boltzmann distribution, and does not require explicit bounding of the support. The function approximator in this case is crucial for generalization in DeepRL. The QR approach allows for a refined estimation of distributions in RL settings, applied to multi-armed bandits for easier exploration. In the Deep RL setting, the QR-DQN approach utilizes Quantile Regression for distribution estimation, eliminating the need for state-action pair counting. This method allows for a refined estimation of distributions in RL settings, applied to multi-armed bandits for easier exploration. The proposed algorithm QUCB estimates the empirical distribution of returns for each arm using QR, selecting the arm with the highest mean plus standard deviation. Algorithm 2 outlines the QUCB method for estimating empirical quantiles in multi-armed bandit problems. The algorithm utilizes Quantile Regression to estimate distributions and allows for the selection of arms based on mean and standard deviation. The presence of the multiplier c t in the algorithm is crucial for optimality in the long run. Additionally, warm-up steps of pure exploration may be beneficial when the number of quantiles is large compared to the sample size. This approach opens up new possibilities for computing upper confidence bounds and exploration bonuses. The asymmetry of empirical distributions in multi-armed bandit problems is a regular occurrence, not an exception. The question arises whether variance is a reliable measure for upper confidence bounds in asymmetric distributions. In the end of training, the agent achieves near-perfect scores in the game of Pong from Atari 2600, leading to non-symmetric distributions. In the context of multi-armed bandits and RL, the use of confidence bounds for asymmetric distributions is discussed. The focus is on the variance decomposition into lower and upper variability measures, with an emphasis on the relevance of upper tail variability in the UCB approach. The availability of empirical PDF allows for truncating the variance in different ways, with \u03c32u being a potential candidate despite potential drawbacks in estimating the mean. The proposed truncated measure of variability based on the median, \u03c32+, is suggested as a more robust upper tail variability measure in the context of multi-armed bandits and RL. Empirical results support this hypothesis, with potential applications in tabular RL and Deep RL settings. The algorithm QUCB+ is a modification of QUCB that incorporates \u03c32+ instead of V ar(\u03b8 t,k ). The update step remains unchanged in Algorithm 3 DQN-QUCB+. The action selection step includes bias in the form of \u03c32+ from Equation 10. The variance in \u03b8 i is influenced by both intrinsic and parametric variations in QR-DQN. The parametric uncertainty decreases during training, as shown in FIG1, indicating a reduction in variance as the network approaches the optimal solution. Following BID18, QUCB and QUCB+ were applied to a multi-armed bandits test bed with normally and asymmetrically distributed rewards. Two configurations were set up, each with 10 arms drawn from a normal distribution with mean \u03bc=1 and \u03c3=1. There was no significant difference between QUCB and QUCB+ in the normally distributed rewards configuration. In the second configuration, rewards are drawn from lognormal distributions with slight asymmetry, leading to better performance of QUCB+. The Deep RL setting was tested using QR-DQN architecture with Huber loss for stability and computational efficiency. Smoothness of Huber loss is preferred for gradient descent. The Deep RL setting tested QR-DQN with Huber loss for stability and efficiency. Smoothness of Huber loss is preferred for gradient descent. Hyperparameters were set closely, except for the learning rate of Adam optimizer. DQN-QUCB explores using \u03c3 2 + term without epsilon greedy schedule. Schedule {c t } t for \u03c3 2 + depends on problem type. In experiments, a schedule was used to gradually vanish exploration. The schedule aims to gradually reduce exploration in DQN-QUCB, similar to QR-DQN. Performance comparison is more accurate this way. DQN-QUCB's performance is sensitive to the schedule, with potential for better results if tuned. Evaluation on 49 Atari games showed DQN-QUCB outperforming in 26 games by over 3% in cumulative reward. This measure is deemed suitable as learning curves show no drastic drops. Recent advancements in RL, like Distributional RL, have introduced new principles. The Distributional RL has achieved state-of-the-art performance in high dimensional environments like Atari 2600. The empirical PDF for the Q function is a by-product, used for mean computation. UCB is an exploration algorithm in multi-armed bandits but doesn't generalize well to Deep RL. This paper connects the UCB idea to Distributional RL and introduces a truncated variability measure as an alternative to variance, showing success in multi-armed bandits and Atari 2600 environments. DQN-QUCB+ could be enhanced through schedule tuning and combined with other advancements in Deep RL, such as Rainbow by BID6, to improve results in visual environments like Atari 2600."
}