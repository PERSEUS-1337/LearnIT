{
    "title": "SJx94o0qYX",
    "content": "Quantization of neural networks faces accumulated quantization error, hindering ultra-low precision. To address this, precision highway enables high-precision information flow during low-precision computation. It reduces quantization error in convolutional and recurrent networks, with superior performance compared to existing methods in 3-bit quantization with no accuracy loss and 2-bit quantization with minimal loss in ResNet-50 and LSTM models. The method proposed significantly outperforms existing methods in 2-bit quantization of an LSTM for language modeling. Energy-efficient inference of neural networks is crucial for servers and mobile devices. Existing quantization methods suffer from accumulated quantization error, hindering ultra-low precision. To address this, a novel concept called precision highway enables high-precision information flow, reducing quantization error and enabling ultra-low precision in deep neural networks. Our proposed method, precision highway, reduces quantization error and enables ultra-low-precision computation in deep neural networks. It offers a generalized concept of high-precision information flow that can be applied to various types of networks. Our contributions include a network-level approach to quantization and a detailed analysis of the benefits in terms of error propagation and convergence difficulty during training. The precision highway method reduces quantization error in deep neural networks, enabling ultra-low-precision computation. It offers significant accuracy improvements with negligible overhead. The method is applied to convolution and recurrent networks, achieving 3-bit quantization for ResNet-50 without accuracy loss and 2-bit quantization with minimal loss. Additionally, sub 4-bit quantization results are provided for LSTM language modeling. Related work includes int8 quantization methods and integer-arithmetic only matrix multiplications for neural networks. Recent advancements in quantization methods for neural networks have shown promising results. BID9 introduced a binarization method for GPUs, while BID21 proposed XNOR-Net, a binary network with comparable accuracy to full-precision models. BID25 presented DoReFa-Net, utilizing tanh-based weight quantization. BID26 introduced balanced quantization to evenly distribute values on quantization levels. BID7 suggested using full precision for LSTM cell states due to their wide value distributions. In comparison, our approach focuses on an end-to-end flow of high-precision activation in the network. Additionally, BID28 demonstrated 4-bit quantization with ResNet-50 using Dorefa-net style weight quantization. Recent advancements in quantization methods for neural networks have shown promising results. BID28 demonstrated 4-bit quantization with ResNet-50 using Dorefa-net style weight quantization. Our proposed idea offers a network-level solution and emphasizes the importance of the end-to-end flow of high-precision information. Additionally, our method is versatile and can be applied to various types of neural networks, not limited to pre-activation residual networks. In precision highway, a path is built from input to output of a network to enable high-precision activation flow while performing low-precision computation. The method focuses on improving quantized network accuracy by providing end-to-end high-precision information flow. It is inspired by residual and LSTM networks, with sections discussing application to other networks. In precision highway, a high-precision skip connection is formed in a residual network to reduce quantization error. This involves applying k-bit linear quantization to activations, with high-precision and low-precision paths shown in a diagram. The quantized input with error enters both the skip connection and residual path, with the output calculated using a residual function. In precision highway, a high-precision skip connection is introduced in a residual network to minimize quantization error. The output of the residual block is determined by a residual function, with quantization errors in the residual path and skip connection accounted for. The proposed idea involves applying quantization only to the residual path after the bifurcation, resulting in a high-precision path for the skip connection. The proposed idea eliminates quantization error in the skip connection, keeping high-precision activation for element-wise addition. Low-precision activation is used for computation in the residual path, reducing computation and memory costs. This method results in smaller quantization error compared to traditional methods. Our proposed method reduces quantization error and offers better accuracy with ultra-low precision. It can be applied to various types of residual blocks and is advantageous for hardware accelerators designed for non-negative input activations. The precision highway construction on LSTM BID4 involves input gates, forget gates, gate gates, and output gates to calculate cell state and output. Quantization is applied to activations before computation, reducing quantization errors for better accuracy with ultra-low precision. The precision highway in the LSTM cell reduces quantization errors by applying quantization only to specific inputs during matrix multiplication, allowing for ultra-low precision without accumulated errors across time steps. The precision highway in LSTM cells reduces quantization errors by utilizing high-precision inputs for computing cell states and outputs, allowing for low-precision matrix multiplications to dominate the total computation cost. This method can also be applied to other types of recurrent neural networks, such as GRU BID3. The precision highway concept can be applied to various networks by using high-precision inputs for key computations and low-precision operations for matrix multiplications. For feed-forward networks with identity paths, quantization can be applied before matrix multiplications while maintaining accuracy with high precision. Non-residual feed-forward networks can be equipped with skip connections to create a precision highway. In networks like DenseNet with multiple parallel skip connections, the precision highway can be implemented effectively. In this section, weight quantization and fine-tuning for weight/activation quantization are described. A Laplace distribution is proposed to model weight distribution in full-precision trained networks. Quantization levels for weights are selected based on this distribution to achieve a target precision, such as 2 bits. Pre-computed quantization levels for the normalized Laplace distribution are used to minimize L2 error, ensuring accurate quantization. Our proposed weight quantization method utilizes a Laplace distribution to determine quantization levels for weights, achieving high precision with only the use of this model. The quantization is applied during fine-tuning after training a full-precision network, showing outstanding results compared to existing methods. The fine-tuning procedure in BID28 involves incremental quantization of activations and weights, with teacher-student training using a deeper full-precision network as the teacher. Quantization is applied during forward pass while updating full-precision weights during backward pass. The method was implemented in PyTorch and Caffe2, using ResNet-18/50 for ImageNet and an LSTM for language modeling. Evaluation was done for 4-, 3-, and 2-bit quantizations, comparing with state-of-the-art methods BID25, BID28, and BID2. The proposed method is compared with state-of-the-art methods BID25, BID28, and BID2. Teacher-student training is conducted using the same teacher network for both the baseline method and the proposed method. The effects of increasing the number of channels to recover from accuracy loss due to quantization are evaluated. Quantization is not applied to the first and last layers, and the LSTM has 2 layers with 300 cells each. The Penn Treebank dataset is used for evaluation, comparing perplexity per word with the state-of-the-art method in BID7. Quantization errors across layers in ResNet-50 are shown when applying 4-bit quantization to activations. Two activation-quantized networks are prepared from the same initial condition, one with precision highway and the other with low precision skip connection. In the existing method, quantization errors increase for deeper layers due to accumulated errors. More aggressive quantization leads to larger errors, causing a 4.8% drop in accuracy for ImageNet classification. This accumulation of errors is a characteristic of quantized networks in both feed-forward and feed-back networks. Our proposed precision highway reduces quantization errors in recurrent neural networks, enabling 3-bit quantization without accuracy drop and better accuracy in 2-bit quantization. The complexity of loss surface is visualized in Figure 5, showing the benefits of precision highway in achieving lower and smoother surfaces near the minimum point. The proposed precision highway reduces quantization errors in neural networks, enabling better accuracy in 2-bit quantization. Compared to existing methods, it offers a smoother loss surface, aiding stochastic gradient descent to converge quickly to a good local minimum. The 2-bit quantization achieves a top-1 accuracy of 73.55%, within 2.45% of full-precision accuracy and outperforming state-of-the-art methods. The proposed precision highway improves accuracy in 2-bit quantization, achieving a top-1 accuracy of 73.55%, within 2.45% of full-precision accuracy. It outperforms state-of-the-art methods, showing comparable results to other methods like PACT and Bi-Real. The method is applicable to both recurrent and feed forward networks. The precision highway method improves accuracy in 2-bit quantization, with a top-1 accuracy of 73.55%, close to full-precision accuracy. Results show that 2-bit quantization with an 8-bit highway only has minimal drops in accuracy for ResNet-18 and ResNet-50. Additionally, 3-bit quantization with an 8-bit highway achieves the same accuracy as the full-precision network in ResNet-50. Doubling the number of channels and quantizing them under 2-bit quantization also shows positive effects. The wide ResNets show better accuracy than full-precision ones even with 2-bit quantization. Future work could focus on minimizing channel size while maintaining full-precision accuracy with ultra-low precision. Results also demonstrate the chip area cost and energy consumption of ResNet-18 at different precision levels on a hardware accelerator. The reduced precision in chip design offers significant reductions in chip area and energy consumption. Precision highway incurs minimal additional energy consumption while providing better accuracy. The accelerator is equipped with a large internal buffer for partial sum accumulation, with additional energy consumption mainly on accesses to on-chip SRAM and main memory. TAB8 compares the number of operations in neural networks, showing why high-precision operations have a small overhead in energy consumption. The concept of end-to-end precision highway is proposed to enable ultra-low precision in deep neural networks, reducing quantization errors with minimal energy consumption. It keeps high-precision activations from input to output, reducing accumulated quantization errors. Experimental results show outperformance of state-of-the-art methods in 3-and 2-bit quantizations of ResNet-18/50. Our work outperforms state-of-the-art methods in 3-and 2-bit quantizations of ResNet-18/50 and an LSTM model, paving the way for mixed precision networks for improved computational efficiency."
}