{
    "title": "rkxyny2qaE",
    "content": "NetDissect method identifies interpretable features of a model using the Broden dataset of visual semantic labels. The Broden dataset is proposed to be extended to include actions for better analysis of learned action models. The annotation process and results from interpreting action recognition models on the extended dataset are discussed to understand the conceptual hierarchy used for classification. Deep convolutional neural networks are successful due to their ability to learn hidden representations capturing important factors of variation in data. In this paper, the visualizations of deep convolutional networks show that individual features act as visual concept detectors, detecting patterns and meaningful concepts. NetDissect uses the Broden dataset to evaluate units, but lacks the ability to detect learned action concepts. The proposal is to extend the Broden dataset to include actions for better analysis of learned action models. The Broden dataset is being extended to include actions for better analysis of action recognition networks. The annotation process involves collecting images and selecting sample videos to show action variation and spatial localization. The goal is to identify interpretable action features in deep networks trained for action recognition. The Action Region dataset will be available online for integration with NetDissect. We extend the Broden dataset by adding actions through an image segmentation dataset. Bounding box annotations for actions in images are collected via Amazon Mechanical Turk. A single frame from 500 videos per 339 action classes is used for a binary annotation task to verify action visibility. This process is repeated for at least 2 rounds to ensure accuracy. The dataset is extended by adding actions through image segmentation. Verified action-frame pairs are annotated on AMT for workers to select important regions in the image. Selected regions are cropped and verified for the presence of actions. The final set consists of 23,244 images from 210 classes. The action regions dataset consists of 23,244 images from 210 classes. Selected regions are considered as masks on the segmented area of the image related to the action. This extends the Broden dataset to include action segmentations for interpreting action features via NetDissect. This process allows for the identification of actions learned by models. The unit interpretability of a network is scored following a similar procedure as outlined in previous work. In analyzing features from a ResNet50 network trained on the Moments in Time dataset, 141 action concepts were identified in the final convolutional layer. The network recognized patterns such as a person standing behind a podium as preaching and crawling with babies. These findings reveal data and class biases in the dataset. The analysis of features from a ResNet50 network trained on the Moments in Time dataset revealed 141 action concepts in the final convolutional layer. The network identified patterns like preaching and crawling with babies, exposing data and class biases. By including action concepts, features previously interpreted as object or texture concepts were found to be closely aligned with actions, such as unit 13 being associated with \"gardening\" and unit 290 with \"typing.\" This highlights the importance of network interpretation in identifying hidden biases. The ResNet50 network trained on the Moments in Time dataset revealed action features mainly in the last convolutional block. Object and scene features were also learned, aiding action classification. Understanding these interpretable features helps in predicting model behavior with different data inputs. By examining interpretable features in the final residual block of the network, we can understand how the model identifies actions related to specific scenes, such as \"highway\" features. This analysis helps in diagnosing the decision-making process of the network. When analyzing interpretable features in the final block of the model, we can identify why the network makes mistakes in its output. For instance, when passing an image through the model, it incorrectly predicts \"juggling\". By setting specific features to 1 and others to 0, we found that the top features contributing to this mistake were also interpreted as \"juggling\" features. These features did not correlate significantly with other concepts in the dataset, prompting further analysis. After analyzing interpretable features in the final block of the model, it was found that the top features contributing to the misclassification of \"juggling\" were the scene \"ball pit\", textures \"polka dotted\" and \"dotted\", and the color \"white\". Class Activation Mapping (CAM) BID6 confirmed that the area around the red and white polka-dotted purse in the image was of interest to the model, revealing the hierarchical concept path that led to the mistake."
}