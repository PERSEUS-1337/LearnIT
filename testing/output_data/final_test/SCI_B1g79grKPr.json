{
    "title": "B1g79grKPr",
    "content": "Many processes can be represented as a sequence of events from a starting state to an end state. A new class of visual generative models, goal-conditioned predictors (GCP), treat videos as start-goal transformations, making video generation easier by using context from the first and final frames. GCP models can synthesize better and longer videos by utilizing non-linear structures and hierarchical prediction. Tree-structured GCP models generate high-quality video sequences over long horizons, making training and inference efficient. These models are easier to parallelize than auto-regressive GCPs and can handle sequences thousands of frames long. GCP approaches are useful for imitation learning without expert actions. Videos can be found on the supplementary website: https://sites.google.com/view/video-gcp. The text discusses the challenges of forward prediction tasks, especially in high-dimensional settings like video synthesis. It highlights the difficulties in making meaningful predictions due to high uncertainties that compound over time, making it impossible to predict accurately after only a few stages of iterative forward prediction. The text proposes using goal-conditioned predictors (GCP) for video synthesis, which reverse-engineer the full video based on the start and goal frames. By conditioning on the goal frame, new types of GCP models can efficiently leverage the hierarchical structure in event sequences. This approach is similar to coarse-to-fine image synthesis, where a temporally downsampled video is synthesized iteratively with sequences of keyframes. The text discusses the use of goal-conditioned predictors (GCP) for video generation, showcasing their ability to produce longer and higher-quality videos compared to standard models. Tree-structured GCPs are highlighted for their parallelizability, enabling fast training and inference on videos with thousands of frames. Additionally, GCPs are shown to support prediction-based control in imitation learning scenarios without access to demonstrator actions, synthesizing visual plans from start and goal images. The text proposes a new model for video generation that conditions the prediction on both the first and last frame of the desired video, aiming to address the challenge of synthesizing longer videos. This approach differs from existing methods that either generate the entire video from scratch or are conditioned on the beginning of the video. The text introduces a novel goal-conditioned video prediction technique that can generate longer video sequences, contrasting with existing short-term interpolation methods based on optical flow. The proposed model conditions the prediction on both the first and last frames of the desired video, enabling the generation of videos up to a minute in length. The approach aims to address the challenge of synthesizing longer videos by leveraging convolutional neural networks for flow field prediction. Methods for solving visual imitation learning problem involve learning reward functions from demonstrations to train reinforcement learning agents, suffering from high sample complexity and exploration issues. A proposed goal-conditioned imitation framework leverages expert demonstrations to overcome exploration problems without the need for reinforcement learning. Visual goal-directed behavior can also be achieved through visual planning methods like visual foresight or Causal InfoGan. Unlike other methods, the proposed approach aims to imitate the expert rather than compute plans internally, making it suitable for longer control horizons. In this section, the goal-condition prediction problem is formalized, and various models are proposed for goal-conditioned prediction. These models include auto-regressive and tree structured models that aim to produce intermediate observations given the first and last elements in a sequence. The goal is to model p(o2, o3, ...oT-1 | o1, oT) instead of the standard forward prediction approach. The goal-conditioned predictors proposed operate in learned compact state spaces for scalability and accuracy. A simple auto-regressive model for goal-conditioned prediction is presented, predicting latent state representations sequentially from start to end. The model's computational complexity scales with sequence length and does not account for hierarchical structure. The hierarchical structure of natural video is not accounted for in the current approach of goal-conditioned prediction. To address this, a tree-structured GCP model is designed, where an intermediate state prediction operator generates states halfway between parent states. This recursive process fills in temporal details until the full video is produced. The GCP-tree model fills in temporal details in a tree-like structure, factorizing the video generation problem for computational efficiency. By batch processing branches in parallel, it reduces sequential operations to O(log T) for synthesizing a video sequence. The GCP-tree model optimizes computation on GPUs through parallelization, with diminishing benefits for large batch sizes. GCP-sequential can handle larger batches than GCP-Tree without significant runtime increase. Adaptive binding allows the intermediate frame predictor to select hierarchical structures in video sequences. The GCP-tree model allows the intermediate frame predictor to select states between parents for prediction. Time steps are represented by a discrete latent variable w, and stochasticity is modeled with a per-frame latent variable z. Inference for w is done efficiently using dynamic programming. Training involves maximizing a lower bound on the likelihood of the sequence through variational inference. The probabilistic framework of goal-conditioned prediction introduces graphical models implemented as deep neural networks to predict high-dimensional sequences of observations. The predictive models consist of a prior, a deterministic recurrent predictor, and a decoding distribution, with an amortized variational inference network used for parameterizing distributions. The probabilistic framework of goal-conditioned prediction utilizes deep neural networks to predict sequences of observations. The model includes a diagonal covariance Gaussian prior, a LSTM recurrent predictor, and a unit variance Gaussian decoding distribution. The posterior distributions are computed using an attention mechanism over embeddings of the evidence sequence. The hierarchical predictor is improved by using TreeLSTM as the backbone. The hierarchical predictor, utilizing TreeLSTM as the backbone, significantly improved performance by capturing long-term temporal dependencies. Skip-connections from the encoder of the first and last frame were used to enhance visual quality. A GCP trained on expert optimal behavior can generate predicted observations for imitation learning without annotated actions. The GCP model can generate predicted observations for imitation learning without annotated actions. A separate inverse model is trained to infer the actions needed to execute the plan, estimating the probability of actions leading from one observation to another. This model can be trained on any dataset with actions, including random behavior, simplifying the action selection process. The GCP model is trained on expert data without access to expert actions. An inverse model is trained in a self-supervised manner on trajectories from a random controller. The aim of the experiments is to study goal-conditioned models' ability to generate long-horizon video sequences and whether tree-structured prediction improves efficiency for long-horizon video prediction. Datasets evaluated include Human 3.6M dataset, pick&place dataset with a simulated Sawyer robot arm, and Maze dataset in a Gym-Miniworld simulator environment. The maze dataset is constructed with a single path between every pair of rooms, allowing for scalability to long-horizon tasks. Evaluation is done on 3\u00d73 and 10\u00d710 room layouts with different frame sequences. Comparison is made with GCP-sequential, GCP-tree, DVF, and CIGAN methods using PSNR and SSIM metrics. The proposed goal-conditioned prediction models outperform the video interpolation baseline significantly. Interpolation methods struggle to capture long-term dynamics, while GCP-sequential and GCP-tree successfully predict rich scene dynamics between distant frames. This is attributed to the more powerful stochastic latent variable model used in the methods. Comparisons are also made with Stochastic Video Generation (SVG) approach. Incorporating goal information improves prediction accuracy compared to prior approaches. Evaluations on perceptual metrics show higher accuracy with goal conditioning. Ablation studies highlight the importance of skip connections and recurrence for high prediction performance. The GCP models outperform DVF in capturing human trajectory and complex motion for long-term goal-conditioned prediction. GCPTree's hierarchical prediction approach allows for heavy parallelization of computation, reducing sequential computational complexity. Comparing training iteration times between forward and tree-structured prediction validates the runtime benefits of GCPTree. The GCP-Tree model shows lower iteration times for long sequences, enabling more efficient training. Adaptive binding helps the model learn dataset structure by binding nodes to easily predictable frames. The experimentally tested adaptive binding did not significantly improve prediction accuracy, but it may help identify bottlenecks. Goal-conditioned visual prediction was evaluated for imitating expert behavior without access to their actions, using a dataset of 30k expert trajectories. The agent successfully followed plans generated by the GCP-sequential model through 5 rooms, with re-planning every 5 steps. The full trajectory has a length of 200 steps in the 3\u00d73 Maze environment. An inverse model is trained using 20k trajectories of 15 frames each for visual imitation learning. The adaptive GCP-tree predicts a semantic bottleneck where the agent drops the object in the bin. Performance is evaluated by measuring the difference between initial and final distance to the goal after the episode finishes. The study compares GCP-sequential to alternative visual planning methods like CIGAN. While behavior cloning requires ground truth action annotations, GCP-sequential outperforms other methods in long-horizon tasks by generating coherent plans and completing more test tasks. Goal-conditioned predictors (GCPs) are predictive models that generate video sequences between start and goal frames, requiring an understanding of the environment mechanics for accurate prediction. They outperform behavior cloning in long-horizon tasks by creating coherent plans and completing more test tasks. GCP models allow for more accurate video prediction and novel model architectures, including tree-structured models. In addition to conventional auto-regressive GCPs, tree-structured GCP models predict video sequences hierarchically, showing improved accuracy. They can be used in imitation learning scenarios, learning behaviors from video demonstrations without example actions. This framework could be valuable for robotic control leveraging data effectively. For the Human 3.6 dataset, videos are downsampled to 64x64 resolution and cropped to 500-frame sequences. The dataset is split into training, validation, and test sets. The TAP dataset uses a convolutional encoder and decoder with 32-dimensional latent variables. An inference network with attention utilizes a 2-layer 1D temporal convolutional network for adding temporal context to latent vectors before attention. The text discusses the importance of adding temporal context to latent vectors before attention in a recursive predictor model. It highlights the use of hyperbolic tangent activation for stability during training and the preference for group normalization over batch normalization. Hyperparameters were manually adjusted for each method and dataset, with a focus on the hyperparameter \u03b2. The convolutional encoder and decoder utilize batch normalization, while local per-image batch statistics are used at test time. To infer q(w|x, z), a distribution over possible alignments between the tree and evidence sequence is produced using Dynamic Time Warping. The energy of an alignment matrix is defined as the cost, and a distribution over alignment matrices is computed. Marginal edge distributions are then calculated to determine the reconstruction error. The reconstruction error can be efficiently computed by determining marginal edge distributions. Cuturi & Blondel (2017) demonstrated an efficient method to compute these distributions using the partition function of aligning subsequences. The total unnormalized density of alignment matrices, including edge (i, j), can be calculated, enabling the computation of the probability of edge (i, j) as well. This allows for the expected reconstruction cost to be computed in quadratic time."
}