{
    "title": "BJlo91BYPr",
    "content": "Specifying reward functions is difficult, motivating the area of reward inference: learning rewards from human behavior. Human behavior is often irrational due to various factors like noise, myopia, and risk aversion. Surprisingly, irrationality can actually aid reward inference by producing more varied policies that help differentiate between different reward parameters. The text discusses the impact of irrationality on reward inference in reinforcement learning. It explores deviations from the Bellman update, measures accuracy of inference, and analyzes gains and losses. The need to accurately model irrationality and train individuals to exhibit helpful irrationalities is also highlighted. The text discusses reward inference in reinforcement learning, where real-world tasks lack explicit reward functions. Reward inference involves estimating a reward function from various inputs like demonstrations, comparisons, natural language instructions, physical corrections, proxy rewards, or scalar reward values. The text discusses reward inference in reinforcement learning, where tasks lack explicit reward functions. Research has uncovered various irrationalities in human decision-making, leading to deviations from optimal behavior. Reward inference models often incorporate decision-making noise, with the Bolzmann distribution being widely used. The text explores how irrationalities in decision-making, such as risk aversion and wrong beliefs, can impact reward inference in reinforcement learning. Despite initial expectations, irrationality can actually aid reward inference by influencing the mutual information between expert policies. The text discusses how irrationalities, like risk aversion and wrong beliefs, can affect reward inference in reinforcement learning. It highlights the importance of considering different biases in reward inference and the impact of irrational behavior on distinguishing between reward parameters. The analysis provides insights into the role of noise and optimism in influencing expert behavior towards their goals. The text discusses how irrationalities, like risk aversion and wrong beliefs, can affect reward inference in reinforcement learning. It emphasizes the importance of considering different biases in reward inference and the impact of irrational behavior on distinguishing between reward parameters. The analysis provides insights into the role of noise and optimism in influencing expert behavior towards their goals. The results suggest that irrationalities can be beneficial for inference, depending on the type and amount. This opens the door to a better understanding of reward inference and practical ways to make inference easier by seeking expert demonstrations that align with the right kind of irrationality. The study explores the effect of irrationalities on reward inference when learners accurately model them. The text discusses modeling irrationalities in reward inference in reinforcement learning. It highlights the challenges of recruiting human subjects with different irrationalities and measuring accuracy without ground truth access to desired rewards. To address this, expert behavior is simulated based on ground truth reward functions to measure performance against the ground truth. The text discusses modeling various irrationalities in reward inference in reinforcement learning by manipulating the components of the Bellman update. Different types of irrational behaviors are created by changing the discount factor or transition function. This approach aims to cover a wide range of irrationalities in decision-making processes. The text explores modeling irrationalities in reward inference in reinforcement learning by altering components of the Bellman update. Changes include using a softmax for noise capture, adjusting transition functions for optimism/pessimism, and modifying reward values based on prospect theory. Boltzmann-rationality introduces a parameter \u03b2 to represent rationality, with higher values approaching optimality and lower values leading to random choices. The text discusses modeling irrationalities in reward inference in reinforcement learning by manipulating components of the Bellman update. It includes adjustments for optimism/pessimism in transition functions and modifying reward values based on prospect theory. The introduction of a parameter \u03b2 in Boltzmann-rationality represents rationality, with higher values approaching optimality and lower values leading to random choices. The next set of irrationalities manipulates the transition function away from reality, including the illusion of control where experts use the Bellman update to act as if in a deterministic environment or with equal chances of transitioning to every possible successor state. The text discusses modeling irrationalities in reward inference in reinforcement learning by adjusting components of the Bellman update. It includes modifications for optimism/pessimism in transition functions and altering reward values based on prospect theory. Parameters control the expert's level of optimism/pessimism and loss aversion, influencing their decision-making process. The text discusses modeling irrationalities in reward inference in reinforcement learning by adjusting components of the Bellman update. It includes modifications for optimism/pessimism in transition functions and altering reward values based on prospect theory. Parameters control the expert's level of optimism/pessimism and loss aversion, influencing their decision-making process. A family of reward transforms is discussed, where c controls the expert's level of loss aversion. Extremal experts focus on maximizing the expected maximum reward along a trajectory. Myopic Discount experts only consider immediate rewards by decreasing gamma in the Bellman update. In reinforcement learning, experts can be modeled with varying levels of rationality by adjusting components of the Bellman update. This includes decreasing gamma to make the expert more myopic and only focused on immediate rewards. Another approach is to limit the number of Bellman updates an expert performs, with a finite horizon h making the expert myopic and caring only about rewards within that horizon. Hyperbolic discounting is also considered, where a high discount rate is applied to immediate rewards and a low rate to future rewards. Alexander & Brown (2010) introduce a Bellman update formula where k modulates the expert's preference for immediate versus future rewards. In a small 5x5 gridworld, experts exhibit different behavior due to irrationalities. The gridworld has ice, holes, and rewards cells. Experts can move in cardinal directions from ice cells. Six irrationalities with parameter settings outperform rational experts. Experts have a 0.8 probability of moving in a certain direction and 0.2 probability of moving in adjacent directions. Holes and rewards are terminal states, penalizing experts for falling into holes. Experts in a 5x5 gridworld face penalties for falling into holes and rewards for reaching specific cells. To measure inference difficulty, Bayesian updates are performed to calculate log loss and L2-distance between inferred and actual values of \u03b8. The performance of reward inference on trajectories of fixed length T is calculated for each irrationality type, measuring log loss and L2 distance from the mean. Different irrationality types show varying slopes and convergence values, with the best performing type differing based on the metric used. The study found that different irrationality types had varying impacts on log loss, with some outperforming the rational expert. Optimism, Boltzmann, Hyperbolic, Myopia, and Extremal experts all showed better performance in certain parameter settings. Increasing the observed trajectory length also affected the quality of inference. The quality of inference varies with the length of the observed trajectory. Results show that different irrationality types impact log loss differently, with Optimism performing best in log loss but not in L2 distance. Mutual information measures uncertainty reduction between variables, with the expert's trajectory and reward parameters being crucial for reward inference. The expert's mutual information influences the learner's posterior log loss. Different policies are produced based on the expert's rationality and reward parameters. The information processing inequality bounds the mutual information between variables. The experts' ability to outperform the rational expert is due to their capacity to differentiate between certain parameters. The correlation between informative policies and trajectories is not perfect, with the Optimism expert having the most informative trajectories but less informative policies compared to the Boltzmann expert. The bound of mutual information between variables is not always achievable, even with a large number of trajectories. Our contribution was to identify a systematic set of irrationalities in reward inference tasks, showing that suboptimal experts can help an agent learn the reward function. We found that optimism bias, myopia, and noise were the most informative irrationalities, surpassing the rational expert in ideal settings. By quantifying and comparing inference performance for different types of irrationalities, we show that being irrational is not always harmful. The study focused on the inference performance for different types of irrationalities in reward tasks. It highlighted the challenge of estimating irrationality type and degree in practice, emphasizing the importance of explicitly reasoning about irrationality for accurate reward inference. Comparing inference with the true model versus assuming a default Boltzmann model showed significant differences in results. The study emphasized the importance of modeling irrationalities for accurate reward inference. Testing different models showed significant differences in results, indicating the need for further research on generalization across different environments. The analysis of mutual information supported the Boltzmann rationality result, showing varied policies with reward parameters. The usefulness of optimism bias depends on task familiarity, with potential hindrance in inference when goals are known but not what to avoid. The paper highlights the complexity of biases in inference and suggests the potential for influencing humans to exhibit helpful types of irrationality. It emphasizes the importance of both learning and teaching in reward inference, indicating that being a good teacher may be easier than previously thought. The paper emphasizes the complexity of biases in inference and the potential for influencing humans to exhibit helpful types of irrationality. It suggests that being a good teacher may be easier than previously thought. The study involves discretizing \u03b8 for exact inference, with 25 possible reward parameters and a uniform prior. Trajectories are sampled for each reward parameter and policy combination, with plots showing log loss for Prospect Theory and Illusion of Control experts, as well as L2 loss for all irrationalities. The study investigates the importance of knowing the exact type of irrationality in inference. Different irrationalities outperform rational experts, with using a misspecified model leading to significant inference impairment. The study shows that using a misspecified irrationality model (Boltzmann) causes more impairment in inference performance than the actual types of irrationality themselves. Expert irrationality is not a major impairment to reward inference compared to using a misspecified model. Misspecification can lead to significant inference impairment due to experts exhibiting the same actions under different reward parameters. The myopic agent may choose a closer but smaller reward, leading to a false inference of its size. This can result in high log loss when the actual reward is smaller. Inferring the myopic agent's preferences as Boltzmann can lead to poor performance in this case."
}