{
    "title": "S1g8K1BFwS",
    "content": "Knowledge graph embedding research has traditionally overlooked the issue of probability calibration in popular embedding models. A novel method is proposed to calibrate models when ground truth negatives are unavailable, using Platt scaling and isotonic regression. Experiments on three datasets demonstrate that this approach leads to well-calibrated models compared to using negatives. Isotonic regression shows the best overall performance, achieving state-of-the-art accuracy without requiring relation-specific decision thresholds. Knowledge graph embedding models learn vector representations of nodes and edges in a knowledge graph. Existing models lack probability calibration, which is crucial for accurate predictions. Prior methods like using a sigmoid layer for calibration have shown poor results. The study focuses on the poor calibration of off-the-shelf TransE and ComplEx models in knowledge graph embeddings, which is crucial for accurate predictions in high-stakes scenarios like drug-target discovery. The models are shown to be miscalibrated, leading to the need for relation-specific thresholds when classifying triples as true or false. This work is the first to address calibration in knowledge embeddings, with a two-fold contribution. The study addresses poor calibration in knowledge graph embeddings, focusing on TransE and ComplEx models. It introduces Platt Scaling and isotonic regression for calibration, especially in the absence of ground truth negatives. A calibration heuristic combining these methods with synthetically generated negatives improves model calibration. Experimental results demonstrate better-calibrated models, even without ground truth negatives, reaching state-of-the-art accuracy in triple classification. The curr_chunk discusses various knowledge graph embedding models such as TransE, DistMult, RotatE, ComplEx, HolE, and TorusE. These models operate in different spaces and have unique characteristics. Various knowledge graph embedding models operate in different spaces and have unique characteristics. Some models include convolutional layers or capsule networks architectures, while others rely on tensor decomposition techniques. Adversarial learning and attention mechanisms are also utilized in certain models. The analysis in this paper focuses on four popular models: TransE, DistMult, ComplEx, and HolE, which do not address the reliability of predictions or calibrating probabilities. Recent interest in neural architectures calibration has shown that modern neural networks are poorly calibrated and can be improved with novel methods. For example, Guo et al. (2017) proposed using temperature scaling for calibrating neural networks in classification problems, while Kuleshov et al. (2018) suggested a Platt scaling procedure for calibrating deep neural networks in regression problems. The Knowledge Vault pipeline by Dong et al. (2014) uses Platt scaling calibration for extracting triples from unstructured knowledge, but this is not applied to knowledge graph embedding models. KG2E utilizes normally-distributed embeddings to address uncertainty but lacks the ability to provide the probability of a triple being true, suggesting that it could benefit from the output calibration proposed. KG2E could benefit from the output calibration proposed here, as it lacks the ability to provide the probability of a triple being true. The only existing work that adopts probability calibration for knowledge graph embedding models is Krompa\u00df & Tresp (2015), who suggest using ensembles and Platt scaling for calibration. Knowledge graphs consist of triples (s, p, o) with subjects, predicates, and objects, and triple classification is a binary task using positive triples for training. Knowledge graph embedding models use positive triples for training and lack calibrated models. Decision thresholds are chosen for each relation type using a validation set. Link prediction involves assigning scores to unlabeled triples in a held-out set. Metrics like mean rank and Hits@N are used. Architectures encode concepts from a knowledge graph into low-dimensional vectors through embeddings learned by training a neural architecture. The training phase involves minimizing a loss function that includes a scoring function for triples. Existing models propose scoring functions that combine embeddings using different intuitions. The scoring functions of common models, like TransE, compute similarity between embeddings using L1 or L2 norm. These functions are used on positive and negative triples in the loss function, often a pairwise margin-based loss or log-likelihood. Synthetic negatives are generated for training by corrupting one side of the triple at a time. Knowledge graph embedding models are identified by their scoring function, which estimates confidence levels for triples. The text discusses the importance of calibrating models in knowledge graph embeddings. It introduces calibration techniques like Platt scaling and isotonic regression for scenarios with and without ground truth negatives. These techniques help ensure that the estimated confidence levels represent true probabilities. In knowledge graph embeddings, calibration techniques like Platt scaling and isotonic regression are used to ensure estimated confidence levels represent true probabilities. When ground truth negatives are not provided, synthetic negatives can be generated to calibrate the model, with sample weights used to maintain population base rates. Following the standard protocol proposed by (Bordes et al., 2013), for every positive triple, one side is corrupted at a time by replacing it with other entities. The number of corruptions generated per positive is defined by the corruption rate \u03b7. To address the imbalance between positive and negative triples, a weighting scheme is proposed to match the population base rate \u03b1. The weights \u03c9+ and \u03c9- are associated with positive and negative triples, respectively, to remove the imbalance caused by a higher number of corruptions. The text discusses the imbalance in having more corruptions than positive triples in each batch and how the \u03c9 \u2212 weight ensures the positive base rate \u03b1 is respected. It also mentions the calibration quality of heuristics and the impact of calibrated predictions on triple classification tasks using datasets with ground truth negatives. The text discusses datasets like WN11, FB13, and YAGO39K, along with benchmark datasets WN18RR and FB15K-237 for knowledge graph embedding models implemented with the AmpliGraph library. The graph embedding models are implemented using the AmpliGraph library with TensorFlow and Python on an Intel Xeon Gold server. Different knowledge graph embedding models are trained with specific hyperparameters and loss functions for calibration success. The calibration methods show better-calibrated results than the uncalibrated case for all datasets. There is considerable variance in results between models and datasets, with TransE performing best for WN11 and FB13, and DistMult working best for YAGO39K. This variance is attributed to the quality of the embeddings themselves. The quality of embeddings affects calibration, as shown in Figure 2 with reliability diagrams. Ground truth generally outperforms synthetic calibration due to more data and better quality. Synthetic calibration is closer to ground truth than uncalibrated methods. The synthetic calibration method is shown to be closer to the ground truth than uncalibrated scores, as seen in Figure 2. Isotonic regression outperforms Platt scaling in general, but it is not a convex or differentiable algorithm, which can be problematic for synthetic calibration. Platt scaling, being convex and differentiable, can be optimized more efficiently. The influence of different loss functions on calibration is explored, with experiments showing that the choice of loss has a significant impact. The quality of embeddings is also assessed, with a focus on whether better embeddings lead to sharper calibration. The mean reciprocal rank (MRR) is used to evaluate the performance of the embeddings. The study explores the influence of loss functions on calibration and the quality of embeddings in link prediction. The mean reciprocal rank (MRR) is used to evaluate the performance of embeddings, showing no correlation between calibration results and MRR. The synthetic calibration method is applied to two benchmark datasets, FB15K-237 and WN18RR, with varying base rates. The model is calibrated using isotonic regression and Platt scaling, with negatives sampled from the negative set. The study examines the impact of loss functions on calibration and embedding quality in link prediction. Isotonic regression and Platt scaling are used for calibration, showing poor performance compared to a baseline. A natural threshold of \u03c4 = 0.5 is proposed to simplify the learning process. Calibration significantly affects the triple classification task, with different thresholds learned for different datasets. The study explores the impact of calibration on link prediction, using isotonic regression and Platt scaling. Results show that a single threshold of \u03c4 = 0.5 provides competitive results compared to multiple thresholds. Different thresholds are learned for different datasets in the triple classification task. Isotonic regression is the best method for achieving state-of-the-art results for WN11. A proposed calibration method with synthetic negatives performs well overall, even with calibration done using only half of the validation set. The study targets datasets with and without ground truth negatives, experimenting on triple classification datasets with different calibration methods. Isotonic regression shows better calibration performance compared to other methods. Isotonic regression improves calibration performance but is computationally expensive. Triple classification benefits from calibration, allowing a single decision threshold for state-of-the-art results. Future work includes evaluating beta calibration and Bayesian binning, as well as ensembling knowledge graph embedding models for improved combination of outputs. Reliability Diagram visually depicts model calibration. The curr_chunk discusses model calibration metrics such as Brier Score, Log Loss, and Platt Scaling. Brier Score measures calibration of a binary classifier, with a lower score indicating better calibration. Log Loss measures uncertainty in probability estimates, while Platt Scaling is a calibration method proposed by Platt et al. (1999). Platt Scaling, proposed by Platt et al. (1999), is a popular parametric calibration technique for binary classifiers. It involves fitting a logistic regression model to the scores of a binary classifier to obtain calibrated probabilities. Isotonic Regression, introduced by Zadrozny & Elkan (2002), is a non-parametric calibration technique that fits a non-decreasing piecewise constant function to the output of an uncalibrated classifier to achieve calibrated probabilities. Isotonic regression learns a function q = g(p) to achieve calibrated probabilities by minimizing square loss. Calibration helps spread instances across bins, with embedding size having a higher impact than negative/positive ratio. Low-dimensional embeddings show worse Brier scores, and any k > 50 does not improve calibration. In Table 5, traditional knowledge graph embedding rank metrics are presented, including MRR, MR, and Hits@10. Results for all datasets and models used in the main text are reported. Choosing \u03b7 > 10 does not affect the calibration score. Relation-specific decision thresholds \u03c4 used in Table 4 are shown in Table 6 under the 'Reproduced' column. The raw scores returned by the model-dependent scoring function are used to determine relation-specific decision thresholds, as shown in Table 6. Refer to Table 4 for triple classification results."
}