{
    "title": "HyxJhCEFDS",
    "content": "Adversarial training is a key defense against adversarial attacks. A study on large-scale adversarial training on ImageNet reveals two intriguing properties. The role of normalization in achieving state-of-the-art performance on vision tasks is studied. Batch normalization (BN) can hinder networks from achieving strong robustness in adversarial training. Removing clean images from training data for models trained with BN significantly boosts adversarial robustness by 18.3%. This phenomenon is linked to the hypothesis that clean and adversarial images come from different domains. Disentangling the mixture distribution for normalization by applying separate BNs to clean and adversarial images improves robustness. Enforcing consistent behavior of Batch Normalization (BN) at training and testing enhances robustness for statistics estimation. The study explores the impact of network capacity on adversarial learning. Deep networks are found to be shallow in this context, requiring deeper networks for higher adversarial robustness. Even pushing network capacity to an unprecedented scale like ResNet-638 shows substantial and consistent improvements in robustness. Adversarial attacks can deceive neural networks with subtle changes in input data. Adversarial training is an effective defense method, training networks on adversarial images generated during training. Various techniques like gradient masking mitigation, logits pairing, and denoising at feature space have been proposed to enhance robustness. However, inconsistent pipelines for adversarial training still leave gaps in understanding how to train robust models effectively. This paper offers a comprehensive analysis of different adversarial learning strategies under a unified training approach. The study provides the first rigorous analysis of adversarial learning strategies on the ImageNet dataset. It reveals that Batch Normalization can hinder robustness in adversarial training, and removing clean images from training data boosts model robustness. This suggests that clean and adversarial images come from different domains. The two-domain hypothesis explains the limitation of Batch Normalization in training with a mixture of clean and adversarial images. Adversarial training can achieve strong robustness by disentangling the mixture distribution at Batch Normalization. An alternative solution is to use group normalization to estimate normalization statistics independently for each image. Model robustness is closely linked to normalization techniques. Model robustness in adversarial training is highly related to normalization techniques. Adding more layers to \"deep\" networks only marginally improves accuracy for traditional image classification tasks, but significantly boosts model robustness for adversarial learning. Larger networks, like ResNet-638, are encouraged for adversarial learning due to the complexity of fitting adversarial images. Our paper highlights the importance of proper normalization in adversarial training for strong model robustness. It also points out that \"deep\" networks are still shallow when it comes to adversarial learning. Adversarial training, the foundation for defending against attacks, involves using both clean and adversarial images for training. Min-max optimization is used to train models for improved robustness. In this work, the authors focus on training adversarial defenders at scale and provide a detailed diagnosis revealing two intriguing properties. They discuss the effectiveness of normalization layers in accelerating deep network training for improved model robustness. Various methods are proposed to exploit batch-wise normalization techniques. Different methods are proposed to exploit batch-wise, layer-wise, or channel-wise information for estimating normalization statistics in deep network training. While Batch Normalization (BN) is typically effective in traditional vision tasks, it may hinder robustness in adversarial training. Properly handling normalization is crucial for improving adversarial robustness, and a unified framework is provided for training and evaluating different models for fair comparison. In a unified framework, different models are trained and evaluated using an adversarial training pipeline on ImageNet. The baseline network is ResNet-152, and the PGD attacker is used to generate adversarial examples during training. Models are trained for 110 epochs with a decrease in learning rate at specific intervals. Performance evaluation focuses on studying various aspects. In this study, adversarial robustness is evaluated using the targeted PGD attacker with specific parameters. Different adversarial training strategies are explored for enhancing model robustness. Adversarial training strategies, including mixing clean images with adversarial counterparts in mini-batches, are used to enhance model robustness. The loss function balances the importance of clean and adversarial image losses. The model achieves 20.9% accuracy against PGD-2000 attacker with \u03b1 = 0.5. Two additional adversarial training strategies are studied for their effectiveness. In a study by Kannan et al. (2018), the relationship between model robustness and the ratio of clean images used for training was investigated. By removing a portion of clean images from the training data, model robustness significantly improved. The strongest robustness was achieved by completely removing clean images, resulting in an accuracy of 39.2% against PGD-2000 attacker, outperforming the baseline model by 18.3%. The study explores the effectiveness of adversarial logits pairing (ALP) as a training strategy, showing a 2.1% improvement over the baseline model. Training exclusively on adversarial images is found to be the most effective strategy for boosting model robustness, with ALP achieving an accuracy of 23.0% against PGD-2000 attacker. The study compares different training strategies for model robustness against strong attacks. Training exclusively on adversarial images (100% adv + 0% clean) boosts accuracy by 18.3% and ensures robustness against strong attacks. Using clean images for training may result in worse robustness as attackers perform more iterations. The study explores training strategies to enhance model robustness against strong attacks by combining adversarial and clean images. Research shows that feature maps of adversarial images are noisier compared to clean images. Various studies demonstrate the possibility of building classifiers to distinguish between clean and adversarial images, suggesting they are drawn from two distinct distributions. The study investigates the impact of using clean and adversarial images in training to improve model robustness. It suggests that clean and adversarial images come from different domains, leading to challenges in utilizing batch normalization. The two-domain hypothesis highlights the importance of batch normalization in achieving state-of-the-art performance in vision tasks. The study explores the impact of using clean and adversarial images in training to enhance model robustness. It suggests that accurately estimating normalization statistics can improve model training even when clean and adversarial images are mixed. Two approaches are considered: maintaining separate normalization layers for clean/adversarial images or replacing batch normalization with batch-unrelated normalization layers. Training with Mixture BN leads to weak model robustness, and decoupling normalization statistics for adversarial training can boost image recognition. Training with Mixture BN (MBN) can boost image recognition by disentangling mixed distribution for clean and adversarial images. Two training strategies, 100% adv + 100% clean and 100% adv + 100% clean, ALP, are tested with separate BNs for adversarial and clean images. Results show performance is influenced by how BN is trained, with MBN clean showing strong performance. The performance of the network is closely tied to how Batch Normalization (BN) is trained. Using Mixture BN (MBN) clean results in similar clean image accuracy as training exclusively on clean images, while MBN adv achieves comparable adversarial robustness to training exclusively on adversarial images. Other factors like ALP for training only lead to minor performance differences. Robustness evaluation curves show that networks using MBN adv can defend against strong attacks, with robustness plateauing with increased attack iterations. BN statistics play a key role in characterizing model performance. In Figure 5, running statistics of MBN clean and MBN adv are compared, showing significant differences between clean and adversarial images. Adversarial images exhibit larger running mean and variance, indicating noisy patterns/outliers in the feature space. This observation supports the idea that clean and adversarial images belong to different domains, and current networks struggle to learn a unified representation for both. MBN structure is also used as a practical trick for training better generative adversarial networks (GAN). Chintala et al. (2016) suggest constructing mini-batches with either real or generated images when training discriminators in GANs. Unlike situations where BN statistics remain divergent after training, successful GAN training usually learns a unified set of BN statistics on real and generated images. Group Normalization (GN) can be used as an alternative to disentangle mixture distributions and achieve comparable performance to BN on various vision tasks. Enforcing consistent behavior of Group Normalization (GN) significantly boosts adversarial robustness in vision tasks. GN replaces all Batch Normalization (BN) with GN, ensuring robustness against strong attacks. Models trained with GN achieve 39.5% accuracy against PGD-500, with only a marginal performance drop of 0.5% against PGD-2000. Future work includes exploring other batch-unrelated normalization methods in adversarial training. In adversarial training, using clean images and constraining perturbations to smaller values like 8 can improve robustness. Previous studies have shown success with clean images on small datasets like MNIST and CIFAR-10. However, standard evaluation on ImageNet uses a perturbation constraint of 16. Batch Normalization (BN) shows inconsistent behavior between training and testing due to the concept of \"batch\" not being valid at inference time. During training, Batch Normalization (BN) computes mean and variance on mini-batches, while at testing, it uses pre-computed statistics from the training set. This inconsistency may impact adversarial training. To assess this, models trained with 100% adversarial and 0% clean data are evaluated for statistics matching. During training, Batch Normalization (BN) computes mean and variance on mini-batches, while at testing, it uses pre-computed statistics from the training set. In Figure 6, batch mean is almost equivalent to running mean, but batch variance does not converge to running variance on certain channels. To address this inconsistency, a heuristic approach is taken by applying pre-computed running statistics for model training during the last 10 epochs. This approach boosts model robustness by 3.0% in adversarial training. The approach of training convolutional filters adversarially improves model robustness under different training strategies, such as MBN adv and ALP. It does not require additional training budgets and shows strong performance on both clean and adversarial images. This suggests that clean and adversarial images share the same convolutional filters for feature extraction. The study explores the effectiveness of using convolutional filters learned on adversarial images for feature extraction on clean images. Results show that filters learned on adversarial images can boost accuracy on clean images significantly. However, the opposite direction does not work, indicating the importance of training filters adversarially. Training convolutional filters adversarially is important as they can effectively extract features from clean images. Adversarially trained models show a tradeoff between clean accuracy and robustness, with strategies for strong robustness resulting in lower accuracy on clean images. Replacing Batch Norms with Group Norms can improve clean image accuracy while maintaining robustness. This tradeoff is also observed in prior work. Future research should focus on improving the tradeoff between clean accuracy and robustness in adversarially trained models. Current networks, like ResNet-152, may underfit the complex distribution of adversarial images, suggesting the need for larger networks with more residual blocks. While adding more layers to deep networks may not significantly improve clean image accuracy, exploring deeper networks could enhance performance on adversarial tasks. The improvement of clean image accuracy saturates beyond ResNet-200 depth. Deeper models trained on adversarial images show possible underfitting. Adversarial learning tasks require deeper networks for stronger robustness. Increasing depth from ResNet-152 to ResNet-338 improves robustness by 2.4%, while clean training only improves by 0.5%. Even with ResNet-638, deeper networks are needed for adversarial learning tasks. Increasing network depth and width is crucial for adversarial learning tasks, as shown by empirical observations and theoretical studies. Deeper networks like ResNet-638 outperform shallower ones like ResNet-152 in clean image accuracy. Conducting normalization correctly is essential for training robust models on large-scale datasets like ImageNet. Our discoveries suggest that deeper networks are crucial for adversarial learning tasks, but current \"deep\" networks are still considered shallow. The differences between clean images and adversarial images may be due to different distributions. Our reproduced ALP outperforms previous results, with key differences in parameter settings such as learning rate decay. Our re-implementation of ALP outperforms previous results by adjusting key parameters such as learning rate decay, training optimizer, and PGD initialization. The total number of training epochs is set differently to ensure similar learning rates between exponential decay and step-wise decay settings. By adjusting key parameters such as learning rate decay, training optimizer, and PGD initialization, our re-implementation of ALP outperforms previous results. Stronger attackers for training, like changing from PGD-10 to PGD-30, are crucial for achieving robustness. Other parameters, such as optimizer, do not significantly impact robustness. Evaluating a ResNet-101 against PGD-2000 shows similar results to the original ALP paper, with a 2.1% accuracy. Changing parameters one by one reveals the performance gap between the original ALP. By changing the attacker from PGD-10 to PGD-30 for training, the robustness against PGD-2000 can be increased by 19.7%. The number of attack iterations used for training is crucial for model robustness, with decreasing iterations leading to weaker performance. Training strategies play a significant role in the performance change based on the number of PGD attack iterations used. The performance change in model robustness is strongly influenced by training strategies. Strategies that do not result in strong robustness experience severe degradation, while those that secure robustness against attacks show only marginal drops in robustness. Decreasing the number of attack iterations during training generally leads to weaker robustness, with the extent of degradation depending on the training strategies. Applying running statistics in training is also studied for effectiveness. In this study, the effectiveness of applying running statistics in training is examined under different settings. Three strategies are tested, each trained with four different attackers, resulting in 12 settings. The results show that this heuristic policy improves robustness in all settings, emphasizing the importance of consistent behavior in Batch Normalization. Additionally, clean image performance of adversarially trained models is evaluated in Table 7 for completeness, and accuracy against various attack strengths is listed for future performance comparisons."
}