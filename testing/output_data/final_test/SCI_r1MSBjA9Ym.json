{
    "title": "r1MSBjA9Ym",
    "content": "Recent theoretical work has shown that deep neural networks outperform shallow networks but face challenges in training, such as the vanishing gradient problem. Even with rectified linear unit (ReLU) activation, deep and narrow neural networks can converge to incorrect states when solving high-order derivative problems. This collapse phenomenon is demonstrated both numerically and theoretically, with estimates of the probability of occurrence provided. A safe region diagram for designing NNs to avoid collapse is presented, along with strategies like asymmetric initialization to mitigate the issue. The best-known universal approximation theorems of neural networks state that every measurable function can be accurately approximated by a single-hidden-layer neural network. These results do not provide information on the required size of a neural network for a pre-specified accuracy. In another study, optimal approximations of smooth and analytic functions in shallow networks were considered, showing that a certain number of neurons can uniformly approximate any function on a compact set with error. Deep neural networks have been effectively used in various applications in the last 15 years. Recent studies show that deep neural networks outperform shallow networks of similar size in terms of approximation. For example, a 3-layer neural network with 10 neurons per layer may be a better approximator than a 1-layer network with 30 neurons. Theoretical results explain the superior performance of deep neural networks in approximation tasks. In approximation tasks, deep neural networks outperform shallow networks of similar size due to the exponential difference in the number of neurons required for a given accuracy level. In approximation tasks, deep neural networks outperform shallow networks of similar size due to the exponential difference in the number of neurons required for a given accuracy level. C \u03b2 functions for ReLU NN show that a minimum depth is given by \u03b2/d to achieve optimal approximation rates. BID25 demonstrated that any Lebesgue integrable function from R d to R can be approximated by a ReLU forward NN of width d + 4 with respect to L 1 distance. BID14 proved that any continuous function can be approximated by a ReLU forward NN of width d in + d out. Deep neural networks with pyramidal structures and certain activation functions require a width larger than the input dimension to create disconnected decision regions. Before 2010, logistic sigmoid and hyperbolic tangent were commonly used non-linear activation functions, but they are difficult to train due to the vanishing gradient problem. In 2011, ReLU was introduced as an activation function that avoids the vanishing gradient problem and results in sparse neural networks. In practice, ReLU and its variants like LReLU, PReLU, and ELU are preferred in deep learning models. Theoretical results may not always align with practical training of neural networks, which can be NP-hard. Local minima, singularity, and bad saddle points are challenges in learning neural networks. Our paper focuses on the collapse of deep and narrow neural networks, specifically on bad local minima encountered during training. Through numerical simulations and theoretical results, we show that without proper normalization techniques like batch normalization, the network may converge to an erroneous state. Weight normalization, for example, can lead to this collapse. Normalization techniques like weight normalization can lead to the collapse of deep and narrow neural networks. Numerical tests demonstrate that these networks tend to converge to the mean value of the function, even with proper initialization methods. Normalization techniques like weight normalization can lead to the collapse of deep and narrow neural networks. The training data is sampled from a uniform distribution, and various optimizers are employed. It is observed that in most cases, the bias in the last layer is the mean value of the function, leading to the optimization stopping due to gradient vanishing. This collapse problem is seen for functions of different regularity, with examples shown in figures. In simulations testing multi-dimensional inputs and outputs, a collapse phenomenon is observed for the target function y(x). Training a 10-layer ReLU network with width 4 often leads to the network collapsing to the mean value or partial mean value of y(x). This collapse is demonstrated for the target function y(x) = |x| using a 10-layer ReLU neural network with width 2. The gradient vanishes in both cases, indicating a common issue in deep neural networks. The collapse problem in deep neural networks is observed when weights are randomly initialized from a symmetric distribution. This initialization helps avoid exploding/vanishing mean activation length, making it beneficial for training neural networks. The loss function used is MSE, and for MAE, the constant is the median value of the target function. The study focuses on a feed-forward neural network with specific weight and bias parameters. The study investigates how the length of the input propagates through neural networks, specifically focusing on the normalized squared length of the vector before activation at each layer. The recursion simplifies to a ReLU activation function, with He normal initialization widely used, which neither shrinks nor expands the inputs, explaining its success in applications. Theoretical analysis of collapse behavior in ReLU feed-forward neural networks with He normal initialization. Assumptions and estimates of collapse probability are derived, with a focus on weight initialization strategies to avoid activation length issues. Theoretical analysis of collapse behavior in ReLU feed-forward neural networks with He normal initialization. Assumptions A1 and A2 are satisfied during NN initialization and training stages, leading to key findings regarding activation behavior and gradient vanishing. Theorem 4 states that for a ReLU feed-forward neural network with certain assumptions during initialization, if a layer exists where the output is always zero, the network will eventually optimize to a constant function. This is applicable when using L2 loss and the function has a constant value. Corollary 6 extends this to include more general converged mean states. Theorem 4 discusses the optimization of a ReLU feed-forward neural network to a constant function under certain initialization assumptions. Corollary 5 and 6 further elaborate on this, with Corollary 5 being a special case of Corollary 6. Lemma 7 addresses the initialization of a one-layer ReLU network with symmetric nonzero distributions. Proposition 9 states that in a ReLU neural network with random initialization using symmetric nonzero distributions, the output is zero with a probability of (1/2) for any fixed nonzero input. Theorem 8 provides an upper bound on this probability. Proposition 10 gives a theoretical formula for the probability in a bias-free ReLU neural network with width 2 and random initialization. In a ReLU neural network with width 2 and L layers, weights are randomly initialized using symmetric nonzero distributions. The probability of collapsing to a constant function is determined by the last component of \u03c0 L, where \u03c0 1 and P represent the probability distribution after the first layer and the probability transition matrix when adding another layer. Numerical simulations are used to estimate probabilities due to the complexity of obtaining an explicit expression. Theoretical and numerical results show that the network has the same probability of collapsing regardless of the symmetric distributions used for initialization. When designing a neural network, it is important to keep the collapse probability low. The depth and width of the network impact the probability of initializing to a zero function, leading to a higher chance of vanishing gradients in multiple layers. In experiments, it was observed that gradients are often zero for all parameters except in the last layer, where ReLU is not used. This limits optimization to the last layer only. To mitigate this issue, it is recommended to maintain a low probability, such as less than 1% or 10%. In designing a neural network, it is crucial to keep the collapse probability low, ideally less than 1% or 10%. A diagram in FIG9 illustrates that as the number of layers increases, the numerical tests align more closely with theoretical predictions. For instance, a 10-layer NN with a width of 10 has a 1% probability of collapsing, while a width of 5 has a probability exceeding 10%. Training techniques are discussed to address the collapse issue, with analysis applicable to symmetric and asymmetric initializations like orthogonal and LSUV initialization. LSUV initialization is an orthogonal initialization combined with weight rescaling to achieve unit variance in each layer's output. However, orthogonal initialization alone cannot prevent collapse issues in deep and narrow neural networks. To address this, various normalization techniques are employed to ensure the collapse probability remains below 1% or 10%. The safe region for designing neural networks is depicted below the blue line, with width playing a crucial role in network stability. Normalization methods like batch normalization (BN), layer normalization (LN), weight normalization (WN), instance normalization (IN), group normalization (GN), and scaled exponential linear units (SELU) are essential for training neural networks. These techniques are crucial for ensuring network stability, especially in narrow networks where the width must be larger than the input dimension for good performance. Highway and ResNet architectures are not considered as they deviate from standard feed-forward neural networks. Normalization methods like batch normalization (BN), layer normalization (LN), weight normalization (WN), instance normalization (IN), group normalization (GN), and scaled exponential linear units (SELU) are crucial for training neural networks. In low-dimensional inputs, only BN and SELU are effective in preventing network collapse and approximating the target function accurately. While BN adjusts weights and biases based on gradients, SELU maintains non-vanishing negative values. On the other hand, WN fails as it only re-parameterizes weight vectors. Dropout does not solve the collapse issue by inducing sparsity and zero activations. Training deep and narrow ReLU neural networks for approximating multi-dimensional functions can lead to convergence issues, where networks converge to incorrect means or medians of the target function regardless of the optimizer used. This collapse problem is independent of the loss function, with MSE loss leading to mean values and MAE loss leading to median values. Loss converges to median values due to symmetric random initialization, maintaining output length. Theoretical analysis shows NN collapse to constant value if certain conditions met, with gradients vanishing. Estimates of collapse probability provided for general cases and deep NNs with width 2. Based on theoretical estimates and numerical tests, a diagram is constructed as a guideline for designing deep and narrow neural networks to avoid collapse. Various methods to prevent collapse are explored, with normalization techniques like batch normalization and SELU found to be effective, while weight normalization and dropout are not successful. DISPLAYFORM1 is a Gaussian distribution of independent Gaussian random variables. Lemma 11 states that for a random matrix A, the probability of Ax = 0 is 0 for a nonzero column vector x. The proof involves considering the first value of Ax and utilizing ReLU in the last layer to show the existence of a layer where h \u2264 0 and x = 0 for all x. The proof involves showing that for a neural network with at least two points, if x1 = 0, then h \u2264 0. By induction, it is proven that for any layer, there exists a layer whose output is zero. The proof involves showing that for a neural network with at least two points, if x1 = 0, then h \u2264 0. By induction, it is proven that for any layer, there exists a layer whose output is zero. By backpropagation, it is shown that the weights and biases in certain layers do not change, leading to N being optimized to a constant function with the smallest loss. The proof involves showing that gradients vanish for specific inputs in a neural network. If the last layer uses ReLU activation, the probability of certain outputs being zero is discussed. The case where the last layer does not have ReLU activation is also considered. For L = 1, N is a single layer perceptron, a ReLU neural network with specific input conditions is analyzed. The output of each hidden layer has 16 possible cases based on the input values. The probabilities of different cases are determined based on the orientation of vectors \u03c9 and \u03c9*. The probabilities of 16 cases in a single layer perceptron neural network are determined based on the orientation of vectors \u03c9 and \u03c9*."
}