{
    "title": "2AGZUDRsHg",
    "content": "Recent pretrained transformer-based language models have shown state-of-the-art performances on NLP datasets, but they are plagued by structural and syntactic biases. This study focuses on the lexical overlap bias, where high lexical similarity leads the model to classify sentences as entailing regardless of their actual meaning. To address this, input sentences are enriched with automatically detected predicate-argument structures to help the models learn different attention patterns and recognize key parts of the sentences. Evaluation on tasks like natural language inference and grounded commonsense inference using BERT, RoBERTa, and XLNET models shows improved understanding of syntactic variations, antonym relations, and named entities. The study focuses on improving transformer-based language models' understanding of syntactic variations, antonym relations, and named entities by incorporating predicate-argument structures during fine-tuning. This enhancement significantly boosts robustness without additional costs at test time or requiring changes to the model or training procedure. Transformer models like BERT, XLNET, and RoBERTa have achieved state-of-the-art performances on NLP datasets, including natural language inference and grounded commonsense reasoning tasks. In this paper, the authors enhance the input sentences of training data for pretrained language models with predicate-argument structures to improve reasoning and prediction tasks. This helps address biases such as lexical overlap in models like BERT, XLNET, and RoBERTa. Adding predicate-argument structures during fine-tuning improves model robustness against adversarial settings without additional cost at test time. This enhancement helps focus on important parts of the sentence and abstract away from less relevant details. The contributions of this work include providing adversarial evaluation sets for the SWAG dataset to assess lexical overlap bias and proposing a simple solution by adding predicate-argument structures to fine-tuning data. This solution enhances model robustness without extra cost during testing. The findings of this work show that models fine-tuned on SWAG are more susceptible to lexical overlap bias. The RoBERTa model performs the best on adversarial test sets, demonstrating robustness against this bias. Discriminating different named entities with high lexical overlap is the most challenging evaluation setting. The accuracy of the RoBERTa-large model fine-tuned with augmented data is 59%. Language models like BERT can capture linguistic phenomena without explicit supervision, but incorporating such information explicitly can improve robustness. The lexical overlap bias in NLI is addressed by using separate models to recognize biased examples and adjusting their importance during training. Our proposed solution aims to make models like BERT more robust against this bias. The use of linguistic information in recent neural models, such as BERT, enhances robustness against lexical overlap bias by improving attention patterns. This approach eliminates the need for separate models to handle biased examples during training. The incorporation of syntactic and linguistic information in neural models for Semantic Role Labeling (SRL) has shown improvements in in-domain evaluations. Using graph convolutional networks (GCNs) and transformer-based encoders, researchers have demonstrated enhanced performance in both in-domain and out-of-domain scenarios. The use of additional linguistic features in neural models for Semantic Role Labeling (SRL) improves performance, but a simple BERT model outperforms them without extra linguistic information. Some approaches require linguistic information during both training and testing, while others only use it during training for tasks like syntax parsing. In this work, the authors do not modify the loss function but enhance input sentences for training data without changing the model. Their solution utilizes syntactic information to improve in-domain performance without altering the training procedure. Predicate-argument structures have been beneficial for tasks like machine translation, reading comprehension, and dialogue systems, with previous approaches based on pre-neural models. The proposed model by Marcheggiani et al. (2018) focuses on neural machine translation. The proposed model by Marcheggiani et al. (2018) incorporates predicate-argument structures at the model-level by adding two layers of semantic GCNs on top of a standard encoder. Unlike previous approaches, they use these structures at the input level only for training data, allowing the use of state-of-the-art models without modifications. This work aims to improve the robustness of models for tasks like neural machine translation. The proposed model by Marcheggiani et al. (2018) enhances state-of-the-art models for NLI and GCI tasks by incorporating predicate-argument structures at the input level during training. GCI involves reasoning about situations and predicting outcomes, while NLI determines the relationship between a premise and a hypothesis. The curr_chunk discusses the use of the MultiNLI dataset for NLI experiments and presents examples of premises and hypotheses. It also mentions the evaluation of model robustness using adversarial datasets. The adversarial datasets created for evaluating model robustness against lexical overlap bias include three different sets: syntactic variations, antonym relations, and named entities. Syntactic variations involve swapping subject-verb-object structures, antonym relations replace verbs with their antonyms, and named entities test understanding in the presence of high lexical overlap. The adversarial dataset includes samples created by replacing named entities in the premise with unrelated entities like \"Eve\" to test model discrimination. This dataset contains 190 samples and utilizes the Stanford named entity recognizer for entity identification. The Stanford named entity recognizer is used for identifying named entities. The Heuristic Analysis for NLI Systems (HANS) dataset is used for adversarial evaluation of natural language inference, containing subsets based on lexical overlap variations. Predicate-argument structures are incorporated by augmenting input sentences with raw text. The text discusses augmenting input sentences with predicate-argument structures using a ProbBank-style semantic role labeling model. Special tokens like [PRD], [ARG0], and [ARG1] are used to indicate predicates and arguments. BERT, XLNET, and RoBERTa models are utilized for experiments. In experiments, BERT, XLNET, and RoBERTa models are used. BERT is pre-trained on BookCorpus and English Wikipedia, XLNET uses permutation-based language modeling, and RoBERTa is trained with dynamic masking. Models are initialized with bert-base-uncased, roberta-base, and xlnet-base-cased. Fine-tuning is done on MultiNLI and SWAG training data for NLI and GCI experiments. Results are reported in two different settings. The impact of augmenting training data with predicate-argument structures is evaluated for in-domain and adversarial evaluations on grounded commonsense reasoning and natural language inference tasks using BERT, XLNET, and RoBERTa models. Results are reported in original and augmented settings for both experiments. The impact of augmenting training data with predicate-argument structures is evaluated for in-domain and adversarial evaluations on grounded commonsense reasoning and natural language inference tasks using BERT, XLNET, and RoBERTa models. Results show that while models achieve human-level performance on in-domain evaluation, their performance drops drastically on adversarial sets, indicating reliance on shallow heuristics. Discriminating different named entities is the most challenging adversarial evaluation, but augmentation improves robustness in all three sets. The impact of data augmentation on models like BERT, XLNET, and RoBERTa is evaluated for in-domain and adversarial evaluations. While augmentation improves performance on adversarial sets, there is still room for improvement. RoBERTa shows the highest performance and is more robust against lexical overlap bias. The main challenge in adversarial evaluation is detecting non-entailment labels. The challenge in detecting non-entailment labels in models like BERT, XLNET, and RoBERTa is due to dataset artifacts in MultiNLI. Fine-tuning on augmented data slightly decreases performance on the original dataset, except for XLNET on the match subset. However, it improves performance on NLI adversarial subsets, though not as much as GCI. Models fine-tuned on NLI dataset may recognize predicate-argument structures better. Fine-tuned models on the NLI dataset outperform those trained on the SWAG dataset in recognizing predicate-argument structures. Results on the HANS dataset can vary significantly with different random seeds during fine-tuning. Attention weights between hypothesis and premise words, as well as predicate-argument structures, are visualized using BertViz. Reproducibility is ensured by reporting results with the default random seed. Data augmentation impact on BERT models was evaluated with varying random seeds. SWAG adversarial test set results remain consistent across different random seeds. The results on the SWAG adversarial test sets remain consistent with different random seeds. However, using a different seed can lead to a 21.3pp increase in improvement for augmented experiments on the HANS dataset. BERT attention weights differences are visualized using BertViz, showing how the model focuses on specific words based on training data. The RoBERTa-large model outperforms the RoBERTa-base model on all GCI evaluation sets, but the addition of predicate-argument structures still significantly enhances performance on adversarial evaluation sets. The RoBERTa-large model outperforms the RoBERTa-base model on GCI evaluation sets. Using OpenIE for extracting relations instead of a state-of-the-art SRL model does not significantly impact the resulting robustness of the model. The addition of predicate-argument structures to the training data helps transformer models learn a different attention pattern, but does not benefit pretrained transformer-based models. ESIM model performance is considerably lower than pretrained language models on adversarial datasets. The ESIM model shows lower performance than pretrained language models on adversarial datasets. Adding predicate-argument structures to the training data improves robustness, especially for BERT, XLNET, and RoBERTa models. This addition helps the transformer model recognize key parts of sentences and learn more informative attention patterns. Our proposed solution enhances training examples to improve model robustness without additional cost or changes during test time. It works with noisy predicate-argument structures and is effective for tasks like natural language inference and grounded commonsense reasoning. The solution is not task-specific and can be applied to other tasks suffering from bias, such as paraphrase identification and question answering. New adversarial evaluation sets for lexical overlap bias will be released along with augmented training data. New adversarial evaluation sets for lexical overlap bias and augmented training data for MultiNLI and SWAG datasets will be released."
}