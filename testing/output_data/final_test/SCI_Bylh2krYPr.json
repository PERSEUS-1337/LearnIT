{
    "title": "Bylh2krYPr",
    "content": "Recent work has shown how predictive modeling can enhance agents' understanding of their environment, improving their performance in complex settings. Question-answering is proposed as a method to decode and interpret the representations developed by these agents. The method was applied to two recent predictive modeling approaches - action-conditional CPC and SimCore. Agents trained in visually-rich 3D environments with various objects, colors, shapes, and spatial configurations were probed with synthetic questions to assess their internal state representations. The results indicate that the agents encode detailed information about objects, properties, and spatial relations. Our approach aims to extract compositional information about objects, properties, and spatial relations from the physical environment. It is intuitive and model-agnostic, revealing implicit knowledge acquired by agents. This method can stimulate the design of stronger predictive learning objectives without relying on large labeled datasets or dense reward signals. In this work, question-answering is proposed as an evaluation paradigm to analyze the objective knowledge encoded in an agent's internal representation of the external environment. This approach allows for a comprehensive analysis of internal states by posing complex questions to the agent, without the need for extensive labeled data or reward design. In this study, question-answering is used to probe an agent's internal representation without training the system end-to-end for answering questions. The focus is on developing agents that can learn task-agnostic representations of the external world through self-supervised predictive modeling, inspired by human learning processes. Predictive modeling is utilized to train agents in a visually-rich 3D environment with various objects. The agent's trajectory is guided by a simulation network that predicts future observations self-supervised by a loss function. A question-answering paradigm is employed to extract information about the environment without affecting the agent's internal memory. The question-answering paradigm is used to decode internal representations developed by agents in a visually-rich 3D environment. These representations are learned through exploration and interaction with the environment and can later be used for goal-directed behavior. The study compares predictive and non-predictive agents on their ability to capture objective knowledge about environment semantics solely through egocentric prediction. In a visually rich 3D room environment, a set of questions is developed to probe semantic, relational, and spatial knowledge. The questions range from identifying shapes and colors to counting and spatial relations, testing visual reasoning and recall skills. In a visually rich 3D room environment, questions are designed to test semantic, relational, and spatial knowledge through tasks like identifying shapes, colors, counting, and spatial relations. RL agents are trained with predictive loss functions for exploration tasks, and their ability to capture global environment structure and semantics is evaluated through question decoding. The QA decoder's performance indicates the agent's proficiency in egocentric prediction. Zero-shot generalization of trained QA decoder to novel questions suggests a degree of compositionality in semantic knowledge. Our work builds on prior research on predictive modeling and auxiliary loss functions in reinforcement learning, as well as grounded language learning and embodied question answering. Techniques such as Word2Vec, SkipThought vectors, and BERT have shown the power of predictive modeling for representation learning in language. In vision, similar principles have been applied to tasks like context prediction, unsupervised tracking, inpainting, and colorization. In reinforcement learning, techniques like successor representations, value prediction, and contrastive predictive coding have been used to train model-free agents. Grounded language learning involves grounding language into actions and pixels in physical environments, including 2D and 3D gridworlds. The task involves Embodied Question Answering in physical environments, where agents are trained to answer questions by moving. In contrast, the focus here is on learning a predictive model through goal-agnostic exploration, using question-answering to evaluate the agent's semantic knowledge. This approach is akin to neural population decoding used in neuroscience. The idea of decoding has been successfully used in neuroscience and deep learning to extract desired information from learned representations. In deep learning, networks are trained to predict specific parts of the environment without backpropagating through the internal state. This concept is extended by using question-answering as a decoder to extract complex high-level information from the internal state of an agent. This approach is related to neural population decoding used in neuroscience. The emergence of semantics in an artificial predictive agent's internal representations is a key focus. The use of a visually-rich 3D environment in Unity is highlighted. The Unity-based 3D environment consists of a single L-shaped room with various objects in different colors and orientations. The agent receives first-person RGB images and can perform movements, turns, and object manipulation with 4 degrees of freedom. See Table 5 for the full set of actions. Table 1: Question-answering task templates for a range of tasks testing scene understanding, visual reasoning, and memory skills. Questions cover object presence, attributes, counts, comparisons, and spatial relations in a programmatically generated dataset. Inspired by previous works, the tasks involve 50 shapes and 10 colors in a Unity-based 3D environment with first-person RGB images. The agent in this setting does not have a global view of the environment like in CLEVR, but must answer questions based on partial observations. It is trained to explore and decode answers from its internal representation, requiring it to encode relevant aspects of the environment for easy decoding into symbols. Learning an exploration policy through predictive modeling helps the agent develop general knowledge as it explores and behaves in the environment. The agent is trained to explore and decode answers from its internal representation, developing general knowledge of the environment. It is given a task to visit all 'important' places, receiving a reward each time it visits an object for the first time. This exploration task continues for the duration of each episode. The agent is trained to explore and decode answers from its internal representation using a convolutional neural network and LSTM policy. The question-answering decoder is operationalized as an LSTM that receives the question as input at every timestep and predicts a softmax distribution over one-word answers. The agent, using a convolutional neural network and LSTM, receives RGB observations to select actions and explore the environment. It builds an internal representation pressured by a predictive module to predict future consequences. Different predictive approaches are experimented with, and the internal representations are analyzed using a question-answering decoder. The decoder, an LSTM, receives questions at each timestep to decode answers from the internal representation. The QA decoder, an LSTM, is trained independently of the agent policy to extract object-specific knowledge from the agent's internal state. It evaluates question-answering performance by measuring top-1 accuracy at the end of the episode, comparing the predicted answer with the ground-truth answer. This differs from prior work where agents were trained to move to answer questions. Agents were trained to move to answer questions, with all parameters having access to linguistic information. The navigation policy was trained for exploration, evaluating how agents encode relevant aspects of the environment in their internal representations. An auxiliary predictive head with a simulation network was added to the baseline architecture to simulate future latent states. (Gregor et al., 2019) uses the simulated state skt to condition a generative model based on ConvDRAW (Gregor et al., 2016) and GECO (Rezende & Viola, 2018) to predict the distribution of true observations in pixel space. Baselines include a vanilla RL agent ('LSTM') and a question-only agent for measuring biases in the question-answering testbed. An agent without blocking QA decoder gradients ('No SG') is also compared. The 'No SG' QA decoder gradients model is trained end-to-end with supervision to answer questions and explore. Performance on a shape question-answering task is analyzed, showing that all agents learn to explore and achieve high rewards. Agents equipped with predictive models such as CPC|A and SimCore collect the most rewards, indicating that predictive modeling aids navigation in the environment. SimCore's internal representations lead to the best question-answering accuracy at 72%, outperforming other models. There is still a 24% gap between SimCore and the No SG oracle, suggesting room for improvement in auxiliary predictive losses. The No SG agent achieves almost-perfect accuracy in QA, but the challenge lies in decoding the answer from Decoder complexity. Experimenting with QA decoders of different depths shows that using a deeper QA decoder with SimCore leads to higher QA accuracy, although excessive depths are detrimental. In the non-predictive LSTM agent, the correct answer cannot be decoded. In the non-predictive LSTM agent, the correct answer cannot be decoded regardless of the QA decoder's capacity. QA accuracy varies across question types, with attribute questions being the easiest, followed by spatial relationship questions, and counting questions being the hardest. SimCore representations achieve higher QA accuracy than all other approaches, showing a strong tendency for encoding and retaining information about objects, their properties, and spatial/temporal relations. The No SG agent trained without stopped gradients achieves the highest accuracy for most questions, but not all, possibly due to trade-offs between optimizing different QA losses and the exploration task. The test involves a variant of the shape question type with compositionally novel question-answer pairs, testing the generality of information encoded in the internal state of an agent. The QA decoder is evaluated on a test of generalization using novel question-answer pairs. The decoder performs above chance on held-out test questions, indicating it extracts and applies information in a factorized manner. The study introduced question-answering tasks to evaluate representations learned by artificial agents in a visually-rich 3D environment. Predictive agents, like SimCore, were found to effectively capture knowledge about the external environment. Predictive agents like SimCore can capture detailed environment semantics in their internal states, allowing for easy decoding of answers to questions. Comparatively, non-predictive agents struggle to form these representations. Different predictive models, such as SimCore and CPC|A, were compared, with SimCore showing better ability to develop representations for answering questions about the environment. The choice of predictive model significantly impacts an agent's ability to decode its position, orientation, and top-down map. The ability to decode an agent's position, orientation, and top-down map reconstructions of the environment is significantly impacted by the choice of predictive model. A question-answering approach can be valuable in comparing agents and developing better ones. The agent's capability to decode answers from internal representations learned solely from egocentric future predictions without explicit supervision indicates the emergence of invariant object identities and properties. The work demonstrates the power of prediction in neural networks by showing the emergence of diverse knowledge through predictive objectives applied to raw pixel observations. Agents were trained using the IMPALA framework, with parallel 'actors' collecting experience and one learner performing learning updates. This research aims to inspire future studies on evaluating predictive agents using natural language interactions. The agent network is unrolled during a learning update to evaluate losses and compute gradients. A residual network with 6 64-channel ResNet blocks is used for frame input processing. A 2-layer LSTM with 256 hidden units per layer is the recurrent core for all agents. A linear layer reduces the frame encoding size to 500 dimensions, and a single layer MLP computes a value baseline and action logits. The simulation network for predictive agents is initialized with the agent state at a random time and unrolled forward for up to 16 steps. It uses the resulting LSTM hidden state as conditional input for prediction loss. The output of the simulation network conditions a Convolutional DRAW, a deep variational auto-encoder with recurrent encoder and decoder. The architecture includes a recurrent prior network for latent variables, replicating the CPC|A model from Guo et al. (2018). The simulation network outputs a conditioning vector after k steps, concatenated with the frame embedding z t`k for discrimination by an MLP. The discriminator classifies true and false embeddings, with negative examples from random time points in the batch. The architecture involves a recurrent prior network for latent variables, replicating the CPC|A model. The simulation network outputs a conditioning vector after k steps, concatenated with the frame embedding for discrimination by an MLP. The discriminator classifies true and false embeddings, with negative examples from random time points in the same batch of trajectories. The question string is tokenized to words and mapped to integers for vocabulary indices, used to lookup 32-dimensional embeddings. A 64-unit single-layer LSTM is unrolled for 15 steps to compute the language representation. Answers are decoded using a second LSTM initialized with the internal state of the agent's LSTM, unrolled for a fixed number of steps. The terminal state is processed through a two-layer MLP to compute answer logits and output the top-1 answer. Hyper-parameter values used in all experiments are listed in Table 3. The environment in the experiment is a single L-shaped 3D room with various objects. Each episode lasts 30 seconds with 900 steps, providing the agent with two observations at each step. The reconstruction loss is lower for the CPC|A agent compared to the LSTM agent, but the QA accuracy is not better. The lack of useful representations in the LSTM agent state affects the QA accuracy. The environment provides the agent with two observations: a first-person view image and the question text. The agent can interact with the environment through various actions like movement and object manipulation. Rewards are given as ground-truth answers instead of traditional rewards. Objects are created and placed randomly in the environment for different tasks. In a setup similar to DeepMind Lab, a rectangular room is populated with objects of different shapes and colors. Agents are trained to navigate and observe all objects by answering questions about their colors. Constraints are checked to ensure accurate placement of objects near specific locations. The DeepMind Lab environment features a rectangular room with 6 randomly selected objects out of a pool of 20 different objects of different colors. Question-answering accuracies show that the SimCore agent has the highest accuracy, while CPC|A and the vanilla LSTM agent perform similarly. The experimental setup in DeepMind Lab did not change any hyperparameters, demonstrating the approach's versatility across different environments."
}