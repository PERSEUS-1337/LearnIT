{
    "title": "rkezdaEtvH",
    "content": "Reinforcement learning (RL) typically uses a discount factor in the Markov Decision Process for future rewards. Humans and animals show hyperbolic time-preferences, leading to a proposed deep RL agent with hyperbolic discounting. This approach approximates hyperbolic discount functions and learns value functions over multiple time-horizons effectively. The discount factor in reinforcement learning (RL) is crucial for future rewards, with a value between 0 and 1. It devalues rewards over time, influencing learning dynamics and convergence properties of RL algorithms. The magnitude of the discount factor is considered a hyperparameter for optimization. The discount factor in reinforcement learning establishes priors over learned solutions. The chosen magnitude of \u03b3 sets an effective horizon for the agent, imposing a time-scale on the environment. Exponential discounting of future rewards may not align with measured value preferences in humans and animals, who instead discount future returns hyperbolically. The discrepancy between exponential discounting of future rewards and hyperbolic time-preferences in humans and animals may be due to maintaining uncertainty over the hazard rate in the environment. Hyperbolic time-preferences are mathematically consistent with this uncertainty, where the hazard rate measures the risk of early death in the environment. The prior belief of the hazard rate implies a specific discount function. The hazard rate \u03bb = e \u2212\u03b3 implies a specific discount function. Humans and animals exhibit hyperbolic discounts with shallower declines for large horizons, while RL agents optimize exponential discounts with a constant rate. Common RL environments introduce risk through techniques like no-ops and sticky actions. Discounting serves as a tool to ensure that policies deployed in the real world perform well even under risks they were not trained under. An algorithm is proposed to approximate hyperbolic discounting while building on successful Q-learning. The algorithm approximates hyperbolic discounting by learning many Q-values with different discount factors \u03b3, demonstrating efficacy in the Pathworld environment. It also explores the benefits of hyperbolic discounting in higher-dimensional deep RL agents in the ALE. The study shows that modeling a set of agents can approximate hyperbolic discounting and generalize to other non-hyperbolic discount functions using deep neural networks. It also demonstrates the benefits of learning a set of Q-values as an auxiliary task, which improves over a state-of-the-art baseline in the ALE environment. This challenges the traditional RL paradigm of learning policies through a single discount function by formulating MDPs with hazards present. The hazardous MDP is defined by the tuple < S, A, R, P, H, d >, where the learner observes samples from the environment transition probability P. The environment emits a bounded reward r. The hazardous MDP is defined by the tuple < S, A, R, P, H, d >, where the learner observes samples from the environment transition probability P. The environment emits a bounded reward r. The episodic MDP has a hazard distribution H, with a hazard \u03bb sampled at the beginning of each episode. The hazard modifies the transition function P \u03bb by discounting with e^-\u03bb. Additionally, a general discount function d(t) is considered, making a connection to partially observable Markov Decision Process (POMDP). The hazardous MDP is defined by the tuple < S, A, R, P, H, d >, where the learner observes samples from the environment transition probability P. The environment emits a bounded reward r. The hazardous MDP setting shows an equivalence between the value function of an MDP with a discount and MDP with a hazard distribution. This connection is illustrated by the hazard modifying the transition function P \u03bb by discounting with e^-\u03bb. Equation 2) describes how an agent acts in a hazard-free environment and discounts future rewards. The alternative Q-value is for when the agent acts under a hazard rate but does not discount future rewards. This formulation connects hazard rate and discount functions, showing their equivalence for generalized Q-values in reinforcement learning. The paper also demonstrates how exponentially-discounted Q-values can be repurposed to compute hyperbolic discounted Q-values. The paper proposes a scheme to calculate hyperbolic and other non-exponential discounted Q-values, overcoming the challenge of using non-exponential discount strategies in reinforcement learning algorithms. The scheme is based on the exponential weighting condition, allowing for the reuse of TD methods without being limited to exponential discounting. The paper introduces a method to compute hyperbolic and other non-exponential discounted Q-values by satisfying the exponential weighting condition. This approach enables the utilization of non-exponential discounting in reinforcement learning algorithms, expanding beyond traditional exponential discounting methods. The paper introduces a method to approximate discounting using a finite set of functions learned via standard Q-learning. A free hyperparameter is introduced to consider a total number of Q-values, each with their own discount factor. The approach emphasizes evaluating larger values of discount factors and computes the discounted sum of returns for each specific discount factor. The integral is estimated through a Riemann sum with a lower bound. The paper presents a method to approximate discounting using a set of functions learned through Q-learning. It emphasizes evaluating larger discount factors and computing the discounted sum of returns for each factor. Hyperbolic discounting is beneficial in uncertain hazard and intertemporal decision scenarios. In uncertain hazard and intertemporal decision scenarios, hyperbolic discounting is advantageous for approximating discounting using learned functions through Q-learning. The method trains an agent in Pathworld to maximize hyperbolically discounted returns under no hazard, while evaluating undiscounted returns subject to hazard, resulting in an agent robust to environmental hazards. The agent in Pathworld makes decisions on which paths to investigate, facing dynamic risks and rewards. Longer paths offer higher rewards but also higher risks of dying due to unobserved hazards. This environment differs from the adjusting-delay procedure. In Pathworld, the agent faces dynamic risks and rewards when choosing paths, with longer paths offering higher rewards but also higher risks of dying due to unobserved hazards. The environment differs from the adjusting-delay procedure and determines time-preferences through risk to the reward rather than variable-timing of rewards. The true hazard parameter in the prior was k = 0.05, and exponential discounting is found to be incorrect in this hazardous setting. In Pathworld, the agent faces dynamic risks and rewards when choosing paths, with longer paths offering higher rewards but also higher risks of dying due to unobserved hazards. The true hazard parameter in the prior was k = 0.05, and exponential discounting is found to be incorrect in this hazardous setting. At deployment, the agent must deal with dynamic levels of risk and faces a non-trivial decision of which path to follow. Even with tuned parameters, the agent fails to capture the functional form and achieves a high error over all paths. Further experiments show the benefit of appropriate priors when the agent's prior over hazard does not exactly match the environment's true hazard rate. The Rainbow variant from Dopamine implements distributional RL, predicting n-step returns, and prioritized replay buffers. The agent maintains a shared state representation and computes Q-value logits for each discount function, allowing for policies based on different effective horizons. Hyperparameter details are provided in Appendix K. The Hyper-Rainbow agent, built on the base Rainbow agent, shows strong performance improvements on a subset of Atari 2600 games. Changes include using hyperbolic Q-values for behavior policy and learning over multiple horizons. Hyper-Rainbow outperforms the base agent on 14 out of 19 games, indicating significant improvements. An ablation study is conducted to understand the driving factors behind this improvement, with the second modification resembling an auxiliary task. The Multi-Rainbow agent, introduced as an auxiliary task, maximizes rewards over multiple horizons in comparison to the Hyper-Rainbow agent. Performance improvements in Atari 2600 games are similar whether using hyperbolic or exponentially discounted Q-values. The Multi-Rainbow agent shows performance improvement in Atari games through a multi-horizon auxiliary task. Strong empirical benefits are found over the state-of-the-art Rainbow agent. The Multi-Rainbow agent demonstrates performance enhancement in Atari games with a multi-horizon auxiliary task, showing consistent improvement in 9 out of 10 games compared to the base C51 agent. However, the current implementation of Multi-Rainbow does not effectively utilize the prioritized replay buffer, impacting four out of ten games negatively. Hyperbolic discounting in economics is a well-studied concept, with differing interpretations proposed by various researchers. Dasgupta and Maskin (2005) propose a softer interpretation than Sozou (1998) on hyperbolic discounting, showing uncertainty over rewards timing can lead to preference reversals. Maia (2009) suggests models that discount quasi-hyperbolically despite exponential discounting, while Alexander and Brown (2010) introduce hyperbolically discounted temporal difference (HDTD) learning. TD-learning has been used in behavioral reinforcement learning and neuroscience. The text discusses modeling behavioral reinforcement learning using TD-learning, which computes errors based on expected and actual values. Traditional exponential discounting in value estimates is challenged by empirical evidence, leading to the proposal of hyperbolic discounting as an alternative. The \u00b5Agent is introduced as an agent that models the value function with a specific discount factor \u03b3. The \u00b5Agent models the value function with a discount factor \u03b3, approximating hyperbolic discounting well. Using a deep neural network, different Q-values are modeled from a shared representation for more flexible discounting in reinforcement learning. Researchers have adopted flexible versions beyond a fixed discount factor, studying optimal policies with different discount factors. Introducing the discount factor as an argument for a set of timescales is considered. Discount factor as an argument for different timescales is explored in various frameworks such as Horde, \u03b3-nets, and the Average Reward Independent Gamma Ensemble. Different discount functions, including time-inconsistent preferences, have been proposed. Strategies for improving training stability through dynamic discount factors have also been suggested. Meta-learning approaches and state-action dependent discount rates have been studied to address rational decision making in sequential processes. In RL, multiple time scales have been explored, with previous work focusing on MDPs with multiple reward functions and discount factors. Recent studies have shown the effectiveness of using different discount factors for intrinsic and extrinsic rewards. Additionally, a new algorithm, TD(\u2206), has been proposed to break down the value function. The TD(\u2206)-algorithm by Romoff et al. (2019) breaks down a value function into smaller discount factors. Auxiliary tasks in reinforcement learning have been beneficial, aiding representation learning. Multiple horizons learning challenges the traditional RL approach of maximizing returns with a single discount factor. By learning over multiple horizons simultaneously, algorithms have been broadened to enable acting according to new discounting schemes. This method approximates hyperbolic discounting well and performs better in hazardous MDP distributions. However, it does not fully capture general aspects of risk, as hazard may be a function of time and a joint property of both policy and environment. There is an interplay between time-preferences and policy that creates a complicated circular dependency. Recent work has proposed state-action dependent discounting to address the interplay between time-preferences and policy. Time preferences are formalized based on the probability of agent survival, where future rewards are discounted accordingly. The hazard rate is defined as a measure of risk over time. Equation 14 defines the hazard rate as the negative rate of change of log-survival at time t. Sozou (1998) shows that beliefs about risk in the environment lead to specific discounting functions. Exponential discounting is used when risk occurs at a constant rate, while uncertainty in the hazard rate leads to hyperbolic discounting. In RL, exponential discounting is assumed when the hazard rate is known to be constant. The survival rate equation is s(t) = e^(-\u03bbt) in this scenario. The survival rate solution s(t) = e^(-\u03bbt) is related to the RL discount factor \u03b3, interpreting it as the per-time-step probability of the episode continuing. The hazard rate \u03bb \u2208 [0, \u221e] is connected to the discount factor \u03b3 \u2208 [0, 1), where increasing hazard makes \u03b3 more myopic and decreasing hazard makes \u03b3 more far-sighted. In RL, a single \u03b3 is commonly chosen based on the belief of a known constant hazard rate \u03bb = -ln(\u03b3). However, relaxing this assumption allows for uncertainty in the environment's hazard rate from a Bayesian perspective. The hazard rate is reflected in the survival rate through a hazard prior p(\u03bb). Different hazard rate priors correspond to different discount functions in reinforcement learning. Maintaining a delta-hazard leads to exponential discounting, while different priors result in non-exponential discount functions. In reinforcement learning, different hazard rate priors lead to non-exponential discount functions. Priors with uncertainty over the hazard rate can imply new discount functions. Validating experiments show the robustness of estimating hyperbolic discounted Q-values in dynamic risk environments. Hyperbolic discounting is shown to be preferable to exponential discounting in dynamic risk environments, even with mismatched priors. This results in a higher prediction error of value but performs more reliably, with a cumulative lower error. The average mean squared error increases as the prior moves further away from the true hazard rate. However, errors for large changes in the hazard rate result in generally lower errors compared to using a single exponential discount factor. The study compares hyperbolic discounting with exponential discounting in dynamic risk environments. Hyperbolic discounting shows better performance with hazards drawn from a uniform distribution. The hyperbolic Q-values are related to standard Q-learning values. The hyperbolic discount factor \u0393 k (t) can be expressed as a weighting over exponentially discounted Q-values, suggesting a connection to standard Q-learning. By considering an infinite set of exponential discount factors \u03b3, hyperbolic discounts for the corresponding time-step t can be derived. This idea of modeling many \u03b3 is further explored in the study. The text discusses how weights can be expressed as a weighting over exponential discount functions, showing a connection between hyperbolic discounting and hazard distribution. It explains how discount factors can be used to learn robust policies and how hazard priors can be computed through integrating specific weightings and discount functions. The table matches hazard distributions to discounting functions per Lemma 3.1. A Dirac Delta Prior over hazard rate \u03b4(\u03bb \u2212 k) is typically used in RL. Derivations can be found in Appendix F. A delta prior on the hazard rate is consistent with exponential discounting. Changing variables yields differentials and limits. The hazard rate value \u03bb = k is equivalent to the \u03b3 = e \u2212k. A single discount factor \u03b3 k is equivalent to the prior that a single hazard exists in the environment. By holding a uniform prior over hazard, the Laplace transform yields the discounting scheme. Choosing a \u03b3 max, the largest \u03b3, is motivated for learning. The largest discount factor \u03b3 max is chosen for learning, with an exponentiation base to bound it at a stable value. This induces an approximation error in the hyperbolic discount, increasing as \u03b3 max decreases. The approximation becomes more accurate as \u03b3 max approaches 1, supported by Table 5. The approximation becomes more accurate as supported in Table 5 up to small random errors. We estimate the hyperbolic discount in two different ways using lower-bound Riemann sums. Two different integrals are considered for computing the hyperbolic coefficients, with the Laplace transform showing sharp peaks as \u03b3 \u2192 1. The difference in integrals is visually apparent in Figure 13. By evaluating the integral up to \u03b3 max instead of 1, an approximation error is induced which increases with t. The hyperbolic discounting scheme is approximated using lower-bound Riemann sums, with sharp peaks in the Laplace transform as \u03b3 \u2192 1. A new prioritization scheme based on the largest \u03b3 value is considered for Q-values, showing performance improvement over previous techniques. Prioritizing according to TD-errors computed from the largest gamma shows performance improvement over averaging in Atari Learning Environment. Multi-horizon auxiliary task results on Rainbow (Multi-Rainbow) are presented in Table 7."
}