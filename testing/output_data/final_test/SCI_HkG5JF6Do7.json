{
    "title": "HkG5JF6Do7",
    "content": "There are various techniques for inducing structured sparsity in parametric models to enable resource-efficient inference. However, none of these techniques specifically target a desired number of floating-point operations (FLOPs) as part of the optimization objective. This work extends a state-of-the-art technique to incorporate FLOPs directly into the optimization objective, allowing practitioners to specify a target number of FLOPs during model compression. The study demonstrates that different neural networks can be successfully trained for image classification while meeting a desired FLOPs requirement. Neural networks can be trained for image classification, but their heavy computational requirements limit deployment on resource-constrained devices. Recent works focus on designing resource-efficient models and compressing popular architectures. Techniques like network slimming have achieved high accuracy with reduced FLOPs. In this work, a sparse neural network is learned in a single training run, aiming to reduce FLOPs while maintaining accuracy. Previous methods do not incorporate FLOPs constraints in end-to-end optimization, using them as heuristics instead. The number of FLOPs is considered a proxy measure for latency and energy usage. Incorporating the number of FLOPs as part of the optimization objective allows for FLOPs-guided compression and setting specific compression targets. The FLOPs objective is defined as a function that associates FLOPs with hypothesis, enabling more principled compression strategies. The FLOPs objective is defined as a function that associates FLOPs with hypothesis, allowing for compression strategies. The empirical risk for a dataset D is controlled by hyperparameters \u03bb f and T. The objective is relaxed to a surrogate of the evidence lower bound with a spike-and-slab posterior as the variational distribution, incorporating a sparsity-inducing prior. Louizos et al. propose using a hard concrete distribution for efficient reparameterization and exact zeros. This distribution is a modified version of the binary Concrete distribution. Sampling from the equivalent Bernoulli parameterization is more efficient than sampling from hard concrete. The optimization is done via the reparameterization trick for the first term and the score function estimator for the second term due to the non-differentiability of the FLOPs objective. The FLOPs objective is non-differentiable, so the reparameterization trick cannot be used. High variance is not an issue as FLOPs are fast to compute. The deterministic estimator for final parameters involves a sigmoid function. Computational savings are achieved when the model is sparse across parameter groups. FLOPs for a 2D convolution layer are defined based on kernel size and input dimensions. The FLOPs objective is computed based on kernel size, input dimensions, padding, and number of input channels. The number of FLOPs for a fully-connected layer is determined by the number of input neurons. Results are reported on MNIST, CIFAR-10, and CIFAR-100 datasets with different FLOPs targets. In experiments, multiple models are trained on datasets with different FLOPs targets using specific initialization and hyperparameters. The network is pruned at epoch 190, and the score function estimator draws 1000 samples at each optimization step. Various models with different FLOPs percentages and sizes are evaluated on MNIST, CIFAR-10, and CIFAR-100 datasets. In experiments, models are trained on datasets with different FLOPs targets using specific initialization and hyperparameters. The network is pruned at epoch 190, and various models with different FLOPs percentages and sizes are evaluated on MNIST, CIFAR-10, and CIFAR-100 datasets. The experiments show that methods achieve comparable accuracy to previous approaches while using fewer FLOPs, with the convolution layers being the most aggressively compressed. In experiments, models are trained on datasets with different FLOPs targets using specific initialization and hyperparameters. The network is pruned at epoch 190, and various models with different FLOPs percentages and sizes are evaluated on MNIST, CIFAR-10, and CIFAR-100 datasets. The experiments show that methods achieve comparable accuracy to previous approaches while using fewer FLOPs, with the convolution layers being the most aggressively compressed. In the last two rows, the median error rate of five runs across two models for each dataset is reported, along with expected and actual FLOPs values. For CIFAR-10, the approaches result in Pareto-better models with decreased error rates and inference-time FLOPs. However, for CIFAR-100, the tradeoff between accuracy and efficiency is noted, with the acceptability depending on the end application."
}