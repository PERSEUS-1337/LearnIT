{
    "title": "rklp93EtwH",
    "content": "In this paper, an automated relational meta-learning (ARML) framework is proposed to address the challenge of task heterogeneity in meta-learning. The framework automatically extracts cross-task relations and constructs a meta-knowledge graph, allowing for quick adaptation to new tasks. This approach aims to overcome limitations of traditional meta-learning methods and task-specific approaches by leveraging knowledge organization from knowledge bases. The ARML framework addresses task heterogeneity with a learned meta-knowledge graph, enhancing model interpretability. Extensive experiments show its superiority over existing methods in 2D regression and few-shot image classification. Meta-learning is crucial for transferring prior knowledge to new tasks in various applications like computer vision and natural language processing. Existing meta-learning algorithms typically use a globally shared meta-learner. Recent works have focused on addressing task heterogeneity in meta-learning by customizing globally shared meta-learners with task-specific information. This approach aims to optimize each heterogeneous task effectively by tailoring transferred knowledge. However, the expressiveness of these methods is limited, posing a challenge in dealing with task heterogeneity. Recent works have focused on addressing task heterogeneity in meta-learning by customizing globally shared meta-learners with task-specific information. Choi (2018) and Yao et al. (2019b) proposed methods to balance customization and generalization, but limitations exist in knowledge generalization between tasks. To overcome this, a framework is proposed to automatically extract relational structures from historical tasks and leverage them for knowledge customization on new tasks, inspired by knowledge bases like knowledge graphs. Automated Relational Meta-Learning (ARML) is a framework that automatically builds a meta-knowledge graph from historical tasks to enhance learning efficiency by leveraging relational structures between entities. The proposed ARML framework constructs a meta-knowledge graph from historical tasks, utilizing prototype-based relational graphs to capture relationships and enhance task representation. This graph is then used to acquire relevant knowledge for new tasks, improving the training process. The proposed ARML framework automatically constructs a meta-knowledge graph to facilitate learning new tasks, outperforming state-of-the-art meta-learning algorithms. It captures task relationships and improves interpretability. Meta-learning aims to quickly learn new tasks with few training examples through black-box amortized methods, gradient-based methods, and non-parametric approaches. Our work builds upon gradient-based meta-learning methods, focusing on handling heterogeneous tasks by customizing initialization for different tasks. This approach aims to improve the effectiveness of learning processes on new tasks, addressing the limitations of globally shared meta-learners. Few-shot learning aims to improve learning processes on new tasks by customizing initialization for different tasks. ARML leverages relevant structures from an automatically constructed meta-knowledge graph, enhancing model interpretability and effectiveness of meta-learning algorithms. The goal of few-shot learning is to optimize parameters for regression and classification tasks using meta-learning. Meta-learning involves sampling tasks from a distribution to improve performance by leveraging knowledge from multiple tasks. The algorithm aims to find a well-generalized meta-learner on training tasks during the meta-learning phase. Model-agnostic meta-learning (MAML) is a representative algorithm that learns a well-generalized initialization during the meta-training process. At the meta-testing phase, the adaptive parameter for new tasks is obtained by finetuning the initialization through gradient updates. The proposed ARML is introduced in this section. The proposed ARML aims to facilitate learning new tasks by leveraging transferable knowledge from historical tasks. It introduces a meta-knowledge graph to organize and memorize historical knowledge, enhancing prototype representations for fast adaptations. Key components include prototype-based sample structuring, automated meta-knowledge graph construction, and task-specific knowledge fusion and adaptation. The proposed ARML method enhances prototype representations for fast adaptations by utilizing a prototype-based relational graph to investigate relationships among samples. This graph helps alleviate issues caused by abnormal samples and is constructed based on prototypes rather than raw samples. The ARML method enhances prototype representations for fast adaptations by utilizing a prototype-based relational graph to investigate relationships among samples. It interacts with a meta-knowledge graph to acquire relevant historical knowledge and tailors the initialization based on class information. The method enhances prototype representations for fast adaptations by clustering samples into K clusters, forming prototype representations as vertices in a relational graph. Edge weights are defined using learnable parameters and a Sigmoid function. The adjacency matrix indicates proximity between prototypes, organizing and distilling knowledge from historical data. To enhance prototype representations for fast adaptations, a meta-knowledge graph is constructed and maintained to distill knowledge from historical learning processes. This graph allows for efficient identification of relational knowledge from previous tasks, optimizing the training of new tasks. The meta-knowledge graph is automatically constructed during the meta-training phase, improving the speed and effectiveness of training experiences. During meta-training, a meta-knowledge graph is constructed to distill knowledge from historical learning processes. The vertex representations are dynamically updated to encourage diversity and are randomly initialized. This graph optimizes training by efficiently identifying relational knowledge from previous tasks. The prototype-based relational graph Ri is constructed with learnable parameters to enhance learning of new tasks with historical knowledge. A super-graph Si is created by connecting Ri with the meta-knowledge graph G for each task Ti, optimizing both graph representations simultaneously during meta-training. Links are established between Ri and G to combine their vertices and edges in the super-graph. The prototype-based relational graph Ri is linked with the meta-knowledge graph G through weighted connections based on similarity. A super-graph Si is formed, allowing relevant knowledge propagation from G to Ri using Graph Neural Networks (GNN) with a message-passing framework. After stacking L GNN layers, the information-propagated feature representation for the prototype-based relational graph Ri is obtained. To address task heterogeneity, task-specific information is incorporated to customize the globally shared meta-learner using a modulating function based on well-discriminated task representations. The function introduces two reconstructions using auto-encoders to create a stable task-specific meta-learner. CR i represents raw prototype information, while \u0108R i gives prototype representations after absorbing knowledge from a meta-knowledge graph. An aggregator encodes CR i into a dense representation, which is then decoded to achieve reconstructions. Similarity between prototypes and meta-knowledge vertices is computed. The super-graph Si is constructed by computing similarity between prototypes and meta-knowledge vertices. A GNN is applied to get the information-propagated representation. Task representations qi and ti are obtained by aggregating encoded dense representations and reconstruction errors. The modulating function tailors task-specific information to the globally shared. The modulating function tailors task-specific information to the globally shared initialization \u03b80 using learnable parameters Wg and bg. The overall objective function of ARML combines reconstruction loss Lt and Lq with meta-learning loss, with balancing parameters \u00b51 and \u00b52. The meta-training process of ARML is outlined in Alg. 2, while details of the meta-testing process are in Appendix A. Extensive experiments demonstrate the effectiveness of ARML on 2D regression and few-shot tasks. Extensive experiments compare ARML with various meta-learning methods for 2D regression and few-shot classification. Different types of baselines are considered, including globally shared and task-specific methods. The implementation includes GRU as the encoder and decoder, one layer GCN with tanh activation, and testing different modulation networks. Sigmoid modulation is found to be the most effective. In the experiment, sigmoid modulation was found to achieve the best performance. The study focused on a 2D regression problem to model complex relational structures. Results showed that ARML outperformed other gradient-based meta-learning methods in 10-shot 2D regression tasks. ARML achieves superior performance compared to other gradient-based meta-learning methods in 10-shot 2D regression tasks. The meta-knowledge graph is crucial for capturing task-specific information, as shown in the heatmap between prototypes and meta-knowledge vertices. The activation patterns of different functions in the graph illustrate the similarities and differences between 2D and 3D functions. In the few-shot classification problem, a meta-knowledge graph is used to filter links with low similarity scores. V3 is the most popular vertice connected with V1, V5, and V4. The dataset includes four fine-grained image classification datasets (Aircraft, FGVCx-Fungi). The difficulty of the classification problem is increased by introducing two image datasets. The difficulty of few-shot classification is increased by applying blur and pencil filters to images in PlainMulti, creating the Art-Multi dataset. Different filters simulate changing distributions of tasks. The datasets are divided into meta-training, meta-validation, and meta-testing classes, with N-way K-shot settings used for training and testing. Standard convolutional layers are used as the base learner for ARML and baselines for fair comparison. The number of vertices in the meta-knowledge graph for Plain-Multi and Art-Multi datasets are 4 and 8, respectively. Experimental results for Plain-Multi and Art-Multi datasets are presented in Table 1 and Table 2, showing task-specific gradient-based models outperform globally shared models. The experimental results show that ARML outperforms globally shared models and other task-specific gradient-based meta-learning methods. The performance gap between ARML and HSML highlights the benefits of relational structure over hierarchical clustering. Additionally, ARML surpasses methods from other research lines and the effectiveness of its components is demonstrated through ablation studies. Extensive qualitative analysis is conducted on the meta-knowledge graph, a key component in ARML. The constructed meta-knowledge graph in ARML is analyzed by randomly selecting tasks and examining the similarity heatmap between prototypes and vertices. V1 is found to be mainly activated by bird and aircraft, reflecting specific features in the graph. The meta-knowledge graph in ARML shows that V1 is mainly activated by bird and aircraft, while V2, V3, V4 are primarily activated by texture. V2 is important for representing texture and facilitating training on other subdatasets. V7 is activated by blur filters, indicating similarity in images with blur. V6, activated by aircraft, connects with V2 and V3, emphasizing the importance of texture for classifying aircraft. The ARML framework utilizes a meta-knowledge graph to extract relations across tasks, aiding in the classification of aircraft based on texture information. The proposed algorithm demonstrates effectiveness in handling heterogeneous tasks. Future investigations include exploring the semantic meaning within the meta-knowledge graph and interpreting the graph on the Art-Multi dataset. The ARML framework utilizes a meta-knowledge graph for task classification. Future work includes exploring semantic meaning within the graph and interpreting it on the Art-Multi dataset. The proposed model focuses on tasks with shared feature and label spaces. In the ARML framework, a meta-knowledge graph is used for task classification. The model involves applying GNN on a super-graph, aggregating representations, and computing task-specific initialization. Parameters are updated with specific step sizes and an embedding function is defined. Different settings are used for 2D regression and few-shot image classification tasks. In the ARML framework, a meta-knowledge graph is utilized for task classification. The model employs GNN on a super-graph, aggregates representations, and computes task-specific initialization. Parameters are updated with specific step sizes, and an embedding function is defined. Different settings are used for 2D regression and few-shot image classification tasks. The convolutional layers have 3x3 filters with a channel size of 32, followed by two fully connected layers with 384 and 128 neurons each. The autoencoder aggregator consists of GRUs, with a meta-batch size of 4 and 5 gradient steps in the inner loop. Various gradient-based baselines and non-parametric meta-learning algorithms are compared using similar settings for meta-training and meta-testing processes. Pencil and blur filters are used to alter the task distribution in the dataset. In this dataset, pencil and blur filters are used to change task distribution. Results are shown in Figure 4, indicating different data distributions. Comparison with other meta-learning models is presented in Table 3 and Table 4 for MiniImagenet and tieredImagenet. Task-specific models like ARML show similar performance on standard benchmarks due to task homogeneity. In Table 4, performance comparison on the 5-way, 1-shot tieredImagenet dataset is presented. ARML outperforms almost all baselines in every sub-dataset. Ablation study is performed in this section. The ablation study of the proposed ARML demonstrates the effectiveness of each component. Results on 5-way, 5-shot scenarios for Art-Multi and PlainMulti datasets are presented in Table 5 and Table 6. Ablation experiments show that structuring samples as prototypes improves performance by better handling relations and reducing anomalies. In ablation studies, the effectiveness of different components in ARML is demonstrated. Removing the meta-knowledge graph and using prototype-based relational graphs show improved performance. Changes in encoder/decoder structure and modulating functions have minimal impact on ARML's performance. Customized gates for each parameter and task representation learning through autoencoder structures are highlighted. The impact of vertice numbers in the meta-knowledge graph on ARML performance is investigated. Results show that performance saturates with around 8 vertices, suggesting that this number is sufficient to capture potential relations. Larger datasets with more complex relations may require more vertices. The impact of vertice numbers in the meta-knowledge graph on ARML performance is explored. Results indicate that around 8 vertices are adequate to capture relations, with more complex tasks potentially requiring additional vertices. A case study comparing HSML and ARML task structures is provided, showcasing how ARML automatically learns relations across tasks using a knowledge graph. In a comparison between HSML and ARML task structures, ARML demonstrates a more flexible approach by utilizing a graph structure to establish relations across tasks. The learned meta-knowledge and similarity heatmap between prototypes and meta-knowledge vertices are highlighted in the analysis. ARML offers greater potential to leverage previous tasks through prototypes and the learned meta-knowledge graph, as opposed to HSML's fixed hierarchical activation. Additional case studies, including 2D regression, further illustrate the capabilities of ARML. In Figure 8, 2D regression cases are shown, and Figure 9 illustrates additional Art-Multi cases supporting observations and interpretations."
}