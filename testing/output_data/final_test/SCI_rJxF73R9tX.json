{
    "title": "rJxF73R9tX",
    "content": "The deep abstaining classifier is a deep neural network trained with a unique loss function that allows for abstention during training. This helps improve performance on non-abstained samples by learning representations for structured noise, enabling robust learning in the presence of arbitrary noise, and serving as an effective out-of-category detector. Analytical results on loss function behavior enable automatic tuning of accuracy and coverage, with demonstrated utility on multiple image benchmarks. The deep abstaining classifier demonstrates significant improvement in learning with label noise. Machine learning algorithms are increasingly used in high-risk fields like medical diagnosis, autonomous vehicle control, and the legal sector. Knowing when to abstain from predicting is crucial for classifiers in situations where errors can have severe consequences. This trait allows decision-making to be directed to a human or a more accurate classifier when needed. In high-risk fields like medical diagnosis and autonomous vehicle control, knowing when to abstain from predicting is crucial for classifiers to avoid severe consequences. Recent research has focused on developing a framework for \"self-aware learning\" in deep networks, with a proposed technique for selective classification to optimize risk-vs-coverage profiles. This area has been under-explored, with previous works mainly focusing on post-processing settings for abstention formulations. The method introduced in this paper utilizes an abstention option while training deep neural networks by modifying the loss function to include an abstention output. This allows the DNN to learn features indicative of unreliable training signals and make uncertain predictions, particularly useful for eliminating structured noise. The paper introduces a method for utilizing an abstention option in training deep neural networks to effectively eliminate structured noise and interpret reasons for abstention. It also discusses using abstention as a data cleaner for filtering out noisy training data and for open-set detection in real-world systems. The contributions include introducing the deep neural network with an abstention option. The paper introduces the deep abstaining classifier (DAC), a DNN trained with a novel loss function to handle label noise. It demonstrates the DAC's ability to learn features associated with systematic label noise and its effectiveness as a data cleaner in the presence of arbitrary label noise. Additionally, the DAC is illustrated as an effective open-set detector that reliably abstains when presented with samples from unknown classes. The paper introduces the deep abstaining classifier (DAC) trained to handle label noise and act as a data cleaner. It discusses the loss function formulation, learning in the presence of noise, and open set detection with the DAC. The paper does not consider adversarial settings. The paper introduces the deep abstaining classifier (DAC) for handling label noise and data cleaning. It discusses the training of a k-class multi-class classifier with a deep neural network (DNN) and the modified cross-entropy loss function used for training the DAC. The paper introduces the deep abstaining classifier (DAC) for handling label noise and data cleaning. It discusses the training of a k-class multi-class classifier with a deep neural network (DNN) and the modified cross-entropy loss function used for training the DAC, which penalizes abstention based on a hyperparameter \u03b1. The penalty for abstention can be adjusted to drive the model to either always abstain or never abstain, depending on the value of \u03b1. The paper introduces the deep abstaining classifier (DAC) for handling label noise and data cleaning. It discusses the training of a k-class multi-class classifier with a deep neural network (DNN) and the modified cross-entropy loss function used for training the DAC, which penalizes abstention based on a hyperparameter \u03b1. The penalty for abstention can be adjusted to drive the model to either always abstain or never abstain, depending on the value of \u03b1. Achieving low cross-entropy on a sample allows the model to abstain, ensuring learning on true classes persists during gradient descent. The DAC introduces a threshold on \u03b1 where j is the true class for sample x. If only a small fraction of the mass over the actual classes is in the true class j, then the DAC will push mass into abstention class provided \u03b1 < DISPLAYFORM3. Auto-tuning on \u03b1 during training is performed with the algorithm given in Algorithm 1. \u03b2 is a smoothed moving average of the \u03b1 threshold, updated at every epoch. Abstention-free training is done for initial epochs to accelerate learning, triggering abstention from epoch L + 1 onwards. \u03b1 is initialized to a smaller value than \u03b2 to encourage abstention on all but the easiest examples learned so far, and is linearly ramped up as learning progresses on the true classes. In the experiments, the DAC is shown to learn abstention well with a linear ramp-up of \u03b1 over epochs. Label noise in real-world data can be systematic, affecting certain classes more than others due to various factors like confusing features or annotator unreliability. Systematic label noise in brain-computer interface applications has been documented, where noisy data is correlated with the participant's state. In scenarios with degraded image quality or uniformly labeled data, there are consistent indications in the input that correlate with label noise. Curating data to a clean set can be costly, especially with large training data requirements. Crowd-sourcing label annotations may not be feasible for sensitive data like patient records. In this paper, the focus is on exploring whether deep neural networks (DNNs) can learn feature mappings indicative of unreliable or confusing samples. Through abstention training, the DNN learns to abstain based on features associated with difficult samples, as demonstrated in experiments on image data. In experiments, a deep convolutional network is trained using the VGG-16 BID32 architecture in PyTorch BID27 framework. The network is trained for 200 epochs with specific settings and uses the labeled version of the STL-10 dataset with 5000 train and 8000 test images. The \u03b1 auto-update algorithm is used with a specific initialization factor. In the experiment, the \u03b1 auto-update algorithm is used with specific parameters. Labels are corrupted on the training set, and sometimes features in the validation set are perturbed. A distinguishing extraneous feature is added to images with randomized labels to simulate situations where the DAC should abstain from predicting. In experiments, a smudge feature (FIG0) is used to represent co-occurring label noise. A DAC and regular DNN are trained with cross-entropy loss, tested on a set with 10% smudged images. DAC learns noise representations and assigns abstention class, with performance measured using accuracy-vs-abstained curves. Post-DAC DNN performance is also evaluated. Results show that when trained on a non-corrupted set, the baseline DNN achieved over 82% test accuracy, dropping to under 75% on a label-randomized smudged set. The DAC abstained on most smudged images in the test set, with high precision for abstained smudged images. The DAC improves performance by abstaining on smudged images, showing high precision and recall. It has learned to associate smudges with unreliable data, treating them as a separate class for abstention. The DAC improves performance by abstaining on smudged images, showing high precision and recall. It significantly boosts accuracy by eliminating abstained data and re-training a DNN, resulting in better risk-coverage curves. This post-DAC DNN outperforms the baseline DNN and enhances the SGR method's performance as well. The DAC complements uncertainty quantification methods like SGR and softmax thresholding, improving overall performance. In a more challenging experiment, a scenario is simulated where a class is prone to mislabeling, with randomized labels for monkeys in the training set. Unlike the previous experiment, the DAC now has to learn complex features over a real-world object. The DAC now has to learn complex features over a real-world object with more intra-class variance. Results show good abstention recall and precision, with the DAC associating monkey features with the abstention class due to label randomization. The DAC learns complex features over real-world objects with more intra-class variance, redirecting the mapping towards the abstention class. The baseline DNN's performance on monkey images is poor, with a high number of confident but incorrect predictions. The DAC shows a small but consistent advantage in risk-vs-coverage compared to the baseline DNN. It can reliably abstain on samples where noise is correlated with an underlying feature. The experiments indicate that the DAC performs well on images it does not abstain from, especially on non-monkey images. Visualization techniques like guided back-propagation and class-based activation maps provide explanations for DNN predictions. The DAC model successfully abstained on smudged and monkey images, with smudging dominating features when abstaining. Visualizations using class-based activation maps show how the DAC behaves in different scenarios. The DAC model abstained on smudged images due to the dominance of the smudge, while correctly predicting images without smudges. The model also correctly identified monkey features leading to abstention on monkey images. This confirms the hypothesis that the DAC effectively maps features to abstention labels. The DAC effectively mapped monkey features to abstention labels and demonstrated its utility in structured noise settings. The model abstained on smudged images and correctly predicted images without smudges. In the presence of unstructured noise, classification performance degrades, with label noise being more harmful than feature noise. Unlike previous works, the DAC does not model label flipping probabilities in detail but assumes a fraction of labels are uniformly corrupted, approaching the problem from a data-cleaning perspective. The DAC can be used to identify noisy samples in the training set by training the DAC and observing its performance on a validation set. If there is training error at the point of best validation error, it indicates label noise, and the DAC is more likely to abstain on such samples for elimination from the training set. The DAC is used to identify noisy samples in the training set by training and observing its performance on a validation set. Noisy samples are eliminated from the training set for subsequent training using regular cross-entropy loss. Experimental setup involves comparing two recent models for training with noisy labels on image data. The DAC is used to identify noisy samples in the training set by training and observing its performance on a validation set. Noisy samples are eliminated from the training set for subsequent training using regular cross-entropy loss. Results in Table 1 show that by eliminating noisy samples using the DAC and training with a cleaner set, significant performance improvement is achieved over comparison methods in most cases, especially for challenging datasets like CIFAR-10 and CIFAR-100. The DAC consistently outperforms the noisy baseline on challenging datasets like CIFAR-10 and CIFAR-100. Even with a significant amount of data eliminated by the DAC, its performance remains comparable to other methods. This approach effectively identifies and eliminates noisy labels for improved classification performance, a novel concept in deep learning. The DAC method improves classification performance by filtering out noisy labels without needing to estimate label confusion matrices or make assumptions about label noise levels. It outperforms noisy baselines on datasets like CIFAR-10 and CIFAR-100, even with a significant amount of data filtered out. The DAC approach filters out noisy labels to improve classification performance without needing to estimate label confusion matrices or assume noise levels. It outperforms noisy baselines on datasets like CIFAR-10 and CIFAR-100, even with a significant amount of data filtered out. The results demonstrate the performance benefit of data cleaning for robust deep learning in the presence of label noise and the effectiveness of the abstaining classifier in cleaning noise. Abstention can counter the memorization tendency of DNNs but may not always prevent it. As the abstention rate approaches 0 or 1 over training epochs, the DAC may abstain on all samples, halting learning. Unless \u03b1 is close to 0, abstention eventually decreases to zero as training progresses. The DAC's behavior of abstaining on smudged samples decreases as training progresses, with steep reductions in abstention rate at epochs 60 and 120 coinciding with learning rate decay. This leads to a memorization phase where the DAC finds complex decision boundaries to fit random labels, resulting in decreased generalization performance. In an open-world detection scenario, the DAC abstains when encountering samples from unseen classes. Recent works focus on open-set and out-of-distribution detection for deep networks using post-training calibration methods. The authors propose a feature-learning approach for the DAC to decide when to abstain in out-of-distribution detection. This method aims to suppress context-dependent features based on associations made during training. The DAC uses fixed features to detect out-of-category images, with known classes showing more salient activations than the smudge feature. Unknown classes result in the smudge being the most salient feature, leading to abstention during training. The DAC can effectively abstain based on the presence of the smudge, suppressing context-dependent features. The DAC uses fixed features to detect out-of-category images, with known classes showing more salient activations than the smudge feature. During training, the DAC abstains unless there are other class-specific features that cause non-abstention. The DAC learns optimal weights on activations per-class and per-feature, functioning as a threshold-based detector optimized for classification. The DAC is a threshold-based detector optimized at the feature level during training. Inference involves augmenting samples with X, and abstention is based on the presence of known class features. Filter visualizations in Figure 5 demonstrate this concept, using a fixed feature like the smudge. Out-of-category detection results on STL-10 and Tiny ImageNet show that DAC activations for class features are more salient than the smudge for in-class data. Conversely, out-of-class data highlights the smudge as the most salient feature, leading to abstention. Real-world image datasets are used for detection experiments, treating them as in-sample and out-of-sample in separate tests. The Deep Abstaining Classifier (DAC) is effective in open-domain situations, as shown in Table 2. It is trained on a novel loss function to learn abstention during training. The DAC serves as a representation learner in the presence of structured noise, a data cleaner in the presence of arbitrary noise, and an out-of-category detector. While adversarial settings were not considered, the DAC could be part of defense strategies. The DAC, including abstention, can be a defense against adversarial attacks. Results show DNNs can self-calibrate and abstain effectively. DAC performs well on blurred images in the test set while maintaining high accuracy on other samples. The DAC, including abstention, performs well on blurred images in the test set, maintaining high accuracy on other samples. Abstention behavior evolves during training, with the DAC continuing to abstain on about 20% of the training data, indicating a strong association between blurring and abstention."
}