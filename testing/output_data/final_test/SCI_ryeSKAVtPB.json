{
    "title": "ryeSKAVtPB",
    "content": "Deep Neural Networks (DNNs) are vulnerable to white-box targeted attacks. This paper proposes learning ordered Top-k attacks to enforce specific predicted labels on adversarial examples. Two methods are presented: an extension of the Carlini-Wagner method and an adversarial distillation framework. The framework involves computing adversarial probability distributions for targeted labels and minimizing the Kullback-Leibler divergence between the distributions. In experiments on ImageNet-1000 val dataset, Top-k attacks were tested using ResNet-50 and DenseNet-121 DNNs. Adversarial distillation approach showed significant improvement in attack success rate and perturbation energy reduction, especially with limited computation budget. Despite advancements, DNNs remain vulnerable to adversarial attacks. Adversarial attacks can easily fool image classification networks with visually imperceptible perturbations, hindering their deployment in applications like autonomous driving and medical diagnosis. This paper focuses on learning visually-imperceptible targeted attacks in image classification tasks under the whitebox setting. Most existing methods address targeted attacks in the Top-1 manner, where success is determined by predicting a randomly selected label as the Top-1 label. The evaluation results of different attack methods show that the widely used C&W method does not push the ground-truth labels very far in the prediction of adversarial examples. If the ground-truth labels still largely appear in the Top-5 of the prediction, there may be over-confidence in the attack success rate. The evaluation results of different attack methods show that C&W method does not push ground-truth labels far in prediction of adversarial examples. Untargeted attack approaches are better at pushing GT labels, but with larger perturbation energies. \"Robust\" attack methods can be developed by combining advantages of both types of attacks. Targeted Top-1 attack setting could limit effectiveness. The paper focuses on developing ordered Top-k targeted attacks to enforce specific predicted labels on adversarial examples. Two methods are presented, including an extension of the C&W method and an adversarial distillation framework. These methods aim to create imperceptible perturbations through iterative back-propagation. The paper introduces an adversarial distillation (AD) framework for generating adversarial examples by minimizing the Kullback-Leibler (KL) divergence between adversarial and predicted distributions. This framework focuses on creating targeted attacks with ordered Top-k labels, inspired by the C&W method and network distillation frameworks. The paper introduces an adversarial distillation (AD) framework for generating targeted attacks by minimizing KL divergence between distributions. Label smoothing methods are explored to improve DNN performance by leveraging label semantic similarities. Word2vec embeddings like Glove are used to measure these similarities. In experiments on ImageNet-1000 val dataset, Top-k targets (k = 1, 2, 5, 10) were tested using ResNet-50 and DenseNet-121 DNNs. Adversarial distillation approach showed the best results, reducing perturbation energy consistently across all k's. Targets distant from the ground truth label were harder to attack. Ordered Top-k attacks improve attack \"robustness\". This paper introduces ordered Top-k adversarial attacks as a novel problem in the field of learning adversarial attacks. The proposed adversarial distillation framework is effective, especially for larger values of k (e.g. k = 5, 10). Additionally, the knowledge-oriented adversarial distillation approach is a novel contribution worth exploring further. The use of Deep Neural Networks (DNNs) in advanced machine learning and AI systems has increased their capabilities but also their vulnerability to attacks. White-box targeted attacks, based on imperceptible perturbations to DNN inputs, can deceive well-trained DNNs. The proposed Adversarial Distillation (AD) method is based on network distillation, where a new model (student) mimics an already trained model (teacher) to enhance robustness. The proposed Adversarial Distillation method leverages distilled knowledge from a trained model to improve attack robustness. It utilizes label semantic driven knowledge for learning ordered Top-k attacks. This method is opposite to traditional distillation, where a student model mimics a teacher model. The discovery of visually-imperceptible adversarial attacks in deep neural networks (DNNs) was a significant development. White-box attacks are used to evaluate model brittleness, with DNNs being universal function approximators capable of fitting random labels in large-scale tasks like ImageNet-1000. Adversarial attacks are learnable with proper objective functions, especially when DNNs are trained with back-propagation. Many white-box attack methods focus on norm-ball constrained objective functions, with the C&W method exploring different loss functions. The best performing loss function from the C&W method has been applied in various attack methods with successful results. The curr_chunk discusses the introduction of momentum in attack methods like MIFGSM and p gradient projection in PGD for better performance in generating adversarial examples. The proposed method leverages label semantic knowledge in the loss function design. It also introduces the white-box attack setting, the C&W method, and defines the ordered Top-k attack formulation. The focus is on classification tasks using DNNs. The curr_chunk focuses on classification tasks using DNNs, specifically discussing the prediction process and targeted attacks. It explains the use of a pretrained DNN and the softmax function for prediction, as well as the traditional Top-1 protocol for learning targeted attacks. The goal is to compute visually-imperceptible adversarial examples. The curr_chunk discusses the computation of visually-imperceptible perturbations for targeted attacks using a pretrained DNN under the white-box setting. The C&W Method is used to learn the perturbation under the Top-1 protocol as a constrained optimization problem. The C&W method formulates the learning problem for targeted attacks using loss functions and a hinge loss to satisfy constraints. Back-propagation with a tanh layer is used for optimization, with a binary search for the trade-off parameter. The method can be extended for ordered Top-k attacks. The C&W method uses loss functions and a hinge loss for targeted attacks, with back-propagation and a tanh layer for optimization. It can be extended for ordered Top-k attacks by modifying the loss function to ensure constraints are met. In an adversarial distillation framework, the network distillation method is utilized to learn ordered Top-k attacks by designing an adversarial probability distribution that satisfies the specified targets and allows control over placing the ground-truth label. In an adversarial distillation framework, a knowledge-oriented approach is used to define the adversarial distribution by specifying the logit distribution and computing the probability distribution using softmax. Adversarial logits for ordered Top-k targets are determined based on an empirically chosen decreasing factor. The remaining categories' adversarial logit is defined using semantic similarity and numerical considerations. The cosine distance between Glove embedding vectors is used to compute semantic similarity. In an adversarial distillation framework, the adversarial probability distribution P AD is defined targeting attack labels and exploiting label semantic knowledge. The KL divergence is used as the loss function, following the optimization scheme of the C&W method. Evaluation is done on ordered Top-k attacks in the ImageNet-1000 benchmark using pretrained DNNs. In the ImageNet-1000 benchmark, attacks were studied using pretrained DNNs ResNet-50 and DenseNet-121. A subset of correct predictions from both models was used for evaluation. A smaller subset of 1000 test images was randomly selected from 500 categories. The C&W method protocol was followed, testing the 2 norm as the energy penalty for perturbations in learning. In experiments, learned adversarial examples are evaluated using three norms (1, 2, \u221e) with different search schemas for the trade-off parameter \u03bb. Computation budget is a key factor, with less computationally expensive options preferred. Results are compared under three scenarios in the C&W method: Best Case, Worst Case, and Average Case settings. The proposed AD method outperforms the C&W baseline in Top-10 attacks, showing a 362.3% ASR improvement. For Top-5 attacks with a low search budget, the AD method also performs better. Both methods achieve 100% ASR in Top-k attacks, but the AD method consistently has lower perturbation energies, indicating more effective attacks. Visual examples of ordered Top-10 and Top-5 attacks are shown in Fig. 2. The results and comparisons of ordered Top-k targeted attacks using different methods are summarized in Table 2. Experiments were conducted to test the hypothesis that label semantic knowledge can help identify weak spots in attack methods. Results for Top-5 attacks using ResNet-50 are presented in Table 3, showing that attacks are more challenging when targets are selected from the least-like set in terms of label semantics. The proposed AD method for targeted attacks shows smaller perturbation energies and cleaner prediction distributions compared to other DNNs like DenseNet-121. Results of ordered Top-5 targeted attacks reveal that selecting targets based on label similarity or prediction scores on clean images affects the attack success rates. This paper introduces a novel approach for ordered Top-k targeted attacks in the white-box setting, aiming to enhance attack robustness. The method utilizes an adversarial distillation framework inspired by network distillation and a modified C&W method as a strong baseline. The proposed approach is tested in experiments to evaluate its effectiveness. The proposed method for ordered Top-k targeted attacks in ImageNet-1000 using DNNs ResNet-50 and DenseNet-121 shows consistently better results. The effectiveness of label semantic knowledge in designing the adversarial distribution is investigated. Further research is needed to improve the adversarial distribution and explore different loss functions like Jensen-Shannon divergence or Earth-Mover distance. The proposed AD method is more effective with limited computation budget. The proposed AD method is more effective with limited computation budget, leading to the question of how aggressive adversarial attacks can be when computation budget is not limited. The modified C&W method and AD method do not work well in learning Top-k attacks with certain search schemas. Research is ongoing to address these issues and provide empirical answers on the aggressiveness of adversarial attacks."
}