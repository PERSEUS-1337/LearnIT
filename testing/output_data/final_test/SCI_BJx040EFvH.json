{
    "title": "BJx040EFvH",
    "content": "Adversarial training for robust deep networks can be achieved using a cheaper adversary like the fast gradient sign method (FGSM) combined with random initialization. This approach is as effective as traditional methods but significantly lowers the cost. It can also be accelerated using standard techniques for efficient training, resulting in a robust CIFAR10 classifier with 45% accuracy at epsilon=8/255 in just 6 minutes. The goal is to learn robust deep networks for safety and security applications by improving their accuracy on adversarially perturbed data. Various defenses have been proposed to enhance the robustness of deep networks, with some achieving significant improvements in a shorter time compared to past methods. Recent work has focused on reducing the computational complexity of generating adversarial examples in order to improve the robustness of deep networks. Certified defenses and adversarial training are effective but come with a significant increase in training time, hindering progress in researching robustness. Despite some improvements in runtime, adversarial training remains slower than standard training. Adversarial training has been accelerated to optimize speed and cost. The Fast Gradient Sign Method (FGSM) can be as effective as projected gradient descent with random initialization points. FGSM training can be significantly accelerated using techniques like cyclic learning rates. The method of adversarial training has been optimized for efficiency by incorporating techniques such as cyclic learning rates and mixed-precision training. A failure mode called \"catastrophic overfitting\" has been identified, leading to faster training of robust classifiers compared to previous methods. For example, a robust CIFAR10 model was trained to 45% accuracy in 6 minutes, a significant improvement over previous training times. Adversarially robust training can achieve comparable results to standard training of deep networks, even with weaker attacks like FGSM. This method has been shown to be efficient, with a robust ImageNet classifier reaching 43% accuracy in 12 hours, matching previous results. The Fast Gradient Sign Method (FGSM) was proposed to generate adversarial examples with a single gradient step. The Fast Gradient Sign Method (FGSM) was initially used for adversarial training by perturbing inputs with a single gradient step. This method was later improved upon with the Basic Iterative Method, incorporating multiple smaller steps and random restarts to enhance adversarial training effectiveness against a projected gradient descent (PGD) adversary. This approach has been recognized as an effective way to train robust networks. Various techniques have been developed to enhance adversarial training against attacks, including optimization tricks, combination with other defenses, and generalization to multiple attack types. In addition to adversarial training, various other defense methods have been proposed, such as preprocessing techniques, detection algorithms, verification, and provable defenses, and theoretically motivated heuristics. The community is concerned about the use of strong attacks for evaluating robustness in defense mechanisms against adversarial examples. Weak attacks can provide a false sense of security, as history shows that many defenses were ultimately defeated by stronger attacks. This highlights the challenge of accurately assessing adversarial robustness. Several best practices have been proposed to evaluate adversarial robustness, with adversarial training using a PGD adversary remaining empirically robust. Recent work has explored \"free\" adversarial training as a more computationally efficient alternative to PGD defense. Recent work has explored using a single backwards pass to update model weights and input perturbation simultaneously, cutting out redundant calculations during backpropagation for faster adversarial example computation. However, these improvements are not significantly faster than traditional training methods, which can still take hours to days. In contrast, top performing training methods can achieve standard benchmark metrics for CIFAR10 and ImageNet architectures in mere minutes to hours using only modest computational resources. Adversarial training involves using techniques like cyclic learning rates and half-precision computations to improve network performance against attacks. It focuses on robust optimization to make networks resilient to adversarial attacks by approximating inner maximization over a threat model \u2206. This method involves using adversarial attacks to update model parameters \u03b8 through gradient descent. Adversarial training improves network performance against attacks by approximating inner maximization over a threat model \u2206. The Fast Gradient Sign Method is used for this purpose, with multiple smaller steps for better accuracy. Random restarts within the threat model further enhance the approximation. This technique, known as PGD adversarial training, involves updating model weights through gradient descent. The PGD adversary is used in adversarial training with multiple steps and dataset size considerations. The algorithm involves updating model weights with an optimizer like SGD. The number of gradient computations is proportional to O(MN) in a single epoch, where M is the dataset size and N is the number of PGD steps. This is more computationally intensive compared to standard training methods. Adversarial training with PGD adversary involves N steps, making it N times slower than standard training. Shafahi et al. (2019) propose \"free\" adversarial training, using FGSM steps with full step sizes \u03b1, and updating model weights for N iterations on the same minibatch. This method reduces the total number of epochs by a factor of N to match the computational cost of standard training. The FGSM adversarial training method is just as effective as PGD-based training, reducing the number of epochs needed for convergence. Techniques from the DAWNBench competition can further speed up adversarial training by using cyclic learning rates. The FGSM adversarial training method, similar to PGD-based training, reduces the number of epochs needed for convergence. Cyclic learning rates can speed up computations. Free adversarial training is robust against PGD attacks due to using the previous iteration's perturbation as the starting point for the next iteration. Starting from a non-zero initial perturbation is beneficial. Using FGSM adversarial training with random initialization for the perturbation is effective. Starting from a non-zero initial perturbation is crucial for the success of FGSM adversarial training with random initialization, which has been found to be as effective as PGD adversarial training. This approach, previously studied by Tram\u00e8r et al. (2017), differs in its initialization and step size, resulting in models robust to full-strength PGD adversaries. To test the effect of initialization in FGSM adversarial training, models were trained to be robust on CIFAR10. Results show that using random or previous-minibatch initialization instead of zero initialization leads to reasonable robustness levels comparable to PGD adversarial training methods. Adversarial accuracies were calculated using a PGD adversary with specific optimization parameters. Increasing the FGSM step size to \u03b1 = 10/255 improved model robustness, matching results from free adversarial training. However, forcing perturbations to lie on the boundary with \u03b1 = 2 led to catastrophic overfitting. These failure modes explain previous FGSM adversarial training failures. The model overfits to a restricted threat model in adversarial training, with differences in computational complexity between FGSM and free adversarial training. FGSM requires two backward passes, equivalent to two epochs of standard training. Top submissions to the DAWNBench competition have shown that CI-FAR10 and ImageNet classifiers can be trained quicker and at lower cost than traditional methods. Cyclic learning rate, introduced by Smith in 2017, can significantly reduce the number of epochs required for training deep networks. Cyclic learning rate schedules linearly from zero to maximum and back to zero, allowing CIFAR10 architectures to converge faster. Mixed-precision arithmetic on newer GPU architectures can speed up training by reducing memory utilization and runtime. These techniques have been key in achieving fast training and low costs in DAWNBench submissions. Adopting fast training techniques in adversarial training reduces training epochs and runtime on GPU infrastructure with tensor cores. These improvements can be easily implemented with minimal additional effort and are accessible to the research community. Experiments on MNIST, CIFAR10, and ImageNet benchmarks demonstrate the effectiveness of FGSM adversarial training. The ResNet50 architecture (He et al., 2016) is used for experiments with FGSM adversarial training. Repositories for reproducing experiments and model weights are available. PGD adversaries are evaluated with 10 random restarts for 50 iterations. Speedup with mixed precision is incorporated using the Apex amp package. The effectiveness of models trained with FGSM adversarial training is demonstrated. To demonstrate the robustness of models trained using FGSM adversarial training, mixed-integer linear programming methods are used to calculate exact robustness. Performance of models trained on CIFAR10 with cyclic learning rates and half precision is evaluated across different adversarial training methods. The number of epochs the model was trained for is shown on the x-axis, with accuracy on natural and adversarial images plotted on the y-axis. The minimum number of epochs needed to train a model to 45% robust accuracy is indicated. The study demonstrates the robustness of models trained with FGSM adversarial training by using mixed-integer linear programming to calculate exact robustness. Results show that FGSM adversarial training provides nearly indistinguishable empirical and verified robustness compared to PGD adversarial training on MNIST. The CIFAR10 experiments combine DAWNBench improvements with various forms of adversarial training, using cyclic learning rates and half precision. The study compares different adversarial training methods, finding that FGSM and PGD require fewer epochs than free adversarial training. Using a cyclic learning rate schedule improves all methods, with FGSM and PGD showing the greatest speedups. Total training time is reported based on reaching a baseline of 45% robust accuracy. The study compares adversarial training methods on CIFAR10 and ImageNet datasets. FGSM adversarial training is the fastest, achieving robust classifier in 6 minutes with 15 epochs. PGD and free adversarial training take comparable time, with FGSM benefiting most from cyclic learning rate. ImageNet benchmarks also show improvements with additional techniques like removing weight decay and resizing images progressively during training. Training is divided into three phases, starting with larger batches of smaller images and progressing to smaller batches of larger images. The models are trained to be robust at = 2/255 and = 4/255, showing similar levels of robustness to free adversarial training. Using techniques like ten restarts and PGD accuracy with one restart, an ImageNet classifier can be trained in 15 epochs in 12 hours using FGSM adversarial training, at a fraction of the cost of free adversarial training. Comparisons are made with the best performing variation of free adversarial training, which uses m = 4 minibatch replays over 92 epochs of training. Free adversarial training can be enhanced with mixed-precision arithmetic, reducing runtime by 25%, but still slower than FGSM-based training. Combining free adversarial training with fast techniques used in FGSM adversarial training for ImageNet results in reduced performance due to catastrophic overfitting caused by design decisions like zero initialization or large step sizes. \"Catastrophic overfitting in adversarial training can lead to a sudden drop in robust accuracy to 0% on training data. An early-stopping scheme can help salvage the situation by measuring PGD accuracy on a small minibatch of data. Empirical hypotheses suggest that adversarial examples need to cover the entire threat model for robustness.\" The restricted nature of generated examples may have caused failures due to limited initialization, resulting in perturbations that only affect each dimension by 0 or \u00b1. Defenders do not require strong adversaries during training, as rough approximations to the inner optimization problem suffice. FGSM adversarial training with random initialization can be as effective as costly PGD adversarial training, converging faster with a cyclic learning rate. Adversarial training converges faster with a cyclic learning rate schedule, allowing for the rapid learning of robust classifiers for CIFAR10 and ImageNet. This work demonstrates the potential for weak adversarial training to learn robust models, suggesting new directions for explaining when approximate solutions are sufficient for robust optimization. The paper compares its approach to Tram\u00e8r et al. (2017) in terms of random initialization and step sizes for FGSM adversarial training. Different initialization methods and step sizes were tested on MNIST, with results summarized in Table 6. The paper compares different initialization methods and step sizes for FGSM adversarial training on MNIST. Using a uniform initialization showed the greatest improvement, while a full step size did not help on its own. The proposed FGSM adversarial training method is more consistent and has low standard deviation over random seeds compared to previous methods. Training parameters include a batch size of 128, SGD optimizer with momentum 0.9 and weight decay 5 * 10 \u22124, and results are averaged over 3 random seeds. The DAWNBench experiments in Table 7 show the effect of step size on FGSM adversarial training performance. Increasing the step size up to \u03b1 = 10/255 improves robust performance, but larger sizes may lead to overfitting. The main experiments in the paper use cyclic learning rate and FGSM adversarial training with uniform random initialization. Many variations of FGSM adversarial training fail similarly, leading to catastrophic overfitting where the model quickly becomes non-robust. This failure may be due to the lack of diversity in adversarial examples generated. Adversarial examples can result in catastrophic overfitting, where the network learns a decision boundary that is only robust at specific perturbation values. This can be detected by evaluating PGD performance on a small subset of training data, with 0% robust accuracy indicating catastrophic failure. A simple 5-step PGD adversary can be used to quickly check for drops in robust accuracy. Using a PGD adversary on a training minibatch can help detect catastrophic overfitting and achieve robust performance. Early stopping is crucial to prevent catastrophic overfitting, especially with larger step sizes. Different training methods use a batch size of 128, SGD optimizer with momentum 0.9 and weight decay 5 * 10 \u22124. Results are averaged over 3 random seeds, and maximum learning rates for cyclic learning rate schedule are shown in Table 8. Mixed-precision can also speed up the process. Using mixed-precision can speed up training. Optimization tricks like cyclic learning rate schedule, progressive resizing, and batch-norm regularization may impact the performance of free adversarial training. Testing free adversarial training as a replacement for FGSM adversarial training with the same optimizations shows promising results. Free adversarial training with specific parameters can be equivalent to standard training epochs. It is important to note that free adversarial training is not a complete solution. By giving free adversarial training more epochs, it may be possible to achieve better performance. Tuning DAWNBench techniques for free adversarial training is not the focus here, but applying the same tricks used for FGSM adversarial training to free adversarial training reveals the need for more epochs. One epoch of FGSM training equals two epochs of free training, suggesting that 30 epochs may be more suitable for optimal performance. Even with double the epochs, free adversarial training doesn't fully recover its original performance compared to FGSM adversarial training."
}