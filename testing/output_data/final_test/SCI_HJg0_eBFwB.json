{
    "title": "HJg0_eBFwB",
    "content": "In this paper, the use of in-training matrix factorization is proposed to reduce the model size for neural machine translation. Parameter matrices can be decomposed into smaller matrices, compressing large machine translation architectures by reducing the number of learnable parameters. In-training matrix factorization can reduce nearly 50% of learnable parameters without affecting BLEU score. It is particularly effective on embedding layers, offering a simple and efficient way to decrease parameters with minimal impact on model performance and sometimes even improving it. Having smaller models for deep learning tools can be beneficial for privacy reasons and resource efficiency. While large parameter matrices are often necessary to capture complex patterns, having many parameters can help overcome limitations in optimization methods and noisy data. The need for large parameter matrices is not essential for the network's representational power, but rather helps in optimizing neural architectures. In-training matrix factorization reduces parameters in neural architectures during training, lessening computational resources. It is a technique to utilize matrix factorization during training time, contributing to efficiency in deep learning systems. In-training matrix factorization reduces parameters in neural architectures during training, lessening computational resources. The technique involves utilizing matrix factorization during training time, which can decrease a model's size by half without impacting performance. Experiments on LSTM and transformer networks show that in-training factorization can even improve model performance. Matrix factorization is traditionally used as a compression method for pretrained models, involving obtaining a low-rank approximation of a matrix through singular-value decomposition. In practice, post-training matrix factorization may require relearning parameter matrices to refine compressed models, leading to increased training time and resources. However, initializing neural networks in factorized form can simplify training. In practice, initializing neural networks in factorized form simplifies training by replacing parameter matrices with products of smaller matrices. This p-rank approximation reduces the number of parameters, especially when p < min(n,m) 2 for square matrices. In practice, initializing neural networks in factorized form simplifies training by reducing the number of parameters. This technique is effective on embedding and projection layers, and can be combined with weight tying. Experiments are conducted using the IWSLT 2014 dataset for English-to-German, English-to-Portuguese, and English-to-Turkish language pairs. The experiments involve two neural machine translation architectures: an LSTM encoder-decoder and a transformer network. The model uses multi-head attention and self attention instead of a recurrent architecture. The implementation includes a 3-layer bidirectional encoder, a 3-layer decoder, and beam search for inference. The vocabulary sizes are 30k for the source and 46k for the target. The model accumulates 8 mini-batches of 12 to 16 sentences between gradient updates. The experiments involve neural machine translation architectures: an LSTM encoder-decoder and a transformer network with multi-head attention. The model uses subwords with a 15k subword vocabulary for both languages. Factorization experiments are conducted with different inner sizes p on various layers, comparing with non-factorized models. A compression baseline by See et al. (2016) is compared with the method, highlighting GPU compatibility advantages. Post-training factorization is also explored with different schemes, comparing results with pruning and in-training factorization. In-training factorization allows for increasing batch size during training, improving performance on transformer models. Results and comparisons for in-training factorization with LSTM encoder-decoder model are provided in Table 1. In-training factorization with the LSTM encoder-decoder model shows significant reduction in model size with minimal impact on performance. Results for the transformer model are provided in Tables 2 and 5, demonstrating the effectiveness of in-training factorization in reducing the number of parameters. Reducing model size through in-training factorization improves BLEU score in transformer model. Compression of embedding, projection, feed-forward, and attention layers by 22% yields a 0.2 increase in BLEU score. In-training factorization improves transformer model performance across languages, as shown in experiments with Portuguese and Turkish datasets. In-training factorization reduces model size, allowing for larger batches to be stored in memory. Comparing a factorized transformer model with a non-factorized one, the factorized model converges 20% faster. This confirms that increasing batch size can decrease training time, as shown in Figure 2. After in-training experiments, comparing results with traditional post-training factorization methods, we analyze singular values of weight matrices in different layers of the transformer model. The embedding and projection layers have a dominant singular value, while attention and feed-forward matrices are more balanced. The large singular value in embedding layers may represent a learned bias. Relevant singular values are those exceeding 10% of the dominant one. The text discusses the analysis of singular values in different layers of the transformer model after in-training experiments. It highlights that relevant singular values are those exceeding 10% of the dominant one, with the embedding and projection layers being dominated by one value. Various compression schemes were experimented with, and the results are reported in Table 5. The post-training factorization improves the baseline transformer model's performance, serving as a regularizer against overfitting. Comparing in-training and post-training factorization, the latter shows slightly better performance, possibly due to additional training. Pruning a post-training factorized model results in a decrease in BLEU score, indicating redundancy in parameter reductions. Neural networks have become prominent in sequence-to-sequence models, with attentional encoder-decoders and transformer networks being key advancements. Parameter pruning has been proposed to reduce model size without significant performance loss. Parameter pruning can reduce model size by 40% without a significant drop in performance. This method involves learning which weights are necessary during training and pruning them accordingly. Other compression techniques include quantization at inference or training, model distillation, and matrix factorization. Low-rank approximation is commonly used to compress Convolutional Neural Networks without performance loss in various applications such as speech recognition and computer vision. The work focuses on in-training factorization for LSTM models, comparing it with post-training compression techniques. While previous studies have used SVD for compression, this work explores factorizing embedding and attention matrices during training, with potential performance gains. However, a limitation is the lack of exploration on selecting hidden sizes for in-training factorized models. Exploring strategies for selecting hidden sizes for in-training factorized models and analyzing the impact on various natural language processing tasks. Comparing in-training factorization with posttraining factorization for model performance improvement. Introducing a method using matrix factorization during training to reduce neural machine translation model parameters. Using in-training factorization for neural machine translation models leads to significant gains in final performance and requires fewer parameters. This technique results in a model with improved accuracy and reduced training time compared to post-hoc parameter reduction methods."
}