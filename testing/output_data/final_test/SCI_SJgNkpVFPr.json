{
    "title": "SJgNkpVFPr",
    "content": "The proposed Variational Imitation Learning with Diverse-quality demonstrations (VILD) addresses the challenge of learning from demonstrations of varying expertise levels by modeling this diversity with a probabilistic graphical model. By using a variational approach, VILD outperforms existing methods on continuous-control benchmarks, enabling scalable and data-efficient imitation learning. VILD outperforms state-of-the-art methods in imitation learning by enabling scalable and data-efficient learning from diverse-quality demonstrations. Demonstrations are often of varying expertise levels, making it challenging to obtain high-quality data. This is especially true in domains like robot control and autonomous driving. In mixed demonstrations, IL tends to perform poorly due to low-quality demonstrations negatively affecting performance. For example, cheaply collected amateur demonstrations for robotics can cause catastrophic damages. Multi-modal IL can be used to learn a good policy with diverse-quality demonstrations when the level of demonstrators' expertise is known. Multi-modal Imitation Learning (MM-IL) aims to learn a policy where each mode represents a demonstrator's decision making. Estimating demonstrators' expertise levels is crucial for selecting high-quality modes. Without this knowledge, distinguishing between expert and amateur decision making is challenging. Previous works have proposed using auxiliary information to estimate demonstration quality. In Multi-modal Imitation Learning (MM-IL), estimating demonstrators' expertise levels is essential for selecting high-quality modes. Different methods have been proposed to infer demonstration quality using diverse approaches such as similarities, confidence scores, and ranking from experts. However, the reliance on scarce or noisy information can lead to poor performance. This paper introduces a novel setting where only diverse-quality demonstrations are considered. In a novel setting of Imitation Learning (IL), diverse-quality demonstrations are the focus without information on demonstrators' expertise or auxiliary input from experts. To address this challenge, a new learning paradigm called Variational Imitation Learning with Diverse-quality Demonstrations (VILD) is proposed. VILD models demonstrators' expertise using a probabilistic graphical model and learns it alongside a reward function representing expert decision-making intentions. The model is scaled up for large state and action spaces using a variational approach and reinforcement learning for flexibility. Importance sampling is utilized to improve data-efficiency when learning the reward function. Reinforcement learning (RL) aims to learn an optimal policy for decision making in a Markov decision process. VILD with Importance Sampling (IS) is robust against diverse-quality demonstrations and is data-efficient, outperforming existing methods significantly. Reinforcement learning (RL) involves optimizing a policy in a Markov decision process (MDP) with continuous state and action spaces. The goal is to find an optimal policy that maximizes the expected cumulative reward, often utilizing deep neural networks for success. Imitation learning (IL) was proposed to address the limitation of RL relying on the reward function. IL aims to learn the optimal policy from demonstrations without using the reward function. Demonstrations are collected from multiple demonstrators, and a graphical model describes this data collection process. Imitation learning (IL) aims to learn the optimal policy from expert demonstrations, which are drawn independently from a probability density. While IL has shown success in benchmark settings, practical applications in the real world are limited due to the high cost of obtaining expert demonstrations. Imitation learning (IL) struggles with the high cost and limited availability of expert demonstrations. A new approach, IL with diverse-quality demonstrations, aims to address this issue by collecting demonstrations from a variety of demonstrators. Imitation learning with diverse-quality demonstrations collects demonstrations from demonstrators of varying expertise levels, including non-experts, to reduce costs. Demonstrators select actions based on probability distributions and may not always execute expert actions in the Markov Decision Process (MDP). This process is repeated multiple times to gather data. The process of imitation learning with diverse-quality demonstrations involves collecting demonstrations from demonstrators of varying expertise levels, who select actions based on probability distributions. The goal is to learn the optimal policy using these diverse-quality demonstrations. Existing methods are deemed inadequate for learning with diverse-quality demonstrations. Existing imitation learning methods are not suitable for diverse-quality demonstrations. These methods treat all demonstrations as if they were from the same distribution, leading to a policy that averages over all demonstrators' decisions. This approach is problematic when amateurs are present, as their decisions differ significantly from experts. The state distributions of amateurs and experts also differ, causing unstable learning. As a result, existing methods tend to learn a policy that achieves average performances and struggle to handle diverse-quality settings. VILD is a robust method for handling diverse-quality demonstrations by building a probabilistic model to describe demonstrators' expertise levels and a reward function. Parameters are estimated using a variational approach and implemented easily by RL. Data-efficiency is improved using importance sampling. The model enables estimating demonstrators' expertise levels, overcoming issues of compounding error in a naive model. The naive model addresses diverse-quality demonstrations by learning a reward function. It minimizes the KL divergence to estimate optimal and noisy policies. However, it suffers from compounding errors and tends to perform poorly compared to supervised learning methods. The naive model, which aims to address diverse-quality demonstrations by learning a reward function, tends to perform poorly due to compounding errors. To avoid this issue, a proposed model utilizes the inverse RL approach to learn a reward function from diverse-quality demonstrations, combining IRL and RL to solve IL problems. The proposed model combines IRL and RL to solve IL problems by learning a reward function from diverse-quality demonstrations. It is based on MaxEnt-IRL and aims to learn the reward function and expertise level using parameters \u03c6 and \u03c9. The model compares to the data distribution to learn the reward parameter \u03c6. The proposed model aims to learn the reward parameter \u03c6 and expertise parameter \u03c9 by minimizing the KL divergence from the data distribution to the model. This involves maximizing the cumulative reward to learn the optimal policy. The proposed model aims to learn reward and expertise parameters by solving an optimization problem. To scale up the model to large state and action spaces, a variational approach is used to compute integrals indirectly. This approach involves finding an optimal variational distribution through a sub-optimization problem. The text discusses using a variational distribution to lower-bound functions f and g, despite them not being joint concave functions. By maximizing the lower bound, the function f can be substituted, allowing for the use of a variational distribution to compute integrals indirectly. By using a variational distribution, we can lower-bound functions f and g, allowing for indirect computation of integrals. The max-min problem can be solved efficiently even for large state and action spaces, as the functions are defined as expectations and can be optimized straightforwardly. In practice, variational distributions are represented by parameterized functions and solved iteratively using stochastic optimization methods. The equalities may not always hold due to various reasons, but when using deep neural networks, the obtained distributions are usually accurate. Models for q and p are specified as q\u03b8 and p\u03c9, with specific choices enabling efficient solutions. The choice of parameters for q and p enables efficient solutions. The model specifications involve solving a MaxEnt-RL problem with a reward function, where trajectories are collected based on a noisy trajectory density induced by a policy. The hyper-parameter \u03a3 determines the quality of the approximation in solving the minimization problem using RL. Choosing a small \u03a3 value yields a reward function and a policy that imitates the optimal policy, aligning with the goal of IL. The model specification for p \u03c9 assumes a Gaussian tendency for the noisy policy, with C \u03c9 indicating the expertise level of demonstrators. VILD is flexible beyond this assumption. VILD is a flexible approach for demonstrators with different expertise levels. The choice of p \u03c9 incorporates different prior assumptions, such as Laplace distribution for outlier actions. The trade-off between reward and squared-error is determined by C \u03c9 pkq, where high-expertise demonstrators minimize squared error and low-expertise demonstrators maximize immediate reward. In VILD, deep neural networks are used to update \u03c6, \u03c9, and \u03c8 iteratively. A regularization term is included to penalize large values of C \u03c9 pkq. Variable k needs to be provided along with demonstrations, but experts do not need to provide it. If k is not given, a simple strategy is to set k \" n and K \" N. In VILD, deep neural networks update \u03c6, \u03c9, and \u03c8 iteratively with a regularization term to penalize large C \u03c9 pkq values. The strategy of setting k \" n and K \" N assumes a one-to-one mapping between demonstration and demonstrator. Importance sampling (IS) is used to improve VILD convergence when updating \u03c6 by maximizing expected cumulative rewards from demonstrators and minimizing rewards from other sources. Low-quality demonstrations lead to uninformative gradients, slowing convergence. IS helps estimate gradients using high-quality data to avoid this issue. To improve VILD convergence, Importance Sampling (IS) is used to estimate gradients using high-quality demonstrations sampled with high probability. By sampling from a distribution that assigns high probabilities to expert demonstrators, the estimated gradients become more informative, leading to faster convergence. Truncated importance weights are used to reduce sampling bias, resulting in an IS gradient that can be accurately estimated. In supervised learning, diverse-quality data has been extensively studied, such as learning with noisy labels where human labelers may assign incorrect labels. Various methods have been proposed to handle this setting, including probabilistic models that infer correct labels and labelers' expertise levels. In supervised learning, diverse-quality data has been extensively studied, including methods that infer correct labels and labelers' expertise levels. Raykar et al. (2010) proposed a two-coin model for estimating labels and expertise, while Khetan et al. (2018) introduced weighted loss functions based on estimated labels and expertise levels. Variational approaches in Imitation Learning (IL) have been used to reduce overfitting and learn multi-modal policies from diverse expert demonstrations. In the context of Imitation Learning (IL), variational approaches have been utilized to reduce overfitting and learn multi-modal policies from diverse expert demonstrations. Specifically, the variational information bottleneck (VIB) has been employed to compress information flow and filter out irrelevant signals, leading to less overfitting. Unlike previous works, the variational approach is used in this study to aid computing integrals in large state-action spaces, rather than for learning a variational auto-encoder or optimizing a variational bound of mutual information. In this section, VILD performance is evaluated in continuous-control benchmarks and real-world crowdsourced demonstrations using four tasks from OpenAI gym and a robosuite reaching task. Performance is measured by cumulative ground-truth reward along trajectories, computed using test trajectories generated by learned policies. 10 test trajectories are used for benchmark tasks, and 100 for the robosuite reaching task. In evaluating VILD performance, comparisons are made against various online IL methods using TRPO for policy updates, except for the Humanoid task where SAC is used. Data generation involves noisy policy types for demonstrations from pre-trained policies. The noisy policy types used for demonstrations include Gaussian noisy policy and time-signal-dependent (TSD) noisy policy. K \" 10 demonstrators with different noise processes are used, generating trajectories with approximately T \" 1000 time steps. The number of state-action pairs in each dataset is around 10000. Results against online IL methods show VILD outperforming existing methods in HalfCheetah and Ant 3 tasks. VILD with Importance Sampling (IS) outperforms existing methods in terms of data-efficiency and final performance, learning better policies with fewer transition samples. VILD without IS tends to have better final performance but is less data-efficient compared to VILD with IS, except in the case of Humanoid with Gaussian noisy policy. The slight bias in gradient estimation from IS may negatively impact performance. Overall, VILD with IS is effective in handling diverse-quality demonstrations, while existing methods perform poorly except in the Humanoid task. InfoGAIL, a context-dependent policy, shows good performance in specific contexts like Walker2d with TSD noisy policy. However, its performance varies across contexts and is poor on average with contexts from a uniform distribution. The study found that MM-IL methods are not suitable for contexts where the level of demonstrators' expertise is unknown. VILD without importance sampling performs better for Gaussian noisy policies compared to TSD noisy policies due to correct model specification. VILD with importance sampling still performs well for both types of noisy policies. The study also compared VILD against offline IL methods. Results in Figure 8 show that VILD outperforms offline IL methods like behavior cloning (BC), Co-Teaching, and BC from diverse-quality demonstrations. BC performs the worst due to compounding errors and low-quality demonstrations. BC-D and Co-Teaching are more robust but still perform worse than VILD with importance sampling. VILD accurately ranks demonstrators' expertise, as shown in Figure 9 by comparing ground-truth values with learned covariances. In an experiment evaluating VILD's robustness against real-world demonstrations, it accurately ranks demonstrators' expertise, except for those with low-level expertise due to highly dissimilar low-quality demonstrations. Real-world demonstrations were collected from a robotic crowdsourcing platform in the robosuite environment for object-manipulation tasks. The experiment focused on a reaching task with N \" 10 demonstrations of length T \" 500 and set K \" 10. The experimental results show that VILD with IS outperforms GAIL, AIRL, and MaxEnt-IRL in terms of performance. VILD also performs better than InfoGAIL in the final performance, showing more robustness against real-world demonstrations with diverse quality. The effectiveness of VILD is demonstrated in comparison to InfoGAIL, showing better performance without the need for context evaluation. VILD's policy generates trajectories independently of context, outperforming InfoGAIL in stability and final performance. In this paper, a robust method called VILD is proposed for imitation learning with diverse-quality demonstrations. VILD learns the reward function and demonstrators' expertise levels using a variational approach, enabling scalable and data-efficient learning. Future work includes exploring alternative parameter estimation methods and handling model misspecification by using more flexible models and the tempered posterior approach. The section derives lower-bounds of functions presented in the paper to improve model robustness. It discusses deriving the objective function of VILD and maximizing it under certain constraints. The method involves using variational distributions to bound the functions and verify their maximization. The section discusses deriving lower-bounds of functions to enhance model robustness by maximizing the objective function of VILD under constraints using variational distributions. It presents a lower-bound for gp\u03c6, \u03c9q using a structured variational distribution. The lower-bound of g can be obtained using a variational distribution with parameter \u03b2, allowing dependency between random variables. However, computing or approximating Gp\u03c6, \u03c9, \u03b2q requires access to the transition probability, which is often unknown and needs to be estimated, especially for large state and action spaces. The structured variational approach is used to lower-bound g by pre-defining conditional dependency between states based on the transition probability of an MDP. The optimal variational distribution q \u03b8 \u2039 pat, ut|st, kq can be found through this approach. The optimal variational distribution q \u03b8 \u2039 pat, ut|st, kq is found by maximizing Gp\u03c6, \u03c9, \u03b8q w.r.t. q \u03b8, which is equivalent to solving a maximum entropy RL problem for an MDP defined by a tuple M. The functions Q and V are soft-value functions defined in this context. The objective function Hp\u03c6, \u03c9, \u03c8, \u03b8q is derived from Fp\u03c6, \u03c9, \u03c8q\u00b4Gp\u03c6, \u03c9, \u03b8q by substituting models and using a Laplace distribution. The expectation of \u03a3 T t\"1 }ut\u00b4at} 2 \u03a3\u00b41 over r q \u03b8 ps1:T , u1:T , a1:T , kq is rewritten using the re-parameterization trick. The optimization problem involves maximizing a function by substituting models and using a Gaussian distribution. The expectation of a certain term is calculated using the reparameterization trick. The optimization problem involves maximizing a function by substituting models and using a Gaussian distribution. Maximizing a quantity with respect to \u03b8 implies maximizing the expected cumulative reward while avoiding difficult states for demonstrators. Demonstrators' expertise level for a state is indicated by the estimated covariance, suggesting states that are difficult for demonstrators should be avoided in the policy. This property may improve exploration-exploitation trade-off in Imitation Learning. The covariance is specified not to depend on the state in this model. In this paper, the covariance is specified not to depend on the state, simplifying the model. The derivation of VILD is concluded, and it is mentioned that distributions other than Gaussian can be used, such as the multivariate-independent Laplace distribution with heavier tails, suitable for modeling outlier actions. In this paper, VILD is implemented using the Laplace distribution for modeling outlier actions. The deep learning framework PyTorch is used with neural networks for function approximators. Parameters are optimized using Adam with specific step-size and mini-batch size. Trust region policy optimization (TRPO) is used for optimizing policy parameters. The policy parameter \u03b8 is optimized using trust region policy optimization (TRPO) with batch size 1000, except for the Humanoid task where soft actor-critic (SAC) is used with mini-batch size 256. TRPO is an on-policy RL method, while SAC is an off-policy RL method. SAC is chosen for Humanoid due to its high data-efficiency. Diagonal covariances are used for the distribution p\u03c9put|st, at, kq \" N put|at, C\u03c9pkqq. For the distribution q \u03c8 pat|st, ut, kq, a Gaussian distribution with diagonal covariance is used. Neural networks are employed to represent q \u03c8 pat|st, ut, kq with K output heads corresponding to the mean and log-standard-deviation. The mean function of q \u03c8 pat|st, ut, kq is pre-trained using least-squares regression for 1000 gradient steps. The policy q \u03b8 pat|stq utilizes a Gaussian policy with diagonal covariance, and an entropy coefficient \u03b1 is used to control exploration-exploitation trade-off. In experiments, an entropy coefficient \u03b1 is used to control exploration-exploitation trade-off. The value of \u03b1 is optimized in SAC, and included in VILD to rescale quantities in the model. A discount factor \u03b3 is also used, with a value of 0.99 in experiments. Regularization of the reward/discriminator function is done with a gradient penalty, and reward values are bounded using a sigmoid function in methods like VILD, AIRL, and MaxEnt-IRL. In experiments, an entropy coefficient \u03b1 is used to control exploration-exploitation trade-off, and a discount factor \u03b3 is set to 0.99. Regularization of the reward function is done with a gradient penalty, and reward values are bounded using a sigmoid function in methods like VILD, AIRL, and MaxEnt-IRL. The reward values of the agent can be highly negative in the early learning stage, leading to premature convergence to poor policies. The distribution of demonstrations and agent's trajectories may not overlap initially in MDPs with large state and action spaces, causing issues in learning a reward function. The gradient penalty regularizer helps but is insufficient, and for VILD, it is beneficial to bound the reward function to control a trade-off between immediate reward and squared error when optimizing \u03c8. In this section, experimental settings and data generation are described. VILD is evaluated on four continuous-control benchmark tasks using the Mujoco physics simulator. Diverse-quality demonstrations are generated using 10 demonstrators. Two types of noisy policies are considered. Source code for the implementation will be publicly available. In this section, two types of noisy policies are considered for generating diverse-quality demonstrations using 10 demonstrators. A Gaussian noisy policy with varying covariance values and a time-signal-dependent noisy policy are used. The performance of demonstrators with the Gaussian noisy policy is shown in Table 2. Additionally, a random policy is initialized for learning with small magnitude actions, following common practice in deep RL. The text discusses generating demonstrations with a time-signal-dependent noisy policy, where noise variance increases over time. Demonstrators are assigned different noise levels, and their performance is evaluated. The text discusses the challenges of learning from demonstrations with a time-signal-dependent noisy policy for object-manipulation tasks. Initial experiments show that none of the imitation learning methods successfully learn these tasks, indicating the need for a hierarchical policy due to the tasks' hierarchical structure. In this experiment, reaching subtasks are considered, challenging for imitation learning due to varied quality of crowdsourced demonstrations. Demonstrations are obtained from object-manipulation tasks, with a dataset of 10 demonstrations terminated after the robot's end-effector contacts the target object. The dataset consists of approximately 5000 state-action pairs. In this experiment, 10 demonstrations were collected with approximately 5000 state-action pairs. The robot's true states were used without visual observations, and the gripper control was disabled for the reaching task. The state space is R44 and the action space is R7. Demonstrations varied in quality, with performance evaluated based on the distance between the object and the end-effector. The experiment was repeated 5 times, and the average performance was reported. In the experiment, 10 demonstrations were collected for a reaching task using true states without visual observations. The robot's gripper control was disabled, and performance was evaluated based on the distance between the object and the end-effector. The experiment was repeated 5 times, and the average performance was reported. For each trial, 100 test trajectories were generated to evaluate performance, with a larger number of test trajectories compared to benchmark experiments due to varied initial states. VILD without IS and VAIL were not evaluated, as VILD with IS performed better and VAIL was comparable to GAIL. Policy parameters were updated using TRPO with neural networks and pre-training was done using behavior cloning. InfoGAIL used a context-dependent policy with variable k as context. In pre-training InfoGAIL, the context-dependent policy uses variable k. VILD applies the log-sigmoid function to the reward function, parameterized as r \u03c6 ps, aq \" log D \u03c6 ps, aq. A substitution is made to obtain an objective similar to GAIL. VILD variant used in the experiment outperformed the standard version. Online IL methods were compared against VILD, including MaxEnt-IRL. MaxEnt-IRL is a well-known IRL method that learns a reward parameter by minimizing KL divergence. It can handle nonlinear reward functions and uses importance sampling or variational approaches to compute log Z. The VILD model is based on MaxEnt-IRL and involves a max-min problem with a variational distribution and noisy policy model. If these are simplified to Dirac delta functions, VILD reduces to MaxEnt-IRL when all demonstrators execute the optimal policy equally. GAIL performs occupancy measure matching in Generative Adversarial IL. GAIL, Generative adversarial IL (GAIL), uses generative adversarial networks to learn the optimal policy from expert demonstrations by matching occupancy measures. It finds a parameterized policy that is similar to the expert's policy. GAIL uses the Jensen-Shannon divergence and a generative-adversarial training objective. AIRL, Adversarial IRL, was proposed to overcome limitations of GAIL. AIRL, Adversarial IRL, overcomes GAIL's limitation by learning the expert reward function through generative-adversarial training. It solves for the reward function by maximizing the likelihood ratio. The policy is learned through RL with the learned reward function. Fu et al. (2018) demonstrated that the gradient of this objective is equivalent to MaxEnt-IRL. They also introduced a disentangle approach for the reward function, improving performance in transfer learning. This approach can be applied to other IRL methods like MaxEnt-IRL and VILD. VILD can be extended to use a training procedure of AIRL by applying the same derivation from MaxEnt-IRL. Peng et al. (2019) improves upon GAIL by using variational information bottleneck (VIB) to compress information flow and reduce over-fitting. VAIL learns the discriminator D \u03c6 through an optimization problem involving an encoder and a prior. VAIL uses an optimization problem with an encoder and prior to learn the discriminator D \u03c6. It compresses information flow to reduce over-fitting but does not significantly outperform GAIL. The information bottleneck may filter out relevant signals in agent's trajectories, leading to poor performance. InfoGAIL is an extension of GAIL for learning a multi-modal policy in MM-IL. It introduces a context variable z to the GAIL formulation and learns a context-dependent policy. InfoGAIL maximizes mutual information between contexts and state-action variables to ensure the context is not ignored during learning. This approach solves a min-max problem by maximizing a variational lower-bound of mutual information. In our experiment, the encoder neural network uses a prior distribution of contexts, with the number of contexts set to the number of demonstrators. The prior distribution is hand-crafted to correspond to high-expertise demonstrators, but for fair comparison, a uniform distribution is used. For the Humanoid task, the Wasserstein-distance variant of InfoGAIL is used. Offline IL methods, such as Behavior Cloning, learn policies through supervised learning without additional transition samples from MDPs. Behavior Cloning (BC) is a simple IL method that treats IL as a supervised learning problem. BC-D is an extension of BC for diverse-quality demonstrations, using supervised learning to learn policy and expertise parameters. It minimizes KL divergence to learn parameters and solves an optimization problem using a variational approach. Co-teaching is a state-of-the-art method for classification with noisy labels, training two neural networks by exchanging mini-batch samples under a small loss criteria. This method is extended to learn a policy through least-square regression, updating parameters iteratively. The parameters \u03b81 and \u03b82 are updated by iterates using mini-batches that incur small loss when using predictions from policy networks. VILD outperforms existing online and offline IL methods overall, except on Humanoid tasks where performance is comparable. The final performance of VILD with and without IS is shown in Table 1, highlighting the poor performance of offline methods, especially on high-dimensional tasks like Humanoid. Compounding error and low-quality demonstrations contribute to this issue, with BC performing the worst due to memorization effects in deep neural networks. Co-teaching is a state-of-the-art method to address this problem effectively. VILD with IS outperforms BC, BC-D, and Co-teaching in addressing the issue of compounding error and low-quality demonstrations. VILD accurately learns the variance ranking compared to ground-truth, except for low-level expertise demonstrators. This discrepancy may be due to the challenge of learning from highly dissimilar low-quality demonstrations. The performance of InfoGAIL varies depending on context, with discrepancies between best and worst performances. VILD with IS outperforms other methods in terms of mean performance."
}