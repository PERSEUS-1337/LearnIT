{
    "title": "S1Auv-WRZ",
    "content": "Effective training of neural networks in the low-data regime requires data augmentation to improve generalization. A Data Augmentation Generative Adversarial Network (DAGAN) is designed to generate a broader set of augmentations, enhancing standard classifiers. A DAGAN can enhance few-shot learning systems like Matching Networks, leading to significant accuracy improvements in low-data regime experiments on Omniglot, EMNIST, and VGG-Face datasets. In Matching Networks for Omniglot, there is a 0.5% increase in accuracy. Deep Neural Networks have shown remarkable performance in various domains, but struggle with limited datasets, leading to overfitting. Techniques like dropout, batch normalization, and layer normalization have been developed to combat overfitting. In low data regimes, traditional normalization techniques like batch renormalization and layer normalization may not be sufficient due to the high flexibility of the network. Data augmentation techniques, such as applying transformations like translations, rotations, and flips, can generate more data from existing datasets and improve model performance, even for models trained on large datasets like Imagenet. These techniques capitalize on known invariances that should not affect the class, making them essential for improving model performance. In this paper, a model of a larger invariance space can be learned through training a conditional generative adversarial network (GAN) in a different domain. This can then be applied in the low-data domain of interest to improve classifiers for one-shot target domains. The graphical model illustrates dataset shift in the one-shot setting, where the distribution over class labels changes significantly, affecting the distribution over latent variables. The Data Augmentation Generative Adversarial Network (DAGAN) enables effective neural network training in low-data target domains by capturing cross-class transformations. It can be applied to novel unseen classes and improve Matching Networks for few-shot target domains by augmenting data with relevant comparator points generated from the DAGAN. This approach targets distances between manifolds defined by the learnt DAGAN. The paper introduces a DAGAN trained on Omniglot dataset for low-data target domains, including EMNIST and VGG-Face datasets. It demonstrates improved performance on classification tasks using standard neural network training and one-shot meta-learning methods. The DAGAN generates high-quality samples and enhances vanilla networks and matching networks. Key contributions include using a novel GAN for data augmentation and improving classification tasks. The paper introduces DAGAN for data augmentation, demonstrating improved performance on classification tasks in low-data regimes and meta-learning. Results show performance beyond state-of-the-art in Omniglot and EMNIST cases. The novel GAN efficiently augments matching networks for one-shot learning. Transfer Learning and Dataset Shift: One-shot learning is a form of dataset shift where there is a significant change in class distributions. Generative Adversarial Networks (GAN) like Deep Convolutional GANs (DCGAN) can transfer information between domains by discriminating between true and generated examples. Recent optimization improvements have enhanced GAN approaches' ability to learn complex joint densities. Recent optimization improvements have reduced failure modes in GAN learning processes. Data augmentation is commonly used in classification problems to encode known invariances in the data. Generating additional data items through transformations can help maintain invariances, such as small shifts, rotations, or changes in intensity. Few works have attempted to learn data augmentation strategies, with one notable paper focusing on class-specific augmentation strategies. Few-shot learning approaches include hierarchical Boltzmann machines, deep learning architectures, hierarchical variational autoencoders, and GAN-based generative models. Siamese networks and nonparametric Bayesian approaches have also been effective. Meta-learning distance metrics and memory augmented networks show clear potential in one-shot learning scenarios. In the context of few-shot learning approaches, networks have proven effective in collating sparse information incrementally. Meta-learning involves learning from related problems to improve learning for a target problem, with Generative Adversarial Methods being one approach for generating examples matching the density of a training dataset. Generative Adversarial Networks (GAN) learn by minimizing distribution discrepancy between generated and true data. The DAGAN Architecture consists of a generator network and an adversarial discriminator network, enabling the generation of new images within the same class. A generative adversarial network can generate new images within the same class but different enough to be considered a different sample. It learns transformations that map out a data manifold, allowing for data augmentation. By combining a generative model with a neural network, it can generate meaningful representations of data points and generate extra augmentation data. The data augmentation model involves generating extra augmentation data using a generative network and a standard Gaussian distribution. The model can be learned in the source domain using an adversarial approach with an improved WGAN critic. The generator x g is trained to minimize discriminative capability measured by Wasserstein distance. Providing original x to discriminator ensures generation of diverse data. A UResNet generator with 8 blocks is used in experiments. The UResNet generator consists of 8 blocks, each with 4 convolutional layers and downscaling or upscaling layers. Different filter sizes were used for Omniglot, EMNIST, and VGG-Faces experiments. Skip connections and strided 1x1 convolutions were also implemented for gradient flow. The DenseNet discriminator in the DAGAN model used layer normalization instead of batch normalization to maintain the WGAN objective function. It consisted of 4 Dense Blocks and 4 Transition Layers with a growth rate of k = 64. Dropout was applied at the last convolutional layer of each Dense Block to improve sample quality. The model was trained for 500 epochs with a learning rate of 0.0001 and an Adam optimizer. The DenseNet classifier used in classification experiments had 4 Dense Blocks and 4 Transition Layers with k = 64. Each Dense Block contained 3 convolutional layers, totaling 17 layers. A dropout of 0.5 was applied on the last convolutional layer in each Dense Block. Standard augmentation techniques were used during training, including random Gaussian noise, shifts, and rotations. The classifier was trained for 200 epochs with a learning rate of 0.001 and tested on Omniglot, EMNIST, and VGG-Faces datasets. The classifier networks were trained on data split into test cases, validation cases, and training cases. Hyperparameter choice was made on validation cases, and test performance was reported on test cases for the target domain set. For one-shot networks, DAGAN training was done on the source domain, and meta learning on the source domain, with results presented on the target domain data. Training cases varied in number, and results were presented on test cases for each target domain class. The Omniglot data BID24 and EMNIST data were split into source and target domains with shuffled class orders. A subset of 100 samples per class was chosen to create a low-data task. In the VGG-Face dataset, 100 uncorrupted images were randomly selected from each class. The resulting dataset was split into source domains for further analysis. The dataset was split into source and target domains, with classes shuffled. A DAGAN was trained on Source Omniglot domains using various architectures, with UResNet being the preferred model. The DAGAN was also trained on VGG-Faces source domains. Improved variability in generations was achieved with more powerful architectures. Examples of generated faces are shown. The DAGAN's ability to augment vanilla classifiers trained on each target domain was tested. The classifier was trained on real data with standard data augmentation, and also on DAGAN generated augmented data with real or fake labels. The network learned to emphasize true over generated data, crucial for maximizing DAGAN augmentations. Varying numbers of augmented samples were provided for each real example in each training cycle. Augmentation improved classification in all cases, as shown in Table 1. An interpolated spherical subspace of GAN generation space using a single real seed image was also demonstrated. Matching Networks, introduced by BID40, create a predictor from a support set in the target domain using an attention memory network. This allows for comparing a test example with each training example by simulating small support sets from the source domain. Matching networks can only learn to match based on individual examples. By augmenting support sets and learning from augmented data, we enhance the classification power of matching networks. We train DAGAN on the source domain and then train matching networks along with a sample-selector neural network to select the best representative input. This process is fully trainable and tested on the validation domain to select the best performing network combination for the test domain. In experiments with matching networks and DAGAN augmentation, the ratio of generated to real data was varied from 0 to 2. Results showed a 33.815% improvement in Omniglot one-shot tasks and a 0.5% improvement over the state of the art, pushing performance to the level of Conv Arc BID32 technique. The study experimented with Omniglot, EMNIST, and VGG-Face datasets using DAGAN augmentation to improve network performance. One-shot results showed a 1.8% improvement over the baseline matching network for EMNIST. In VGG-Face experiments, the augmented system performed similarly to the baseline, indicating the embedding network may not have had enough capacity for the task. The study used a embedding network architecture with 4 convolutional layers with 64 filters each. Larger embedding functions are needed to improve DAGAN performance on VGG-Face dataset. Data augmentation with DAGANs can enhance classifier performance even after standard data augmentation. Meta-learning the best augmentation choice in a one-shot setting leads to better performance than other methods. DAGANs could be beneficial in low data settings. The study demonstrated that DAGAN augmentation can significantly improve classifier performance in low data settings. Results show that DAGAN augmentation can enhance matching networks to the level of Conv-ARC BID32, which utilizes the stroke structure of characters for improved performance. Additionally, DAGAN augmentation can also boost simple pixel distance nearest neighbor models to non-negligible levels. The UResNet generator network and model details are provided in the appendix, with full code available upon acceptance."
}