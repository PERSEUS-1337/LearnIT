{
    "title": "rkgpCoRctm",
    "content": "The paper demonstrates a method using deep feature statistics to distinguish between in-distribution and out-of-distribution samples without the need for re-training or model changes. This method significantly outperforms existing techniques in benchmarking tasks, improving the true negative rate from 39.6% to 95.3% on CIFAR-100 dataset using DenseNet. Deep neural networks have become state-of-the-art in various domains due to their high accuracy and generalization ability. However, they tend to be overconfident when tested on unseen samples and are easily fooled by minor perturbations. Calibrating confidence scores from these networks remains a challenge. Obtaining a calibrated confidence score from deep neural networks is a key challenge in AI safety. Despite low classification errors, DNNs often provide overconfident scores due to factors like depth, width, weight decay, and batch normalization. Temperature scaling in softmax scores has been shown as an effective method to improve calibration. The problem of detecting samples generated from a known distribution (out-of-distribution) remains open despite effective solutions for training data. One approach is to calibrate the classifier's confidence by training a secondary classifier on both in-distribution (ID) and out-of-distribution (OOD) data to score anomalies differently. Re-training networks can be computationally intensive, with other solutions involving training both classification and generative neural networks. In this work, a novel method is proposed for detecting out-of-distribution (OOD) samples by exploring low-level statistics from feature layers, avoiding the need for re-training models with different loss functions and additional parameters. The proposed method extracts statistics from batch normalization layers in DNN architectures like DenseNet and ResNet to detect out-of-distribution samples without the need for re-training or changing network architecture. The method uses mean and standard deviation differences between in-distribution and out-of-distribution samples as features for an OOD detector. The proposed method achieves state-of-the-art performance in image classification tasks, surpassing competitors by a large margin. It is more efficient, requiring only one forward pass compared to other methods. The paper discusses prior work on OOD samples detection, introduces the proposed method, details experiments, and compares results with state-of-the-art methods. Recent work on OOD detection methods is also described. The ODIN method uses temperature scaling and small perturbations to improve the separation between in-distribution (ID) and out-of-distribution (OOD) samples in neural networks. It outperforms baseline methods in detecting OOD samples. ODIN improves ID and OOD sample separation, outperforming BID12 but is slower. BID29 introduces margin entropy loss to increase distance between ID and OOD samples. They partition training data into ID and OOD, train an ensemble of classifiers, and use input pre-processing. BID21 is the latest work on OOD detection using generative classifier posterior distribution. The generative classifier under Gaussian discriminant analysis improves separation between in-and out-of-distribution samples by defining confidence scores using Mahalanobis distance. Abnormal samples are better characterized in the DNN feature space rather than the softmax-based posterior distribution. Intermediate generative classifiers for all layers in the network are considered to enhance performance, with the final OOD sample detector computed as an ensemble of confidence scores. The proposed model for out-of-distribution detection incorporates information from training data without being influenced by the loss function. It utilizes a linear decision function at each BN layer to predict if a sample is out-of-distribution. The model aims to be applicable to a wide variety of networks and not specific to a given architecture. The method for out-of-distribution detection involves using a linear decision function at each BN layer to distinguish between ID and OOD samples based on feature space statistics. The model focuses on utilizing first and second order statistics within each feature map for effective discrimination. The linear classifier combines statistics from different layers to detect out-of-distribution samples. It proposes using mean and standard deviation along the spatial dimension for each channel to summarize the per-channel distribution, which can distinguish between ID and OOD effectively. The statistics normalized by BN layers during training can effectively distinguish between in-distribution (ID) and out-of-distribution (OOD) samples. The mean and variance are calculated per channel per layer, aiding in OOD detection. The features for ID/OOD discrimination are derived from low-order statistics and combined within each BN layer to obtain average mean and average variance per layer. This aggregation takes advantage of the normalized features by simply averaging them for all channels within a layer. The features for ID/OOD discrimination are derived from low-order statistics and combined within each BN layer to obtain average mean and average variance per layer. Aggregating the normalized features by averaging them for all channels within a layer may not be suitable for deeper layers where activations are concentrated over fewer channels. However, experiments show that this did not impact the proposed method's performance, but further investigation is needed. The final classification involves fitting a logistic regression model with learned parameters using a separate validation set of ID and OOD samples. In this section, the proposed method is evaluated in state-of-the-art deep neural architectures like DenseNet BID15 and Wide ResNet BID32 on various computer vision benchmark datasets including CIFAR BID17, TinyImageNet, LSUN, and iSUN. Gaussian noise and uniform noise are used as synthetic datasets. Experiments were conducted on four models trained from scratch for each architecture to address parameter variance. The code for reproducing results is publicly available. For backbone training, CIFAR-10 and CIFAR-100 datasets with 50,000 images in the training set and 10,000 images in the test set are used. At test time, CIFAR-10 and CIFAR-100 test images are considered as positive samples. Various natural and synthetic datasets are used as negative samples. Validation/test set split follows a specific procedure. Backbone models DenseNet and Wide ResNet are used for training. The benchmark networks used for training are DenseNet-BC-100-12 and WRN-28-10. Hyperparameters are kept identical to their original papers. DenseNet is trained for 300 epochs with a batch size of 64, while WRN is trained for 200 epochs with a batch size of 128. Error rates for each network are shown in TAB2 over 4 independent runs. Logistic regression is trained using only validation partitions. The logistic regressor is trained using validation partitions for ID and OOD datasets. Features include mean and standard deviation, with 50 features for WRN-28-10 models and 198 features for DenseNet-BC-100-12 models. Training involves 5-fold cross-validation with 2 minimization methods and regularization factor selection. Evaluation metrics include True Negative Rate (TNR) at 95% True Positive Rate (TPR) and Area Under the Receiver Operating Characteristic Curve (AUROC). The logistic regressor is trained using validation partitions for ID and OOD datasets, with features including mean and standard deviation. t-SNE is applied to visualize the high-dimensional feature space, showing clear clusters and differences between synthetic and natural images. OOD datasets like TinyImageNet and LSUN have distinct behavior, reflecting dataset generation methods. The OOD samples are in different clusters than the ID (CIFAR-10) ones, indicating the feature choice is suitable for separation. A comparison was made between the proposed method and four recent methods using various datasets. The OOD classifier is trained for each ID/OOD pair, with results showing differences in behavior between synthetic and natural images. Our method outperforms baseline and ODIN methods, as well as BID21 and BID29, without requiring preprocessing or changing the backbone model. It can correctly detect all OOD samples with a 95% TPR, showcasing superior performance in practical applications. A good OOD detector should be able to identify samples from OOD distributions for which its parameters have not been adjusted. We propose fitting an OOD detector to one or a few OOD datasets and testing it on all available OOD datasets, which is not a standard practice in previous works. Preliminary experiments evaluate the generalization ability of the detector and guide the selection of features, such as layers average mean and standard deviation. In this experiment, the performance of a linear classifier using standard deviation features was evaluated on various OOD datasets. The classifier showed good generalization to unknown datasets, leading to the decision to use only standard deviations per layer as features for training. In this experiment, a logistic regression classifier was trained on CIFAR-100 and OOD datasets using standard deviation features. The classifier's performance was evaluated on unseen OOD datasets, showing good generalization for natural images but not for random noise datasets. The effectiveness of the method was further evaluated by testing an extreme case where no OOD samples were available for training. An unsupervised algorithm was used to fit only to ID samples, showing reasonable performance even without OOD samples. This confirms that the features used are a good indicator of OOD-ness. Performance comparison between our method and ODIN BID22 on the TinyImageNet dataset shows our model outperforms ODIN by a large margin, indicating better generalization to unseen OOD datasets. Using batch statistics computed by BN improves generalization to unseen OOD datasets. Our method can detect OOD samples accurately with only a small number of images. For WRN 28-10 trained on CIFAR-10 (CIFAR-100), using just 27 images is enough to achieve 87.6% TNR @ 95% TPR. Our proposed OOD detector, trained on TinyImageNet and Gaussian validation sets, outperforms ODIN in differentiating CIFAR-100 from other OOD datasets. Deep neural networks trained for specific datasets show stable activation statistics for in-distribution samples but significant deviations for out-of-distribution samples. This observation forms the basis of our proposed method for OOD detection. Our method for detecting out-of-distribution samples is based on computing averages of low-order statistics at batch normalization layers and using them as features in a linear classifier. It outperforms current state-of-the-art methods by a large margin in traditional ID/OOD fitting tasks. Additionally, our method generalizes well to unseen OOD datasets and even performs reasonably in unsupervised scenarios. Our method for detecting out-of-distribution samples involves training a Wide ResNet with specific parameters and conducting tuning on in-and out-of-distribution samples. A grid search is used for reproducibility, and results on CIFAR-10 and CIFAR-100 are compared using various metrics including AUPR. Additionally, the method generalizes well to unseen OOD datasets and performs reasonably in unsupervised scenarios. The study tested the OOD detector using the SVHN dataset and DenseNet BC 100-12 model. Results showed generalization to unseen OOD sets with TNR @ 95% TPR values. The logistic regression was fitted on TinyImageNet + Gaussian validation sets, yielding average results across all OOD test datasets. The proposed model was tested on different TPR levels, and the results are averaged over all OOD test datasets, as shown in TAB1."
}