{
    "title": "rkgNKkHtvB",
    "content": "Large Transformer models achieve state-of-the-art results on various tasks, but training them can be costly, especially on long sequences. Two techniques are introduced to improve efficiency: using locality-sensitive hashing for attention complexity reduction and reversible residual layers to store activations only once. The resulting model, the Reformer, performs similarly to Transformer models. Large Transformer models are widely used in natural language processing, yielding state-of-the-art results on various tasks. Researchers have trained ever larger Transformer models with up to 0.5B parameters per layer and 64 layers. These models can process sequences of up to 11 thousand tokens, including text, music, and images. However, the resource-intensive nature of these models has led to concerns about the sustainability of this trend in NLP research. Large Transformer models with 0.5B parameters per layer require significant resources for training, leading to the question of whether such resources are truly necessary. Despite calculations showing that memory requirements could theoretically be met on a single accelerator, the reality is that these models cannot be fine-tuned on single machines. This discrepancy raises concerns about the efficiency of large Transformer models and their practicality for widespread use. The Reformer model addresses memory issues in large Transformer models by using reversible layers and other techniques to optimize memory usage. The Reformer model utilizes reversible layers and other memory-saving techniques to optimize memory usage in large Transformer models. These techniques include splitting activations in feed-forward layers, approximate attention computation using locality-sensitive hashing, and using reversible residuals. These changes have a negligible impact on the training process compared to the standard Transformer model. Locality-sensitive hashing in attention is a significant change in the Reformer model that can impact training dynamics. By experimenting with different parameters, a value efficient for use and yielding results close to full attention was found. The Reformer model matches results of the full Transformer but runs faster and with better memory efficiency, especially on text tasks. The standard attention in the Transformer is scaled dot-product attention, where queries, keys, and values are computed and scaled before applying a softmax function. The attention function in the Transformer model involves computing queries, keys, and values, scaling them, and applying a softmax function to obtain weights. Multi-head attention is used to compute the outputs. Training the model on long sequences requires a large amount of memory, but the QK T matrix does not need to be fully materialized. Attention can be computed for each query separately, reducing memory usage. The Transformer model uses a memory-efficient implementation of attention for full-attention baselines. Q, K, and V are derived from a single tensor of activations A using linear layers. LSH attention involves random rotations to establish buckets for queries and keys. In LSH attention, Q, K, and V tensors are used with a shared-QK Transformer model. The main focus is on the attention computation involving the term QK T, which is normalized for performance evaluation. In LSH attention, the focus is on efficiently finding nearest neighbors in high-dimensional spaces using locality-sensitive hashing. This hashing scheme assigns vectors to hashes so that nearby vectors have the same hash with high probability. Random projections are used to achieve this efficiently. LSH attention uses random projections and hashing to efficiently find nearest neighbors in high-dimensional spaces. The method employs a known LSH scheme and is easy to implement. The LSH attention formalizes the attention mechanism for a single query position, introducing notation for the set the query attends to and the partition function. Attention is typically performed over a larger set for batching purposes. LSH attention restricts the set of target items a query can attend to by using hash buckets. The attention matrix for full attention is sparse but not efficiently utilized. Sorting queries and keys by hash bucket allows for approximating the full attention pattern within each bucket. Uneven bucket sizes make batching across buckets challenging. To address uneven bucket sizes, the LSH attention method ensures h(k j ) = h(q j ) by setting k j = qj qj. Queries are sorted by bucket number and sequence position, forming a permutation. Batching involves chunks of consecutive queries attending to each other and one chunk back. The process is detailed in Figure 2, with an average bucket size of l n buckets and m = 2l n buckets set for practicality. The LSH attention method involves multiple rounds of hashing with distinct hash functions to reduce the probability of similar items falling into different buckets. Causal masking is used to prevent positions from attending into the future in Transformer decoder, implemented by associating query/key vectors with position indices and re-ordering them using permutations. LSH attention involves hashing to reduce similar items falling into different buckets. Modified masking forbids a token from attending to itself, except in certain cases. A synthetic task of duplicating symbol sequences is used to study LSH attention. To study LSH attention, a language model is trained on examples where each word is of length 511. The task can be solved perfectly by a 1-layer Transformer model with non-local attention lookups. A 1-layer Transformer with specific parameters is trained in different settings, including full attention and LSH attention with varying rounds. The study explores LSH attention by training a language model on examples with words of length 511. Results show that models trained with LSH attention achieve almost perfect accuracy, with the best performance seen when evaluated with 8 hashes. The complexity of attention can be reduced from square to linear, but a model cost term cannot be avoided. In this section, the text discusses how to reduce memory usage in big Transformers by using reversible layers and chunking. Reversible residual networks, introduced by Gomez et al. (2017), allow activations at any layer to be recovered from the following layer using only model parameters. This eliminates the need to checkpoint intermediate values for the backward pass. Reversible residual networks, introduced by Gomez et al. (2017), allow activations at any layer to be recovered from the following layer using only model parameters. This eliminates the need to checkpoint intermediate values for the backward pass. Rather than storing activations in each layer, the reversible Transformer reverses layers one-by-one during back-propagation, combining attention and feed-forward layers inside the revnet block. This approach reduces memory usage in big Transformers. In Section 5, it is shown that the reversible Transformer performs similarly to the normal Transformer with the same number of parameters. Thicker layers can still consume a lot of memory, especially the feed-forward layer which can use high-dimensional vectors. To reduce memory usage, computations in feed-forward layers can be split into chunks. Additionally, for models with large vocabularies, log-probabilities at the output are chunked for calculating loss. With chunking and reversible layers, memory usage for activations in the network is independent of the number of layers. Parameters can be swapped to and from CPU memory efficiently in Reformer due to large batch sizes. This contrasts with standard Transformers where memory transfer to CPU is slow. The Reformer model extends the Transformer to handle diverse data types. The Reformer model extends the Transformer to handle diverse data types such as music scores and images. It has been successfully applied in self-supervised training of large language models. Efforts are being made to reduce the memory footprint and computational requirements of Transformer models, including exploring more efficient versions of the self-attention mechanism. Leveraging sparsity in attention layers has shown promise in this regard. OpenAI introduced the sparse Transformer which utilizes sparsity in attention layers. Product-key attention and locality-sensitive hashing have not been directly applied to Transformer attention layers before. Previous work on external memory with neural networks used memories of large sizes, requiring fixed memory prior to training and strong supervision to encourage correct memory queries. In previous work, external memory with neural networks required fixed memory and strong supervision for correct memory queries. Techniques such as reversible layers and shared query-key spaces did not impact performance. Hashing attention and the full Reformer model were analyzed for their impact on performance in experiments on imagenet64. In experiments on imagenet64, hashing attention and the full Reformer model were analyzed. The experiments were conducted on tasks like enwik8-64K, using 3-layer models with specific parameters. The models were trained with Adafactor optimizer and batch sizes of one sequence per GPU. The effect of shared-QK attention on a regular Transformer model was also considered. In Figure 3, perplexity curves for regular and shared-QK attention are compared. Shared-QK attention performs similarly to regular attention, showing no sacrifice in accuracy. The reversible Transformer model is also compared to a regular Transformer, with both models showing nearly identical learning curves and accuracy. The memory savings in the reversible Transformer do not compromise accuracy. LSH attention in Transformer is an approximation for full attention that becomes more accurate with an increasing number of hashes. The computational cost grows with the number of hashes, so this hyperparameter can be adjusted based on available resources. The number of hashes can be increased at evaluation time for more accurate results. LSH attention speed remains consistent with longer sequence lengths compared to regular attention. Large Reformer models can fit on a single core and train quickly on long sequences. The Reformer model, with up to 20 layers, shows clear improvements in training efficiency and memory usage compared to Transformer baselines. A 12-layer Reformer model achieved 1.05 bits/dim on the enwik8 test set, showcasing its ability to handle long sequences effectively. This efficiency makes large, parameter-rich Transformer models more accessible and widespread. The Reformer model's ability to handle long sequences effectively opens up possibilities for various generative tasks beyond text generation, such as time-series forecasting, music, image, and video generation. The multi-hash version of the LSH attention mechanism is described in detail, allowing for independent computation of query positions in batching for efficient processing. In the implementation of the Reformer model, a term N i,j is included to prevent double-counting elements in the union of sets. The masking term m is adjusted to handle a special case for i = j, ensuring that position i does not attend to itself in a shared-QK formulation. This prevents attention-in-place, except when a token has no other valid targets, like the first token in a sequence."
}