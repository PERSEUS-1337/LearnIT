{
    "title": "rkgO66VKDS",
    "content": "Learned Step Size Quantization is a method for training deep networks with low precision operations, achieving high accuracy on the ImageNet dataset with 2-, 3-, or 4-bit precision models. It improves quantized network training by estimating and scaling task loss gradients at each layer's quantizer step size, allowing for learning alongside other network parameters. This approach adapts to different levels of precision as needed. Deep networks are being used in various technologies such as image recognition, speech recognition, and driving assistance. Research is focused on developing methods to create deep networks that maintain high accuracy while reducing the precision needed for their activations and weights. This reduces computation and memory requirements, making them suitable for low precision hardware. The advantages of using algorithms to create networks for low precision hardware have been demonstrated in deployed systems. Low precision networks can be trained with stochastic gradient descent by updating high precision weights that are quantized, along with activations. The quantization is defined by mapping real numbers to discrete values supported by low precision representation. Most approaches for training low precision networks have used uniform quantizers, but optimizing task performance with this method remains an open question. Recent work in the field of low precision deep networks has focused on optimizing quantization methods to fit the data distribution or minimize quantization error during training. Various techniques such as backpropagation, FAQ, LQ-Nets, PACT, Regularization, and NICE have been used to learn quantizers that minimize task loss. Recent work in low precision deep networks has focused on optimizing quantization methods to minimize task loss. Fixed mapping schemes based on user settings do not guarantee optimal network performance. Learning the quantization mapping to minimize task loss directly improves the metric of interest. A new approach is introduced to learn the quantization mapping for each layer in a deep network, called Learned Step. LSQ is a new method for learning the quantization mapping in deep networks. It improves on prior methods by approximating the gradient to the quantizer step size and balancing step size updates with weight updates. LSQ can be used for quantizing activations and weights, and has shown significantly better accuracy on the ImageNet dataset compared to previous approaches. It also achieves the milestone of 3-bit quantized networks. LSQ is a new method for learning the quantization mapping in deep networks, achieving the milestone of 3-bit quantized networks reaching full precision accuracy. It involves quantizing weights and activations using low precision integer operations, with defined quantization levels and step sizes. LSQ enables learning the quantization mapping in deep networks by quantizing weights and activations using low precision integer operations. It involves defining quantization levels and step sizes, allowing for input values to be used in low precision integer matrix multiplication units for inference. The output is then rescaled using a high precision scalar-tensor multiplication, potentially merged with other operations like batch normalization. The step size parameter is learned based on training loss through a gradient derived using the straight through estimator. LSQ enables learning quantization mapping in deep networks by quantizing weights and activations using low precision integer operations. The gradient in LSQ differs from related approximations by learning a transformation of the data prior to discretization. Previous approaches do not consider the proximity of values to the transition point between quantized states when estimating the gradient. LSQ gradient is more effective than QIL and PACT gradients in training networks, as it considers the proximity of values to quantization transition points. This results in higher accuracy and a more natural gradient descent process. The LSQ gradient naturally emerges from the quantizer formulation and straight through estimator. Each layer has a distinct step size for weights and activations, ensuring good convergence during training. This approach helps prevent overshooting of local minima and reduces convergence time. For each layer, LSQ gradient adjusts step sizes for weights and activations to aid convergence in training. Step size updates are scaled based on the number of weights and features in a layer, improving trained accuracy. In Section A of the Appendix, model quantizers are trained with LSQ by making step sizes learnable parameters. Full precision weights are stored and updated, while quantized weights and activations are used for forward and backward passes. Stochastic gradient descent is used to update parameters, and input activations and weights are set to 2-, 3-, 4-, or 8-bit for matrix multiplication layers. Activations and weights are quantized to 2-, 3-, 4-, or 8-bit for matrix multiplication layers, except for the first and last layers which always use 8-bit. Networks are initialized with weights from a full precision model before fine-tuning in the quantized space. Training includes a momentum of 0.9, softmax cross entropy loss function, and cosine learning rate decay. 8-bit networks are trained for 1 epoch, while other networks are trained for longer durations. In a study by McKinstry et al. (2018), networks were trained on the ImageNet dataset using various architectures such as ResNet, VGG, and SqueezeNext. Different learning rates were used for networks with different precision levels. Images were resized and cropped for training and testing. LSQ was implemented in PyTorch to reduce model overfitting and regularization. In a study by McKinstry et al. (2018), networks were trained on the ImageNet dataset using various architectures like ResNet, VGG, and SqueezeNext. LSQ was implemented in PyTorch to reduce model overfitting and regularization. A hyperparameter sweep on weight decay for ResNet-18 showed that lower precision networks achieved higher accuracy with less weight decay. Performance improved by reducing weight decay for 3-bit and 2-bit networks. These weight decay values were used for further experiments comparing accuracy with other quantized networks and full precision baselines. LSQ models achieved higher accuracy than previous approaches for 2-, 3-, and 4-bit networks. They also showed the best top-5 accuracy and top-1 accuracy on these networks, as well as on 8-bit versions. Increasing precision from 4-bit to 8-bit did not consistently improve accuracy. Another low precision method used progressive fine tuning, which increased training time and complexity compared to LSQ's direct fine-tuning approach from full precision models. When comparing full precision to 2-bit precision models, ResNet-18 only drops 2.9 in top-1 accuracy, while SqueezeNext-23-2x drops 14.0. SqueezeNext may be sensitive to reductions in precision due to its design for maximum performance with minimal parameters. Choosing the highest performing model within memory limitations is crucial for model size-limited applications. Plotting network accuracy against model size helps identify the best performing models. 2-bit ResNet-34 and ResNet-50 offer an advantage over smaller networks in terms of performance. 2-bit ResNet-34 and ResNet-50 outperform smaller networks in terms of performance, with higher precision. VGG-16bn lags behind due to older design. Figure 3 shows 2-bit networks achieve highest accuracy at a given model size. Scaling step size gradient impacts training efficiency. The imbalance between step size updates and weights increases with precision in neural networks. Adjusting for weight and feature count led to a decrease in network accuracy after training a 2-bit ResNet-18. After adjusting for weight and feature count, a decrease in top-1 accuracy was observed. Dropping the initial learning rate by multiples of ten resulted in the best top-1 accuracy achieved using no gradient scale. The full gradient scaling with a ten-fold increase or decrease also reduced accuracy. The chosen heuristic for scaling the step size loss gradient showed benefits. Cosine learning rate decay was used in experiments to avoid selecting learning rate schedule hyperparameters and minimize training time. LSQ training with step-based learning rate decay on a 2-bit ResNet-18 model achieved a top-1 accuracy of 67.2 after 90 epochs. The model showed a 1.5 improvement over other methods. LSQ was found to minimize quantization error without explicit encouragement. LSQ training on a 2-bit ResNet-18 model achieved a top-1 accuracy of 67.2 after 90 epochs, showing a 1.5 improvement over other methods. LSQ was found to minimize quantization error without explicit encouragement. The percent absolute difference between the value that minimizes quantization error and the LSQ solution was significant, indicating that LSQ does not actually minimize quantization error. LSQ combined with knowledge distillation improved low precision network training, using a distillation loss function with equal weight to standard loss. This approach increased top-1 accuracy by up to 1.1 on a 3-bit ResNet-50 model. LSQ combined with knowledge distillation improved low precision network training, increasing top-1 accuracy by up to 1.1 on a 3-bit ResNet-50 model. This approach also helped low precision networks reach the accuracy of full precision baselines, demonstrating superior performance on the ImageNet dataset. LSQ outperforms prior approaches for creating quantized networks on the ImageNet dataset. The approach involves rescaling the quantizer step size loss gradient based on layer size and precision. LSQ does not aim to minimize quantization error but achieves good performance with a simple method requiring only a single additional parameter per weight or activation layer. While 2-bit networks may not reach the accuracy of their full precision counterparts, they are useful for achieving the best accuracy for a given model size. Reducing network precision while maintaining high accuracy is a promising approach to decrease model size and increase throughput, providing performance advantages in real-world applications. This trend towards higher performance at lower precision strengthens the analogy between artificial and biological neural networks, with potential implications for data representation efficiency. The gradient scale value is computed to reduce model size and increase throughput in deployed deep networks. The approximation is based on the expected growth of l2-norm with the number of elements normalized, the balance between zero and non-zero weight values, and the precision of quantized states. The step size is adjusted based on the number of quantized states to better encode outliers and improve performance. The gradient scale factor for weights is computed to reduce model size and increase throughput in deployed deep networks. It is based on the balance between zero and non-zero weight values, the precision of quantized states, and the expected growth of l2-norm with the number of elements normalized. The step size is adjusted based on the number of quantized states to better encode outliers and improve performance. In LSQ implementation, the imbalance between step size updates and activation changes grows with the number of features in a layer. The activation step size is set to 1 / \u221a N F Q P. Pseudocode is provided for LSQ implementation, requiring functions with non-standard gradients like gradscale and roundpass. Custom gradients are implemented using a detach function for automatic differentiation in deep learning frameworks. The LSQ implementation involves a detach function that blocks gradient propagation during the backward pass. This function is used to quantize weights and activations before each convolution or fully connected layer. The pseudocode provided is simple and widely applicable, although more efficient approaches are possible. The example assumes unsigned activations but can be adjusted for signed activations."
}