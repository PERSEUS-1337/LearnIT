{
    "title": "HygJq12VtH",
    "content": "We propose new approximate Bayesian algorithms that optimize a criterion derived from the loss to calculate a pseudo-posterior. This approach provides error guarantees on predictions and can be used to derive sparse Gaussian process algorithms applicable to various likelihoods. Learning theory suggests that minimizing training set loss through empirical risk minimization (ERM) or regularized loss minimization (RLM) can lead to good solutions with bounded true loss. In this paper, the authors propose new approximate Bayesian algorithms that optimize a criterion derived from the loss to calculate a pseudo-posterior. They suggest using learning theory to guide this process, particularly in cases where exact inference is not possible. The authors propose new approximate Bayesian algorithms that optimize a criterion derived from the loss to calculate a pseudo-posterior. They suggest using learning theory to guide this process, particularly in cases where exact inference is not possible. The variational approximation is a key method for approximate inference in Bayesian models, maximizing the ELBO as a lower bound on the marginal likelihood. This approach involves minimizing the negative ELBO, which consists of a loss term and a regularization term, offering good generalization guarantees in statistical learning theory. The authors propose new approximate Bayesian algorithms that optimize a criterion derived from the loss to calculate a pseudo-posterior. They suggest using learning theory to guide this process, particularly in cases where exact inference is not possible. This differs from standard non-Bayesian algorithms that perform ERM or RLM to find the best parameter w. In this paper, the authors propose new approximate Bayesian algorithms that optimize a criterion derived from the loss to calculate a pseudo-posterior q(w). They focus on sparse Gaussian processes (sGP) and develop risk bounds for a smoothed variant of log loss, diverging from current practice in the literature. This approach offers a different principle for designing inference algorithms, moving away from optimizing the marginal likelihood or ELBO. The significance lies in the conceptual shift towards directly optimizing a criterion related to the loss. The authors propose new algorithms based on direct loss minimization for sparse Gaussian processes, with stronger risk guarantees compared to previous work. This approach diverges from existing literature by optimizing a criterion related to the loss, offering a different principle for designing inference algorithms. The authors introduce new algorithms for sparse Gaussian processes with improved risk guarantees, focusing on direct loss minimization. They aim to provide an \"agnostic PAC guarantee\" for the sparse GP posterior, regardless of its proximity to the full GP solution. The technical details are outlined in Appendices A to E, presenting three approaches for learning with a Lipschitz loss under a bounded hypothesis space. The authors introduce new algorithms for sparse Gaussian processes with improved risk guarantees, focusing on direct loss minimization. Three approaches for learning with a Lipschitz loss under a bounded hypothesis space are outlined in Appendices A to E. Approach 1 uses standard discretization, Approach 2 adapts results based on Rademacher complexity, and Approach 3 is new and has the potential to provide bounds with unbounded losses. These approaches can be utilized to obtain guarantees under a Lipschitz loss and bounded hypothesis space. The authors introduce new algorithms for sparse Gaussian processes with improved risk guarantees, focusing on direct loss minimization. Appendices D and E provide details for sGP and CTM. The algorithm considers a loss over a hypothesis space \u0398 \u2282 R M and example/label spaces X and Y. It assumes a closed and bounded hypothesis space with Lipschitz continuity. The ERM objective averages random neighbors of the solution \u03b8, leading to jitter in the randomized ERM objective. The source of jitter in the randomized ERM objective is addressed by applying the compression lemma with specific bounds to control potential overfitting. The expected risk of Randomized ERM is shown to be bounded by the risk of any posterior in \u0398 plus a decaying term at a rate of 1/ \u221a n. In the sparse GP model, the pseudo-posterior is defined by q(w|\u03b8) = N (w|m, C C) within the parameter space \u0398. The pseudo-posterior q(w|\u03b8) = N(w|m, C C) in the sparse GP model allows for exact calculation of the induced distribution q(f|\u03b8) from Gaussian identities. A \"smoothed\" log loss is defined to ensure a bounded and Lipschitz loss function with a smoothing parameter \u03b1 \u2208 (0, 1). Applying Corollary 2 to non-conjugate sparse GP models requires verifying the existence of \u03be such that E[q(f*|\u03b8)[p(y*|f*)]] < \u03be and calculating bounds on the terms. The pseudo-posterior q(w|\u03b8) = N(w|m, C C) in the sparse GP model allows for exact calculation of the induced distribution q(f|\u03b8) from Gaussian identities. Randomized ERM using smoothed log loss with the sparse GP predictive distribution enjoys strong performance guarantees for regression, binary classification, and Poisson regression. Minimizing a different loss function may require an explicit prediction in some scenarios. The Bayesian predictor minimizes a loss function by identifying the optimal prediction and suffering the loss. The natural loss term for optimization is L B = i (\u0177 q(w) (x i ), y i ). The results potentially apply to a more general setting as long as theorem conditions hold. The theory does not directly apply to square loss due to the need for smoothing, but it is interesting to consider using DLM for square loss. The paper discusses the use of sGP for prediction, highlighting the role of regularization in optimizing the predictor. The optimization criterion simplifies into a sparse variant of kernel least squares without regularization. The potential of DLM algorithms to yield a new type of approximate pseudoBayesian algorithm is also mentioned. The paper focuses on the analysis of ERM and its application to sparse GP. Future work includes analyzing RLM, hyperparameter selection, and investigating empirical properties of algorithmic variants. The proof shows that a Lipschitz condition and bounded loss make the problem simple by learning on a grid. The paper assumes a bounded loss for the discretization approach and discusses the existence of a finite \u03c1-cover of \u0398. The paper discusses the analysis of ERM and its application to sparse GP, focusing on a Lipschitz condition and bounded loss for learning on a grid. It assumes a bounded loss for the discretization approach and discusses the existence of a finite \u03c1-cover of \u0398. Meir and Zhang's result from 2003 is adapted to handle Bayesian predictors in the context of Rademacher complexity. Zhang (2003) can handle Bayesian predictors by assuming parameterized predictors and averaging predictions. The proof technique can be adapted for loss functions like the smoothed log loss. Meir and Zhang's results are for unbounded losses, but it's unclear how to apply them directly to Bayesian predictors like sparse GP. The upper bound for the predictor p(y|x, w) is shown to be less than a constant p y|w. Uniform convergence for the averaged predictor under the smoothed log loss is proven. The results from Shalev-Shwartz and Ben-David (2014) and Meir and Zhang (2003) are referenced. The argument is adapted for a sample-independent distribution p(w) over w using the compression lemma. The text discusses utilizing a lemma from ShalevShwartz and Ben-David (2014) to derive results related to Lipschitz constants and ERM hypotheses. The proof presented potentially yields bounds independent of dimension M, although there is implicit dependence on M when applied to Gaussian distributions. Meir and Zhang (2003) demonstrate how structural risk minimization can overcome dimension dependence in data-dependent bounds. They consider a family of distributions over a known subset \u0398, introducing \"jitter\" distributions. By leveraging a lemma from Germain et al. (2016) and Sheth and Khardon (2017), they derive results independent of dimension M. The novelty in this work is applying the compression lemma at a higher level than previous studies, specifically at the parameter level \u03b8. This introduces the concept of jitter, resulting in a qualitatively different outcome. The lemma presented involves applying Fubini's theorem and Jensen's inequality to derive the statement. Additionally, for an L-Lipschitz function, a norm over \u0398 is considered, leading to a general result for all \u03b8 in \u0398. The text discusses the application of the compression lemma at the parameter level \u03b8, introducing the concept of jitter. It involves applying Fubini's theorem and Jensen's inequality to derive a general result for all \u03b8 in \u0398, considering an L-Lipschitz function and a norm over \u0398. The text introduces the concept of jitter distributions and defines \u0398 parametrically as a function of some \u03c1 > 0. It discusses the uniform density jitter distribution q jit (\u03b8|\u03b8) with support in \u0398 and its relation to the compression lemma at the parameter level \u03b8. The text discusses constants \u03ba, M, and the 1-norm in relation to standard Gaussian distributions and matrix operations."
}