{
    "title": "HJxw9lStPH",
    "content": "Probability density estimation is a well-studied problem, but traditional methods struggle with complex image distributions. Generative models using neural networks can learn and represent these distributions. Explicit probability density estimates can be extracted from GANs, but interpreting natural image density functions is challenging. Better interpretability may be achieved by estimating density on latent image representations. Researchers have long aimed to estimate image probability density functions. In the context of estimating image probability density functions, recent advancements in deep generative models, particularly GANs, have shown promise in generating realistic complex images. While some models explicitly focus on building probability densities, GANs implicitly encode these densities. This paper explores whether these implicit densities capture the essence of probable images. In this paper, the authors investigate whether implicit densities in GANs capture probable images. They find that the answer is \"no\" but propose computing PDFs over latent representations for better results. By extracting probability densities from GANs and modifying formulas, they can compute densities of generated images and train a regressor for arbitrary images. Sanity checks confirm the reasonableness of GAN-produced image densities. We perform sanity checks to ensure that GANs produce reasonable densities on images, showing that they are similar for training and test images. The density of real or generated images correlates with complexity, with low complexity images having the highest density. However, interpreting these probability densities is challenging due to their unintuitive behaviors. The influence of visual complexity on learned probability density functions (PDFs) causes background details to dominate the distribution shape. For instance, a GAN trained on MNIST tends to generate mostly digit 1 images, even when other digits are equally present in the training set. This bias towards simpler backgrounds is evident even when training on CIFAR images, as the GAN still favors generating digit 1 images over CIFAR images. High-dimensional density functions have peaks of large probability density away from typical points, such as in a Gaussian with an identity covariance matrix. Real images residing in these high-density peaks pose interpretability issues. To address this, probability density estimation on latent image representations is proposed, yielding distributions that align better with intuition. In the Gaussian latent space, natural images are located near the unit sphere, mitigating the problem of high-density peaks. The interpretability of high-density peaks in density functions over images is addressed by aligning natural images near the unit sphere in latent codes. Outliers can be detected by comparing density values. Density estimation accuracy for unusual images is discussed, highlighting the difficulty in interpreting the true underlying density function. The curr_chunk discusses classical models for density estimation in low-dimensional spaces, including non-parametric methods like Kernel density estimation and nearest-neighbor classifiers. These models have limitations in scaling up to the complexity or dimensionality of image distributions. There is a history of approximating image PDFs using simple statistical models. The curr_chunk discusses various approaches to modeling image density, including statistical models like Markov models, neural networks, and Boltzmann machines. These models aim to capture the high-dimensional distribution of complex images for tasks like denoising and texture synthesis. Various methods for modeling image densities have been explored, including Smolensky (1986) and Deep Boltzmann machines (Salakhutdinov & Larochelle, 2010). While Variational Autoencoders (Kingma & Welling, 2013) offer a powerful approach, they tend to produce blurry samples and are limited in application. Generative adversarial networks (GANs) have emerged as a powerful new way to build generative models of images with realistic results. In the original GAN setup, a generator and discriminator are used to reach a saddle point through training. GANs do not provide explicit density models and are not invertible, limiting access to latent representations of images for density calculations. Real NVP is a method that learns an invertible transformation from the latent space to images, allowing for the computation of exact density values. It can be trained using maximum likelihood methods, adversarial methods, or a combination of both. In contrast, GANs do not provide explicit density models and are not invertible, limiting access to latent representations of images for density calculations. Non-invertible GANs like FlowGAN produce higher quality images than invertible GANs. They use simpler network architectures and training procedures. Our proposed methods can be applied to any GAN to leverage improvements in new architectures. GANs present challenges in extracting density estimates due to learning lower-dimensional latent spaces. In this paper, a simple regressor network is created to estimate the probability density of images without the need for complex noise models or image-to-latent code projections. Training the regressor network is straightforward with a large dataset of labeled images. Training a regressor network to estimate image probability densities is made easy with a large labeled dataset. A GAN generator produces images from a learned distribution, but the challenge lies in computing the determinant due to non-bijectivity. Calculations are performed on the low-dimensional manifold of the latent space under G to overcome this issue. The intrinsic dimensionality of the manifold is the same as the latent space. The formula for computing the determinant involves the transpose of the coordinate matrix and a metric tensor. The Jacobian can be computed analytically or numerically, and the determinant can be found via a QR decomposition. The determinant of the Jacobian can be computed via a QR decomposition. Probability predictions for novel images are generalized using a pixel regressor trained on log-probability densities. The regressor is based on a DCGAN model and is described in detail in the supplementary material. The accuracy of GAN-based density estimation relies on the quality of generated density labels and the regression network's ability to generalize. The study compares predicted density histograms in train and test datasets quantitatively and qualitatively to ensure meaningful results. Issues like under-fitting and overfitting are addressed by analyzing probability density histograms for similarities between train and test data. Results are depicted in Figure 3. The study compares predicted density histograms in train and test datasets to ensure meaningful results. Results in Figure 3 show high similarity between test and train distributions, indicating a good model fit without overfitting. GAN models for MNIST and CIFAR produce typical samples that reflect desired images, but outliers have low probability density and lie away from the distribution modes. The study compares predicted density histograms in train and test datasets to ensure meaningful results. GAN models for MNIST and CIFAR produce typical samples that reflect desired images, but outliers have low probability density and lie away from the distribution modes. More sophisticated GANs like StackGAN and StyleGAN show that low density images have complex textures, while high density images are uniform. However, these density estimators can have irregular and non-uniform distributions, making them difficult to interpret and not correlating well with human intuitions. In this section, we delve into the factors that determine image density, focusing on the most likely and least likely real images from the MNIST dataset. The preference for images of the digit 1 is explained by correct density estimation, as these images are well aligned and similar in a high-dimensional space. The digit 1s in the MNIST dataset form a high-density mode due to their alignment and similarity in a high-dimensional space. Even when trained on images excluding 1s, the density estimator still predicts 1s as the most likely digits, likely due to the constant black background in 1 images. In the CIFAR dataset, images with simple blue backgrounds are common due to the \"airplane\" class having similar backgrounds. High-density images have small objects with dominant backgrounds, while outliers have low densities in Euclidean space. In the study, a density model trained on CIFAR was used to evaluate the likelihood of MNIST images being outliers. The results showed that MNIST images were far more likely than CIFAR images, consistent with the idea that smooth, geometrically structured images lie to the right of the distribution. The study used a density model trained on CIFAR to evaluate the likelihood of MNIST images being outliers. MNIST images were found to lie in an extremely high density mode, while CIFAR images were more structured and less likely to appear in high density regions. Probability densities on complex image datasets correspond more to geometric properties than human-recognizable categories. \"Typical\" images often lie far from the distribution modes. The lack of interpretability in image distributions is due to the Euclidean distance not capturing semantic similarity. Embedding images into a deep feature space allows for meaningful semantic structure, making distributions interpretable with distinguishable modes and outliers. Selecting a deep embedding is crucial for this process. In the unsupervised setting, choosing a deep embedding is crucial for interpretability. One simple choice is to associate images with their latent representation z from a GAN. This embedding uses a Gaussian density function, learned by associating each image with the density of its pre-image z. A regressor is trained to predict the density of the z code that generated an image, known as the \"code regressor.\" This approach allows for the interpretation of image distributions with distinguishable modes and outliers. The deep feature model in Figure 7 shows diversity in likely CIFAR images, with more uniform densities than the pixel model. MNIST digits are inliers, while many CIFAR images are outliers in their own distribution. Deep feature space captures a more intuitive notion of outliers, as seen in the histogram of estimated densities in Figure 8. The histogram in Figure 8 shows CIFAR images in high density regions and MNIST images in low density \"outlier\" regions. The most probable images are from the CIFAR distribution, clustered around the unit sphere in the latent space. This explains why the rightmost histogram is more clustered than the pixel-space histogram on the left. The images on the right of Figure 5 are more typical CIFAR images. The deep feature model in Figure 7 favors well-defined foreground objects in images, with densities depending on content and color rather than complexity. GANs were used to explore complex image distributions, but interpreting inliers and outliers as typical or atypical images is challenging. The lack of interpretability in image probability densities can be addressed by considering the latent codes instead of the images themselves. This approach provides more semantically meaningful representations and can be used for various applications like outlier detection and domain shift analysis. The histograms show the difference in log probability densities predicted using pixel-space density estimators versus latent code regressors for GANs trained on different datasets. The log density values predicted by a latent code regressor for a GAN trained on MNIST are more clustered than in pixel space, remaining near the top of the distribution."
}