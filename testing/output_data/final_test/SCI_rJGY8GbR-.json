{
    "title": "rJGY8GbR-",
    "content": "In this paper, the authors explore two methods for controlling the behaviors of random residual networks without batchnorm. The first method, width variation (WV), involves varying the widths of layers as a function of depth to reduce gradient explosion. The second method, variance variation (VV), changes the initialization variances of weights and biases over depth to reduce gradient explosion of tanh and ReLU resnets. The paper explores phase transitions in dynamics affected by variance decay, showing how VV at initialization impacts training and test performance on MNIST. The mean field theory tracks the relationship between gradient norms and metric expressivity in random neural networks. Based on insights from deep mean field theory and information geometry, the gradient explosion/vanishing problems are linked to ill-conditioning of the Fisher information matrix, causing optimization issues. Deep mean field theory studies random neural networks with increasing depth and infinite width, showing regularity in neural networks. Recent research has revitalized mean field theory in deep learning, focusing on practical design questions and combining it with Riemannian geometry to quantify the expressivity of random neural networks. Theoretical studies on critical phenomena of neural networks and loss landscape properties have provided a rigorous foundation for experimental observations. Deep mean field theory, combined with Riemannian geometry, helps understand dynamical isometry in random neural networks. In this paper, the focus is on studying how width variation (WV) can impact gradient norms in neural architectures. The authors also explore variance variation (VV) in random neural networks, particularly in tanh and ReLU residual networks, showing its effectiveness in reducing gradient explosion. In this study, the authors demonstrate that Variance Variation (VV) can significantly improve gradient explosion in neural networks, particularly in tanh and ReLU residual networks. Previous research has focused on how network architecture and activation functions impact mean field dynamics, with VV showing promising results in enhancing performance. In this study, it is shown that Variance Variation (VV) can control gradient dynamics and improve performance in neural networks without changing architecture or activation functions. Methods for reducing gradient explosion or vanishing by varying initialization variance and/or width across layers are proposed for the first time. Gradient norms and \"metric expressivity\" are found to be good predictors of performance based on VV at initialization in ReLU resnets. Larger gradient explosion in one phase seems to lead to better performance. In this study, larger gradient explosion in one phase appears to improve performance, with no clear explanation provided. The optimal initialization for tanh resnets balances trainability and expressivity. The focus is on relative performance between different initialization schemes rather than absolute performance. The goal is to determine which initialization is better for test set accuracy after the same amount of training. The Fisher information matrix ill-conditioning is linked to the gradient explosion/vanishing problem. Information geometry shows that the Fisher matrix defines a Riemannian manifold for probability distributions. This concept is utilized in the natural gradient method for optimization. The natural gradient method computes a \"natural direction of greatest descent\" that is invariant to reparametrization. It has been successfully applied in supervised, unsupervised, and reinforcement learning. An F(\u03b8) with eigenvalues all approximately equal indicates an isotropically curved neighborhood, while a large condition number \u03ba(F(\u03b8)) means the gradient is a poor proxy for the natural gradient. F(\u03b8) is also the Hessian of the KL divergence. The condition number of the Fisher information matrix for a deep network is exponentially large in depth with high probability. The matrix can be partitioned into blocks based on parameter groups. The largest eigenvalue of the Fisher information matrix is determined by the Hermitian min-max theorem. The largest eigenvalue of F(\u03b8) is given by max x=1 xTF(\u03b8)x and the smallest eigenvalue is given by min x=1 xTF(\u03b8)x. The condition number of the Fisher information matrix for a deep network is exponentially large in depth with high probability. The condition number of the Fisher information matrix for a deep network is exponentially large in depth with high probability, causing gradient dynamical problems that deviate exponentially from the natural gradient and violate the information geometry of the information manifold P \u03b8. This leads to the number of gradient descent iterations diverging exponentially when minimizing KL divergence from a specific distribution. Adjusting the learning rate alone cannot solve these issues. The depth-wise dynamics of random neural networks at initialization time have been studied in recent papers, showing that activation norm and angle between input vectors converge exponentially fast in depth. This has led to a resurgence of mean field techniques applied to deep learning. Yang and Schoenholz (2017) introduced \"angular expressivity\" as a measure of network performance. They found that networks initialized on a \"critical curve\" show neither exploding nor vanishing gradients, leading to the best test time performance. Adding skip connections to tanh networks makes their dynamics subexponential. Gradient dynamics control the test time performance of tanh residual networks. The expressivity of ReLU resnets controls their test time performances, while gradient dynamics influence tanh resnets. Metric expressivity measures the distance between input vectors in a random network. Angular expressivity, proposed in a previous study, does not predict test time performance. The optimal weight variance for tanh resnets balances trainability and expressivity to prevent gradient explosion or network collapse. In this paper, the authors extend previous results on weight variance optimization for ReLU resnets to include depthwise variation of widths and variances. They show that this approach can reduce gradient explosion and manipulate the expressivity of random networks. The findings also improve the performance of tanh resnets by controlling gradient dynamics. In this study, Yang and Schoenholz (2017) enhance tanh resnet performance by managing gradient dynamics and prevent activations from overflowing in ReLU resnets. However, there are instances where worsening gradient explosion oddly leads to better performance in ReLU resnets, a phenomenon that remains unexplained. The neural network structure is defined with layers and neurons, utilizing activation functions like tanh or ReLU. In this study, a simplified residual architecture is adopted for analysis, where every residual block is defined by a set of weights and biases for each layer. To adjust the width of a residual network, projection residual blocks are inserted every couple of layers. A simplified projection residual block is assumed for ease of presentation. The paper introduces a simplified projection residual block for analysis, focusing on the average behavior of fully-connected affine layers. The study explores varying weight parameter variances across depth, while keeping the projection matrix constant. The paper defines central quantities for deep mean field theory in neural networks, with a glossary of symbols provided to reduce notation confusion. Expectations are taken over random weight and bias initializations in the large width limit. The paper defines key quantities for deep mean field theory in neural networks, including correlation and gradient quantities. Expectations are taken over random weight and bias initializations in the large width limit. In deep mean field theory, investigations of large width networks rely on the central limit theorem. Activations passing through affine layers result in Gaussian variables, leading to a depthwise dynamical system of activation random variables. Gradient dynamics can be derived based on certain assumptions. The gradient dynamics in deep mean field theory can be derived based on certain assumptions. While WV does not change the mean dynamics of forward quantities, it can control gradient dynamics. VV, on the other hand, affects both forward and backward dynamics of mean field quantities. Phase transitions show that as variance is dampened with depth, both forward and backward dynamics will also dampen, except for weight gradients in ReLU resnets. In contrast to WV, VV theory is influenced by different quantities depending on the nonlinearity used in neural networks. Experiments show that the complexities of VV theory are reflected in the practice of training neural networks, allowing for the prediction of test time accuracy using mean field theory. Yang and Schoenholz (2017) observed that the optimal initialization scheme for tanh resnets balances expressivity and trainability by avoiding gradient explosion or network constancy issues. The tradeoff between expressivity and trainability in ReLU resnets is not observed due to bounded gradients against weight parameters. In tanh resnets, decaying initialization variances with depth affects test set accuracy. Gradient explosion limits accuracy without decay, while strong decay constrains expressivity and bottlenecks performance. Test set accuracy can be predicted by gradient norm ratio in the region of small variance decay and by metric expressivity in the region of large decay. The performance of deep networks is influenced by the interaction between metric expressivity and gradient dynamics. In regions of large decay, the intersection of these factors leads to peak performance. While mean forward dynamics are preserved, variance increases in deeper layers with width decay. Gradient dynamics are suppressed compared to tanh resnets without width decay. The tradeoff between expressivity and trainability is not observed in ReLU resnets. In the regime of small to moderate variance decay, VV affects metric expressivity rather than gradient dynamics. Width variation (WV) in deep networks can have unexpected effects on gradient dynamics. Strong decay can lead to gradient explosion, but worse explosion can actually correlate with better performance. The phenomenon is not yet fully understood, but width variation has been shown to impact the mean gradient norm in different ways. Width variation in deep networks can have unexpected effects on gradient dynamics. Changing the width can cancel out gradient expansion, leading to a restart of backpropagation at a certain layer. Remarkably, altering the width does not affect the mean field forward dynamics. However, reducing the width can increase the variance of the sampled dynamics, while increasing the width can decrease the variance. Increasing the widths of earlier layers while keeping N (L) the same can decrease the variance of sampled dynamics in tanh residual networks. Gradient norms are bounded across layers when projection blocks are placed, as shown in the comparison in FIG1. Different hyperparameter settings were tested on MNIST, and the network's accuracy on training and test sets was reported after 30 epochs. The network's accuracy on the training and test sets was reported after 30 epochs. Plots show contour plots of different dynamics with and without projection blocks. The theory accurately tracks gradient dynamics with and without projection blocks, with a noticeable increase in deviation from mean dynamics at projection layers in the forward case. Width variation in neural networks can help mitigate gradient explosion or vanishing issues without affecting the mean forward dynamics of the network. This can be achieved by either truncating neurons to reduce parameters or adding neurons for larger deviations from the mean dynamics. Adding neurons to a neural network increases parameters, compute, and decreases deviations from mean dynamics. Variance variation in ReLU resnet involves a zigzag of parameters controlling different asymptotics, with gradient dynamics becoming more explosive. The zig increases polynomial dependence in l, while the zag reduces it to \u0398(log l) and \u0398(1). For more detailed information, refer to Appendix B.1. In experiments testing VV dynamics for ReLU resnet, parameter space is explored by training randomly initialized networks. Results are shown in FIG2 with details in the caption. Heatmaps and contour plots of quantities like p, e, and \u03c7 are provided. Sweeps of Ut are conducted to determine performance, showing almost identical heatmaps for different approaches. In experiments testing VV dynamics for ReLU resnet, parameter space is explored by training randomly initialized networks. Results are shown in FIG2 with details in the caption. Heatmaps and contour plots of quantities like p, e, and \u03c7 are provided. Sweeps of Ut are conducted to determine performance, showing almost identical heatmaps for different approaches. The heatmap is obtained with the same procedure as in FIG2 from the test set after training. Training fails in the upper left corner due to numerical instability caused by exploding p and \u03c7. Increasing \u03b2 w = \u03b2 b effectively solves the activation explosion problem without changing the activation function. Performance dips in the direction where \u03c7 (0) /\u03c7 (l) decreases, explained by the bounded and polynomial behavior of gradients against weights, \u03c7 w and \u03c7 v. Biases experience similar behavior as \u03c7 but are less important. Performance also dips where s decreases. The weight gradient dynamics become more explosive as s decreases exponentially in (V r , L)-space, leading to a loss in expressivity that dominates performance. In the zag regime, greater gradient explosion correlates with better performance, especially when \u03b2 a is large. The level curves of \u03c7 v accurately predict test set performance. The weight gradient dynamics are explosive in (V r , L)-space, leading to a loss in expressivity. In certain situations, larger gradient expansion may be beneficial, contrary to prevailing theory. The VV dynamics of tanh resnets are briefly discussed, with a focus on scenarios where higher layers are essentially unused by the network. The weight gradient dynamics in tanh resnets are determined by different phases of tanh resnet VV dynamics. The gradient ratio \u03c7 (0) /\u03c7 (l) transitions from exponential to polynomial to bounded as W t changes. The dynamics of p are simple, with p = \u0398(l 1\u2212Ut ) when U t < 1 and p = \u0398(log l) when U t = 1. This theory helps predict and optimize VV initialization schemes for tanh resnets, as shown in experimental results in FIG4. In FIG4, hyperparameters \u03b2 affect U t = min(\u03b2 v , \u03b2 a) with peak accuracies in upper left. Gradient norm \u03c7 DISPLAYFORM0 w predicts accuracy to the left of peak, while metric expressivity s predicts to the right. Heatmap level set boundaries' slopes decrease as accuracy levels increase, but become steeper at peak. The study in FIG4 shows that hyperparameters affect peak accuracies, with gradient norm predicting accuracy to the left of the peak and metric expressivity predicting to the right. The mean field theory developed in the paper allows for manipulation of the dynamics of a residual neural network, leading to accurate predictions of performance on trained MNIST data. The mean field theory accurately predicts the performances of trained MNIST models with different initializations. Test set accuracy increases as gradient explosion worsens in random ReLU resnets. Open problems include batchnorm, convolution layers, and recurrent layers. More work is needed to justify the \"physical\" assumptions Axiom 1 and Axiom 2. Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli discuss resurrecting the sigmoid in deep learning through dynamical isometry. The text discusses learning through dynamical isometry in deep neural networks, focusing on the theory and practice. Key concepts include exponential expressivity, mean field assumptions, and the roles of operators V and W. The work builds on previous research by Pennington, Schoenholz, and Ganguli on resurrecting the sigmoid function in deep learning. The text discusses deep mean field theory assumptions for precise predictions in neural networks. Axioms 1 and 2 outline gradient independence and weight sampling conditions for backpropagation. These assumptions lead to highly accurate predictions in deep learning. The text discusses the mean field theory of gradients for feedforward networks, introducing the concept of weight virtualization (WV) as a post-hoc technique. It addresses issues with exploding norms in ReLU resnets and proposes a simple fix without variance decay. The text discusses the gradient mollification in deep networks, showing that increasing \u03b2 \u2022 can fix the problem without architectural changes. The gradient norm varies based on V r, with empirical verification and accurate predictions. Different trainable parameters are affected differently by V r. The text discusses how different parameters, such as \u03c7 w and \u03c7 v, are affected by V r in deep networks. \u03c7 w transitions to poly(l) for V r \u2265 1, while \u03c7 v remains poly(l) for V r > 1. Greater gradient explosion in \u03c7 v predicts better performance in the V r > 1 regime. V r mollifies the behavior of squared activation norms p and q, with p transitioning to polynomial dynamics for V r \u2265 1. The dynamics of p in deep networks are governed by a zigzag through parameter space. As U r increases past 1, p becomes constant. The cosine distance e measures input space geometry preservation, with simple dynamics compared to p and \u03c7. If e (0) < 1, e approaches 1 regardless of initial data. Displayform2 is bounded above when V r = 1.6. DISPLAYFORM2 is bounded above when Vr = 1.6, with dashed lines indicating asymptotics. For Vr > 1 and Ur > 1, e converges to a fixed point e* < 1 dependent on initial data. ReLU tends to collapse input space into a single point unless \u03b2\u2022s are high enough. The dynamics of tanh resnets are determined by key quantities U t and W t, with V t playing a minor role. In the setting where q \u2192 \u221e as l \u2192 \u221e, higher layers become essentially unused by the neural network. As long as U t stays below 1, the asymptotics of \u03c7 is governed by W t. The dynamics of tanh resnets are governed by key quantities U t and W t, with V t playing a minor role. As long as U t stays below 1, the behavior of \u03c7 is determined by W t. The results of Yang and Schoenholz (2017) are recovered by setting all \u03b2 \u2022 s to 0, resulting in a polynomial DISPLAYFORM1. Gradient expansion is bounded when W t dips below 0. Length quantities have simpler asymptotics determined by U t: either U t < 1 and p = \u0398(l 1\u2212Ut), or U t = 1 and p = \u0398(log l). Cosine distance can be controlled effectively by \u03b2 a and \u03b2 v, with higher layers affected differently based on their values. The dynamics of tanh resnets are influenced by U t and W t, with V t having a minor role. The behavior of higher layers is determined by the values of \u03b2 a and \u03b2 v, affecting the stability or chaos phase. The fixed point e * can be controlled explicitly by \u03c3 v and \u03c3 a when \u03b2 a = \u03b2 v, with e * determined by a specific equation. Integral transforms V and W are defined, and asymptotic notations are sign-less unless specified. The dynamics of tanh resnets are influenced by U and W, with V having a minor role. Higher layer behavior is determined by \u03b2 a and \u03b2 v, affecting stability or chaos phase. Integral transforms V and W are defined, with recurrences for mean field quantities p, q, \u03b3, \u03bb, \u03c7 provided. Backward dynamics are derived for non-constant network width. Theorem C.2 states properties of nonlinearity in FRN, while Theorem C.3 discusses fixed gradient vectors in random residual networks. The text discusses the derivative computations in a FRN with nonlinearity \u03c6 and Gaussian variables. It shows the proof for the projection connection case and derives the asymptotics of mean field quantities. The independence assumptions Axiom 1 and Axiom 2 are used in the computations. The text derives the asymptotics of mean field quantities for tanh resnet by bounding dynamics with known difference equations. It shows that if Ut < 1, then 1 > \u03b2w + Ut, and if Ut = 1, then \u03b2w = 0. The proof involves V\u03c6(q) = 1 - 2\u03c0q^(-1/2) + \u0398(q^(-3/2)), leading to q = \u0398(log l) as q approaches infinity. The text discusses the asymptotics of mean field quantities for tanh resnet, showing that if Ut < 1, then 1 > \u03b2w + Ut, and if Ut = 1, then \u03b2w = 0. The proof involves V\u03c6(q) = 1 - 2\u03c0q^(-1/2) + \u0398(q^(-3/2)), leading to q = \u0398(log l) as q approaches infinity. Theorem C.7 states that if e(0) < 1, then e(l) converges to a fixed point e*. The text discusses the asymptotics of mean field quantities for tanh resnet, showing that the LHS converges to 1 if \u03b2v > \u03b2a, leading to e* = 1. As l approaches infinity, e(l) approaches 0 for e(0) < 1. The asymptotics of ReLU resnet depend on specific values outlined in Definition C.8. The asymptotics of ReLU resnet are determined by specific values outlined in Definition C.8. The theorem states that if certain conditions are met, p and q have specific asymptotic behaviors. If Vr > 1 and Ur > 1, e(l) converges to a fixed point e* < 1. The proof shows that for specific values of p and q, the sequence e(l) converges to a limit e*. Depending on the relationship between p and certain constants, different scenarios lead to e* = 1. The proof demonstrates that the sequence e(l) converges to a limit e* under certain conditions. Specifically, when e* = 1, it implies that J1(e*) = e*. This relationship is further explored through the monotonicity of J1, leading to the conclusion that e* = 1. The convergence of p and \u03b3 to fixed points p* and \u03b3* is also discussed, with e* being defined as \u03b3*/p*. The proof in the current section discusses the convergence of sequences e(l) to a limit e* under certain conditions. It also explores the convergence of p and \u03b3 to fixed points p* and \u03b3*, where e* is defined as \u03b3*/p*. Theorem C.11 is presented, assuming \u03c6 = ReLU, with various conditions and proofs outlined. In this section, various lemmas are presented, including Lemma D.1 which discusses asymptotic behavior, and Lemma D.7 which deals with a recurrence relation. Lemma D.8 explores another recurrence relation with different cases based on the value of beta. Lemma D.8 explores a recurrence relation with different cases based on the value of beta, where (l) = \u0398(l \u03b4 log l) if 1 \u2212 \u03b4 = \u03b1. Furthermore, for \u03b2 = \u2212\u03b4 = 1: (l) \u223c l \u22121 if \u03b1 > 2, (l) \u223c l 1\u2212\u03b1 if \u03b1 < 2, and (l) \u223c l \u03b4 log l if \u03b1 = 2."
}