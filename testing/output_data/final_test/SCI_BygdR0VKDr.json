{
    "title": "BygdR0VKDr",
    "content": "The transformer model has been successful in various NLP tasks, showcasing the effectiveness of stacked attention over recurrence. A new discrete transformer architecture is proposed in this work, utilizing hard attention and a separate \"syntactic\" controller to improve internal model decision separation. This approach maintains performance levels on multiple datasets while enhancing reasoning by sparsifying the model further through direct regularization. The transformer model achieves state-of-the-art performance in sequence modeling tasks by using attention mechanisms for parallel training and reducing dependencies. Compared to recurrent models, the attention mechanism adds interpretability to the model's decisions. In this work, a variant of the transformer architecture is proposed to force sharper, discrete internal decisions by treating attention as a categorical latent variable and using a hard attention mechanism for discrete attention decisions. The proposed model utilizes a hard attention mechanism for discrete attention decisions and separates the querying mechanism from value computation. Training is similar to standard transformer training, but the key benefits come at inference time, where a simple decoding procedure and limited receptive field usage improve performance. The paper introduces a new approach using attention and sparsity regularization to improve the structure of feed-forward networks. Experiments validate the effectiveness of the approach on language modeling and machine translation tasks. The paper also discusses the importance of attention for transparency in model predictions. The importance of attention for transparency in model predictions has been highlighted in various domains, including healthcare and natural language sequence modeling tasks. While larger attention weights are often assumed to indicate higher importance, recent research shows that attention magnitude may not always correlate with input element importance. To address this, researchers have proposed using sparse attentions, such as sparsemax, to induce sparse attention structures. Our work builds on previous research by Lei et al. (2016) on attention models, but we focus on multi-head multi-layer attentions in transformers. Instead of using the REINFORCE algorithm, we employ the Gumbel-Softmax trick to reduce gradient variance. There are two directions for improving interpretability: model interpretability and prediction interpretability. Prediction interpretability involves using an external interpreter that is interpretable and consistent with the black box model. Our approach aims to enhance model interpretability by modifying the attention mechanism and objective function, assuming that more sparsity in the attention structure implies greater interpretability. While not directly leading to prediction interpretability, connections can be drawn to the prediction interpretability framework of Alvarez-Melis & Jaakkola (2017) through local permutations of input embeddings. The local interpreter in our approach does not need to use input embeddings that are not attended to at a specific prediction step, as there is no causal relationship with the prediction. The separation between query mechanism and value computation is similar to the two-stream attention mechanism in XLNet. Russin et al. used word embeddings as content vectors with attention computed based on LSTM outputs, aiming to separate syntax and semantics like our approach. Transformer's multiple layers and multi-headed attentions present unique challenges due to lack of recurrence. The transformer architecture for a simplified classification task involves encoding input tokens with position-specific embeddings, producing new vectors in each layer using a feed-forward NN, and utilizing key, value, and query matrices for attention distribution. The lack of recurrence in transformers poses challenges due to multiple layers and multi-headed attentions. The transformer architecture utilizes attention to aggregate information from other tokens, serving as the main source of inter-word information routing. The attention layer plays a crucial role in determining the receptive field of the transformer model. The attention layer in the transformer model has been extensively studied for its role in inter-word information routing. However, current research has shown limitations in the ability to truly separate out model decisions due to the soft pooling of elements into a vector. Recent works have explored alternatives to soft attention to improve model interpretability. The focus of recent work is on replacing soft-attention in transformer models with latent control variables for explicit decision-making. This approach makes the transformer stochastic, requiring computation of p(y|x) = z p(y, z|x) without observing z. The complexity of summing over all possible choices for z in stacked attention is combinatorial, leading to proposed alternative methods in the literature. The Gumbel-Softmax approach is used for training transformer models, providing a continuous approximation to sampling from a categorical distribution. It involves sampling from a Uniform distribution and applying Gumbel noise with a temperature parameter. This method allows for differentiable samples during training. The Gumbel-Softmax approach provides differentiable samples for training transformer models by approximating sampling from categorical attention distributions. A fixed temperature is used during training, with tuning on the validation set. At test time, Gumbel-Softmax is replaced with argmax for a greedy choice. This method ensures a fixed tree of previous words influencing each position in the model. The discrete transformer architecture utilizes separate syntactic and semantic streams. It employs discrete attention to make hard choices for model routing, ensuring decisions are based on the original words' pathways. The structure of hard attention defines the receptive field based on the hidden state at the top layer, with a growing exponential receptive field and branching factor. The discrete transformer architecture separates syntactic and semantic streams, using hard attention for model routing decisions based on original words' pathways. A proposed extension aims to separate \"syntactic\" routing control from \"semantic\" computation, incorporating a fixed sparse feed forward network for the semantic part and allowing the syntactic part to consider the entire sentence at any step. This two-stream transformer network maintains a continuous receptive field by keeping key and value calculations soft. The model separates syntactic and semantic streams in a discrete transformer architecture. It uses hard attention for routing decisions based on original words' pathways. The syntactic representation is computed separately from the semantic representation. The two streams are updated independently, with the syntactic stream being completely separate from the semantic stream. The model allows for imposing structural constraints on the latent variable. The model allows for imposing structural constraints on the latent variable by penalizing the size of the receptive field at the final layer. The final layer receptive field size is calculated based on dependencies between tokens at different layers. The final layer receptive field size is calculated based on dependencies between tokens at different layers. Experiments were conducted on various benchmark datasets including machine translation and language modeling to test the approach's ability to recover true underlying dependencies with a sparsity regularizer. Our approach aims to discover true dependencies in data by constructing a synthetic language modeling task with known underlying dependencies. The architecture and hyperparameters are based on standard transformer models, using encoder and decoder stacks with two stream attention for self-attention. The encoder-decoder context attention utilizes keys and values from syntactic and semantic representations. The encoder provides keys and values, the decoder provides queries. Different hyperparameters are used for WMT and IWSLT datasets. Models are implemented using Fairseq. Gumbel temperature is set to 1, sparsity regularizer strength to 0.1. Only the decoder network is used for language modeling. Baselines are established for comparison. For the synthetic stack language modeling task, different transformer models with discrete attention are compared. Experiments separate syntactic and semantic streams using a synthetic stack language dataset. The vocabulary includes {0, 1, 2, 3, 4, (, )} with nesting limited to depth 4. The model aims to learn sparse attentions that correspond to true underlying dependencies. The study compares transformer models with discrete attention for a synthetic stack language modeling task. The models are trained on a dataset with nesting limited to depth 4, aiming to learn sparse attentions that correspond to true dependencies. The SINGLE STREAM DISCRETE TRANSFORMER model achieved a precision of 0.959 and recall of 0.920 compared to ground truth dependencies. The two-stream model separates syntactic routing control from semantic computation. Analysis shows embeddings cluster by part-of-speech for syntactic controller and by semantics for value network. The study explores utilizing different source encoders for a syntactic chunking task, where text is divided into syntactically correlated parts of words. The pretrained encoder is used to obtain vector representations of the source sentence, which are then projected to the space of chunk types. Results show that both models outperform a standard transformer in this task. The discrete transformer, a modification to the transformer, makes structured attention decisions and separates out dependencies from semantic state value. Experiments show that the model maintains similar performance on machine translation benchmarks while separating out syntactic properties and learning precise decisions on clean data. This model opens up possibilities for experiments in NLP by making hard intermediary decisions that show the semantic model depends only on a subset of the data, potentially allowing for bias detection and removal. The discrete transformer separates out dependencies from semantic state value, allowing for bias detection and removal. This method can enforce specific syntactic structure and train pretrained models with discrete intermediary structure."
}