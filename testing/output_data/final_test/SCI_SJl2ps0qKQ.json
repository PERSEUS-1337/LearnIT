{
    "title": "SJl2ps0qKQ",
    "content": "Our model for knowledge-based question answering aims to relax the assumption of answerable questions from simple to compound questions. It consists of a learning-to-decompose agent and three simple-question answerers. The model learns complex rules of compositionality and achieves state-of-the-art results on WebQuestions and MetaQA. Knowledge-Based Question Answering (KBQA) involves analyzing interpretable decomposition processes and generated partitions to bridge a curated knowledge base with answerable questions. Many systems have made remarkable improvements in answering various datasets, but most assume only simple questions are answerable. Compound questions, which have multiple relations, are also important in KBQA. In Knowledge-Based Question Answering (KBQA), compound questions with multiple relations are important. The paper aims to relax the assumption of answerable questions from simple to compound ones. It proposes a learning-to-decompose agent to assist in answering compound questions by breaking them down into simpler ones. The paper introduces a learning-to-decompose agent for answering compound questions in KBQA, aiming to relax the assumption of answerable questions from simple to compound ones. It argues that compound questions are more suitable for KBQA context than multi-hop questions. The agent produces partitions and computes the compositional structure of questions based on feedback from downstream simple-question answerers. Our model discovered composite rules for answering compound questions in KBQA, leading to better generalizations and state-of-the-art performance on challenging datasets without complex neural network redesign. Many approaches in KBQA focus on parsing natural language questions to structured queries, either through topic entity linking and relation detection or by representing questions with fix-length vectors using CNNs or RNNs. Some approaches use pre-defined rules or grammars for query construction, while others emphasize representing natural language questions over constructing knowledge graph queries. Our proposed model aims to decompose compound questions into simpler ones, reducing the burden of learning vector representations. This process allows for the direct decoding of simple questions into an inference chain of relations, addressing the bottleneck of KBQA. Reinforcement learning approaches focus on learning sentence representations either through tree structures or sequentially selecting useful words. The vector representation is built from words in a tree structure or sequence, benefiting natural language processing tasks like text classification and natural language inference. Our model decomposes compound questions into simple ones, addressing the KBQA bottleneck. Different communities are interested in natural question understanding, with tasks like SequentialQA and ComplexWebQuestion datasets focusing on decomposing questions. Our work focuses on decomposing compound questions into simple ones for knowledge-based question answering. Unlike previous approaches that rely on explicit supervisions, we allow the learning-to-decompose agent to discover partition strategies without heavy feature engineering. Semantic role labeling based on deep neural networks captures dependencies between predicates and arguments. The end-to-end system takes only original text information as input. Our approach focuses on improving role classifiers by incorporating vector representations and self-attention mechanisms. Unlike traditional methods, our system trains at the sentence level without word-by-word annotations, generating annotations on the fly through exploration. Our model consists of a learning-to-decompose agent that divides input questions into three partitions, each containing necessary information for simple-question answerers. The agent includes a Memory Unit and an Action Unit, using a feedforward neural network as policy. The agent utilizes a feedforward neural network as a policy network for the Action Unit, with a Long Short-Term Memory (LSTM) cell for input history memorization. The agent's state is defined by a memory cell unrolling for each time step, and it also has a stochastic policy network for decision-making. The agent uses a two-layer feedforward network to process its state and generate a sequence of actions to decompose questions. The episodic reward is based on whether the agent helps answerers get the correct answers. Another reward function based on log probability is also used in reinforcement learning for natural language processing tasks. The agent uses reinforcement learning for natural language processing tasks, focusing on question decomposition. Unique rollouts correspond to different question decompositions, allowing the agent to explore partition strategies. The goal is to learn strategies that benefit answerers. Simple-question answerers can classify partitions as relations in a knowledge graph. LSTM networks are used to construct simple-question representations for each partition. The agent uses reinforcement learning for question decomposition in natural language processing tasks. Simple-question answerers classify partitions as relations in a knowledge graph using LSTM networks to construct representations. A two-layer feedforward neural network is used for prediction, with each answerer processing its partition independently. Parameters are not shared to avoid data conflicts and unstable training. The approach is simpler than existing baselines for simple question answering. The agent and answerers in the study use a simpler approach for question answering over knowledge graphs compared to modern architectures. The agent decomposes compound questions into simpler versions for answerers to classify. The answerers update word embeddings during training, and three answerers are trained independently using Cross Entropy loss. No pre-training is used due to consistent convergence observed in experiments. The study uses a Monte-Carlo Policy Gradient method to decompose compound questions into simpler ones for answerers to evaluate. A baseline is subtracted to reduce variance, and a linear regressor is used to estimate future rewards. The agent learns an optimal policy for question decomposition and receives episodic rewards in return. The study uses a Monte-Carlo Policy Gradient method to decompose compound questions into simpler ones for answerers to evaluate. The agent takes the final episodic reward in return. The experiments aim to evaluate the model's ability to discover useful question partitions and composition orders that benefit answerers. The model is trained on arithmetic operators and evaluated on the MetaQA dataset. The agent's compound question decomposition ability is validated through training on complex algebraic expressions. The study uses a Monte-Carlo Policy Gradient method to decompose compound questions into simpler ones for answerers to evaluate. Specifically, the complex algebraic expression is a sequence of arithmetic operators including +, \u2212, \u00d7, \u00f7, ( and ). The task aims to test the learning-to-decompose agent's ability to assign a feasible order of arithmetic operations, with a focus on operations surrounded by parentheses. The study uses a Monte-Carlo Policy Gradient method to decompose compound questions into simpler ones for answerers to evaluate. The agent learns an arithmetic skill where multiplication and division have higher priority than addition and subtraction. The experiment results show that the agent can generalize from short to long expressions. The study's agent learns to prioritize arithmetic operations, with multiplication and division taking precedence over addition and subtraction. Increasing the distance between parentheses can harm performance due to the Long Short-Term Memory Unit struggling to carry information over long distances. The proposed model is evaluated on challenging KBQA datasets, showing that MetaQA has twice as many compound questions as simple ones. The vocabulary size is 39,568, with a knowledge graph containing 43,234 entities and 9 relations. The study's agent prioritizes arithmetic operations, with multiplication and division taking precedence. The model is evaluated on KBQA datasets, showing MetaQA has twice as many compound questions as simple ones. The vocabulary size is 39,568, with a knowledge graph containing 43,234 entities and 9 relations. WebQuestions dataset has 2,834 training questions, 944 validation questions, and 2,032 testing questions. 602 relations are used for relation classification. Compound questions in WebQuestions are decomposed into two partitions. The topic entity of each question is linked to the knowledge graph using character trigrams matching. The study evaluates relation detection performance using 100-dimensional word embeddings and a memory unit with hidden state and cell state dimensions of 128. Different optimizers are used for training the agent, with a fixed learning rate of 0.0001. The model utilizes four samples for Monte-Carlo Policy Gradient estimator of REINFORCE. Results on MetaQA dataset show total accuracy as the most representative metric for model performance. The study evaluates relation detection performance using word embeddings and a memory unit. The model focuses on leveraging Freebase relation names and decomposing compound questions into simple ones. An ablation study is conducted on the assumption of answerable questions, with a comparison between questions containing two and three relations. The MetaQA dataset results show total accuracy as the key metric for model performance. The evaluation aims to demonstrate the model's improvement on 1-hop and 2-hop questions by sacrificing the ability to answer three-hop questions. Results from the ablation test show a trade-off between complexity of questions and performance. The learning-to-decompose agent generates different question structures, with rewards for correct classifications. The model optimizes strategies to decompose questions for downstream answerers, understanding compound questions by decomposing the meaning into parts. This is supported by evidence showing the model's ability to correctly classify relations and decompose complex questions effectively. The model optimizes strategies to decompose compound questions efficiently by generating partitions based on question semantics. This approach aims to maximize information utilization and improve downstream relation classification. The model optimizes strategies to decompose compound questions efficiently by generating partitions based on question semantics. This approach aims to maximize information utilization and improve downstream relation classification. The learning-to-decompose agent can also be used for other question answering tasks that require understanding compound questions. The paper proposes a novel approach that leverages question semantics efficiently, focusing on the Principle of Semantic Compositionality. Relaxing the answerable question assumption is necessary to generalize question answering."
}