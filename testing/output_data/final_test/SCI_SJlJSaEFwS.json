{
    "title": "SJlJSaEFwS",
    "content": "Recent advances in cross-lingual word embeddings have primarily focused on mapping-based methods, which project pretrained word embeddings from different languages into a shared space. However, these approaches assume isomorphism between word embedding spaces in different languages, which has been proven not to hold in practice. To address this limitation, joint learning methods are being explored to simultaneously learn embeddings across languages. By leveraging bilingual extensions of the CBOW method and sentence-aligned corpora, robust cross-lingual word and sentence representations can be obtained. This approach has shown significant improvements in cross-lingual sentence retrieval performance compared to other methods. Our bilingual method significantly improves cross-lingual sentence retrieval performance compared to other approaches. It also achieves parity with deep RNN methods on zero-shot cross-lingual document classification tasks, requiring fewer computational resources. Additionally, it enhances the quality of monolingual word vectors despite training on smaller datasets. Our code and models are publicly available. Cross-lingual representations, like embeddings of words and phrases, are crucial in multilingual natural language processing. They enable a joint understanding of concepts across languages and facilitate knowledge transfer between different languages. These embeddings are beneficial for tasks such as bilingual lexicon induction, cross-lingual information retrieval, machine translation, and zero-shot transfer learning. Existing methods can be categorized into mapping methods and postprocess steps to map embeddings into a shared space. Recent work on cross-lingual representations has focused on mapping methods and joint methods for learning shared embeddings across languages. However, linguistic and domain divergences, especially for distant language pairs, hinder the performance of these methods. Cross-lingual hubness and structural non-isometry between embeddings are identified as key challenges in achieving effective cross-lingual representations. The BI-SENT2VEC algorithm proposed by Ormazabal et al. (2019) aims to address the challenges of cross-lingual representations by using joint learning to predict words or context in both source and target languages. This approach results in embeddings that are closer to isomorphic, less sensitive to hubness, and perform better on cross-lingual benchmarks compared to mapping-based methods. The algorithm extends the SENT2VEC algorithm and revisits the TRANS-GRAM method to assess the effectiveness of joint learning. BI-SENT2VEC outperforms competing methods on cross-lingual sentence-retrieval and monolingual word representation quality evaluations, showing robustness on dis-similar language pairs. It performs similarly to LASER on a zero-shot crosslingual transfer task but improves computational efficiency significantly, making it suitable for resource-constrained cross-lingual NLP applications. Joint learning methods consistently dominate state-of-the-art. The study demonstrates that joint learning methods outperform state-of-the-art mapping methods in cross-lingual word and sentence retrieval. Training on parallel data enhances monolingual representation quality, as shown by BI-SENT2VEC's superior performance over FASTTEXT embeddings. The research contributes to the extensive literature on cross-lingual representation learning, focusing on unsupervised and supervised mapping algorithms using existing monolingual word embeddings. The authors propose supervised learning of a linear map from a source language embedding space to a target language embedding space based on a bilingual dictionary. They also enforce orthogonality constraints on the linear map to mitigate hubness in translated embeddings. Additionally, they align embedding spaces starting from a parallel seed lexicon and use an adversarial training method to learn a linear orthogonal map. (Conneau et al., 2017) propose an adversarial training method to learn a linear orthogonal map without bilingual supervision. They refine the mapping using the Procrustes procedure iteratively with a synthetic dictionary. The 'Cross-Domain Similarity Local Scaling' (CSLS) retrieval criterion improves word translation accuracy. This work is referred to as Multilingual Unsupervised and Supervised Embeddings (MUSE). (Chen & Cardie, 2018) use \"multilingual adversarial training\" and \"pseudo-supervised refinement\" to obtain unsupervised multilingual word embeddings (UMWE). Hoshen & Wolf (2018) propose an unsupervised approach for aligning multilingual word embeddings by optimizing a convex relaxation of the CSLS function. While existing methods assume structural isomorphism between embeddings, S\u00f8gaard et al. (2018) argue that this assumption is not always realistic. Many geometric configurations of points cannot be linearly mapped to their targets, leading to the development of joint learning algorithms like TRANSGRAM and Cr5. TRANSGRAM extends Skipgram to train bilingual embeddings in the same space, while Cr5 uses document-aligned corpora for cross-lingual document retrieval. TRANSGRAM embeddings have not been widely discussed in recent work. The growing abundance of sentence-aligned parallel data merits a reappraisal of word retrieval methods. Ormazabal et al. use BIVEC, a bilingual extension of Skipgram, and compare it with VECMAP. Their experiments show that the extra supervision in BIVEC is redundant for state-of-the-art performance. The BI-SENT2VEC model is a cross-lingual extension of SENT2VEC, optimized for obtaining word embeddings from sentence contexts. SENT2VEC is a model that obtains sentence representations by averaging word-ngram embeddings. The training objective involves predicting a masked word token in the sentence using logistic loss and negative sampling. The model uses source n-gram and target word embedding matrices, with negative words sampled from a multinomial distribution. The BI-SENT2VEC model adapts the SENT2VEC model to bilingual corpora by introducing a cross-lingual loss in addition to the monolingual loss. This ensures that word and n-gram embeddings of both languages lie in the same space, as illustrated in Figure 1. The BI-SENT2VEC model is trained on bilingual corpora using a cross-lingual loss in addition to the monolingual loss. Model parameters are updated using asynchronous SGD with a decaying learning rate. Training data includes ParaCrawl datasets for various language pairs and additional corpora for the English-Russian pair. The BI-SENT2VEC model is trained on bilingual corpora using a cross-lingual loss. The number of parallel sentence pairs in the corpora ranges from 17-32 Million, with 2 million pairs for English-Hungarian and English-Finnish. Different models were trained with unigram and bigram embeddings. New TRANSGRAM embeddings were trained on the same data used for BI-SENT2VEC for a fair comparison. The BI-SENT2VEC model is trained on bilingual corpora using a cross-lingual loss. Parameters for training TRANSGRAM and BI-SENT2VEC models were adjusted, with TRANSGRAM trained for 8 epochs and BI-SENT2VEC for 5 epochs. A hyperparameter search did not improve results. VECMAP and BIVEC methods were also trained on the same corpora. Quality of word and sentence embeddings were compared using four benchmarks. In comparing alignment quality, the study evaluates results using four benchmarks: cross-lingual word retrieval, monolingual word representation quality, cross-lingual sentence retrieval, and zero-shot cross-lingual transfer of document classifiers. The analysis includes studying the impact of training data on representation quality and corpus size. The evaluation utilizes the MUSE library for most evaluations, except for zero-shot classifier transfer tested on the MLDoc task. Translation accuracy is assessed using bilingual dictionaries, with P@1 scores reported for 1500 source-test queries and 200k target words for each language pair. The study evaluates alignment quality using benchmarks such as cross-lingual word retrieval, monolingual word representation quality, and cross-lingual sentence retrieval. It reports P@1 scores for 1500 source-test queries and 200k target words for each language pair in Table 1, assessing the monolingual quality improvement of cross-lingual training. Performance on monolingual word similarity tasks is also evaluated. The study evaluates alignment quality using benchmarks such as cross-lingual word retrieval and monolingual word representation quality. Performance on monolingual word-similarity tasks is assessed using various datasets and Pearson scores to measure correlation between human-annotated and predicted similarities. FASTTEXT monolingual vectors trained on CommonCrawl data are included, showing high correlation scores for different language pairs. Additional evaluation results for other language pairs can be found in the appendix. The primary contribution of the study is to deliver improved cross-lingual sentence representations. Sentence embeddings are tested for sentence retrieval across different languages using the Europarl corpus. Results for unsupervised and supervised benchmarks compared to the models are shown in Table 3. The study presents improved cross-lingual sentence representations, showing significant performance enhancements in cross-lingual word and sentence retrieval tasks for English-Finnish and English-Hungarian language pairs. Results for different language pairs are included in Table 4, demonstrating the effectiveness of the models in a zero-shot setting. The study compares the performance of BI-SENT2VEC with LASER sentence embeddings in cross-lingual document classification. LASER model is a multi-lingual sentence embedding model trained on 223M sentences for 93 languages. The classifier used is a feed-forward neural network with two hidden layers. The study compares BI-SENT2VEC embeddings' performance on different training corpus sizes. Results are shown in Figures 2 and 3, with further analysis in Tables 1-5. Improved crosslingual sentence retrieval is highlighted, along with word translation. BI-SENT2VEC outperforms other methods in cross-lingual word and sentence retrieval tasks, showing superior performance in word-retrieval tasks and significantly better results in sentence retrieval compared to competing methods. The absence of the hubness phenomenon in BI-SENT2VEC for word retrieval tasks is noted, with consistent performance across different criteria. Table 3 demonstrates that BI-SENT2VEC outperforms other methods in cross-lingual evaluations, reducing P@1 error significantly. The model shows less variance in quality across language pairs and excels in sentence retrieval tasks due to its optimized learning objective. English-Finnish and English-Hungarian pairs stand out in the evaluation for their unique language composition. In cross-lingual evaluations, BI-SENT2VEC shows significant performance improvement on dis-similar language pairs compared to similar ones, indicating its suitability for learning joint representations across different language families. It also outperforms existing methods on monolingual word similarity tasks, showing large gains on benchmarks like SimLex-999 and WS-353. This performance advantage is observed even when compared to models trained on larger corpora like CommonCrawl. BI-SENT2VEC outperforms TRANSGRAM and BIVEC on monolingual word quality benchmarks like SimLex-999 and WS-353, indicating the effectiveness of bilingual contexts for learning word representations. Incorporating n-grams in training improves results for word vectors, with bigrams significantly enhancing performance over unigrams alone. This trend holds true for bilingual evaluations as well. In bilingual evaluations, using n-grams can degrade cross-lingual performance for dissimilar language pairs. Increasing corpus size for English-French datasets up to 1-3.1M lines saturates BI-SENT2VEC's performance on cross-lingual word/sentence retrieval. Joint methods show promising results with less data, suggesting they are not limited to high-resource language pairs. Further experimentation is needed to confirm this claim. The monolingual quality improves with corpus size increase. To enhance cross-lingual performance, adjusting weights for monolingual and cross-lingual components of the loss is suggested. In the MLDoc classifier transfer task, BI-SENT2VEC achieves similar performance to LASER for English language pairs. BI-SENT2VEC is a multilingual bag-of-words method that encodes sentences by averaging vectors, making it computationally efficient for cross-lingual NLP. It outperforms other methods on cross-lingual sentence retrieval while matching performance on word translation tasks. Our method achieves parity with LASER on zero-shot document classification and demonstrates significant improvement in monolingual word representation quality through training on parallel data. The success of our model on the bilingual level calls for extension to the multilingual level, especially for language pairs with limited parallel corpora. We aim to explore training cross-lingual embeddings using a large amount of raw text combined with a smaller amount of parallel data. The MUSE and RCSLS vectors were trained from FASTTEXT vectors obtained from Wikipedia dumps, except for the En-Ru pair which used OpenSubtitles and Tanzil corpora combined."
}