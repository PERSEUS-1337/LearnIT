{
    "title": "ryxSrhC9KX",
    "content": "Sparse, non-negative representations of objects were estimated using human behavioral judgments on images of 1,854 object categories. These representations predicted a latent similarity structure between objects, capturing most of the explainable variance in human behavioral judgments. The dimensions in the low-dimensional embedding were reproducible and interpretable, conveying taxonomic membership, functionality, and perceptual attributes. The embeddings were also found to predict other forms of human behavior, such as categorization and feature ratings, indicating that they reflect human conceptual representations of objects beyond specific tasks. This research aims to understand how object concepts are represented and their relationship to human behavior. The research aims to understand how object concepts are represented and their relationship to human behavior. To comprehend how we interact with objects, three problems need to be addressed simultaneously: determining the object concept, identifying behaviorally-relevant dimensions, and understanding the cognitive representation of objects. The research aims to understand how object concepts are represented and their relationship to human behavior. Object concepts are distinguished by determining integrated representations in perceptual decisions and analyzing the influence of context on those decisions. Various attempts have been made to represent object concepts using semantic features, with binary properties being commonly used. A landmark study departed from this approach by asking subjects to name binary properties of objects, resulting in 2,526 semantic features. The study involved naming binary properties of objects to generate semantic features, revealing shared and distinguishing features among concepts. Subsequent research expanded on this, highlighting issues with the lack of degree in features and the omission of common properties. Absence of specific context may limit subjects' ability to think of valid properties for objects. The study introduced a new approach to estimate representations of object concepts using behavioral judgments about grouping object images with other objects, avoiding the need for experts to specify features in advance. The study presents a new approach to estimate object concepts by predicting human behavior with new object combinations. It also allows predictions in other behavioral tasks and suggests a simple model for judgments of semantic similarity. The Odd-one-out behavioral dataset contains information on 1,854 object concepts. A triplet of three concepts is presented to Amazon Mechanical Turk workers as photographs without labels. Workers must identify the odd-one-out by determining the pair with the greatest dissimilarity. The dataset contains a large number of objects from various semantic categories. The study involves presenting triplets of objects from different semantic categories to participants. They are asked to identify the odd-one-out by considering the objects in various contexts. This helps to understand how objects can be related based on different situations. The study involves presenting triplets of objects from different semantic categories to participants to identify the odd-one-out. The dataset contains judgments on 1,450,119 triplets, with plans to collect more data and release it by the end of the study. The method used, Sparse Positive Similarity Embedding (SPoSE), is based on three assumptions. The Sparse Positive Similarity Embedding (SPoSE) is based on the assumption that each object concept can be represented by semantic features in a vector. These features correspond to dimensions where the relationships between vectors encode the similarity of concepts. The model estimates these features from behavioral data alone, with each feature being real, non-negative, and sparse. The Sparse Positive Similarity Embedding (SPoSE) model represents object concepts with semantic features in vectors, where relationships encode concept similarity. The model estimates features from behavioral data, with each feature being real, non-negative, and sparse. In sparse positive word embeddings, dimensions indicate semantic category membership, influencing subject performance in grouping objects. The model aims to explain semantic decision-making behavior by considering similarity between embedding vectors of concepts presented. The third assumption in the SPoSE model is that the outcome of the triplet task is stochastic, reflecting trial-to-trial inconsistencies and subject differences in decision-making. The probability of a subject choosing a pair is proportional to the exponential of the similarity between vectors x i and x j. The vectors x i are not normalized, allowing for arbitrarily large dimensions and vector similarities. The text discusses modeling choice probabilities using embedding vectors and fitting parameters through a regularized maximum likelihood objective function. The dataset is split into training and validation sets for model building. The dataset is randomly split into training and validation sets. The objective function for SPoSE involves L1 norm and non-negativity constraints. The Adam algorithm is used for optimization with a fixed number of epochs. The regularization parameter \u03bb is selected from a grid of candidate values. The regularization parameter \u03bb is selected based on the lowest cross-entropy CE v on the validation set. The dimensionality of the embedding, p, can be determined heuristically from the data. SPoSE algorithm shrinks dimensions towards zero with an L1 penalty objective. Existing methods for estimating embeddings include GNMMDS framework introduced by BID1 for behavioral judgments. The behavioral task studied is a 'quadruplet' task where subjects compare pairs of items. GNMMDS learns embedding vectors for concepts to approximate dissimilarity. BID21 examines a 2AFC task with parametric features. BID17 looks at a triplet task comparing similarities. BID20 also explores embedding methods. Our study introduces a new method for embedding concepts with sparse, positive vectors and a three-choice probabilistic model. Unlike previous studies, we focus on interpretability and do not require a priori feature matrix. While GNMMDS could be applied with added constraints, it does not provide probability estimates. The model in BID17 could potentially be extended for our task. The model in BID17 could potentially be extended to the three-choice triplet task with sparsity and positivity constraints, requiring an extra tuning parameter \u00b5 in addition to the L1 sparsity penalty \u03bb. An alternative approach for representing concepts is the use of word vector embeddings derived from information about word co-occurrence in large text corpora. Pilehvar & Collier (2016) introduced a method for estimating vectors for different concept senses of a word from its word2vec embedding vector and synset relationships in WordNet ontology. However, these dense synset vectors do not meet the requirements for our study. The embedding method NNSE and BID12 produce word vectors instead of synsets, used for behavioral predictions. Tensorflow BID0 was used to fit the model to triplets, with \u03bb=0.008 chosen through validation. The 90-dimensional model converged to a 49-dimensional embedding with sparse dimensions. The 49-dimensional embedding used in experiments had varying non-zero dimensions across object categories. To ensure reproducibility, the model was run 11 times with different random seeds, resulting in dimensions ranging from 47 to 50. Comparisons were made with synset and NNSE vectors of different dimensionalities. After collecting an independent test set of 25,000 triplets with 25 repeats for each of 1,000 randomly selected triplets, 614 unique triplets with at least 20 repeats remained. This allowed for a model-free estimate of Bayes accuracy, with a ceiling estimated at 0.673. The model's accuracy was 0.637, above all baselines. An independent test set was collected for 48 objects, containing 43,200 triplets. The model was re-trained on a dataset excluding certain triplets for evaluation. After re-training the model on a dataset excluding specific triplets, we achieved an accuracy of 0.592, surpassing baseline methods. Pairwise similarities between objects were calculated for the 48-object dataset to examine prediction quality. The study calculated similarity matrices for a 48-object dataset and compared the predicted similarity matrix with the actual pairwise similarities, showing a high correlation of 0.899. The evaluation also included testing semantic vector representations for predicting human-generated semantic features. The study evaluated the prediction of human-generated semantic features using a logistic regression model with L2 regularization. The evaluation focused on the CSLB dataset, selecting 100 features with the highest density for prediction. Prediction was done using 10-fold cross-validation, with nested cross-validation to determine the regularization parameter. The study evaluated the prediction of human-generated semantic features using a logistic regression model with L2 regularization. The evaluation focused on the CSLB dataset, selecting 100 features with the highest density for prediction. Prediction was done using 10-fold cross-validation, with nested cross-validation to determine the regularization parameter. The results show that SPoSE dimensions contain information present in CSLB features for the objects considered, with higher AUC compared to synset vectors in some dimensions related to shapes and perceptual aspects. The analysis aimed to demonstrate that SPoSE dimensions can be explained in terms of elementary CSLB features by testing their predictive power. The study used a L1-regularized non-negative regression model to predict SPoSE features from CSLB features for 496 concepts. The median correlation between features and predictions was 0.58, indicating good predictability. A model was then fitted to determine which CSLB features influenced the prediction of each SPoSE dimension. The most influential CSLB features for four SPoSE features were displayed in FIG0. Dimension A indicates the object's belonging to the \"animal\" semantic category, while Dimensions B and C correspond to the type of material in objects. Dimension D captures purely visual aspects. The NNLS weights and CSLB features play a significant role in predicting these dimensions. Dimension D captures purely visual aspects and is strongly explained by the \"is red\" CSLB feature. Typicality ratings for objects within the same semantic category are assessed by measuring distances between semantic vectors. Various norms of typicality were collated and a collaborative filtering method was used to extract an aggregate norm for objects. After collating various typicality norms, a proxy typicality score was calculated for objects by computing the dot product between their vector and the category centroid vector. The correlation between these scores and typicality norms was then analyzed, with 5 categories showing statistically significant results (weapon, vehicle, clothing, vegetable, fruit). The median correlation across categories was 0.55. The study compared typicality scores of objects in different categories, with some categories showing significant results. Results indicate that SPoSE vectors for typical objects are more similar to each other than for atypical ones. Semantic category clustering was evaluated by assigning objects to categories based on nearest neighbor similarity. In this paper, a low-dimensional semantic representation of concrete concepts is shown to explain human behavioral judgments effectively. The 49-dimensional vector embedding allows for predicting subject behavior with new concept combinations and other human-annotated data. The representation is interpretable with easily identifiable concept loadings on each dimension, and the value of each dimension can be explained by elementary features from human subjects. The low-dimensional semantic representation of concrete concepts effectively explains human behavioral judgments. The dimensions represent different types of information, from taxonomic to functional or perceptual. The representations are estimated from behavioral data, suggesting a simple model of decision making. This is related to the distinction between dimensional and featural approaches in judging concept similarity. The use of sparsity and positivity in the SPoSE representation blends featural and dimensional approaches when making decisions about concept similarity based on shared semantic categories and features. If two concepts share a category and the other does not, they are likely grouped together based on shared features. If three concepts share a category, they also share most, if not all, of their non-zero features, making the decision dimensional rather than featural. The use of sparsity and positivity in the SPoSE representation blends featural and dimensional approaches when making decisions about concept similarity based on shared semantic categories and features. The results of grouping concepts together are determined by a few common features with higher values, reflecting their importance in decision making. Object representations capture necessary information for explaining subject behavior in tasks, with potential for further exploration by sampling additional information. One possible extension is to sample additional triplets to obtain more within-category distinctions. Another direction is to explore different types of similarity judgments, such as grouping objects based on specific attributes. Additionally, predicting synset vectors from SPoSE vectors and representing residuals as new concept features could complement SPoSE dimensions."
}