{
    "title": "BkeK-nRcFX",
    "content": "Designing neural architectures for high performance was once considered a complex task requiring expert tuning. Guidelines like avoiding exploding or vanishing gradients exist, but no pre-training metric reliably predicts post-training network performance. The nonlinearity coefficient (NLC) is introduced as a novel metric that predicts test error in neural networks. It is cost-effective, robust to various factors, and crucial for achieving optimal performance in fully-connected feedforward networks. The NLC can help in architecture search and design by identifying potential training issues early on. The presence of pathological exploding or vanishing gradients in deep neural networks is still a topic of debate. Different metrics, such as the length of the gradient vector, size of individual components, or eigenvalues of the Jacobian, can be used to determine this. Strategies for combating these issues vary depending on the metric used, such as manipulating the width of layers. The He initialization stabilizes gradient vector length for ReLU networks, while the Xavier initialization stabilizes the size of gradient vector components for tanh networks. The NLC is introduced as a measurement of nonlinearity in neural networks and is a predictor of test error. The NLC is a powerful predictor of test error in neural networks, combining the Frobenius norm of the Jacobian with input and output variability. It accurately predicts nonlinearity, activation function behavior, and network output susceptibility to perturbations. The neural network's output susceptibility to small random input perturbations is tied to the network's prediction framework, aiming to minimize classification accuracy error over the training set using softmax plus cross-entropy loss function. The Jacobian of the output with respect to the input is denoted as J(x), where x(i) and f(x, j) represent input and output vector components respectively. The Jacobian J at x defines a local linear approximation of neural network f around x. If f(x) + J(x)\u03b4 is far from the co-domain F, the approximation is inaccurate. This insight helps establish an approximate bound for the size of \u03b4. The size of \u03b4 can be bounded using the Frobenius norm property. For accurate local linear approximation at x, ||\u03b4|| 2 must be within the diameter of the linearly approximable region. The nonlinearity of f is judged based on the relationship between the domain D and the linearly approximable region. The nonlinearity of a network f at x is estimated using the nonlinearity coefficient (NLC), which is defined as the inverse of the relative diameter of the linearly approximable region in a random direction. This metric is based on the quadratic expectation of the distance of two random points from the data distribution, and it equals the NLC as defined in the text. The NLC, or nonlinearity coefficient, is a metric that represents the standard deviation of activation of input and output neurons under the data distribution. It can be re-written in terms of covariance matrices. For a linear network, the NLC equals 1 when the covariance matrix is a multiple of the identity or the matrix A is orthogonal. This value is expected to be close to 1 for large, random matrices A in randomly initialized neural networks. The NLC, or nonlinearity coefficient, represents the sensitivity of network output to small input changes normalized by input variability. It is cheap to compute and can be calculated stochastically. A large-scale study investigates its empirical properties using randomly sampled architectures with varying characteristics. The study investigated the NLC of randomly sampled architectures with different characteristics, including activation functions and skip connections. Three datasets were used: MNIST, CIFAR10, and waveform-noise, with consistent results across all datasets. The study evaluated various architectures using different datasets and training protocols. 250 architectures per dataset were sampled, totaling 750, with skip connections highlighted in blue. Each architecture was trained with SGD using 40 different learning rates, selecting the optimal one via validation sets. Results are detailed in figures 1, 2, 3, 4, 7, 8, 9, and 10. The study analyzed various neural architectures with skip connections using different datasets and training methods. Results are presented in figures 1, 2, 3, 4, 7, 8, 9, and 10, showing correlations and statistical significance. Nonlinearity was measured using the NLC, validated in FIG0 with linearly approximable regions. The study analyzed neural architectures with skip connections using various datasets and training methods. Results in figures 1, 2, 3, 4, 7, 8, 9, and 10 show correlations and statistical significance. Nonlinearity measured by NLC is validated in FIG0 with linearly approximable regions. The NLC predicts test error, with a narrow range of 1 to 3 leading to optimal results. Some architectures within this range still performed poorly. In the study, neural architectures with skip connections were analyzed using various datasets and training methods. Results in figures 1, 2, 3, 4, 7, 8, 9, and 10 show correlations and statistical significance. The NLC predicts test error, with a narrow range of 1 to 3 leading to optimal results. Despite having an NLC in or close to this range, some architectures performed poorly. The value of the NLC before training versus after training was plotted in FIG2, showing a decrease for most architectures. The after-training NLC significantly predicts test error, with a narrow range leading to optimal results. Despite this, most architectures achieve near-zero training error regardless of NLC. This contrasts with previous claims about network performance. The study by Schoenholz et al. (2017) and Xiao et al. (2018) suggested that networks sensitive to small input changes are hard to train. However, the researchers were able to train such architectures by conducting an extensive search for optimal learning rates and using 64-bit precision. They found that architectures with high sensitivity often require very small learning rates and parameter updates to train successfully. Despite high sensitivity, many architectures predicted to have high test error were still trainable. Additionally, a test was conducted to measure the impact of small random perturbations on test error, showing that high sensitivity does not always lead to poor generalization. The study found that high sensitivity architectures require small learning rates to train successfully. Networks with biased outputs had high test errors, even with small NLC. Initial NLC must be close to critical for optimal performance. In this study, it was found that a low initial bias is necessary for optimal test error. Skip connections in architectures lead to lower NLC values and test errors. The bias tends to decrease throughout training, with a very low bias being crucial after training. Activation functions and metrics used in the study are detailed in Table 1. In section B.3, we analyze architectures with and without skip connections. The connection between NLC and activation functions is explored. For a 1d activation function \u03c4, if input x follows a Gaussian distribution with zero mean and covariance matrix \u03c3I, and f applies \u03c4 to each input component, then we have DISPLAYFORM1. In a wide network with fully-connected linear operations, batch normalization, and component-wise activation function \u03c4, pre-activations of \u03c4 are approximately unit Gaussian distributed. Each layer contributes N LC \u03c4 (1) to the network's nonlinearity. Training confirms this expectation. The nonlinearity of a network is closely tied to the activation function used, with nonlinearity compounding exponentially. The NLC values for different activation functions in 2-layer and 49-layer networks show a close match, indicating the impact of activation functions on network behavior. Unstable activation functions like 'square' and 'odd square' diverge from the expected NLC values at high depths. The nonlinearity of activation functions impacts network behavior, with some functions causing inputs to grow or collapse with depth. N LC \u03c4 is a meaningful measure of nonlinearity, with a close correspondence in linear approximation error for ReLU, SELU, tanh, sigmoid, and Gaussian functions. The NLC of an architecture can be adjusted by changing the linear approximability of activation functions. The NLC metric can be calibrated by adjusting the linear approximability of activation functions. It is compared against other metrics for predicting test error in deep learning, showing that it is invariant to confounders that affect other metrics. However, it is confounded by simple multiplicative rescaling. When adjusting activation functions, the NLC metric can be calibrated to predict test error in deep learning. It is unaffected by certain confounders but is influenced by simple multiplicative rescaling. The GVCS can be controlled by scaling the input or output of the network, impacting its trajectory during training without affecting test error directly. This effect is observed even with extreme activation functions that eliminate meaningful input data structure. The NLC metric captures nonlinearity in input data, while GVCS remains stable. Nonlinearity increase is reflected in Q i (S x f (x, i)), confounding GVCS. Changing network width affects GVCS, with adjustments in input dimensions and weights. The NLC metric captures nonlinearity in input data, while GVCS remains stable. GVCS can be reduced by a transformation without affecting NLC. Gradient vector length and Lipschitz constant are also used as predictors for network performance. These metrics are susceptible to scaling but not input width changes. Preserving input correlation is crucial for trainability and low test error, but additive bias can be a confounding factor. In a study on deep networks, biases in input features do not impact learning dynamics due to batchnorm. Adding a constant to inputs can increase correlation without affecting outputs. Deeper networks show a positive correlation with test error on CIFAR10, possibly due to larger NLC. Depth alone is not a direct predictor of performance. The nonlinearity coefficient (NLC) is introduced as a metric closely tied to neural network nonlinearity and linear approximability in the input space. It is argued to be the best standalone metric for predicting test error in fully-connected feedforward networks due to its predictive value in the randomly initialized state, stability throughout training, robustness to network changes, ease of computation, and conceptual simplicity. The NLC is a key metric for predicting test error in fully-connected feedforward networks. It helps in neural architecture search by discarding sub-optimal architectures before training. Avoiding excessive output bias and using skip connections also impact performance. Overfitting in neural networks is linked to output sensitivity rather than depth or parameters. High output sensitivity affects generalization but not trainability. The NLC is crucial for predicting test error in feedforward networks and aids in neural architecture search. Achieving an ideal nonlinearity level is more important than avoiding exploding gradients. Architectures designed for stable gradients may still exhibit a divergent NLC at great depth. The NLC shows exponential behavior, which can be harmful for network performance. In future work, the study plans to investigate the ideal range of NLC values for different datasets and architectures, including convolutional and densely-connected networks. The connection of NLC to adversarial robustness, quantizability, sample complexity, training time, and training noise will also be explored. The NLC measurement was found to be too noisy to detect underfitting conclusively. The section aims to provide an intuitive graphical explanation of NLC, illustrated with an example function f. The local linear approximation of f at sample inputs x1 and x2 is shown, along with the tangent line of the curve. The NLC measurement is illustrated with a graphical explanation using a function f. The local linear approximations at sample inputs x1 and x2 are shown, along with their respective tangent lines. The shaded regions represent where the approximations are accurate within the co-domain F. The NLC highlights the importance of staying within these shaded areas for accuracy. The NLC is a generalization of the concept of shaded regions covering a domain to multiple dimensions. It measures the ratio of the diameter of the domain to the linearly approximable region. In this section, neural networks are illustrated at varying levels of nonlinearity using plain, fully-connected, He-initialized batchnorm-ReLU networks at different depths. The study involved generating three 100-dimensional Gaussian random inputs and associating points on a unit sphere with these inputs. Architectures with poor test or training errors were omitted based on set thresholds. Inputs were propagated through the network to obtain a 3-dimensional output on a unit sphere, which was color-coded and used to color points on the input sphere accordingly. The study involved associating points on a unit sphere with 100-dimensional Gaussian random inputs. The output sphere was color-coded based on the 3-dimensional output of the network, with RGB values corresponding to the output neuron values. \"Purer\" colors indicate more confident predictions in a 3-class classification scenario. The color and value of the output change rapidly as network depth and NLC increase, leading to smaller linearly approximable regions. In this section, the study expands on findings from a large-scale empirical study, focusing on the exhaustive search over starting learning rates for training with SGD. 750 architectures were trained with 40 different starting learning rates, forming a geometric sequence with a spacing factor of 3. Thresholds were set at 80% for CIFAR10 and 50% for waveform-noise. The smallest learning rate was chosen to ensure meaningful weight updates in 32 bit precision. The study conducted a large-scale empirical search over starting learning rates for training with SGD. A range of learning rates was provided to contain the ideal learning rate with high probability. A histogram was plotted to show the index of the training run with the lowest validation error for CIFAR10. The study found a wide range of training run indices chosen, confirming the effectiveness of the approach. The study conducted a large-scale empirical search over starting learning rates for training with SGD. The ideal learning rate for each architecture was found with high probability. 50 waveform-noise architectures were retrained without early stopping, aiming for the lowest training classification error. Results were plotted in FIG2, using 60 training runs. The smallest starting learning rate was chosen for meaningful weight updates in 64 bit precision. In FIG6, a wider range of training run indices was observed. Some architectures with high NLCs and poor generalization were trainable with very small learning rates in 64 bit precision. In practice, some architectures can be trained successfully with very small learning rates in 64 bit precision. The NLC in the randomly initialized state is plotted against the starting learning rate in FIG8, showing that good generalization falls within a smaller range. Large NLCs are associated with large gradients, which need to be down-scaled to keep weight updates bounded. FIG8 suggests that architectures can be trained with a learning rate as small as 5e-18. The NLC growth suggests learning rate decay as the square of NLC. High bias before training leads to high test error. Bias decreases during training, often reaching near-zero for better test error. This is necessary for network performance, especially in datasets like CIFAR10. Many architectures achieve success with small learning rates. In FIG9, architectures near the bottom show high bias but low NLC, indicating bias contributes to test error prediction. Bias values were computed on the training set. An \"improved\" version of SGD is being developed to train high-bias architectures. Architectures with skip connections in FIG2 have lower NLC and outperform those without skip connections. In a large-scale study, randomly sampled architectures were used, consisting of fully-connected linear operations with bias and activation functions. Some architectures included normalization operations and skip connections that bypass two layers. The difference in behavior between architectures with and without skip connections is evident in the results plotted in figure 10. The architecture selection process involves choosing depth and width parameters randomly. Depth is selected from odd numbers between 3 and 49 to avoid conflicts with skip connections. Width is determined automatically to have approximately 1 million trainable parameters. The multiplier for preserving signal scale is based on the ratio of outgoing to incoming dimensions. The weight matrix initialization process involves preserving the signal scale by setting biases and weight matrices with specific probabilities and scaling factors. Additionally, normalization is applied with a 50% probability. The weight matrix initialization process involves setting biases and weight matrices with specific probabilities and scaling factors. Normalization is applied with probabilities for batch normalization, layer normalization, and no normalization. Activation functions are selected with probabilities for ReLU, SeLU, Gaussian, tanh, even tanh, sigmoid, square, and odd square, with additional modifications applied. The weight matrix initialization process involves setting biases and weight matrices with specific probabilities and scaling factors. Normalization is applied with probabilities for batch normalization, layer normalization, and no normalization. Activation functions are selected with probabilities for various functions. Skip connections are determined with probabilities for different configurations. After setting biases and weight matrices, normalization and activation functions are applied with specific probabilities. Skip connections are determined with probabilities for different configurations. Connections in the network vary after linear or normalization operations to achieve a diverse range of NLCs. Post-processing assigns batch normalization or layer normalization with 50% probability to networks with square activation functions or skip connections without normalization to prevent exponential instability. Sampling 250 architectures for each dataset resulted in changes in aggregate frequencies for normalization methods. After setting biases and weight matrices, normalization and activation functions are applied with specific probabilities. Skip connections are determined with probabilities for different configurations. Sampling 250 architectures for each dataset resulted in changes in aggregate frequencies for normalization methods. We used softmax+cross-entropy as the loss function and measured the scale c of activations fed into the loss function to prevent confounding outcomes. The preference of softmax+cross-entropy for outputs of a certain size has confounded results in past studies. We balanced relevance and diversity in our architectures by avoiding pathological cases and using orthogonal initialization. We introduced variations like activation function dilation and skip connection strength to increase diversity. Our experiments included datasets like MNIST and CIFAR10. The study conducted experiments on three datasets: MNIST, CIFAR10, and a dataset from the UCI repository. MNIST and CIFAR10 are popular for evaluating deep neural networks, while the third dataset was chosen based on filters applied by Klambauer et al. (2017). After filtering datasets based on specific criteria, the study selected the waveform-noise dataset over waveform due to its greater number of input features. The waveform-noise dataset consists of wave attributes with three category labels based on wave type. Mean and variance normalization was applied to the features for this dataset. The study applied mean and variance normalization to the waveform-noise dataset and processed CIFAR10 and MNIST datasets by normalizing features and reducing dimensionality using PCA. This preprocessing scheme resulted in faster training and lower error values compared to training on raw data. The study applied mean and variance normalization to the waveform-noise dataset, while CIFAR10 and MNIST datasets were processed by normalizing features and reducing dimensionality using PCA. This preprocessing aimed to reduce input dimensionality to avoid excessive computation in the first layer. MNIST dataset has 60,000 training data points and 10,000 test data points, split into training and validation sets. CIFAR10 dataset has 50,000 training data points and 10,000 test data points, also split into training and validation sets. The waveform-noise dataset contains 5,000 data points split into training, validation, and test sets. Input dimensionality for CIFAR10 was 810, for MNIST was 334, and for waveform-noise was 40. Output dimensionality for CIFAR10 and MNIST was 10 classes. The study focused on linearly approximable regions for different datasets. It defined the relative diameter of a region based on input and output directions, using the Jacobian to approximate the true value of the function. The goal was to determine the largest value of k for a linear approximation to remain close to the true function value. The study defined the relative diameter of a region based on input and output directions using the Jacobian to approximate the true function value. It aimed to find the largest k for a linear approximation to stay close to the true function value, considering minibatches for networks using batchnorm. The study aimed to find the largest k for a linear approximation to stay close to the true function value, considering minibatches for networks using batchnorm. The \"largest k\" is computed by starting with k = 10 \u22129 and then checking the condition for increasing k until it fails. Values of k were checked in a geometric series with spacing factor 10 1 10. Smaller values of k could not be reliably checked due to numerical underflow. In a predictiveness study, Gaussian U and 10 random Gaussian V were used to obtain relative region size values for different configurations. The NLC was computed for each architecture with a single random initialization and trained using SGD with minibatches. Starting learning rates were tuned independently for each architecture to avoid bias, and training runs were conducted until validation classification error did not decrease for 10 epochs. To determine the optimal learning rate for training, the process involves dividing the learning rate by 3 and continuing training until validation classification error does not improve for 5 epochs. This cycle repeats with further reductions in the learning rate until it has been divided by 3 ten times. Training runs are terminated when the validation classification error does not improve for 5 epochs. Each architecture underwent 40 training runs with varying starting learning rates, determined through initial SGD optimization runs. The study used a starting learning rate of 10 for weight matrix updates, ensuring minimal perturbation during training. This choice was validated to ensure optimal performance across different architectures. The study used a starting learning rate of 10 for weight matrix updates to ensure optimal performance across different architectures. The 'trained network' refers to the network obtained during the training run with the lowest validation error, while the 'initial network' is in the randomly initialized state. Figures show the training runs, learning rates, test error, depth, and bias value of the networks. In FIG4, depth versus test error is plotted for the trained network. In FIG2, bias value of the initial network is compared with test error of the trained network. NLC of initial and trained networks is also compared in FIG2. NLC of trained network versus test error is shown in FIG2. Bias and NLC comparisons are further detailed in FIG9. Results from FIG2 are broken down in FIG0 for architectures with and without skip connections. 50 random architectures were selected for retraining. After selecting 50 random architectures for retraining, the learning rate was reduced by a factor of 3 once the classification error had not improved for 10/5 epochs. 60 different step sizes were considered, allowing successful training of architectures with high NLC. Results are presented in FIG2, where architectures were retrained with Adam instead of SGD. After retraining 250 waveform-noise architectures with Adam instead of SGD, the protocol remained the same with 40 training runs. Before measuring \u03b4W lb, Adam was run for 4 epochs without updates to warm-start running averages. The maximal error-preserving perturbation was computed similarly to the linearly approximable region relative diameter. The test error over the path from X to X was required to be at most 5% higher than at X. Starting with k = 10, the process was verified with the first and last 5 training runs never used. The values in FIG2 are the median over 100 values obtained from random minibatches of size 250 and Gaussian random direction matrices. NLC \u03c4 (1) 48 is the exponentiated value, and the linear approximation error is computed as ( DISPLAYFORM0 ) 2. NLC was computed as in section G, showing the median across 10 random initializations. The values for different initializations show little variation except for 49-layer networks with square or odd square activation functions. A matrix A and a random vector u are defined. Further, computations involving Q i (S x x(i)) and Q j (S x f (x, j)) are discussed, with a distinction made for batchnorm usage. In practice, we generalize the NLC to batchnorm networks by computing Q j (S X,\u03b2 f (X, j, \u03b2)) by dividing the dataset into minibatches and taking the standard deviation of output activation values. We estimate Q x ||J (x)|| F stochastically by replacing the loss gradient with Gaussian random vectors during backpropagation. In practice, the NLC is generalized to batchnorm networks by computing Q x ||J (x)|| F stochastically. This involves sampling minibatches and clamping Gaussian random vectors at the output during backpropagation. The value of Q x ||J (x)|| F is dependent on batch selection and can be computed using a block-diagonal matrix for networks without batchnorm. Stochastically compute Q X ||J (X)|| F by sampling random minibatches and backpropagating a unit Gaussian matrix. Sample 100 minibatches of size 250."
}