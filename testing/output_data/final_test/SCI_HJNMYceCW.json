{
    "title": "HJNMYceCW",
    "content": "RESLOPE is a novel algorithm for reinforcement learning and bandit structured prediction problems with sparse loss feedback. It automatically learns a denser reward function and uses a contextual bandit oracle to balance exploration and exploitation. It outperforms state of the art algorithms in various environments and settings, reducing the need for large training datasets. In sparse feedback settings, interactive systems face challenges like credit assignment and exploration/exploitation. Learning in such settings is crucial, especially when feedback is only provided episodically at the end of an episode. This scenario is common in reinforcement learning problems, such as a barista robot receiving feedback only after a customer's cappuccino is finished. The feedback from a customer after their cappuccino is finished is crucial in reinforcement learning. A novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), aims to learn effective representations of the loss signal for credit assignment. It decomposes the episodic loss into per-time-step losses, allowing for non-linear representations of the episodic loss. RESLOPE is a novel algorithm that aims to learn effective representations of the loss signal for credit assignment in reinforcement learning. It decomposes the episodic loss into per-time-step losses, allowing for non-linear representations. The algorithm operates as a reduction to contextual bandits, focusing on the credit assignment problem and showing efficacy on benchmark reinforcement problems and bandit structured prediction problems. The paper introduces RESIDUAL LOSS PREDICTION as a solution to bandit structured prediction problems in episodic Markov Decision Processes. It operates as a reduction to contextual bandits, aiming to map the reinforcement learning problem to an oracle for a good solution. The algorithm discussed involves a learning agent interacting with its environment through Markov Decision Processes. The goal is to learn a policy that guides the agent's behavior in the environment. The text discusses the learning goal of estimating a policy \u03c0 from a hypothesis class of policies \u03a0 with minimal expected loss in the context of a learning agent interacting with its environment through Markov Decision Processes. The policy's quality is quantified by its value function or q-value function, which associates states or state/action pairs with expected future loss. The learning problem is a special case of reinforcement learning with episodic operation. At each round, the world reveals a context x, the system chooses an action a, and a scalar loss is revealed. The goal is to learn a policy with low regret from a policy class. Contextual bandit algorithms assume access to an oracle supervised learning algorithm for optimization. In structured prediction, the goal is to predict correlated output variables based on structured input sequences. This involves transforming the contextual bandit problem into a cost-sensitive classification problem using an oracle supervised learning algorithm. The algorithm explores and updates based on observed losses to train on supervised examples. Recently, structured prediction problems are being solved incrementally using recurrent neural network (RNN) models. RNNs can predict each part of the output sequentially, conditioning on previous decisions. Training these models with cross-entropy has drawbacks like overfitting and sensitivity to errors. The focus of this paper is on the bandit structured prediction setting, where only bandit feedback is available during training. The goal is to learn a good policy in a Markov Decision Process, with losses arriving at the end of episodes. The proposed solution, RESIDUAL LOSS PREDICTION (RESLOPE), automatically deduces the best course of action. RESLOPE addresses sparse reward signals and credit assignment by learning a decomposition of the reward based on episodic losses. It helps in estimating the impact of different actions taken during a task, similar to calculating the time lost due to taking a different path or talking to a friend on the way to a coffee shop. RESLOPE tackles the problem of sparse reward signals and credit assignment by decomposing the reward signal and relying on a contextual bandit learning algorithm for exploration vs exploitation. It operates as a reduction from reinforcement learning to contextual bandits, predicting residual losses to solve the credit assignment problem. In the structured residual loss prediction, the system assigns part-of-speech tags to a sentence. Each state represents a partial labeling, with an episodic loss associated with the end state. The algorithm doesn't assume access to the optimal output structure or hamming loss at every time step. In the contextual bandit setting, a reference policy is used to improve learning, with a hyperparameter controlling trust in the reference policy. In reinforcement learning, the reference policy is replaced as learning improves. RESLOPE is a variant of the Locally Optimal Learning to Search approach with key differences in residual loss prediction, exploration strategies, and parameter update strategies. It relies on a contextual bandit oracle for actions, costs, and updates. The contextual bandit algorithm predicts costs and selects actions based on minimal predicted cost. Algorithm 1 outlines the formal construction of the reduction using a MAKEENVIRONMENT(t) function to create a new environment. RESLOPE uses single-step contextual bandit training examples to learn a good policy by reducing long horizon trajectories. The contextual bandit algorithm uses a mixture of policies \u03c0 learn and \u03c0 ref, with exploration handled by CB.ACT and cost estimation by CB.COST. A single contextual bandit training example is constructed for each deviation step, with input as the observation, action, and probability selected by CB.ACT, and cost calculated as the observed total cost minus the cost of other actions in the trajectory. This example is sent to CB.UPDATE for training. When the policy is an RNN, a loss is computed and back-propagated through the RNN. The rollout policy \u03c0 mix is chosen as \u03c0 ref with probability \u03b2 or \u03c0 learn t\u22121 with probability 1 \u2212 \u03b2. The contextual bandit oracle learns a policy for predicting actions minimizing expected loss by estimating target costs for unpredicted actions and balancing exploitation exploration trade-off. It then calls a cost-sensitive multi-class oracle to update its policy based on input examples. The reduction challenge in cost-sensitive classification involves constructing cost-sensitive classification examples using three methods: inverse propensity scoring, doubly robust estimation, and multitask regression. Inverse Propensity Scoring corrects for the shift in action proportions predicted by the policy, while doubly robust estimation combines observed costs with predicted costs for all actions. The DR estimator estimates the target cost vector y, reducing variance. Multitask regression differs from IPS and DR by directly reducing to importance-weighted regression. MTR maintains K regressors for predicting costs. Different exploration strategies like Uniform, Boltzmann, and Bootstrap Exploration are used in MTR. Bootstrap is a method that uses multiple policies to vote on actions, ensuring diversity. It can operate in \"greedy update\" and \"greedy prediction\" modes. When having a good reference policy but no Q-value estimates, roll-outs are used to estimate Q-values. Suboptimal or missing reference policies change the goal to competing with both the reference policy and a local one. The analysis shifts to competing with both the reference policy and a local optimality guarantee. Theorem 1 states that running RESLOPE with a contextual bandit algorithm results in regret equal to the suboptimality of the reference policy. When the problem is realizable and the contextual bandit algorithm is no-regret, RESLOPE is also no-regret. If the reference policy is not known to be optimal, a regret to a convex function is obtained following the LOLS analysis. The analysis introduces a regret to a convex combination of \u03c0 ref and the learned policy's one-step deviations. The combined regret of \u03c0 is defined for arbitrary \u03b2, showing suboptimality to \u03c0 ref and the policy's own realizable one-step deviations. The multiple deviation variant of RESLOPE is presented, operating by performing deviations at each time step. The multiple deviation variant of RESLOPE updates the learned policy multiple times per episode by generating H examples, where H is the length of the time horizon. This approach leads to superior performance by increasing the number of available samples for training. Theoretical analysis holds when \u03b2 = 0 in single deviation mode, guarantee reduces to local optimality. Experiments conducted on reinforcement learning and structured prediction tasks to evaluate learning algorithms' speed from episodic loss. Implemented on DyNet neural network optimization package BID30, tested in Blackjack, Hex, Cartpole, and Gridworld environments. Results reported in terms of cumulative loss. The evaluation framework considers fully online setup for structured prediction tasks, measuring algorithm performance in terms of average cumulative loss and balancing exploration and exploitation. Experiments are conducted on English Part of Speech Tagging, English Dependency Parsing, and Chinese Part of Speech Tagging tasks. In structured prediction experiments, various reinforcement learning algorithms are compared, including Reinforce, Proximal Policy Optimization (PPO), and Advantage Actor-Critic (A2C). A recurrent neural network is used as the policy, incorporating a real-valued hidden state and features from the environment. An upper bound on performance is estimated using supervised learning with full information. The policy in reinforcement learning experiments utilizes a recurrent neural network with a hidden state, environment features, and previous action embedding. A feedforward network selects actions based on the hidden state, with output values used as predicted costs in certain scenarios. In reinforcement learning experiments, a recurrent neural network is used to determine actions based on predicted costs. The RESLOPE approach is compared to other methods, showing superior performance in tasks like Blackjack, Hex, and Grid. RESLOPE outperforms competing approaches in tasks like Hex and Grid, but underperforms in Cartpole compared to Reinforce and PPO. Bootstrap exploration improves upon Boltzmann exploration in Blackjack and Grid. In structured prediction problems, RESLOPE significantly outperforms other algorithms on training and development loss. In the construction of RESLOPE, various tunable parameters such as contextual bandit learner (IPS, DR, MTR) and exploration strategy (Uniform, Boltzmann, Bootstrap) significantly impact training error on tasks like English and Chinese POS tagging and dependency parsing. Ablating these parameters shows that MTR and DR are competitive, Bootstrap exploration is superior, and greedy prediction and update are beneficial. In the final set of experiments, RESLOPE's performance is studied under different non-additive loss functions to learn good representations for episodic loss. Different incremental loss functions are considered for each time step, including Hamming, Time-Sensitive, and Distance-Sensitive. The importance of multiple deviations is observed for all algorithms, with Reinforce and PPO performing poorly with only single deviations. RESLOPE's performance is studied under different non-additive loss functions to learn good representations for episodic loss. Different incremental loss functions are considered for each time step. Experiments with PPO using larger minibatches are reported in the appendix. Adjustments were made to match the PPO algorithm described in BID39. PPO's performance falls between RESLOPE and Reinforce on Blackjack, slightly superior to RESLOPE on Hex. RESLOPE's performance is studied under different non-additive loss functions to learn good representations for episodic loss. Experiments with PPO using larger minibatches are reported in the appendix. RESLOPE and Reinforce perform better on Blackjack and Hex compared to other environments like Cartpole and Gridworld. Memory limitations prevented experiments in the structured prediction setting. Results show RESLOPE's ability to learn optimal representations. RESLOPE can learn optimal representations for incremental loss functions, with faster learning observed when the episodic loss function is additive. Performance degrades with non-additive loss functions, particularly with L-5 norm and incremental hamming loss. The model can still learn well with smoother loss functions like distance and time sensitive losses. RESLOPE can learn optimal representations for incremental loss functions, with faster learning observed when the episodic loss function is additive. Performance degrades with non-additive loss functions, particularly with L-5 norm and incremental hamming loss. The model can still learn well with smoother loss functions like distance and time sensitive losses. RESIDUAL LOSS PREDICTION builds on bandit learning to search frameworks, achieving strong regret guarantees and good empirical performance without needing additional feedback. Recent algorithms have updated classic learning to incorporate deep learning for sequence-level global loss functions in structured prediction. Mixing supervised learning and reinforcement signals has become popular for task-specific loss tuning. Bandit feedback is studied for optimizing structured prediction models, with reward shaping used for sparse and episodic reward signals. RESLOPE is a method that automatically learns task-specific reward shaping for a learning agent, going beyond rewards from the environment. It outperforms Proximal Policy Optimization (PPO) in terms of learning speed, but has some limitations like memory and time constraints with the bootstrap sampling algorithm. The amortized bootstrap approach by BID27 uses amortized inference with implicit models to approximate the bootstrap distribution over model parameters. Contextual bandit algorithms with RNNs operate on arbitrary policy classes, but the shifting distribution of costs in the multi-deviation setting poses a significant challenge for RESLOPE. RESLOPE is designed for discrete action spaces but extending it to continuous action spaces remains an open problem. The authors acknowledge the helpful comments and insights from reviewers and thank individuals for reviewing earlier drafts. The work was partially funded by an Amazon Research Award and supported by the National Science Foundation under Grant No. 1618193. The National Science Foundation funded the research. Contextual bandit learning involves exploring/exploiting unknown distributions. Algorithms like EXP4.P BID5 aim for sublinear regret in time T. These algorithms are called \"no regret\" as average regret per time step approaches zero. The algorithms assume access to an oracle for further reduction. In this paper, algorithms are used to transform the contextual bandit problem into a cost-sensitive classification problem by assuming access to an oracle supervised learning algorithm. These algorithms explore how to choose actions when faced with new data and update based on observed losses. An example is the -greedy exploration algorithm that explores randomly on 10% of examples and returns the best guess on 90% of examples. The update rule involves \"inverse propensity scaling\". In cost-sensitive learning, the update rule involves \"inverse propensity scaling\" (IPS) BID18. This scaling justifies that the expected value of the cost vector is equal to the true costs for each action. Alternative exploration and variance reduction strategies are discussed. Structured prediction problems are solved incrementally using recurrent neural network (RNN) models, predicting each word in sequence. By framing structured prediction as a sequential decision-making problem, the \"Learning to Search\" framework avoids overfitting and error sensitivity issues. It converts problems into search tasks, defines state and action sets, and uses structured features to capture output dependencies. This approach applies imitation-learning algorithms to solve structured prediction problems effectively. In the \"Learning to Search\" framework, structured prediction is framed as a sequential decision-making problem. It involves capturing output dependencies, constructing a reference policy, and learning a policy that imitates or improves upon the reference policy. This approach effectively solves structured prediction problems using imitation-learning algorithms. In the \"Learning to Search\" framework, structured prediction is viewed as a sequential decision-making task. The goal is to transition from an initial state to a final output by minimizing the structured prediction loss. Contextual bandit approaches are often used to reduce this problem to cost-sensitive classification, where the aim is to learn a classifier with low expected loss. The classification task involves learning a classifier f that is small. Cost-sensitive classification is solved by reducing it to regression in a one-against-all framework. A regression function g(x, i) predicts costs for input/class pairs, with the predicted class chosen based on argmin i g(x, i). The cost-sensitive approach achieves low regret with a good regressor, using regression against Huber loss. A lemma shows that the difference in total loss between policies can be computed as a sum of per-time-step advantages. The text discusses the analysis of a contextual bandit algorithm, focusing on minimizing regret and estimating costs. It mentions the use of a learned policy and a cost estimator, as well as the concept of internal regret in a K-player game. The analysis of a contextual bandit algorithm involves minimizing regret and estimating costs. By minimizing internal regret in a K-player game, overall values converge to the game's value. Converting from external regret to internal regret is necessary, achieved through a reduction algorithm. With a strong realizability assumption, multiple no-regret minimizers reach a time-averaged minimax value as N \u2192 \u221e, reducing the approximation error term. If the contextual bandit algorithm has sublinear regret in N, both the algorithm and the approximation error term approach zero as N \u2192 \u221e. The proof shows that the algorithm is no-regret by demonstrating that both CB(N) and the approximation error term decrease as N approaches infinity. The final step relies on the contextual bandit algorithm estimating the inner-most expectation, leading to the joint no-regret minimization of all \"players.\" Blackjack is a card game where players aim to reach a sum of 21 without exceeding it, playing against a dealer who hits until they have a certain total. Blackjack is a card game where players aim to reach a sum of 21 without exceeding it. Aces can count as 11 or 1, and face cards have a value of 10. The reward for winning is +1, drawing is 0, and losing is -1. Hex is a two-player board game where players connect their two opposing sides with their stones to win. The reward is +1 for winning and -1 for losing. Cart Pole is a classic control problem known as the \"cart-pole\". Cart Pole is a classic control problem involving balancing a pole on a cart by applying forces to the cart. The goal is to prevent the pole from falling over, with the episode ending if the pole tilts more than 15 degrees or the cart moves too far from the center. The state is represented by four values indicating the pole's position, angle, and velocities. The Cart Pole problem involves balancing a pole on a cart by applying forces to prevent it from falling over. The total cumulative reward is based on the number of time steps the pole remains upright. Grid World is a 3x4 grid with rewards and obstacles. The agent has partial visibility and incurs costs for each step taken. English POS Tagging experiments are conducted on the Penn Treebank tags. Domain adaptation is simulated by training a policy on TweetNLP dataset. The study evaluates the performance of BID33 and BID49 in domain adaptation settings using Hamming loss as a measure. It involves conducting POS tagging experiments on the Penn Treebank and Chinese Penn Treebank datasets, simulating bandit episodic feedback. The search space is defined by selecting part-of-speech tags sequentially for words in the sentence. The text discusses the use of an arc-eager dependency parser for English Dependency Parsing, trained on the TweetNLP dataset and evaluated on the Penn Treebank corpus. It utilizes pretrained Glove embeddings for English and FastText embeddings for Chinese, running a bidirectional LSTM over the input sentence. The input features for labeling words in POS tagging experiments are the biLSTM representations at each position. The input features for dependency actions in POS tagging experiments are a concatenation of biLSTM features. Model parameters are optimized using the Adam optimizer with tuned learning rates and hyperparameters for structured prediction experiments are also tuned. For reinforcement learning experiments, the optimal network architecture for supervised pretraining was fixed. Parameters tuned included Policy RNN dimension and number of policy layers. Nonlinearities, action embeddings size, and input RNN form were not tuned. No regularization or dropout was used. In reinforcement learning experiments, the structured prediction models were pretrained for 20 passes with early stopping. The optimizer state was reset before bandit learning. A two-layer policy with 20-dimensional vectors was used for all tasks. Hyperparameters were tuned for each algorithm, with Adam consistently performing well. Different configurations were tested for English and Chinese POS tagging tasks. In reinforcement learning experiments, a 50 dim 2 layer LSTM with a 1 layer 50 dimensional policy was used. Results of ablating parts of the RESIDUAL LOSS PREDICTION approach are shown in Table 1. Different parameters were tested for A2C, PPO, and RESLOPE strategies in reinforcement/bandit experiments. In reinforcement learning experiments, algorithm hyperparameters and learning rate are optimistically chosen based on final evaluation criteria. 100 replicates are performed in the RL setting and 20 replicates in the structured prediction setting. Various aspects of RESLOPE are ablated, and only two default tricks are employed. A comparison is made between single-deviation and multiple-deviation versions of RESLOPE, as well as variants of Reinforce, PPO, and DAgger. The study compares the performance of different algorithms in bandit structured prediction problems. Results show that algorithms like PPO and Reinforce suffer when only allowed single deviations, while DAgger still learns, albeit more slowly. RESLOPE behaves similarly but not as poorly. Overall, the findings suggest that samples generated with multiple deviations are beneficial. Experiments on a synthetic sequence labeling dataset with random integer input sequences of length 6 were conducted using RESLOPE with bootstrap sampling in multiple deviation mode. The study focused on RESLOPE's performance under different nonadditive loss functions, showing that the gain in number of samples outweighs the lack of independence in the generated samples. In a grid world reinforcement learning environment, a 4x4 grid is studied with specific rewards and blocked cells. The agent incurs a cost for each step taken and has a high probability of success. Two episodic reward settings are considered: additive and nonadditive. Results are similar to a structured prediction setting. Results in FIG9 show that performance is better with additive loss (blue) compared to non-additive loss (green) in a grid world reinforcement learning environment."
}