{
    "title": "rJl5rRVFvH",
    "content": "Most deep reinforcement learning systems struggle to effectively learn from off-policy data without online exploration. To address this, a novel off-policy batch RL algorithm called Way Off-Policy (WOP) uses KL-control to penalize divergence from a pre-trained prior model, reducing extrapolation error and enabling offline learning without exploration. Dropout-based uncertainty estimates are used to improve efficiency in estimating target Q-values. The WOP algorithm is tested on traditional RL tasks and open-domain dialog generation. Way Off-Policy (WOP) is a novel off-policy batch RL algorithm that uses KL-control to penalize divergence from a pre-trained prior model, reducing extrapolation error and enabling offline learning without exploration. It achieves significant improvements over state-of-the-art prior methods in batch deep RL and is tested on traditional RL tasks and open-domain dialog generation. To scale deep reinforcement learning to real-world domains, algorithms need to learn from off-policy data and be able to test policies offline before deployment. This off-policy, batch reinforcement learning setting poses a challenging problem as most deep RL algorithms struggle with data not correlated with the current policy. Off-policy algorithms like Q-learning struggle to learn in offline batch settings when unable to explore. Batch RL models can suffer from extrapolation error if data is insufficient, leading to unrealistic value estimates for state-action pairs not in the batch. Mismatched distributions between batch data and learned policy can make it impossible to correct for extrapolation error. Leveraging pre-trained models can help resolve these issues. To address issues in offline batch settings, a pre-trained generative model of the state-action space is used during RL training to penalize divergence from the prior model. This technique ensures the RL model learns a policy that aligns with the state-action distribution of the batch data, combating extrapolation errors. The KL-constrained model learns the optimal policy within the support of batch data, using dropout for uncertainty estimates and addressing overestimation bias. Outperforms Batch Constrained Q-learning in both traditional RL and real-world reinforcement learning scenarios. Improving open-domain dialog systems by using human reactions in chat to enhance machine learning performance. Utilizing implicit human reactions like sentiment and conversation length to enhance the policy. Challenges arise in applying off-policy batch RL to language generation due to the vast state space and 20,000-dimensional action space. The text discusses the challenges of applying off-policy batch RL to language generation, particularly due to the large state and action space. A platform was developed for humans to interact with deep neural network dialog models, and data collected from this platform was used to train BRL models effectively. The Way Off-Policy algorithm was utilized to learn from a diverse set of dialog models and reward data. The paper introduces a novel algorithm called Way Off-Policy learning, which utilizes KL-control from a pre-trained prior model to reduce extrapolation error in batch RL. It also presents experiments demonstrating the effectiveness of WOP in traditional RL tasks and open-domain dialog generation, using novel conversation rewards based on implicit human preferences in text. This approach is the first to learn from implicit signals in conversation offline using batch RL. KL-control is a form of stochastic optimal control that uses the Kullback-Leibler divergence to regularize a reinforcement learning policy. Examples include Trust Region Policy Optimization (TRPO) and conservative, KL-regularized policy updates. It can also be applied to entropy maximization and has been used to improve transfer learning between maximum likelihood estimation training. Our work proposes penalizing KL-divergence from a learned prior model of the state-action space to enhance offline batch RL, distinct from other strategies focusing on exploration or off-policy policy evaluation. In off-policy deep batch reinforcement learning, various methods have been proposed such as importance sampling, model estimation, correction to policy gradients, covariance-shift methods, and normalized feature representations. Batch Constrained Q-learning (BCQ) addresses off-policy deep batch RL by training a generative model of the batch and selecting actions based on a Q-estimate, but it does not directly integrate information about the action distribution. Our approach proposes using dropout to estimate uncertainty in the target Q-network, allowing the model to strategically deviate from the prior for more reward. Different forms of uncertainty estimates have been used in reinforcement learning, such as Bayesian uncertainty estimates as an alternative to double DQN. Improving dialog systems with reinforcement learning has been limited to task-oriented systems, often incorporating human input through explicit or implicit signals. Efforts to expand RL to open-domain dialog settings are limited, with authors sometimes opting for a restricted action space. Ziegler et al. (2019) used human feedback to enhance language model performance, emphasizing the importance of penalizing KL-divergence from a pre-trained model. In open-domain dialog settings, using implicit cues from real human interactions for training dialog models through batch RL is a novel approach. The goal is to maximize reward by taking actions based on policy \u03c0 and receiving rewards. Q-learning is utilized with a discount factor \u03b3 for future rewards. This method differs from previous approaches that focused on restricted action spaces and penalizing KL-divergence from a pre-trained model. In batch RL, Q-learning uses a discount factor \u03b3 to estimate future rewards through iterative updates based on the Bellman equation. A Q-network approximates the total expected discounted future reward and drives the policy \u03c0. Training is done with a fixed batch of data B, where the Q-network weights are updated to minimize extrapolation errors. Extrapolation errors in batch reinforcement learning can lead to inaccurate value estimates, which are compounded by the optimistic nature of algorithms based on the Bellman operator. This can result in overestimation of expected future rewards, hindering learning off-policy. In normal RL settings, this overestimation bias drives exploration to refine value estimates with high variance. In batch reinforcement learning, overestimation of Q-values can hinder learning off-policy. Clipped Double Q-learning addresses this by using two Q-networks and a Bayesian uncertainty estimate. This method is more efficient than traditional approaches. Batch Constrained Q-learning (BCQ) addresses the problem of overestimation in batch reinforcement learning by constraining Q-network actions to be close to the batch data. It uses a generative model to sample actions and a perturbation model to alter actions within a specified range. This approach leads to more pessimistic estimates and favors actions well covered by the batch data. Batch Constrained Q-learning (BCQ) uses a generative model to sample actions and a perturbation model to alter actions within a specified range. BCQ learns Q-estimates that incorporate the perturbation model, and the policy is selected based on the action with the maximum Q-value. An adaptation of BCQ to discrete action spaces (DBCQ) is proposed, which does not use a continuous perturbation model. DBCQ policy incorporates dropout-based uncertainty estimates and penalizes divergence between the learned prior and the Q-network policy while maximizing reward. The objective is to maximize the expected value function of the policy by balancing rewards for high probability actions under the prior distribution and entropy regularization to maintain diversity in the action space. This approach is crucial for generative models like dialog systems to prevent collapse to repeated samples. The \u03a8-function, derived as a soft version of the entropy-regularized Q-function, improves learning by reducing overestimation of future rewards. It avoids noisy estimates and leads to more stable temporal-difference updates. The Way Off-Policy (WOP) algorithm combines Monte Carlo target estimation, \u03a8-learning, and KL-control for effective results in traditional RL tasks. In traditional RL tasks using the OpenAI gym, experiments were conducted with techniques such as online Q-learning, storing experience samples in a replay buffer, training a prior model using a VAE, and using the prior for imitation learning. These techniques were benchmarked against vanilla Q-learning on batch data under different conditions. The study compared different offline learning algorithms using experience samples from online training. Results showed that WOP outperformed Batch Q, imitation learning (BC), DBCQ, and the original behavior policy. Noisy demonstrator and expert demonstrator conditions affected the performance of Batch Q and BC. In training an open-domain dialog model from human feedback, the response of a human to the bot's utterance is used to compute a reward signal. The model constructs a response utterance by choosing actions iteratively based on the conversation history. WOP consistently outperforms other algorithms in this environment by learning to balance staying close to the prior and obtaining higher rewards. Applying RL to dialog generation involves iteratively choosing actions from a high-dimensional action space. Over 40 dialog models with different architectures were trained on various datasets, resulting in models with diverse language distributions. These models were deployed on a web server for real-time inference, and human interaction data was collected for further learning. The batch data was used to train RL models with a pre-trained language model to estimate p(a|s). Trained RL models were re-deployed to the web for evaluation by 90 Mechanical Turk workers who provided ratings on bots' quality, fluency, diversity, contingency, and empathy. Participants could also give feedback through upvoting or downvoting utterances. The text discusses using human feedback to improve a dialog model's ability to engage in natural conversation. The model is rewarded for eliciting positive sentiment, longer conversations, and more words from the user. The goal is to create an agent that is intrinsically motivated to produce positive reactions in its human conversation partner. The model is rewarded for eliciting positive sentiment, longer conversations, laughter, high semantic similarity, and asking questions. These rewards were designed post-hoc from human data and are crucial for effective batch RL. Reward-maximizing methods like Batch Q diverge from realistic language. KL-control methods output plausible language by staying close to the prior and using polite, cheerful language to maximize implicit human reward. Comparing models involves looking at human users' ratings, votes, and automatic signals detectable from the text itself. MC Target Q estimation leads to modest improvements in votes and human reward, while using \u03a8-learning improves all three metrics. KL-control models show significant gains in performance over baseline models in both ratings and human reward. A one-way ANOVA confirms the superiority of KL-control models, validating the use of a strong, pre-trained prior to enhance batch RL. Without KL-regularization, baseline RL models diverge from the prior quickly, leading to poor performance as seen in DBCQ. The use of a pre-trained prior in batch RL models, such as KL-control, helps prevent divergence from the prior and unrealistic Q-estimates, leading to more realistic language generation in dialog settings. Without this regularization, models like DBCQ struggle to scale effectively to dialog domains, resulting in highly diverse and unrealistic utterances. The KL-control model shifts towards cheerful and polite speech to elicit positive human responses. Models trained with implicit human rewards use more realistic and supportive language. Post-hoc metrics are created to measure this effect, showing differences in metrics and rewards across models. KL-control models rely on realistic and polite dialog to achieve higher human rewards compared to baseline methods like Batch Q. Table 3 presents results of WOP models trained with only a. The results of WOP models trained with a single reward function are presented in Table 3, showing that maximizing positive and minimizing negative sentiment in user interactions leads to the highest quality bot. Bots trained on manual upvotes and downvotes from users fail to perform well, as users did not vote frequently enough to provide a good training signal. This highlights the importance of affective signals for successful conversations with humans. This paper introduces the Way Off-Policy (WOP) algorithm for batch RL, utilizing KL-control from a strong prior model to improve performance. Results show WOP outperforms state-of-the-art BRL techniques in traditional RL tasks and dialog generation, emphasizing the importance of KL-control in high-dimensional settings. The paper introduces the Way Off-Policy (WOP) algorithm for batch RL, emphasizing the importance of KL-control in high-dimensional settings. Several reward functions are proposed for open-domain dialog generation models to learn from human interaction cues, with implicit rewards leading to better performance than explicit feedback. The total reward used for training bots is a combination of various factors like question, semantic coherence, laughter, sentiment transition, sentiment, words elicited, and conversation length. The paper introduces the Way Off-Policy (WOP) algorithm for batch RL, emphasizing the importance of KL-control in high-dimensional settings. It discusses the use of a sentiment-detection model trained on Twitter data to compute sentiment in short texts like conversation utterances. The model outputs a probability distribution over 64 emojis, and weights are defined over these emojis to calculate a sentiment reward. Sentiment-transition rewards are also computed. The curr_chunk discusses the computation of rewards in a conversation based on sentiment transition, conversation length, words elicited, and occurrences of laughter. The sentiment reward is calculated using weights defined over emojis from a sentiment-detection model trained on Twitter data. The curr_chunk discusses rewarding bots for maximizing user laughter and maintaining topic coherence in conversations. It leverages a state-of-the-art sentence embedding model to compute semantic similarity between user input and bot responses. The bot is rewarded for using question words and marks in its utterances, leading to a shift towards more polite and supportive speech. Post-hoc metrics measure politeness and supportiveness based on the presence of specific phrases in the bot's responses. The Q-networks shared the same architecture with three fully-connected layers. The model of p(a|s) was learned using a Variational Autoencoder. The VAE model had a latent dimension of size 256 and used a mean squared error loss. The encoder and decoder consisted of two linear layers with 750 neurons each. The model learned functions for predicting the next action from the latent embedding z. All models were trained with the Adam optimizer. Each experiment ran 50 trials with different random seeds. The Behavior policy was trained for 20,000 steps in the environment. In the Full buffer condition, offline agents saw 20,000 experience samples. In the Expert demonstrator condition, offline agents received the last 10,000 experience samples from the trained agent. In the Concurrent condition, offline agents saw a moving window of 1000 samples. The online learner used the most recent 1000 samples for learning with a learning rate of .001 and decayed linearly from 1.0 to .01 over 2000 steps. The KL-constraint was computed as D KL [q(\u03c4 )||p(\u03c4 )] = \u03b1 log p(a|s) \u2212 \u03b2 log \u03c0(a|s), where \u03b1 = 0.5 and \u03b2 = 0.1. DBCQ sampled n = 2 actions before selecting the best action based on the maximum Q-value. The language models employed were Variational Hierarchical Recurrent Encoder Decoder (VHRED) with knowledge distillation. The models were trained on movie dialogs and a dataset from reddit.com/r/casual_conversation. The RL models used weights from the best model trained on the Reddit dataset. They were trained for 800-1000 batches with a batch size of 32. Hyperparameters included discount \u03b3 = 0.5, weight on RL reward vs. KL-divergence term c = 2, and learning rate r = .0001. VHRED model parameters were Context RNN hidden size = 1000, decoder hidden size = 1250, encoder hidden size = 1250, z embedding size = 600, and dropout d = 0.2. The experimental setup included a z embedding size of 600, gradient clip of 1.0, and dropout of 0.2. The maximum conversation length was limited to 5 utterances, with a maximum sentence length of 30 tokens. Additional layers were added to the Context RNN for semantic prediction using knowledge distillation. Experiments were also conducted in the Acrobot-v1 environment with a smaller VAE and different hyperparameters. In the experimental setup, a z embedding size of 600, gradient clip of 1.0, and dropout of 0.2 were used. The maximum conversation length was limited to 5 utterances with a maximum sentence length of 30 tokens. Additional layers were added to the Context RNN for semantic prediction. The weight on the prior was \u03b1 = 0.5 and the entropy term was \u03b2 = 0.1. Bots trained to optimize certain types of human response do not perform best in testing with a new set of users. The small batch size of data collected for the task limited the bots' ability to elicit long responses from users. Despite KL-control methods outperforming the MLE prior in obtaining implicit reward signals, it did not result in significantly higher quality ratings from human judges. This suggests that the reward functions optimized by RL bots may not fully capture what constitutes a high-quality conversation with a human user. The study focused on improving measures of human enjoyment in conversations with bots by collecting data through a platform called https://neural.chat. The platform hosted deep neural network dialog models online for real-time inference, allowing users to rate the bots after interacting with them. The server used Google Cloud Platform with a Django program served by NGINX and uWSGI, simplifying the process and increasing reliability. The server configuration for the study supported hundreds of users and more than 30 bots concurrently. Chatbots were kept separate from the Django project and maintained independently. Each chatbot extended an abstract class with key methods for Django to use and was registered in a globally accessible dictionary. Django was able to dynamically access and use the chatbots through this dictionary. The chatbots utilized PyCUDA, which is important to note as PyCUDA does not work in a certain way. The chatbots used PyCUDA, which does not work in a multiprocessing environment. To accommodate this, uWSGI was configured to have only one python process and disable multiprocessing. All chatbots were kept in memory at all times in the Django process, requiring a high amount of RAM on a 64GB virtual instance with a GPU with 16GB RAM. This setup allowed for fast server response times without an increase in response time when using the bots in requests. For more information on server configuration, refer to the server documentation available at the provided URL. The platform allows users to host their own bots and evaluate them interactively. For server configuration details, refer to the documentation at the provided URL."
}