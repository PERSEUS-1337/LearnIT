{
    "title": "rylfg-DNM",
    "content": "We propose enhancing Deep RL algorithms by allowing them to choose action sequences, improving exploration and convergence. This modification can be easily integrated into any Deep RL framework and has shown superior performance compared to the GA3C algorithm on Atari Games. In basic reinforcement learning, an agent interacts with an environment by taking actions, observing states, and receiving rewards until reaching a terminal state. Deep Reinforcement Learning aims to find a policy that maximizes overall reward by interacting with an environment until reaching a terminal state. Deep Q-Learning Networks have shown that Deep Neural Networks can approximate value and policy functions by storing agent data in an experience replay memory. This approach improves upon prior reinforcement learning methods by avoiding myopic handcrafted designs. Several video games can be played by directly observing raw image pixels and demonstrating super-human performance on the ancient board game Go. To address heavy computational requirements in training Deep Q-Networks (DQN), methods increasing parallelism and reducing computational cost were proposed, leading to impressive performance. A breakthrough method called Asynchronous Advantage Actor-Critic (A3C) achieves state-of-the-art results on gaming tasks, learning to play Atari games efficiently from raw screen inputs. A3C, a method for playing Atari games efficiently from raw screen inputs, has been further improved with GA3C on GPUs, showing accelerated learning. However, current Deep RL algorithms still face slow progress due to poor exploration, with initial experiences being random sequences with low rewards until high-reward actions are identified and learned. In this work, the focus is on improving the convergence of deep reinforcement learning by expanding the action space to include sequential actions. This approach aims to address the challenge of slow progress in current Deep RL algorithms due to poor exploration, where random sequences with low rewards are initially experienced before identifying high-reward actions. Our proposal expands the action space in Deep RL to allow for premeditated sequences of actions at a given state, leading to better convergence behavior compared to existing frameworks. This approach outperforms the fastest known implementation, GA3C, and can be easily incorporated into existing implementations like Deep Q Network and A3C. In reinforcement learning, methods can be categorized into Value-based, Policy-based, and Actor-Critic approaches. Value-based methods involve defining a Q-function to estimate future rewards for state-action pairs, with Deep-Q learning being a popular technique. The Q-function is learned iteratively by minimizing a loss function, following the Bellman equation in Dynamic Programming. This approach, known as 1-step Q-Learning, involves observing a single action and its reward. In policy-based model-free methods, a neural network computes the policy \u03c0(a t |s t ; ) using a function approximator with parameters \u03b8. The popular REINFORCE method updates \u03b8 to maximize cumulative rewards. A baseline is used to determine if an action is 'good' or 'bad', with the baseline being redefined as b t (s t ) to provide context of the current state. In the Actor-Critic Framework, the policy function acts as an actor responsible for taking actions, while the Value function evaluates the actions. A3C (Asynchronous Advantage Actor-Critic) is a state-of-the-art algorithm that uses an Asynchronous framework with multiple agents sharing a central policy and communicating gradients to update it. The A3C algorithm utilizes a shared neural network to update the central policy using gradients from multiple agents. The policy distribution and Value function are outputs of the network. The objective function includes maximizing the policy objective and incorporating entropy to encourage exploration of new actions. The A3C algorithm uses a shared neural network to update the central policy with gradients from multiple agents, incorporating entropy to encourage exploration. The gradients are calculated and stored by each agent until they communicate them to the central network for updates. A major concern is the reliance on sequential training in A3C and the challenge of leveraging information while training in Reinforcement Learning paradigms. GA3C BID0 is an alternative framework for A3C that allows GPU usage by using larger batches of input-output pairs. It maintains PredictionQueue and TrainingQueue for each agent to submit policy requests and input-reward pairs. A predictor takes PredictionQueues from multiple agents, sends an inference query to the GPU, and updates policies for all agents. Our proposal extends the basic action set A in GA3C to include sequences of actions up to length K, presenting new challenges in dealing with trade-offs like data transfer size vs number of transfers to GPU and number of predictors vs size of prediction batches. Our proposal extends the basic action set A in GA3C to include sequences of actions up to length K, presenting new challenges in dealing with trade-offs like data transfer size vs number of transfers to GPU and number of predictors vs size of prediction batches. The new action space A + = A \u222a A 2 = {L, R, LL, LR, RL, RR} consists of meta-actions, which can be single basic actions or sequences. Instead of outputting Q values or policy distributions for basic actions, our algorithm outputs values for each meta-action in the enlarged action set A +, allowing the network to anticipate the \"goodness\" of meta-actions earlier in the exploration phase. This approach is inspired by the importance of \"Combo\" actions in sports and video games, where sequences of common actions can be very powerful. For example, in games like CounterStrike, a jump-shoot combo can be a very effective action. The advantage of anticipatory actions in reinforcement learning, similar to higher n-grams in NLP, improves exploration and parameter sharing. Parameter sharing, as shown by BID3 in 1997, benefits neural network generalization by using a single representation for multiple tasks. The addition of meta-actions forces network layers to learn a more comprehensive representation. In reinforcement learning, the addition of meta-actions forces network layers to learn a more comprehensive representation, improving exploration and parameter sharing. The proposed Anticipatory Deep Q Network (ADQN) enhances the current A3C algorithm by incorporating a shared representation learned from basic actions and meta-actions. This additional constraint on the network helps regularize the representation, especially in the early stages. ADQN algorithm enhances DQN by incorporating meta-actions, generating two updates for each state and improving convergence through parameter sharing. This approach performs well in the CartPole game. The ADQN algorithm improves DQN by introducing meta-actions, leading to better performance in the CartPole game. It can also be applied to the A3C algorithm by expanding the number of policy nodes in the neural network without altering its architecture. The Anticipatory asynchronous advantage actor-critic (A4C) algorithm utilizes a neural network for prediction and training, allowing for the execution of single actions or action sequences. It is a generalization of A3C and supports three types of gradient updates: dependent updating (DU), independent updating (IU), and switching. A meta-action in A4C is a combination of single actions. The Anticipatory asynchronous advantage actor-critic (A4C) algorithm introduces dependent updating (DU) as a type of gradient update, where meta-actions have corresponding basic actions. This results in more aggressive updates and accelerated learning compared to A3C. The Anticipatory asynchronous advantage actor-critic (A4C) algorithm introduces dependent updating (DU) as a type of gradient update, resulting in aggressive updates and accelerated learning compared to A3C. This kind of dependent updating version of A4C is referred to as DU-A4C, with pseudocode presented in Algorithm 1. The Anticipatory asynchronous advantage actor-critic (A4C) algorithm introduces dependent updating (DU) as a type of gradient update, resulting in aggressive updates and accelerated learning compared to A3C. DU-A4C is a version of A4C that uses independent update, treating each meta-action as a separate action offered by the environment. The reward of a meta-action is the sum of rewards of taking all the basic actions in the sequence, and the next state is the state after taking all the actions in the sequence. The updating process only considers the reward and next state of the meta-action. The Anticipatory asynchronous advantage actor-critic (A4C) algorithm introduces dependent updating (DU) as a type of gradient update, resulting in aggressive updates and accelerated learning compared to A3C. Independent updating (IU) leads to less aggressive updates but shows superior performance in experiments due to the network's ability to explore high-reward action patterns. DU-A4C initially converges faster than A3C but saturates quickly with prolonged training, similar to Stochastic Gradient Descent (SGD) updates. The proposed switching method combines dependent updating (DU) and independent updating (IU) to leverage the advantages of both. Initially using DU for fast convergence, then switching to IU for sustained growth. This approach stabilizes training and is similar to decaying learning rate in Neural Network training. The proposed switching method combines dependent updating (DU) and independent updating (IU) to leverage advantages. Switching from DU to IU halfway through training stabilizes and improves performance. Experiments show robust results with different switching points. Study of exploration using CartPole game shows results comparing ADQN and DQN, with action distributions at different training stages analyzed. In the Cartpole game, a 2-step Anticipatory DQN with Dependent Updates is compared against regular DQN. The use of metaactions space {L, R, LL, LR, RL, RR} leads to a significant improvement in score. The probability distributions of metaactions show that as learning progresses, the agent shifts towards using basic actions more frequently. This trend indicates that multi-step actions aid in better exploration initially. The A4C experiments on Atari-2600 games show improved performance in reinforcement learning algorithms compared to the state-of-the-art GA3C framework from NVIDIA. The experiments were conducted using environments provided by OPENAI GYM, with results plotted and compared for robustness. The A4C experiments on Atari-2600 games showed better performance than the GA3C framework from NVIDIA. Various hyperparameters were tested, with the optimal setting being MinTrainingBatchSize=40, GradientClipping=False, LearningRate=0.003. Independent Updates(IU) outperformed GA3C in most cases except for Qbert. The comparison of A4C updates against GA3C for four games is shown in FIG0. The Independent Updates(IU) outperforms GA3C in most cases, achieving a score of 4300 on BeamRider in just 12.5 hrs compared to GA3C's 21 hrs. IU also scores >750 on SpaceInvaders, surpassing GA3C's <600. The Dependent Updates(DU) method rises faster than GA3C initially but doesn't sustain growth, except in Pong. The hybrid switching method(Sw) consistently performs well, achieving higher scores than GA3C on all games, such as 12000 on Qbert in 7 hrs. Switching from DU to IU after a few hours is the most robust method, while IU alone performs well on two games. A technique of adding anticipatory actions to the GA3C method for reinforcement learning leads to significant improvements in convergence and scores on Atari-2600 games. However, challenges remain in addressing the large action space that grows exponentially with the order of anticipation. Addressing the large action space that grows exponentially with the order of anticipation is a pressing concern for future work. Human behavior information is believed to help select the best higher order actions."
}