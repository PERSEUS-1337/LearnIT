{
    "title": "HJnQJXbC-",
    "content": "New types of compute hardware are revolutionizing deep learning, but existing software frameworks and training algorithms have not fully adapted. An asynchronous model-parallel training algorithm called AMP shows promise in accelerating models that rely on structured input and complex control flow. This algorithm converges to the same accuracy as synchronous training but utilizes hardware more efficiently, resulting in shorter training times. The development of dynamic neural network frameworks like Chainer, DyNet, and PyTorch highlights the challenge of scaling up deep learning models that react dynamically to input properties. These models, known as dynamic neural networks, defy the traditional GPU-driven minibatch processing paradigm and require new training approaches for efficient computation. The challenge of efficiently training dynamic neural network models without batching, exploring the idea of processing items individually for faster computation. This concept is inspired by recent work on specialized hardware for deep learning. The proposed asynchronous model-parallel (AMP) training algorithm addresses the constraint of limited memory on devices by allowing parameter updates without global synchronization. This approach increases device utilization and avoids the decreased parameter update frequency seen in synchronous updates. The asynchronous model-parallel (AMP) training algorithm allows for parameter updates without global synchronization, increasing device utilization and avoiding decreased update frequency. It introduces gradient staleness but still shows fast convergence with good hardware utilization. Contributions include presenting an IR with constructs for branching and joining control flow to support AMP training efficiently in dynamic networks. The IR introduced in the previous section enables AMP training on dynamic neural network models like Tree RNN and GGNN. The implementation on a multi-core CPU shows comparable accuracies to synchronous algorithms. The work highlights the benefits of AMP training and offers a new approach to designing neural network libraries with dynamic control flow. Additionally, performance estimates on a hypothetical device with 1TFLOPS compute capability are provided. The curr_chunk discusses the implementation of dynamic neural network models like Variable-length RNNs and Tree-structured neural networks. It mentions the use of different units like gated recurrent units and the challenges of variable sequence lengths. Tree-structured neural networks are highlighted for parsing natural language and sentiment analysis. The curr_chunk discusses tree neural networks for semantic representation and sentiment analysis, utilizing specialized units for traversal and backpropagation through structure. GNNs combine temporal and structural recurrence, performing operations over general graph structures with shared parameters. Various models with flexible control flow are also mentioned. The curr_chunk introduces a framework for AMP training, where each node in a computation graph is associated with a worker for neural network training and inference through message passing. The state of each message includes source and sink node IDs, payload, and control flow information. The state in a variable-length RNN includes instance identifier, current position, and sequence length. Messages in nested loops contain loop counters and instance id. Workers perform operations on messages and enqueue outgoing messages. Loss layer initiates backward propagation and workers carry out weight updates on sink nodes using gradients. The workers perform weight updates on the sink node using accumulated gradients. Communication between workers involves sending activations and gradients, while weights are stored locally. Messages can arrive asynchronously, requiring caching until all parent nodes send their payloads. In the backward pass, the cache distinguishes payloads from different parents and instances. The adjoint operation reverses changes made in the forward pass. In the backward pass, the adjoint operation reverses changes made in the forward pass. Two hyper parameters control the effect of asynchrony: min update interval and max active keys. The min update interval determines the minimum number of gradients needed before updating parameters, affecting gradient staleness. The max active keys control the maximum number of active instances in-flight, impacting hardware utilization and convergence speed. In the context of controlling asynchrony in training, the optimal assignment of neural network computation graph nodes to workers is a non-trivial scheduling problem. Various heuristics were explored, but a simple procedure of balancing heavy nodes across workers by affinitizing them achieves high performance in practice. This approach involves partitioning nodes into heavy and light operations, then assigning heavy nodes to workers based on a specific pattern. Computation graphs are expressed using a static intermediate representation (IR) that can be a compilation target for high-level libraries. The IR graph is instance-independent but can execute dynamic control flow decisions. Each IR node has forward and backward semantics, and a model is specified by an IR graph. The model is specified by an IR graph with forward and backward semantics. Payload transformations like Parameterized payload transform (PPT) nodes encode fully connected layers, recording activations for gradient computation in the backward pass. PPT nodes process forward and backward messages asynchronously and out of order. The IR graph specifies the model with forward and backward transformations. It includes nodes for payload transform, loops, state, and control flow. An RNN is encoded using invertible state update nodes. The IR graph specifies the model with forward and backward transformations, including nodes for payload transform, loops, state, and control flow. An RNN is implemented with Cond, Phi, and Isu nodes for the loop at its core. The controller generates sequences in a lookup table, and the Ungroup node produces tensors for each token with a time-step tag. Isu increments the time-step in forward mode, while in backward mode, it decrements the time-step. The loop executes in both forward and backward modes. The IR graph specifies the model with forward and backward transformations, including nodes for payload transform, loops, state, and control flow. The RNN loop involves constructs for aggregation and disaggregation, such as Concat, Bcast, Ungroup, and Replicate. These constructs allow for manipulating hidden states, embeddings, and gradients in both forward and backward modes. The GNN combines aggregation on the graph structure with an outer loop. The controller pumps data containing node embeddings and graph topology. Distribute node creates sub-matrices for edge types and passes them through linear layers. Collect node re-groups results based on graph topology. In backward mode, gradients are re-grouped and summed based on control information. Collect operator is symmetric. The Collect operator in the pipelined model parallelism can be augmented with data parallelism. One approach is to split the linear layer into smaller tiles and compute them in parallel. Another approach is to replicate the linear layer and place the replicas inside Cond and Phi nodes for processing messages in a pipeline-parallel fashion. AMPNet utilizes infrequent global synchronization for fast convergence among replicas, enabling parameter sharing. The asynchronous runtime is designed for dynamic neural network execution, leveraging emerging hardware like FPGA accelerators. The models and datasets used in experiments are briefly described, with further details in Appendix B. The performance of AMP runtime is evaluated by increasing asynchrony levels while using a multi-core CPU runtime. Comparison with existing frameworks like TensorFlow is also conducted to assess its efficiency. AMPNet utilizes global synchronization for fast convergence and is designed for dynamic neural network execution on FPGA accelerators. The AMP runtime is compared with existing frameworks like TensorFlow to show competitive performance even on CPUs that do not match the target hardware model. On MNIST, there is a 3x speedup from synchrony to asynchrony in terms of throughput. AMP training achieves a 3x throughput gain compared to pipeline parallel training. The AMP runtime demonstrates higher throughput with max active keys = 8, but requires more epochs to converge. Replicas show speedup in certain datasets, with 2 and 4 replicas achieving 2.5x and 3.5x speedup respectively. The sentiment tree-RNN dataset shows competitive runtime without batching compared to TensorFlow Fold BID21. The runtime allows for specifying different min update interval parameters for each operation. The min update interval parameter is set differently for each operation, with a value of 1000 for the embedding layer and 50 for other layers. This reduces gradient staleness in the embedding layer. Asynchrony is shown to improve performance on real-world tasks with complex control flow, outperforming TensorFlow on CPUs. The BiLSTM w/ char model in BID25 on Wikiner dataset achieves competitive performance without batching. Additional analysis on the effect of asynchrony is provided. Increasing the max active keys (asynchrony) in training networks can improve performance up to a certain point, beyond which there are diminishing returns. Dynamic computation graph per-instance is a method used in Chainer, DyNet, and PyTorch to handle instance-dependent control flow challenges. The challenges in accelerating the approach include model parallelism and BLAS level parallelism, with automatic dynamic batching implemented in DyNet BID24. This method inspects and merges unrolled computation graphs to create batched BLAS operations, but its effectiveness depends on the model. In contrast, the proposed IR can achieve pipeline parallelism using a static computation graph that is easy to distribute and optimize. The proposed IR achieves pipeline parallelism using a static computation graph that is easy to distribute and optimize. Unlike Theano and TensorFlow, AMPNet is streaming and asynchronous, with implicit queuing and stream-based execution as the default. TF can support streaming programmatically and static description of dynamic control flow, but AMPNet departs from the classic dataflow architecture. The proposed IR in AMPNet encapsulates algorithmic state in messages, eliminating the need for control dependencies. This allows nodes to run asynchronously and independently without a scheduler, simplifying algorithmic state management. The proposed IR in AMPNet encapsulates algorithmic state in messages, allowing nodes to run asynchronously without a scheduler. Data parallel training is a popular approach to scale out optimization by removing synchronization and improving convergence. The curr_chunk discusses the optimization dynamics of BID28 and BID22 in data parallelism, aiming to extend these results to a similar setting. It introduces BID16's approach of training different model parts asynchronously using synthetic gradients from local neural networks. This method allows for independent local gradient calculations and asynchronous parameter updates, especially beneficial when local network evaluation is more cost-effective than computing real gradients. The chunk also presents an asynchronous model-parallel SGD algorithm for distributed neural network training. The curr_chunk discusses implementing an AMPNet runtime for multi-core CPUs to enable scalable distributed training of dynamic models. The goal is to deploy the system on specialized hardware and develop a compiler for automatic information deduction and state keying function generation. This aims to explore models currently on the horizon that may become mainstream in the future. The curr_chunk explains the communication process in a distributed environment using message passing and prioritization for faster backpropagation. It involves offloading messages from a queue to a priority queue on each worker, allowing for efficient processing of IR nodes. The methods of updating parameters using gradients are configurable options within the system. The curr_chunk discusses the implementation of runtime configuration options for selecting optimization algorithms and controlling training hyper-parameters. Experiments were conducted on machines with specific specifications, and training and validation throughputs were compared between AMPNet and TensorFlow baselines using a 4-layer perceptron on MNIST dataset. The curr_chunk discusses training a 4-layer perceptron on MNIST dataset using AMPNet and TensorFlow baselines. It compares validation accuracy, throughputs, and the impact of synchronous vs. asynchronous training. The scheduler's role in gradient staleness is highlighted, suggesting the need for better scheduling. The curr_chunk discusses training a vanilla RNN on variable length lists of digits for reduction operations. The dataset includes training and validation instances, presented as a classification problem. The RNN has ReLU activation and a hidden dimension of 128. Training instances are batched into sequences, and validation accuracy vs. time is compared for different methods on the list reduction dataset. Increasing asynchrony from synchronous is noted. Increasing the number of replicas in training a vanilla RNN on variable length lists of digits for reduction operations significantly boosts throughput, with almost linear improvement from 13k sequences/s (synchronous) to over 70k sequences/s (4 replicas). Convergence is minimally affected, even with max active keys = 16, indicating that the slowdown observed with 4 replicas is not due to asynchrony. Synchronizing parameters more frequently did not yield significant improvements. The slow-down in training with more replicas is not due to asynchrony but rather the increase in effective minibatch size, leading to fewer updates per epoch. The sentiment classification dataset used consists of binarized constituency parse trees with sentiment labels. A Tree LSTM model is employed for classification, trained with architectural modifications proposed by BID21 and BID32. The model is split into Leaf LSTM and Branch LSTM cells without affecting its expressiveness. The LSTM cell in the model is not affected by the split into Leaf LSTM and Branch LSTM cells, as they do not share weights except for bias parameters. A comparison is made on the time to reach 82% accuracy on the validation set, showing that AMPNet converges faster than TensorFlow Fold. This speedup is attributed to not batching and updating every 50 gradients, with 50 gradients corresponding to roughly 2 trees. The lower throughput compared to TensorFlow Fold is due to this batching strategy. The lower throughput compared to TensorFlow Fold is due to only grouping leaf operations, not branch operations. Extending IR nodes to group branch operations is being worked on. Increasing min update interval shifts validation accuracy peak later and lower, closer to TensorFlow Fold's curve. Min update interval has minimal impact on training throughput. GNNs are used for predicting organic molecule properties in the QM9 dataset. The text discusses the application of GNNs in predicting the norm of a molecule's dipole moment using a regression layer. The model uses a hidden dimension of 100, 4 propagation steps, and batches molecules into groups of 20. Results show that GGNN can handle large max active keys and increase throughput significantly. Performance comparison is made with AMP training using DyNet with and without autobatching. The model utilizes a BiLSTM tagger with a char model from the DyNet benchmark suite for named entity recognition on the Wikiner dataset. Throughput increases significantly from max active keys = 1 to max active keys = 32 without impacting validation accuracy, except for a slight decrease due to overfitting after the third epoch. The performance of AMPNet on a hypothetical device with 1 TFLOPS compute capability was estimated by replacing fully connected layers with a dummy operation. The dummy operation waits for specified times based on input and weight matrix dimensions. This allows for maintaining data-dependent control decisions while measuring real-time operations. The time to reach target accuracy is calculated based on the median number of epochs needed by the original network. The performance of different models was evaluated on a hypothetical device with 1 TFLOPS compute capability. The time to reach target accuracy was calculated based on the number of training and validation instances, and throughput for each model. The 4-way replicated RNN showed a 3.7x speedup compared to CPU runtime, while tree RNN and GGSNN had milder speedups of 30-70% due to more complicated operations."
}