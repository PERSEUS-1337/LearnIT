{
    "title": "HJe5_6VKwS",
    "content": "Adversarial perturbations can cause a shift in salient features of an image, leading to misclassification. Gradient-based saliency approaches fail to capture this shift, prompting the development of a new defense using learned saliency models. Two approaches were studied: a CNN trained to distinguish natural and adversarial images using saliency masks, and a CNN trained on salient pixels directly. These defenses successfully detect various adversarial attacks, including strong ones like C&W and DeepFool, outperforming gradient-based methods. The salient pixel-based detector proves more robust than saliency map-based detectors. The salient pixel-based detector is more robust to white-box attacks compared to saliency map-based detectors. Adversarial examples exploit the difference between human vision and computer image processing, causing misclassification by altering pixels irrelevant to humans. Saliency maps help understand why classifiers are fooled by identifying the pixels used for predictions. Researchers have shown that adversarial perturbations shift the saliency of classifiers. Researchers have qualitatively shown that adversarial perturbations cause a shift in classifier saliency. Adversarial images exhibit perceptible differences in saliency maps compared to original images, with the classifier focusing on irrelevant aspects. Various techniques exist for finding saliency maps. Various techniques exist for finding saliency maps, with some being more informative than others. The Jacobian can be used to determine pixel saliency in image classification and detect adversarial perturbations. Zhang et al. (2018) propose a defense using Jacobian-based methods to identify adversarial inputs. The saliency maps of original and adversarial images show stark differences, with the classifier focusing on different aspects. The saliency maps of original and adversarial images exhibit significant differences. Gradients may not always capture these distinctions, but saliency remains an effective tool for detecting adversarial images. Model-based saliency techniques show that the shift in saliency maps due to adversarial perturbations can exceed the L2 distance between saliency maps of different natural images. In this work, the authors investigate the effects of adversarial perturbations on classifiers using two CNN architectures. They demonstrate the defensive capability of their model-based saliency against various attacks and show that incorporating pixel values improves classifier performance in detecting adversarial perturbations. The salient-pixel based defense is shown to generalize well and be more robust than saliency map defense against white-box attacks. Saliency can detect adversarial examples generated by small perturbations, unlike other defenses that exhibit threshold behavior. Saliency and adversarial perturbations have similar mathematical formulations, both computed by investigating the relation between pixel values and classification scores. The effectiveness of saliency maps in detecting adversarial perturbations generated using BIM (Kurakin et al., 2016) is discussed. Different techniques for generating saliency maps vary in their ability to capture changes due to adversarial perturbations. These attacks create smaller L2 perturbations, making them harder to detect. Saliency maps can identify the smallest region of an image necessary for correct classification (SSR) or the region whose removal leads to incorrect classification (SDR). The concept of saliency maps and adversarial perturbations in neural networks is closely related. Adversarial attacks involve adding perturbations to an image to cause misclassification, requiring non-linear optimization due to the complexity of neural networks. Different approaches, such as setting perturbations based on the sign of the gradient, have been proposed. Similarly, saliency maps can be computed using forward derivatives to identify regions crucial for correct classification. Researchers have explored using saliency to create adversarial attacks and defenses in neural networks. Adversarial attacks leverage saliency to identify sensitive pixels for perturbation, requiring minimal changes to alter image classification. Defenses involve detecting adversarial perturbations using heat-map visualizations, with some methods hypothesizing a mismatch between saliency and adversarial examples. Researchers have proposed a defense against adversarial attacks by training a classifier on images concatenated with their saliency map, computed using the Jacobian of the classifier. Different studies have conflicting results on the effectiveness of this method in detecting adversarial images. Saliency maps can be used to explain adversary classifications, showing that not all techniques used to compute saliency maps exhibit shifts in saliency due to adversarial perturbations. In this section, the construction and evaluation of saliency-based adversarial example detectors are explained. A convolutional neural network image classifier is trained and targeted with black-box attacks. The use of cross-entropy loss and optimization with Adam is detailed, along with the implementation from cleverhans. Hyper-parameters are summarized in Appendix B for generating saliency masks. The saliency masks are generated using a method adapted from Dabkowski & Gal (2017), focusing on high-quality masks at low computational cost. The loss function targets specific criteria like SDR, SSR, and mask sparsity. The saliency loss function ensures small map area, correct class identification, and classifier inability to recognize the class without the saliency map. We adapt the PyTorch implementation by Dabkowski & Gal (2017) to train a saliency model on standard images. Saliency maps are generated for both natural and adversarial images using predicted classifications to prevent information leaks. Our hypothesis is that adversarial images may mislead the classifier. Two classifiers are built for saliency maps and salient pixels with the same architecture as black-box image classifiers. We build a detector based on saliency maps of images by training a classifier and generating adversarial images. A binary detector predicts if an image is adversarial or natural. We also construct a classifier for salient pixels using element-wise product of saliency maps and images. The defense method SPD (Salient Pixel Defense) is compared with baseline classifiers ID (Image Defense) and JSD (Jacobian-based Saliency map Defense). The saliency map is adapted to avoid underflow by taking the derivative with respect to the logits. The defense method JSD (Jacobian-based Saliency map Defense) is more general compared to ID and similar to SMD. It differs from SPD as CNN filters cannot multiply channels. The evaluation protocol involves training each defense to detect adversarial images generated by specific attacks, resulting in six different detection models. Training data includes both clean and adversarial images. The study compares the performance of models on various datasets including MNIST, CIFAR-10, and ASSIRA cats and dogs. Many defenses are effective against black-box attacks but struggle with white-box attacks. White-box attacks tailored to defense strategies are generated for evaluation. The study evaluates white-box attacks tailored to defense strategies, using iterative gradient-based attacks targeting both the classifier and the defense. The attacks iterate between Equations 3 and 4 to generate strong attacks for various defenses, limiting iterations to 5 for optimal performance. This method differs from previous approaches by iterating between equations rather than applying them simultaneously, proving to be more effective. The study evaluates white-box attacks tailored to defense strategies, using iterative gradient-based attacks targeting both the classifier and the defense. The attacks iterate between Equations 3 and 4 to generate strong attacks for various defenses, limiting iterations to 5 for optimal performance. This method differs from previous approaches by iterating between equations rather than applying them simultaneously, proving to be more effective. The shift in saliency maps due to adversarial perturbations is quantified, with the L2 distance between saliency maps of natural and adversarial images compared. Results show effectiveness against different adversarial attacks, with details in Appendix B. The study quantitatively proves the significant shift in saliency maps due to adversarial attacks on CIFAR-10 and ASSIRA images using a Mann-Whitney U-test. The saliency-based method effectively captures adversarial perturbations, as shown in Table 1 with L2 distance comparisons. Nonparametric tests were used to avoid assuming normality of the data. Defense models trained on a single adversarial attack were evaluated for performance against different attacks, as summarized in Figure 2. The study demonstrates the effectiveness of saliency-based defense models in detecting adversarial attacks. Salient Pixel Defense outperforms other defenses, showing good generalization even against stronger attacks like JSMA. The study shows that saliency-based defense models are effective in detecting adversarial attacks, with Salient Pixel Defense outperforming other defenses. Different types of attacks, such as L 0 -norm, L \u221e -norm, and L 2 -norm, generate varying perturbations on images. Both ID and JSD exhibit threshold behavior in detecting adversarial examples based on perturbation size. The study found that ID and JSD exhibit threshold behavior in detecting adversarial examples based on perturbation size. Gong et al. (2017) observed that ID efficiently detects adversarial images with perturbations of \u03b5 \u2265 0.03 but struggles with \u03b5 = 0.01. JSD performs similarly, with the parameter space of ID being a subset of JSD's parameters. The additional input in JSD (the Jacobian) makes it harder to train, leading to differences in results. The study observed that the increased difficulty in training the model with more parameters results in the model not learning to ignore additional input. FGSM and JSMA attacks generate large perturbations, with JSMA perturbing as few pixels as possible but by a large amount. The white-box attack is effective against MNIST and ASSIRA but not CIFAR-10. Adversarial training is then performed to train the detectors against the white-box attack. The study shows that the white-box attack is no longer successful against SPD, making it more robust, while SMD remains vulnerable. Saliency maps of adversarial images differ from natural images, with pixel-based defenses outperforming saliency map defenses. A new defense method can detect adversarial perturbations from various attacks, including black-box attacks. Gradients are ineffective in capturing saliency shifts, leading to the development of a defense using learned saliency models effective against both black-box and white-box attacks. The study introduces a defense method against adversarial attacks, showing robustness against white-box attacks but vulnerability to SMD. The method detects perturbations across different attacks and can be applied in real-time for video analysis. The architecture and hyper-parameters of the image classifier are detailed in Figure 3. The image classifier model is trained with a drop-out rate of 0.6 and ReLu activation for penultimate layers. It is trained for 10 epochs on batches of size 50 and tested on MNIST, CIFAR-10, and ASSIRA datasets. Further experimentation is done on the ID and JSD architectures to assess performance. The study evaluated the performance of different adversarial attacks on the image classifier model trained on MNIST and CIFAR-10 datasets. Various hyperparameters were adjusted to ensure strong attacks, but the changes did not improve performance. The results include success rates of attacks and examples of adversarial images generated. The study evaluated adversarial attacks on an image classifier trained on MNIST and CIFAR-10 datasets. Adversarial perturbations are visible in gray-scale low-resolution images. The classifier incorrectly predicts images as 4, 9, 9, 8, 9, 9. White-box attacks for JSD and defenses are provided in Algorithms 1, 2, and 3. The white-box attacks for the defenses SPD and SMD involve generating saliency maps and manipulating the input image to fool the classifier and detector. The algorithm iterates through different steps to create adversarial images. The saliency maps of adversarial images generated by different attacks show no perceptible differences, with gradients being zero-valued for some methods. When training a single detector against various black-box attacks, all defenses perform similarly, slightly worse than against specific attacks. When training a single detector against various black-box attacks, all defenses perform similarly, slightly worse than against specific attacks. This is useful in practice when it is unclear which adversarial attack is used."
}