{
    "title": "rJgfjjC9Ym",
    "content": "Innovations in architecture design have led to deeper and wider neural network models that show improved performance on various tasks. However, the increased memory footprint of these models poses a challenge during training, as all intermediate layer activations need to be stored for back-propagation. To address this issue, a new approximation strategy is introduced to significantly reduce a network's memory usage without affecting training performance or computational expense. This approach involves replacing activations with lower-precision approximations during the forward pass, freeing up memory for use during the backward pass. Using lower-precision approximations of activations during the forward pass reduces memory usage without impacting training performance. Experiments on CIFAR and ImageNet show that 8- and 4-bit fixed-point approximations of 32-bit floating-point activations have minimal effect on performance. Deeper neural networks with residual and skip connections can effectively leverage this approach for visual inference tasks. The shift to larger network architectures for visual tasks has improved performance but increased demand on computational resources. Deeper architectures require more memory during training to retain intermediate layer activations for gradient computation. This leads to inefficiencies with smaller batch sizes and complicates batch normalization. The use of batch-normalization BID11 can be challenging due to the computation of batch statistics over fewer samples, affecting training stability. Prior work focused on exact gradient computation, but we propose an algorithm that computes reasonably approximate gradients to reduce a network's memory footprint without additional computational cost, motivated by distributed training algorithms. Proposed Approach involves using low-precision approximations of activations to reduce memory usage during network training. This method limits errors in gradient flow and preserves the signs of activations, ensuring most computations are exact during back-propagation. In this work, a new backprop implementation is proposed that uses low-precision approximate activations to reduce memory usage during training. The method limits approximation errors during backpropagation while preserving the signs of activations for more exact computations. During the backward pass, low-precision approximations of activations are used to reduce memory usage in training. Gradients are computed with full precision but using the approximate activations, minimizing errors in backpropagation. Experimental results show minor degradation in training quality with 4-bit fixed-point approximations for 32-bit floating-point activations. This approach significantly reduces memory requirements for training. Our memory-efficient version of backprop reduces memory usage during training by using low-precision activations, allowing for larger batch sizes and exploration of larger architectures. This contrasts with methods focusing on reducing model memory during inference, which still require storing full versions of weights and activations during training. During training, memory can be a bottleneck due to the need to store all intermediate activations, especially with larger network architectures. Using multiple GPUs is inefficient, leading to under-utilization of parallelism. Checkpointing, storing activations for only a subset of layers at a time, reduces memory requirements but increases computational cost. Proposing strategies for efficient training with reduced memory requirements and computational cost, researchers have explored methods like using reversible layers and approximations of gradients to improve efficiency without compromising training ability. Asynchronous methods for distributed training delay model synchronization to enhance performance. Our work focuses on reducing memory usage during training by approximating activations with lower-precision representations. This strategy results in significant reductions in a model's memory footprint without compromising training performance. This approach can also be applied within checkpointing to further improve memory cost efficiency. Our approach focuses on reducing memory usage during training by approximating activations with lower-precision representations. This allows for a significant reduction in memory footprint without compromising training performance. The strategy limits error accumulation across layers, enabling the replacement of 32-bit floats with 8-and 4-bit fixed-point approximations with minimal impact on performance. Our approach aims to reduce memory usage during training by approximating activations with lower-precision representations, allowing for a significant reduction in memory footprint without compromising performance. The strategy involves compressing intermediate activations to a greater degree and using 16-bit precision for computation in neural networks, which consist of linear and non-linear functions organized into layers. Each layer applies batch-normalization and ReLU activation to its input followed by a linear transform to yield output activations fed into subsequent layers. The output activations A l:o are tensors with dimensions indexing training examples, channels, and spatial location. Mean(\u00b7) and Var(\u00b7) aggregate statistics to yield vectors \u00b5 l and \u03c3 2 l. Element-wise addition and multiplication are carried out by \"broadcasting\". The linear transformation is represented by \u00d7 denoting matrix multiplication or convolution. Parameters \u03b3 l , \u03b2 l , and W l are defined with respect to learnable parameters for fully connected layers or convolution kernels. The parameters for fully connected layers or convolution kernels are learned iteratively using SGD. Gradients are computed with respect to these parameters based on a loss function, and back-propagated through the layers using the chain rule. During training, gradients are computed with respect to the layer's parameters and input for further propagation. Intermediate activations need to be stored for back-propagation, requiring sufficient memory to hold all layer activations in the network. During training, gradients are computed with respect to the layer's parameters and input for further propagation. To conserve memory, it is proposed to store an approximate low-precision version of the activation values right before the ReLU, along with the variance vector. This approach reduces the memory required for storing all intermediate activations in the network. During training, to conserve memory, an approximate low-precision version of activation values is stored before the ReLU, along with the variance vector. This reduces the memory needed for storing all intermediate activations in the network. The full-precision versions are used during the forward pass, while only the approximate activations and variance vector are retained for back-propagation. During training, an approximate low-precision version of activation values is stored before the ReLU, along with the variance vector, to conserve memory. The integers from the normalized A l:2 are computed and stored with K-bits for efficiency. The approximation ensures that the sign of each value is preserved, reducing memory usage without introducing errors in the activations. The error introduced by using approximate low-precision activation values is analyzed by considering the effect on gradients. The approximation error is bounded by half the width of quantization intervals and is lower for higher values of K. The relationship between the approximated values A l:2, A l:3, and A l:1 is discussed, showing that the error decreases due to clipping negative values. The error in activations affects gradient computations during back-propagation. Approximation errors in activations do not affect gradient computations for certain layers due to their independence from activations. Only the gradient to the scale parameter depends on activations during back-propagation through scale and bias. The approximation errors in activations do not affect gradient computations for certain layers due to their independence from activations. While there may be some errors in the gradients for learnable weights, the majority of computations for backpropagation to the input of each layer are exact. This is illustrated in Fig. 1, with green arrows showing exact computations and red arrows indicating those affected by the approximation. Our training algorithm applies approximation strategy to every layer during forward and backward pass, handling skip and residual connections easily. The method relies on ReLU activations for gradient computation, which can also be used for other non-linearities like \"leaky\"-ReLUs. Our approach handles various non-linearities like \"leaky\"-ReLUs and avoids approximating activations in the final output layer for classifier networks going through Soft-Max. It simplifies average pooling by incorporating it with the linear transform and does not approximate max-pooling due to the decreasing use of max-pool layers in recent architectures. Our approach simplifies memory usage in deep neural networks by storing approximate activations for each layer at a reduced memory rate. The memory usage depends on the connectivity of the layers, and we need to store activations for up to W layers during the forward pass. The same amount of space is required for storing gradients during back-propagation until they are used by previous layers. Our approach simplifies memory usage in deep neural networks by storing approximate activations for each layer at a reduced memory rate. The algorithm requires O(W +1+\u03b1L) memory, compared to the standard requirement of O(L), leading to substantial savings for deep networks with large L. A library has been developed to implement the method for approximate memory-efficient training, including residual layers. Global buffers are allocated for direct and residual paths, holding full-precision activations needed for computation of subsequent layers during the forward pass. The library developed for approximate memory-efficient training stores low-precision activations for each layer, while global buffers hold full-precision activations for computation. Comparisons are made with 8-and 4-bit activations against exact training with full-precision activations as a baseline on CIFAR-10 and CIFAR-100 datasets. The ResNet-164 models are trained using three-layer \"bottleneck\" residual units with parameter-free shortcuts. The network is trained for 64k iterations with specific batch size, momentum, and weight decay settings. Data augmentation techniques are applied during training. The performance of approximate training closely matches that of the exact baseline. Visualizations show errors in computed gradients of learnable parameters for different layers during training. The ResNet-164 models are trained using three-layer \"bottleneck\" residual units with parameter-free shortcuts. The network is trained for 64k iterations with specific batch size, momentum, and weight decay settings. Data augmentation techniques are applied during training. The performance of approximate training closely matches that of the exact baseline. Visualizations show errors in computed gradients of learnable parameters for different layers during training. Errors between true gradients and computed approximations are compared to SGD errors, showing significant differences in variance. Accuracy comparisons on CIFAR and ImageNet are reported for different models trained with varying random seeds. The final test errors of models trained with low-memory approximation closely follow those of exact back-propagation. Even with 4-bit approximations, the median test errors are only 0.07% higher compared to exact computations. Visualizations also show the error in final parameter gradients, demonstrating the robustness of the training process. The study compares the approximation error in gradient calculations to the noise in stochastic gradient descent (SGD) during training. The approximation error is consistently lower than the SGD noise for all layers in the models trained for ImageNet. Despite the error from approximation, it is overshadowed by the higher error from SGD, limiting further degradation in performance. The study evaluates the approximation error in gradient calculations compared to the noise in stochastic gradient descent during training. The models are trained with a 152-layer architecture using pre-activation parameter-free shortcuts. Training includes standard data augmentation techniques and a batch size of 256 for 640k iterations. The top-5 validation accuracy for models trained with exact computation and 8-bit/4-bit approximations shows a small drop in accuracy. Memory and computational efficiency are also discussed, with the method fitting a full 128-size batch on a single GPU for CIFAR experiments. In experiments with different batch sizes, our method fit half a batch (size 128) on each GPU for ResNet-152, while the baseline required two passes with 64-sized batches per-GPU per-pass. The running times per iteration were almost identical for CIFAR experiments and ImageNet with ResNet-34, with a slight increase in our case due to the cost of computing approximations. However, for ResNet-152 on ImageNet, the 64-sized batch for exact training underutilized parallelism, resulting in a longer time per iteration compared to approximate (4-bit) training. Our method allows significantly larger batches to be fit in memory compared to the baseline, resulting in improved computational efficiency. The time per-iteration for exact vs approximate training was 2s vs 1.7s across two GPUs. We evaluated memory usage and computational efficiency using residual networks for CIFAR-10 of various depths up to 1001 layers, including a version with four times as many feature channels. The results are summarized in TAB0. Our method allows larger batches to be fit in memory, providing a computational advantage for larger networks by fully utilizing GPU cores. The new algorithm for approximate gradient computation reduces on-device memory requirements without compromising model quality or computational expense. This enables training with larger batches, improving efficiency and stability, and allows exploration of deeper architectures previously impractical to train. The method demonstrates that SGD is robust with approximate activations, with a simple approximation strategy used. The text chunk discusses the use of approximate activations in training algorithms, exploring more sophisticated techniques for better trade-offs. The approach to partial approximation is considered for reducing inter-device communication in distributed training. The approximate training algorithm was implemented using TensorFlow, focusing on individual forward and gradient computations without automatic differentiation functionality. The text chunk discusses the implementation of TensorFlow ops for forward and backward passes through each layer, utilizing custom ops in CUDA for low-precision representations. Data persistence between calls is managed through Tensorflow variables, avoiding memory fragmentation. Sequential ops are called for forward and backward passes, followed by updating model parameters based on computed gradients. Common variables are used as buffers for all layers to prevent memory allocation issues. In Sec. 4, common variables are used as buffers for all layers to store activations and gradients in the network. The buffer size is determined by the largest layer, with slices used for smaller layers. Old data is overwritten with new data to reuse the buffers efficiently."
}