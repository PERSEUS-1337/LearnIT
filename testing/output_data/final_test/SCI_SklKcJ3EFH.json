{
    "title": "SklKcJ3EFH",
    "content": "Newtonian Monte Carlo (NMC) improves Markov Chain Monte Carlo (MCMC) convergence by analyzing gradients of the target density to determine a suitable proposal density. Existing methods struggle with step size determination, but NMC uses second order gradients to automatically scale step sizes. The goal is to find a parameterized density that matches the local curvature of the target density, which is then used as a Metropolis-Hastings proposal. Newtonian Monte Carlo (NMC) improves Markov Chain Monte Carlo (MCMC) convergence by directly matching constrained random variables to a proposal density with the same support, preserving the curvature of the target density intact. NMC demonstrates efficiency in various domains, recovering posterior easily for conjugate models. It outperforms other methods for large non-conjugate models. Markov Chain Monte Carlo (MCMC) methods are commonly used to generate samples from unnormalized probability densities in Bayesian inference. The Random Walk MCMC (RWM) method, proposed by Metropolis et al. (1953), uses a Gaussian proposal distribution centered at the current state with a specified step size. This method is effective when the target density is differentiable. The Metropolis Adjusted Langevin Algorithm (MALA) and Hamiltonian Monte Carlo (HMC) are advanced MCMC methods that improve upon the basic Random Walk MCMC (RWM) by utilizing gradients and Hamiltonian dynamics. MALA reduces convergence steps to O(n 1/3) from O(n) for RWM, while HMC uses a dynamic number of steps learned by the No-U-Turn Sampler (NUTS) algorithm. Selecting the appropriate algorithm remains an open problem in MCMC methods. In RWM, MALA, and HMC algorithms, selecting the optimal step size is a challenge. The step size is typically learned adaptively to achieve a desired acceptance rate, leading to issues with accommodating dimensions with different variances. Girolami and Calderhead (2011) suggest using Reimann manifold versions of MALA (MMALA) and HMC (RMHMC) to address differential scaling in each dimension. These methods involve using a Reimann manifold with a metric tensor based on the expected Fisher information matrix and the negative Hessian of the log-prior. However, analytic knowledge of the Fisher information is required for this approach. The method involves using the negative Hessian of the log-probability as the metric tensor, but it may not be positive definite. An alternate approach is to use a preconditioning matrix in MALA, HMC, and NUTS algorithms, but the computation of this matrix is unclear. Another approach is to approximately compute the Hessian using ideas from optimization methods like L-BFGS. The key observation in our work is that for single-site methods, we only need to compute the Hessian of one coordinate at a time, which is usually tractable. The Hessian can be used to match the curvature of any parameterized density that best approximates the conditional posterior, allowing us to deal with constrained random variables without introducing a transformation. This paper introduces the Newtonian Monte Carlo (NMC) technique for sampling from a target distribution using a proposal distribution that incorporates curvature around the current sample location. The method involves matching the target density to a parameteric density that best explains the current state. The NMC technique matches the target density to a parameteric density using inference rules to determine \u03b1 and \u03b2. It utilizes multivariate Gaussian or Cauchy distributions for \u03b8 \u2208 R k, with the update term resembling the Newton-Raphson Method. Negative eigenvalues in \u03a3 are adjusted, and various estimation methods are listed in the appendix. A Gamma proposer is used for positive real values without the need for log-transform. Our approach utilizes a Gamma proposer without the need for log-transform. We rely on Tensor libraries like PyTorch for easy model writing and gradient computation. The method does not require computing the Hessian of all latent parameters, as most models can be decomposed into multiple variables, allowing for single site MCMC methods. Our approach uses a single site approach to efficiently compute gradients and Hessians for each variable in a model with N variables drawn from R K. This method reduces the computational cost compared to computing the full Hessian. Our estimation methods automatically recover the appropriate conditional posterior distribution, even in cases of non-conjugacy. Experimental results are presented for Bayesian Logistic Regression, Robust Regression, and a Crowd-Sourced Annotation Model. In experiments with Bayesian Logistic Regression, Robust Regression, and a Crowd-Sourced Annotation Model, various Probabilistic Programming Languages (PPLs) were compared for convergence speed. Pyro and Stan showed fast convergence to the true posterior, but were slow overall. NMC was significantly faster than Stan, making it suitable for larger datasets. JAGS was also mentioned in the context of Robust Regression. The Crowd-Sourced Annotation Model, a hierarchical statistical model, had a log-concave posterior. JAGS and Stan had convergence issues, with Stan taking twice as long as JAGS. NMC performed well compared to JAGS and was much faster than Stan on larger datasets. A novel MCMC method was introduced for faster convergence without adaptive tuning. The multivariate Cauchy distribution and the Gamma distribution have specific log-densities. Estimation rules are derived from these distributions, including for the Dirichlet distribution. Robust estimation rules are also discussed in the context of these distributions. The robust estimation rule in the context of multivariate Cauchy, Gamma, and Dirichlet distributions involves N items, K labelers, and C categories. Each item is labeled by a set of labelers, with true labels and confusion matrices determining label probabilities. Parameters \u03b1 and \u03b3 are set to specific values for experimentation."
}