{
    "title": "rkr1UDeC-",
    "content": "In this paper, a variant of distillation is explored, which allows for faster training on large datasets without the need for complex setups or additional hyperparameters. Online distillation enables parallelism to speed up training even after reaching a point where it no longer benefits synchronous or asynchronous gradient descent. Two neural networks trained on different data subsets can share knowledge by aligning their predictions. Online distillation allows two neural networks to share knowledge by aligning their predictions, encouraging agreement between models. This method is cost-effective and improves the reproducibility of model predictions. Experiments on various datasets support these claims, showing potential for faster training and improved model quality in large-scale neural net training problems. Descent algorithms like synchronous and asynchronous SGD are commonly used for large-scale neural network training across multiple machines. However, scalability is limited by infrastructure constraints and optimization barriers, leading to diminishing returns with increasing number of machines. Communication overhead and latency issues slow down execution, causing challenges in distributed minibatch SGD. Gradient interference in asynchronous algorithms can result in thrashing and hinder learning progress. Scaling distributed SGD for training neural networks can be challenging due to infrastructure limitations and optimization barriers. It is difficult to effectively scale beyond a hundred GPU workers in realistic setups. Once the scalability limit is reached, creating an ensemble of models can improve accuracy and stability in predictions. Ensembling can improve accuracy and stability in predictions, but it can increase test time costs. To address this, distillation can be used to convert an n-way ensemble into a single still-servable model. This process involves training an n-way ensemble with distributed SGD and then training a student network to mimic the ensemble. Although distillation increases training time and complexity, it offers quality improvements similar to the larger ensemble model. However, the additional training costs may deter practitioners from using ensemble distillation, despite its potential to enhance results. Codistillation is a simpler online variant of ensemble distillation that trains multiple models in parallel by matching their average predictions. It improves accuracy and speeds up training by efficiently utilizing computational resources. Compared to traditional distillation methods, codistillation does not increase training time and is easier to use in practice. Codistillation is a method that trains multiple models in parallel by matching their average predictions, improving accuracy and speeding up training. It does not increase training time and is easier to use in practice. Codistillation also reduces prediction churn, which is important for testing and launching new model versions in a non-disruptive way. It has been independently described by multiple researchers and has been experimentally validated at scale. The primary contribution of codistillation is its experimental validation at scale. The exploration of design choices and implementation considerations has practical utility. Codistillation offers minor quality gains over offline distillation, with potential as a distributed training algorithm using more delay-tolerant communication. Various tactics exist for scaling up neural network training, including model parallelism. Model parallelism has become more prevalent over multiple cores, with distributed training shifting towards data parallelism on GPUs. Methods like ensembling and distillation are separate from distributed training infrastructure, but mixture of experts models offer natural integration with data parallelism. Recent work includes examples like BID7 and BID19 in this area. Researchers aim to scale neural network training to larger datasets and models, exploring optimization algorithms. In an effort to scale neural network training on larger datasets and models, researchers are exploring optimization algorithms. Distillation, a meta-algorithm, allows any algorithm to incorporate the benefits of ensembles by training a teacher model first and then a student model with a loss function that encourages similar predictions. Distillation encourages student model predictions to mimic those of the teacher model, with various options for teacher model types, loss functions, and training datasets. Even if the teacher and student models are similar neural networks, distillation still offers benefits. The distinction between teacher and student models is not necessary, as multiple models can distill from each other effectively. Codistillation involves using the same architecture and dataset for multiple models, training them simultaneously with a distillation loss. The distillation term can be based on various measures of agreement between model predictions, such as cross-entropy error. Initially, the distillation term may not be very effective during training. To maintain model diversity and avoid a complex loss function schedule, the distillation term in the loss function is only enabled once training has started. Codistillation allows updating network parameters using predictions from other networks, reducing communication needs. Stale predictions are considered less problematic than stale gradients for training. During training, weight updates should only substantially change predictions on a small subset of training data. Different copies of weights may have arbitrary differences, making averaging gradients meaningless unless models are very similar. Output units have a clear meaning enforced by the loss function and training data, unlike weights and gradients which can have arbitrary differences. Using stale predictions for other neural networks in codistillation has little to no adverse effect on the final trained model quality. This tolerance for stale teacher predictions suggests a distributed training strategy that is less communication-intensive than synchronous or asynchronous SGD. Each worker trains independently on a subset of data, occasionally checkpointing parameters for others to load the freshest checkpoints. When combining distributed SGD and codistillation, workers can exchange checkpoints between independent groups to optimize training efficiency. This strategy reduces the need for frequent communication and allows for the deployment of additional workers in separate groups for improved performance. Distributed codistillation allows for the exchange of checkpoints between independent worker groups, reducing the need for high-precision floating point numbers. This method leverages compressed model parameters for communication efficiency in large-scale neural network training, particularly in tasks like neural language modeling. Neural language modeling is a key application for distributed codistillation in large-scale neural network training. The study aims to demonstrate scalability improvements by using a large dataset that challenges existing parallelization strategies. Results were validated on ImageNet to show the benefits of codistillation in reducing prediction churn. To demonstrate the benefits of codistillation in reducing prediction churn and study algorithm properties, smaller experiments were conducted using a large dataset from Crawl. The dataset contained roughly 915 million English language documents with long paragraphs for modeling longer range dependencies. The language model used in the experiments was an RNN with two LSTM layers of 1024 units each. The plan is to release the list of document ids and code for the invertible tokenizer for others to utilize the dataset. The RNN language model used two LSTM layers of 1024 units each with layer normalization. It had 256 dimensional input embeddings and a vocabulary of 24006 word pieces. The dataset had 673 billion tokens and batches were 32 word pieces long. The model used the ADAM optimizer for all experiments on Common Crawl. The strategy for training models includes sending predictions back to the server after each update or continuously updating predictions for each training data piece. Training all model copies in the same process is another alternative, especially when the model size is small relative to hardware capabilities. The experiments did not use the entire dataset due to scalability issues, but it is hoped to be useful for future research. ImageNet is a popular image classification benchmark, and experiments followed a specific setup closely. In recent years, experiments on ImageNet closely followed the setup of BID6 and used fully-synchronous SGD. A batch size of 16384 was used with a learning rate scaling and schedule, achieving 75% accuracy as the primary baseline. Increasing the batch size beyond 8192 showed diminishing returns. The Criteo Display Ad Challenge dataset 4 is a benchmark for predicting click-through rates in online advertising, with 43 million examples and a binary classification task. A feedforward neural network with ReLU activations and hidden layer sizes of 2560, 1024, 256 was trained using Adagrad optimizer with a learning rate of 0.001. In experiments with a large dataset, the researchers tested asynchronous SGD with varying numbers of GPU workers. They found it challenging to maintain stability and prevent divergence with a high number of workers. Ultimately, they focused on the synchronous algorithm to reduce dependency on infrastructure characteristics. The researchers found it challenging to maintain stability and prevent divergence with a high number of GPU workers when using asynchronous SGD. They focused on synchronous training as the default due to the debilitating effect of stale gradients on learning progress. Asynchronous SGD tends to diverge easily and practitioners are moving towards synchronous training. Preliminary experiments showed that gains from codistillation were independent of the choice of asynchronous or synchronous SGD. Increasing the effective batch size in synchronous SGD can lead to faster convergence and higher quality updates, even with a large number of GPU workers. Training language models on Common Crawl with different worker batch sizes showed improved performance with larger effective batch sizes. The effective batch size ranged from 4096 to 32768. Learning rates of 0.1, 0.2, 0.3, and 0.4 were tested for different numbers of workers. The best performing learning rates were 0.1 for 32 and 64 workers, and 0.3 for 256 workers. Increasing the number of workers reduced the steps needed to reach the best validation error until 128 workers, with no additional improvement at 256 workers. In experiments with varying numbers of synchronous workers, using 256 workers resulted in a large degradation in step time and learning progress. Despite the possibility of improving step time with a more sophisticated scheme, the scalability limit was the diminishing return from increasing the effective batch size. Synchronous SGD with 128 workers was the strongest baseline in terms of training time and final accuracy. Our experiments compared synchronous SGD with 128 workers for language modeling on Common Crawl, achieving the best results among configurations tested. Increasing to 256 GPUs did not improve training time. Codistillation with two groups of 128 GPUs using synchronous SGD may improve training time by exchanging checkpoints periodically. Comparisons were made with label smoothing baselines to address concerns about codistillation's effectiveness. In experiments comparing synchronous SGD with 128 workers for language modeling on Common Crawl, the best results were achieved. Codistillation with two groups of 128 GPUs using synchronous SGD may improve training time by exchanging checkpoints periodically. Trade-off hyperparameters were tuned by hand in preliminary experiments. Another comparison was made to an ensemble of two neural networks trained with 128 GPUs and synchronous SGD. The expectation is that codistillation, if achieving the benefits of traditional distillation, would have a training curve close to a two-way ensemble. Codistillation with two groups of 128 GPUs reduces training time significantly compared to baseline methods. It reaches the same validation error in half the steps and achieves a lower final error, indicating even greater gains. In our implementation, codistillation is free in terms of step time as the GPU is not fully utilized. Even with worst-case assumptions, network costs will only result in a modest increase in time per step. We confirmed our results by trying codistillation on ImageNet as well. Codistillation achieves 75% accuracy on ImageNet after 5250 steps, similar to the baseline's 7250 steps. Two-way codistillation reaches 75.6% accuracy at 7250 steps, showing its effectiveness in both language modeling and ImageNet. Potential reasons for the gains include different training data for codistilling models, ruling out label smoothing effects. Codistillation using different subsets of training data outperforms using the same data. The models successfully transmit useful information to each other. Codistillation can handle stale predictions, unlike SGD. Training with different checkpoint exchange frequencies shows slight degradation beyond 50 steps. Training with different checkpoint exchange frequencies, such as 50 steps, can slightly degrade the learning curve. The choice between multi-phase distillation and codistillation has little impact on quality improvements but affects training time. Experiment results show that codistillation can reach similar validation error levels in fewer steps compared to basic distillation. The study compares distilling models M1 and M2 into model M3, highlighting the importance of avoiding overfitting in the teacher model. Reproducing experiments on CIFAR-100 with different checkpoints shows varying performance in offline and online distillation. A teacher network must provide additional information beyond training labels for distillation to be effective. Codistillation offers faster training times compared to multi-phase distillation variants, with similar quality improvements. Codistillation simplifies the training pipeline by using the same architecture and dataset for all models, avoiding the need to choose different teacher models. This protocol reduces the complexity of the distillation process and eliminates the need for human intervention between phases. Unlike traditional multi-phase distillation, codistillation prevents the need to track and reproduce multiple teacher models, reducing unnecessary data dependencies and potential issues with model rollbacks. Training neural networks can lead to unpredictable predictions due to non-convex optimization, leading to different outcomes even with the same architecture and data. Minor changes in model architecture or retraining can cause significant variations in predictions, especially on atypical examples with rare features. The weights learned during training depend on factors like initialization, data order, and infrastructure, making it challenging to control all variables. Prediction churn, a problem where retraining a neural network leads to different predictions even with minor changes, can be mitigated by model averaging. This approach helps to make predictions more consistent across ensemble retrains and base model modifications. Codistillation, which offers similar benefits to model averaging, is hypothesized to reduce prediction churn as well. Codistillation, a method similar to model averaging, reduces prediction churn by 35% without increasing serving costs. This was tested by training a deep neural network on the Criteo dataset and measuring the prediction differences between retrains of the same model and ensemble models. The results showed that codistillation achieves similar results to ensembling in reducing prediction churn. Distillation is a flexible tool that can accelerate training, improve quality, and reduce prediction churn. There are still questions to explore, such as different model topologies and the limits of accuracy for teacher models. Aggressively quantizing the teacher model could make codistillation cheaper for large models. In distillation, bad models codistilling can learn faster than models training independently. The mistakes made by the teacher model provide information to help the student model perform better. Extracting predictions from checkpoints is a method used, and there is potential to extract more information without facing issues with gradients. Distillation-based methods could enhance federated learning in bandwidth-constrained settings."
}