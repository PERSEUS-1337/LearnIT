{
    "title": "rkmtTJZCb",
    "content": "Recent research has focused on video prediction and generation, particularly for short-term time horizons. The hierarchical video prediction method by Villegas et al. (2017) is a state-of-the-art approach for long-term video prediction but requires a ground truth pose during training. This paper introduces a long-term hierarchical video prediction model that learns its own higher-level structure, providing sharper results without the need for a ground truth pose. The model's efficiency is demonstrated on the Humans 3.6M and Robot Pushing datasets. In this work, the method of BID20 is explored as a novel way to generate long-term video predictions without requiring ground truth human pose annotations. The task involves predicting future video frames based on observed frames and considering the action conditional setting. The goal is to predict the pixel-level outcome of an action in the future without needing high-level structure annotations. The video prediction problem is addressed without the need for ground truth pose annotations. The method is hierarchical, generating a high-level structure for next frame predictions. Previous patch-level studies showed promise on synthetic data but struggled with higher resolution videos. Recent work focuses on frame-level prediction using convolutional encoder/decoder frameworks, with some networks explicitly predicting movement for each pixel in the previous frame. The network is trained to predict pixel movements for final predictions, minimizing L2 loss. Various approaches have been proposed for video prediction, including adversarial training and decomposing motion and content. Different networks have been suggested for long-term prediction, such as predictive coding and autoregressive generation schemes. Despite promising results, long-term high-resolution video prediction beyond 20 frames remains a challenge. BID14 proposed a convolutional encoder-decoder architecture for long-term prediction on video games. BID20 demonstrated hierarchical prediction using human pose supervision. The method involves an encoder generating an embedding vector from the previous image and a predictor network forecasting future encodings. The predictor network makes predictions based on context frames and previous predictions. The encoding of context frames is used to generate predictions, which are then used by a visual analogy network to generate corresponding images. The Visual Analogy Network (VAN) transforms encoding from the first to t-th timestep by mapping images to a space where analogies can be represented. The predicted image at timestep t is obtained using VAN with a hardcoded function to transform pose into a 2-dimensional representation. Training the encoder, predictor, and VAN does not require ground truth pose annotations in this method. In the Visual Analogy Network (VAN), encoding is transformed from the first to t-th timestep without requiring ground truth pose annotations. The network's higher level structure is represented by e t and p t, with a fully connected network mapping the encoding instead of a convolutional neural network. The VAN equation includes f enc as a fully connected network, f img as a conv net, and f dec as a deconv network. Training methods for these networks are explored in the absence of ground truth human pose. In the absence of ground truth pose annotations, alternative training methods for the Visual Analogy Network (VAN) are explored. One approach is to connect the networks the same way as in inference time and train them end to end (E2E) by optimizing the L2 loss of the predicted image. The encoder produces an encoding easily predicted by the predictor, leading to similar images to the ground truth. The encoder and predictor do not need to represent information from the first ground truth frame, as the VAN has access to it. The size of e t and p t is a hyperparameter. The E2E method involves training the encoder and predictor together to predict subsequent encodings for the VAN to produce pixel-level predictions. An alternative method, EPEV, explicitly trains the encoder to make e t easy to predict and use for future frames. The EPEV method trains the encoder to minimize the difference between et and pt, and to generate an informative encoding for the VAN to produce images. The network is trained to minimize two losses: one for predicting et easily and one for producing good images. See figures 2 and 3 for diagrams of the training process. The EPEV method involves training the encoder to generate an informative encoding for the VAN to produce images. The predictor predicts encoder outputs in future timesteps, while the VAN uses the encoder output to generate frames. This approach is similar to an autoencoder and can be enhanced with ground truth pose information or high-level frame annotations. In the proposed method, the encoder and predictor vectors are split to represent pose and additional information. Separate optimizers minimize losses for inferring pose, predicting pose, and end-to-end learning. This approach ensures the VAN learns to utilize the encoded information effectively. The proposed method involves splitting encoder and predictor vectors to represent pose and additional information. Separate optimizers minimize losses for inferring and predicting pose, as well as end-to-end learning. This approach ensures effective utilization of encoded information by the VAN. The method was compared to a baseline where networks are trained individually, without using adversarial loss. Experimental results show that the EPEV method performs best when \u03b1 starts small and gradually increases during training. The dataset contains videos of a robot arm pushing objects on a table, with joint angles and end effector location provided for pose. Different methods were trained to predict subsequent frames using two frames of context. Encoding sizes varied between methods, with E2E using 16, INDIVIDUAL using 12, and others using 32. The dataset was split into training, validation, and test sets, with 64x64 images used. Test results are presented in the following section. The test results from the dataset show that the EPEV method outperformed the INDIVIDUAL method in predicting object movements accurately. The VAN network was able to infer object movements well based on the arm's start and end states, supporting the hypothesis that methods allowing the network to learn its own pose equivalent would perform better. The EPEV and E2E methods show better object predictions than the INDIVIDUAL method, as seen in the magnified ground truth frame 19. The BID3 model outperforms in Peak Signal to Noise Ratio (PSNR) on this dataset. The movement in the dataset is deterministic, making the BID3 model, which predicts object movement and minimizes L2 loss, effective. Training on a toy task with known factors of variation confirms the method's success in long-term predictions. The EPEV and CDNA methods were trained on a dataset with a bouncing shape that changes size predictably. Results from a test set showed that when CDNA fails, the shape disappears, while EPEV changes color. A script was used to evaluate shape presence and color accuracy in frames 1012 to 1022. The CDNA method predicts shape color correctly 25% of the time, while the EPEV method predicts it correctly 97% of the time. EPEV sometimes fails by predicting the shape in the same location frame to frame. The methods may not accurately predict shape location in frame 1000 due to small errors propagating in each step. The E2E and EPEV methods were tested on the Humans 3.6M Dataset using specific subjects for training, validation, and testing. In this paper, results for testing Subject 11 using 64 by 64 images are reported. The dataset was subsampled to 6.25 frames per second, with methods trained to predict 32 frames and results showing prediction of 64 frames. Each method receives the first 5 frames as context frames, predicting about 10 seconds into the future from 0.8 seconds of context. The E2E method used an encoding size of 32, while the EPEV method used an encoding size of 64. A visual comparison of the EPEV method and CDNA from BID3 is shown in Figure 6, with results indicating significant movement in the ground truth. The EPEV method produces sharp predictions up to frame 42, with blurry predictions at frame 63. When there is significant movement in the first 5 ground truth frames, EPEV's predictions are sharper than CDNA's. CDNA produces blurry images as it is trained to minimize L2 loss directly. The CDNA method from BID3 produces blurry images due to direct L2 loss minimization. In contrast, the EPEV method trains the predictor and VAN separately to prevent blurry images. Comparing to BID20, our method yields less sharp results possibly due to the use of adversarial loss. We propose a quantitative comparison based on the presence of a recognizable person in generated videos using object detection. The \"person score\" for the detector in the images ranges from 0 to 1, with higher scores indicating higher confidence levels. The person score on ground truth frames is around 0.4, but decreases to 0.26 for images generated by the EPEV method and 0.18 for CDNA from BID3. The EPEV method produces clearer predictions further into the future compared to CDNA, with similar scores on frame 63 for EPEV and frame 8 for CDNA. The EPEV method was trained to predict 32 frames into the future and shows no significant drop in person score at frame 32, indicating good generalization. Comparisons with the CDNA baseline method using Mechanical Turk showed that the EPEV method was rated as more realistic 53.6% of the time, while the CDNA method was rated as more realistic 11.1% of the time. The EPEV method, which predicts future frames, outperforms BID20 by allowing the network to define its own high-level structure. It generates sharper images than BID3 on non-deterministic datasets and can predict further into the future on a toy dataset. An adversarial loss between the predictor and encoder could improve uncertain scenarios and address the issue of blurry images. The encoder in the EPEV method encodes the person's location effectively. Results from the EPEV approach show that the encoder's output on the ground truth frame is not as good as expected due to constraints. Training was done for 3 million steps using async SGD across 32 worker machines with a minibatch size of 8 sequences. Hyperparameters were optimized separately for both datasets on a validation set."
}