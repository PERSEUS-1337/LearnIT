{
    "title": "SyfIfnC5Ym",
    "content": "Adversarial training aims to improve deep learning model robustness by injecting adversarial examples into training data. Existing approaches often lack representative samples from different attacks, leading to weak generalization. To address this, a novel Adversarial Training with Domain Adaptation (ATDA) method is proposed to enhance generalization by incorporating efficient FGSM adversaries. The proposed Adversarial Training with Domain Adaptation (ATDA) method aims to improve generalization by treating adversarial training as a domain adaptation task with limited target domain samples. It focuses on learning a representation that is semantically meaningful and domain invariant across clean and adversarial domains. Empirical evaluations on various datasets show that ATDA enhances generalization and outperforms state-of-the-art methods. Additionally, ATDA's transferability is demonstrated by extending it to iterative attacks like PGD-Adversarial Training (PAT), resulting in significantly improved defense performance. Recent works have shown that deep learning models are vulnerable to adversarial examples, which can deceive the target model and transfer across models. Effective defense against these attacks is crucial for security-critical computer vision systems like autonomous driving. Adversarial training, which involves training a classifier with adversarial examples, has been found to increase the robustness of neural networks. However, it often focuses on specific attack techniques, limiting its defense capabilities. In this paper, a novel adversarial training method is proposed to improve generalization by addressing the risk of overfitting to specific attack techniques. The method aims to bridge the domain gap between clean and adversarial examples in the high-level representation space, enhancing the robustness of neural networks against adversarial perturbations. Our contributions focus on the amplification of adversarial perturbations along neural network layers, the limitations of adversarial training with specific attacks, and the lack of theoretical analysis for iterative noisy attacks. The approach aims to enhance the generalization of adversarial training by treating it as a domain adaptation problem, incorporating unsupervised and supervised domain adaptation techniques. This helps minimize the gap between clean and adversarial example distributions, improving model performance against various attacks. The ATDA method shows significant improvements in generalization ability on adversarial examples, despite a slight decrease in accuracy on clean data. The text introduces notations and provides an overview of advanced attack and defense methods in adversarial training. It defines clean and adversarial data domains, classifier based on a neural network, perturbation magnitude, adversarial image computation, cost function for image classification, and logits layer representation. The text discusses different types of attacks in adversarial training, including white-box attacks with complete knowledge of the target model and black-box attacks with limited knowledge. Four attack methods are considered, with adversarial examples clipped in [0, 1]. The Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) are highlighted as efficient adversaries. Projected Gradient Descent (PGD) is a stronger iterative variant of FGSM, applying FGSM iteratively for k times with a budget \u03b1. The clip(\u00b7, a, b) function ensures input resides in the range of [a, b]. PGD has a higher success rate in white-box settings but weaker capability in black-box settings. R+FGSM introduces a small random perturbation before FGSM. Momentum Iterative Method (MIM) won the NIPS 2017 Adversarial Attacks Competition by utilizing gradients of previous steps with a decay factor \u00b5 before applying FGSM. An intuitive technique to defend deep models against adversarial examples is adversarial training, which involves injecting adversarial examples into the training data. Goodfellow et al. proposed increasing robustness by training with both original and adversarial examples generated by FGSM. BID7 scaled this approach to ImageNet and replaced half of the clean examples with adversarial ones. However, their method was not robust against the RAND+FGSM adversary. BID22 introduced ensemble adversarial training to improve robustness against black-box attacks. The text discusses improving robustness against black-box attacks by injecting adversarial examples into training data. Various approaches such as training only with adversarial examples and using noisy PGD for defense have been explored. However, scaling these techniques to large-scale neural networks is challenging due to increased training time. Another method involves minimizing loss for the worst case within a perturbation ball around each clean data point. In this work, the focus is on training with clean data and efficient FGSM adversarial examples to improve generalization for different adversaries while keeping computational costs low. Previous approaches struggled with high test errors on clean data and scalability to deep neural networks. Generating representative adversarial examples for large datasets is computationally expensive. The proposed Adversarial Training with Domain Adaptation (ATDA) method aims to defend against adversarial attacks by treating adversarial training as a domain adaptation task. By combining standard adversarial training with a domain adaptor, the model minimizes the domain gap between clean and adversarial examples, leading to effective defense against FGSM attacks and strong generalization for various adversaries. The goal is to align the covariance matrices and mean vectors of clean and adversarial data in the logit space to minimize the shift. The CORAL method defines a covariance distance between the two distributions, while also using Maximum Mean Discrepancy (MMD) to minimize the distance of the mean vectors. The loss function for Unsupervised Domain Adaptation (UDA) involves aligning mean vectors and covariance matrices of clean and adversarial data in the logit space. To address the issue of samples from the same label not mapping nearby in the logit space, a supervised domain adaptation (SDA) approach is introduced. This includes a margin loss function to minimize intra-class variations and maximize inter-class variations. The SDA loss function is denoted as Eq. FORMULA10, with specific functions and variables defined within it. The training process involves updating the logits center for each class using a learning rate \u03b1. Adversarial examples are generated using a variant of FGSM attack to avoid label leaking effects. However, the sampled adversarial examples may not be sufficiently representative. The sampled adversarial examples may not be representative as they lie at the boundary of the \u221e ball of radius. Adversarially trained models may overfit on clean and FGSM attack data, leading to weak generalization. Combining adversarial training with domain adaptation can improve generalization on adversarial data. New adversarial examples are generated using a variant of FGSM attack. In this section, new adversarial examples are generated using a variant of FGSM attack. The ATDA method is evaluated on benchmark datasets to demonstrate robustness against white-box and black-box attacks. Clean test accuracy and defense accuracy on various attacks are reported to evaluate generalization power. The code for these experiments is available at https://github.com/JHL-HUST/ATDA. The ATDA method is compared with various adversarial training methods, including Normal Training, Standard Adversarial Training, Ensemble Adversarial Training, and Provably Robust Training. These methods differ in their approach to training with clean and adversarial examples to improve defense accuracy against attacks like FGSM, PGD, R+FGSM, and MIM. The comparison includes common settings for these attacks as shown in Table 5 of the Appendix. Training with a method of sampling in the \u221e ball of radius around clean data points is evaluated on various attacks. Ensemble Adversarial Training (EAT) uses two static pre-trained models. Experiments are conducted on a single Titan X GPU with specific hyper-parameter settings. Network architectures and training details are provided in Appendix A. The focus is on ensuring the networks work rather than optimizing them. The ATDA method is evaluated for defense performance on various datasets like Fashion-MNIST and SVHN. Results show NT performs well on clean data but poorly on adversarial examples, while PRT has lower error against adversaries but higher error on clean data. ATDA shows stronger robustness against different adversaries compared to SAT. On SVHN, PRT degrades performance on clean data, while ATDA exhibits stronger generalization ability compared to SAT. ATDA demonstrates stronger generalization ability on adversarial examples and higher accuracy on white-box adversaries compared to SAT. It outperforms other methods on most adversaries while maintaining similar performance on clean data. Results on CIFAR-10 and CIFAR-100 datasets show ATDA's superior performance in defending against various attacks. ATDA demonstrates superior generalization ability on adversarial examples compared to SAT, achieving better performance on various attacks without degrading accuracy on clean data. The method outperforms competitors and shows strong defense performance on CIFAR-100. Further analysis includes metrics like local loss sensitivity and shift in adversarial data distribution. The results suggest that adversarial training methods increase model smoothness compared to normal training, with ATDA performing the best. Distribution Discrepancy is quantified by comparing learned embeddings with competing methods on Fashion-MNIST using t-SNE. SAT and EAT increase MMD distance across domains, while PRT and ATDA learn domain invariance. Our final algorithm ATDA achieves the best performance on domain invariance by aligning the covariance matrix and mean vector of clean and adversarial examples. Combining UDA and SDA with SAT, ATDA shows stable improvements in defense quality on standard adversarial training. The performance of ATDA is slightly better than SAT+UDA, with stable improvements over various attacks. ATDA achieves the best performance on domain invariance by aligning clean and adversarial examples. Extending ATDA to PAT, called PATDA, shows mixed results on Fashion-MNIST and SVHN datasets. PATDA fails to increase robustness compared to PAT on Fashion-MNIST, while both fail to converge properly on SVHN due to the complexity of training with hard adversarial examples. In this study, domain adaptation is applied to adversarial training to improve defense performance against various attacks. PATDA shows stronger robustness compared to PAT on CIFAR-10 and CIFAR-100 datasets. By combining adversarial training with unsupervised and supervised domain adaptation, the generalization ability on adversarial examples can be greatly improved for robust defense. Additionally, ATDA can be extended to iterative attacks like PGD to further enhance defense performance. The proposed ATDA and PATDA achieve better generalization results in defense performance against attacks. The appendix provides details on common settings, neural network architectures, and training hyper-parameters. Fashion-MNIST training phase uses Adam optimizer with a learning rate of 0.001 and batch size of 64. Hyper-parameters for adversaries are detailed in Table 5, with adversarial perturbations denoted by . Fashion-MNIST neural network architectures for main model and pre-trained models are shown in TAB6. The model architectures for CIFAR-10 and CIFAR-100 include the use of Adam optimizer with a learning rate of 0.001, batch size of 32, Exponential Linear Unit (ELU) as activation function, and Group Normalization. Adversarial training methods involve perturbations of 0.02 or 4/255 in \u221e norm. The magnitude of perturbations is 4/255 in \u221e norm for CIFAR-100, using the same training settings as CIFAR-10. The neural network architectures for the main model and pre-trained models are shown in Table 8."
}