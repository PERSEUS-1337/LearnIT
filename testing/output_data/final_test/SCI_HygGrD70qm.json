{
    "title": "HygGrD70qm",
    "content": "In our study, we utilized computational techniques to classify protein functions due to the time-consuming nature of manual annotation. Using data from Swiss-Prot with 40433 proteins grouped into 30 families, we compared RNN, LSTM, and GRU models with trigram and deep/shallow neural networks. Our approach achieved up to 78% accuracy in classifying protein families. Proteins are essential for life, carrying out various functions like DNA replication, molecule transportation, and metabolic reactions based on genetic information. They are classified into globular, membrane, and fibrous types, with globular proteins often acting as enzymes. Membrane proteins facilitate molecule transport across cell membranes, while fibrous proteins like collagen provide structural support. Proteins make up a significant portion of cell composition, contributing to the field of proteomics. Proteins are essential for life, carrying out various functions based on genetic information. Proteomics is a crucial field in modern computational biology, focusing on predicting protein family classification and studying their functionalities. Proteins are polymeric macromolecules made up of amino acid residue chains joined by peptide bonds. The proteome of a cell type consists of proteins specific to that type, with each protein having a unique 3D structure determined by protein folding. The primary structure of proteins is represented as a sequence of 20-letter alphabets associated with amino acid base subunits. The primary structure of proteins is an abstracted version of the complex 3D structure but retains sufficient information for protein family classification and infer the functionality of the families. Protein family consists of a set of proteins that exhibits similar structure at sequence as well as molecular level involving same functions. Members of the same protein family can be identified using sequence homology which is defined as the evolutionary relatedness. It also exhibits similar secondary structure through modular protein domains which further group proteins families into super families. These classifications are listed in database like SCOP. Protein family database (Pfam) is an essential resource for protein classification. In this work, protein family classification is done using data from Swiss-Prot with a classification accuracy of about 96%. The study gathered family information of 40433 protein sequences from Pfam, consisting of 30 distinct families. Keras embedding and n-gram techniques were applied with deep learning architectures and traditional machine learning classifiers for text classification in cyber security. In the cyber security domain, protein family classification is conducted using Keras word embedding and deep neural network models like RNN, LSTM, and GRU. The performance is compared using trigram with deep and shallow neural networks. The model is tested on a dataset of 12000 sequences. The paper is organized into sections discussing related work, deep learning architecture, methodology, results, and conclusions. Many studies have focused on identifying protein functions based on primary structures or sequences. In the field of protein family classification, researchers have utilized computational techniques like machine learning, deep learning, and pattern recognition instead of traditional alignment methods. Needleman and Wunsch developed a global alignment algorithm, while Smith focused on local alignment and clustering based on fragment length. Recent works have used machine learning to classify protein families based on primary structures. In protein family classification, primary structure is used to classify protein families using classifiers like SVM. Additional information such as polarity, hydrophobicity, and charge is required for feature extraction, requiring significant computational power. Studies have achieved high accuracy in protein classification using methods like PSSM and classifiers like NB, DT, SVM, and RF. Hashing was introduced to map high-dimensional features to low-dimensional features using hash keys. Caragea et al. BID16 used this technique with k-gram representation, achieving an accuracy of 82.83%. Yu et al. BID17 proposed a method for representing protein sequences as a k-string dictionary using singular value decomposition (SVD). Mikolov et al. BID18 introduced a model architecture called word2vec to represent words as continuous vectors in a low-dimensional space, allowing for the study of linguistic context. In BID19, word2vec architecture was applied to biological sequences to create bio-vectors (BioVec) for protein and gene sequences. The k-mers from the data were inputted to the embedding layer, achieving a 93% family classification accuracy using ProtVec. The proposed architecture in our work is trained solely on primary sequence information, achieving high accuracy in protein family classification. Text encoding can be done in sequential or non-sequential ways through preprocessing and tokenizing the texts. During preprocessing, text is tokenized and converted to lowercase. A dictionary assigns unique IDs to characters for mapping to vector sequences. The vocabulary is created from training data, converting varying length sequences to fixed length. Text data is represented as a sequence to maintain word order. Keras embedding is used for text representation, mapping character IDs to continuous vectors capturing semantic meaning. The character embedding in protein sequences maps them to a high dimensional space. N-grams are used to represent text data by combining continuous sequences of characters. Trigrams, a combination of three adjacent elements, are used as features in some models. Pseudo-words are used to create the first trigram in sentence probability computation. After preprocessing data into continuous vectors, it is input into layers like RNN, LSTM, and GRU. RNN differs from FFN by passing information through a loop, making it recurrent. RNN efficiently handles sequence data for NLP tasks, acting on arbitrary length sequences and sharing weights across time steps. The RNN model shares weights across time steps, representing memory flow mathematically. LSTM was introduced to address vanishing and exploding gradient problems in long sequences. Variants of LSTM include forget gate and peephole connections. RNN has the same parameters at each layer, reducing the total number of parameters to be learned. The LSTM model was introduced to address the vanishing and exploding gradient problems in long sequences encountered by traditional RNNs. The weight matrix plays a crucial role in training RNNs, leading to slow learning if the gradient becomes very small or very large. This limitation of RNNs motivated the development of the LSTM model. The LSTM model introduced memory cells with input, forget, and output gates to address limitations of traditional RNNs. It uses identity function in self recurrent connections to prevent gradient issues. GRU is a variant of LSTM with fewer parameters for memory control, both are used for sequence modelling capturing short and long term dependencies. The GRU network has fewer parameters compared to LSTM, making training faster. It uses reset and forget gates to protect memory and make long-term predictions. Performance of deep and shallow neural networks with different architectures is evaluated on a protein sequence dataset. Proteins are classified into 30 families based on their primary structure, which is represented by a sequence of 20-letter alphabets. This information is gathered from Swiss-Prot, a curated database of protein sequences. Identifying the family of a protein sequence provides insight into its functions. The protein sequences are divided into test and training data, with 30 family names listed in Table 1. The DeepProteomics architecture includes Character Embedding, Feature representation, Regularization, and Classification sections. Matrices of size 28433*3988 for training and 12000*3988 for testing are created and passed to an embedding layer with a batch size of 128. Each character is mapped to a 128-length vector for deep learning. The deep learning architectures provide freedom and collaborate with other layers during backpropagation, facilitating sequence character clustering. Trigram representation is used for protein sequences with a length set to 1000. Deep layer RNN is adopted for feature representation, extracting sequential information. RNN, LSTM, and GRU are used as recurrent structures with 1 layer and 128 units. A dropout of 0.2 is applied, followed by a fully connected layer with 30 neurons. The deep learning architecture utilizes recurrent structures like RNN, LSTM, and GRU with 1 layer and 128 units. A dropout of 0.2 is applied for regularization. Character level vectors work with recurrent structures to learn sequence similarities. The feature representations are then passed to a fully connected network for classification based on family probabilities. The fully connected layer uses a non-linear activation function for classification. The protein sequence vectors are used in shallow DNN, deep DNN, and other classifiers for comparison. The deep learning models predict protein families using categorical-cross entropy loss and Adam optimization algorithm. The architecture details of GRU, LSTM, and RNN are in tables, as well as the shallow and deep DNN modules. The DNN module details are provided in Table 3 and Table 6. The experiments were conducted using GPU-enabled TensorFlow and Keras. The performance of neural network models surpasses traditional machine learning techniques, showcasing the effectiveness of character-level URL embedding for automatic feature extraction in protein family classification. Various recurrent models like RNN, LSTM, and GRU were analyzed, along with comparisons between trigram with deep neural network and shallow neural network approaches. Neural networks, including deep and shallow models, are preferred for protein family classification due to their ability to capture optimal feature representation. Deep neural networks have a complex architecture, making the internal workings somewhat of a black box. Future work could involve exploring the network's internal operations by examining Eigenvalues and Eigenvectors across different time steps."
}