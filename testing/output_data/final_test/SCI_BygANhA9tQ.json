{
    "title": "BygANhA9tQ",
    "content": "Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. Cost-sensitive robustness is advocated as the criteria for measuring classifier performance in tasks where some adversarial transformations are more important than others. A general objective function is proposed to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness by encoding the potential harm of each adversarial transformation in a cost matrix. Experiments on MNIST and CIFAR10 models show that this approach can reduce cost-sensitive robust error while maintaining classification accuracy. Recent studies have shown that deep learning models are vulnerable to adversarial examples, which are crafted with visually-imperceptible perturbations. Defense mechanisms have been proposed, but new attacks continue to circumvent these defenses. Recent works propose certifying examples to be robust against specific adversarial perturbations and training models for certifiable robustness. These methods aim to improve the overall robustness of the classifier. Recent studies have shown that deep learning models are vulnerable to adversarial examples, with defense mechanisms proposed to improve classifier robustness. However, achieving overall robustness remains challenging, as certain adversarial misclassifications pose more significant threats than others. From a security perspective, only specific transformations matter. In this paper, a method is proposed to adapt provable defenses against norm-bounded perturbations by considering the potential harm of different adversarial class transformations. A cost matrix is used to capture the impact of these transformations, with the goal of minimizing the cost-weighted robust error during training. This approach aims to address the challenges of achieving overall robustness in deep learning models against adversarial examples. The proposed method introduces cost-sensitive robustness by incorporating a cost matrix into the training objective function. It aims to improve robustness guarantees on class transformations while maintaining classification accuracy. The method can handle various cost scenarios and has been shown effective on MNIST and CIFAR10 datasets. Our model introduces cost-sensitive robustness by incorporating a cost matrix into the training objective function, achieving significant improvements in robustness for different tasks on MNIST and CIFAR10 datasets while maintaining classification accuracy. The method can handle various cost scenarios and has been effective in enhancing robustness guarantees on class transformations. Neural network classifier represented by function f : DISPLAYFORM0, with mapping function f k (\u00b7) consisting of affine transformation and ReLU activation. Feature vector denoted as z k, defined as DISPLAYFORM1 with weight parameter matrix W k and bias vector b k. Output function f K\u22121 (\u00b7) maps feature vector to output space Y through matrix multiplication: DISPLAYFORM2. Training involves loss function DISPLAYFORM4. To train the neural network, a loss function DISPLAYFORM4 is used for multiclass image classification. Model parameters are updated iteratively using backpropagation. Adversarial examples are inputs that are visually indistinguishable from natural examples but can mislead the classifier. The classifier f \u03b8 (\u00b7) is defined with a maximum perturbation distance > 0. Different geometric transformations can be used to find adversarial examples. Recent work has proposed defenses against norm-bounded perturbations for neural networks. Our work builds on previous approaches for training certified robust classifiers, specifically focusing on deep ReLU-based networks. An adversarial polytope is defined for a given example x, containing all possible output vectors by perturbing x within a specified radius. A seed example is considered certified robust if its corresponding adversarial example set is empty. This certification is achieved by solving an optimization problem for any output class, ensuring robustness within the defined perturbation distance. The text discusses training a robust model using robust optimization to minimize sample loss on worst-case locations. Due to nonconvexity introduced by ReLU activation, solving the optimization problem directly is computationally intractable. An alternative training objective based on convex relaxation is proposed for efficient optimization through a dual network. The text discusses training a robust model using robust optimization to minimize sample loss on worst-case locations. They propose an alternative optimization problem based on convex relaxation for efficient optimization through a dual network. This approach provides stronger robustness guarantees compared to previous methods. The text discusses training a robust model using robust optimization to minimize sample loss on worst-case locations. They propose an alternative optimization problem based on convex relaxation for efficient optimization through a dual network. The method achieves certified robustness but has quadratic computational complexity, limiting scalability to small networks. To address this, nonlinear random projections were used to scale to larger networks. However, a small decrease in performance was observed when compared to the less scalable method. Cost-sensitive learning was proposed to handle unequal misclassification costs and class imbalance issues. Cost-sensitive learning algorithms address unequal misclassification costs and class imbalance in classification applications. Existing algorithms tend to overwhelm the major class, neglecting the minor class which may be of primary interest, such as in medical diagnosis. Some proposed algorithms consider adversarial settings, like a cost-sensitive robust minimax approach for linear discriminant classifiers. However, these methods are limited to simple classifiers and cannot be directly extended to neural networks. Our work introduces a practical training method to enhance neural network classifiers with certified cost-sensitive robustness against adversarial perturbations. We provide a formal definition of cost-sensitive robustness and propose a general method for training cost-sensitive robust classifiers. Our approach introduces a method for training cost-sensitive robust models using a cost matrix C to encode the harm of adversarial examples. The matrix determines if adversarial transformations from a seed class to target classes matter, certifying cost-sensitive robustness. The text discusses the concept of certified cost-sensitive robustness in machine learning models, where the lower bound of the error must be non-negative for all classes. It introduces a cost matrix to quantify the harm of adversarial examples and measure the average certified cost of these examples. The text introduces the concept of certified cost-sensitive robustness in machine learning models, using a cost matrix to quantify the harm of adversarial examples. It proposes robust optimization for a neural network classifier to achieve this goal. The text discusses the use of a regularization parameter \u03b1 in the optimization problem for achieving cost-sensitive robustness in machine learning models. The loss function penalizes adversarial examples based on their cost, with the regularization term adjusting the impact of different costs on the training objective. Gradient-based algorithms like stochastic gradient descent and ADAM can efficiently solve this optimization problem. The text discusses cost-sensitive robustness training using gradient-based algorithms like stochastic gradient descent and ADAM. Performance evaluation is done on image classification models for MNIST and CIFAR-10 datasets, focusing on adversarial perturbations bounded in an \u221e -norm ball. Different types of cost matrices are tested on representative tasks with specific convolutional neural network architectures. The text discusses training a cost-sensitive robust model using gradient-based algorithms like stochastic gradient descent and ADAM. The model consists of convolutional layers with ReLU activations, fully-connected layers, and is trained on five folds of the training dataset. Learning rate scheduling and decay techniques are applied, and the model's performance is evaluated on image classification tasks for MNIST and CIFAR-10 datasets. The model is trained using robust loss with a maximum perturbation distance of 0.2. The best classifier achieves 3.39% classification error and 13.80% overall robust error on the MNIST testing samples. The vulnerability to adversarial transformations varies among class pairs and is correlated with perceptual similarity. The effectiveness of cost-sensitive robustness training is evaluated by considering different binary cost matrices for adversarial transformations. Four types of tasks are defined, each capturing different sets of adversarial transformations. The cost matrix is defined for each setting, with a focus on producing models that are more robust for valuable adversarial transformations. For cost-sensitive robustness training, different binary cost matrices are considered for adversarial transformations. Various adversarial goals are selected based on vulnerability levels, with a focus on improving robustness against valuable transformations. The text discusses the results of cost-sensitive robust error comparison between baseline and defense models trained using cost-sensitive robust optimization techniques. The defense model is trained with a specific parameter value based on a loss function and cost matrix. The model achieves significant improvement in cost-sensitive robustness compared to the baseline model across various tasks, with a reduction in cost-sensitive adversarial examples. The regularization parameter \u03b1 is tuned through cross validation, resulting in a decrease in cost-sensitive robust error by 30% to 90%, especially with sparse cost matrices. The cost-sensitive robust model achieves stronger guarantees on adversarial transformations from smaller to larger digit classes, with quadratic cost based on the absolute difference between seed and target class digits. The model is tuned via cross validation on the MNIST dataset, showing improved robustness compared to the baseline model. The cost-sensitive robust classifier achieves stronger guarantees on adversarial transformations with larger costs, shown through heatmaps of robust test error on MNIST. The model uses a neural network architecture with convolutional and fully-connected layers, incorporating an approximation technique for efficiency. Training is optimized using SGD with random projection of 50 dimensions. The cost-sensitive robust classifier is trained on specific tasks with tuned parameters and achieves lower error rates compared to the baseline. Results on testing data show significant reduction in cost-sensitive robust error. The model also addresses adversarial transformations in real-valued tasks. The proposed cost-sensitive robust classifier addresses adversarial transformations in real-valued tasks, achieving better adversarial robustness compared to the baseline model. The classifier shows lower error rates and significant reduction in cost-sensitive robust error on specific tasks with tuned parameters. The proposed cost-sensitive robust classifier outperforms the baseline in adversarial robustness while maintaining similar classification accuracy. It focuses on important transformations and incorporates a cost matrix to account for variations in harm caused by adversarial attacks. The method shows effectiveness across different types of cost matrices. The proposed cost-sensitive robust classifier outperforms the baseline in adversarial robustness by incorporating a cost matrix to account for variations in harm caused by attacks. The scalability of the techniques is limited to toy models, but the approach is a step towards achieving more realistic robustness goals. The implementation code for all experiments is available as open source. For experiments on the MNIST dataset, a coarse tuning on regularization parameter \u03b1 is performed with a searching grid. The most appropriate \u03b1 is selected based on classification error and cost-sensitive robust error. Further fine tuning is done on \u03b1 to choose the best robust model. Learning curves for task B with digit 9 as the seed class show that as \u03b1 increases, the classifier has lower cost-sensitive robust error but higher classification error. This aligns with the design expectations. In the CIFAR10 experiments, a tuning strategy with a 35% threshold for overall classification error is implemented. The focus is on investigating the robustness of a cross-entropy based cost-sensitive classifier and comparing it with a proposed cost-sensitive robust classifier. The evaluation metric for cost-sensitive learning is defined as the average cost of misclassifications. The text discusses the average cost of misclassifications in a cost-sensitive training objective, comparing it with cost-sensitive robust training. It mentions the design of a cost matrix for a small-large real-valued task for MNIST. The comparison results are shown in TAB5. Our proposed classifier trained for cost-sensitive robustness significantly improves cost-sensitive robustness compared to a standard cost-sensitive classifier. TAB5 demonstrates the comparison results of different classifiers in this setting."
}