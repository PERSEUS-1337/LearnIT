{
    "title": "SkfTIj0cKX",
    "content": "One of the key challenges in session-based recommender systems is enhancing users' purchase intentions. The sequential interactions between user sessions and a recommender agent are formulated as a Markov Decision Process (MDP). The purchase reward is often delayed and sparse, making it difficult for policy learning. To address this, an Imagination Reconstruction Network (IRN) is proposed, inspired by prediction error minimization and embodied cognition. IRN allows the agent to explore its environment and learn predictive representations through three key components: imagination core, trajectory manager, and action policy optimization. The Imagination Reconstruction Network (IRN) optimizes action policy by minimizing imagination error and maximizing extrinsic reward. It promotes quicker adaptation to user interest, shows improved robustness, and achieves excellent next-click prediction performance. A good recommender system can enhance satisfaction for users and profit for content providers. Session-based recommenders using Recurrent Neural Networks (RNNs) focus on next-click prediction but struggle to differentiate between user clicks and purchases. This paper proposes treating session-based recommendation as a Markov Decision Process (MDP) to consider both click and purchase rewards, utilizing Reinforcement Learning (RL) for learning. The paper proposes using Reinforcement Learning to learn the recommendation strategy for session-based recommenders. Challenges include handling large numbers of discrete actions, specifying click and purchase rewards, and adapting quickly to user interest. The focus is on using purchase as the sole reward for enhancing purchase performance in short sessions. The Imagination Reconstruction Network (IRN) is introduced to augment reward and encourage exploration in session-based recommenders. Inspired by prediction error minimization theory, IRN utilizes active sensorimotor predictions to enhance policy learning in cases where purchase rewards are delayed and sparse. The brain uses active sensorimotor predictions to represent the world in an action-oriented manner. The imagination core of IRN simulates sensorimotor predictions by predicting future trajectories based on actions sampled from the imagination policy. The imagination-augmented executor minimizes prediction errors and maximizes rewards using RL, adapting the agent to changes from exploratory actions. The imagination policy in IRN creates a loop between brain and body, guiding the agent to take actions that reduce prediction errors. IRN combines model-based planning and self-supervised RL, providing dense training signals for auxiliary task learning. The paper formulates session-based recommendation as a Markov Decision Process and discusses challenges in applying RL. In a special case where only purchases are used as rewards, the IRN architecture is proposed to optimize sparse but critical purchase signals. A self-supervised reconstruction method for predictive learning minimizes imagination errors in simulated trajectories. IRN shows excellent performance in click and purchase without external rewards, improving data efficiency, user interest adaptation, and robustness in cold-start scenarios for higher purchase performance. Session-based recommenders face challenges in the absence of user profiles. The neighborhood approach, like item-to-item recommendation, uses an item similarity matrix based on co-occurrences of clicked items. However, it only considers the last clicked item, ignoring sequential information. Previous works have explored using MDPs and RNNs, with GRU4Rec being the first to model session data using RNNs. GRU4Rec BID9 is the first application of RNNs to model session data for providing recommendations after each click in new sessions. The proposed Imagination Reconstruction Network (IRN) handles sparse purchase rewards by distilling a roll-out policy and combining it with a static environment model to predict future observations. This approach aims to enhance purchase willingness in session-based recommendation models. In this paper, sequential recommendation is formulated as an MDP with a focus on handling sparse purchase rewards. The Imagination Reconstruction Network (IRN) is introduced, inspired by neu-science Prediction Error Minimization, to enhance purchase willingness in session-based recommendation models. By minimizing intrinsic imagination error and maximizing probability, IRN automatically balances exploration and exploitation, resulting in a more robust action policy. Experiments show significant improvements in purchase outcomes, even without treating clicks as rewards. In this study, the Imagination Reconstruction Network (IRN) is proposed to improve purchase intentions in session-based recommendation models. Unlike previous methods like GRU4Rec, IRN focuses on modeling purchase behavior rather than next-click prediction. IRN outperforms other models on real-world datasets by effectively capturing users' sequential behavior and historical interests. The Imagination Reconstruction Network (IRN) enhances purchase intentions by modeling purchase behavior and incorporating off-policy experience. IRN outperforms other models by capturing users' sequential behavior and historical interests. The proposed IRN enhances purchase intentions by incorporating off-policy experience and modeling users' sequential behavior and historical interests. It utilizes trajectories as additional training signals for self-supervised reconstruction, without requiring external supervision from the environment. The proposed IRN enhances purchase intentions by incorporating off-policy experience and modeling users' sequential behavior and historical interests through self-supervised reconstruction on internal imagined trajectories. The agent receives rewards based on user feedback after recommending items, and aims to find an optimal policy for maximizing returns. The paper introduces the Asynchronous Advantage Actor-Critic (A3C) algorithm, which improves upon the A3C approach by adjusting the policy and value function networks towards the bootstrapped k-step return. The value function is updated using the Bellman Equation to minimize the error between the target return and the current estimate. A3C applies updates to increase the probability of rewarding actions. The A3C algorithm involves multiple agents interacting in parallel, with asynchronous execution to accelerate learning. It combines session-parallel mini-batches to gather samples and update policy and value networks. An LSTM is used to approximate policy and value function based on previous interactions. The imagination reconstruction module is incorporated into model-free agents like A3C. The imagination reconstruction module is integrated into model-free agents like A3C to improve data efficiency and learning performance under sparse rewards. It consists of three components: the imagination core predicts future time steps, the trajectory manager determines rollout strategies, and the Imagination-augmented Executor updates action policy by combining internal and external data for reward optimization. The Imagination Reconstruction Module (IRM) is integrated into model-free agents like A3C to enhance data efficiency and learning performance under sparse rewards. It includes three key components: the imagination core predicts future time steps, the trajectory manager determines rollout strategies, and the Imagination-augmented Executor updates action policy for reward optimization. The Imagination Reconstruction Module (IRM) enhances data efficiency in model-free agents by predicting future time steps and updating action policies for reward optimization. It utilizes a static environment model derived from state transitions and self-supervised reconstruction of I2E using imagined rollouts. The imagination policy \u21e1 is made more robust to errors by generating accurate actions during training. The Imagination Reconstruction Module (IRM) uses a shared target network to imitate policy and learn rewarding states. It relies on environment models to simulate imagined trajectories and train in an unsupervised fashion. However, model errors can lead to poor agent performance and require extra computational resources. The Imagination Reconstruction Module (IRM) utilizes environment models to simulate imagined trajectories and encourage exploration through imagination rollouts. However, model errors can result in decreased agent performance and additional computational costs. The Imagination Reconstruction Module (IRM) uses environment models to simulate imagined trajectories and improve exploration through imagination rollouts. It enhances the action policy to generate more accurate actions and can be obtained through policy distillation or fixed target network. This approach helps the agent learn from imagined trajectories similar to real experiences, aiding in sparse reward signals. The Imagination Reconstruction Network 7 (IRN) is a simple architecture inspired by Prediction Error Minimization. The Trajectory Manager (TM) determines how to roll out the Imagined Consequences (IC) and produces imagined trajectories of an observable world state. The Imagination-augmented Executor (I2E) combines internal and external data to update its action policy. IRN distills a roll-out policy from the action policy and uses imaginations as self-supervised reconstruction signals. IRN uses imaginations as self-supervised reconstruction signals to balance intrinsic imagination error and extrinsic purchase probability, improving predictive representations and action policy. It enhances purchase and click performance, even without click rewards, and is robust to cold-start scenarios. Compared to sequential recommenders like RNN or Markov chains, IRN achieves better performance on real-world datasets. In real-world applications, users' current interests are influenced by their historical behavior. IRN balances intrinsic imagination error and extrinsic purchase probability to improve predictive representations and action policy. It enhances purchase and click performance, even without click rewards, and is robust to cold-start scenarios. IRN improves data efficiency and can be used as a self-supervised reconstruction signal for action policy. IRN can enhance purchase and click performance by balancing intrinsic imagination error and extrinsic purchase probability. It can improve predictive representations and action policy, even without click rewards. IRN uses imaginations as a self-supervised reconstruction signal for action policy, resulting in a more robust approach. Our method achieves better performance on real-world datasets compared to state-of-the-art methods. Users' current interests are influenced by their historical behaviors, and the imagination-augmented Executor aggregates data to update its strategy. The imagination-augmented Executor (I2E) optimizes its action policy by maximizing extrinsic rewards and minimizing intrinsic imagination errors. It encourages exploration and adapts to user interests by simulating imagined trajectories using environment models. However, model errors lead to poor agent performance and require additional computational costs. The imagination-augmented Executor (I2E) optimizes its action policy by simulating imagined trajectories using environment models. This approach helps in exploration and adaptation to user interests but can lead to model errors and increased computational costs. The Imagination-augmented Executor (I2E) optimizes its action policy by simulating imagined trajectories using environment models, promoting exploration and adaptation to user interests while minimizing errors and computational costs. The Imagination-augmented Executor (I2E) utilizes environment models to simulate imagined trajectories, improving action policy optimization and reducing computational costs. The network action policy is transformed into a smaller rollout network, utilizing shared parameters. The Imagination Core predicts future states, while the Trajectory Manager controls the granularity of imagined trajectories. The Imagination-augmented Executor optimizes the network using internal imagination data and external rewarding data. In this work, a static environment model is derived from state transitions to improve agent performance. The model uses imagined rollouts to generate item predictions for self-supervised reconstruction, making the action policy more robust to errors. The imagination policy \u03c0 is obtained from policy distillation or a fixed target network like DQN. By imitating the action policy \u03c0, imagined trajectories will be similar to agent experiences in the real environment, helping in learning predictive representations of rewarding states. The Trajectory Manager rolls out the IC over multiple time steps into the future, generating multiple imagined trajectories. The Trajectory Manager generates multiple imagined trajectories for trajectory simulation, supporting breadth-n and depth-m planning strategies. Breadth-n imagination focuses on short-term events, while depth-m imagination plans for the long-term future, balancing long-term rewards. The Trajectory Manager generates n trajectories with depth m to balance long-term and short-term rewards. It uses external rewarding data and internal imagined trajectories to update its action policy. A multi-step reconstruction objective is defined using mean squared error, with action representation learning being crucial for performance. The policy \u03c0 is optimized using imagination reconstruction and A3C updating to predict purchases accurately and minimize reconstruction errors of imagined items. The overall reconstruction loss for a session is defined as L IRN = q t=0 n j=1 L j (s t ). Imagination reconstruction offers several advantages, such as balancing long-term and short-term rewards and improving action representation learning for performance. Imagination reconstruction (IAE) has advantages like providing signals for reward augmentation, enabling exploration and exploitation, and reconstructing imagined trajectories for predictive learning. It can be seen as goal-oriented or semi-supervised learning, achieving good performance even without external rewards. The proposed model for training uses counterfactual predictions and predictive perception. It is evaluated on the ACM RecSys 2015 Challenge 2 dataset, focusing on click-streams ending with purchase events. Purchase and click rewards are set at 5 and 1 respectively. Recent events are found to be effective, so the model collects the latest one month of data. Preprocessing steps are followed, and the model is tested on sessions from the last three days. The training set consists of 72274 sessions and 683530 events, while the test set has 7223 sessions and 63100 events. The evaluation is done by incrementally adding the previous observed event to the session and checking the rank of the next event. Recall and Mean Reciprocal Rank (MRR) are used for top-K evaluations, with K set to 5 for both metrics. The environment is built using session-parallel mini-batches, where the agent interacts with multiple sessions simultaneously. Various baseline agents are chosen for comparison, including BPR and GRU4Rec. The evaluation involves incrementally adding previous events to sessions and checking the rank of the next event using Recall and Mean Reciprocal Rank (MRR) metrics. Various baseline agents are compared, including GRU4Rec BID9, CKNN BID12, A3C-F, A3C-P, IRN-F, IRN-P, and PRN-P. The architecture utilizes LSTM and hyperparameters are tuned using grid search on the validation set. The implementation of IRN is done through Tensorflow and will be publicly released upon acceptance. The model utilizes a LSTM with 256 units and two fully connected layers to predict value function and action probabilities. The discounting value is 0.99, and the imagination policy is updated using a fixed target network. The A3C updating includes immediate purchase rewards or 3-step returns. The weights of the Imagination Reconstruction Network are initialized using Xavier-initializer and trained with Adam optimizer. The weights of IRN are initialized using Xavier-initializer and trained via Adam optimizer. Evaluation shows A3C-P outperforms classical recommenders on Recall metrics. IRN-P improves purchase performance through imagination reconstruction. IRN-P consistently outperforms IRN-F and A3C-P outperforms A3C-F for purchase prediction. Comparing PRN-P with A3C-P and IRN-P, reconstructing previous actual trajectories improves purchase performance. IRN-P outperforms PRN-P as it promotes more robust policy learning through semi-supervised learning. GRU4Rec achieves excellent next-click performance compared to BPR and CKNN. A3C-F outperforms A3C-P and GRU4Rec, indicating that RL-based recommenders trained on clicks can better preserve sequential properties. IRN-P significantly outperforms A3C-P, showing the ability to reconstruct previous clicked trajectories even with only purchase rewards. A3C-F and IRN-F are robust to varying purchase reward density, as purchases are sometimes included in click sequences. The proposed IRN-P model shows improved performance in purchase prediction even without external rewards, utilizing imagination reconstruction to learn sequential patterns in an unsupervised manner. Performance comparison on different reward sparsity settings demonstrates gradual improvement, with IRN-P learning faster with a larger d parameter. The IRN-P model's performance improves with a larger d parameter, indicating better exploration and exploitation. Different planners equip the agent with varying prediction capacities, with IRN-P showing better results on First and Click metrics with a larger n. The combination of breadth-n and depth-m planning in IRN-P balances short-term and long-term rewards. Depth-2 planner performs better than depth-1 and breadth-2 without external rewards, possibly due to predictive representations. However, longer imagined trajectories do not improve purchase performance in IRN with purchase reward. The IRN-P model learns to predict future events recursively by capturing differences in input states and accurately predicting future purchase signals. It shows robustness in a cold-start scenario, outperforming other models like A3C-P and A3C-F. The model can guess user preferences and improve user profiles, with A3C-F achieving slightly better results than A3C-P. A3C-F achieves slightly better results than A3C-P in preserving sequential session properties and providing implicit information under the cold-start setting. This adaptation to user interest demonstrates improved data efficiency and quick adaptation in online learning scenarios."
}