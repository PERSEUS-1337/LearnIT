{
    "title": "SylR-CEKDS",
    "content": "We propose a neural program generation framework for modeling human question asking, representing questions as formal programs and generating them with a deep neural network. Our method can ask optimal questions in synthetic settings and predict human questions in unconstrained settings. Additionally, we introduce a grammar-based question generation framework trained with reinforcement learning to generate creative questions without supervised data. Machine learning has been used to address challenges in representing and searching for questions. Traditional methods use heuristic rules, while neural network approaches select questions from past experience or map inputs to questions. These approaches are limited in generating diverse questions and do not consider partially unobservable states. Recent work in question generation aims to bridge the gap between human and machine question asking by incorporating aspects of cognitive science. While some approaches simulate potential answers to questions, others model questions as symbolic programs. Our approach utilizes neural program generation to combine symbolic program generation with deep neural networks. Our approach combines symbolic program generation with deep neural networks to synthesize questions efficiently. Symbolic programs offer a precise and structured language for question synthesis, while neural networks enable rapid question generation through encoder-decoder modeling. This integration aims to leverage the strengths of both approaches in question asking systems. Our approach combines symbolic program generation with deep neural networks for rapid question synthesis using encoder-decoder modeling. This eliminates the need for expensive symbolic search and feature evaluations. The questions can be quickly synthesized and formally evaluated for quality. The Battleship task involves asking questions to seek information about the hidden board, which can be used to train question asking systems using reinforcement learning. The paper develops a neural model for generating questions in an information-search game similar to \"Battleship\". The model uses a convolutional encoder and Transformer decoder to generate questions through a grammar-enhanced framework. It can be trained from human demonstrations or through reinforcement learning. The paper introduces a neural network model for generating human-like questions using reinforcement learning. It evaluates the model on various aspects of question asking behavior and proposes a novel framework for generating creative questions. Previous research focused on rule-based methods, while recent approaches utilize deep neural networks for question generation. Methods for question generation using deep neural networks, particularly the encoder-decoder framework, can generate questions without manual rules. These data-driven methods map inputs to questions through pattern recognition. While some aspects of human question-asking remain challenging, recent research draws inspiration from cognitive science to address this issue. Research by Rothe et al. (2017) and Lee et al. (2018) generate questions based on goal-oriented metrics, with this paper extending Rothe et al.'s work to overcome limitations. This paper extends Rothe et al.'s work on generating goal-oriented programs with neural networks, building on neural network approaches to program synthesis. Various models incorporate Deep Reinforcement Learning to optimize generated programs for specific tasks like SQL queries, Excel sheet translations, 3D scene construction, and object understanding. Recent work has focused on incorporating grammar information into program synthesis processes. Yin & Neubig (2017) designed a module to capture grammar information for generation. Some papers have proposed network architectures for program synthesis, using Transformer decoders and neural networks to encode grammar. Si et al. (2019) utilized Deep Reinforcement Learning for optimizing generated programs. Incorporating grammar into program synthesis processes, Si et al. (2019) used DRL to encourage semantically correct program generation. This work aims to generate human-like questions in a new domain, integrating grammar information to directly generate programs. The task involves an information search game called \"Battleship\" with three colored ships on a 6x6 grid. The game board is a 6x6 grid with ships that are 2, 3, or 4 tiles long. Players flip tiles to reveal ship colors or water. The goal is to determine ship configurations with the least flips. In a modified version, players ask questions to gain information about the ships. Computational models are tasked with generating questions about the game board. The game board is a 6x6 grid with ships of varying lengths. Players reveal ship colors or water by flipping tiles to determine ship configurations with the least flips. In a modified version, players ask questions to gain information about the ships. Computational models, such as the one designed by Rothe et al. (2017), use a context-free grammar to generate questions in the Battleship domain. This grammar, represented in a LISP program-like format, captures a wide range of questions and can generate an infinite set of possible questions beyond collected human questions. The neural network used includes a Convolutional layer. The neural network used for encoding the game board includes a Convolutional Neural Network (CNN) and a Transformer decoder. The input is a binary representation of the 6x6 game board with five channels, each representing a color. The CNN maps the input to an encoder output, which is then fed into the Transformer decoder. The model is compatible with both supervised and reinforcement training. The model is compatible with both supervised and reinforcement training. In supervised training, the goal is to model the distribution of questions in the training set using symbol-level cross entropy loss. The model can generate questions symbol-by-symbol from left to right by predicting the probability of each symbol at each step. In sequence-based RL, the framework can be adapted to generate a sequence of symbols without constraints. The framework can generate a sequence of symbols without stepwise supervision, using a grammar-enhanced RL training procedure. A Markov Decision Process is used to choose production rules for generating a program, solved with DRL. The process repeats until only terminals are in the sequence. The framework uses a leftmost derivation to replace non-terminals in the sequence until only terminals remain. Three tasks were designed to test the model's ability to learn compositional rules and reasoning strategies, including counting visible ship tiles and locating missing ship tiles. Models generate programs based on the tasks, with training and testing examples provided. The framework includes tasks for testing compositional rules and reasoning strategies. One task involves selecting the missing tile on a ship by generating a response in a specific format. Another task requires combining strategies to find the missing tile on the ship with the least visible tiles. The tasks evaluate compositionality by withholding certain question types from training. The model is trained on various question types with different amounts of training data to test generalization. Results show high accuracy for counting and missing tile tasks. Ablation analysis of the model's encoder filters is also performed. The model struggles to generalize to new question types without training data. The model struggles to generalize to new question types without training data, but quickly adapts with more training data. Comparing with a model using different operations, it shows potential to generalize to unseen scenarios by decomposing tasks. The experiment tests if the neural network can capture human question distributions. The neural network captures human question distributions by training on paired game boards and questions sampled from a computational model. Game boards are generated with randomly placed ships, and questions are sampled using importance sampling based on a cognitive model. The model uses an energy function to estimate the likelihood of a question being asked by a human, considering features like informativeness and complexity. Pretraining the decoder with a larger set of questions helps capture grammatical structure. Evaluation includes log-likelihood of reference questions generated by the full model and lesioned variants. Details on hyperparameters, training, and pre-training are provided in the appendix. The model uses an energy function to estimate the likelihood of human questions, considering features like informativeness and complexity. The convolutional encoder is replaced by a simple MLP encoder, and a model with only a decoder is used. Evaluation involves log-likelihood of reference questions generated by the full model and lesioned variants. Two different evaluation sets are used, one sampled from the same process on new boards, and the other a small set of questions collected from human annotators. The model's performance is evaluated using a generative model, with results indicating that pretraining, Transformer decoder, and convolutional encoder are crucial. Surprisingly, a model without an encoder performs well, even outperforming the full model with an LSTM-decoder on human questions. Contextual information from the board leads to improvements, but it is not the most critical factor for predicting human questions. Further analysis is conducted to explore the role of contextual information in the model's effectiveness. The board's importance is highlighted in scenarios divided by entropy levels. High entropy reduces encoder importance, while low entropy shows higher model performance with board access. The model captures context-sensitive question characteristics, crucial for experiments like reinforcement learning framework evaluation. In this experiment, a reinforcement learning framework is evaluated for generating novel questions without a large training set. The reward for training the agent is based on the energy value of the question, transformed to a proper reward range. The model is optimized using the REINFORCE algorithm, with a baseline established as the average reward in a batch. Manual tuning of the energy function parameter is done to enhance information-seeking. Models are compared based on their ability to generate diverse questions with high expected information gain. The experiment evaluates a reinforcement learning framework for generating questions with high expected information gain. The framework is compared with a text-based model and a supervised program-based model. A sequence-based reinforcement learning agent is also implemented, which requires pretraining for 500 epochs. The models are evaluated on 2000 randomly sampled boards, with results shown in Table 3. The experiment evaluates a reinforcement learning framework for generating questions with high expected information gain. The text-based model generates questions included in the training data, but the supervised program-based model performs better. Using programs can overcome limitations of text-based training data, creating a larger dataset for self-supervised training. This approach, combined with grammar-enhanced RL, boosts performance in generating informative and creative questions. The grammar-enhanced model can generate novel questions with high EIG, surpassing the supervised and RL models. It produces diverse questions, including clever ones like \"Where is the bottom right of all the purple and blue tiles?\" and some ungrammatical or meaningless ones. Additional examples are provided in the supplementary materials. The grammar-enhanced model can generate diverse questions with high EIG, surpassing supervised and RL models. Examples of generated questions are provided in Appendix B, showcasing different types of queries that can be asked. The model can generate various types of questions, including true/false, number, location-related, and compositional questions. It can adapt to new constraints and produce meaningful questions. A comparison with human-generated questions shows that the model can generate clever and human-like questions, although it struggles with questions involving quantifiers like \"any\" and \"all\". The model can generate creative human-like questions using a neural program generation framework. It has limitations in generalizing to different scenarios and sometimes generates meaningless questions. Future work will focus on exploring the model's compositional abilities. Future work will focus on exploring the model's compositional abilities, modeling question asking and answering jointly, allowing iterative question asking in games, and using the framework in dialog systems and open-ended question scenarios. The encoder uses a simple CNN with one layer of filters to encode the board, capturing position-sensitive information and translation-invariant patterns. The encoder utilizes three convolution operations with different filter sizes to process input data, ensuring feature maps have the same dimensions. The outputs are concatenated and passed through a linear projection to obtain the encoder output. The decoder, based on the Transformer model, processes input sequences to generate output. The Transformer model decoder computes hidden states through stacked Decoder Attention Layers, each consisting of self-attention, attention over the encoded input, and a feed-forward network. Residual connections and layer normalization are used. The final output layer transforms hidden states to output vectors. The Transformer model decoder utilizes multi-head attention mechanisms with inputs projected using different matrices. It has 2 layers, each with 4 heads, and is trained for 500 epochs using the Adam optimizer. The model encoder has C out = 10, L = 50, and each word is embedded with 50 dimension vectors in the decoder. The attention module in the model has 4 heads and is trained for 500 epochs with an Adam optimizer. A version of the model replaces the decoder with linear transformations to predict specific outputs. The model encoder remains the same as in the first experiment, but the decoder size is increased with 3 layers and 6 heads. The model is trained for 500 epochs with the same optimizer. Additionally, the decoder is pretrained for 500 epochs on a larger set of questions to improve grammar familiarity. The decoder is pre-trained for 500 epochs on a larger set of question programs sampled from a PCFG grammar. Some simple questions become complicated in program form, affecting the evaluation set. To address this, the last 10% of human questions with low energy values are removed for a more robust evaluation. The neural model for the experiment has the same hyper-parameters as in the last one, optimized by the REINFORCE algorithm. An -greedy strategy is applied to encourage model exploration. The model is trained for 500 epochs, passing 10,000 different boards each epoch. The models show a preference for generating similar programs of low complexity, so parameters of the energy model are tuned accordingly. The sequence RL baseline is trained with the MIXER algorithm, a variant of the REINFORCE algorithm, to generate a diverse set of questions. The model is pretrained for 500 epochs and trained with RL for 300 epochs, with grammar rules provided in tables 4 and 5. Table 4 contains grammatical rules for the model, with specific references to the Battleship game board for evaluation. The curr_chunk discusses various functions related to sets, locations, and colors in the context of a game. It also mentions an ablation test on a neural network in the first experiment. The full neural program generation model demonstrates strong reasoning abilities with 99.80% accuracy for the counting task and 95.50% for the missing tile task. Weakened variants with only one filter size show a significant drop in performance on the missing tile task. The 3x3 convolution filters excel in selecting the correct color but struggle with choosing the right tile, while the 1x1 convolution filters perform poorly in both aspects. In the current architecture, predicting the correct location requires precise information that seems to be lost without filters of different sizes. Table 7 provides a full table of log-likelihood of different models on evaluation set of different uncertainty levels. Examples of questions generated by the models are shown in Experiment 5.3. Figures 5, 6, 7, and 8 display examples of generated questions and text-based model outputs."
}