{
    "title": "Hygxb2CqKm",
    "content": "In this work, stable recurrent neural networks are investigated. Theoretical analysis shows that stable recurrent models can be well approximated by feed-forward networks for both inference and training. Empirical results demonstrate that stable recurrent models perform comparably to unstable ones on sequence tasks, suggesting that sequence learning can occur effectively in the stable regime. This sheds light on the power of recurrent networks and explains why practitioners can often replace them with feed-forward models. Recurrent neural networks are commonly used for sequence learning in speech recognition and natural language processing. Despite the importance of stability in dynamical systems, it has not been a focus in the practice of recurrent neural networks. The difficulty in training these models has led to the successful replacement of recurrent models with non-recurrent, feed-forward architectures. This raises questions about the inherent instability of sequence modeling and the necessity of recurrent models. In this work, stability in recurrent models is investigated. It is shown that stable recurrent models can be approximated by feed-forward networks, implying that unstable models have exploding gradients. Recurrent models can often be made stable without performance loss, and nominally unstable models operate in the stable regime on the data distribution. Stable recurrent models can be approximated by feed-forward networks, implying that unstable models have exploding gradients. Nominally unstable models often operate in the stable regime on the data distribution. This sheds light on why empirical research is replacing recurrent models with feed-forward models in various applications. Stable models cannot have long-term memory, which is important for tasks that require it. Stable recurrent models can be approximated by feed-forward networks, ensuring stability and performance comparable to unstable models. New conditions for stability of LSTM networks are provided, along with an efficient projection operator for training. Extensive experimentation on sequence benchmarks shows the effectiveness of stable models. In this section, stable recurrent models are defined and illustrated for various popular model classes. Stability in recurrent models is related to the gradients of the training objective not exploding over time. Common recurrent models can operate in both stable and unstable regimes based on their parameters. Sufficient conditions for stability are provided, and methods to enforce these conditions during training are described. A recurrent model is a non-linear dynamical system with a hidden state evolving in discrete time steps according to an update rule. The update rule DISPLAYFORM0 for a recurrent neural network involves weight matrices W and U, with the state evolving according to DISPLAYFORM1. Stable recurrent models are defined as models where the gradients do not explode over time, making them suitable for gradient descent training. A stable recurrent model is characterized by the existence of a parameter \u03bb < 1. Stable recurrent models are well-behaved and justified theoretically. Unstable models can have exploding gradients, making convergence to a stationary point difficult. An example is given where gradient descent fails to converge. Sufficient conditions for stability in common recurrent models are provided. Sufficient conditions for stability in recurrent models involve imposing stability conditions via projection after each iteration of gradient descent. For linear dynamical systems and recurrent neural networks with a Lipschitz nonlinearity, stability is ensured if the model parameters satisfy certain constraints. Long Short-Term Memory (LSTM) networks are a commonly used class of sequence models. The state is a pair of vectors, and the model is parameterized by matrices. Conditions are provided for the system to be contractive in the \u221e norm, implying stability in the 2 norm for certain values of r. The iterated system \u03c6 r LSTM is contractive in the 2 norm for r = O(log(d)). The proof is in the appendix. Despite restrictive conditions, models meeting stability criteria perform well in experiments. Finding different parameter regimes for stability remains an open problem. Training involves row-wise normalization of weight matrices and inputs. Stable recurrent models can be approximated by feed-forward networks. Stable recurrent models can be approximated by feed-forward networks for both inference and training. The equivalence between the two models in terms of memory usage has important implications for sequence modeling. If a stable recurrent model performs well, a feed-forward network can achieve similar results, suggesting that recurrent models may not always be necessary for sequence learning. The key difference between the two models is that feed-forward models have finite-context. The feed-forward model is a simplified approximation of a stable recurrent model, using a sliding window of inputs to make predictions. The truncated model is implemented as a depth-k feed-forward network with weight sharing, mapping a state to outputs. This approach suggests that recurrent models may not always be necessary for sequence learning. The truncated model is a depth-k feed-forward network that maps a state to outputs. It is shown that stable models do not have long-term memory, and distant inputs do not change the system's states. The proof in the appendix shows that long-term memory and distant inputs do not affect the system's states. If the prediction function is Lipschitz, both recurrent and truncated models make similar predictions. Gradient descent for stable recurrent models yields similar solutions as for truncated models, indicating their predictions are essentially the same. The difference in weights between recurrent and truncated models is limited by \u03b5 in gradient descent. Even if gradients are similar, slight differences can accumulate over time, leading to divergent weights. However, gradient descent is stable, preventing this divergence. Lemmas bound the gradient difference and establish Lipschitz gradient maps for both models. Loss functions for both models are evaluated in a stable domain, ensuring stable predictions. The text discusses the stability of gradient descent in the context of recurrent and truncated models. Lemmas bound the gradient difference and establish Lipschitz gradient maps for both models, ensuring stable predictions in a compact, convex domain. The difference in weights between the models is limited by \u03b5 in gradient descent, preventing divergence over time. The decaying step size in the theorem is consistent with stable gradient descent for non-convex objectives. Empirical evidence shows the necessity of the O(1/t) rate for the theorem, with examples of stable systems trained with different rates. The bound in Proposition 4 approaches 0 as k \u2192 \u221e, leading to the main theorem for Lipschitz and smooth functions. After N steps of projected gradient descent with step size \u03b1 t = 1/t, stable recurrent models show solid performance on various sequence tasks like word-level language modeling and polyphonic music modeling. Unstable models can be made stable without performance loss, indicating a potential \"price of stability\" in data-dependent scenarios. In language modeling, models predict the next word or character in a sequence. Character-level models are trained on Penn Treebank BID14, while word-level models are trained on the larger Wikitext-2 dataset. Performance is measured using bits-per-character and perplexity. In polyphonic music modeling, models predict the next 88-bit binary code in a sequence representing piano keys. Models are evaluated on JSB Chorales dataset by J.S. Bach, with performance measured using negative log-likelihood. 382 harmonized chorales by J.S. Bach BID0. Performance is measured using negative log-likelihood. Slot-filling model for Airline Travel Information Systems (ATIS) benchmark, with F1 score reported for unconstrained RNN and LSTM models. Models are trained using plain SGD, then retrained with projected gradient descent for stability. In the RNN and LSTM models, stability is enforced using gradient descent without retuning hyperparameters. W is constrained to be less than 1 in RNNs, and weight matrices are normalized in LSTMs to ensure stability. Stable and unstable models show similar performance across various tasks, with stable models achieving comparable results to published baselines. However, there is a performance gap between stable and unstable LSTM models in language modeling tasks. Stability in LSTM models is crucial, with a gap existing between stable and unstable models in language modeling tasks. The restrictive conditions imposed on LSTMs for stability surprisingly allow them to perform well, but weaker stability conditions could narrow this gap. However, there may be a trade-off in representational capacity for some tasks. The question arises whether there is an intrinsic performance cost for using stable models on certain tasks. Data-dependent measures of stability show that unstable LSTMs can be stable, suggesting the gap may be illusory. In some cases, instability can offer modeling benefits, especially with short sequences. Our conservative notion of stability requires it to hold for every input and hidden state pair, but a weaker, data-dependent notion could be considered. In contrast to the conservative notion of stability in LSTM models, a weaker, data-dependent definition is considered. This definition focuses on stability parameter estimation using input sequences from the data and evaluating stability on hidden states reachable via gradient descent. The data-dependent measure serves as a useful diagnostic to determine if the model is operating in a stable regime, especially when the input representation is fixed during training. In FIG3 (a), the iterated character-level LSTM and word-level language model are stable for around 80 and 100 iterations, respectively. Even \"unstable\" models operate in a stable regime on the data distribution, offering performance improvements for short-time horizons. Training unstable models is less difficult with short sequences, as exploding gradients are less of an issue. Truncated unstable models show performance gains on the polyphonic music task. In short sequences, the final model is more unstable with a stability parameter \u03bb \u2248 3.5, leading to better test-likelihood. For longer sequences, \u03bb decreases towards stability (\u03bb \u2248 1.5) and the test-likelihood improvement disappears. Unstable LSTMs in language modeling are stable in a weaker, data-dependent sense. However, for polyphonic music modeling with short sequences, instability can enhance model performance. Previous evidence suggests that nominally unstable models can exhibit stability in a data-dependent manner. This further supports the idea that unstable models operate in a stable regime, explaining why stable and unstable models perform similarly. Unstable RNNs and LSTMs exhibit vanishing gradients in language and music modeling tasks. Both models show limited sensitivity to distant inputs during training, with LSTM gradients vanishing more slowly than RNNs. Truncating unstable models leads to diminishing returns in both tasks. In language and music modeling, RNNs and LSTMs show diminishing returns for large truncation parameter k. Larger k doesn't affect performance in LSTMs, while for unstable RNNs, it slightly decreases performance. Proposition (4) holds for unstable models, with the distance between weight matrices decreasing slowly as k becomes large. Empirical results for unstable word-level language models confirm this. Training the full recurrent model is impractical, so k = 65 is assumed to capture it well. The text discusses the diminishing returns of using larger truncation parameter k in RNNs and LSTMs for language and music modeling. It mentions Proposition 4, which shows that the distance between weight matrices decreases slowly as k increases for unstable models. The experiments suggest that stable recurrent models can be approximated by feed-forward networks. Recurrent networks can be approximated by feed-forward networks, but the necessity of recurrent models is still debatable. Truncated recurrent architectures like LSTMs may offer useful inductive bias, although implementing the approximation as a feed-forward network increases the number of weights. It is essential to find more parsimonious feed-forward approximations and prove that natural feed-forward models can approximate stable recurrent models during training. Learning dynamical systems with gradient descent is a recent topic of interest in the machine learning community. Our work explores the theoretical and empirical consequences of stability assumptions in learning stable linear and non-linear dynamical systems using gradient descent. We show that models trained in practice can closely resemble theoretical analyses without significant performance penalties. Additionally, we extend previous results on truncation for linear systems to the non-linear setting and analyze the impact of truncation on training. Our work extends stability analysis in learning linear and non-linear dynamical systems using gradient descent. We provide new conditions for stability in RNNs and LSTMs, considering the impact of truncation on training and feed-forward approximation. Recent works have focused on ensuring isometry in the system to avoid vanishing and exploding gradients. BID1 BID27 BID10 BID18 BID11 propose maintaining W \u2208 [1 \u2212 \u03b5, 1 + \u03b5] instead of strictly requiring W = 1 for improved performance. BID28 uses a \"soft-isometry\" constraint based on SVD to stabilize training without the need for projection. BID12 stabilizes training with a residual parameterization. BID13 introduce a non-chaotic recurrent architecture that performs well compared to LSTMs. BID2 evaluate recurrent and convolutional feed-forward models on various sequence tasks, finding feed-forward models effective in diverse settings. In diverse settings, feed-forward models outperform recurrent models on sequence modeling tasks. Stable recurrent models can often replace recurrent models, equivalent to feed-forward networks. Proof of Proposition 1 shows the stability of a linear dynamical system with specific parameters and input-output sequences. The gradient of the system is shown to grow exponentially for large T if |a| > 1. Iterates from gradient descent with step size \u03b1 i = (1/i) diverge if a 0 is initialized outside of [\u22121, 1]. The model is Lipschitz in U, W and B U Lipschitz in x, and smooth with respect to h. The model is Lipschitz in U, W, and B, and smooth with respect to h. DISPLAYFORM10 is Lipschitz in h and w. The state-transition map is not Lipschitz in s unless c is bounded. Lemma 4 proves weights are bounded. Proposition 2 shows \u03c6 LSTM is \u03bb-contractive. Hidden states are bounded, and \u03c3 is Lipschitz. Lemma 1 proves that the model is contractive in the infinity norm. The initial state is assumed to be zero, and the map is stable within a compact, convex domain. The weights are bounded, and the state-transition map is Lipschitz if the state is bounded. The text discusses stability and Lipschitz assumptions in the context of hidden states at different time steps. It introduces smoothness conditions for the map \u03c6 w and argues that the difference in gradient between recurrent and truncated models is negligible for large k. The proof leverages the \"vanishing-gradient\" phenomenon. The text discusses stability and Lipschitz assumptions in the context of hidden states at different time steps, introducing smoothness conditions for the map \u03c6 w. It argues that the difference in gradient between recurrent and truncated models is negligible for large k, leveraging the \"vanishing-gradient\" phenomenon. The proof focuses on bounding the long-term and short-term components of the gradient for the recurrent model. In this section, the text proves the Lipschitz property of the gradient map \u2207 w p T by showing that small differences in weights do not significantly alter the trajectory of the recurrent model. It leverages smoothness conditions and the \"vanishing-gradient\" phenomenon to bound the difference in gradients between models with different weights. The text proves the Lipschitz property of the gradient map \u2207 w p T by showing that small differences in weights do not significantly alter the trajectory of the recurrent model. It leverages smoothness conditions and the \"vanishing-gradient\" phenomenon to bound the difference in gradients between models with different weights. The proof involves geometric series and bounding terms using the triangle inequality. The text presents the proof of the main gradient descent result, utilizing smoothness and truncation lemmas. It involves the Euclidean projection onto \u0398 and recurrence relations for \u03b4 i+1. By applying lemmas and bounding terms, the proof concludes with the result after N steps of gradient descent. The key result in the proof involves bounding the parameter difference while running gradient descent. The error scales comparably with the bound given in Proposition 4, showing the necessity of the O(1/t) rate condition. Larger step-sizes lead to the failure of the bound, indicating the importance of the O(1/t) condition. Parameters are set for the gradient steps to ensure the bound from Proposition 4 holds. The experiments are conducted using public code from BID16 and BID2, with hyperparameters shared between stable and unstable models. Theoretical results apply to single-layer networks trained with vanilla SGD. Stability enforcement in RNN is straightforward but computationally intensive. Enforcing stability in RNN is computationally expensive but conceptually simple, while in LSTM it is conceptually more difficult but computationally simple. In RNN, stability is maintained by ensuring W < 1, while in LSTM, inequalities are enforced after each gradient update to ensure stability."
}