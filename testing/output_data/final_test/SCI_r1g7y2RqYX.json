{
    "title": "r1g7y2RqYX",
    "content": "Graph networks have gained interest for semi-supervised learning by generating node representations propagated in weighted graphs. A differentiable neural version of Label Propagation is proposed for propagating labels in the graph and learning edge weights. Non-linear steps enhance the label-propagating mechanism. The utility of our approach is demonstrated in experiments conducted in two distinct settings. In the context of graph-based semi-supervised learning, methods often rely on the assumption of smoothness, where adjacent nodes are likely to have similar labels. This smoothness is encouraged by optimizing an objective with a loss term over labeled nodes augmented with a quadratic penalty over edges. This approach, known as Laplacian Regularization, has been widely adopted in early SSL methods. Recent deep learning methods have addressed limitations in traditional algorithms like Label Propagation by offering intricate predictive models trained discriminatively. However, many still require edge weights as input, despite the importance of good weights highlighted in previous work. Some methods consider weight learning, but have moved away from the original quadratic criterion. Other works propose disciplined ways for learning weights. Our goal is to address issues in learning predictive models and edge weights simultaneously. We propose a framework that uses Label Propagation as a differentiable neural network, allowing for efficient optimization of weights using gradient descent. Labeled information is used as input to both the loss and the network, distinguishing our approach from current methods. Our network's hidden layers directly propagate labeling information, using an information-gated attention mechanism and a \"bifurcation\" operator to control label convergence. The model is tailored for the semi-supervised setting, with few parameters and one model-specific hyper-parameter (depth). Suitable for tasks with few labeled nodes. The final network, with a model-specific hyper-parameter (depth), is designed for tasks with few labeled nodes. It outperforms strong baselines in experiments on benchmark datasets in different learning settings. Many SSL methods are based on similar quadratic forms but differ in input assumptions, optimization objectives, and prediction forms. Classic methods like LP BID51 do not assume a parametric form for predictions and require edge weights as inputs. Other propagation methods relax prediction constraints or add regularization terms. Some methods focus on learning edge weights and modeling relations between the graph and features, while others optimize for accuracy with specific parameterizations. Recent works in deep learning have explored graph inputs for inductive SSL, utilizing weighted graphs to create node representations for classification. These methods are designed for scenarios with only a graph input or when node features are available. Various methods have been proposed to utilize node features in graph inputs for inductive SSL. BID38 and BID20 use random walks and SkipGram models to generate node representations, while BID43 focus on optimizing similarities between node embeddings. Spectral methods, including spectral graph convolutions and adaptive convolution filters, have also been explored. Embedding approaches handle bag-of-words representations and general node attributes. These methods propagate features over the graph in different ways. Label propagation is the main advantage in SSL methods, where labeled information is used to generate predictions. The input includes a graph with labeled nodes in a \"seed\" set, and unlabeled nodes. The goal is to predict labels for all unlabeled nodes in the transductive setting. Most methods output \"soft\" labels in a probability simplex. In semi-supervised learning, predictions are made using a graph with labeled nodes and unlabeled nodes. The goal is to predict soft labels for all unlabeled nodes. The input may include features for all nodes but not edge weights. Smoothness across edges is encouraged in predictions through optimization of a quadratic objective. In semi-supervised learning, predictions are made on a graph with labeled and unlabeled nodes. The objective encourages similar predictions for adjacent nodes by minimizing a quadratic term subject to an agreement constraint on labeled nodes. The goal is to learn weights in a discriminative manner rather than assuming them as input. Overcoming difficulties in optimization, a simple algorithm for approximating the solution can be formulated as a deep learning problem. In semi-supervised learning, predictions are made on a graph with labeled and unlabeled nodes. The objective encourages similar predictions for adjacent nodes by minimizing a quadratic term subject to an agreement constraint on labeled nodes. A simple algorithm for approximating the solution can be cast as a deep neural network, with weights optimized using gradient descent. The LP algorithm BID51 approximates the solution using iterative averaging updates, leading to convergence to the optimal solution f * for any initial soft labels f (0) BID51. The method aims to learn weights for f(T) directly instead of f* by designing a neural architecture for iterative updates in label propagation. The model's main component is the label-propagation layer, taking predicted soft labels and true labels as inputs to produce new soft labels. The model's label-propagation layer produces new soft labels by updating nodes based on neighbors' values. Multiple layers with shared parameters are stacked to form a basic network, with the depth being the only hyper-parameter. Each layer acts as an iterative update, making the overall algorithm differentiable and parametrized. In practice, parameterizing H by w can be done using more sophisticated forms. General networks denoted by H(\u03b8) have learned parameters \u03b8. Edge features {\u03c6 e } e\u2208E can be used to parametrize w with linear scores s ij and softmax normalization. Three types of features are proposed: node-feature similarities, graph measures, and path-ensemble features. The curr_chunk discusses edge centrality measures, path-ensemble features, and graph-partitions for generalization across nodes based on edge properties. It also mentions seed relations for quantifying node reliability and introduces novel components for handling distributions in label propagation layers. The general layer replaces weights and inputs with functions of the previous layer's output. The curr_chunk introduces a dynamic edge-weight function and labeling function for improved label propagation. The edge-weight function allocates weights based on node states and neighbors, while the labeling function controls label convergence rates. This approach replaces fixed weights with dynamic weights that change over time, enhancing the adaptability of the model. The curr_chunk introduces dynamic weights that depend on node states and neighbors through an attention mechanism. The attention parameters determine the row-normalized weight matrix, which is computed using specific parameters. The design of the attention mechanism considers the nature of inputs, utilizing negative entropy and KL-divergence to quantify label certainty and similarity. Class-dependent weights are learned to parameterize these measures. The curr_chunk discusses the use of class-dependent weights to direct node attention based on entropy and divergence of neighbors' states. This approach accelerates convergence by bootstrapping confident predictions as hard labels, reducing entropy. The curr_chunk introduces a flexible bifurcation mechanism for dynamically adjusting label entropy. By varying the parameter \u03c4, entropy can be increased or decreased, amplifying confident labels or making labels uniform. This mechanism aims to learn the parameters of the network H(\u03b8; S) by constraining predictions for labeled nodes. The proposed method aims to minimize the leave-one-out loss to adjust label entropy in the network H(\u03b8; S) by comparing true labels to predictions. This approach encourages consistent label propagation while weighting examples by the inverse class ratio. The proposed method minimizes the leave-one-out loss to adjust label entropy in the network, encouraging consistent label propagation while weighting examples by the inverse class ratio. In SSL, training the model on all sets introduces computational overhead, but for small values of lambda, it becomes feasible. When lambda is small, the model can easily overfit, resulting in noisy labels for nodes in U. The graph-SSL literature includes evaluation settings with and without node features. The evaluation of the proposed method in SSL includes two settings: one with node features available and one without. Benchmark datasets with real networked data are used, and a standard SSL setup is followed. Soft labels are generated for nodes using input data, and hard labels are set using argmax. Different Label Propagation Network variants are used with varying edge weight determinations. The evaluation of the proposed method in SSL includes two settings: one with node features available and one without. Benchmark datasets with real networked data are used, and a standard SSL setup is followed. Different Label Propagation Network variants are used with varying edge weight determinations, including bifurcation with linear time-dependency and a-symmetric bi-directional edge weights. LPN was initialized to simulate vanilla LP with uniform weights, and T values were chosen through cross-validation. Training involved class-balanced cross-entropy loss with regularization, optimization with Adam, and the use of LINQS collection datasets. The evaluation of SSL methods includes two settings: one with node features and one without. LPN model uses a linear function with edge features based on node features, graph, and labeled set. Baselines include LP with uniform and RBF weights, ADSORPTION, ICA, GCN, GAT, RIDGEREG, and NODE2VEC. FLIP collection is used for real networks without features, LPN model equipped with attention mechanism for meaningful weights. LPN outperforms other baselines on most datasets, showing improved accuracy when learning weights based on features or using attention. Some deep methods perform well on certain datasets but fail on others due to their complexity and need for more labeled data. LPN requires few parameters and a single model-specific hyper-parameter. Analysis shows that LPN learns good weights based on the Laplacian's eigenvalues, specifically \u03bb 2. Learning leads to weights with increasing \u03bb 2, resulting in improved accuracy. The effect of bifurcation for different depths T is demonstrated, with a model with bifurcation outperforming the same model without it. The LPN model with bifurcation (LPN bif) outperforms the model without it (LPN nobif), especially with larger values of \u03c4 for label convergence rate. LPN nobif degrades with large T, while LPN bif remains robust. The design process focused on learning edge weights and propagating labeled data in a slim, parameterized model. The resulting model is a powerful generalization of the original algorithm, efficient for training with few labeled nodes. The LPN model with bifurcation (LPN bif) outperforms the model without it (LPN nobif), especially with larger values of \u03c4 for label convergence rate. The design process focused on learning edge weights and propagating labeled data in a slim, parameterized model, efficient for training with few labeled nodes. Future work includes exploring parametric update schemes and using Laplacian's eigenvalues for regularization. Our main guideline in choosing features for SSL settings with few labeled nodes is to use a minimal number of features to keep model parameters low. We propose three types of features, including edge features derived from node similarities. However, this approach has limitations such as potential overfitting due to large node feature spaces and the lack of consideration for global graph-dependent properties in edge features. In SSL settings with few labeled nodes, features are chosen to keep model parameters low. Three types of features are proposed, including raw features, graph features, and labeled \"seed\" set features to overcome limitations like potential overfitting and lack of consideration for global graph-dependent properties in edge features. In SSL settings with few labeled nodes, features are chosen to keep model parameters low. Three types of features are proposed: raw features, graph features, and labeled \"seed\" set features. Global roles of nodes are informative features, including node attributes, centrality measures, path-ensembles, and graph-partitions. Seed features involve associating incoming edges with lengths of paths originating from labeled nodes to determine reliability of neighbors as a source of label information. Features should lead to good generalization based on available data, graph type, and layout of labeled set. In SSL settings, features are chosen to keep model parameters low. Three types of features are proposed: raw features, graph features, and labeled \"seed\" set features. Features should lead to good generalization based on available data, graph type, and layout of labeled set. TAB4 provides a list of useful features used in experiments. BID30 |\u0393(u) \u2229 \u0393(v)|/|\u0393(u) \u222a \u0393(v)| Graph Link Prediction Edge Adamic Adar Index BID30 w\u2208\u0393 FORMULA10 x, y denote feature vectors of two different nodes. DEEPWALK, NODE2VEC, and LINE were used with source code provided by the authors."
}