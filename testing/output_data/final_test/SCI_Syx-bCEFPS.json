{
    "title": "Syx-bCEFPS",
    "content": "This paper establishes a benchmark of real-world noisy labels at 10 controlled noise levels to study deep learning performance. The study shows that Deep Neural Networks (DNNs) generalize better on real-world noise, may not learn patterns first on real-world noisy data, and ImageNet architectures perform well on noisy data when fine-tuned. Real-world noise is found to be less harmful but more challenging for robust DNN methods. Real-world noise is less harmful but more challenging for robust DNN methods. Robust learning methods may not perform equally well on synthetic and real-world noise. The benchmark and findings aim to aid deep learning research on noisy data. Deep Neural Networks (DNNs) trained on noisy data can memorize random training labels but struggle with clean test data. Research on noisy data highlights the importance of studying DNNs in such conditions. Previous work injects synthetic noises into datasets to study DNN behavior, with noise levels ranging from 0% to 100%. The most common noise used is uniform label-flipping noise. Controlled experiments on noise levels are crucial for understanding DNN properties and comparing different methods. Synthetic noise allows for testing on controlled levels, while real-world datasets like WebVision and Clothing-1M provide noisy labels for testing. In this paper, the authors establish a benchmark of controlled real-world noisy labels by collecting noisy labels using text-to-image and image-to-image search via Google Image Search. They annotate every training image independently with 3-5 workers, resulting in a total of 527,489 annotations over 147,108 images. Ten different noise levels from 0% to 80% are created by gradually replacing the original images with annotated noisy images. Our study establishes a benchmark for real-world noisy data by replacing original images with annotated noisy images. We compare synthetic noise (Blue noise) with real-world noise (Red noise) across various noise levels, network architectures, and learning methods. DNNs generalize better on real-world noise, with a smaller generalization gap compared to synthetic noise. Our study compares synthetic noise (Blue noise) with real-world noise (Red noise) on DNNs. Real-world noise proves more challenging for robust DNNs to improve, with patterns learned on noisy data becoming insignificant. ImageNet architectures generalize well on noisy data when fine-tuned, with a correlation of r = 0.87 and 0.89 for synthetic and real-world noise, respectively. Our contribution includes establishing a large benchmark of real image search noise and conducting a comprehensive study on DNN training across various noise levels, architectures, and methods. The findings suggest that DNNs generalize better on real-world noise compared to synthetic noise, but may not learn patterns first on noisy data. Fine-tuned ImageNet architectures perform well on noisy data, indicating potential for future deep learning research on real-world noisy data. Fine-tuned ImageNet architectures generalize well on noisy data, with the addition of noisy examples potentially improving performance as long as noise levels stay below a certain threshold. Research on deep learning properties with noisy training data often involves experiments with synthetic noises, such as uniform label-flipping noise, to drive theory and methodology development in the field. Studies have examined various types of noise, including class-conditional noises and noises from other datasets, to approximate real-world noise distribution. Synthetic noises can lead to inconsistent observations, with some studies showing DNNs are robust to label noise while others verify DNNs on real-world noisy datasets. The most common type of noise involves images tagged according to surrounding texts, like in the Clothing-1M dataset. Several studies have utilized datasets like Clothing-1M, Instagram, and WebVision for model verification and training. While some studies focused on noisy Instagram hashtags, there is a lack of research on investigating real noisy labels in a controlled setting. Robust deep learning methods are gaining traction in the deep learning era due to the prevalence of noisy training data. In the deep learning era, DNNs are facing challenges with noisy training data, leading to poor generalization on clean test data. Recent contributions have addressed this issue through techniques like dropout, regularization, label cleaning, example weighting, semi-supervised learning, and data augmentation. However, few studies have compared these methods systematically across different noise types and training conditions. In this study, six methods from four different directions are compared to handle noisy data in deep learning. The methods are selected based on their coverage and performance on CIFAR-100 with synthetic noise. The benchmark is conducted on Mini-ImageNet and Stanford Cars datasets for image classification tasks. Mini-ImageNet consists of 84x84 images with 100 classes, while Stanford Cars has 16,185 high-resolution images of 196 classes. The datasets are split into training and test sets for evaluation. The existing noisy datasets in the literature are constructed by automatically collecting images for a class using text matching, resulting in false positive examples. Real-world noisy datasets have a fixed and unknown data noise level, making them unsuitable for controlled studies. In contrast, synthetic noisy datasets are built on well-labeled datasets, where the label of each training example is independently changed to a random incorrect class with a certain probability. In contrast to existing noisy datasets constructed with false positive examples, synthetic datasets in our study draw false positive images from similar noise distributions as real-world noisy datasets. The study constructs datasets with noisy labels by replacing training images with false-positive images from Google text-to-image and image-to-image searches. Different levels of label noise (0% to 80%) are studied on Mini-ImageNet and Stanford Cars datasets to mimic real-world noise distributions. The study constructs datasets with label noise by sampling noisy images from Google Image Search. Images are collected, deduplicated, and annotated from text-to-image and image-to-image searches. Text queries and training images are used to retrieve a large pool of similar images, which are then filtered based on occurrence frequency. The dataset is constructed by sampling noisy images from Google Image Search, filtering based on occurrence frequency, and removing near-duplicates. Images are annotated by labeling professionals with binary labels indicating true positives, with majority voting for final labels. The dataset includes 51,687 images from Stanford Cars, with 28,691 and 12,639 images containing false labels. A percentage of these noisy images are used to replace original training images in Mini-ImageNet and Stanford Cars datasets. Annotated images are also added to create two larger augmented datasets with noise levels of 19% and 21%. Performance results are reported in Section 5.3. Additionally, 10 uniform label-flipping datasets are constructed for comparison. Blue Noise represents synthetic noise, while Red Noise denotes real-world image search noise. Test sets are shared across all training conditions. The size difference in Mini-ImageNet is due to a lack of noisy images for common classes like \"carton\" and \"hotdog\". Obtaining noisy labels for common classes is challenging, with only one noisy label obtained for every 22 images on average. Despite the size difference, Mini-ImageNet shows similar test performance. Blue noise is synthetic, while red noise is real-world image search noise, with differences in relevance to true positive images and inclusion of images outside the dataset classes. The paper evaluates robust deep learning methods on a benchmark with noisy images in Mini-ImageNet. Six methods from different directions are compared for dealing with noisy training data. The study aims to understand the performance differences on synthetic versus real-world noise levels ranging from 0% to 80%. The study evaluates robust deep learning methods on noisy images in Mini-ImageNet, comparing six methods for dealing with noisy training data. Hyperparameters of robust DNNs are crucial across noise levels from 0% to 80%, showing competitiveness on CIFAR-100 with synthetic noise. Extensive experiments are necessary to ensure methodology improvement over hyperparameter settings. The paragraph discusses the optimization objective of a deep neural network (DNN) with parameters, weight decay, dropout regularization methods, and label/prediction correction methods. Weight decay is tuned with a default value of e^-4, while dropout is applied with different keep probabilities. Two methods for label/prediction correction are mentioned, with Reed's soft version preferred for better performance. Reed introduces a new loss function for DNN optimization, incorporating noisy and learned labels. Goldberger & Ben-Reuven propose a label transition layer to correct predictions, using a softmax function over the parameter matrix B. The softmax function is applied over a parameter matrix B, initialized using a specific formula. MentorNet is a method that assigns smaller weights to noisy examples by introducing a learnable weight variable. The method includes a regularization term and computes example weights at the mini-batch level. The burn-in epoch is set to 10-20% of total training epochs. Mixup is a method for robust training that minimizes vicinal risk by mixing label vectors and drawing pairs randomly from the same mini-batch. The mixing weight \u03bb is sampled from a Beta distribution. Hyperparameter \u03b1 is searched in {1, 2, 4, 8} for noisy training data. Two training settings are examined: fine-tuning from ImageNet checkpoint and training from scratch using Inception-ResNet-V2 as the default network architecture. Other architectures like EfficientNet-B5, MobileNet-V2, ResNet-50, ResNet-101, and Inception-V2 are also experimented with for vanilla training. The top-performing architectures for image classification include MobileNet-V2, ResNet-50, ResNet-101, Inception-V2, and Inception-V3, with accuracy ranging from 71.6% to 83.6% on the ImageNet dataset. Training is done on clean data with fixed settings across noise levels. Vanilla training results are shown on Blue and Red noisy benchmarks, with training curves plotted for different noise levels. Fine-tuning and training from scratch are compared for robust training. The comparison between training from scratch and fine-tuning is shown in the test accuracy results. DNNs generalize better on Red noise compared to Blue noise, as indicated by smaller test accuracy standard deviation. Networks trained from scratch on Mini-ImageNet are illustrated in Fig. 3 for clarity. Fig. 3a displays training accuracy at different noise levels, while Fig. 3b shows the drop in test accuracies with increasing noise levels. DNNs generalize poorly with synthetic noise but better with real-world noise. DNNs generalize better on real-world noisy data due to similarities with clean training images and being sampled out of training classes. The drop in test accuracy increases with noise levels, supporting the idea that DNNs learn patterns first on noisy data. Early stopping at peak accuracy is effective on Blue noise. Our hypothesis is that Blue noise images are sampled uniformly from a fixed number of classes, making it easier for DNNs to mitigate errors during early training. Real-world noisy images, on the other hand, are sampled non-uniformly from an infinite number of classes, posing challenges for DNNs to identify meaningful patterns. Fine-tuning ImageNet architectures leads to better generalization on noisy data compared to training from scratch, as shown in the test accuracy results. The study compares fine-tuning performance using ImageNet architectures and correlation coefficients between ImageNet accuracy and test accuracy on noisy data. Robust deep learning methods are compared on Blue and Red noise, focusing on peak accuracy and performance variances. The study compares fine-tuning performance using ImageNet architectures and correlation coefficients between ImageNet accuracy and test accuracy on noisy data. Robust deep learning methods show big performance variances, indicating the importance of hyperparameters. Different noise levels require varying hyperparameters, with red noise posing a greater challenge for robust DNNs. No single method performs best across all noise levels and types, with Dropout being effective for training from scratch on Stanford Cars. Weight Decay benefits fine-tuning to a small extent. Reed achieves the best result in 10 trials on MiniImageNet. S-Model shows marginal gains over vanilla training. MentorNet and Mixup achieve the best accuracy in 21 and 23 trials, respectively. Mixup is more effective on Red noise, suggesting pair-wise image mixing is better than example weighting. Adding noisy or weakly-labeled examples to a clean dataset can improve performance without manual labeling effort. Adding noisy or weakly-labeled examples to a clean dataset can improve performance without manual labeling effort. Studies have shown that noisy examples can be beneficial for training, with a maximum noise level yet to be determined. A benchmark study adds additional images to training sets with controlled noise levels to investigate this question. The study added images with noise levels ranging from 0% to 80% to analyze the impact on test accuracy. Results showed that accuracy decreases as noise level increases, with an equilibrium between 30% to 50%. Accuracy improved when noise levels were below 30%, with accuracies of 0.770 and 0.865 on Mini-ImageNet, and 0.927 and 0.932 on Stanford Cars. The study established a benchmark for controlled real-world noise to understand deep learning on noisy data. The study conducted a large-scale analysis on deep learning with noisy data, revealing new findings and comparing six robust methods. Real-world noise proved more challenging to address than synthetic noise, highlighting the need for research on controlled real-world noise. The implementation details include network architectures and model checkpoints obtained from various sources. The top-1 accuracy of various architectures on ImageNet ILSVRC 2012 validation ranges from 71.6% to 83.6%. Training involved grid search for optimal settings with different learning rates and epochs. Networks were trained to full convergence with a maximum of 200 epochs on Mini-ImageNet and 300 epochs on Stanford Cars using learning rate warmup. The training on Stanford Cars utilized Nesterov momentum with a momentum parameter of 0.9 and a batch size of 64. EfficientNet required a batch size reduction to 8 due to larger image input. Vanilla training included batch normalization layers without label smoothing, dropout, or auxiliary heads. Data augmentation was done following standard preprocessing in EfficientNet 8. Reasonable performance was achieved on the clean Stanford Cars test set, with Inception-ResNet-V2 scoring 90.8 (without dropout) and 92.4 (with dropout). Fine-tuning experiments involved initializing networks from an ImageNet checkpoint. Fine-tuning experiments involved initializing networks with ImageNet-pretrained weights and fixing the learning rate at 0.01. Learning rate warmup was not used, and the maximum number of epochs was scaled down by a factor of 2. This approach yielded improved performance on the clean Stanford Cars test set, with Inception-ResNet-V2 achieving 92.4% accuracy and EfficientNet-B5 achieving 93.8%. In method comparison, Inception-ResNet was used as the default network. The study extensively searched hyperparameters for robust deep neural networks, comparing 6 methods across various noise levels and types. A total of 1,840 experiments were conducted on two datasets, showing peak accuracy in Tables 4 to 7. Dropout required additional training epochs for convergence. The examined robust learning methods demonstrated comparable or superior performance. The study compared robust learning methods on noisy CIFAR-100 data with uniform label-flipping noise. Results showed hyperparameter search led to a 2% gain over published results. The examined methods were comparable to state-of-the-art performance. In this section, the examined methods are compared to state-of-the-art results, showing superiority in achieving the best result on the dataset. The experiments exclude images from image-to-image search, focusing on Google text-to-image search images with controlled noise levels. The goal is to verify the findings on this new data subset, comparing training and test curves as well as generalization errors. The study compares generalization errors and fine-tuning models on different ImageNet architectures. DNNs perform similarly on dark red noise and red noise, showing that they generalize better on dark red noise. The results confirm that DNNs may not learn patterns first on dark red noise and that ImageNet architectures generalize well on noisy data when fine-tuned. The red noise studied in the paper is consistent with text-to-image only noise, which contains a more diverse types of label noises. The subsection presents numerical results for the best trial in the main manuscript."
}