{
    "title": "H1O0KGC6b",
    "content": "One of the main challenges in deep learning is choosing the right training strategy. Additional steps like unsupervised pre-training have been proven to enhance deep structures. A new training step called post-training optimizes only the last layer of the network. This approach, analyzed in the context of kernel theory, ensures that the data representation is utilized effectively for the task at hand. Testing on various architectures and datasets consistently shows improved performance. The choice of an appropriate training strategy is crucial in deep learning models. The stochastic gradient descent (SGD) algorithm is commonly used to train deep networks by updating weights based on the gradient of a cost function. This method typically converges to a local minimum with good generalization properties. The gradient of the error on the input distribution is optimized using variance reduction techniques like Adagrap, RMSprop, or Adam for faster convergence. Pre-training is a common method to find a good starting point by using unsupervised learning to capture data information before fine-tuning with SGD. Pretraining strategies have been successful in various applications such as classification, regression, robotics, and information retrieval. The influence of different pre-training strategies on network layers has been extensively studied. The first layers of deep neural networks are general and learn reusable feature extractors, while the last layers are task-specific. Deep learning outperforms shallow structures but is harder to train. For convex models like logistic regression, training is convex with a fixed data representation. Separating representation and model learning enhances stability, while simultaneous learning can lead to non-convex problems. In deep learning, the separation of representation and model learning is crucial for stability. While training deep neural networks, the initial layers learn general features, and the later layers are task-specific. However, the problem becomes non-convex when using dictionary learning or EM algorithms. To address this, a new training step called post-training focuses on optimizing the learned representation for the specific task after the network training is completed. The post-training step focuses on optimizing the learned representation for the specific task by training only the last layer, ensuring efficient usage of the representation for the given task. The post-training step optimizes the learned representation for the task by training only the last layer, ensuring efficient usage. It can be easily added to learning strategies, is computationally inexpensive, and shows a connection to kernel techniques. Experimental results demonstrate improvements for various architectures and datasets. The training of the neural network involves mapping the input space to the output space using layers with weights and activation functions. The objective is to find weights that minimize a convex loss function. The last layer plays a special role in the network compared to the others. When \u03a6 L\u22121 is fixed, finding W W W L is simple for popular activation functions and loss functions like softmax and cross entropy, leading to multinomial logistic regression. Training the last layer involves regression of labels y using data embedding x in X L by mapping \u03a6 L\u22121. Optimization techniques efficiently produce optimal weights W W W L given \u03a6 L\u22121, which is the idea behind post-training. Regular training aims to learn data representation in space X L through L \u2212 1 layers and optimize its use with W W W L, a strongly non-convex problem. The post-training step involves joint minimization of data representation with W W W L, which is a strongly non-convex problem. Regular training aims to obtain interesting features through deep learning training, optimizing empirical loss using various strategies like stochastic gradient descent and regularization techniques. During post-training, the last layer of the network is trained while the first L-1 layers are fixed. This step involves minimizing a problem with 2-regularization to reduce overfitting. The optimization takes place in a lower dimensional space, making it computationally faster. The importance of 2-regularization in post-training is emphasized. During post-training, 2-regularization is added to all network architectures to improve efficiency and promote model generalization. Dropout should not be applied to previous layers during post-training to avoid changes in the feature function. The kernel framework is compared, and a continuous positive definite kernel is defined for approximating DISPLAYFORM0 using kernel methods. The kernel used in post-training is continuous positive definite and belongs to the Reproducing Kernel Hilbert Space generated by it. The generalized representer theorem can be applied with mild hypotheses, leading to an optimal solution for the problem. The optimal solution should not be confused with the solution obtained through regularization, as they are closely related. The kernel used in post-training is continuous positive definite and belongs to the Reproducing Kernel Hilbert Space. In experiments, W * appears to be a nearly optimal estimator of W * L. The problems only differ in the choice of regularization norm, with the RKHS norm being equal to the 2-norm when Vect(\u03a6 L\u22121 (X 1 )) spans the entire space X L. The 2-norm is chosen for experiments as it is easier to compute than the RKHS norm. In the context of post-training, the kernel used is a continuous positive definite function in the Reproducing Kernel Hilbert Space. The computation of W* can be achieved by combining formulas and matrices, leading to a classical Kernel Ridge Regression problem. This approach can be extended to multidimensional output spaces, encouraging the use of post-training in various settings. The post-training method is applied to feedforward convolutional neural networks, with experiments conducted using python and Tensorflow. The method's performance is evaluated on CIFAR10, MNIST, and FACES datasets, showing promising results. The experiments use the default architecture proposed by Tensorflow for CIFAR10, consisting of 5 layers. The neural network consists of 5 layers with common tools like lrn, max pooling, and RELU activation. The last layer uses a softmax activation function. The network is trained for 90k iterations with SGD, dropout, and an exponential weight. Post-training is done by training for 100 iterations after regular training. Performance is evaluated on CIFAR10 dataset with promising results. The neural network is trained with batches of size 128 using stochastic gradient descent, dropout, and exponential weight decay. Post-training involves training for an additional 100 iterations after regular training, improving generalization despite a slight increase in training cost. Regularization parameter \u03bb for post-training is set to 1 \u00d7 10 \u22123. Post-training improves generalization by training for an additional 100 iterations after regular training, resulting in a consistent gain in performance. Post-training iterations are 4\u00d7 faster than classic iterations, making them more cost-effective. Evaluation on MNIST and FACES datasets shows the impact of post-training on different convolutional neural network architectures. The study evaluates the impact of post-training on various convolutional neural network architectures, showing improved test performance with as little as 100 additional iterations. The results indicate that post-training consistently enhances generalization, with the improvement varying based on network complexity and dataset characteristics. In this experiment, post-training is applied to Long Short-Term Memory-based networks (LSTM) using the PTB data set. The network architecture consists of 2 layers of 1500 LSTM units with tanh activation, trained to minimize perplexity for 100 epochs with gradient descent and dropout for regularization. Performance is evaluated after each epoch compared to the last 100 steps. The regularization parameter for post-training is set to 1 \u00d7 10 \u22123, and the results show that post-training improves test performance even after network convergence. The experiment evaluates the close-form solution for regression tasks with the activation function of the last layer as the identity and the least-squared error as the loss function. The evolution of the performances of the Recurrent network on the PTB data set is shown in Figure 4. The experiment evaluates the performances of a Recurrent network on the PTB data set, showing train and test perplexity. Post-training improves test performance even after network convergence. A neural network is trained on regression problems using real and synthetic data sets, with 70% for training and 30% for testing. The real data set used is the Parkinson Telemonitoring data set with 5,875 instances of 17-dimensional data. The neural network is trained on a synthetic data set with inputs generated from a uniform distribution and outputs computed using randomly generated weights. The data set consists of 5,875 instances of 17-dimensional data, and the network is trained with two hidden layers of size 17 and 10. Regularization is applied with a fixed parameter, and post-training is conducted for 200 iterations. The results are compared to closed-form solutions. The neural network is trained on a synthetic data set with inputs from a uniform distribution and outputs computed using randomly generated weights. The data set consists of 10,000 pairs, and a neural network with two hidden layers of size 10 is trained for 250, 500, and 750 iterations. Post-training improves performance compared to the optimal solution, indicating overfitting with full training. The experiments in Section 4 demonstrate that post-training enhances the performance of various networks, including recurrent, convolutional, and fully connected networks. The gap between losses with and without post-training is more significant when training is stopped early, decreasing as the network converges to a better solution. Post-training results in a larger drop in error rates when applied earlier in the training process, as shown in the results presented in TAB0. Post-training improves network performance, especially when training is stopped early. The advantage diminishes as the network converges, with larger improvements seen in earlier stages of training. Overfitting can negate the benefits of post-training, as features learned become less suitable for the general problem. Post-training offers efficiency benefits with low computational cost compared to full training. In experiments like CIFAR10, post-training iterations are 4\u00d7 faster on the same GPU. Performance gaps can be observed after just 100 batches due to reaching a local minimum quickly and using computationally cheaper iterations. The post-training optimization problem is generally convex, leading to improved efficiency. The post-training optimization problem is generally convex, guaranteeing rapid convergence to optimal weights for the last layer. Post-training does not lead to overfitting, as seen in CIFAR10 where test error improves without a significant increase in training loss. The simplicity of post-training optimization in a small-dimensional space, combined with 2-regularization, effectively prevents overfitting. The regularization parameter \u03bb is crucial, with a very large or too small value leading to reduced explanatory capacity or overfitting. The experiments showed that post-training with a reasonably small \u03bb (10^-5 \u2264 \u03bb \u2264 10^-2) can significantly improve network performance without degrading it. The post-training step can be applied to most trained networks, providing consistent performance gains at low computational cost. Numerical experiments in Subsection 4.3 highlighted the connection between post-training and kernel methods, showing that using optimal weights from kernel theory boosts network performance. The post-training step estimates this optimal layer using gradient descent optimization. The post-training step aims to estimate the optimal layer using gradient descent optimization. It is known that computing optimal weights for the last layer is only feasible for small datasets due to matrix inversion requirements. Post-training can improve network performance without degrading it, and is linked to the idea of pre-training to extract good features from data. In this work, the concept of post-training is explored as an additional step after regular training, focusing on training only the last layer to fully utilize learned data representation. Empirical evidence shows that post-training is computationally efficient and significantly improves performance in most neural network structures. Future research could investigate the relationship between the number of frozen layers in post-training and resulting performance enhancements. The post-training problem is proven to be convex for softmax activation and cross entropy loss. The function F is shown to be convex, with the matrix P(W) being positive semidefinite. The matrix P(W) is positive semidefinite, and the Kronecker product of XX T is also positive semidefinite, leading to a positive conclusion."
}