{
    "title": "rkx0g3R5tX",
    "content": "In recent years, softmax has become the standard loss function for deep neural networks with multiclass predictions. However, it may lead to biased results in problems where outcomes are not mutually exclusive. For scenarios with positive and unlabeled data, a relaxation of the softmax formulation is proposed, where outcomes are conditionally independent but share a common set of negatives. An adversarially-trained model is used to derive a new negative sampling approach in the absence of explicit negatives. Cooperative Importance Sampling (CIS) is a new negative sampling and weighting scheme introduced in an adversarially-trained model. It shows advantages when applied to the Word2Vec algorithm, outperforming other negative sampling schemes in language modeling and matrix factorization tasks. PU learning, focusing on positive and unlabeled data, is a well-defined task in machine learning with various applications due to the difficulty in obtaining negative data. This is particularly relevant in fields like language modeling and computer vision. In various applications like language modeling and computer vision, the use of softmax loss functions is common. The Word2Vec algorithm models the conditional probability of observing items near a context item using softmax loss. However, this approach is biased and refuted by data, as there are multiple possible words for each context word. In our paper, we introduce Partially Mutual Exclusive Softmax (PMES) as a new model that relaxes the mutual exclusivity constraint in language modeling. PMES splits outcomes into possible and impossible sets based on context, introducing a model for negative examples. The training involves two neural networks - a generator for positives and sampled negatives, and a discriminator to distinguish true positives from generated pairs. The solution presented introduces Partially Mutual Exclusive Softmax (PMES) as a model for negative sampling in language modeling. It involves a generator for positives and sampled negatives, and a discriminator to distinguish true positives from generated pairs. The method aims to improve performance by addressing over-simplifying assumptions in previous approaches and shows significant performance gains in experiments on language modeling and matrix factorization. The paper introduces Partially Mutual Exclusive (PME) Softmax for problems with positive and unlabeled data. It also presents Cooperative Importance Sampling (CIS) for negative sampling and evaluates the approach in Word2Vec models on various tasks and datasets. Related work on Word2Vec and negative sampling schemes is discussed in the paper. In Section 2, the paper discusses schemes for softmax and GANs. Section 3 introduces the PME-Softmax loss and CIS negative sampling scheme. The performance of the method is highlighted in Section 4, with future work ideas in Section 5. Word2Vec has been widely used for language modeling tasks, with recent advancements like the FastText model incorporating n-gram features. Candidate sampling algorithms can replace softmax in training multi-class models with numerous output classes. Candidate sampling algorithms like Noise Contrastive Estimation (NCE) and negative sampling loss have been proposed to speed up training and improve the efficiency of learning word embeddings. Sampled softmax is also used to train language models, showing state-of-the-art results in performance and time complexity by avoiding computing scores for every possible continuation. The biased importance sampling scheme for approximating the softmax focuses on the benefits of sampling from the unigram distribution. Leveraging this analysis, a dynamic sampling method based on inner-product rankings is proposed. GANs, introduced in 2014, have been successful in generating realistic images and are a framework for training generative models through a minimax game. Our approach builds on recent work on discrete GANs. Our approach, called cooperative importance sampling, involves training the generator and discriminator to work together. The task involves sets of objects I and J, with a generative process P(.|i) and joint distribution X over I \u00d7 J. The model G = {G \u03b8 } \u03b8\u2208\u0398 outputs a score G \u03b8 (i, j) \u2208 R for input in I \u00d7 J. The model G \u03b8 aims to learn a generative model that fits the conditional process observed in the data. It uses a softmax formulation to define a multinomial distribution P(.|i) \u2208 P(J). The probability of observing one target given another in a context is not independent, leading to mutual exclusivity. To fit the model, Kullback-Leibler divergence minimization is performed between the true distribution P(.|i) and the model distribution g \u03b8. In language modeling and sequence prediction tasks, the softmax formulation is commonly used for multi-class and single-label tasks. However, for tasks involving multiple labels, a subset of target items is observed for a given context. This approach generalizes the current formulation to better represent the data by training a product of independent distributions. This method allows for a more comprehensive representation of the multiplicity of the data. In language modeling and sequence prediction tasks, the softmax formulation is commonly used for multi-class and single-label tasks. To better represent data involving multiple labels, a product of independent distributions is trained to model the joint conditional distribution over a set of target items. The goal is to find the right parameter for each Bernoulli variable, relaxing the assumption of mutual exclusivity in events. In language modeling and sequence prediction tasks, the softmax formulation is commonly used for multi-class and single-label tasks. The model assumes that words sharing the same grammatical category or semantic context might not co-occur. A simplified version of the model assumes a set of negatives that will not co-occur with a given context independently of the observed target word. The PME-Softmax formulation models the probability of items co-occurring with a context, with a true support set S i. However, in reality, the support set is replaced with a probabilistic model D i. In language modeling and sequence prediction tasks, the PME-Softmax formulation is replaced with a probabilistic model D i. This model weights sampled items by the probability of being in the negative set. To train the model, a generator and discriminator are used simultaneously, with D \u03b1 trained using binary cross-entropy loss to discriminate between positive and negative data. In language modeling and sequence prediction tasks, a cooperative importance sampling model is used to sample negatives close to the decision boundary. The model oversamples true negatives using D \u03b1 to address the issue of sampling positives as the model improves. This approach improves gradient by finding true hard negatives that are informative. Our cooperative importance sampling model oversamples true negatives using D \u03b1 to sample negatives close to the decision boundary. The sampling method ensures meaningful samples and improves gradient by finding true hard negatives that are informative. The algorithm is described in Algorithm 1 and shows empirical improvements in the following section when compared against full softmax and other negative sampling schemes on various datasets. The different baselines for evaluating word ranking, similarity, and analogy include Full Softmax (FS), Uniform sampling (UniS), Popularity sampling (PopS), and Selfplay method (SP). The Selfplay method samples negatives close to the decision boundary without reweighting, which may result in lower performance compared to other methods. The Selfplay method samples negatives close to the decision boundary without reweighting, which may result in lower performance compared to other methods. Across experiments, embedding size is fixed at 150, and 20% of the dataset is held out for testing. Artificial datasets with 2D shapes like Swiss Roll and S Curve were used to verify true support modeling. Blobs-based simulations help control the ratio of positive data. In experiments comparing different sampling methods, cooperative importance sampling outperformed others and was comparable to full softmax. The impact of CIS was smaller when the distribution was easier to learn. Language modeling tasks were conducted on the text8 dataset with Skip-Gram Word2Vec using different word frequencies and negative sampling. Three tasks were evaluated: next word prediction, similarity, and analogy. In next word prediction task, the model outperformed other sampling methods including full softmax. Results for Mean Percentile Rank (MPR) and Precision@1 metrics are shown in Table 2. The selfplay method also showed improvement in performance. In the word analogy task, different embedding structures are compared using semantic and syntactic analogies. The Google analogy test set is used to test the robustness of the embeddings. The full softmax and cooperative sampling method show similarities in results, with softmax sometimes lacking coherence as the number of neighbors increases. The analogy test set developed by BID16 evaluates embeddings through semantic and syntactic analogies. Results in Table 6 of CIS show improvements in quality metrics like Prec@1, Prec@5, and Prec@15. FastText outperforms CIS in syntactic analogies, especially on a dataset with 30k different words. Experiments on matrix factorization were conducted on shopping and movie recommendation datasets, filtering movies with high ratings. In this paper, the authors propose Partially Mutual Exclusive Softmax, a relaxed version of full softmax suitable for cases with no explicit negatives, such as positive-only co-occurrence datasets. Results show that CIS outperforms all baselines in terms of Prec@1 on test data, with performance improving as the vocabulary size increases. The authors proposed a new softmax, called Partially Mutual Exclusive Softmax, for cases with positive and unlabeled data. They introduced a cooperative negative sampling algorithm and plan to explore its effectiveness on more complex models in the future."
}