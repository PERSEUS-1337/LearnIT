{
    "title": "rJxEso0osm",
    "content": "In this paper, the authors explore the relationship between Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs). They demonstrate that HMMs can be seen as a special case of RNNs through architectural transformations and empirical hybridization. The study focuses on three key design factors and their empirical effects on the model family's expressivity and interpretability. The curr_chunk discusses the interplay between expressivity and interpretability in sequence modeling using Hidden Markov Models. HMMs are widely used for generating observed variables from latent variables, allowing for unsupervised training via the Baum-Welch algorithm. The discrete nature of latent variables in HMMs provides interpretability by recovering contextual clustering of output variables. In contrast to Hidden Markov Models (HMMs) which recover contextual clustering of output variables, Recurrent Neural Networks (RNNs) assume continuous latent representations. Both models involve hidden states, transition functions, and emission functions for sequential data. The relationship between HMMs and RNNs is explored through architectural transformations in this paper. The paper investigates architectural transformations between HMMs and RNNs, demonstrating how forward marginal inference in HMMs can be reformulated as equations for computing an RNN cell. Three key design factors are explored: independence assumptions, softmax placement, and use of non-linearities. Our work investigates the connection between HMMs and RNNs, focusing on key design factors such as independence assumptions, softmax placement, and non-linearities. We provide a thorough theoretical investigation into model variants, highlighting the importance of using a sigmoid instead of softmax linearity in the recurrent cell for better HMM performance. The paper explores the relationship between HMMs and RNNs, emphasizing design factors like independence assumptions and non-linearities. It discusses the superiority of HMMs over other RNN variants for predicting POS tags and presents a structured approach to transforming between HMMs and RNNs. The empirical analysis focuses on the effects of design choices on hybrid models. The forward algorithm updates hidden states at each time step by feeding previous word predictions, similar to RNNs. It defines hidden state and emission distributions for sequences of random variables. The distribution P(x) is marginalized over hidden states. Inference for HMMs is done using the forward-backward algorithm, which is equivalent to differentiating the forward algorithm. Traditional HMM implementations required both forward and backward algorithms, but now only the forward algorithm is implemented in deep learning software for efficient training on GPUs. The observed sequence is represented as w = {w BID0 , . . . , w (n) } and the forward probabilities a are defined recursively. Equation 6 is rewritten in terms of parameters for distribution definitions with vectorized computations. The computation of s (i) can be delayed until time step i + 1. This step can be viewed as a recurrent neural network with tied input and output embeddings. It embeds the previous prediction, updates the hidden state, and computes the output next word probability. Comparing this formulation against the definition of an Elman RNN. The Elman RNN with tied embeddings and a sigmoid non-linearity updates the hidden state and computes the next word probability. By relaxing the independence assumption of the HMM transition probability distribution, the expressiveness of the HMM \"cell\" can be increased to model more complex interactions between the fed word and the hidden state. Model variants with intermediate expressiveness between HMMs and RNNs will be empirically evaluated and their interpretability investigated. The model variants proposed involve non-homogeneous HMMs with tensor-based feeding, additive versions, and a gating mechanism for interaction control. These modifications aim to increase the expressiveness of HMMs by relaxing independence assumptions and using unnormalized embeddings. The model proposes a transformation to replace the HMM's emission computation with that of the RNN, breaking the independence assumption. It involves taking the expectation over embeddings with respect to the state distribution and using a sigmoid non-linearity in the transition instead of softmax. The recurrent state is renormalized to compute a distribution in the output. The model proposes transformations to make Elman networks more similar to HMMs by using delayed emission softmax and replacing sigmoid non-linearity with softmax in emission computation. The curr_chunk discusses modifications to HMMs and Elman cells, exploring architectural changes such as using element-wise product for integration instead of affine transformations. The effects of these changes are analyzed quantitatively in a language modeling benchmark setup. In a standard PTB language modeling setup, one-layer models with a budget of 10 million parameters are trained using batched backpropagation through time. Input and output embeddings are tied in all models, which are optimized with a grid search over optimizer parameters for two strategies: SGD and AMSProp. Results show that HMM models perform significantly worse than Elman networks, with many HMM variants not outperforming the vanilla HMM. The exception is the gated feeding model, which shows substantial improvement. The gated feeding model in HMMs shows substantial improvement by incorporating more context into the transition matrix. Using a sigmoid non-linearity before the output of the HMM cell improves performance, and combining it with delaying the emission softmax gives a significant boost. Replacing the sigmoid non-linearity with the softmax function leads to a drop in performance, but still outperforms HMM variants. Normalizing the hidden state output before applying the emission function performs somewhat worse than using the softmax non-linearity. The sigmoid non-linearity performs worse than the softmax non-linearity in the emission function. A neural bigram model outperforms these approaches with a validation perplexity of 177. Replacing the RNN emission function with an HMM leads to even worse performance. Multiplicative integration results in a small drop in performance compared to a vanilla Elman RNN. Preliminary experiments suggest the second transformation matrix is crucial for performance. The second transformation matrix is crucial for the performance of the vanilla Elman network. Results are compared against an LSTM baseline with the same parameters. Tagging accuracies for representative models are shown in TAB1. HMM bottlenecks force an interpretable hidden representation, beneficial for tasks like part-of-speech tag induction. Language modeling involves part-of-speech tag induction, and the architecture of models may affect their ability to discover syntactic properties. The study evaluates this by analyzing models' predicted tag distributions at each time step. HMMs are expected to preserve tag patterns, unlike RNNs. The accuracy of predicting the next word's tag is computed without explicit training. The model's distribution over POS tags is estimated by marginalizing over output word distribution and context-independent tag distribution. Tagging accuracy is evaluated by comparing the predicted tag against the ground truth. The evaluation compares models' tagging accuracy using Viterbi decoding in HMMs and Elman models. Results are presented in TAB1 and FIG3, showing that simpler variants of gated RNNs perform competitively. The curr_chunk discusses various types of RNN variants, including those with recurrent non-linearities, gated RNNs with type constraints, and rational recurrences. It also mentions recent work on neural models learning discrete structures and segmental structures over sequences. The goal is to compare RNNs against HMMs for insights. In this work, a theoretical and empirical investigation into model variants combining HMMs and RNNs is presented. Key insights include the use of sigmoid instead of softmax linearity in the recurrent cell and an unnormalized output distribution matrix for better RNN performance. HMMs outperform other RNN variants in a POS tag prediction task, highlighting the advantages of models with discrete bottlenecks for increased interpretability."
}