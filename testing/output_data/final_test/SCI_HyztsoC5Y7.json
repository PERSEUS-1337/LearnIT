{
    "title": "HyztsoC5Y7",
    "content": "This work proposes online adaptation in model-based reinforcement learning to quickly adapt to new tasks, overcoming challenges of expensive sample generation and unexpected perturbations. Meta-learning is used to train a dynamics model prior for rapid adaptation to local context, demonstrated on continuous control tasks for simulated and real-world agents. The curr_chunk discusses the importance of online adaptation for autonomous agents in dynamic environments, showcasing simulated agents adapting to novel terrains and real-world applications on a legged millirobot. It highlights the ability of agents to quickly adapt online to various challenges like missing limbs, terrain changes, and errors in pose estimation using model-based and model-free reinforcement learning methods. Humans can rapidly adapt their behavior to unseen physical perturbations and changes in dynamics, such as learning to walk on crutches in seconds or picking up unexpectedly heavy objects. This ability stems from encountering a large number of perturbations in the past, allowing for quick adaptation without relearning. A meta-learning approach is proposed for online adaptation in autonomous agents, inspired by human adaptability. In this work, a meta-learning approach is proposed for online adaptation in autonomous agents. The model-based meta-reinforcement learning algorithm allows for updating the model with recent experiences at every timestep, leading to more sample-efficient training compared to model-free approaches. This method considers each timestep as a potential new \"task,\" allowing for a more general meta-RL problem setting. The meta-RL approach allows for adapting a model online, addressing the challenge of acquiring a globally accurate dynamics model. This adaptive model can handle disturbances and new goals without needing to be perfect everywhere a priori. Previous methods have struggled to scale to complex tasks and nonlinear systems, even with the use of deep neural networks. The primary contribution of the work is an efficient meta reinforcement learning approach that enables online adaptation in dynamic environments. This approach trains a global model to quickly adapt using recent experiences, achieving fast online adaptation. The approach evaluates two versions, ReBAL and GrBAL, on stochastic and simulated continuous control tasks with complex contact dynamics. Experiments show a quadrupedal \"ant\" and a \"half-cheetah\" robot adapting to failures and navigating terrains with different slopes. The model-based meta RL method outperforms prior approaches, requiring only 1.5 \u2212 3 hours of real-world experience for meta-training across multiple tasks. The GrBAL approach demonstrates high sample efficiency in meta model-based reinforcement learning, showcasing the agent's ability to adapt online to various challenges in real-world scenarios. Unlike model-free methods, GrBAL requires significantly less real-world experience to learn tasks and emphasizes fast online adaptation. Model-based RL approaches achieve superior sample efficiency by learning a model of system dynamics and adapting it online based on recent observations. Prior methods used Gaussian Processes to handle model uncertainty but had limitations in scaling to high dimensional environments. Recent advancements have shown that neural network models can address these challenges. Recent advancements in model-based reinforcement learning have demonstrated that neural network models can benefit from incorporating uncertainty, leading to improved performance with reduced sample complexity. This approach is different from prior online adaptation methods that focus on learning a global model and adapting it at test time. Instead, the focus is on adapting the forward model to fit the current local distribution, enhancing model-based methods in high-dimensional environments. In this paper, the focus is on training a model for fast and effective adaptation in the context of meta-learning. The goal is to enable artificial agents to efficiently learn new tasks by learning to learn. The model achieves more effective adaptation compared to prior works, as validated in experiments. Meta-learning approaches involve learner architecture, optimization algorithms, and recurrent neural networks. Two instantiations of the approach, ReBAL and GrBAL, build on existing methods like MAML. GrBAL focuses on online meta-learning, where tasks are constructed automatically from experience. Reinforcement learning meta-learning has mainly focused on model-free approaches. In contrast to model-free reinforcement learning approaches, recent work has developed a model-based meta RL algorithm using GPs for episodic adaptation to dynamics changes. The proposed approach involves learning online adaptation of high-capacity neural network dynamics models, with results shown on simulated agents and a real legged robot. This section introduces model-based reinforcement learning, meta-learning formulation, and two main meta-learning approaches. Reinforcement learning agents aim to maximize cumulative rewards in Markov decision processes (MDPs) by finding an optimal policy \u03c0 : S \u2192 A. Model-based RL focuses on learning the transition distribution p(s |s, a) to achieve this, using a dynamics model represented by a function approximator p\u03b8(s |s, a). Meta-learning involves automatically learning more efficient learning algorithms by leveraging data from previous tasks to quickly adapt to new tasks. These algorithms assume that previous and new tasks share a common structure from the same task distribution, allowing for faster learning. Meta-learning aims to find a learning procedure that can quickly adapt to new tasks by optimizing parameters for the learning procedure. Model-agnostic meta-learning (MAML) focuses on learning initial neural network parameters for effective generalization to new tasks with small amounts of data. Model-agnostic meta-learning (MAML) focuses on learning initial neural network parameters for effective generalization to new tasks with small amounts of data. MAML uses gradient descent as a learning algorithm, where a learned initialization of an overparameterized deep network followed by gradient descent is as expressive as update rules represented by deep recurrent networks. Another approach to meta-learning is recurrence-based, using recurrent models to always learn the update function. In this section, we present our approach for meta-learning for online model adaptation, building upon prior gradient-based and model-based meta-RL methods. Our notion of task is flexible, considering each segment of a trajectory as a different task, enabling adaptation in dynamic environments in an online manner. In this work, the focus is on adapting the model using past time steps to predict future timesteps in dynamic environments. The concept of environment E is used to represent different settings or configurations of a problem. The distribution of environments \u03c1(E) shares common structure but may differ in dynamics. Trajectory segments \u03c4 E (i, j) represent sequences of states and actions. The algorithm assumes local consistency in the environment, allowing for fast adaptation without knowing when the environment changes. The meta-RL problem is framed as optimizing parameters (\u03b8, \u03c8) based on a maximum likelihood meta-objective. This objective is the likelihood of data under a predictive model p\u03b8(s|s, a), where \u03b8 = u\u03c8(\u03c4E(t - M, t - 1), \u03b8). This optimization involves trajectory segments \u03c4E(t - M, t + K) \u223c D. The meta-learning approach presented focuses on fast adaptation at test time by fine-tuning the model using just a few data points from past trajectories. This allows for optimization of adaptation in sequence modeling domains, particularly in reinforcement learning problems. The update rule u \u03c8 for inner updates and gradient steps on \u03b8 for outer updates enable efficient adaptation for future timesteps. The algorithm presented focuses on fast adaptation in sequence modeling domains using a gradient-based meta-learner and a recurrent model. The update rules enable efficient adaptation for future timesteps. The algorithm enables fast adaptation in various environments by using a model-based meta-reinforcement learning approach. The agent adapts its model parameters based on recent experience to better capture local dynamics, which are then used by the controller for task performance. The algorithm enables fast adaptation in various environments through model-based meta-reinforcement learning. Model predictive control compensates for inaccuracies by replanning at each time step. Online adaptation improves the model itself, with state transitions appended to the dataset. This adaptation procedure is also performed during meta-training for on-policy rollouts. The evaluation aims to assess if adaptation changes the model, enables fast adaptation to different dynamics and tasks, compares performance with other methods, compares GrBAL and ReBAL, compares meta model-based RL with meta model-free RL in terms of efficiency and performance, and evaluates online adaptation on a real robot. The setup and results are presented based on these questions, with videos available online and additional analysis in the appendix. A comparative evaluation of the algorithm is conducted on various simulated robots using the MuJoCo physics engine. The setup involves simulated robots in MuJoCo physics engine BID53 with transition probabilities modeled as Gaussian random variables. Each agent requires different types of adaptation for success at run-time, such as disabling joints in the Half-cheetah environment. Testing includes extrapolation to out-of-distribution environments and fast adaptation to changing dynamics. The experiments involve testing different types of adaptation for success in various environments. The Half-cheetah is tested on sloped terrain and a pier with floating blocks on water, while the Ant robot has a crippled leg for unexpected dynamics changes. The evaluation of the model-based meta-RL methods (GrBAL and ReBAL) is compared to prior methods like Model-free RL (TRPO), Model-free meta-RL (MAML-RL), Model-based RL (MB), and Model-based RL with dynamic evaluation (MB+DE). The evaluation involves testing the agent with a crippled leg at test time to assess adaptation capabilities. At test time, the model is adapted by taking a gradient step at each timestep using past observations, evaluating the benefit of explicitly training for adaptability. Model-based approaches use model bootstrapping and the same neural network architecture and planner. Results from test-time runs on three environments are analyzed, showing a distribution shift in model prediction errors. Our approach utilizes past observations to update the model, reducing prediction errors and improving performance compared to model-free methods. Meta-training is efficient, requiring only 1.5-3 hours of real-world experience. Our method outperforms model-free agents trained with 1000 times more data and non-meta-learned model-based approaches. Additionally, our performance closely matches that of model-free meta-RL methods for the half-cheetah disabled scenario. The model-free meta-RL method shows high performance for half-cheetah disabled and suboptimal performance for ant crippled with 1000 times less data. This suboptimality is a known issue with model-based methods, highlighting a future research direction. The method demonstrates improved sample efficiency and the ability to adapt online to drastic dynamics changes in a few timesteps. The final test time performance of GrBAL and Re-BAL is evaluated in comparison to other methods in low data regimes for real-world applications. In low data regimes, all agents were meta-trained on a distribution of tasks/environments and evaluated for adaptation ability on unseen environments at test time. The fast adaptation component was tested on disabled joints and crippled legs of robots, while the generalization component was evaluated as well. The robot experienced malfunctions during a rollout, including in tasks with disabled joints and crippled legs not seen during training. Generalization was also tested on sloped terrain, with results showing our approach outperforming others in all experiments. Our approach surpasses the model-based oracle in fast adaptation in stochastic environments. ReBAL excels in longer sequential inputs, while GrBAL performs better overall for generalization and fast adaptation. GrBAL was tested on a real legged millirobot, outperforming model-based RL methods. The GrBAL robot, a small 6-legged robot, presents modeling and control challenges due to its highly stochastic and dynamic movement. It is an excellent candidate for online adaptation due to its unique dynamics that cannot be reproduced consistently. The robot's state space is a 24-dimensional vector, including various parameters like center of mass positions, velocities, and motor readings. The action space consists of velocity setpoints for the rotating legs, with experiments conducted to evaluate its performance on the real robot. The GrBAL robot, a small 6-legged robot, is used for experiments in a motion capture room. Data is collected from three different terrains using a random policy. Results show that training a dynamics model with data from a random policy yields promising outcomes. The GrBAL robot, a 6-legged robot, demonstrates superior performance in fast online adaptation to new environments compared to model-based methods without meta-training or online adaptation. It can adapt to missing legs, novel terrains, pose estimation errors, and pulling payloads. GrBAL outperforms MB and MB+DE in online adaptation to new environments, leveraging prior knowledge and fine-tuning quickly. The robot can effectively follow target trajectories in new environments and unexpected perturbations at test time. In this work, a model-based meta-RL approach is presented for fast, online adaptation of large models in dynamic environments. The method prevents drift from missing legs, sliding down slopes, pose errors, and adjusts to pulling payloads. It can adapt to unseen situations or sudden changes in the environment efficiently. Two instantiations of the approach (ReBAL and GrBAL) are provided, showing superior performance in online adaptation compared to other methods. Our approach (ReBAL and GrBAL) outperforms other methods in online adaptation for continuous control tasks. The GrBAL instantiation demonstrates the effectiveness of adaptation through lower prediction errors post-update compared to pre-update models. An experiment on training distribution's impact on test performance using GrBAL for 7-DOF arm models shows promising results. The study shows that models trained on a wide range of force perturbations perform better during testing. There is no strict requirement on the training data for adaptation to occur at test time. The algorithm's sensitivity to hyperparameters K and M is analyzed, with K set equal to M in all experiments. The average return of GrBAL across meta-training iterations for different K = M values is shown in FIG6. The algorithm's performance is not significantly affected by different values of hyperparameters K and M. Optimal values for these hyperparameters vary depending on task details. GrBAL's performance is robust to hyperparameter values, as shown in data aggregation iterations during meta-training. The reward functions used for each MuJoCo agent are shown in TAB2, with x t representing the agent's x-coordinate, ee t referring to the end-effector position of the 7-DoF arm, and g corresponding to the desired goal position."
}