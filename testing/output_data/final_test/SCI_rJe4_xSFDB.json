{
    "title": "rJe4_xSFDB",
    "content": "LiPopt is a polynomial optimization framework that computes tighter upper bounds on the Lipschitz constant of neural networks. The optimization problems involved are either linear (LP) or semidefinite (SDP) programming. By leveraging the sparse connectivity of a network, the complexity of computation can be significantly reduced, especially for convolutional and pruned neural networks. Experimental results on networks with random weights and MNIST-trained networks demonstrate that LiPopt provides superior estimates for the $\\ell_\\infty$-Lipschitz constant compared to other existing methods. In this work, the focus is on estimating the Lipschitz constant of a neural network, which is crucial for the generalization ability of neural network classifiers in supervised learning. The Lipschitz constant is a measure of model complexity and plays a significant role in successful applications of deep learning. The Lipschitz constant of neural network classifiers is crucial for model complexity and robustness against adversarial perturbations. It can provide certificates of robust classification and is essential for generating high-quality images in deep learning applications. Various methods have been proposed to handle constraints on the Lipschitz constant, such as heuristic penalties, upper bounds, and choice of activation function. Accurate estimation of this constant is key for network robustness. Various methods have been proposed to provide true certificates of robustness to input perturbations in deep reinforcement learning applications. Tighter upper bounds on the Lipschitz constant are needed, with some researchers deriving upper bounds based on semidefinite programming. This work aims to overcome limitations in current methods. Our work, LiPopt, introduces a general approach to upper bounding the Lipschitz constant of neural networks using polynomial optimization. By leveraging the sparse connectivity of neural network architectures, we derive smaller linear programs compared to traditional methods. We conduct experiments on networks with random weights and those trained on MNIST, evaluating different configurations of depth, width, and sparsity. The proposed approach, LiPopt, aims to provide tighter upper bounds on the Lipschitz constant of neural networks using polynomial optimization. It considers different configurations of depth, width, and sparsity, showing that the sequence of linear programs can outperform other baselines. The notation and definitions used in the network architecture are also explained. The definition of network covers architectures with dense and convolutional layers, extendable to any directed acyclic computation graph. An upper bound on Lipschitz constant is derived using a polynomial optimization problem. The Lipschitz constant of a function is defined in an euclidean space, assuming Lipschitz continuity and differentiability of the activation function. The assumptions of Theorem 1 are fulfilled for the network f d, allowing for various activation functions like ELU or softplus. The gradient formula for f d is derived using the chain rule, with a bound on the derivative of activation functions. This leads to an upper bound on the Lipschitz constant L(f d), defined as the norm-gradient polynomial of the network. The network f d is analyzed with a focus on the polynomial inequalities that can be written for certain p -norms. When p \u2265 2, a single polynomial inequality t p p \u2264 1 is equivalent to t p \u2264 1. The optimization problem in this case is a POP, which is NP-hard. A systematic approach called LiPopt is proposed to approximate an upper bound on L(f d) using tractable methods. Additional bounds on the network input are considered in practical scenarios. In practical scenarios, additional bounds on the network input are considered. Local Lipschitz constants can be computed by deriving tighter bounds on the input. The importance of computing good upper bounds on L(f d) with respect to the \u221e -norm is highlighted. In practice, it is important to compute good upper bounds on L(f d) with respect to the \u221e -norm for assessing robustness in adversarial examples. Polynomial optimization methods are used, employing positivity certificates like Krivine's certificate or sum-of-squares. In theory, any positivity certificate like sum-of-squares (SOS) can be used. An adaptation of Krivine's certificate to the setting shows that if the polynomial \u03bb \u2212 p is strictly positive on [0, 1]^n, then there exist finitely many positive weights c \u03b1\u03b2. Truncating the degree of Krivine's positivity certificate and minimizing over all possible upper bounds \u03bb results in a hierarchy of LP problems. This sequence of LPs can be implemented by equating coefficients in the canonical monomial basis, with the degree of the certificate needing to be at least that of the norm-gradient polynomial p. The first nontrivial bound corresponds to the depth d, converging to the maximum upper bound. Krivine's positivity certificate offers an LP hierarchy, advantageous over SOS for handling large instances. DSOS and SDSOS hierarchies also provide similar benefits, leading to LP or SOCP. However, the drawback is that the size of LPs from Krivine's certificate can become large due to the dimension of the variable c. For example, in the MNIST dataset with a one-hidden-layer network of 100 neurons, the dimension can be significant. In the next section, the sparsity of neural network architectures is exploited to find LPs of smaller size with similar approximation properties. Up to 90% of network weights can be pruned without affecting accuracy, leading to polynomial positivity certificates of smaller size. An implementation of LiPopt is described for this purpose. In the next section, the sparsity of neural network architectures is utilized to reduce the size of LPs while maintaining approximation properties. A valid sparsity pattern for a polynomial is defined, leading to a smaller positivity certificate for \u03bb \u2212 p over [0, 1] n. This sparse Krivine's certificate is an extension of Theorem 2, providing upper bounds on L(f d) with less computation and memory requirements. The sparse Krivine's certificate, based on a valid sparsity pattern for a polynomial, can reduce the number of polynomials in the certificate, impacting the sparsity pattern's quality. A graph derived from the network f d is used to extract the sparsity pattern for the norm-gradient polynomial. The computational graph of the network f d, represented by vertices for neurons in each layer with edges between connected neurons, can extract a valid sparsity pattern. For dense networks, a valid sparsity pattern for the norm-gradient polynomial can be induced by this graph. However, for non-dense networks, the sparsity pattern may not hold, impacting the convergence of LP values to the maximum of the POP. The LP values converge to the maximum of the POP, providing a valid positivity certificate for upper bounding L(f d). A valid sparsity pattern can be obtained through chordal completion of the correlative sparsity graph. The size of cliques and hierarchy degree impact the optimization problem size. The number of polynomials for each LP in the hierarchy is bounded, with mild dependence on the number of cliques. The size of the optimization problem is impacted by hierarchy and the number of cliques in the sparsity pattern induced by G d. Different network architectures affect the amount of polynomials in the sparse Krivine's certificate. Fully connected networks reduce the size of LPs compared to Krivine's certificate. The cliques in this case have a common intersection up to the second-to-last hidden layer. The total number of polynomials in sparse Krivine's certificate is bounded by O(n(n-1), with sparsity patterns varying in size at runtime. The size of cliques is bounded by s = O(rd) under the layer-wise assumption, leading to an exponential dependency on depth. The number of different polynomials is O(ndr^dk) in 2D Convolutional networks with a certain local structure in weight matrices. The weight matrices of convolutional layers exhibit a local structure with neurons connected to contiguous inputs. The size of cliques is proportional to the volume of a pyramid, resulting in a drastic decrease in complexity. The sparse Krivine's certificate contains O(nd^3k) different polynomials, showing a significant improvement in efficiency compared to unstructured sparsity. This approach has been applied in polynomial optimization and various fields such as safety verification and sensor localization. Our work aligns with applications in safety verification, sensor localization, and optimal power flow. We transform polynomial optimization problems into quadratically constrained quadratic programs (QCQPs) using a well-known procedure. For d = 3, we illustrate the QCQP reformulation with variables corresponding to hidden layers and input. The norm-gradient polynomial is transformed into a quadratic polynomial by introducing new variables corresponding to the product of hidden layer variables. This leads to a QCQP with a SDP relaxation, resolving misconceptions about its limitations to networks with specific dimensions. The QCQP approach includes a relaxation step, limiting its ability to tightly upper bound the value of L(f d). Off-the-shelf semidefinite programming solvers have limitations in handling variables efficiently. For d = 2, this relaxation provides a constant factor approximation to the original QCQP. Further approximation quality results for hierarchical optimization approaches to NP-hard problems are not discussed in this work. The QCQP approach, despite appearing different from hierarchical optimization approaches, is actually similar in essence. Shor's SDP relaxation corresponds to the first degree of the SOS hierarchical SDP solution. Different methods for estimating L(f d) with 2-norm are studied by various authors. The SeqLip method is a heuristic approach that lacks guarantees, while the LipSDP method provides true upper bounds on L(f d) and demonstrates superior performance. The LipSDP method provides true upper bounds on L(f d) and outperforms SeqLip and CPLip. It is limited to the 2-norm, but LiPopt can accommodate any norm describable by polynomial inequalities, offering a key advantage in robustness certification. The local Lipschitz constant can be computed in a subset to provide a larger region of certified robustness. Refining bounds in the restricted domain can yield a tighter estimate. Including additional information in LipSDP for unconstrained networks is unclear. Raghunathan et al. (2018a) found an upper bound for L(f d) with \u221e metric for one-hidden-layer networks. Their approach is limited to d = 2 but is a specific instance of the SDP relaxation method. The SDP relaxation method for QCQPs is a specific instance of the LiPopt framework, using SOS certificates. It provides upper bounds on the local Lipschitz constant for networks with various activation functions. Different estimators of L(f d) with respect to the \u221e norm are considered, including those from SDP relaxation, LP hierarchy, and layer-wise Lipschitz constants. The study compares bounds obtained by different algorithms on networks with random weights and one or two hidden layers. The sparsity level of a network is defined as the maximum number of connections between neurons in consecutive layers. Non-zero weights are sampled uniformly in each layer. Lipschitz bounds are averaged over 10 random networks for various configurations of width and sparsity, with relative error plotted for comparison. The true relative error is approximated using a lower bound from LBS. The study compares bounds obtained by different algorithms on networks with random weights and one or two hidden layers. The computation time for LiPopt-k heavily depends on network sparsity, while SDP time depends only on network size. LiPopt-k becomes better than SDP when the degree is increased by 1. The upper bounds from UBP and LipSDP are too large to be plotted. LiPopt-k offers a better upper bound and faster runtime as network size increases, compared to SDP. It allows for increased computation power for tighter bounds when needed, making it more flexible. LiPopt uses Gurobi LP solver, while SDP uses Mosek. The comparison is done on networks trained on MNIST with a specific architecture. The Lipschitz constant is estimated with respect to a particular label to improve scalability. LiPopt-k offers a better upper bound and faster runtime as network size increases, compared to SDP. It allows for increased computation power for tighter bounds when needed, making it more flexible. The network is trained using a pruning strategy to remove 95% of the weights while preserving test accuracy. Lipschitz bounds for various methods are compared, showing clear improvement with LiPopt-k over SDP, even with k = 3. The input dimension is too large for the Lip-SDP method to provide a competitive bound. Incorporating tighter upper and lower bounds on variables in polynomial optimization problem improves the upper bound obtained by LiPopt. The algorithm for computing bounds on pre-activation values is fast and described in Wong & Kolter (2018). LiPopt-3 provides local upper bounds for increasing radii, global constant bounds, and lower bounds on local Lipschitz constants. Sampling in the neighborhood shows a clear gap between estimates, indicating potential for larger certified balls. In this work, a general approach for computing an upper bound on the Lipschitz constant of neural networks is introduced. The method is based on polynomial positivity certificates and has been shown to tightly upper bound the constant. The optimization problems resulting from this approach are computationally expensive, but the network's sparsity can help reduce the burden. Possible directions for further scaling the method to larger networks include divide-and-conquer approaches, parallel optimization algorithms, and custom optimization algorithms with low-memory costs. The method introduced in this work computes an upper bound on the Lipschitz constant of neural networks using polynomial positivity certificates. Optimization algorithms with low-memory costs and exploiting symmetries in the polynomial can help reduce computational burden. The network's sparsity pattern is crucial for scaling the method to larger networks. The valid sparsity pattern of a dense neural network is crucial for computing an upper bound on the Lipschitz constant using polynomial positivity certificates. The network's architecture ensures that maximal cliques form a valid sparsity pattern, and the computational graph extension is shown to be chordal. The valid sparsity pattern of a dense neural network is crucial for computing an upper bound on the Lipschitz constant using polynomial positivity certificates. In the computational graph extension, it is shown that the graph is chordal, with maximal cliques corresponding to sets in the proposition. This implies that in the subsequence of neurons, at most three belong to the last layer, and there are always at least two nonconsecutive neurons not in the last layer forming a chord."
}