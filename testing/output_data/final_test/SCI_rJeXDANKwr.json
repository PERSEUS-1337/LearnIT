{
    "title": "rJeXDANKwr",
    "content": "Machine learning systems face Out-of-Distribution (OoD) errors with testing data from a different distribution. It is crucial to develop systems that accurately quantify predictive uncertainty and filter out anomalous inputs. Current OoD detection methods are error-prone. To address this, Neural Architecture Distribution Search (NADS) proposes guiding principles for designing uncertainty-aware architectures. Unlike traditional methods, NADS searches for a distribution of architectures. NADS searches for a distribution of architectures to identify common building blocks for uncertainty-aware models. It optimizes a stochastic outlier detection objective and constructs an ensemble of models for OoD detection, outperforming state-of-the-art methods. Detecting anomalous data is crucial for safe machine learning in critical applications like autonomous driving, disease monitoring, and fault detection. The widespread use of deep learning models in autonomous systems has led to issues with detecting abnormal inputs, especially Out-of-Distribution (OoD) data. Current OoD detection approaches are error-prone, and there are no established guidelines for designing architectures to accurately screen out OoD data. Neural Architecture Search (NAS) is a promising option to explore for addressing this gap in knowledge. Neural Architecture Search (NAS) is a promising approach to design uncertainty-aware models by finding optimal neural network architectures. Previous work suggests that ensembles can improve OoD detection performance, indicating the potential benefits of exploring a distribution of architectures for uncertainty awareness. Designing an optimization objective for uncertainty-aware models is challenging, and unsupervised/self-supervised generative models are key for uncertainty quantification. However, these models often assign high likelihoods to out-of-distribution (OoD) data, making log-likelihood maximization inadequate for OoD detection. An alternative approach proposed by Choi & Jang (2018) is using the Widely Applicable Information Criterion (WAIC) as an OoD detection criterion. The text discusses the use of the Neural Architecture Distribution Search (NADS) framework to optimize the WAIC score for uncertainty-aware models. NADS aims to find a distribution of well-performing architectures for OoD detection models by formulating the architecture search as a stochastic optimization problem. This approach is different from traditional NAS optimization strategies and utilizes weight sharing to make the optimization problem tractable. Neural Architecture Search (NAS) algorithms aim to automatically discover optimal neural network architectures for various tasks. The optimization problem is made tractable through weight sharing and parameterization of the architecture distribution. A Bayesian ensemble of deep models is constructed for Out-of-Distribution (OoD) detection using the learned posterior architecture distribution. Previous NAS work has shown success in tasks like image classification, segmentation, object detection, structured prediction, and generative adversarial networks. Neural Architecture Search (NAS) aims to discover optimal network architectures. NAS components include the proxy task, search space, and optimization algorithm. Various optimization methods like reinforcement learning, Bayesian optimization, random search, and gradient-based methods have been used. The design of the proxy task is crucial for evaluating architectures efficiently. Existing proxy tasks in NAS involve leveraging shared parameters, predicting performance using a surrogate model, and early stopping. Unlike existing NAS algorithms that focus on finding a single best architecture, searching for a distribution of architectures allows for analysis of common building blocks. This approach can enhance ensemble methods by creating a more diverse set of models for deep uncertainty quantification. Techniques for uncertainty quantification and out-of-distribution detection in deep models can be model-dependent or model-independent. Model-dependent techniques aim to provide confidence in model predictions. Model-dependent techniques aim to yield confidence measures for a model's prediction when given input data. However, a limitation is that they may discard information regarding the data distribution when learning the task-specific model. Existing methods to calibrate model uncertainty estimates assume access to out-of-distribution data during training, which may not always be available. On the other hand, model-independent techniques seek to estimate the likelihood of data distribution without relying on specific model structures. In this paper, the focus is on finding invertible flow-based architectures for likelihood estimation, which can assign higher likelihoods to out-of-distribution data. These models offer exact computation of data likelihood without requiring out-of-distribution samples during training. Other model-independent techniques include Variational Autoencoders, generative adversarial networks, autoregressive models, and invertible flow-based models. Likelihood-based models can assign higher likelihoods to out-of-distribution data compared to in-distribution data. One hypothesis is that most data points lie within the typical set of a distribution, rather than the region of high likelihood. It is recommended to estimate entropy using multiple data samples to screen out out-of-distribution data. Model ensembling is a natural consideration for entropy estimation in practical data streams. Model ensembles are used for entropy estimation by producing multiple likelihood estimates to augment data points. NADS is proposed as a method to identify diverse architectures for uncertainty quantification, allowing for reliable detection of out-of-distribution data. In NADS, the search formulation is broken down into three main components: the proxy task, the search space, and the optimization method. The training objective aims to derive an ensemble of deep models to improve model uncertainty, different from existing NAS methods. The search space must be large enough to include diverse architectures while allowing efficient traversal by the search algorithm. Our aim is to derive an ensemble of deep models to improve model uncertainty quantification and OoD detection by maximizing the Widely Applicable Information Criteria (WAIC) of the training data. The WAIC score is a Bayesian adjusted metric to calculate the marginal likelihood and is robust towards assigning high likelihoods to OoD data. This strategy captures model uncertainty in a Bayesian fashion. The strategy involves maximizing the WAIC score to improve OoD detection by optimizing the distribution of network architectures p(\u03b1). NADS constructs a layer-wise search space with a pre-defined macro-architecture for this purpose. The search space for network architectures in OoD detection includes different components for each layer, with an emphasis on optimizing the affine coupling layer. Various operations such as pooling, convolutions, skip-connections, and identity are considered in the search space to improve architecture for OoD detection. The search space for network architectures in OoD detection includes different components for each layer, with an emphasis on optimizing the affine coupling layer. Various operations such as pooling, convolutions, skip-connections, and identity are considered in the search space to improve architecture for OoD detection. The following questions towards better architectures for OoD detection are addressed: What topology of connections between layers is best for uncertainty quantification? Are more features/filters better for OoD detection? Which operations are best for OoD detection? Our optimization method for NADS involves addressing difficulties in optimizing a distribution over high-dimensional discrete random variables jointly with network parameters. To overcome these challenges, we introduce a continuous relaxation for the discrete search space and use Monte Carlo samples to estimate the stochastic objective. This allows us to approximately optimize discrete architectures through backpropagation and weight sharing between common architecture blocks. Our optimization method for NADS involves addressing difficulties in optimizing a distribution over high-dimensional discrete random variables jointly with network parameters. Specifically, we aim to find a distribution that minimizes the expected loss of sampled architectures. To achieve this, we reparameterize the architecture distribution to enable backpropagation through the outer expectation and optimize the parameters effectively. Our optimization method for NADS involves reparameterizing the architecture distribution to enable backpropagation through the outer expectation and optimize the parameters effectively. We focus on sampling an architecture with a single hidden layer by finding a probability vector \u03c6 and using Gumbel-Softmax reparameterization to make optimization tractable. The optimization method for NADS involves reparameterizing the architecture distribution for backpropagation. By approximating the gradient of the discrete architecture with a continuously relaxed categorical random variable, backpropagation can be computed. This allows for sampling candidate architectures for retraining, finetuning, or evaluation by annealing the temperature parameter. By sampling architectures from the distribution, we approximate the WAIC score expectation and variance terms. Our architecture search findings show the most likely structure for each block on five datasets. We used the Adam optimizer with a fixed learning rate and batch size for 10000 iterations. The WAIC score was approximated using 4 architecture samples with a temperature parameter of 1.5. The number of layers and latent dimensions is the same as the original Glow architecture, with 4 blocks and 32 flows per block. Images were resized to 64x64 as inputs. Neural architectures were identified in less than 1 GPU day. Neural architectures were identified in less than 1 GPU day, with findings summarized in Figure 2 and more samples in Appendix C. The first few layers have a simple feedforward structure, while deeper layers prefer more complex structures with skip connections. Max pooling operation is rarely selected in the architecture search. The architecture search findings show that max pooling is rarely chosen, while average pooling is preferred in the first layers. Deeper layers tend to have a more complex structure with skip connections resembling ResNets. This complexity aids in extracting more features and improving training speed and stability. The architectures sampled from the NADS perform well in image generation without retraining. A Bayesian ensemble of models is created to estimate the WAIC score, with each model weighted according to its probability. The ensemble is based on posterior architecture distribution, optimizing the WAIC score. The proposed method trained on datasets CIFAR-10, CIFAR-100, SVHN, and MNIST using an ensemble of 5 models from the posterior architecture distribution. Retraining the architectures led to improved out-of-distribution detection, with models retrained for 150000 iterations using Adam optimizer. The ensemble method trained on CIFAR-10, CIFAR-100, SVHN, and MNIST datasets with 5 models from the posterior architecture distribution. Retraining for 150000 iterations using Adam optimizer improved out-of-distribution detection. Increasing ensemble size improved OoD detection performance, outperforming traditional ensembling methods on CIFAR-10. NADS ensemble method effectively screened out samples from datasets not in the original model's training data. The study compared their method against baseline and popular OoD detection methods like ODIN and Outlier Exposure. ODIN recalibrates uncertainty estimates by adjusting softmax scores and perturbing input data, using DenseNet as the base model. Outlier Exposure models minimize a loss term with outlier exposure, requiring OoD access. Our method outperforms baseline methods like ODIN and Outlier Exposure, achieving high AUPR% in screening out OoD data, especially in rare occurrence scenarios. ODIN struggles with OoD detection, indicating instability in certain settings. The study found that ODIN may not be stable for detecting anomalies in different distributions. Developing a novel neural architecture distribution search (NADS) formulation, the researchers identified key features for accurate uncertainty quantification. This approach optimized the WAIC score to improve outlier detection performance. The NADS architecture enables uncertainty quantification in deep learning models without needing access to out-of-distribution samples. It outperforms existing methods in detecting anomalies and is crucial for applications in healthcare, autonomous driving, and disaster response."
}