{
    "title": "Hkxj_LpvvV",
    "content": "Learning domain-invariant representation is crucial for domain generalization. A new method called AFLAC maximizes domain invariance without compromising accuracy, outperforming baseline methods. This approach addresses the dependency between classes and domains, improving classification accuracy in practical situations where distribution assumptions may not hold. This paper discusses domain generalization (DG) in situations where domain and class labels are statistically dependent due to a common latent factor. For example, the WISDM Activity Prediction dataset exhibits this dependency due to data characteristics and collection errors. The dependency between domain and class labels in real-world datasets like BID23 can impact classification accuracy in domain generalization studies. Prior methods focused on invariant feature learning (IFL) to prevent overfitting to source domains, but this approach may negatively affect classification performance in target domains under the dependency. In this paper, a novel method called AFLAC is proposed to maximize domain invariance without interfering with classification accuracy in domain generalization studies. AFLAC aims to achieve accuracy-constrained domain invariance by balancing the entropy values. Empirical validations demonstrate that AFLAC outperforms baseline methods, highlighting the importance of considering domain-class dependency in improving performance. The proposed approach, based on Domain Adversarial Nets (DAN), addresses domain generalization by maximizing domain invariance. Prior methods mainly use IFL, but alternative approaches like semantic alignment have been explored to overcome the trade-off between classification accuracy and domain invariance. The proposed approach in the current text chunk focuses on maximizing classification accuracy for source domains while improving domain invariance, specifically addressing the change of p(y) within source domains in the context of domain generalization. The current text chunk introduces the concept of accuracy-constrained domain invariance in the context of domain generalization. It discusses the maximum domain invariance within a range that does not interfere with classification accuracy, emphasizing the importance of classification for unseen domains. The text also presents a novel method named AFLAC designed to achieve accuracy-constrained domain invariance. The text introduces AFLAC, a method for accuracy-constrained domain invariance in domain generalization. It aims to achieve H(d|h) = H(d|y) through a Nash equilibrium analysis. The Biased Rotated MNIST dataset (BMNISTR) was created by modifying the MNISTR dataset to have different class distributions among domains. Four variants of BMNISTR were generated with varying degrees of domain-class dependency. Training was done using a leave-one-domain-out setting. The study trained models on five domains of sensor data for human activities and tested on the remaining one. Different methods were compared, including CNN, DAN, CIDDG, and AFLAC-Abl for domain generalization and semantic alignment. The study compared AFLAC-Abl with other methods for domain generalization and found that AFLAC outperformed baselines in most dataset-class pairs. It was observed that AFLAC tended to increase performance more significantly for classes with domain-class dependency. The study introduced a novel method called AFLAC, which focuses on maximizing domain invariance without compromising classification accuracy. AFLAC showed superior performance in domain generalization compared to other methods, particularly for classes with domain-class dependency. Additionally, AFLAC demonstrated robustness towards hyperparameter choices, as it maintained accuracy even with strong regularization. AFLAC demonstrated superior domain generalization performance compared to baseline methods, emphasizing the importance of domain-class dependency and the effectiveness of the proposed method."
}