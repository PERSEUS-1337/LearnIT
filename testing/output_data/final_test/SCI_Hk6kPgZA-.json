{
    "title": "Hk6kPgZA-",
    "content": "Neural networks are vulnerable to adversarial examples, and researchers have proposed various attack and defense mechanisms. A principled approach using distributionally robust optimization guarantees performance under adversarial input perturbations. By perturbing the data distribution in a Wasserstein ball, a training procedure augments model updates with worst-case perturbations of training data, achieving moderate levels of robustness with little computational cost. Statistical guarantees allow for efficient certification of robustness, outperforming heuristic approaches for imperceptible perturbations. Recent work shows that neural networks are vulnerable to adversarial examples, where seemingly imperceptible perturbations to data can lead to model misbehavior. Robustness to changes in the data-generating distribution is crucial in modern performance-critical systems, such as self-driving cars and tumor detection. It is irresponsible to deploy models without understanding their robustness and failure modes. Researchers have proposed adversarial attack and defense mechanisms to address misclassification issues in neural networks. While formal verification of deep networks is computationally expensive, convex relaxations of the verification problem have shown some success. This work focuses on developing efficient procedures with rigorous guarantees for small to moderate levels of robustness through distributionally robust optimization and adversarial training. In the context of addressing misclassification issues in neural networks, this work focuses on distributionally robust optimization and adversarial training to provide efficient procedures with rigorous guarantees for small to moderate levels of robustness. The proposed adversarial training procedure offers provable guarantees on computational and statistical performance, with robustness sets that apply even for non-convex losses. The method shows comparable runtimes to other adversarial training procedures in Tensorflow implementation. The training procedures BID22, BID29, and BID33 aim to protect against adversarial perturbations in the training dataset, allowing for generalization to prevent attacks on the test dataset. The approach involves defining a cost function for perturbations and considering a robustness region under the Wasserstein metric. The formulation of the problem is intractable for deep networks, leading to a Lagrangian relaxation approach with a fixed penalty parameter. The penalty problem addresses defense against imperceptible adversarial perturbations by using a robust surrogate function. It allows for moderate levels of robustness at no significant computational or statistical cost for smooth losses. The function in the surrogate is strongly concave, making optimization easy with large enough penalty values. Stochastic gradient methods applied to the problem have similar convergence guarantees as for smooth losses. In Section 3, a certificate of robustness is provided for any \u03c1, with an efficiently computable upper bound on worst-case loss. Results suggest advantages of networks with smooth activations over ReLU's. Experimental verification in Section 4 shows state-of-the-art performance on various adversarial attacks. Robust optimization minimizes losses under uncertainty sets, but has limitations. The robust approach in adversarial training involves perturbing data during optimization using a locally linearized loss function. This method aims to optimize the objective under uncertainty sets, but faces challenges due to the non-concave nature of the inner supremum. The inner supremum in model-fitting techniques is often non-concave, leading to convergence uncertainties and difficulty in finding worst-case perturbations, especially with ReLU activations in deep networks. Smoothness from exponential linear units (ELU's) allows for low computational cost in finding Lagrangian worst-case perturbations. Distributionally robust optimization considers the choice of uncertainty set richness and optimization tractability. Distributionally robust optimization involves considering various parametrizations for uncertainty sets, such as f-divergences and Wasserstein distances, to ensure robustness to unseen data. Tractable classes of uncertainty sets and losses are studied, with approaches like convex optimization for f-divergence balls and converting saddle-point problems to regularized ERM problems for worst-case regions formed by Wasserstein balls. In this work, a larger class of losses and costs is considered, providing direct solution methods for a Lagrangian relaxation of the saddle-point problem. The approach focuses on distributionally robust optimization to defend against adversarial perturbations and develop efficient optimization procedures. The key insight is to assume smoothness of the function and strong convexity in the first argument of the cost function. For robust optimization, a Taylor expansion shows that a 1-strongly convex penalty in the first argument leads to a strongly-concave optimization problem. By leveraging this insight, a computationally efficient approach is provided for robust optimization. A duality result and Lagrangian relaxation for Wasserstein-based uncertainty sets connect distributional robustness to a \"lazy\" surrogate, allowing for efficient minimization using stochastic gradient descent methods. The Wasserstein distances define closeness between distributions in robust optimization. The transportation cost function satisfies certain conditions, and the duality result provides an equality for the Lagrangian relaxation in the robust problem. Proposition 1 provides an alternative proof for convex, continuous cost functions in the robust surrogate problem. It focuses on the Lagrangian penalty problem and its empirical counterpart, relaxing the strict robustness requirements for computational benefits. Stochastic gradient-type methods are developed for the relaxed robust problem, emphasizing the amount of robustness that can be provided. Assuming a continuous function c: Z \u00d7 Z \u2192 R + is 1-strongly convex with respect to the norm \u00b7, and the loss function : \u0398 \u00d7 Z \u2192 R satisfies Lipschitzian smoothness conditions, the robust surrogate \u03c6 \u03b3 is tractably computable and smooth. Lemma 1 shows that for large enough \u03b3 and under Assumption B, the surrogate remains smooth. The text discusses the differentiability of a function f under certain assumptions, the use of Lagrangian relaxation in a stochastic-gradient approach for a penalty problem, and the convergence properties of the algorithm. The convergence properties of Algorithm 1 depend on the loss function. When the loss is convex in \u03b8 and \u03b3 is sufficiently large, a stochastic monotone variational inequality is efficiently solvable with a convergence rate of 1/\u221aT. For nonconvex losses in \u03b8, convergence to a stationary point is guaranteed at the same rate when \u03b3 \u2265 Lzz. Theorem 2 shows that the stochastic gradient method achieves convergence rates on the penalty problem achievable in practice. The gradient method achieves convergence rates on the penalty problem achievable in practice, with the accuracy parameter having a fixed effect on optimization accuracy. The loss function's smoothness in z is key to the convergence guarantee, allowing for computation and certificates of optimality. Replacing ReLU's with sigmoids or ELU's makes distributionally robust optimization tractable for deep learning. In supervised-learning scenarios, adversarial perturbations to feature vectors are of interest. The Wasserstein cost function is defined for the feature vector X. Results can be generalized with minor modifications to the robust surrogate and assumptions. The distributionally robust framework can consider adversarial perturbations to only a subset of coordinates in Z, such as a small fixed region of an image. The cost function can be modified to cover such variants. Our general formulation covers variants of adversarial perturbations on the training dataset. Results show Algorithm 1 protects against attacks on the test set. Results hold uniformly over parameter space \u03b8 \u2208 \u0398. The first main result provides an upper bound on the worst-case objective for any level of robustness. The second result demonstrates adversarial perturbations on the training set. In Section 3.2, adversarial perturbations on the training set generalize, guaranteeing robustness. A data-dependent upper bound for the worst-case population objective is provided. The empirical worst-case loss gives a certificate of robustness to Wasserstein perturbations. The main result of this section provides an upper bound for the worst-case population performance under Wasserstein perturbations up to level \u03c1 n. The bound relies on covering numbers for the model class, ensuring uniform convergence guarantees for empirical risk minimization. Theorem 3 shows that a tight certificate of robustness can be obtained for any level of robustness \u03c1. The certificate of robustness provided in Theorem 3 guarantees robustness up to level \u03c1 for Lipschitz functions, scaling linearly with d despite the infinite-dimensional Wasserstein penalty. Corollary 1 further extends this guarantee for L-Lipschitz functions under certain assumptions. The key consequence is that \u03b3\u03c1 + E Pn [\u03c6 \u03b3 (\u03b8; Z)] certifies robustness. The certificate of robustness provided in Theorem 3 guarantees robustness up to level \u03c1 for Lipschitz functions, scaling linearly with d despite the infinite-dimensional Wasserstein penalty. The key consequence of the bound FORMULA0 is that \u03b3\u03c1 + E Pn [\u03c6 \u03b3 (\u03b8; Z)] certifies robustness for the worst-case population objective for any \u03c1 and \u03b8. The certificate FORMULA0 is easy to compute via expression (10) and the strong duality result, Proposition 1, still applies. The strong duality result, Proposition 1, applies to any distribution, guaranteeing bounds on the sensitivity of a parameter \u03b8 to a particular sample and predicted labeling. Leveraging smoothness shown in Lemma 1, we can efficiently compute the Monge-map and test loss to ensure robustness on the training set generalizes. The text discusses the concentration of robustness achieved for the empirical problem, showing that it uniformly concentrates around its population counterpart. The bound gives the generalization rate for the distance between adversarial perturbations and natural examples. The technique for distributionally robust optimization with adversarial training extends beyond supervised learning, with empirical evaluations on supervised and reinforcement learning tasks. The text presents empirical evaluations on supervised and reinforcement learning tasks comparing performance with different training methods, including fast-gradient method (FGM), iterated variant (IFGM), and projected-gradient method (PGM). It discusses defending against weaker adversaries by training against L2-norm attacks instead of L\u221e-norm attacks commonly considered in adversarial training literature. The text discusses using squared Euclidean cost for feature vectors and covariate-shift adversary for WRM in training against adversarial perturbations. It visualizes differences between approaches and considers supervised learning for MNIST and reinforcement learning tasks. WRM provides theoretical guarantees for large \u03b3 but becomes heuristic for small \u03b3. In Appendix A.4 and A.5, WRM is compared with other methods on attacks with large adversarial budgets. WRM matches or outperforms other heuristics against imperceptible attacks but underperforms for attacks with large budgets. Synthetic data is generated for the first experiment, training a small neural network with different activations and comparing WRM with ERM and FGM. The study compares WRM with other methods on attacks with large adversarial budgets, showing that WRM performs well against imperceptible attacks but struggles with larger budgets. Synthetic data is used to train neural networks with different activations, highlighting how WRM pushes classification boundaries further outwards compared to ERM and FGM. WRM with ELU's provides a certified level of robustness, yielding an axisymmetric classification boundary that hedges against adversarial perturbations in all directions. Certificates of robustness apply for any level of robustness \u03c1, with the worst-case loss evaluated through Lagrangian relaxation for different values of \u03b3 adv. Adversarial perturbations are considered for the test dataset, with WRM's worst-case losses analyzed. The text discusses the certification of robustness for a model \u03b8 against adversarial perturbations, with a focus on worst-case losses on the test dataset. The model is trained on the MNIST dataset using various convolutional filter layers and activations. The certification provides a performance guarantee for different levels of robustness \u03c1, illustrated through figures and scaling of budgets for the adversary. The certification of robustness for a model \u03b8 against adversarial perturbations provides a performance guarantee for different levels of robustness \u03c1. Adversarial training techniques are compared, showing little test-time penalty for robustness levels used for training. Methods like ERM, FGM, IFGM, PGM, and WRM are tested under PGM attacks, with WRM offering more robustness. Training with the Euclidean cost provides robustness to fast gradient attacks. Stability of the loss surface with respect to perturbations to inputs is also studied. The adversarial-training method defends against gradient-exploiting attacks by reducing gradient magnitudes near the nominal input. Different training methods show varying levels of stability, with WRM being the most stable. Adversarial examples cause misclassifications, with IFGM predicting 2, PGM predicting 0, and other models predicting 3, while WRM's misclassifications are consistent. The text discusses WRM's defense mechanisms against gradient-based attacks, focusing on creating a more stable loss surface by reducing gradient magnitudes and improving interpretability. It also explores distributional robustness in Q-learning, a reinforcement learning technique applied to Markov decision processes. In reinforcement learning, robust MDP's aim to maximize worst-case rewards by considering an ambiguity set for state-action transitions. Adversarial state perturbations can be used to ensure distributional robustness in scenarios with continuous state-spaces. This approach provides stability against uncertainties in state-action transitions. In reinforcement learning, robust MDP's aim to maximize worst-case rewards by considering an ambiguity set for state-action transitions. Adversarial state perturbations provide stability against uncertainties in state-action transitions. For tabular Q-learning, adversarial training is tested in the cart-pole environment to balance a pole on a cart by moving left or right. The environment caps episode lengths at 400 steps and ends prematurely if the pole falls too far from the vertical or the cart translates too far from its origin. In reinforcement learning, robust MDP's aim to maximize worst-case rewards by considering an ambiguity set for state-action transitions. Adversarial state perturbations provide stability against uncertainties in state-action transitions. Tabular Q-learning is tested in the cart-pole environment to balance a pole on a cart by moving left or right. The system's dynamics are altered by perturbing physical parameters, resulting in varying levels of instability compared to the original environment. The robust model outperforms the nominal model in harder environments and learns more efficiently in the original MDP. A method for guaranteeing distributional robustness with adversarial data perturbation is provided, showing strong statistical guarantees and fast optimization rates for a wide range of problems. The NP-hardness of certifying robustness for ReLU networks suggests using smooth networks for guaranteed robustness. Empirical evaluations show robustness to data perturbations, outperforming adversarial training techniques. The approach is simple and widely applicable across models. Future investigations are needed due to limitations in optimization and statistical guarantees for deep networks. The recent works of BID1, BID18, and BID39 aim to improve learning-theoretic guarantees in deep learning by introducing margin-based bounds. These guarantees focus on guarding against small-perturbation attacks efficiently, providing protection against attacks with large adversarial budgets. In the large-perturbation regime, training certifiably secure systems is a key challenge. Conventional defense heuristics for image classification may not be effective against attacks with large budgets, rendering images indiscernible to human eyes. Moving beyond current attack and defense models may be necessary for advancements in security research in deep learning. In the large-perturbation regime, training certifiably secure systems is a challenge. Gradient-based perturbations transform images to different labels, with reasonable misclassifications indicating interpretable gradients. Visualizing stability over inputs shows diminishing gains in achieved robustness as penalty parameter \u03b3 decreases. In the large-perturbation regime, training certifiably secure systems is challenging. The smallest perturbation necessary to cause misclassification is illustrated. Figures 9 and 10 show performance differences between various methods are less apparent with larger adversarial budgets. The inner supremum is no longer strongly concave for over 10% of the data, indicating a lack of performance guarantees. For large adversaries, the approach becomes heuristic like other methods. Training FGM, IFGM, and PGM with p = \u221e is considered, along with comparing WRM trained with squared Euclidean cost and squared \u221e-norm cost. Our method, WRM, is trained to defend against L2-norm attacks using a specific cost function. We compare our method with standard adversarial training methods that defend against L\u221e-norm attacks. Direct comparison between these approaches is not immediate, as we need to determine a suitable training budget for different norms. When trained with a small adversarial budget, WRM matches the performance of other methods. When trained with a small adversarial budget, WRM matches the performance of other techniques. However, with a large training adversarial budget, WRM outperforms other heuristics against imperceptible attacks for both Euclidean and \u221e norms. On natural images, WRM consistently achieves a smaller price of robustness compared to other methods. Yet, on attacks with large adversarial budgets, especially in the case of \u221e-norm attacks, the performance of WRM is worse than other methods. These findings confirm that WRM is a practical alternative for moderate levels of robustness where guarantees hold. Our computational guarantees in Theorem 2 do not apply to \u221e-norm adversaries. Solving the inner supremum problem with the \u221e-norm is challenging due to the lack of negative curvature in all directions. A heuristic algorithm based on proximal algorithms is proposed in Appendix E for this task. Comparisons with other adversarial training procedures against \u221e-norm adversaries are shown in figures. Computing worst-case perturbations for feedforward neural networks with ReLU activations is NP-hard. The optimization problem for feedforward neural networks with ReLU activations is NPO, with a separable structure on U. This problem can be solved in polynomial time with respect to the dimension of the inputs. The optimization problem for feedforward neural networks with ReLU activations is NPO and NP-hard. A decision reformulation is introduced to show that the problem is in NP, and it Turing-reduces to 3-SAT. Using an oracle, the decision problem reduces to the optimization problem, proving its NP-hardness. An alternative proof using convex analysis is also provided. The proof provided uses convex analysis and requires the cost function to be continuous and convex. A general duality result is given as a special case of Proposition 1. A function g is considered a normal integrand if it meets certain criteria. Theorem 5 states conditions for functions f and c, with a specific result for any \u03c1 > 0. The mapping P \u2192 W c (P, Q) is shown to be convex in the space of probability measures. The mapping P \u2192 W c (P, Q) is convex in the space of probability measures, allowing for the application of standard duality results. The proof involves showing equality in a joint supremum over P and measures M \u2208 \u03a0(P, Q), with a focus on regular conditional probabilities. The function z \u2192 P (A | z) is measurable for each measurable A. Using Rockafellar & Wets' measurability results, we have DISPLAYFORM7 because f \u2212 c is upper semi-continuous. Let x(z) be a measurable function close to attaining the supremum. Define the conditional distribution P (\u00b7 | z) supported on x(z). This implies equality (6) and completes the proof of uniqueness and Lipschitzness of z (\u03b8). By strong concavity, for any \u03b8, \u03b8, and z, summing inequalities gives where the last inequality follows because g(\u03b8, z) T (z \u2212 z) \u2264 0. Using a cross-Lipschitz condition and Holder's inequality, we obtain. To see the second inequality, we show that f is differentiable with \u2207f(\u03b8) = g(\u03b8, z). If f is inf-compact, then f is directionally differentiable. From Assumption B, it is shown that f is inf-compact and directionally differentiable. By applying Lemma 3 and the strong convexity of f, it is concluded that f is differentiable. The proof is based on a previous one, and gradient steps are performed with a constant step size. Using the L \u03c6-smoothness of the objective F, a Taylor expansion is used to show a specific bound. The text discusses the bound (11) and the concentration of E Pn [\u03c6 \u03b3 (\u03b8; Z)] around its population counterpart. By applying results on Rademacher complexity and entropy integrals, the desired result is obtained. Substituting \u03c1 = \u03c1 n leads to the second result (12), showing that P * (\u03b8) and P * n (\u03b8) are attained for all \u03b8 \u2208 \u0398. The text discusses the attainment of P * (\u03b8) and P * n (\u03b8) for all \u03b8 \u2208 \u0398. By applying Prohorov's theorem and continuity properties of Wasserstein distances, it is shown that P * is attained for all P 0. The concentration result is also demonstrated. The concentration result is shown for the transportation mapping T(\u03b8, z). The lemma states conditions for \u03b81, \u03b82 in \u0398. The proof shows Lipschitz properties of c and z in different cases. The lemma provides conditions for \u03b81, \u03b82 in \u0398 and shows Lipschitz properties of c and z in different cases. The transport map for covariate shift is defined as \u03b2 \u2192 i:vi>\u03b2 (v i \u2212 \u03b2) \u2212 \u03b1\u03bb\u03b2 =: h(\u03b2), which is decreasing. There exists a \u03b2 such that h(\u03b2) = 0 and \u03b2 \u2208 (0, v \u221e)."
}