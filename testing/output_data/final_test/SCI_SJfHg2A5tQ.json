{
    "title": "SJfHg2A5tQ",
    "content": "Deep neural networks (DNN) are resource-intensive for edge devices. Binary neural networks (BNN) use 1-bit activations and weights to reduce resource requirements. An improved binary training method (BNN+) introduces regularization to train weights around binary values and adds trainable scaling factors for model performance. Experimental results on CIFAR-10 show 86.5% accuracy with AlexNet and 91.3% with VGG network. The method outperforms traditional BNN on ImageNet as well. Deep neural networks have shown success in various tasks, but their complexity and large model sizes hinder deployment on resource-constrained devices. For example, AlexNet requires 200MB of memory and VGGNet requires 500MB. The computational cost also necessitates GPU implementation. Model sizes are a challenge for deploying deep neural networks on edge devices due to limited resources. Various methods like network pruning, architecture design, and weight compression through quantization have been developed to reduce computation costs. Quantization approaches like binary and ternary can significantly decrease memory usage by 8x to 32x while maintaining competitive accuracy. Quantization of weights and activations in deep neural networks can reduce computation costs for hardware accelerators like FPGAs and neural network chips. However, there is a challenge in maintaining accuracy compared to full precision models, especially when quantizing activations. The issue stems from noise and lack of precision in training objectives during back-propagation. Despite the computational benefits, closing the accuracy gap between full precision and quantized models remains a challenge due to information loss and increased difficulty in training neural networks. Quantization of weights and activations in deep neural networks can reduce computation costs for hardware accelerators. However, maintaining accuracy compared to full precision models is a challenge due to noise and lack of precision in training objectives during back-propagation. To control the stability of training, heuristics and approximations are proposed to match forward and backward passes. Our contribution includes modifying the activation function to better approximate the sign function in the backward pass and proposing two regularization functions to improve convergence and generalization accuracy of binary networks. Our method focuses on training binary networks by approximating the sign function in the backward pass and introducing regularization functions to encourage weights around binary values. A scaling factor is also used to mitigate accuracy drop. The training procedure restricts weights and activations to single-bit for efficient computation using XNOR and popcount operations. Custom hardware would be needed for operations with higher bits. The binary training method aims to reduce model size and improve inference speed by quantizing weights to {-1, +1}. BID10 is the first network to quantize both weights and activations, achieving comparable accuracy to BinaryConnect. However, it performed poorly on large datasets like ImageNet. The BID23 proposed XNORNet as a trade-off between compression and accuracy on large datasets like ImageNet. They introduced scaling factors for weights and activations to improve performance compared to BNN. DoReFa-Net BID30 further enhanced XNOR-Net by approximating activations with more bits for low bit back-propagation. The BID29 proposed strategies to improve binary neural networks, achieving similar accuracy as XNOR-Net without scaling overhead. They suggest increasing the number of filters to compensate for accuracy loss and use a second-order approximation for the sign activation function. Additionally, they pre-train the network in full precision before binarizing it. The BID29 proposed strategies to enhance binary neural networks, achieving comparable accuracy to XNOR-Net without scaling overhead. They recommend increasing filter numbers to offset accuracy loss and utilizing a second-order approximation for the sign activation function. Additionally, they pre-train the network in full precision before binarizing it. In this section, our method for training 1-bit CNNs is detailed, addressing challenges with weights and activation functions in binary neural networks. The weights are quantized using the sign function, and a continuous approximation is used for the sign activation during the backward pass. In the training of 1-bit CNNs, real-valued weights are binarized to w b in the forward pass, and binary activations are obtained using the sign function for hidden units. A straight through estimator BID7 is used to approximate the gradient of the sign function during backpropagation. The weights are updated within [\u22121, +1] to maintain accuracy, as shown in Figure 1. The training of 1-bit CNNs involves binarizing weights in the forward pass and using binary activations with the sign function. Gradient updates push weights to two modes at -1 and +1. Modifications are made to the activation function to improve performance. The SignSwish activation function, controlled by the parameter \u03b2, is a bounded approximation of the sign function that allows weights to change signs during training. This modification improves performance compared to the htanh activation, as shown in comparisons. The SignSwish activation function, controlled by the parameter \u03b2, allows weights to change signs during training. By adjusting \u03b2, the location at which gradients saturate can be modified. A regularization term is typically added to prevent overfitting in models, with L1 and L2 norms being common choices. In binary training, it is important to encourage weights around -1 and +1 rather than near zero. BID23 introduces a scale to enhance binary network performance, computed dynamically using weight statistics. The regularization term in the SignSwish activation function introduces scaling factors \u03b1 to create a symmetric regularization function with two minimums. These scales are embedded into the network layers and can be learned using backpropagation. The Manhattan and Euclidean regularization functions are defined with \u03b1 as the scaling factor. The proposed regularizing terms aim to penalize weights at varying degrees away from the objective quantization values. The regularization introduced in BID29 does not penalize weights outside of [\u22121, +1]. A trainable scaling factor is included in the regularization function to adapt accordingly. The training procedure is modified by replacing the sign backward approximation with the derivative of SS \u03b2 activation. Real weights are no longer clipped as the network can back-propagate through the SS \u03b2 activation and update the weights. The BNN training introduces additional scales to the network, which are multiplied into the weights of the layers. Regularization terms are added to the total loss function, where meaningful scales are incorporated into the basic blocks of a convolutional neural network to adjust weights prior to operations. The scale parameter in BNN training adjusts weights prior to operations, with scales learned jointly through backpropagation. Initialization is crucial for the scales, and regularization terms are added to the loss function to minimize the objective. The scales are initialized appropriately for the Manhattan and Euclidean penalties. The BNN+ training method introduces regularization terms to the loss function and utilizes the SignSwish function. Scales are initialized with optimal values, and the algorithm is defined in Algorithm 1. The weights and activations are updated on the backward pass, leading to improved training results. The BNN+ training method introduces regularization terms and utilizes the SignSwish function. It updates weights and activations on the backward pass for improved training results. BatchNorm() and BackBatchNorm() handle activation normalization and back-propagation. ADAM() updates parameters based on gradients. The method is evaluated on CIFAR-10 and ImageNet datasets, showing accuracy gains compared to other binary networks. The CIFAR-10 dataset consists of 50,000 train images and a test set of 10,000. AlexNet and VGG architectures are trained using the ADAM optimizer. VGG architecture includes convolutional and fully connected layers with batch normalization. Different learning rates were experimented with, and batch sizes of 256 were used for training. The learning rates for AlexNet and VGG were set at 10^-3 and 3 x 10^-3 respectively, with a reduction factor of 10 every 10 epochs for 50 epochs. Regularization parameter \u03bb was set to 10^-6. Weights were initialized using BID2 initialization and scales were introduced for each convolution filter. The ILSVRC-2012 dataset consists of 1.2M training images and 1000 classes, with typical data augmentation techniques applied. Images are resized to 256x256, randomly cropped to 224x224, and normalized using mean and standard deviation statistics. At inference time, images are scaled to 256x256, center cropped to 224x224, and normalized. Training method performance is evaluated on AlexNet and Resnet-18 architectures with batch-normalization before each activation function. First and last layers are kept in full precision to maintain accuracy. Results are compared to BID10, BID23, and BID29 in experiments involving R1 and R2 regularization. Weights are initialized using a pre-trained model with htan activation function. Learning rate for AlexNet is set to 2.33 x 10^-3 and reduced by 0.1 at the 12th and 18th epochs. The learning rate for AlexNet is set to 2.33 x 10^-3 and reduced by 0.1 at the 12th and 18th epochs, while for the 18-layer ResNet, it starts at 0.01 and is reduced at the 10th, 20th, and 30th epochs. Two regularization terms and an activation term with a trainable parameter \u03b2 were proposed. Experiments showed that heavy penalization in the regularizing term can hinder network convergence. The regularization term with small values was found to be important for network convergence. SwishSign approximation was beneficial compared to STE, and using moderate values of \u03b2 was better than small or large values. The order of R1 matched the cross-entropy loss metric better than R2. The regularization term with small values is important for network convergence. Moderate values of \u03b2 are better than small or large values because small \u03b2 leads to poor gradient approximation and large \u03b2 causes large fluctuations in weights. Comparisons with BinaryNet, XNOR-Net, and ABC-Net show the importance of properly tuning the learning rate for convergence. Top-1 and top-5 accuracies of BNN+ are summarized in Table 3, showing competitive results. The text discusses the importance of regularization in network convergence and the impact of tuning learning rates for binary training. Three incremental ideas are proposed to improve binary training, resulting in competitive results when training AlexNet and Resnet-18 on the ImageNet dataset. Future work includes extending these ideas to efficient models like CondenseNet, MobileNets, and MnasNet for object recognition tasks."
}