{
    "title": "Syx33erYwH",
    "content": "Imitation learning is extensively studied for single-agent and multi-agent settings with MDP and MG models. A novel framework for asynchronous multi-agent generative adversarial imitation learning (AMAGAIL) is proposed for extensive Markov games, guaranteeing subgame perfect equilibrium. Experiment results show AMAGAIL outperforms state-of-the-art baselines in inferring expert policies from demonstration data. Imitation learning allows agents to imitate expert demonstrations in decision-making scenarios. Inverse reinforcement learning recovers a reward function from expert demonstrations, but it is an ill-posed problem due to multiple reward functions matching the data. Various principles are employed to solve this ambiguity. Recent works have extended imitation learning to scenarios with multiple interacting agents, using multi-agent Markov games (MGs). While existing methods focus on synchronous MGs, where all agents make decisions simultaneously, real-world scenarios often involve asynchronous decision-making. This limitation hinders applications in multiplayer games like Go and card games, where agents take turns influencing each other's decisions. In this paper, a novel framework called asynchronous multi-agent generative adversarial imitation learning (AMAGAIL) is proposed for Markov games. It focuses on asynchronous decision-making processes where experts provide demonstration data, and AMAGAIL learns each expert's decision-making policy. The participation order of agents in decision-making can be deterministic or stochastic, and a player function captures this order and dependency among agents. The AMAGAIL model generalizes MAGAIL from synchronous to asynchronous Markov games, guaranteeing subgame perfect equilibrium. It outperforms GAIL and MAGAIL in inferring expert policies from demonstration data in asynchronous decision-making scenarios. Markov games involve N interacting agents making sequential decisions based on the current state. In Markov games, agents interact based on the current state. A Markov game is defined by a tuple with sets of states, actions, and participation probabilities. The participation vector indicates active agents at each time step, and the player function governs the probability of agent actions based on participation history. The player function in non-Markov processes can be extended to higher-order forms based on previous participation and state-action history. Agents aim to maximize their total expected return by choosing actions through a stochastic policy. Bold variables represent concatenation of variables for all agents, while subscript -i denotes all agents except for i. In Markov games, agents aim to maximize their return by choosing actions through a stochastic policy. Differentiating from synchronous Markov games, where all agents act simultaneously, Markov games involve agents making decisions independently at each time step. In synchronous Markov games (SMGs), agents make simultaneous decisions to maximize their expected return. Nash equilibrium (NE) is used to resolve interdependent optimal policies. However, in asynchronous Markov games, agents may face subgames from other agents' actions, not considered in NE. Subgame perfect equilibrium (SPE) addresses this issue by ensuring NE for every possible subgame. In synchronous Markov games, MAGAIL was proposed to learn experts' policies constrained by Nash equilibrium using a maximum causal entropy regularizer. The optimal policies are found by solving a multi-agent reinforcement learning problem with a weight \u03b2 for entropy regularization. MAGAIL applies multi-agent IRL (MAIRL) to recover experts' reward functions with \u03c8 as a convex regularizer. In extending multi-agent imitation learning to general Markov games, the challenge lies in asynchronous decision making and dynamic state participation. To address this, subgame perfect equilibrium (SPE) solution concept is applied to ensure Nash equilibrium at each state. An SPE is a set of agents where no agent can unilaterally achieve a higher reward by changing their strategy at any state. The constrained optimization problem in multi-agent imitation learning ensures subgame perfect equilibrium (SPE) where no agent can unilaterally achieve a higher reward by changing their strategy. The objective is to define a suitable inverse operator for policies forming a SPE under a reward function. The objective is to define an inverse operator AMAIRL for policies in a subgame perfect equilibrium (SPE). The key idea is to create a margin between sets of policies to capture the \"difference\". By implementing (t+1)-step constraints, the Lagrangian dual is constructed. By implementing the (t+1)-step formulation, the Lagrangian dual of the primal is constructed to focus on constraints and obtain the dual problem. Theorem 2 shows that a specific \u03bb can recover the difference in expected rewards between optimal and non-optimal policies. The dual function L(t+1)r(\u03c0*, \u03bb*\u03c0) represents the probability of generating trajectories for each agent. The asynchronous occupancy measure in Markov games defines the distribution of state-action pairs encountered by an agent under different situations. With a hyper-parameter \u03b2 controlling the strength of the entropy regularization term, the AMAIRL objective function is established with a regularizer \u03c8. The occupancy measure \u03c1 can be interpreted as the distribution of state-action pairs encountered by an agent. The text discusses practical algorithms for asynchronous multi-agent imitation learning, introducing scenarios with different player function structures. It mentions the use of a generator and discriminator in a generative adversarial model for each agent, where the generator's behavior is scored by the discriminator. The text introduces practical algorithms for asynchronous multi-agent imitation learning, utilizing a generative adversarial model with a generator and discriminator for each agent to maximize its score. The input for AMAGAIL is demonstration data from expert agents, and the RL process aims to find each agent's policy. In the RL process of finding each agent's policy, Multi-agent Actor-Critic with Kronecker-factors (MACK) is used for variance reduction. The player function Y determines the order in which agents make decisions, with options for synchronous, deterministic, or stochastic participation. In cooperative and competitive games, agents can have deterministic or stochastic player functions, where they can only output 1 or 0 at each step. AMAGAIL is evaluated with both structures and compared to BC and MAGAIL. The study evaluates generative adversarial imitation learning (MAGAIL) in a particle environment with four games involving cooperative navigation. Three agents cooperate to reach landmarks, avoiding collisions, with rewards based on proximity. Agents follow a deterministic order for actions. The games involve deterministic and stochastic player functions for cooperative navigation and reaching landmarks with minimum collision. In one game, slower agents chase a faster agent with a stochastic player function. In a game environment, faster agent acts first, followed by adversaries with a 50% chance to act, all aiming to catch the faster agent while avoiding landmarks. Adversaries collect rewards by touching the agent, who is penalized. Agents trained with Multi-agent ACKTR are evaluated based on true reward functions. AMAGAIL is compared with behavior cloning and other baselines. In comparison to behavior cloning and decentralized Multi-agent generative adversarial imitation learning (MAGAIL), AMAGAIL is evaluated in three particle environment games: deterministic cooperative navigation, stochastic cooperative navigation, and deterministic cooperative reaching. The comparison includes normalized rewards when learning policies with BC, MAGAIL, and AMAGAIL. When there is a small amount of expert demonstrations, BC and AMAGAIL show increased rewards, especially with less than 400 demonstrations. After using more than 400 demonstrations, AMAGAIL outperforms BC and MAGAIL. AMAGAIL accurately learns expert policies by correctly characterizing non-participation events with the player function Y. BC's rewards remain unchanged, contradicting previous findings. In stochastic cooperative navigation, BC's performance improves with more demonstrations, converging to the best performance around 0.65 with 300 demonstrations. Deterministic cooperative navigation is easier to learn compared to reaching. Comparison of agent rewards in stochastic predator-prey shows BC and MAGAIL methods, with AMAGAIL outperforming BC and MAGAIL after 400 demonstrations. In the stochastic cooperative navigation game, AMAGAIL outperforms MAGAIL and BC, but in the deterministic game, AMAGAIL does not perform as well due to player #3's lack of motivation to maximize shared rewards. In a mixed game mode with cooperative and adversarial players, AMAGAIL's performance is evaluated in a stochastic predator-prey game. Due to the presence of competing sides, direct comparison of methods' performance is challenging. In a mixed game mode with cooperative and adversarial players, AMAGAIL's performance is evaluated in a stochastic predator-prey game. Comparing methods' performance directly is challenging due to the presence of two competing sides. Using Song et al. (2018)'s evaluation paradigm, AMAGAIL consistently outperforms MAGAIL and BC in playing against other methods. Various approaches like Imitation Learning (IL), Behavioral Cloning (BC), Apprenticeship Learning, and Inverse Reinforcement Learning (IRL) are studied for learning policies from expert demonstrations and recovering underlying rewards for reinforcement learning. The curr_chunk discusses expert trajectories and imitation learning methods like GAIL and cGAIL, which incorporate maximum casual entropy IRL and generative adversarial networks to learn policy and reward functions. It also mentions recent studies on multi-agent imitation learning in synchronous Markov games. However, these studies fail to address more general interaction scenarios like turn-based games. In this paper, the authors introduce an asynchronous multi-agent generative adversarial imitation learning (AMAGAIL) framework for modeling asynchronous decision-making in Markov games. The proposed player function captures agent participation dynamics, allowing accurate learning of expert policies from trajectory data. The framework can handle more complex participation scenarios beyond binary choices, demonstrating superior performance compared to existing methods. The text discusses the subgame perfect equilibrium in the context of asynchronous decision-making in Markov games. It presents a theorem and lemma showing the equivalence of solutions in AMA-RL(r) and subgame perfect equilibrium. The conditions for subgame perfect equilibrium are defined based on the discounted expected return and agent actions. The text discusses the conditions for subgame perfect equilibrium in asynchronous decision-making in Markov games. It shows that if certain constraints hold, value iteration converges. If a policy exists where an agent can receive a higher reward, then there is a violation in the constraints. In asynchronous decision-making in Markov games, the optimal reply to a policy is the subgame perfect equilibrium if certain constraints hold. Value iteration converges when these constraints are met, and a unique solution is assumed in the single-agent scenario. In asynchronous decision-making in Markov games, the optimal reply to a policy is the subgame perfect equilibrium if certain constraints hold. Value iteration converges when these constraints are met, and a unique solution is assumed in the single-agent scenario. In Proposition 1, the entropy regularizer is removed due to the dependency of causal entropy on the policies of other agents. The algorithm generates state-action pairs from policies and filters out state-action pairs for each agent in the batch. In asynchronous decision-making in Markov games, the algorithm filters out state-action pairs for each agent in the batch. The policy generator network uses two-layer perceptrons with 128 cells in each layer for the particle environment."
}