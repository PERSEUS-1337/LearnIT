{
    "title": "Skvd-myR-",
    "content": "Measuring visual (dis)similarity between instances in data distribution is crucial for applications like image retrieval. Non-metric distances can create a more accurate similarity model by capturing non-linear data distribution. This study explores using deep learning networks to approximate non-metric similarity functions for image retrieval datasets. In computer vision, determining visual similarity between images is challenging due to the semantic gap problem, which is the difference between low-level image pixels and higher-level semantics. Estimating visual similarity is essential in computer vision for tasks like image retrieval, classification, and object recognition. This involves bridging the semantic gap between low-level image pixels and high-level concepts. Various methods have been proposed to represent visual information from raw pixels in images, aiming to accurately rank images based on similarity scores. Various methods have been proposed to represent visual information from raw pixels in images, aiming to accurately rank images based on similarity scores. Visual similarity is commonly measured by computing a standard metric between images, but regular distance metrics may not capture nonlinear data dependencies. Learning a similarity estimation directly from visual data can improve performance on image retrieval tasks. Data can enhance image retrieval performance by learning nonlinear dependencies within the dataset through similarity functions. Traditional distance metric learning algorithms rely on linear metrics, but nonlinear approaches are more effective for visual data with nonlinear interdependencies. Standard metric axioms may not apply to human perception of visual similarity, leading to the proposal of non-metric similarity functions. Deep learning methods focus on mapping pixels to a linear space for Euclidean distance application, while a neural network-based approach suggests learning a non-metric similarity score in the feature space. The proposed approach utilizes neural networks to learn a non-metric similarity score in the feature space for image retrieval tasks. By training deep learning models, a visual similarity function is estimated, outperforming standard metric computations. Two neural networks are used to extract image representations and compute the visual similarity score, trained with pairs of similar and dissimilar images. The output score from the similarity network can be directly applied for image ranking in retrieval tasks, as demonstrated in experiments on standard datasets. The network can discriminate between similar and dissimilar image pairs, improving standard metrics scores. Content-based image retrieval ranks images based on visual similarity to a query image. Traditional methods use hand-crafted features like SIFT, while newer approaches leverage deep learning features from CNNs for better efficiency and scalability. Deep Learning for Image Retrieval has advanced with the use of CNNs, where mid-layer representations from convolutional layers have shown better performance than fully connected layers. Various methods have been explored to aggregate high-dimensional convolutional representations into compact vectors or train networks to learn compact binary codes directly. Fine-tuning networks with similar data has been shown to be effective in this process. Some authors have shown that fine-tuning networks with similar data can significantly improve performance. Recent work has also demonstrated the benefits of adding attention models for image retrieval. Learning a similarity function directly from data may better capture human perception of visual similarity. Popular similarity learning methods include OASIS and MLR, which optimize weights for linear metric learning. Nonlinear similarity learning based on deep learning has been applied to various visual contexts, including low-level image matching with CNNs for stereo matching and optical flow, and high-level image matching for face verification, retrieval, classification, and product search using siamese or triplet architectures. These methods aim to learn a mapping from image pixels to a low-dimensional target space for computing similarity decisions. Our approach seeks to learn the nonmetric visual similarity score itself, rather than projecting visual data into a linear space. Inspired by previous results, we propose training a deep learning algorithm for non-metric image retrieval systems. The proposal is to train a deep learning algorithm to learn non-metric similarities for image retrieval, aiming to improve results on top of high-quality image representation methods. Visual similarity is defined as measuring the relatedness of two images based on their visual content. The goal is to learn a visual similarity function that computes similarity scores from global image representations using a nonlinear function. The proposal suggests training a deep learning algorithm to learn non-metric similarities for image retrieval, aiming to enhance results on top of high-quality image representation methods. It is argued that rigid constraints of metric axioms may not be compatible with human perception, as shown in the example of a centaur being visually similar to a person and a horse, but the person and the horse not being similar to each other. To address this, a non-metric similarity function g is proposed to be learned using a neural network approach. The image representation method used is RMAC, a deep global representation obtained from the last convolutional layer of a pretrained CNN. The response of the filters in the last convolutional layer is represented by a regional feature vector fR. The regional feature vector fR is defined as the maximum activation of each filter within a spatial region in the feature map. Multiple regional features are extracted at different multi-scale overlapping regions, post-processed with normalization and PCA-whitening, and then summed and normalized to obtain a final compact vector. The final vector size is independent of the input image size or aspect ratio. To compare two images and obtain a visual similarity score, a deep learning architecture is trained to learn the similarity function g using K-dimensional global vectors extracted from the input images. The similarity network architecture involves concatenating two K-dimensional global vectors and feeding them into fully connected layers with non-linear functions. Unlike the standard siamese architecture, this approach trains and updates the similarity network on top of high-quality vector representations. The image representation block weights are not necessarily shared in this architecture. The input size is fixed at 1 \u00d7 K \u00d7 2, with hidden layers of size 1 \u00d7 Ch \u00d7 2 \u00d7 Ch. The similarity network architecture involves fully connected layers with non-linear functions for hidden layers of size 1 \u00d7 Ch \u00d7 2 \u00d7 Ch. The output layer size is 1 \u00d7 Ch \u00d7 2 \u00d7 1 without a ReLU layer to cover a full range of values. The regression loss function penalizes the difference between predicted and annotated similarity scores. Four configurations A-D with different filter numbers and hidden layers are tested. The network is trained in three stages, progressively learning a standard similarity function based on cosine similarity in Stage 1. The similarity network architecture involves fully connected layers with non-linear functions for hidden layers of size 1 \u00d7 Ch \u00d7 2 \u00d7 Ch. In Stage 2, the model learns to discriminate between similar and dissimilar image pairs by adjusting similarity scores based on a margin parameter \u2206. In Stage 3, the similarity network is refined by training it specifically using difficult pairs of images to improve performance. Fine-tuning neural networks with challenging samples has been shown to be beneficial for discrimination between different classes. In Stage 3, the similarity network is refined by training it with difficult image pairs to enhance performance. Evaluation is done using mean Average Precision (mAP) on standard image retrieval datasets like Oxford5k and Paris6k. For Land5k, relevance is determined by belonging to the same class. Having a training dataset similar to the final task is crucial for this approach. Creating different versions of the training dataset is crucial for evaluating the impact of using various samples in the training process. Landmarks BID8 is a subset of the Landmarks BID1 dataset with 33,119 training images and 4,915 validation images. Landmarks-extra500 includes additional images from Oxford5k and Paris6k datasets, totaling 33,619 training images. Landmarks-extra expands on this by adding more images from Oxford5k and Paris6k classes, resulting in 35,342 training images from 605 landmarks. The curr_chunk discusses the use of RMAC representations with the VGG16 network for image processing. It mentions the sensitivity of RMAC representations to PCA matrices and the training of similarity learning using random pairs. The query images are kept unseen by the system. The curr_chunk discusses the use of RMAC representations with the VGG16 network for image processing. PCA whitening is applied to Paris5k images, and images are rescaled to 1024 pixels. Four different configurations A-D TAB0 are explored for the similarity network. The network is optimized using backpropagation and stochastic gradient descent with specific parameters. The visual similarity network involves millions of parameters, increasing computational cost but still feasible to compute in a reasonable time. Training time is about 5 hours on a GeForce GTX 1080 GPU. The training time for the visual similarity network is approximately 5 hours on a GeForce GTX 1080 GPU. Testing time for image pairs is 1.25 ms on average (0.35 ms with cosine similarity). Four configurations A-D are proposed for the network, with configuration C showing the best performance in terms of mean squared error and correlation coefficient. In this section, the performance of networks B and D is very close to network C, which requires 76 million parameters. Configuration B is chosen as the default architecture for further experiments. The benefits of using a non-metric distance function trained with neural networks are studied, focusing on the visual similarity computation. The mAP is computed at each training stage, with results compared to cosine similarity. Our approach, DeepCosine, DeepSim, and DeepSimH, outperforms the standard similarity learning algorithm OASIS in all testing datasets. Linear metric learning is also compared, showing that our similarity networks excel in performance. Our approach, DeepCosine, DeepSim, and DeepSimH, outperforms the standard similarity learning algorithm OASIS in all testing datasets. Results show significant improvements ranging from 20% to 40% when using Landmarks-clean-extra as a training dataset. Visual similarity can be learned even with a reduced subset of the target image domain. Simple linear metrics struggle with affine transformations, unlike our proposed methods. Training dataset significantly impacts the network's ability to learn image similarity. The results suggest that the network can learn image similarity and provide a similarity score. DeepSimH consistently improves mAP compared to cosine similarity, especially when \u2206 is not 0. Comparison with state-of-the-art techniques shows the effectiveness of our method. The methods are categorized into off-the-shelf and fine-tuning approaches for fair evaluation. Our DeepSimH approach outperforms previous methods in every dataset by using a similarity network instead of cosine similarity. DeepSimH achieves the best mAP precision in Ox5k dataset and comes second in Ox105k and Pa106k. The method learns visual similarity directly from visual data by training a neural network model. Training a neural network model to learn visual similarity estimation outperforms rigid distance-based approaches in image retrieval. Learning a nonmetric visual similarity function is beneficial with a small subset of images available during training. Standard image retrieval techniques like query expansion can be applied on top of the similarity network. Future work includes efficient computation of K-nearest neighbors based on the learned network similarity function. In this appendix, the influence of the dataset used to train the similarity network and estimate visual similarity between images is discussed. Visual similarity does not transfer well across domains, requiring a subset of samples from the target dataset during training to learn a meaningful similarity function. The performance of the similarity network is correlated with the number of samples from the target dataset used during training. In the study on metric learning (BID18), it is noted that not including samples from the target dataset during training a similarity function can be detrimental. The similarity network surpasses standard metric results even with a small number of target dataset samples used in training. Visual results are presented, showing the effectiveness of the similarity network in generalizing from a limited subset of target samples. The study highlights the benefits of training a similarity network over a standard metric function like cosine similarity for image retrieval. Results show that using the similarity network, called DeepSim, improves performance compared to traditional methods. The network isolates the similarity computation part in the image retrieval pipeline, leading to better results on testing datasets. In this appendix, an end-to-end approach for image retrieval is explored, utilizing a visual similarity network function. The approach involves feeding the system with pixels to obtain a similarity score between images. The architecture includes feature extraction using MAC compact image representation and a visual similarity network called DeepSim. The system is end-to-end differentiable, allowing for backpropagation during training. The approach for image retrieval involves using MAC as compact image representation and a VGG16 network for feature extraction. The image is pre-processed by resizing to 720 pixels, mean subtraction, and l2-normalization. The weights of the similarity network are learned through different stages of training, including freezing and fine-tuning the VGG16 network. Fine-tuning the architecture involves training all layers one last time using pre-trained layers. This process is done with 200,000 pairs of images from the Landmarksextra dataset for 5,000 iterations. Using MAC Tolias instead of RMAC shows a performance boost, especially when trained end-to-end. The key message is that fine-tuning the final similarity computation, rather than relying on cosines, can significantly improve accuracy results. Fine-tuning the final similarity computation, rather than relying on cosines, can significantly improve accuracy results, as researchers have been doing so far. This step may push accuracy results higher irrespective of the feature vector computation."
}