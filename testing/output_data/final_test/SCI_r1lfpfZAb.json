{
    "title": "r1lfpfZAb",
    "content": "Recurrent Neural Networks (RNNs) are used for learning patterns in natural language, but their generated language can be generic and repetitive. A new learning framework is introduced to improve generation quality by combining different models. Human evaluation shows the effectiveness of the approach. The new learning framework collectively addresses the limitations of RNN generation, significantly enhancing the overall coherence, style, and information content of the generated text. RNN-based language models like LSTMs and GRUs have been successful in learning fluency patterns in natural language but struggle as generators, producing generic, repetitive, and self-contradictory output. When RNNs are used for long-form text generation, they often prioritize repetitive and generic sentences over higher quality ones due to a lack of strong inductive bias in their network architectures. This results in the loss of long-term context in favor of short-term context. When RNNs are used for long-form text generation, they prioritize short-term context over long-term context, leading to shallow and myopic patterns. Methods to improve generation include using diversity-boosting objective functions and prohibiting recurrence of the same trigrams. These constraints offer a partial solution to the issues faced by RNNs in generating complex and coherent language. The curr_chunk discusses the limitations of hand-tailoring rules for improving text generation and proposes a general learning framework to construct a better decoding objective. By combining discriminatively trained models with a generatively trained RNN language model, the framework aims to address the shortcomings of the base RNN generator. This approach offers a more robust solution to improving text generation compared to traditional methods. The learning framework generalizes over existing modifications to the decoding objective by incorporating language generated from RNNs as negative samples to train companion models. Empirical results show a significant improvement in converting a generic RNN language model into a stronger generator. Human evaluation confirms the model's superiority in coherence, style, and information content. The framework is motivated by Grice's Maxims of communication, specifically focusing on Quantity. The RNN generations tend to be overly short and generic, lacking in information. To address this issue, length and repetition models are proposed. Additionally, RNNs used for long generation can become self-contradictory and digress easily. To combat these problems, entailment and relevance models are suggested. The RNN generation also lacks style and specificity, favoring generic words and phrases. The lexical style model addresses the issue of generic language generation by proposing a learning framework for conditional language generation. It combines RNN language model probabilities with additional scores produced by communication models and weighted with mixture coefficients. Generation is done using beam search, scoring candidate generations at each time step. The RNN language model decomposes into per-word probabilities to allow for more diverse and specific generation. The lexical style model proposes a learning framework for conditional language generation, combining RNN language model probabilities with additional scores from communication models. Generation is performed using beam search, scoring candidate generations at each time step. The RNN language model decomposes into per-word probabilities for diverse and specific generation, while introducing models based on Grice's Maxims of communication to discriminate between good and bad generation. The model is trained to discriminate between good and bad generation using Grice's Maxims. Different aspects of communication are focused on by varying model parameterization and training examples. The classifier scores are used as classification probabilities in the objective function for conditional generation. The first layer embeds words into 300-dimensional vectors, and RNNs tend to bias towards shorter generation even with attention. The model is trained to discriminate between good and bad generation using Grice's Maxims. RNNs tend to bias towards shorter generation even with attention, so length normalization is common practice. The goal is to learn to distinguish between RNN-generated and gold continuations by exploiting the observation that repetitions are more common in RNN-generated completions. A geometrically decaying length reward is used, with an initial value and decaying rate tuned on final systems. The base RNN is used as a source of negative examples for repetition detection. The model is trained to discriminate between good and bad generation using Grice's Maxims. RNNs tend to bias towards shorter generation even with attention, so length normalization is common practice. The goal is to learn to distinguish between RNN-generated and gold continuations by exploiting the observation that repetitions are more common in RNN-generated completions. A geometrically decaying length reward is used, with an initial value and decaying rate tuned on final systems. The base RNN is used as a source of negative examples for repetition detection. Natural levels of repetition are modeled by computing a score for each token in the continuation based on pairwise cosine similarity of word embeddings within a fixed window of the previous words. The model is trained to maximize binary cross-entropy, with gold continuations as positive examples and samples from the base RNN as negative examples. Word embeddings are kept fixed during training for this model. The entailment model guides the generator to avoid contradicting its own past generation. The model is trained to guide the generator using Grice's Maxims to avoid contradictions and entailments in its output. A classifier is trained on sentence pairs to predict relations as contradiction, entailment, or neutral, with a focus on discouraging contradiction and entailment. Sentence representations are obtained using word embeddings, and an MLP classifier is used for prediction. An MLP classifier with a single tanh hidden layer is used to score completed sentences in a context and continuation sequence. The classifier achieves 63% validation accuracy and is applied to sentence embeddings with element-wise difference and multiplication. The entailment score is computed against preceding sentences in both x and y, selecting the pair with the least confidence in neutral entailment. The relevance model predicts if a candidate continuation is relevant to the context by distinguishing between true and random continuations. A single convolutional layer is applied to context and candidate embeddings, with a scoring function defined for similarity amplification. The model optimizes ranking log likelihood to distinguish between true and random endings, achieving 85% accuracy on the validation set. Context influences the relevance of topics in the continuation, with a centroid predicted in the embedding space to capture this intuition. The score is computed accordingly. The lexical style model is trained with a ranking criteria similar to the relevance model, improving diversity in word distribution while encouraging topic continuation. The scoring function is defined as DISPLAYFORM0, and the model learns weight coefficients \u03bb for the combined objective. The model is trained to linearly combine scoring functions using weight coefficients \u03bb. The objective ranks the gold continuation higher than the predicted one. Inference is based on current \u03bb values during training, creating a dynamic learning signal. Generation is done with beam search due to limitations of greedy decoding. During generation, a beam search procedure is used with a beam size of 8. The approach involves rescoring current hypotheses with the full objective function to increase diversity. Next word candidates are sampled instead of selecting the highest scoring ones. Working vocabulary score is integrated before sampling next word candidates. During generation, a beam search procedure with a beam size of 8 is used. The entailment score of a candidate is recomputed only when sentence-terminating punctuation is generated. The score is not accumulated across sentences in beam-search. Two corpora, TripAdvisor reviews and ROCStory corpus, are used for evaluation. The language model is trained on two corpora: the TripAdvisor corpus and the ROCStory corpus. The model is pre-trained on the Toronto Books corpus before training on the ROCStory text. The repetition ratio of words in the generated endings by the language model is higher than the reference endings in both corpora. The language model is evaluated with a BLEU score of 1.48 compared to 1.17 for the references. Human evaluation is considered the most reliable measure for open-ended generation tasks. Automatic measures like BLEU, ROUGE, and Meteor may not be suitable for long or creative text generation. Multiple generation samples are provided to showcase different model characteristics. The model's evaluation involves generating an appropriate ending based on an initial context of n sentences, compared against a human-written gold reference ending for automatic evaluation. The evaluation of the language model includes human assessment based on Grice's Maxims criteria and a Turing test question to determine if the ending was written by a human. 1000 passages from each corpus are used for automatic and human evaluation, with scores compared between baselines and the model on both datasets. The evaluation of the language model includes human assessment based on Grice's Maxims criteria and a Turing test question to determine if the ending was written by a human. Results for both automatic and human evaluation metrics are presented for both corpora in Table 2. The L2W (NO WK.VOCAB) model scores higher on human metrics and achieves nearly twice the percentage of passed Turing tests compared to the language model baseline. The language model performs well on the TripAdvisor task but slightly worse due to vocabulary scoring. The full model brings more focus but reduces overall coherence. In the ROCStory task, the raw language model competes with L2W (FULL) due to simpler data. However, L2W (FULL) still outperforms on all human metrics. The ROCStory corpus requires specific responses to maintain coherence. The ROCStory corpus requires specific responses for coherence. L2W (FULL) excels in matching context topics, yielding higher scores. Low BLEU scores in TripAdvisor are due to length penalty. Learning to Write models generate shorter endings, affecting BLEU scores. Base language model completions are longer but repetitive. L2W generations are more topical and coherent. The L2W system excels in matching context topics, condensing content into fewer words while maintaining coherence. However, it still has room for improvement in avoiding repetitive phrases and maintaining fluency. The L2W system excels in condensing content while maintaining coherence, but can improve in avoiding repetitive phrases and maintaining fluency. RNN-based paragraph generation has been attempted in various domains, using sequence-to-sequence architectures to guide output generation. The large-scale training data required for robust sequence-to-sequence models is often difficult to obtain, leading to repetitive and contradictory generation. Alternative decoding objectives, such as diversity-promoting objectives, have been proposed to address these issues. The communication models presented in the current work offer a contextual approach by comparing context with continuation candidates. Incorporating reverse conditional probability can prevent the explaining away problem of long-term context, but it is more expensive for decoding. These models can be easily integrated into existing beam search procedures and are lightweight to compute. Our work presents a general learning framework with composite communication models that address limitations of base RNN models. These models are trained to integrate RNN generation into discriminative learning, allowing for customization of the decoding objective. Inspired by previous approaches, our lightweight models offer a more comprehensive set of communication models. The learning framework presented here broadens the search space by targeting model architectures, hyperparameters, and learning algorithms. It involves learning the base RNN language model, composite communication models, and a generator to combine sub-components for generating long, coherent text. This framework overcomes limitations of RNNs in text generation. Our framework enhances RNNs for text generation by incorporating sub-models that capture linguistic qualities, guiding the fluency of general RNN language models to generate more lengthy and sensical text. It proposes a framework for learning a decoding objective through component models and a weighing scheme based on Grice's Maxims. This approach complements existing literature on long text generation and is useful for domains with limited in-domain training data. The text discusses a framework that enhances RNNs for text generation by incorporating sub-models to improve the quality of generated text. Human evaluation shows that the text produced by this model surpasses RNN baselines and scores higher on a Turing test evaluation. The context mentions the convenient location of a hotel near transportation hubs and the canal grande in Venice. The language model praises the hotel's cleanliness, comfort, friendly staff, and good breakfast. The hotel in Venice is highly recommended for its friendly and helpful staff, spacious and clean rooms, good breakfast, and peaceful garden. Despite a poor breakfast, the rooms are decent and very clean. The staff at the reception are amazing and provide a pleasant experience. The Abbazia hotel in Venice is praised for its friendly and helpful staff, clean rooms, and good breakfast. Despite the high price, it is considered an excellent choice for a comfortable stay. The hotel uses a 2-layer RNN language model with 1024 GRU cells per layer for text processing. The entailment model uses backpropagation for 35 time steps with gradient clipping at 0.25. Dropout is applied to input word embeddings and MLP hidden layer at a rate of 0.5. Training is done with Adam optimizer at a learning rate of 0.0005 and batch size of 128. The relevance model employs a 1D convolutional layer with filter size 3 and stride 1, padded sequences to maintain input length. Training is done with Adam optimizer at a learning rate of 0.001, dropout before the final linear layer at a rate of 0.5, and SGD with a learning rate of 1."
}