{
    "title": "rJlGdsC9Ym",
    "content": "Curriculum learning involves training on progressively difficult tasks to improve learning efficiency. Teacher-Student algorithms determine the next tasks based on the agent's learning progress. However, challenges arise when the agent is trained on tasks it can't learn or has already mastered. The new algorithm using min max ordered curriculums prioritizes learnable but not yet mastered tasks, outperforming Teacher-Student algorithms on small curriculums and excelling on complex ones. Curriculum learning allows agents to learn tasks through reinforcement, but some very difficult tasks may be impossible without prior knowledge. One solution to enable an agent to learn difficult tasks through reinforcement is curriculum learning. This involves training the agent on easier versions of the task first, gradually increasing the difficulty. Curriculum learning consists of defining a curriculum (set of tasks) and a program (sequence of tasks) to train the agent effectively. Curriculum learning algorithms aim to efficiently define a curriculum and program for training agents by selecting tasks based on learning state. These algorithms can prevent catastrophic forgetting and identify learnable tasks. They are categorized into curriculum algorithms and program algorithms, focusing on defining the curriculum and deciding the next tasks to train the agent on. In this paper, the focus is on program algorithms in the context of reinforcement learning. A new algorithm is introduced in section 5, based on the notion of mastering rate, which selects tasks that are learnable but not yet learnt. This algorithm outperforms Teacher-Student algorithms on small curriculums and significantly on sophisticated ones. The paper introduces a new algorithm based on the mastering rate for program algorithms in reinforcement learning. It outperforms Teacher-Student algorithms on small curriculums and significantly on sophisticated ones with numerous tasks. Curriculum learning notions are defined, with a curriculum being a set of tasks and a distribution over tasks represented by an attention over tasks. Various distribution converters are used, including the argmax distribution converter. The paper presents a new algorithm for program algorithms in reinforcement learning based on the mastering rate. It outperforms Teacher-Student algorithms on small and complex curriculums. Curriculum learning involves a set of tasks and attention over tasks. Various distribution converters are used, including the argmax distribution converter. The paper introduces four attention functions called Online, Naive, Window, and Sampling, based on learning progress estimates. The paper introduces a new algorithm for reinforcement learning based on mastering rate, outperforming Teacher-Student algorithms. It involves attention functions like Online, Naive, Window, and Sampling, estimating learning progress and using distribution converters. The GAmax Window program algorithm is proposed, utilizing Boltzmann or greedy argmax distribution converters. Three curriculums were used to evaluate the algorithms. Three curriculums, BlockedUnlockPickup, KeyCorridor, and ObstructedMaze, were used to evaluate algorithms in partially observable Gym MiniGrid environments. The agent receives an image and textual instruction at each time-step, with rewards based on executing the instruction within a certain number of steps. Suggestions are given on distribution converters and attention functions to use in improving Teacher-Student algorithms. In a paper on improving Teacher-Student algorithms in partially observable environments, two distribution converters (greedy argmax and Boltzmann) and four attention functions (Online, Naive, Window, Sampling) are proposed. It is advised not to use the Boltzmann converter due to difficulty in tuning \u03c4. The Naive and Sampling attention functions are also not recommended as they perform worse than the Window function. The Online and Window attention functions are the preferred choices, with the Window function being the recommended option. The paper recommends using the Window attention function with the greedy argmax distribution converter, known as the GAmax Window algorithm. This algorithm is simplified and improved by using a greedy proportional distribution converter. The GProp Linreg algorithm is introduced as the \"baseline\" for further discussion in the article. The GAmax Linreg algorithm performs similarly to the GAmax Window algorithm, with the former being slightly more stable. The GProp Linreg algorithm outperforms both GAmax algorithms, focusing on learning progress and potential issues of training on already learned tasks or tasks beyond the agent's current capabilities. The new algorithm introduced focuses on the notion of mastering rate to address issues related to training on tasks the agent can't learn yet or has already mastered. The new algorithm introduced focuses on the mastering rate to address training on tasks that are learnable but not yet mastered. It defines learnt and learnable tasks and presents a min-max curriculum for training. The new algorithm introduced focuses on the mastering rate to address training on tasks that are learnable but not yet mastered. It defines learnt and learnable tasks and presents a min-max curriculum for training. The MR algorithm is a more general version of Teacher-Student algorithms and the baseline algorithm, with similarities to the GProp Linreg algorithm under certain conditions. The MR algorithm with \u03b4 = 0.6 outperforms the baseline algorithm on all curriculums, especially on KeyCorridor and ObstructedMaze tasks. Three curriculums were used: BlockedUnlockPickup, KeyCorridor, and ObstructedMaze. In the MR algorithm, different tasks involve unlocking doors or picking up boxes with varying levels of difficulty. The algorithm is tested with different agents on the BlockedUnlockPickup curriculum, showing promising results."
}