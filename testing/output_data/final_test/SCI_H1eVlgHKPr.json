{
    "title": "H1eVlgHKPr",
    "content": "In Reinforcement Learning, environments are often partially observable. One solution is to provide the agent with information about past observations. This paper suggests a new approach based on recording important changes in the environment during an episode, inspired by human memory and discovered through self-supervision. Our history representation method is evaluated using challenging RL benchmarks like Atari-57 games and Obstacle Tower. The advantage of our solution over common RNN-based approaches is demonstrated. Deep RL algorithms use NN to represent the environment observation and learn the agent's optimal policy. Past representation is crucial for partially observable RL environments. In partially observable RL environments, representing the past is crucial for the agent to navigate effectively. Using Recurrent Neural Networks (RNNs) to input the state history helps the agent localize itself in complex domains. However, training RL models with highly nonstationary data can lead to instability. In partially observable RL environments, representing the past is crucial for effective navigation. Training with RNNs can lead to instability due to nonstationary data. A new approach is proposed where the agent's observation history is represented by discrete events, inspired by human decision-making behavior. Environment-specific events are automatically discovered during training and used as landmarks in the history representation. The paper proposes an Event Discovery History Representation (EDHR) approach for effective navigation in partially observable RL environments. Events are discovered by clustering past state observations using Mutual Information (MI) to maximize semantic information shared between frames. This self-supervision method eliminates the need for additional annotation during event discovery. The paper introduces the Event Discovery History Representation (EDHR) approach for navigation in partially observable RL environments. Clusters are formed from past state observations to create a semantic representation, which is then used for policy optimization using the PPO algorithm. The approach allows for adaptation to new observations as the agent progresses through the environment. The paper introduces the Event Discovery History Representation (EDHR) approach for navigation in partially observable RL environments. It utilizes a modified version of the Invariant Information Clustering (IIC) algorithm to identify significant events in the agent's past observations. The proposed method replaces common RNN-based history representations with a time-dependent probability distribution of the observations for the discovered events. Evaluation using the PPO algorithm on Atari-57 benchmark and Obstacle Tower environments shows that EDHR outperforms plain PPO in cases where history increases observability and RNN-based methods in common cases, while requiring 2\u00d7 less training time. The source code for the method and experiments is available at https://github.com/iclr2020anon/EDHR. The paper discusses Partially Observable Markov Decision Processes (POMDP) where an agent interacts with an environment to maximize rewards. The framework includes state space S, action space A, reward function R, transition function T, observations \u2126, observation function O, and discount factor \u03b3. In POMDPs, the agent interacts with the environment in unobserved states. Actions lead to new states and observations, with rewards following a probability distribution. Various approaches, like DQN and LSTM layers, have been proposed to address partial observability in deep RL models. Our method explicitly represents each step of the past using a dictionary of discovered events and concatenates a sequence of steps in an event-based history to augment the current state representation with important past changes of the environment. In this paper, an alternative history representation is proposed to outperform RNN-based solutions in RL frameworks. Unsupervised and self-supervised learning aim to extract useful representations from data without annotation or reward signal. An alternative history representation is proposed to outperform RNN-based solutions in RL frameworks. Unsupervised feature-extraction methods include generative encoding-decoding schemes like Variational Autoencoders (VAEs) combined with RNNs. DeepCluster offers clustering in the feature space as an alternative to reconstruction loss in the input space. DeepCluster utilizes k-means clustering in the feature space and updates network parameters with pseudo-labels obtained through clustering. Self-supervised paradigms focus on maximizing Mutual Information (MI) between latent representations by collecting positive and negative sample pairs. Discriminators are trained to distinguish between these pairs to compute a lower-bound on MI. Various methods like CPC, ST-DIM, and IIC use different techniques for generating positive and negative samples. DIM extends this concept by including locality in MI computation. In DIM, Mutual Information (MI) is computed between global and local representations of images. The evaluation in ST-DIM focuses on disentanglement degree, not RL task benefits. Event discovery stage is based on IIC with temporally close frames from agent trajectory. Our goal is discrete event representation, not clustering like IIC. The text describes the use of IIC for unsupervised event discovery, utilizing a clustering method to obtain discrete events and encode observations. The unsupervised encoder is based on a CNN with a fully-connected and softmax layer. The clustering process in IIC involves pairs of randomly transformed images. The unsupervised encoder architecture utilizes two replicas to map observations into event probability distributions, maximizing mutual information over latent variables to obtain a set of events and supervision signals for parameter updates. The temporal translation window parameter L affects the clustering process by balancing uncertainty and intra-cluster variability. In experiments, the parameter L = 3 is used for clustering in RL environments. Clustering is done by computing a matrix for joint probability and maximizing mutual information. An Actor-Critic architecture with a neural network is used for policy optimization using the PPO algorithm. The agent receives input of concatenated raw-pixel frames and a history at each time step. The proposed history representation in RL environments involves concatenating raw-pixel frames and a history matrix H(t) that captures past events. By updating \u03c6 through cluster soft-assignment vectors, a low-dimensional, sparse matrix is created, containing significant information about the agent's recent trajectory. This compact representation contrasts with RNN-based approaches, offering a more efficient way to encode historical data. The proposed history representation in RL environments involves storing past events in a history matrix H(t) along with timedependent evolution. Visual information is extracted by a CNN (\u03a8) to create a vector v t, which is concatenated with H(t) and input to the agent. The unsupervised encoder (\u03a6) and \u03a8 have different parameters, with \u03a8 trained using RL rewards. The proposed method involves using a fashion, \u03a8, to extract visual information from data, trained using RL rewards. The process includes running the agent in parallel environments, updating parameters for policy optimization and event discovery stages, and iterating through these stages to adapt to observation distribution changes. The full algorithm is detailed in Alg. 1. The weights \u03c6 and \u03b8 are initialized with a (semi-) orthogonal matrix, except for the head of \u03a6, which is initialized randomly. The head of \u03a6 is duplicated 5 times with different initial random parameters for improved robustness. Each head produces a different representation, and the \"best\" head is selected using the MI value as the criterion. Training is split into two phases, with the agent receiving only I(t) in the first phase. In the first phase, the agent receives only I(t) as input, and \u03b8 and \u03c6 are updated. The head with the highest MI value is selected and kept fixed for the second phase, where the agent receives the full input. The network architecture and hyperparameters are consistent with previous work. In the evaluation, EDHR is tested on deep RL benchmarks ALE and Obstacle Tower, comparing results with PPO and PPO-RNN. Changes are made to the encoder \u03a6, using network \u03a5 without softmax layer and with 256 neurons in the fully-connected layer. Past observations are input to a GRU with a hidden layer of size 512. No self-supervision is used to train the network components. In ALE, classic Atari 2600 games are used for evaluation without self-supervision. The action space is discrete with varying numbers of actions. The Obstacle Tower is a 3D game with procedurally generated floors and rooms containing puzzles of increasing complexity. The Obstacle Tower is a challenging environment for training agents, with high visual fidelity, diverse observations, and sparse rewards. The agent receives rewards for passing floors and solving puzzles, with a limited action space of 12 movements. Training is done with seeds 0-7 and evaluation with seeds 50-57 to test generalization abilities. The experiments in the Obstacle Tower environment were conducted with seeds 50-57 to test the algorithm's generalization abilities. Results were averaged over 100 episodes and each experiment was repeated 3 times with different random seeds. The final scores were reported with average values and standard deviations in Tab. 1-2, comparing the results with the original PPO algorithm and a reproduction. The benefits of the history representation were highlighted, showing significant score boosts in most games, although Breakout did not show any improvement. PPO-RNN underperformed compared to the EDHR on the ALE benchmark. PPO-RNN underperforms compared to our EDHR on ALE benchmark due to challenges in training an RNN simultaneously with the agent in an RL framework, especially with limited training budget. The training budget consists of 10M time steps, equivalent to the number of observations an agent receives from the environment. PPO-RNN is about two times slower than our EDHR in terms of wall-clock time. Additional experiments in Appendix B include different hyperparameters of EDHR, an ablation model without self-supervision, and another common RNN-based model. The importance of past information in the Obstacle Tower environment is highlighted by the high score of PPO-RNN. EDHR successfully exploits discovered events, outperforming PPO in some cases. However, in this game, EDHR underperforms PPO-RNN, indicating a potential loss of useful information in the compact history representation. The efficiency of EDHR with respect to the training budget is demonstrated in Fig. 3, showing that EDHR effectively utilizes knowledge acquired in unsupervised training stages. Using an event-based history representation makes training easier most of the time. The method uses events recorded in H(t) as a summary of important information in a past trajectory. Visual examination in MsPacman and Obstacle Tower environments shows events correspond to specific changes like collecting a \"Power Pellet\" in MsPacman. Predominant events tend to be constant for a while, reflecting low frequency changes in important information. In the Obstacle Tower environment, events recorded in H(t) serve as a summary of important information from past trajectories. The color of ghosts changes to blue, allowing them to be eaten by the agent, leading to a predominant event change. Observations in sequential order show transitions to new rooms or different viewpoints. The agent's history representation in Obstacle Tower is sparse, with high-frequency selected observations revealing the semantics behind clusters discovered by the encoder. The history representation in Obstacle Tower is sparse and binary, concentrated around values of 0 and 1. A method called EDHR compresses important information using events discovered through agent training. Visual information is represented using this method as an alternative to RNN-based solutions. The method EDHR uses two networks, \u03a6 and \u03a8, to represent visual information. \u03a8 is trained with a reward signal for task-specific details, while \u03a6 is trained with self-supervision to focus on repeated patterns in the data stream. This approach effectively captures past information compared to traditional task-oriented representations like PPO and PPO-RNN, as shown in results from ALE and Obstacle Tower benchmarks. In Tab. 3, all hyperparameters used in the EDHR method are listed, different from (Schulman et al., 2017). Tab. 4 shows the impact of various EDHR hyperparameter choices on ALE benchmark environments. A shorter temporal window (L = 1) performs similarly to L = 3, while a larger value (L = 8) degrades performance. A short history (S = 16) degrades performance in MsPacman and SpaceInvaders, but improves in Breakout. A longer history (S = 64) degrades the score. The impact of self-supervision on EDHR is shown in Tab. 4. Without self-supervision, there is a dramatic degradation in MsPacman and SpaceInvaders, highlighting the importance of self-supervision. In Breakout, the model ignores history, achieving scores similar to plain PPO. The last row in Tab. 4 discusses a variant of the PPO-RNN algorithm. The last row in Tab. 4 presents a variant of the PPO-RNN algorithm where input I(t) and CNN \u03a8 are removed. The network receives the last S observations as images, processes them with CNN, inputs features to RNN, and is trained with PPO. This model shows higher variance in CNN parameter updates, requiring more training. A self-supervised clustering method is briefly described, with more details in the original paper (Ji et al., 2019). The text discusses the use of a network \u03a6 to assign images to clusters based on their semantic content. The goal is to maximize Mutual Information between representations of similar images. This is achieved by comparing the probability distributions of images x and x in different clusters. The text discusses the conditional joint distribution P(z=c, z=c|x, x) = \u03a6c(x) \u00b7 \u03a6c(x), where z and z are independent. The joint probability distribution is represented by a C \u00d7 C matrix P, with marginals Pc and Pc corresponding to sums over rows and columns. The optimization objective in Eq. 2 is computed using Mutual Information. Event visualization in Obstacle Tower shows various events, some of which are semantically general classes not solely based on visual appearance. Events in the Obstacle Tower are visually distinct, with specific rooms corresponding to events 2 and 12, door detection in event 8, and room views in events 4 and 14."
}