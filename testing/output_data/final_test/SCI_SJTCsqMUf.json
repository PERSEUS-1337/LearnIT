{
    "title": "SJTCsqMUf",
    "content": "A new type of deep contextualized word representation is introduced, capturing complex word characteristics and variations in linguistic contexts. These word vectors are derived from a deep bidirectional language model pretrained on a large text corpus. The representations significantly enhance performance on various NLP tasks and allow downstream models to benefit from different types of semi-supervision signals. Pretrained word vectors like BID59, BID39, and BID44 are essential for NLP tasks such as question answering, textual entailment, and semantic role labeling. These word vectors provide context-independent representations for words. In contrast, ELMo representations capture individual word meanings along with the larger context in which they appear. This approach overcomes the limitations of traditional word vectors by incorporating subword information or learning separate representations. Our approach benefits from subword units and seamlessly incorporates multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work focuses on learning contextual embeddings using bidirectional LSTM to encode context around a pivot word. Different approaches include incorporating the pivot word itself in the representation using either a supervised neural machine translation system or an unsupervised language model. In this paper, the authors leverage monolingual data to train a biLM on a corpus with 30 million sentences. They extend these techniques to deep contextual representations, demonstrating their effectiveness across various NLP tasks. Previous studies have shown that different layers of deep biRNNs encode different types of information, with lower layers benefiting from multi-task syntactic supervision. Additionally, research has indicated that the top layer of an LSTM for encoding word context can learn representations of words. The authors demonstrate that ELMo representations induce signals similar to those learned by the top layer of an LSTM for word context encoding. They propose a method where a pretrained biLM is fixed and additional task-specific model capacity is added for downstream tasks, leveraging universal biLM representations for smaller supervised models. This section explains how ELMo representations are computed and utilized to enhance NLP models. Recent state-of-the-art neural language models utilize a biLM approach to compute ELMo representations, which are then added to existing NLP architectures. The biLM is pretrained to generate context-dependent token representations, enhancing the prediction of the next token in a sequence. The backward LM, similar to a forward LM, predicts the previous token given future context. It can be implemented in a way analogous to a forward LM, with each backward LSTM layer producing representations in a deep model. A biLM combines both forward and backward LMs, jointly maximizing log likelihood in both directions. Parameters for token representation and Softmax layer are tied in both directions, with separate parameters for LSTMs. This formulation shares some weights between directions, departing from previous work by introducing a new approach for learning word representations as a linear combination of the biLM. ELMo is a task-specific combination of intermediate layer representations in a biLM. It computes multiple representations for each token, collapsing them into a single vector for downstream models. The best performance is achieved by weighting all biLM layers with learned scalar weights. The scalar parameter \u03b3 allows scaling of the ELMo vector, aiding in the optimization process. The activations of each biLM layer have different distributions, and applying layer normalization BID0 before weighting can be helpful. Using a pre-trained biLM to enhance a supervised NLP model is a straightforward process. Most NLP models in this paper use RNNs, but the method is also applicable to CNNs. Adding ELMo to the lowest layers of the supervised model in a consistent manner is beneficial. It is standard to create a context-independent token representation for each token position using pretrained word embeddings and character-based representations. Adding a moderate amount of dropout to ELMo BID57 is found to be advantageous. Incorporating dropout and regularization in ELMo BID57 enhances the model's performance. The pre-trained biLMs used in this study are similar to previous architectures but modified for joint training and including a residual connection between LSTM layers. The focus is on large-scale biLM training, emphasizing their superiority over forward-only LMs. Model dimensions were halved from the best model in BID22 to balance perplexity, model size, and computational requirements. The best model CNN-BIG-LSTM in BID22 uses 2048 character n-gram convolutional filters, two highway layers BID58, and two LSTM layers with 4096 units. The average perplexities on the 1B Word Benchmark BID4 are 39.7 for forward and 30.0 for CNN-BIG-LSTM. Fine-tuning on task-specific data resulted in improved performance. ELMo's performance across six NLP tasks is shown in TAB1, with significant benefits observed in downstream tasks. ELMo significantly improves performance across a diverse set of NLP tasks, establishing new state-of-the-art results with relative error reductions ranging from 6-20% over strong base models. Textual entailment, such as the Stanford Natural Language Inference (SNLI) corpus, benefits from ELMo when added to models like ESIM sequence model. Adding ELMo to the ESIM model improves accuracy by 0.7% on average across five random seeds, surpassing the CoVe enhanced model. A five-member ensemble achieves an accuracy of 89.3% on the Stanford Question Answering Dataset (SQuAD), with a 4.2% improvement in F1 score after adding ELMo to the baseline model. Adding ELMo to the baseline model improved test set F1 by 4.2% to 85.3%, surpassing the single model state-of-the-art by 1.0%. Semantic role labeling (SRL) system models the predicate-argument structure of a sentence, answering \"Who did what to whom\". SRL was modeled as a BIO tagging problem using an 8-layer deep biLSTM. When ELMo was added to the model, test set F1 increased by 3.2% to 84.6%, setting a new state-of-the-art on the OntoNotes benchmark. Coreference resolution is the task of clustering mentions in text referring to the same real-world entities. The baseline model for clustering mentions in text uses a biLSTM and attention mechanism to compute span representations and find coreference chains. Adding ELMo improved the average F1 by 3.2% to establish a new state of the art. Named entity extraction involves tagging entity types in newswire text using a biLSTM-CRF based sequence tagger. Our ELMo enhanced biLSTM-CRF model achieves 92.22% F1 score, outperforming the previous state of the art. Unlike previous models, we allow the task model to learn from all biLM layers, leading to improved performance across multiple tasks such as fine-grained sentiment classification in the Stanford Sentiment Treebank. The study compares the use of CoVe and ELMo embeddings in a biattentive classification network (BCN) for movie review sentiment analysis. Replacing CoVe with ELMo results in a 1.0% accuracy improvement. The analysis validates the benefits of using deep contextual representations from ELMo in downstream tasks. The study compares CoVe and ELMo embeddings in a biattentive classification network for sentiment analysis, showing ELMo provides better performance. It explores contextual information in biLMs, sensitivity to ELMo placement, training set size, and visualization of ELMo weights. Different ways to combine biLM layers are discussed, emphasizing the importance of the regularization parameter \u03bb. The study discusses the importance of the regularization parameter \u03bb in combining biLM layers for improved performance in tasks like sentiment analysis. Different \u03bb values affect the weighting function, with smaller values allowing layer weights to vary and improving overall performance. Comparisons on SNLI, SRL, and SQuAD show that including representations from all layers enhances performance, especially when using contextual representations from the last layer. For example, in SQuAD, using all biLM layers instead of just the last one improves F1 score by 1.1%, and learning individual layer weights further boosts performance by 1.2%. Including ELMo at both input and output layers for tasks like SNLI and SQuAD improves results, while for tasks like SRL, performance is highest when ELMo is included at the output layer. The inclusion of ELMo at different layers affects performance in various NLP tasks. For tasks like SNLI and SQuAD, ELMo improves results when included at both input and output layers. However, for tasks like SRL, performance is highest when ELMo is included only at the input layer. This difference in performance may be due to the importance of task-specific context representations in SRL, where ELMo enhances performance over word vectors alone. The biLM's contextual representations encode information useful for NLP tasks that is not captured in word vectors, suggesting that the biLM helps disambiguate word meanings through context. The biLM's contextual representations help disambiguate word meanings through context, enabling predictions for word sense disambiguation and POS tagging tasks. This approach allows comparison to CoVe and individual layers for improved performance in NLP tasks. The biLM's contextual representations aid in disambiguating word meanings for tasks like word sense disambiguation (WSD). By using biLM, representations are computed for words in the training corpus, and the nearest neighbor sense is selected at test time. The biLM top layer representations achieve an F1 score of 69.0 for WSD, outperforming the first layer. This performance is comparable to state-of-the-art WSD models using handcrafted features and task-specific biLSTM models. The CoVe biLSTM layers outperform the biLM in semantic labeling and POS tagging tasks. The biLM representations are competitive with task-specific biLSTMs for POS tagging, with higher accuracies at the first layer compared to the top layer. The experiments confirm that different layers in the biLM represent various types of information, emphasizing the importance of including all layers for optimal performance in downstream tasks. ELMo's representations are more transferable to WSD and POS tagging compared to CoVe, leading to better performance in downstream tasks. Adding ELMo to a model significantly improves sample efficiency, reducing the number of parameter updates needed to reach state-of-the-art performance. For example, the SRL model achieves maximum development F1 after only 10 epochs with ELMo, compared to 486 epochs without it. After adding ELMo, the model surpasses the baseline maximum at epoch 10, reducing the number of updates needed by 98% to achieve the same performance level. ELMo-enhanced models are more efficient with smaller training sets compared to models without ELMo. Performance improvements with ELMo are most significant for smaller training sets, reducing the amount of data needed to reach a certain performance level. In the SRL case, the ELMo model with 1% of the training set performs similarly to the baseline model with 10% of the training set. The softmax-normalized learned layer weights show a preference for the first biLSTM layer at the input layer across tasks. The distribution of emphasis in the biLSTM layers varies across tasks, with a preference for the first layer in coreference and SQuAD. The output layer weights show a slight preference for lower layers. ELMo significantly improves performance on various NLP tasks by efficiently encoding syntactic and semantic information. Future work will explore why the first biLSTM layer is favored and investigate the broader implications of the approach. The curr_chunk discusses future work on two themes: the best training regime for learning NLP representations and the best way to use deep contextual representations for other tasks. It mentions the benefits of using a biLM training objective and the potential for further improvements in utilizing deep contextual representations. The Appendix contains details on model architectures, training routines, and hyper-parameter choices. The curr_chunk discusses the fine-tuning process of biLM on task-specific data, resulting in significant drops in perplexity for various tasks. Fine-tuning the biLM for one epoch on the training split and evaluating on the development split led to improved perplexities in most cases, such as SNLI. The impact of fine-tuning on supervised performance varies depending on the task. Fine tuning the biLM had a positive impact on supervised performance, with a 0.6% increase in accuracy for SNLI. The \u03b3 parameter was crucial for optimization, especially in the last-only case. Without it, performance suffered significantly. The baseline model used 300 dimensions for LSTM and feed forward layers, along with pretrained GloVe embeddings. Regularization was done by adding 50%. The study utilized 300 dimensional GloVe embeddings and added regularization with variational dropout. Parameters were optimized using Adam with gradient norms clipped at 5.0. ELMo vectors were incorporated with layer normalization. Test set accuracy was compared to previously published systems. The study improved accuracy by 0.7% by adding ELMo to the ESIM model, reaching a new state-of-the-art accuracy of 88.7%. A five-member ensemble further increased accuracy to 89.3%. The QA model is a simplified version of a model from BID8, utilizing GloVe word vectors and character-derived embeddings through a convolutional neural network. Token embeddings are processed through a shared bi-directional GRU and attention mechanism from BiDAF. The results are then passed through various layers including linear and self-attention layers. The QA model incorporates ELMo without layer normalization, uses GRUs for prediction, and achieves 80.7 F1 on the dev set. Performance improvements are anticipated by incorporating these changes into the design. The baseline SRL model utilizes 100-dimensional vector representations for words, a 200-dimensional token representation passed through an 8-layer biLSTM, and semantic roles from PropBank with a BIO labeling scheme. Training involves minimizing negative log likelihood using Adadelta. During training, negative log likelihood of tag sequence is minimized using Adadelta with a learning rate of 1.0 and \u03c1 = 0.95. Viterbi decoding enforces valid spans at test time using BIO constraints. Variational dropout of 10% is applied to LSTM hidden layers. Gradients are clipped if exceeding 1.0. Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs. Pretrained GloVe vectors are fine-tuned, and LSTM cells are initialized to be orthogonal. Forget gate bias is set to 1 for all LSTMs. Our ELMo augmented model achieves a new state-of-the-art F1 score of 84.6 on the CONLL 2012 Semantic Role Labeling task, surpassing previous results. The baseline coreference model utilizes ELMo for input and biLSTM layers with dropout. Character-based representation with CNN is also employed. The best configuration includes ELMo at the lowest layer biLSTM and dropout of 50% for ELMo representations. Token representation goes through two biLSTM layers. The model utilizes two biLSTM layers with varying hidden units, CRF loss during training, Viterbi algorithm for decoding, variational dropout, gradient rescaling, Adam optimizer, fine-tuned Senna embeddings, early stopping, and ELMo input for the lowest layer task biLSTM. Training constraints are applied to improve performance on the CoNLL 2003 NER dataset. The ELMo enhanced biLSTM-CRF tagger achieved a new state-of-the-art F1 score of 92.22% on the test set, outperforming previous results. Using representations from all layers of the biLM showed a slight improvement. The model architecture is similar to BID35, with a simpler feedforward network replacing the final maxout network. Validation accuracies were lower with a batch-normalized maxout network. ELMo enhanced biLSTM-CRF tagger achieved a new state-of-the-art F1 score of 92.22% on the test set. The model uses 300-d hidden states for the biLSTM and optimizes parameters with Adam Kingma & Ba (2015) using a learning rate of 0.0001. ELMo vectors are added to both the input and output of the biLSTM."
}