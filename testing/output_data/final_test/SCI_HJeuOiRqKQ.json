{
    "title": "HJeuOiRqKQ",
    "content": "Many assumptions about how neural networks operate, including the need for stability in convolutional neural networks for image recognition tasks, have not been thoroughly tested. The use of interleaved pooling layers for stability in CNN architectures has decreased in recent years, leading to questions about the importance of deformation stability and the necessity of pooling for deformation invariance. Research shows that deformation stability in convolutional networks is more complex than previously thought, with different tasks requiring varying degrees of stability at different layers. Deformation stability in convolutional networks is adjusted during training through the smoothness of convolutional filters. Interleaved pooling layers are not essential for optimal deformation stability in natural image classification. Pooling provides excessive deformation stability initially, requiring networks to counteract this bias during training. These findings challenge common assumptions in deep learning and emphasize the importance of empirical testing in understanding neural network operations. In this paper, the authors aim to empirically study the foundational intuition behind convolutional neural networks (CNNs) for visual object recognition, focusing on the need for stability to small translations and deformations in input images. This principle influenced the architectural choices of CNNs, emphasizing the importance of representations that are invariant to such transformations for better performance. The core principles behind convolutional filters and interleaved pooling in CNNs, along with parametric data augmentation strategies, have influenced the architecture for visual object recognition. The relationship between object recognition and deformation stability remains untested, raising questions about the necessity of deformation stability in CNNs. Recent models have achieved success without interleaved pooling layers, challenging previous assumptions. In the context of visual object recognition in CNNs, the necessity of deformation stability and the role of interleaved pooling are explored. A broad class of image deformations is studied to compare CNNs' responses to original and deformed images. Networks without pooling layers show sensitivity to deformation at initialization but learn to adapt their representations over the course of training. Pooling in neural networks initially sensitive to deformation but stabilize over time. Networks with and without pooling converge in deformation stability patterns. Deformation stability is influenced by the smoothness of learned filters. Invariances in non-neural models like SIFT features are also explored. SIFT features and deformable parts models provide robustness to translation, scaling, rotation, and deformations. Deformation invariance inspired the use of pooling layers in CNNs for translation and deformation invariance. The use of pooling layers in CNNs provides invariance to distortions and translations. Scattering networks are stable to deformations but do not apply to widely used CNN architectures due to architectural differences. The recent theoretical study of BID1 examines the inductive biases of architectures similar to CNNs, but with fixed wavelet filters and interleaved pooling layers. It cannot explain the success of newer architectures lacking these features. The curr_chunk discusses the use of bilinear interpolation in computing values in a neighbourhood around the tail of an arrow in an original image. It also mentions examples of deformed ImageNet images and empirical investigations on the sensitivity of CNNs to geometric transformations. The curr_chunk explores the sensitivity of CNNs to geometric transformations and the need for a controllable source of deformation to study CNN representations. It focuses on a flexible class of local deformations of image coordinates for analysis. The curr_chunk discusses the use of deformations to investigate CNN stability and task demands. It approximates common data augmentation methods and aims to cover a wide range of natural deformations. Future work may explore richer transformations requiring a renderer or adversarial selection. The curr_chunk discusses measuring the stability of CNNs to deformations by sampling random deformations and applying them to input images. Deformed images are generated using a method involving control points and interpolation. The process for generating deformed images is illustrated. Sensitivity to deformation is measured using the Normalized Cosine Distance. Results are averaged over 128 images and networks are based on a modified VGG network. In CIFAR-10 experiments, different downsampling layers were compared, including Subsample, Max-pool, Average-pool, Strided, and Strided-ReLU. For ImageNet experiments, only Max-pool and Strided-ReLU were compared due to computational constraints. Experiments were repeated with 5 random seeds to account for variability. Error bands in plots represent 2 standard deviations. Pooling influences deformation stability in neural networks. The error bands in the plots correspond to 2 standard deviations estimated across 5 experiments. Pooling leads to more stable representations to deformation at initialization compared to networks without pooling. Pooling confers stability to deformation at initialization but the stability changes significantly over the course of training and converges to a similar stability regardless of whether pooling is used. Networks with max-pooling are less sensitive to deformation at initialization, but after training, networks with and without max-pooling have similar patterns of sensitivity to deformation. The choice of downsampling layers has little effect on deformation stability throughout the layers after training. The layers in the network include downsampling layers, with the final layer corresponding to the final downsampling layer. For CIFAR-10, there are 1 input layer, 8 convolutional layers, and 4 pooling layers, totaling 13 layers. The sensitivity to deformation of networks with pooling increases significantly during training, suggesting that the bias for deformation stability from pooling may be too strong. Networks with and without pooling eventually converge to similar patterns of deformation stability across layers. The layerwise pattern of sensitivity to deformation for networks with and without pooling was highly similar, indicating that pooling has little influence on deformation stability. Experiments on CIFAR-10 with different downsampling layers yielded similar results. The downsampling layer had a significant effect on deformation sensitivity at initialization, but this effect disappeared by the end of training. Networks without pooling achieved similar accuracy to those with pooling on image classification tasks. Filter smoothness may contribute to deformation stability. In this section, the importance of filter smoothness in determining deformation stability is empirically demonstrated. By measuring filter smoothness and showing that forcing it at initialization leads to stability, it is observed that CNNs learn progressively smoother filters in tasks requiring deformation stability. Additionally, filter smoothness increases during training, even for networks with pooling layers, as shown on ImageNet and CIFAR-10 datasets. Initialization with smooth filters leads to deformation stability in convolutional neural networks. Different amounts of filter smoothness were tested to determine if it resulted in greater deformation stability. The normalized total variation of convolutional weights was used to measure filter smoothness, with the expectation that smoother filters would lead to more stable deformations. This was consistent across network architectures, with a fixed mean and small standard deviation. Initialization with smooth filters leads to greater deformation stability in convolutional neural networks. Randomly-initialized smooth filters are found to be more stable to deformation, suggesting that filter smoothness is sufficient for stability. The distribution of learned filters may differ from random filters, but smoother filters are learned in tasks requiring stability to stronger deformation. The study focused on the impact of deformation strength on filter smoothness in convolutional neural networks. Stronger deformations led to smoother filters, showing a direct correlation between deformation and filter smoothness. Training on real datasets also increased filter smoothness. Additionally, filters became smoother over the course of training on both ImageNet and CIFAR-10 datasets across various network architectures. The study showed that filter smoothness increases with deformation strength in convolutional neural networks trained on ImageNet and CIFAR-10 datasets. The amount of filter smoothness correlates with deformation stability, which is learned during training. The role of input image distribution (P(X)) and label conditional distribution (P(Y|X)) in promoting filter smoothness and deformation stability remains unclear. To analyze the impact of input image distribution (P(X)) and label conditional distribution (P(Y|X) on filter smoothness and deformation stability, networks were trained on modified CIFAR-10 datasets with random labels. Networks with different architectures converged to varying patterns of deformation stability, consistent across random seeds. This suggests that both P(X) and P(Y|X) play a role in the smoothness of learned filters. The architecture and task influence deformation stability patterns, with smoother random filters leading to stability. Deformation stability is architecture-dependent when training with random labels. In this work, the authors rigorously tested properties associated with deformation stability in CNNs. They found that pooling confers stability at initialization but does not determine the pattern of stability across layers. The inductive bias from pooling is too strong for ImageNet and CIFAR-10 classification and needs to be counteracted during training. Filter smoothness significantly contributes to achieving deformation stability. The patterns of stability are influenced by the task being learned, with the joint distribution of inputs and outputs playing a crucial role. The joint distribution of inputs and outputs is crucial for determining deformation stability in deep neural networks. The study highlights the importance of learned weights in understanding network behavior. Future research could explore worst-case deformations and how characteristics change during training. The study explores the evolution of filter smoothness and deformation stability in deep neural networks during training. Different network architectures and downsampling layers were compared in ImageNet and CIFAR10 experiments. Future research is needed to further investigate the trajectory of filter smoothness and deformation stability. In ImageNet experiments, the study compared Max-pool and Strided-ReLU layers due to computational constraints. Experiments were repeated 5 times for each setting to account for random factors. Deformations were used to approximate changes in pose, translation, and other geometric transformations. Deformations were used to approximate pose, translation, and rotation in ImageNet experiments. Local translations were visualized in FIG5, and rotations in FIG8."
}