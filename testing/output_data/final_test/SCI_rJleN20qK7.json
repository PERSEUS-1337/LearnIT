{
    "title": "rJleN20qK7",
    "content": "In this work, a two-timescale network (TTN) architecture is introduced to enable linear methods to learn values with a nonlinear representation learned at a slower timescale. This approach allows for the use of algorithms developed for linear settings, such as data-efficient least-squares methods and eligibility traces, to provide nonlinear value learning. The text discusses the introduction of a two-timescale network (TTN) architecture to enable linear methods to learn values with a nonlinear representation. It emphasizes the importance of quality state representation in value function approximation and compares the benefits of TTNs to other nonlinear value function approximation algorithms. Learning representations through basis adaptation or neural networks can alleviate the burden of expert specification and enable scaling to high-dimensional observations like images. Linear function approximation for estimating value functions has benefits over nonlinear estimators, including data efficiency and robustness to meta-parameters. Linear algorithms, such as those utilizing eligibility traces, have shown to speed up learning but struggle with nonlinear value function approximation. Various algorithms have been developed for linear settings, both for on-policy and off-policy learning, with theoretical exploration and empirical insights into improvements from gradient methods, true-online traces, and emphatic weightings. These algorithms are relatively simple to implement. In this work, a simple strategy is pursued to leverage the benefits of linear methods while learning the representation. Two learning processes run in parallel: one learns nonlinear features using a surrogate loss, and the other estimates the value function linearly based on those features. These Two-timescale Networks (TTNs) converge as the features change slowly, allowing them to be effectively fixed for the fast linear value function estimator. The Two-timescale Networks (TTNs) utilize a separation of loss for representation and value function, enabling simpler objectives for representation learning while still using linear algorithms based on mean squared projected Bellman error (MSPBE). This approach avoids the complexity of nonlinear MSPBE but retains its useful properties. Previous basis adaptation methods lacked this key aspect, making TTNs a novel strategy for efficient learning. TTNs utilize a separation of loss for representation and value function, simplifying objectives for representation learning while using linear algorithms. This approach avoids the complexity of nonlinear methods, making TTNs a novel strategy for efficient learning. TTNs are effective for control with neural networks in a finite Markov Decision Process. They simplify representation learning objectives and can exploit benefits of linear value function approximations. The dynamics of the MDP are defined by the 3-tuple (S, A, P), where S is the set of states, A the set of actions, and P the transition probability function. The goal in policy evaluation is to compute the value function V\u03c0: the expected sum of discounted rewards from every state under a fixed policy \u03c0. This can be achieved by finding parameters to approximate the value function using linear or nonlinear function approximation. The learning problem involves estimating a nonlinear function V(s) to approximate V \u03c0. A Bellman operator B \u03c0 is defined to find a fixed point that satisfies B \u03c0 V \u03c0 = V \u03c0. Linear value functions may not satisfy the recursion, so a projected fixed point \u03a0 F B \u03c0V = V is found. Linear policy evaluation algorithms estimate this projected fixed point. The objective for the projected fixed-point in nonlinear function approximation is more complex. For linear function approximation, the projection simplifies into a closed form solution. However, for nonlinear function classes, the projection does not have a closed form solution and may be expensive to compute. The nonlinear mean-squared projected Bellman error (MSPBE) and resulting algorithm are more complex and have not seen widespread use. Two-timescale Networks (TTNs) are introduced as a strategy for nonlinear value function approximation. TTNs use two optimization processes for network parameters \u03b8 and value function parameters w. The value function is approximated as V(s) = x\u03b8(s)w, where x\u03b8: S \u2192 Rd are features adjusted to provide better representation. The two optimization processes in Two-timescale Networks (TTNs) involve adjusting parameters \u03b8 for representation and parameters w for value estimation. The slow process for \u03b8 is driven by a separate objective from the fast process for w, which could be problematic. To address this, a surrogate loss related to value estimation is selected for the slow process. The output V(s) corresponds to the value estimate used by the agent, while \u0176(s) represents the output for the slow part. The slow part of the network minimizes the mean-squared TD error (MSTDE) using a surrogate loss, with its own weights associated with estimating the value function. Stochastic gradient descent on the MSTDE is straightforward, providing worse value estimates than the mean-squared Bellman error (MSBE). The MSTDE provides worse value estimates than the MSPBE in experiments, but can still be useful as a surrogate loss to inform representation for estimating values. Other surrogate losses related to the value function, like MSRE and MSBE, have drawbacks such as requiring large amounts of data or delays in updating the representation. The gradient of the nonlinear MSBE is less complex than the MSPBE, but suffers from the double sampling problem. Surrogate losses for estimating values can be defined not directly related to the value function, such as predicting the next state and reward. Predicting the next state and reward is important for value prediction, with theoretical support showing that features capturing immediate rewards and expected next states lead to small Bellman error. Learning representations through auxiliary tasks is also beneficial. Learning representations through auxiliary tasks in reinforcement learning has shown success, such as using self-supervised tasks like predicting next state and reward. These tasks, like using rotated images, can produce useful representations without training with the main loss. The focus is on simpler surrogate objectives to demonstrate the effectiveness of separating representation learning in Temporal Transition Networks (TTNs). Training TTNs is done online using stochastic gradient descent to reduce the surrogate loss. The descent reduces surrogate loss in Temporal Transition Networks (TTNs) coupled with linear policy evaluation algorithms. The network evolves slowly relative to linear prediction weights, ensuring convergence. The convergence of neural networks with linearly dependent features is ensured by projecting iterates to compact, convex sets. The analysis is general for any twice continuously differentiable network architecture, proving asymptotic convergence to stable equilibria. Main results are provided for TD(\u03bb) cases. The stochastic sequence generated by the TTN converges almost surely to a set of stable equilibria. Additionally, the TD(\u03bb) algorithm within the TTN setting converges almost surely to a limit w*. TTN setting converges almost surely to the limit w*, satisfying DISPLAYFORM1 with \u03b8* \u2208 K. Performance of TTNs compared to other nonlinear policy evaluation algorithms is investigated, along with the impact of choices within TTNs. Questions addressed include the benefits of optimizing MSPBE for value estimates, the gains of TTNs over other algorithms, and the potential for TTNs to rival deep Q-learning in control. Experiments conducted on-policy in five environments, including classic continuous-state domains like Puddle World. In various continuous-state domains and game domains, different tasks are performed such as balancing a pole in Cartpole and catching falling apples in Catcher. The study evaluates policies for these domains and presents results in the main body and appendix. Results for Cartpole are similar to Acrobot, and the value of the algorithms is analyzed with low-dimensional and high-dimensional observations. Results in Cartpole are similar to Acrobot; Cartpole results are only in the appendix. Value estimates are evaluated using root-mean-squared value error (RMSVE) across 500 states. The algorithms use specific settings for minimizing mean-squared TD error (MSTDE) and network weights initialization. Hyperparameters are chosen after a preliminary sweep on a broad range. In Appendix D, results are reported for hyperparameters in a refined range chosen based on RMSVE. TTN is compared to various algorithms, showing competitive performance in Puddle World with significantly lower error than the second best algorithm. Nonlinear GTD performs well across domains, indicating an advantage for theoretically-sound algorithms. The TTN benefits from a second head learning at a faster timescale, with optimizing the MSPBE yielding better results than optimizing the MSTDE. Using the MSTDE, despite being a poor objective, can still improve the value function. Using the MSTDE can be effective for driving feature-learning and outperforms fixed representations. Various linear policy evaluation algorithms like TD, LSTD, FLSTD, emphatic TD, gradient TD, and their true-online versions are compared for learning the value function. GTD and ETD are newer temporal difference methods with better convergence properties. True-online variants improve algorithm behavior when learning online and outperform counterparts empirically. Linear algorithms like TD, LSTD, FLSTD, emphatic TD, and gradient TD, along with their true-online versions, are compared for learning the value function. These methods can use eligibility traces to increase sample efficiency by propagating TD errors back in time. Incremental versions of least-squares algorithms are used for TTNs to maintain estimates online. FLSTD is included for advantage when feature representation changes over time. The trace parameter \u03bb provides a bias-variance tradeoff for value estimates in nonlinear function approximation. Nonlinear TD(\u03bb) extends eligibility traces to this case by keeping one trace per weight. TTNs benefit from using different linear policy evaluation algorithms and traces, with LSTD showing dominance in various domains. Most TTN variants benefit from a nonzero \u03bb value, with high settings near 1 being optimal in many cases. Least-squares methods are an exception in sensitivity to \u03bb. In nonlinear function approximation, the trace parameter \u03bb provides a bias-variance tradeoff. Nonlinear TD(\u03bb) extends eligibility traces to this case, but performs worse as \u03bb increases. Surrogate loss functions were optimized for the slow part of the network, with different objectives showing varying effectiveness. In Acrobot, alternate losses like semi-gradient MSTDE were more successful, indicating no universal best approach. The results suggest that there is no universally superior surrogate loss for policy evaluation. In the control setting, the DQN algorithm utilizes experience replay and a target network to stabilize training. An alternative strategy to target networks is used for TTN, motivated by fitted Q-iteration. TTNs provide a mechanism to directly use FQI for updating Q-values, incorporating a regularization term to prevent significant weight changes. Each FQI iteration involves solving a least squares problem on the entire buffer, with network updates every k steps to reduce computation cost. The slow part drives feature-learning by minimizing the semi-gradient MSTDE. TTNs utilize FQI for updating Q-values with a regularization term to prevent significant weight changes. The slow part drives feature-learning by minimizing the semi-gradient MSTDE. Experimental details differ for control, with TTN performing well on both non-image and image Catcher tasks, learning more quickly than DQN variants. TTNs, or Two-timescale Networks, show promise in improving sample efficiency in control tasks compared to DQN variants. TTNs stabilize on a better policy despite suffering from catastrophic forgetting later in training. The algorithm combines slow feature adaptation and fast linear value function learning for improved performance. By leveraging two timescales, TTNs improve sample efficiency in control tasks by combining slow feature adaptation and fast linear value function learning. This approach enables the use of linear methods, such as least-squares algorithms and eligibility traces, leading to improved learning outcomes. Stochastic approximation algorithms with traces show significant effects within TTNs, unlike nonlinear TD methods. The ability to use traces is a promising outcome for TTNs, with further research needed to determine the most effective combinations of surrogate losses and value function approximation algorithms. TTNs offer the potential to explore the effectiveness of linear value function algorithms in complex domains with learned representations. Emphatic algorithms, known for improved asymptotic properties, have not yet been utilized with neural networks. TTNs also show promise for off-policy learning, where multiple value functions are learned simultaneously to mitigate variance issues. This approach avoids destabilizing learning caused by large updates in traditional end-to-end training methods. TTNs can avoid destabilizing learning caused by large updates in traditional end-to-end training methods. Preliminary experiments in the appendix support this hypothesis. The convergence proof of two-timescale networks includes definitions and notations related to upper-semicontinuity and diagonal matrices. The curr_chunk discusses the properties of norms, feature matrices, Frechet differentiability, and assumptions related to step-size sequences and ergodic Markov chains in the context of two-timescale networks. The curr_chunk discusses the existence of a unique steady-state distribution in a Markov chain and the analysis of algorithm behavior using ODE-based analysis of stochastic recursive algorithms. The curr_chunk discusses the qualitative behavior of solutions of the ODE to determine asymptotically stable sets in the context of a multi-timescale stochastic approximation framework. The analysis guarantees that limit points of the stochastic recursion will belong to the compact connected internally chain transitive invariant set of the equivalent ODE. The policy evaluation algorithms depend on feature vectors \u03b8 t independently, with a projected stochastic recursion to ensure stability. Lemma 1 characterizes the limiting behavior of the iterates {\u03b8 t}. The lemma characterizes the behavior of the iterates {\u03b8 t} generated by the TTN algorithm, showing convergence to a set of stable equilibria K inside a compact, convex subset \u0398. The stochastic recursion updates \u03b8 t using a projection operator \u0393 \u0398 onto \u0398, ensuring stability. The lemma characterizes the behavior of the iterates {\u03b8 t} generated by the TTN algorithm, showing convergence to a set of stable equilibria K inside a compact, convex subset \u0398. The stochastic recursion updates \u03b8 t using a projection operator \u0393 \u0398 onto \u0398, ensuring stability. \u0393 \u0398 is single-valued and Lipschitz continuous in \u03b8. The noise sequence {M 1 t+1 } t\u2208N is a martingale-difference noise sequence, and C3 follows from ergodicity and finiteness of the underlying Markov chain. C4: o(\u03be t ) \u2192 0 as t \u2192 \u221e. The stochastic recursion asymptotically tracks the ODE equilibria inside a compact set \u0398, ensuring stability. Determining the constraint set \u0398 without prior knowledge of the ODE limit set is non-trivial, but starting with an arbitrary convex, compact set can help overcome this challenge. The stochastic recursion starts with an arbitrary convex, compact set \u0398 and gradually spreads to the whole space. It is important to characterize the hypothesis of Lipschitz continuity with respect to the features. Considering the non-projected form of the ODE is encouraged when using the spreading approach. The condition of being twice continuously differentiable ensures Lipschitz continuity. The TD(\u03bb) algorithm with linear function approximation can estimate the value function using features from a neural network. The algorithm involves a step-size sequence {\u03b1 t } that converges faster than another sequence {\u03be t }, leading to asynchronous convergence behavior. The feature parameter sequence {\u03b8 t } converges slower relative to the TD(\u03bb) sequence {w t }. The neural network SGD has a slower increment term compared to the TD(\u03bb) recursion, leading to unique pseudo heterogeneity and multiple perspectives. The slower timescale recursion appears quasi-static, while the faster timescale recursion seems equilibrated. Analyzing the asymptotic behavior involves considering the slow timescale stochastic recursion as quasi-stationary. The TD(\u03bb) algorithm within the TTN setting converges almost surely to the limit w* for any \u03bb \u2208 [0, 1]. Similar results apply to ETD and LSPE methods. LSTD method smoothly integrates into the TTN setting without additional considerations. Original GTD2 and TDC algorithms cannot be directly applied due to the non-singularity condition. In the TTN setting, ensuring non-singularity of feature specific matrices is crucial for the convergence of algorithms like GTD2/TDC. When features are controlled by a neural network, guaranteeing this condition is challenging. Projected versions of these algorithms, like projected GTD2, are considered to address this issue. In the TTN setting, ensuring non-singularity of feature specific matrices is crucial for the convergence of algorithms like GTD2/TDC. Projected versions of these algorithms, like projected GTD2, are considered to address this issue. For each transition in O \u03c0, projection operators onto pre-determined convex, compact subsets are used to ensure stability of iterates. The asymptotic behavior of GTD2 algorithm is analyzed under certain assumptions. The asymptotic behavior of the GTD2 algorithm is analyzed under the assumption that the feature vector \u03b8t is quasi-static. The algorithm involves a multi-timescale stochastic approximation recursion with a pseudo asynchronous convergence induced by different learning rates. The multi-timescale stochastic recursion exhibits pseudo asynchronous convergence behavior over finite time windows. Different viewpoints arise depending on the timescale observed, with the slower timescale appearing quasi-static and the faster timescale seeming equilibrated. Analyzing the faster timescale recursion shows its limiting behavior. The faster time-scale stochastic recursion of the GTD2 algorithm is analyzed under a quasi-stationary premise, showing stable iterates and martingale-difference noise sequence properties. The stochastic recursion of the GTD2 algorithm is analyzed under a quasi-stationary premise, showing stable iterates and martingale-difference noise sequence properties. The noise sequence converges asymptotically to stable equilibria almost surely. The stable limit set of the linear system inside W is the solutions, and the system is consistent as it can be viewed as the least squares solution to \u03a6\u03b8w = \u03b4u with respect to the weighted-norm Dd\u03c0. The least-squares solution may not be unique due to the singularity of \u03a6\u03b8Dd\u03c0\u03a6\u03b8, leading to a collection of asymptotically stable equilibria. The collection of asymptotically stable equilibria of the flow induced by the ODE may not be unique for every u. The slower time-scale stochastic recursion of the GTD2 algorithm is managed on a faster timescale relative to the neural network stochastic recursion. The equation can be rearranged to show the Frechet derivative of the projection operator. The Frechet derivative of the projection operator \u0393 U is defined in Equation FORMULA22. The equation can be interpreted in terms of stochastic recursive inclusion, where h3(u) = \u0393 U u (Bw) and B = E (xt \u2212 \u03b3t+1xt+1)xt. In our setting, the slower timescale recursion has multiple limit points, unlike in the GTD2 algorithm where non-singularity of certain matrices is assumed. In analyzing the GTD2 algorithm under a relaxed singularity setting, the stochastic recursion is viewed as an inclusion BID1. Recent results from Ramaswamy and BID15 are applied to study the asymptotic behavior of multi-timescale stochastic recursive inclusions. Observations include h3(u) being a singleton for each u \u2208 U and the boundedness of W and U leading to a finite supremum of (wt + ut) a.s. The stochastic recursion involves a martingale-difference noise sequence with respect to a filtration. The bias becomes asymptotically irrelevant as the boundary of U is smooth. The set A u is globally attracting and Lyapunov stable, with a map q that is upper-semicontinuous. Theorem 2 states the asymptotic behavior of the GTD2 algorithm, with conditions on compact, convex subsets and Lipschitz continuity. The set A* represents stable equilibria, and similar results can be obtained for projected TDC. The TTN algorithm converges almost surely to stable equilibria under certain conditions. TD(\u03bb) and GTD2 algorithms also converge almost surely to their respective limits under specific assumptions. The GTD2 algorithm generates sequences that satisfy certain conditions. Policy evaluation experiments were conducted on an image-based catcher with 2 stacked 64x64 frames as input. In the classic Cartpole environment, the agent balances a pole on a cart with rewards given at each timestep. In the Cartpole environment, the episode ends if the pole dips too low or the cart strays too far. The policy involves applying force to stabilize the pole or the cart's velocity. Stochasticity is added to prevent the policy from performing too well. In Puck World, the agent moves towards a good puck while avoiding a bad puck in an 8-dimensional state. Each action increases the agent's velocity in cardinal directions. The policy in Puck World involves moving the agent towards the good puck while avoiding the bad puck, with a soft cap on the agent's velocity. Eligible actions are chosen based on the agent's position relative to the good puck, with a penalty for being too close to the bad puck. In Puck World, the agent's policy involves moving towards the good puck while avoiding the bad puck with a velocity cap. Eligible actions are determined by the agent's position relative to the good puck, penalizing proximity to the bad puck. In a preliminary experiment, TTN is tested for advantages in the off-policy setting with different behavior and target policies, resulting in importance sampling ratios ranging from 0 to 8.7. TTN is compared to off-policy Nonlinear TD using three off-policy algorithms (TD, TDC, and LSTD), with features learned by optimizing the MSTDE on the behavior policy. Nonlinear TD and TTN are compared in terms of off-policy updates. TTN shows better performance with reduced variance, suggesting added stability. LSTD is used in policy evaluation experiments with specific inputs and initialization steps. The policy evaluated in Puddle World involves random north and east actions with episodes starting in the southwest corner. The policy evaluated in Catcher increased the velocity towards the apple within 25 units or chose \"None\" with a 20% random choice. Nonlinear TD uses semi-gradient TD update with nonlinear function approximation, known to have divergence issues. Nonlinear GTD has proven convergence results with nonlinear function approximation. ABTD and ABBE are adaptive bases algorithms optimizing different objectives. Nonlinear TD-Reg algorithm is derived from MSTDE, omitting ABPBE due to computational inefficiency. It combines DQN with periodic LSTD for policy evaluation, using semi-gradient TD and regularization for online training. Hyperparameter ranges were refined for optimization. The refined hyperparameter ranges were swept over for experimental runs. Different ranges were used for Puddle World and Catcher domains. Learning rates were set for two-timescale networks, with some exceptions for experiments on surrogate losses. The hyperparameters for various algorithms were adjusted, including learning rates, trace parameters, and initializers. Control experiments in the Catcher environment involved modifications to the default settings, such as giving the agent only 1 life and terminating episodes after failing to catch an apple. The reward for catching an apple in the Catcher environment is +1, and -1 at the end of an episode. The discount factor is set to 0.99. For image-based Catcher, two consecutive frames are stacked to create the state. Both DQN and TTN algorithms use an \u03b5-greedy policy. DQN anneals \u03b5 from 1.0 to 0.01 for non-image Catcher and from 0.1 to 0.01 for image Catcher. TTN keeps \u03b5 constant at 0.01 for non-image Catcher and 0.1 for image Catcher. A replay buffer of size 50000 is used for non-image Catcher and 200000 for image Catcher. The buffer is initialized with 5000 transitions from a random policy. The minibatch size for DQN and feature learning is 32. TTN uses the AMSGrad optimizer with \u03b21 = 0 and \u03b22 = 0.99, while DQN uses the ADAM optimizer with default settings, \u03b21 = 0.9, \u03b22 = 0.999. For image catcher, hyperparameters were manually tuned due to long training times. For nonimage catcher, hyperparameter tuning focused on key parameters using a preliminary search followed by a grid search. Final hyperparameters for nonimage catcher: TTN - \u03b1 slow = 10^-3, \u03bb reg = 10^-2, DQN - \u03b1 = 10^-3.75, decaying over 20k steps, update target network every 1000 steps. LS-DQN does FQI update every 50k steps with regularization weight of 1. Final hyperparameters for image catcher: TTN - \u03b1 = 10^-3. The final hyperparameters for image catcher are TTN -\u03b1 slow = 10^-5, \u03bb reg = 10^-3, and update the target network every 10,000 steps. LS-DQN does a FQI update every 500,000 steps with a regularization weight of 1."
}