{
    "title": "S1MdLyrYom",
    "content": "High performance deep learning models often have large sizes and long computation times, limiting their use on memory and battery-constrained devices. A novel pruning technique is proposed in this work, which removes filters and neurons based on their L1-norm compared to the rest of the network. This results in a more compact network that does not require special infrastructure for deployment. The method achieves significant compression rates for LeNet-5, ResNet-56, and ResNet-110 models without sacrificing performance compared to the baseline. To compress deep learning models without loss in accuracy, previous work proposed pruning weights by optimizing network's complexity using second order derivative information. Another approach explored low rank approximations to reduce the size of weight tensors. Additionally, a method was proposed to prune individual layer weights with the lowest absolute value, incorporating quantization and Huffman coding. BID12 proposed pruning network weights in a class-blind manner based on magnitude compared to all weights. Structured pruning in BID11 and BID15 removed whole filters or neurons to create non-sparse compressed models. BID13 implemented a data-free algorithm to iteratively remove redundant neurons in fully connected layers. BID6 focused on connections leading to weak weights. In BID13, neurons were pruned iteratively on fully connected layers, while in BID6, connections leading to weak activations were removed. BID16 pruned neurons based on their importance with respect to the penultimate layer. This work introduces blindness, where all layers are considered simultaneously, and structured pruning, which removes entire filters instead of individual weights. The contribution of this paper is proposing a structured approach to pruning filters based on their relative L1-norm compared to the sum of all filters' L1-norms across the network. The paper introduces a structured class-blind pruning technique to compress the network by removing filters and neurons, achieving higher compression gains with higher accuracy compared to state-of-the-art results on ResNet-56 and ResNet-110 on the CIFAR10 dataset. Each filter is denoted as Filter i in a convolutional layer, and each neuron is denoted as Neuron m in a fully connected layer. The paper introduces a structured class-blind pruning technique to compress the network by removing filters and neurons based on their normalized L1-norm. The pruning algorithm iterates over filters and neurons, removing those with norms below a certain threshold. Blindness in the algorithm refers to constructing a hidden importance score for pruning criteria. The paper introduces a structured class-blind pruning technique based on normalized L1-norm to compress the network by removing filters and neurons. Constructing a hidden importance score for pruning criteria involves normalizing filters' L1-norms and retraining the model to regain base performance. The paper introduces a structured class-blind pruning technique based on normalized L1-norm to compress the network by removing filters and neurons. The model is retrained using an iterative pruning schedule until maximum compression is achieved without losing base accuracy. Results on LeNet-5 show error percentages for different levels of parameter pruning. The proposed method is evaluated on LeNet-5 on MNIST, ResNet-56 and ResNet-110 on CIFAR-10 with identical training settings. When a filter is pruned in ResNet, the corresponding batch-normalization weight and bias are also pruned. After structured class-blind pruning based on normalized L1-norm, a new model with reduced parameters is created. Results on benchmark datasets show outperformance of state-of-the-art compression. Different components of the method are analyzed, including structured pruning and blindness, with comparisons between one-shot and iterative pruning strategies. Our method of structured class-blind pruning offers superior compression results compared to non-structured weight pruning techniques. It outperforms previous versions using one-shot pruning and iterative pruning, with fewer parameters and comparable performance to BID2. The approach involves pruning layers simultaneously based on a global threshold, resulting in state-of-the-art compression on ResNet-56 and ResNet-110 on CIFAR-10. Our method of structured class-blind pruning achieves state-of-the-art compression results on ResNet-56 and ResNet-110 on CIFAR-10 BID16, compressing over 47% and 53% respectively. It also demonstrates that only 11K parameters are needed to surpass baseline performance on LeNet-5, with a compression rate exceeding 97%. No specialized hardware or libraries are required for our method. Future work will focus on applying the technique to different architectures and datasets such as VGG-16 and ResNet on ImageNet."
}