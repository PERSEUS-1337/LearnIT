{
    "title": "rJl3YC4YPH",
    "content": "Recently, Generative Adversarial Network (GAN) and its variants have been used for image-to-image translation with impressive results. However, most GAN-based methods face issues with generator-discriminator imbalance. To address this, GuideGAN with an attention mechanism is proposed to improve image generation quality. The proposed GuideGAN framework is evaluated for image transfer tasks, showing superior results compared to existing approaches. GANs have gained attention for generating realistic images, solving various computer vision problems like super-resolution, colorization, denoising, and style transfer. Existing literature demonstrates the success of GAN variants in supervised and unsupervised settings. Incorporating attention mechanism to address the imbalance between generator and discriminator in GANs. Previous solutions focused on new objectives or regularization terms, affecting the generator. Proposing critical locating areas for translation to enhance generator performance. The text discusses the importance of critical locating areas in translation and proposes an attention-augmented discriminator to improve the generator's performance in GANs. The discriminator provides an attention map in addition to realness probability, aiming to help the generator in the translation process. In a unified GAN framework, attention information is used to enhance the generator's translation by combining the attention map with raw input. Experimental validation on various benchmarks shows the effectiveness of GuideGAN, which utilizes attention information from the discriminator for image-to-image translation. This approach strengthens communication and guidance between the generator and discriminator, leading to improved results. The significance of our work lies in the discovery that attention information from an auxiliary network influences image-to-image translation results, potentially impacting future research. Generative Adversarial Networks (GANs) have shown impressive results in image translation tasks, with a generator fooling a discriminator to distinguish between real and synthesized samples. Various improvements to GANs have been proposed, such as enhanced objective functions and advanced training strategies. A recent framework, FAL, iteratively improves synthesized images using a spatial discriminator. Image translation involves using a spatial discriminator to generate images, but more data or iterations may be needed for stability. pix2pix and TextureGAN are frameworks for supervised image-to-image translation, while other methods use disentanglement or semantic information for improvement. Cycle consistency loss in CycleGAN addresses unsupervised translation challenges. Our flexible framework can be applied to both supervised and unsupervised image translation settings. Attention learning involves biasing processing resources towards informative components, with approaches categorized as post hoc network analysis and trainable attention modules. Trainable attention models are divided into hard and soft categories. Attention is utilized in image translation, with various methods focusing on boosting attention components. The framework directly compares against state-of-the-art approaches and transfers information from the discriminator to the generator. Our framework for unsupervised translation using cycle consistency aims to learn mapping functions between two different image domains without labels. The goal is to minimize the differences between the distributions of the source and target domains. Our approach aims to minimize differences between distributions of image domains by incorporating attention maps generated by the discriminator. This involves learning mapping functions for conditional distributions and transforming attention maps into alpha channels or pixel weight maps. Different attention mechanisms and concatenation methods are studied to improve image-to-image translation, achieving promising results in various task settings. Our method involves joint-mapping learning from attention-augmented spaces to improve translation. Our method, GuideGAN, utilizes cycle consistency to enhance image translation by focusing processing resources on key areas. The framework includes a generator, discriminator, and attention transfer block. It can be extended to unsupervised settings with additional components. Training is based on generator-discriminator pairs. The GuideGAN method enhances image translation using cycle consistency and attention transfer. The generator G X translates images from domain X to Y, while discriminator D Y distinguishes real from fake images in domain Y. The attention embedded discriminator provides a probability of realness and an attention map A xi. This map is used in the attention transfer block to create an alpha channel or pixel weight map M xi, which is concatenated with x i for input to the generator G X. At the start of training, the attention map of each image is initialized as an all-ones matrix. The translation process of generator G X is formulated with parameters \u03b8 and iteration index k. The attention map from D Y is crucial for G X to focus on informative areas, preventing wastage of processing resources on inessential locations. The generator utilizes the attention map from the discriminator to focus processing resources on specific areas. This framework can be extended for unsupervised translation by incorporating additional GAN components and enforcing cycle consistency. Two types of attention are considered: post hoc attention (PHA) and trainable attention module (TAM). The discriminator uses PatchGAN as its base, with layers denoted as D = {l0, l1, ..., lm} and activations as Act D = {a1, a2, ..., am}. The attention map in the network is sensitive to layer selection, with different layers leading to different maps. While a simple attention map works well in most cases, a Trainable Attention Module (TAM) is more suitable for complex images, increasing the capacity of the generator and discriminator. The TAM follows a 2-branch architecture and contains ResBlock in each branch for implementation. In our framework, we replaced ResBlock with a simple convolution layer in the discriminator to address the capacity gap. The trunk branch extracts low-level information and passes it to following branches. The mask branch learns an attention map that weights the trunk branch output. Consecutive convolutional layers make the final prediction based on the attention map constructed from the mask branch output. We propose two methods to blend the attention map. In the proposed framework, two methods are suggested to blend the attention map with its corresponding input. The first method involves computing the Residual Hadamard Production (RHP) to address issues such as pixel degradation and maintaining the importance of all pixels in the image translation process. The RHP includes a transfer function to up-sample the attention map. Another method involves converting an RGB image to its RGBA version, where RGBA represents red-green-blue-alpha color space. The RGBA version, standing for red-green-blue-alpha, adds an alpha channel to the RGB color model to indicate pixel opacity. This helps highlight crucial areas by making nonessential areas more transparent. The generator G and discriminator D in supervised translation aim to minimize the adversarial loss of vanilla GAN, known for its training difficulty. The modified least-squares loss from LSGAN is used to stabilize training and improve image quality. Mixing traditional loss like L1 distance is also beneficial. The final objective function includes L1 loss and can be extended for unsupervised translation with cycle consistency. The GAN component in the framework focuses on translating between different domains, utilizing cycle consistency to overcome the lack of paired data. The objective functions for the generator and discriminator are defined based on distinguishing between the generated and real images. In the test phase, attention maps from previous iterations can aid in the inference process. In the training phase, attention maps from previous iterations can be used, but in testing, placeholders are needed. To address this issue, the generator should not overly rely on the attention map. Our proposed concatenation methods handle this by ensuring the attention map only enhances information. An all-one attention map is used as a placeholder, assuming the entire image is important. Two attention mechanisms and concatenations were implemented, and the challenge lies in combining them effectively. TAM struggles with simple datasets like apple2orange but performs better with complex datasets like summer2winter. Comparing alpha concatenation with RHP concatenation, we find that... In the evaluation phase, Kernel Inception Distance (KID) is used to compare alpha concatenation with RHP concatenation. The results in Table 1 show that KID is a reliable metric for assessing the similarity between real and generated images. Most experiments default to using PHA and RHP due to their overall performance across different tasks. The method is evaluated on benchmark datasets such as orange2apple and horse2zebra. In the evaluation phase, Kernel Inception Distance (KID) is used to compare alpha concatenation with RHP concatenation. Results show that KID is a reliable metric for assessing image similarity. The method is evaluated on benchmark datasets like orange2apple and horse2zebra, with promising performance in object transfer and scenery transfer tasks. The proposed framework outperforms baselines in most tasks, except for one instance in day2night where it is very close to the best result. The proposed framework outperforms baselines in most tasks, except for day2night where it is very close to the best result. Our fake horse (zebra) is more realistic than baselines. Despite background changes, we lead in apple2orange and summer2winter. CycleGAN surprisingly excelled in day2night, a challenging task. Our framework scored 6.96 in night\u2192day, while CycleGAN got 7.68 in the easier day\u2192night direction. Image-to-Image translation results show our method's performance on apple2orange and horse2zebra. The method significantly outperforms baselines in image translation tasks, especially when PHA and RHP are combined. The improvement in pixel-level accuracy is attributed to the attention map guidance. Evaluation on Cityscape using FCN scores shows the effectiveness of the approach. The supervised translation improvement is not as sharp as unsupervised translation, but still shows potential for further enhancement with minimal extra computation, especially with PHA. Strong regularizations from L1 distance between paired images play a major role in the framework, where the generator receives feedback from both the L1 loss and discriminator predictions. The idea is to let the discriminator provide more useful information, although the information from L1 loss may already be sufficient. Appendix C presents qualitative results of a novel method for image-to-image translation using attention maps from a discriminator. The method shows successful translation in both supervised and unsupervised settings, with potential for application in various GAN-based models. Different attention mechanisms and tasks could be explored in future research. The model architecture for unsupervised Cityscape translation is based on CycleGAN, utilizing a ResNet generator and PatchGAN discriminator. For supervised translation, a UNet-128 architecture is adopted. The curr_chunk discusses a modified discriminator architecture called TAM with a 2-branch structure, replacing ResBlock with a convolution layer. The discriminator features a mask branch for attention and a trunk branch for feature processing. This modification is illustrated in Figure 5. The curr_chunk describes the architecture of the discriminator with a mask branch and trunk branch, using convolution layers for feature extraction and classification. The attention maps are computed from the 4th convolution layer and resized to the original input shape. Tasks like horse2zebra and apple2orange are performed in an unsupervised setting using ResNet 9-blocks generator and PatchGAN discriminator. In the preprocessing step, the input image is resized to 143 \u00d7 143 and then cropped back to 128 \u00d7 128 for Cityscape tasks, and resized to 286 \u00d7 286 then cropped back to 256 \u00d7 256 for other tasks. The weight factor for GAN loss is set to 10 and for L1 loss is set to 10. Adam optimizer with batch size 1 is used for training on a Quadro 8000 GPU. The networks are trained from scratch with a learning rate of 0.0002 for both generator and discriminator. Learning rate is kept constant for the first 100 epochs and then linearly decayed to 0 for the next 100 epochs. In training CycleGAN, the learning rate was kept constant for the first 100 epochs and then linearly decayed to 0 for the next 100 epochs for apple2orange and Cityscape tasks, and for the first 50 epochs and then linearly decayed to 0 for the next 50 epochs for horse2zebra and day2night datasets. Evaluating image quality is challenging, with researchers using pretrained classifiers to assess the realism of generated images. Pretrained classifiers are used to evaluate the realism of generated images. For the Cityscapes dataset, the FCN-8s network is utilized, with metrics including per-pixel accuracy, per-class accuracy, and Intersection over Union (IoU) used in the experiment. Per-pixel accuracy measures the ratio of correctly predicted pixels to the total number of pixels, per-class accuracy calculates the accuracy for each class and then averages it, and IoU is a commonly used metric. The text discusses the evaluation metrics for image segmentation, including per-pixel accuracy, per-class accuracy, and Intersection over Union (IoU). The highest score for these metrics is one, indicating better performance. Additional experiments are conducted to justify limited improvement over per-class accuracy and IoU. Statistic information for each class, such as frequency and average frequency, is presented in figures. The text discusses the evaluation metrics for image segmentation, including per-pixel accuracy, per-class accuracy, and Intersection over Union (IoU). The statistic information for each class, such as frequency and average frequency, is provided in Figure 8. The attention map of the whole training set is extracted and the average per-class attention map intensity is computed. The attention map focuses on small classes like rider and terrain, potentially affecting the overall performance. The experiment results showed that focusing on generating good cars had a greater impact on accuracy than generating good riders and terrain. Attention maps were used to analyze the discriminator's behavior, showing the importance of different regions in the images. Additional translation results on a dataset were also presented."
}