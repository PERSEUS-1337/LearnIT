{
    "title": "S1eOHo09KX",
    "content": "In this paper, a novel approach for cost-sensitive feature acquisition at prediction-time is proposed. Features are acquired incrementally based on a context-aware feature-value function formulated in the reinforcement learning paradigm. MC dropout sampling is used to measure expected variations of model uncertainty, and representations are shared between the class predictor and value function estimator networks. The approach is online and applicable to stream learning setups, evaluated on three datasets including MNIST, Yahoo Learning to Rank, and a cost-sensitive dataset. The proposed method in the paper focuses on cost-sensitive feature acquisition at prediction-time, using a context-aware feature-value function in the reinforcement learning paradigm. It aims to efficiently acquire features and make accurate predictions, considering various costs such as computational, privacy, and energy consumption. The approach is evaluated on benchmark datasets and shows promising results. In medical diagnosis, creating a complete feature vector from all relevant information is impractical. Physicians start with basic features and incrementally prescribe tests based on current information for a reliable diagnosis. Real-world scenarios often require online learning and prediction due to data volume and urgent decisions. Online processing means samples are handled one at a time as they arrive, crucial for timely diagnoses. Various approaches for cost-sensitive feature acquisition have been suggested in the literature. Traditional methods focus on limiting the set of features used for training, while more recent probabilistic methods measure the value of each feature based on current evidence. These methods aim to make feature acquisition decisions based on the sample at hand and at prediction-time. Cost-sensitive feature acquisition methods involve making limiting assumptions like binary features and classes. Probabilistic methods are computationally expensive, leading to the suggestion of cascade and tree-based classifiers. These methods are limited to fixed structures, but a recent work proposed a gating method using adaptive classifiers for different instances. This outperforms previous tree-based classifiers but is limited to simple models like linear classifiers or pruned random forests. Sensitivity analysis offers an alternative approach. Sensitivity analysis of trained predictors is suggested to measure feature importance in the context of BID7 BID18. Approaches either require exhaustive sensitivity measurements or rely on approximations, with easy usability but potential for suboptimal solutions. Modeling feature acquisition as a learning problem in imitation or reinforcement learning domains shows promise in performance and scalability. This paper introduces a novel method using deep Q-networks for cost-sensitive feature acquisition based on uncertainty analysis in neural network classifiers. The approach aims to adjust the trade-off between cost and accuracy at prediction-time, allowing for more intuitive adjustments in real-world scenarios. The paper proposes a novel method using deep Q-networks for cost-sensitive feature acquisition based on uncertainty analysis in neural network classifiers. The approach adjusts the trade-off between cost and accuracy at prediction-time, allowing for more intuitive adjustments in real-world scenarios. The suggested reward function measures the value per unit of cost in the current context without requiring hyper-parameter tuning. Features are acquired incrementally while maintaining a budget or stopping criterion, and the solution is stream-based and online, optimizing acquisition costs during training and prediction. Additionally, the paper suggests sharing representations between the class predictor and action-value models. The paper introduces a method for sharing representations between class predictor and action-value models to enhance training efficiency in a scenario where samples are inputted incrementally with a feature acquisition budget. The goal is to optimize acquisition costs while considering prediction confidence and ground truth labels in an online stream setting. In an online stream processing task, Opportunistic Learning aims to make accurate predictions for each sample by acquiring necessary features while updating the model within budgets. Ground truth labels are only available after prediction for a subset of samples, with a mask vector indicating feature availability in each input sample. The total feature acquisition cost is calculated at each time step in x t i. The feature query operator (q) is defined for presenting the suggested solution. Prediction certainty is measured using Bayesian neural networks and Monte Carlo dropout to improve accuracy. The distribution of model parameters in each layer of a neural network is considered for certainty measurement. In Bayesian neural networks, prediction certainty is measured using Monte Carlo dropout. The uncertainty of final predictions with respect to inputs is found by integrating equations with respect to parameters. MC dropout interprets dropout forward path evaluations as Monte Carlo samples for accurate prediction uncertainty estimation. In this paper, prediction certainty is denoted as Cert(x t i) and formulated as a reinforcement learning problem. Each episode involves interactions between the algorithm and a data instance. The current state is defined by the feature vector x t i, with valid actions being acquiring unacquired features. Incremental feature acquisition is suggested based on the value per unit cost of each feature. The value of acquiring a feature is defined as the expected change in prediction uncertainty it causes. The reward function models weighted changes in hypothesis after acquiring each feature, resulting in an incremental solution selecting the most informative feature at each point. This property is beneficial in selecting features in reinforcement learning. In our experiments, we demonstrate the benefit of using a model under a budget determined at prediction-time. Instead of exhaustively measuring feature values, we train an action value function to estimate the gain of acquiring each feature based on the current context, following the idea of the deep Q-network (DQN). This approach involves end-to-end learning of the action-value function through exploration and updating the value function gradually. The proposed method involves training a predictor network (P-Network) and an action value network (Q-Network) jointly. The P-Network makes predictions and includes dropout layers to find prediction uncertainty, while the Q-Network estimates the value of unknown features. Representations learned from the P-Network are shared with the Q-Network to increase model stability during training. Back-propagation from Q-Network outputs to P-Network weights is not allowed for model stability. The proposed method involves training a predictor network (P-Network) and an action value network (Q-Network) jointly. Back-propagation from Q-Network outputs to P-Network weights is not allowed for model stability. The suggested sharing method of using fully-shared layers between P-and Q-Networks is reasonably efficient. Algorithm 1 summarizes the procedures for cost-sensitive feature acquisition and training the networks on a stream of input instances. In Algorithm 1, the feature acquisition procedure stops when a stopping function is met or all features are acquired. Ground-truth labels are stored separately during episodes and added to the experience replay memory after prediction. The method assumes independently acquirable features, but can also handle bundled feature sets. The paper discusses the implementation of a method using PyTorch for acquiring bundled feature sets. Experiments were conducted on GPU servers with fully connected neural network architectures. Features were normalized and missing values were imputed with zeros. NaN values were used to represent unavailable features for efficiency. The method implemented using PyTorch involved using normalized features and imputing missing values with zeros. Cross-entropy and mean squared error loss functions were used for the objective functions, with Adam optimization for training. Dropout was applied to hidden layers of the P-Network, and the target Q-Network was updated softly. Hyperparameters were determined using the validation set, with a decay in random exploration probability. The suggested solution is not sensitive to hyper-parameter values, with any reasonable setting resulting in good performance after enough training iterations. The proposed method was evaluated on three datasets: MNIST, Yahoo Learning to Rank, and a health informatics dataset. MNIST was used as a benchmark with equal feature acquisition cost assumed for all features. The LTRC dataset utilized feature acquisition costs provided by Yahoo! corresponding to the computational cost of each feature. The method was evaluated using a real-world health dataset for diabetes classification, considering feature acquisition costs and budgets. A feature set including demographic information, lab results, examination data, and questionnaire answers was used. An expert suggested costs for each feature based on financial burden, patient privacy, and inconvenience. Fasting glucose values defined three classes: normal, pre-diabetes, and diabetes. The dataset consisted of 92062 samples of 45 features. In the current study, reinforcement learning is used as an optimization algorithm for processing data in a sequential manner. Fully-connected neural networks with ReLU non-linearity and dropout are applied to hidden layers. MC dropout sampling with 1000 evaluations is used for confidence measurements and reward values, while 100 evaluations are used for prediction at test-time. The dataset is randomly split into 15% for test, 15% for validation, and the rest for training, with random exploration during training and validation phases. The study uses reinforcement learning with fully-connected neural networks for data processing. Random exploration is used during training and validation phases. Multiple model trainings are done, and outcomes are averaged. Feature acquisition continues until all features are acquired. The number of hidden neurons in P and Q networks is reported in a summary table. The study utilizes reinforcement learning with fully-connected neural networks for data processing, with random exploration during training and validation. Results are averaged from multiple model trainings, and feature acquisition is continued until all features are acquired. The Q-Network architecture includes shared neurons from the P-Network plus specific neurons. Results show that the proposed cost-sensitive feature acquisition method (OL) outperforms other methods like RADIN, GreedyMiser, and RL-Based (BID15) in terms of accuracy and cost efficiency. Fair comparisons were made with RL-Based by using similar network sizes and learning algorithms. The study compares the OL model, which outperforms other methods in accuracy and cost efficiency, with the RL-based method (BID15). Evaluating the BID15 method took more time compared to OL. The proposed method achieves higher NDCG values with a lower acquisition budget on the LTRC dataset. The proposed method achieves higher NDCG values with a lower acquisition budget compared to tree-based approaches like CSTC, Cronus BID3, and Early Exit. Visualization in FIG4 shows OL feature acquisition on the diabetes dataset prioritizing features based on context rather than static importance. OL gives priority to less costly and informative features like demographics and examinations, leading to superior accuracy in diabetes classification. In this section, the effectiveness of three ideas suggested by the paper is demonstrated: using model uncertainty as a feature-value measure, representation sharing between networks, and using MC-dropout for prediction uncertainty. The influence of budget on algorithm performance is also studied using the diabetes dataset. Comparison between the suggested feature-value function (OL) and a traditional function (RL-Based) is shown in FIG2 and FIG4. The paper presents a RL-Based feature-value function using a similar architecture as OL but with a different reward function. The suggested reward function leads to more efficient feature acquisition. The importance of MC-dropout is demonstrated by measuring prediction accuracy at different certainty values. The paper demonstrates the importance of MC-dropout for accurate prediction certainty estimates, showing that MC-dropout estimates are highly accurate compared to softmax estimates. The suggested architecture achieves better accuracy using MC-dropout certainty estimates. Sharing between P and Q networks improves convergence speed. The suggested architecture improves convergence speed by sharing between P and Q networks. It efficiently operates at different budget constraints and shows high accuracy in validation for MNIST and Diabetes datasets. In this paper, an approach for cost-sensitive learning in stream-based settings is proposed. The use of certainty estimation in neural network classifiers as a measure for feature value is demonstrated. The algorithm achieves higher validation accuracy as more data samples are observed and eventually converges after a certain number of episodes. The training algorithm and parameters used influence convergence in reinforcement learning setups. The introduced method in this paper uses model certainty per unit cost as a measure of feature value in a reinforcement learning solution. It is evaluated on real-world datasets for MNIST digits recognition, Yahoo LTRC web ranking, and diabetes prediction, showing accurate predictions and reduced feature acquisition cost."
}