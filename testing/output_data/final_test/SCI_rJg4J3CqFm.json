{
    "title": "rJg4J3CqFm",
    "content": "Euclidean embeddings have limitations in capturing latent semantic structures, unlike Wasserstein embeddings which use discrete probability distributions in a larger, more flexible space. Wasserstein embeddings can represent complex metric structures more effectively with smaller distortion compared to Euclidean embeddings. This approach shows promise in word embedding applications. Word embeddings like word2vec, GloVe, fastText, and ELMo are widely used in natural language processing for tasks like machine translation. Graph embeddings like node2vec represent knowledge graphs, while pre-trained image models are used in computer vision pipelines. Choosing the right target space for embeddings is crucial for capturing semantic structures effectively. Wasserstein embeddings offer a unique advantage by allowing direct visualization in a low-dimensional space, eliminating the need for dimensionality reduction techniques like t-SNE. In this paper, input data is embedded as probability distributions in a Wasserstein space, which provides an optimal transport metric for measuring distances between distributions. This approach offers a unique advantage over Euclidean spaces in capturing complex relationships between inputs. Recent theory has shown that Wasserstein spaces are more flexible than Euclidean spaces, allowing various metric spaces to be embedded within them while preserving their original distance metrics. This makes them attractive for machine learning embeddings to capture complex relationships between objects. Unlike prior work, this study focuses on embedding input data as discrete distributions supported at fixed points to access the full flexibility of Wasserstein spaces. The computation of optimal transport metrics and their gradients is costly, so an approximation called the Sinkhorn distance is used for efficiency. Our work explores the embedding properties of the Sinkhorn divergence, an approximation to the Wasserstein distance. We demonstrate the representational capacity of Wasserstein embeddings by embedding complex networks and computing Wasserstein word embeddings with comparable retrieval performance to existing methods. Our work explores the embedding properties of the Sinkhorn divergence, an approximation to the Wasserstein distance. The distributions can be visualized directly, unlike most embeddings, which require dimensionality reduction. The p-Wasserstein distance between probability distributions is calculated over a metric space X. The Wasserstein distance is the cost of the optimal transport plan matching distributions. This paper focuses on discrete distributions supported on finite sets of points in R^n. The Sinkhorn divergence approximates the Wasserstein distance for discrete distributions supported on finite sets of points in R^n. It involves solving a linear program with a modified version using an entropic regularizer to improve efficiency. The optimal solution is obtained with a regularization parameter \u03bb \u2265 0. The optimal solution for the Sinkhorn divergence involves optimizing for diagonal matrices with diagonal elements r and c, reducing the problem size. Matrix balancing is used to solve this, starting from an initial matrix K and projecting onto marginal constraints until convergence. This algorithm is differentiable and can be incorporated into machine learning pipelines by applying automatic differentiation to a fixed number of Sinkhorn iterations. The embedding space \u03c6 : A \u2192 B preserves distances with small distortion, characterized by the smallest C in Equation 6. Representational capacity determines data types well-represented in the space, with Wasserstein spaces allowing many embeddings with low distortion. Recently, it has been shown that every finite metric space can be embedded into Wp(R3) with vanishing distortion, suggesting that W1(R3) may be a suitable target space for arbitrary metrics on symbolic data. This universality result is not seen in other spaces like Lp or hyperbolic spaces. The reverse direction of embedding Wasserstein spaces into others is well-studied for discrete distributions. Theoretical results focus on efficient algorithms for approximating Wasserstein distances by embedding into spaces with easily-computed metrics. Low-distortion embeddings are challenging to find, with some positive results existing. Empirical investigation is done on the embedding capacity of Wasserstein spaces by learning low-distortion embeddings for various input spaces using Sinkhorn divergence. In representation learning, there is a shift towards more complex target spaces like probability distributions, Euclidean norm balls, Poincar\u00e9 balls, and Riemannian manifolds. These structures help capture uncertainty and complex relationships. Our work focuses on embedding probability distributions in a Wasserstein space, where the distance measure plays a crucial role in defining the model. Representation learning models often use L p distances and angle-based discrepancies, along with the KL divergence, when embedding into probability distributions. However, the KL divergence and L p distances can be problematic for distributions with disjoint support as they ignore the geometry of the domain being compared. Optimal transport distances, on the other hand, explicitly account for the domain's geometry, making models based on optimal transport increasingly popular in machine learning. Recent research suggests embedding into Gaussian distributions, while embeddings into Wasserstein spaces remain relatively unexplored. In contrast to embedding into Gaussian distributions, this study focuses on embedding into discrete probability distributions with the Wasserstein distance. The objective is to recover a pairwise distance or similarity relationship from the Wasserstein distance between the embedded objects. The relationship r(u, v) can be retrieved from the Wasserstein distance between \u03c6(u) and \u03c6(v) for any u, v \u2208 C. Examples include metric, graph, and word embedding tasks. A map \u03c6 : C \u2192 W p (X) is learned from training samples to predict semantic similarity. The range of \u03c6 must be defined within the infinite-dimensional space of W p (X). The map \u03c6 is defined within the infinite-dimensional space of W p (X), with a focus on discrete distributions with a fixed number of support points M. Optimal transport is reduced to a linear program, and the distribution is parameterized by the locations of its support points. Uniform weights are used for simplicity, and non-uniform weights do not improve the approximation error. Sinkhorn divergence is used as a replacement for the costly exact computation of W p. The solution of a linear program involves replacing W p with Sinkhorn divergence W \u03bb p for solvability through fixed-point iteration. Learning entails empirical loss minimization over a hypothesis space of maps H, with the goal of embedding various metrics using Wasserstein spaces. The representational power of learned Wasserstein embeddings is demonstrated by computing shortest-path distances between vertices in networks to learn a map \u03c6 : C \u2192 W p (R k) for 1-Wasserstein distance W 1. The goal is to learn a map \u03c6 : C \u2192 W p (R k) such that the 1-Wasserstein distance W 1 (\u03c6(u), \u03c6(v)) closely matches the shortest path distance between vertices u and v. The embedding minimizes distortion by minimizing the mean distortion of the input space distance metric. The performance of Wasserstein embedding is evaluated on random and real networks to test robustness. The experiments focus on the representational capacity of learned Wasserstein embeddings, using standard generative models for random networks like Barab\u00e1si-Albert. Random networks are generated using standard generative models like Barab\u00e1si-Albert, Watts-Strogatz, and stochastic block model. These networks exhibit different properties such as shorter distances, clustering of vertices, and community structure. Random trees are also generated by choosing a random number of children for each node. In random network generation, networks with 128 vertices are created using various models. The convergence order in Wp of the nearest weighted point cloud to the target measure is O(M \u22121/d) as more points are added. Non-leaf nodes have a uniform number of children from {2, 3, 4}. Comparisons are made with Euclidean and hyperbolic embeddings, with hyperbolic embeddings capturing exponential scaling of graph neighborhoods. The embedding of random networks shows that Wasserstein embeddings achieve lower distortion compared to Euclidean and hyperbolic embeddings as the total embedding dimension increases. Hyperbolic space outperforms both Euclidean and Wasserstein embeddings specifically on tree-structured metrics. The performance between R3 and R4 ground metric spaces is nearly indistinguishable for all random networks examined. In real network fragments, Wasserstein embeddings show lower distortion compared to Euclidean and hyperbolic embeddings. The study includes ArXiv co-authorship, Amazon product co-purchasing, and Google web graph networks. Distortion results are illustrated in Figure 2, demonstrating the effectiveness of Wasserstein embeddings. The curr_chunk discusses the use of algebra and word embeddings in a mathematical context. It explains the association of words in a sentence using a symmetric window and encodes this association as a label. The goal is to embed words near each other based on their co-occurrence in the context. The curr_chunk discusses a Siamese architecture model using a linear layer with 64 nodes and a point cloud embedding layer. It utilizes the Wasserstein distance to connect the branches of the network. The training dataset is Text8 with 17M tokens from Wikipedia. The vocabulary size is 8000 words, with a context window size of 2 words on each side. The study focuses on the impact of point cloud dimensionality on quality. The study evaluates the impact of point cloud dimensionality on the quality of the learned representation. Increasing dimensionality improves the semantic neighborhood captured by the embedding. Performance on benchmark retrieval tasks improves with higher dimensionality for a fixed total number of parameters. The study evaluates the impact of point cloud dimensionality on learned representation quality, showing improved performance on benchmark tasks with higher dimensions for a fixed parameter count. Results of proposed models and baselines are compared in Table 2, with gradual improvement seen with increasing dimensionality. Various methods are used to compute point cloud similarities, with performance measured by correlation with ground-truth rankings. Wasserstein embeddings in low-dimensional ground metric spaces allow for direct visualization of the embedding as a point cloud. This unique property enables the exact display of learned word representations, showing strong clustering and distinct modes in density. The point cloud of word embeddings shows strong clustering and distinct modes in density. Kernel density estimation is used to visualize the densities, revealing how well-separated different sets of words are. Military and political terms overlap, while sports names are more distant. Each word tends to concentrate its mass in multiple distinct regions, allowing for the dissection of relationships between different sets of words. The word embeddings exhibit distinct modes in density, showing how words concentrate in multiple regions. This allows for multifaceted relationships between words. The visualization helps identify errors in capturing semantics, like mistaking \"nice\" for a city in France. The effectiveness of an embedding space depends on its ability to embed various metrics. Wasserstein spaces are effective for representing complex semantic structures when Euclidean space is not enough. Entropy-regularized Wasserstein distances enable visualization of embeddings and suggest directions for further research, such as lifting representation spaces to distributions. In this paper, the focus is on representing W(X) as point clouds, specifically when X = Rn. The use of \u03b4-functions allows for a general \"lifting\" procedure to enhance representation capacity. Other potential tasks include co-embedding different modalities in the same transport space. Empirical results indicate the potential for studying the embedding capacity of Sinkhorn divergences. There is also interest in using learned mappings for interpolation, following recent work on computing geodesics in Wasserstein space."
}