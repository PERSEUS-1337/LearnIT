{
    "title": "rJNpifWAb",
    "content": "Stochastic neural net weights are used in various contexts like regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Flipout is introduced as an efficient method for decorrelating gradients within a mini-batch by sampling pseudo-independent weight perturbations for each example. It achieves linear variance reduction for different types of networks and shows significant speedups in training with Gaussian perturbations. Flipout is effective at regularizing LSTMs and outperforms previous methods, enabling vectorization of evolution strategies on a single GPU. In experiments, a single GPU with flipout can match the throughput of 40 CPU cores, reducing costs on Amazon Web Services. Stochasticity is crucial in modern neural net architectures and training algorithms. Evolution strategies evaluate weight perturbations in parallel to minimize a black-box objective, showing impressive performance in robotic control tasks. Some methods perturb network activations or weights, with stochastic weights being useful for regularization and exploration. Stochastic weights are beneficial for regularization and exploration in neural networks, but they are costly to compute and store compared to stochastic activations. Stochastic activations allow for more perturbations in less time, leading to a decay in the variance of stochastic gradients. This is why stochastic activations are more commonly used for neural net regularization. In this paper, the authors introduce flipout, a method for decorrelating gradients in neural networks without biasing estimates. Flipout is efficient and applicable to various perturbation distributions and network architectures, providing unbiased stochastic gradients. In this paper, the authors introduce flipout, a method for decorrelating gradients in neural networks without biasing estimates. Flipout is efficient and applicable to various perturbation distributions and network architectures, providing unbiased stochastic gradients. The method demonstrates reduced variance in gradient calculations and shows speedups in training time, particularly in large batch regimes. Additionally, flipout outperforms dropout in regularizing recurrent connections in LSTMs and allows for vectorized evolution strategies, resulting in cost reductions on Amazon Web Services. Weight perturbation methods, like Gaussian perturbations, aim to reduce costs on Amazon Web Services by sampling neural network weights stochastically during training to minimize expected loss. This method allows for efficient decorrelation of gradients without biasing estimates, leading to reduced variance in gradient calculations and faster training times. Weight perturbation methods, such as Gaussian perturbations and multiplicative perturbations, aim to reduce costs on Amazon Web Services by stochastically sampling neural network weights during training. This allows for efficient decorrelation of gradients without biasing estimates, leading to reduced variance in gradient calculations and faster training times. Variational Bayesian neural nets offer an alternative approach by adopting a Bayesian method of putting a prior on the weights instead of fitting a point estimate. DropConnect, inspired by dropout, randomly zeros out a subset of weights as a regularization method. Variational Bayesian neural nets use a Bayesian approach with a prior distribution over weights to approximate the posterior distribution. By fitting an approximation using variational inference and maximizing the evidence lower bound, one can derive unbiased stochastic estimates of the gradient. This method combines insights from Gaussian weight perturbation and the reparameterization trick to optimize training efficiency. Evolution strategies (ES) are black box optimization algorithms that use weight perturbations to search for model parameters. ES is a parallelizable algorithm that generates perturbations independently by different workers. The ES algorithm aims to maximize the objective function by estimating the gradient of the parameters and updating them using a learning rate and Gaussian noise. Evolution strategies (ES) use weight perturbations for optimization. Weight perturbations can be reformulated as activation perturbations, allowing for efficient computation independently for different examples in a mini-batch. The local reparameterization trick (LRT) enables unbiased stochastic gradients to be computed without explicit weight perturbations in fully connected networks with no weight sharing. Variational dropout, inspired by LRT, is a regularization method that performs well empirically. Control variates, like flipout, are strategies for variance reduction in optimization. They can be combined to achieve larger variance reduction. Weight perturbation algorithms suffer from high variance due to shared perturbations in mini-batches. In this section, flipout is introduced as an efficient way to perturb weights quasi-independently within a mini-batch. The weight distribution assumptions include independent perturbations and a symmetric distribution around zero, encompassing important use cases like Gaussian perturbations and DropConnect. The perturbation distribution is invariant to elementwise multiplication by a random sign matrix under these assumptions. Flipout is a method that perturbs weights quasi-independently within a mini-batch by using a base perturbation shared by all examples and multiplying it by a different rank-one sign matrix for each example. This approach yields an unbiased estimator for loss gradients by decorrelating the gradients between examples. By decorrelating gradients between training examples, lower variance updates can be achieved when averaging over a mini-batch. Flipout allows for efficient matrix multiplications on GPUs and TPUs, with activations in neural net layers defined by matrices R and S. Backpropagation through the vectorized computations yields unbiased estimators for loss gradients. Flipout's forward pass requires two matrix multiplications instead of one, making it roughly twice as expensive as a forward pass with a single shared perturbation. The backward pass with flipout is also expected to be about twice as expensive as an update with a single shared perturbation. Flipout enables Evolution Strategies (ES) to run efficiently on a GPU by allowing each worker to evaluate multiple perturbations in a batch, increasing parallelism. The update rule for ES using Flipout involves replicating the starting state by the number of perturbations at each worker, enabling efficient evaluation of perturbations on a GPU architecture. In this section, the analysis focuses on the variance of stochastic gradients with and without flipout. Flipout is shown to reduce the variance of gradient estimates compared to using shared perturbations. The gradient averaged over a mini-batch is denoted as a random variable, with decomposition into data and estimation variance terms using the Law of Total Variance. The analysis in this section compares the variance of stochastic gradients with and without flipout. Flipout reduces gradient estimate variance compared to shared perturbations. The gradient over a mini-batch is a random variable decomposed into data and estimation variance terms using the Law of Total Variance. The estimation term may not decay with N due to its dependence on shared perturbations, which can be broken down for analysis. The shared perturbation scheme involves generating \u2206W by sampling and multiplying it by a random sign matrix, yielding an identical distribution to standard shared perturbations. The Variance Decomposition Theorem defines \u03b1, \u03b2, and \u03b3 under certain assumptions for shared perturbations and flipout. The variance of gradients under shared perturbations and flipout perturbations can be expressed in terms of \u03b1, \u03b2, and \u03b3. \u03b1 represents the variance of gradients on individual training examples, \u03b2 reflects covariance from sampling r and s, and \u03b3 reflects covariance from sampling \u2206W. Flipout eliminates \u03b2 but not \u03b3. The variance reduction effect of flipout perturbations was empirically verified, showing a consistent 1/N variance reduction regardless of the range of N values explored. Flipout eliminates covariance from shared perturbations but not from sampling \u2206W, resulting in a variance that remains approximately \u03b1/N throughout. In experiments, the variance of gradients was measured under different perturbations for various neural network architectures and batch sizes. Flipout was found effective for regularizing LSTM networks and converges faster than shared perturbations with large minibatches. Evolution Strategies combined with flipout were tested in supervised and reinforcement learning tasks using four architectures. In experiments, the variance of gradients was measured under different perturbations for various neural network architectures and batch sizes. The analysis predicts variance curves for flipout and shared perturbations with a specific functional form. Confidence intervals are based on 50 independent runs of the estimator. The experiments measured gradient variances under different perturbations for neural networks. The analysis predicts variance curves for flipout and shared perturbations. Flipout shows ideal linear variance reduction across mini-batch sizes, while shared perturbations exhibit a phase transition between batch sizes of 100 and 1000. Flipout achieves smaller variance than explicit weight perturbations, but is more versatile for various architectures. Regularization methods for RNNs were evaluated on language modeling tasks using the Penn Treebank corpus. Flipout was compared to other methods like dropout, variational dropout, recurrent dropout, zoneout, and DropConnect. Different methods applied dropout to different connections in the RNN. Flipout showed linear variance reduction across mini-batch sizes and outperformed explicit weight perturbations in terms of variance. For character-level experiments, a weight-dropped LSTM model was used with 1000 hidden units. Training was done on 100-character sequences in batches of 32 using AMSGrad variant of Adam. Flipout was applied to the hidden-to-hidden weight matrix. Results in bits-per-character (BPC) for validation and test sequences of PTB are shown in Table 2, where Mult. Gauss + Flipout outperforms other methods. Mult. Gauss + Flipout achieves the best results for word-level experiments using a 2-layer LSTM with 650 hidden units and 650-dimensional word embeddings. Training was done on sequences of length 35 in batches of 40 for 100 epochs. Flipout was used for DropConnect implementation and recurrent regularization. Embedding dropout was applied with probabilities of 0.1 for most models and 0.2 for Gal. Results for perplexity on the PTB word-level validation and test sets are reported. In Table 3, WD+Flipout outperforms other methods in terms of validation and test perplexity. WD+Flipout shows significant variance reduction for large mini-batches, converging faster than WD with batch size 8192. The variance reduction effect of flipout is more pronounced in the large mini-batch regime, speeding up training iterations for Bayesian neural networks. Training loss comparisons are made using flipout with shared perturbations and LRT. In comparing flipout with shared perturbations and LRT for training loss, flipout converged faster with comparable performance to LRT for the FC model. Flipout is more expensive but provides a 1.5x speedup overall. ES with flipout allows for efficient GPU processing by evaluating a batch of perturbations. Flipout ES is shown to be just as sample-efficient as IdealES, allowing for significantly higher throughput on a GPU. With 5,000 samples needed for stable performance in supervised learning tasks, FlipES enables the same number of samples to be evaluated with fewer explicit perturbations. Mini-batches of size 40 were used in the experiments. Flipout ES, compared with IdealES and cpuES, shows no loss in performance on the MNIST dataset. FlipES scales better due to running on the GPU and achieves data efficiency comparable to backpropagation on both FC and ConvLe models. FlipES reduces the computational gap between ES and backpropagation, making it applicable to models that are not fully differentiable. Flipout is an efficient method for decorrelating weight gradients in models with discrete loss or stochastic units. It reduces variance compared to shared perturbations, leading to significant speedups in training time and outperforming dropout-based methods for regularizing LSTMs. Flipout also enables the practical application of GPUs to evolution strategies, increasing throughput for a given computational cost. In this section, the proof of Theorem 2 (Variance Decomposition Theorem) is provided. The data and estimation terms are decomposed, and the cases of fully independent perturbations, shared perturbations, and flipout are analyzed. If the perturbations are fully independent, the second term disappears, resulting in a simplified expression of \u03b1/N. The proof of Theorem 2 decomposes data and estimation terms for fully independent perturbations, shared perturbations, and flipout. The network configurations for experiments include a 3-layer FC network and a ConvLe network similar to LeNet. The ConVGG network is based on VGG16 with modifications for experiments on CIFAR-10. Batch normalization was not used for variance reduction. LSTM and language modeling experiments had specific architectures and hyperparameters. Stochastic gradient update variance was computed using a pre-trained model with 85% accuracy on MNIST. The ConVGG network, based on VGG16, was used for experiments on CIFAR-10 without batch normalization. The gradient variance of weights was computed by repeating feed-forward passes and backpropagation 200 times. Confidence intervals on the variance estimates were computed using a t-test after repeating the procedure 50 times. Multiple GPUs were required for the variance reduction experiment. For the ConVGG network, multiple GPUs were used to run the variance reduction experiment with large mini-batch sizes. Independent weight perturbations were generated on different GPUs, but a shared base perturbation was used to understand the effects of variance reduction. In LSTM experiments, a two-layer LSTM was trained on the Penn Treebank dataset for 3 epochs. Large mini-batches were split into sub-batches, with one shared base perturbation and independent matrices for each sub-batch. Long Short-Term Memory networks (LSTMs) have various regularization approaches to prevent memory loss over long sequences. Some methods include dropping the cell update vector with a dropout mask, or zone out units by stochastically updating or maintaining their previous value. For word-level models, gradient clipping was used. Regularization approaches for Long Short-Term Memory networks (LSTMs) include variational dropout, embedding dropout, and zoneout masks. Different hyperparameters were used for various models, such as dropout probabilities and mask sampling. Gradient clipping was also applied for word-level models. Regularization techniques for LSTM networks include variational dropout, embedding dropout, and zoneout masks. Different hyperparameters were used for each model, such as dropout probabilities and mask sampling. Gradient clipping was also applied for word-level models. Regularization techniques for LSTM networks include variational dropout, embedding dropout, and zoneout masks. Gaussian noise with \u03c3 = 1 was sampled for flipout and shared perturbation LSTMs. Training on multiple GPUs induces independent noise for each sub-batch, leading to lower variance with flipout compared to shared perturbations. Large batch training with flipout using mini-batch sizes of 8192 and 4096 showed improved performance for FC and ConvLe networks. The use of flipout in LSTM networks allows for faster convergence without overfitting, enabling efficient training with large mini-batches. Flipout is applied to implement DropConnect in an LSTM word-level language model, providing a different mask per example for regularization. This approach differs from shared perturbations, offering improved performance in large batch training scenarios. DropConnect mask per example was applied to both hidden-to-hidden (h2h) and input-to-hidden (i2h) weight matrices in the models WD and WD+Flipout. Both models used embedding dropout 0.1, output dropout 0.4, and had DropConnect probability 0.5 for the i2h and h2h weights. Training with Adam at a learning rate of 0.001, WD+Flipout showed significant variance reduction for mini-batch sizes larger than 256 compared to WD. Additionally, WD+Flipout converged faster and achieved a lower training perplexity with a batch size of 8192, demonstrating the optimization benefits of flipout in large mini-batch settings."
}