{
    "title": "BJerQWcp6Q",
    "content": "Biomedical knowledge bases are crucial in modern data-driven biomedical sciences. In this paper, the problem of disease entity normalization is addressed with the presentation of NormCo, a deep coherence model. NormCo models entity mentions using a semantic model and coherence as a disease concept co-mention sequence. Distantly supervised data and synthetic data from the BioASQ dataset were used to overcome data sparsity. Experimental results show that NormCo outperforms state-of-the-art baselines. NormCo outperforms state-of-the-art baseline methods on disease normalization corpora in terms of prediction quality and efficiency. Biomedical sciences rely on reliable databases of knowledge, especially in precision medicine where treatments are tailored to genetic profiles. Natural language processing has advanced in extracting knowledge from research publications for constructing knowledge bases in the biomedical domain. The paper focuses on disease normalization in constructing biomedical knowledge bases. Current NLP algorithms are not keeping up with the rapid growth of new publications, while AI and machine learning in other domains are advancing quickly. Deep learning approaches like DNorm and TaggerOne have shown promise in disease normalization, but more advanced methods are needed. Deep learning has been successful in entity linking and word sense disambiguation, but applying it to disease normalization remains a challenge. The paper introduces NormCo, a deep learning model designed for disease normalization, addressing questions on accuracy, training with limited data, and efficiency compared to shallow learning methods. NormCo combines semantic features to improve disease normalization. The paper introduces NormCo, a deep learning model for disease normalization that combines semantic features and topical coherence. It addresses data sparsity by augmenting existing datasets with distantly supervised data, outperforming state-of-the-art methods in prediction quality and training speed. The paper introduces NormCo, a deep learning model for disease normalization that combines semantic features and topical coherence. It addresses data sparsity by augmenting existing datasets with distantly supervised data, outperforming state-of-the-art methods in prediction quality and training speed. The primary problem in entity normalization is to map ambiguous spans of text to a unique concept defined in an ontology, which involves disambiguating and correctly mapping text segments. Disease normalization involves mapping disease mentions to concepts in an ontology, using synonyms and context for disambiguation. Co-mentioned diseases in an article can form a coherence topic for normalization. Disease normalization involves mapping disease mentions to concepts in an ontology, considering similarity of mention text, context, and coherence with other diseases. It is challenging due to evolving disease conceptualization and nomenclature. Disease concepts are composite and can be divided into components. Previous work focused on searching for the best match from a list of synonyms. DNorm BID25 is a method using pairwise learning to rank for normalization. The method uses pairwise learning to rank for disease normalization. It models entity mentions and concept names as TF-IDF vectors, learning a similarity matrix to determine scores. The model is trained with a margin ranking loss, matching mentions with concept names in the ontology. TaggerOne BID24 learns NER and entity normalization using semi-Markov models, considering context for better performance. It relies on rich features for NER and TF-IDF weighting for normalization, training a similarity matrix to measure mention and concept name similarities. Recent attempts involve developing deep models for disease normalization. Proposed deep models for disease normalization do not consider mention, context, and coherence simultaneously as in recent work on entity linking. Recent methods in entity linking include the use of expectation maximization to train a selective context model, knowledge graph densification, and deep neural networks. In disease normalization, models like BID21, BID35, and deep neural networks BID10 BID40 BID41 BID13 BID22 utilize context information to improve performance. The concept of \"global\" context is studied in entity linking to induce a coherent set of concepts. BID21 uses surrounding referring phrases as context features, while BID10 embeds words and entities in a shared space, scoring words in a context window around the mention text. The model in BID22 utilizes local and document level context to ensure coherence among entities within a document. It exploits latent relationships between mentions to induce coherence. Current methods rely on dense vector representations of text for downstream tasks, modeling semantics and morphology based on the distributional hypothesis. The models of BID30, BID34, and BID33 use different approaches to derive word meaning from context. BID18 and BID14 employ neural networks and bag of words models to encode and decode sentences. NormCo is a deep model for disease normalization that maps mention entities to a disease concept space. NormCo is a deep model that maps mention entities to a disease concept space by utilizing two sub-models: an entity phrase model and a coherence model. The entity phrase model focuses on the morphological and semantic patterns of mentions, while the coherence model considers other diseases mentioned in the same document. These sub-models are trained jointly to create the final model for disease normalization. The entity phrase model of NormCo maps mentions to concepts by creating a composite embedding of the mention through word embeddings. For example, the mention \"Crohn disease\" is tokenized into \"Crohn\" and \"disease\", and their word embeddings are obtained. The entity phrase model of NormCo creates a composite embedding of mentions through word embeddings. A bidirectional gated recurrent unit network is used to model coherence and predict diseases likely to appear together in a document. This approach has shown to improve performance on entity linking tasks. The entity phrase model of NormCo uses a bidirectional GRU network to improve performance on entity linking tasks by considering the coherence of mentions in a document. The model ensures that selected concepts for entities like \"ulcerative colitis\" are likely to appear together, preventing nonsensical mappings. Using a GRU instead of LSTM has shown better performance in this context. The use of a GRU instead of an LSTM has shown better performance in entity linking tasks. The GRU yields a forward and backward representation for each concept, which are concatenated to include features from both past and future concepts in the sequence. This joint training approach aims to learn concept embeddings and models for improved performance. The joint training approach involves training concept embeddings and models to ensure the entity phrase model learns a good mapping from tokens to concepts, while the coherence model adjusts phrase representations based on surrounding concepts. Experimental results confirm that joint training outperforms separate training. The distance metric used is the Euclidean distance, combined with learned parameter \u03b2 to automatically adjust distances. The purpose of \u03b2 is to automatically learn how to combine the score for each model. When a mention m i maps to a concept c j, the model is trained to minimize the distance between them using a max-margin ranking loss with negative sampling. The entity phrase model and coherence model are jointly trained to improve mapping from tokens to concepts and adjust phrase representations based on surrounding concepts. The entity phrase model and coherence model are trained together to map mentions to concepts using supervised and distantly supervised data. The models process annotations one document at a time, matching vectors to concept vectors in the ontology. The selected concept for each mention is determined using specific equations. Biomedical entity normalization datasets are small, while training deep neural networks requires substantial data. To compensate for a lack of training examples for co-occurring diseases, two additional corpora were generated using existing labeled datasets. The first corpus, obtained from the BioASQ dataset BID42, contains PubMed abstracts labeled with MeSH terms. However, these annotations are at the paper level and not directly linked to disease mentions in the abstract. High-quality labels for disease mentions were generated from the MeSH terms to train models effectively. The second corpus consists of synthetic data generated using co-occurrence probabilities derived from the BioASQ dataset. The probabilities are estimated by counting the co-occurrences of concepts based on the list of MeSH terms in each abstract, covering approximately 40% of the concepts in the ontology. Unseen concepts are estimated by replacing their count with that of the nearest parent in the concept poly-hierarchy with the lowest unary occurrence. In the concept poly-hierarchy, concepts with the lowest unary occurrence are chosen for their specificity to the target concept. For instance, \"Teeth disorders\" is selected over \"Wounds and injuries\" as a parent concept of \"Teeth injury\" due to its closer relation. Co-occurrence probabilities from the BioASQ dataset are used for data augmentation by sampling sets of concepts and associated synonyms during training batches, crucial for the model's predictive accuracy. Data augmentation is crucial for model accuracy. The ontology used maps disease concepts to identifiers. Each concept has a name, identifier, and synonyms. Batches of mention only examples are passed through the entity phrase model to optimize L with \u03b2 set to 1.0. This provides additional training examples for the model to learn rare concepts. PyTorch was used for implementation with d = 200 dimensional word2vec embeddings pre-trained on PubMed abstracts and articles. The text describes the use of 200 dimensional word2vec embeddings pre-trained on PubMed abstracts and articles to initialize word embeddings. Concept embeddings are initialized by summing up word embeddings of tokens in the preferred name for each concept. Text preprocessing involves removing punctuation, lowercasing, and tokenizing using NLTK Treebank word tokenizer. Abbreviations are resolved by mapping them to their long form. Token sequences are truncated to a maximum length of 20. Training is done in batches of 10 for mentions only, and single batches for full model training. Negative samples are randomly selected from ontology concepts, with 30 samples. PyTorch's Adam optimizer is used for network optimization. To optimize the network, PyTorch's Adam optimizer with an initial learning rate of 0.0005 is used. A validation set is employed for benchmarking the model's generalization performance and early stopping. Models typically converge within 10-30 epochs. Evaluation is conducted on two biomedical entity normalization corpora: the NCBI disease corpus and the BioCreative V Chemical/Disease Relation corpus. The NCBI disease corpus consists of labeled disease mentions in scientific abstracts from PubMed, with training, development, and test sets. Disease mentions are labeled with MeSH or OMIM identifiers from the MEDIC Disease Dictionary. The BioCreative V Chemical/Disease Relation (BC5CDR) corpus contains abstracts with disease mentions labeled with MeSH/OMIM identifiers. The training set was split into train and validation sets, with additional labeled abstracts obtained. The NCBI disease corpus also had additional abstracts for distant supervision. For the BioCreative V task, a unique concept labeled \"unk\" was added to the ontology for mentions that should return a NIL result. Baseline methods used were DNorm BID25 and TaggerOne BID24. Different versions of the model were created and compared, including Mention only (MO), Mention + Coherence (MC), Distantly supervised data (*-distant), and Synthetic data (*-synthetic). The model trained on synthetically generated data, distantly supervised data, dictionary data, and the dataset. It uses dense word embeddings to capture compact representations, requiring significantly fewer parameters compared to baseline methods. Performance was evaluated using micro-F1 and perfect tagger accuracy, with the micro-F1 score calculated at the instance level. In BID25 and BID24, the micro-F1 score is calculated at the instance level, considering true positives for correct concepts found within abstracts regardless of span boundaries. Results are averaged across all documents, using micro-F1 for comparison to recent literature metrics. Perfect tagger accuracy was tested for each system's ability to normalize text from the gold standard test set. Abbreviation resolution was disabled for \"AwA\" results to compare normalization methods. The text discusses evaluating different normalization methods by testing their performance using a perfect tagger and measuring accuracy. However, accuracy alone may not reflect the overall quality of predictions, so the normalized lowest common ancestor (LCA) distance metric is used to assess prediction quality based on ontology concepts. The text introduces the normalized lowest common ancestor (LCA) distance metric to evaluate prediction quality based on ontology concepts. This metric considers both accuracy and severity of errors, normalizing over both aspects. Additionally, training time is used to compare the performance of each model. The results show consistent improvement in micro-F1 scores over baseline methods while maintaining good recall. Our coherence model with distantly supervised and synthetic data outperformed BID24 in terms of F1 by 0.033 on the NCBI disease corpus. NormCo maintains good performance, especially in precision, when the abbreviation resolution module is removed. The model's accuracy with the entire pipeline outperforms baselines on the NCBI disease corpus and is competitive on the BioCreative diseases dataset. The lower F1 on the BC5CDR dataset is attributed to the model prioritizing precision over recall. Our models consistently produce higher quality predictions than baseline methods in terms of the average distance to common ancestors. For example, on the NCBI disease corpus, errors made by our best model tend to be related to the original concept, either a parent or child concept of the ground truth concept. This is evident in behavior such as the mention \"congenital eye malformations\" being mapped to \"Congenital Abnormalities,\" a parent concept of the ground truth. The model with coherence showed slight improvement in normalization by utilizing surrounding mentions to identify correct concept mappings, unlike the mention only model which made errors like mapping \"cystitis\" to \"Hemorrhage\" instead of \"Cystitis\" due to composite labels in the training data. The study observed improved performance in normalization with coherence during training, influencing how the model learns to map mentions to concepts. Joint training of the two models yielded better results, especially when the distance was combined using a learned parameter \u03b2. Augmenting the dataset with ontology terms and distantly supervised data also had notable impacts on the model's performance. The study found that training with both supervised mentions and synonyms from the ontology is crucial for model performance. Removing dictionary mentions and distantly supervised data led to a 12% drop in model performance. Additionally, the use of distantly supervised data significantly contributed to the overall performance of the system, as shown in FIG7. The model achieved over 76% accuracy with distantly supervised data and ontology names. Adding supervised training data improved accuracy linearly. Combining both datasets led to high performance. Runtime performance was also evaluated. The experiments were conducted on a machine with specific hardware specifications. The models scaled linearly with the size of the training set. TaggerOne was the most memory-intensive, while DNorm took about 6-10 minutes per epoch to train. NormCo outperformed DNorm in terms of training speed, with a 7.88x-15x speedup compared to DNorm and a significant improvement over TaggerOne. NormCo is significantly faster to train than TaggerOne and DNorm, achieving 76% accuracy on the test set with only distantly supervised data. Deep models have not been effectively utilized in accelerating biomedical knowledge base construction due to challenges like limited training data. A proposed deep model addresses disease normalization using MeSH ontology and BioASQ dataset. The MeSH ontology and BioASQ dataset support training a deep model for disease normalization, providing higher quality predictions than current solutions. Challenges remain in fully automating biomedical knowledge base construction, requiring more attention and investment from the AI research community. This work is backed by IBM Research AI through the AI Horizons Network."
}