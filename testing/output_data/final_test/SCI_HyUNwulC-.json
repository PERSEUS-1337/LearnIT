{
    "title": "HyUNwulC-",
    "content": "Recurrent neural networks (RNNs) with linear dependencies can be trained in parallel over sequence length using the parallel scan algorithm, leading to faster training on long sequences. A parallel linear recurrence CUDA kernel speeds up training and inference of RNN architectures by up to 9x. A new framework of linear surrogate RNNs, including the GILR-LSTM model, extends sequence learning to previously unreachable long sequence regimes. By successfully training a GILR-LSTM on a synthetic sequence classification task with a one million timestep dependency, the new framework extends sequence learning to extremely long sequence regimes. Recurrent neural networks (RNNs) like LSTMs and GRUs are commonly used for sequence modelling tasks in various domains. However, the non-linear dependencies in RNNs limit their ability to efficiently process long sequences. To address this, parallelizing the forward and backward pass over minibatches of inputs is used to mitigate the inefficiency in evaluating RNNs on long sequences. This is crucial for domains such as robotics, remote sensing, control systems, speech recognition, medicine, and finance. Persistent RNNs aim to achieve high GPU utilization with small minibatch sizes, but face limitations with serial evaluation inefficiency at smaller hidden sizes. Training with larger minibatches can reduce generalization ability and increase latency, impacting optimization steps. Minibatches transform RNN computation into more efficient matrix-matrix multiplications, but come with disadvantages such as limited model size by GPU memory and linear memory requirements. Numerous prior works have shown strong performance from neural sequential models with linear dependence on earlier sequence elements. BID2 investigated RNNs with elementwise linear recurrence relations and developed linear variants of LSTM and GRU. BID4, BID7, and van den have successfully applied convolutional sequence models for tasks such as machine translation, language modeling, and audio generation, observing increased training throughput compared to RNN alternatives. Convolutional sequence models typically rely on an attention mechanism or a recurrent layer to integrate information at scales larger than the filter width. Linear recurrence is a type of computation involving repeated application of a binary operator over data arrays. Scans and reductions are examples of such computations, with reductions finding the sum or maximum of an array, and scans performing cumulative operations. Parallelization of scans and reductions is possible when the operator is associative. First order recurrences can be parallelized using the parallel scan algorithm. Linear recurrence computations of the form h t = (\u039b t \u2297 h t\u22121 ) \u2295 x t can be parallelized with the parallel scan algorithm if three conditions are met. This allows for evaluation in parallel over time steps t for vectors x t and square matrices \u039b t. The study classifies RNNs that satisfy these conditions, including QRNNs, and provides a CUDA kernel implementation that speeds up training of QRNN and SRU. The text describes a CUDA kernel that accelerates training for QRNN and SRU architectures by up to 9x. It introduces a linear surrogate for LSTM, trained with a speedup of 5-10x using a parallel linear recurrence algorithm. The algorithm's cost is compared between serial and parallel scans over a sequence of length T. The text discusses the parallel algorithm's cost for matrix multiplication in neural network models, focusing on diagonal matrices for efficient parallelization. This constraint is necessary for favorable performance and memory efficiency, as separate recurrent matrices for each sequence element would require excessive memory. The text discusses the use of recurrent coefficient vectors \u03bb t for implementing schemes like exponential moving averages or gating mechanisms in neural networks. It also explains how parallel linear recurrence over sequence elements allows for efficient forward and backward passes in linear RNNs using GPUs with multiple parallel processors. The work implements parallel linear recurrence as a CUDA kernel with TensorFlow bindings, allowing for efficient evaluation in parallel. The CUDA kernel uses warp processors to achieve a theoretical speedup factor of several hundred. Peak performance is achieved on sequences of several thousand steps with at least a 32 element vector. The implementation details are available on GitHub and can be used for various differentiable modules like gating schemes and exponential moving averages. A GILR layer in a linear recurrent network applies non-linear transformations to sequence elements and accumulates them with a gating mechanism. Stacking GILR layers allows for rich non-linear dependence on previous events while enabling fast parallel sequence evaluation. The implementation of parallel linear recurrence as a CUDA kernel with TensorFlow bindings achieves peak performance on sequences of several thousand steps. The GILR layer in a linear recurrent network enables fast parallel sequence evaluation by applying non-linear transformations and using a gating mechanism. It allows for rich non-linear dependence on previous events and achieves peak performance on long sequences. The RNN performance heavily depends on minibatch size due to serial matrix multiplications. The GILR layer in a linear recurrent network enables fast parallel sequence evaluation by applying non-linear transformations and using a gating mechanism. It allows for rich non-linear dependence on previous events and achieves peak performance on long sequences. The RNN performance heavily depends on minibatch size due to serial matrix multiplications. GILR performance is much less dependent on batch size as the matrix multiplication kernel sees an \"effective batch size\" of bT and T is typically large. RNNs learn a transition function s t = f (s t\u22121 , x t ) which combines previous state s t\u22121 with input x t to compute current state s t. Non-linear f prevents application of the parallel linear recurrence algorithm and forces slow serial evaluation. The GILR layer in a linear recurrent network allows for fast parallel sequence evaluation with non-linear transformations and a gating mechanism. It enables rich non-linear dependence on previous events and performs well on long sequences. RNNs rely heavily on minibatch size for performance, while GILR's performance is less affected by batch size. RNNs use a transition function to compute current state based on previous state and input, but non-linear functions hinder parallel linear recurrence algorithms, leading to slower evaluation. The GILR-LSTM introduces a linear surrogate layer to handle dependencies on previous events. It contains more parameters than a regular LSTM to map inputs to hidden states. Experiments show that a parallel linear recurrence kernel can achieve higher throughput and speed up LS-RNNs like QRNNs. The GILR-LSTM architecture, computed with the parallel linear recurrence algorithm, trains significantly faster than an optimized LSTM. The parallel linear recurrence algorithm trains faster than an optimized LSTM on a long-term dependency problem. Throughput advantage is illustrated for evaluating the linear recurrence using CUDA kernels. Performance depends on sequence length and features/minibatch size. Performance measurements are made at the kernel level to avoid TensorFlow overhead. The parallel kernel algorithm offers a significant speedup of up to 40x for long sequence lengths compared to TensorFlow. LS-RNNs like SRUs and QRNNs show improved performance with the parallel linear recurrence method, providing a notable speedup without numerical changes. In experiments comparing CuDNN LSTM with serial and parallel linear recurrence, parallel linear recurrence can accelerate inference up to 9x. The experiments controlled for GPU memory usage and used a popular architecture with two stacked RNN layers. Simpler architectures are more affected by the speedup from parallel linear recurrence. The GILR-LSTM can handle long-term dependencies and outperform CuDNN LSTM in inference tasks. It is demonstrated using a canonical example from BID10, showing the architecture's ability to extend to sequences. The structure of the synthetic example and the GILR-LSTM architecture used are depicted in Figure 1. The GILR-LSTM architecture can handle long-term dependencies and outperform CuDNN LSTM in inference tasks. It is demonstrated using a canonical example from BID10, showing its ability to extend to sequences. The RNN model must read in a sequence and output the sign of the first element, struggling with remembering the first element over the length of the sequence. The study compared a two layer GILR-LSTM with 512 hidden units to a two layer LSTM with 512 hidden units using sequences of different lengths. The experiments were conducted on a NVIDIA K80 GPU, with a brief search for optimal parameters. The study compared the GILR-LSTM and CuDNN LSTM models on convergence speed. GILR-LSTM converged 6-10 times faster than CuDNN LSTM, despite CuDNN being highly optimized. GILR-LSTM is implemented in standard TensorFlow with a linear recurrence op. The GILR-LSTM model converges faster than CuDNN LSTM, indicating that non-linearities in LSTM may not be necessary for solving long-term dependency problems. GILR-LSTM can handle sequences up to one million elements, making it a significant advancement in sequential learning for neural networks. Linear RNNs have shown comparable prediction accuracy to non-linear RNNs, highlighting the importance of efficient computation in deep learning. Linear RNNs can achieve similar prediction accuracy to non-linear RNNs in a fraction of the training time. The framework of LS-RNNs is proposed to streamline sequential neural nets by utilizing linear recurrence as a parallelizable building block. This approach leads to significant speedups in solving sequential dependency problems. Parallel linear recurrence could be applied in training memory augmented models or creating new image filters for high-resolution images, potentially revolutionizing large-scale sequence modeling."
}