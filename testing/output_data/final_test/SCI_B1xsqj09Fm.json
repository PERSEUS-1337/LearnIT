{
    "title": "B1xsqj09Fm",
    "content": "Despite recent progress in generative image modeling, generating high-resolution, diverse samples from complex datasets like ImageNet remains challenging. By training Generative Adversarial Networks at a large scale and applying orthogonal regularization to the generator, a \"truncation trick\" can be used to control sample fidelity and variety. These modifications have led to models (BigGANs) that set a new state of the art in class-conditional image synthesis, achieving an Inception Score of 166.3 and Frechet Inception Distance of 9.6 on ImageNet at 128x128 resolution. In recent years, generative image modeling has advanced with Generative Adversarial Networks (GANs) leading the way. Despite progress, the current state of conditional ImageNet modeling falls short in fidelity compared to real data. The goal is to bridge this gap by improving GANs. We contribute to improving GANs by scaling models with more parameters and batch size, introducing architectural changes, and enhancing conditioning. Our modifications allow for fine-grained control over sample variety and fidelity using the \"truncation trick.\" We address instabilities in large scale GANs and propose techniques to reduce them, although complete training stability comes at a cost to performance. Our modifications substantially improve class-conditional GANs, achieving state-of-the-art results on ImageNet at various resolutions. BigGANs show significant improvements in Inception Score and Fr\u00e9chet Inception Distance. Additionally, the models are successfully trained on larger datasets like JFT-300M, with code and weights publicly available. The GAN objective involves finding a Nash equilibrium in a two-player min-max problem using latent variables drawn from a distribution. Training GANs for images requires stabilization techniques due to brittleness, leading to modifications in the objective function to encourage convergence. Researchers have focused on improving stability through various empirical and theoretical insights. Various techniques have been proposed to stabilize GAN training, such as gradient penalties, normalization, and Spectral Normalization. These methods aim to ensure that the discriminator provides gradients everywhere to the generator, improving stability and convergence. Studies have shown that employing Spectral Normalization in the generator can reduce the number of discriminator steps per iteration, leading to better performance. Further analysis is needed to understand the challenges in GAN training. In GAN training, various techniques like gradient penalties and Spectral Normalization have been proposed to stabilize the process. Additional works focus on architecture choices, such as SA-GAN and ProGAN for high-resolution GAN training. Conditional GANs and modifications like providing class information to G are also explored in the literature. In GAN training, techniques like gradient penalties and Spectral Normalization stabilize the process. Works focus on architecture choices, such as SA-GAN and ProGAN for high-resolution training. Conditional GANs and modifications like providing class information to G are also explored. Dumoulin et al. (2017) modify class conditioning in G by supplying classconditional gains and biases in BatchNorm layers. Miyato & Koyama (2018) condition D using cosine similarity between features and learned class embeddings. Evaluating implicit generative models is challenging. In this section, methods for scaling up GAN training are explored to improve performance with larger models and batches. The Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) are used as approximate measures of sample quality. The SA-GAN architecture is employed as a baseline, with class information provided to G and D for better conditioning. The optimization settings for training the GAN models include class-conditional BatchNorm and projection, with modifications to learning rates and steps per iteration. Moving averages of G's weights are used for evaluation, along with Orthogonal Initialization. Models are trained on Google TPUv3 Pod with BatchNorm statistics computed across all devices. Progressive growing is deemed unnecessary for 512x512 models. Increasing batch size by a factor of 8 improves IS by 46%, leading to better gradients for both networks. Further widening the layers by 50% doubles parameters and boosts IS by 21%, enhancing model capacity relative to dataset complexity. Training collapse is discussed in Section 4. The increased model capacity in the BigGAN-deep model improves training speed by 37% by using shared embeddings for conditional BatchNorm layers. Direct skip connections from the noise vector to multiple layers of G allow for the latent space to directly influence features at different resolutions and levels. In BigGAN-deep, skip connections from the noise vector to G allow for direct influence on features at different resolutions. Concatenating z with the conditional vector without splitting it into chunks provides a modest performance improvement and faster training speed. Previous works have mostly drawn z from N(0, I) or U[\u22121, 1], but we explore alternative priors in our implementation. The Truncation Trick involves sampling z from a truncated normal distribution, improving sample quality at the expense of sample variety. Reducing the threshold leads to samples approaching the mode of the output distribution. This technique allows for fine-tuning post-training. The Truncation Trick involves sampling z from a truncated normal distribution to balance sample quality and variety for a given G. Computing FID and IS for different thresholds results in a variety-fidelity curve similar to a precision-recall curve. IS increases with lower truncation thresholds, while FID initially improves but sharply drops as variety diminishes. Some larger models experience saturation artifacts when using truncated noise, requiring measures to ensure model amenability. To ensure model amenability to truncation, Orthogonal Regularization is used to enforce smoothness in the weight matrix W by removing diagonal terms and minimizing pairwise cosine similarity between filters. A small penalty of 10^-4 for \u03b2 values is found to be effective in improving model likelihood for truncation. Current GAN techniques enable scaling to large models and distributed training, with 60% of models amenable to truncation when trained with Orthogonal Regularization. Despite advancements, models still experience training collapse, requiring early stopping. Further investigation is needed to understand why stable settings at small scale become unstable at larger scales. At large scale, instabilities in training occur despite stability at small scale. Monitoring weight, gradient, and loss statistics reveals that the top three singular values of each weight matrix are most informative. Some layers, particularly the first layer in G, exhibit ill-behaved spectral norms that grow and explode at collapse. Spectral norms in training can lead to collapse, so additional conditioning on G is studied to counteract spectral explosion. Techniques like regularizing top singular values or clamping them prevent gradual increase and explosion, maintaining stability. The study focuses on the behavior of D in training, analyzing its weight spectra to stabilize training by imposing constraints. Unlike G, D's spectra show noise with well-behaved singular values that grow steadily but only jump at collapse. The spikes in D's spectra suggest large gradients periodically, but Frobenius norms remain smooth, indicating concentration on top singular directions. The study analyzes the noise in D's weight spectra during training, focusing on the top singular directions. To stabilize training, gradient penalties are employed, resulting in improved stability and smoothness in both G and D. However, performance degrades with a 45% reduction in IS. Reducing the penalty lessens degradation but leads to ill-behaved spectra, with a 20% reduction in IS even at the lowest penalty strength. During training, applying various regularization strategies such as Orthogonal Regularization, DropOut, and L2 can lead to improved stability but at the cost of performance. The discriminator's loss approaches zero during training but sharply increases at collapse, indicating potential overfitting to training data. Evaluating uncollapsed discriminators on ImageNet shows high training accuracy but potential memorization of training examples. The validation accuracy of the model falls in the range of 50-55%, indicating memorization of the training set. Stability in the model comes from the interaction between the generator and discriminator during adversarial training. Additional experiments and discussions are provided in the appendix. In experiments evaluating models on ImageNet ILSVRC 2012 at different resolutions, it is found that ensuring reasonable conditioning is necessary for training but not enough to prevent collapse. Enforcing stability by strongly constraining D leads to a significant performance cost. Better final performance can be achieved by relaxing conditioning and allowing collapse at later training stages. Our models were evaluated on ImageNet ILSVRC 2012 at 128\u00d7128, 256\u00d7256, and 512\u00d7512 resolutions. Results are presented in FIG2 and TAB3, showing improved performance compared to previous state-of-the-art models. Trade-offs between sample variety and quality were analyzed at different truncation settings, demonstrating the ability of our models to outperform in all scenarios. Our models, including BigGAN and BigGAN-deep, outperform previous state-of-the-art IS and FID scores. Results on JFT-300M at 256\u00d7256 resolution show improved sample quality with increased depth. The models were evaluated on ImageNet ILSVRC 2012 at different resolutions, demonstrating superior performance in all scenarios. Our observation that D overfits to the training set raises the question of whether G memorizes training points. Nearest neighbors analysis and interpolations show our model does not simply memorize data. Some failure modes are different from previous observations. The previous failures in image generation involve local artifacts, texture blobs, and class leakage. Some classes on ImageNet are more challenging for the model, with dogs being easier to generate than crowds. Results on a subset of JFT-300M dataset are also presented to validate the design choices for larger and more diverse datasets. The dataset contains 292M real-world images labeled with 8.5K categories. An Inception v2 classifier is used to compute IS and FID for GANs trained on this dataset. Results show that techniques like shared embedding and orthogonal regularization substantially improve model performance. Our results demonstrate that techniques like shared embedding and orthogonal regularization significantly enhance model performance on a dataset containing 292M real-world images labeled with 8.5K categories. Increasing model capacity to 128 base channels also leads to additional improvements, unlike in ImageNet GANs. Truncation plots for models trained on this dataset show that optimal truncation values range from 0.5 to 1, likely due to the intra-class variability and complexity of the image distribution. Our findings show that training models on JFT-300M dataset leads to stable performance, unlike ImageNet models. Scaling up to larger datasets may help alleviate GAN stability issues. Generative Adversarial Networks benefit from scaling up, improving fidelity and variety of generated samples, setting a new performance level among ImageNet GAN models. Our models have achieved a new level of performance among ImageNet GAN models, surpassing the state of the art by a significant margin. The BigGAN model utilizes the ResNet GAN architecture with modifications for improved stability and performance, including hierarchical latent spaces and skip connections for the latent vector z. The BigGAN-deep model utilizes hierarchical latent spaces and skip connections for the latent vector z, with each chunk concatenated to the shared class embedding and passed to a residual block for conditioning. The bias projections are zero-centered, while the gain projections are centered at 1. The full dimensionality of z varies based on image resolution, with 120 for 128 \u00d7 128, 140 for 256 \u00d7 256, and 160 for 512 \u00d7 512 images. The BigGAN-deep model incorporates skip connections and residual blocks with bottlenecks to preserve identity. In G, channels are reduced by retaining the first group, while in D, channels are increased by concatenating input channels with those produced by a 1 \u00d7 1 convolution. The discriminator configuration remains unchanged. BigGAN-deep models have a 1 \u00d7 1 convolution and are four times deeper than BigGAN. Despite the increased depth, they have fewer parameters due to bottleneck structure in residual blocks. BigGAN-deep models use attention at 64 \u00d7 64 resolution, channel width multiplier ch = 128, and z \u2208 R 128. Different learning rates are used for D and G in BigGAN-deep models. In the 256 and 512 \u00d7 512 models, two D steps per G step yielded the best results. An exponential moving average of G weights is used with a decay rate of 0.9999. Cross-replica BatchNorm is employed in G, and Spectral Normalization is used in both G and D. Training is done on a Google TPU v3 Pod with the number of cores proportional to the resolution. BatchNorm and Spectral Norm values are adjusted to address numerical issues. Data is preprocessed by cropping and rescaling. When sampling images with batch normalized classifier networks, using batch statistics instead of activation moments at test time can lead to performance variations based on test batch size. This issue is exacerbated when using exponential moving averages of G's weights for sampling. To address this, \"standing statistics\" are employed to compute activation statistics. To address issues with performance variations based on test batch size when sampling images with batch normalized classifier networks, \"standing statistics\" are employed to compute activation statistics. This involves running the network on CIFAR-10 and ImageNet datasets to achieve specific Inception Scores (IS) for different image sizes. The Inception classifier has an IS of 348 for training data and 241 for validation data. The discrepancy in scores is due to the high-confidence outputs from training data. Different truncation values are explored for latent distribution in the design process. In the design process, different latent distributions were explored, with Bernoulli {0, 1} and Censored Normal max (N (0, I), 0) found to work best without truncation. The choice of latent space dimensionality was also ablated, showing successful training with dimensions as low as z \u2208 R 8 and minimal performance drop with z \u2208 R 32. The text discusses different latent space distributions, including Bernoulli {0, 1} and Censored Normal max (N (0, I), 0), which were found to work best without truncation. These distributions aim to introduce sparsity and reflect the idea that certain latent features are sometimes present and sometimes not. The text discusses various latent space distributions, including Bernoulli {\u22121, 1}, Independent Categorical {\u22121, 0, 1}, and N (0, I) multiplied by Bernoulli {0, 1}. These distributions aim to introduce sparsity and allow for both discrete and continuous latent factors. The latents vary in intensity and performance, with some outperforming others by 15-20% in terms of IS. The text discusses different latent space distributions, including Bernoulli {\u22121, 1}, Independent Categorical {\u22121, 0, 1}, and N (0, I) combined with Bernoulli {0, 1}. Variance annealing is also mentioned, where the variance is sampled from N (0, \u03c3I) with \u03c3 annealed from 2 to 1 during training. Per-sample variable variance is introduced as N (0, \u03c3 i I), with \u03c3 i \u223c U[\u03c3 l , \u03c3 h ] independently for each sample. In this section, additional investigations into the stability of the models are presented and discussed. The symptoms of collapse are sudden and sharp, with sample quality dropping rapidly over a few hundred iterations. Collapse can be detected when the sample quality decreases from its peak to its lowest value. During training, collapse in the model can be detected when singular values in G explode, but there is no consistent threshold for this occurrence. Intervention experiments showed that increasing learning rates led to immediate collapse, even when taking checkpoints before collapse. Increasing learning rates (values) in G or D, or both, led to immediate collapse, even when doubling the rates. Changing momentum terms or resetting momentum vectors did not prevent collapse. Noise spikes in the spectra resemble an impulse response, possibly due to D memorizing training data. The hinge loss can lead to biased outputs in D if gradients attenuate to zero, causing misclassification of real examples. Using an unbounded loss like Wasserstein loss may prevent this issue. However, even with gradient penalties, models struggled to train stably with this loss. Adjusting the margin of the hinge loss was explored as a compromise. Adjusting the margin of the hinge loss can lead to more examples falling within the margin, contributing to the loss. Various techniques were explored but did not improve performance in the specific setup used. Reporting these negative results aims to save time for future work and provide a comprehensive view of attempts to enhance performance. In experiments with GANs, doubling the depth hampered performance. Sharing class embeddings between G and D initially helped but scaled poorly. Trying different normalization techniques like WeightNorm and Spectral Normalization also had negative effects on training. Adding BatchNorm to D was explored but did not improve performance. In experiments with GANs, adding BatchNorm to D did not improve performance. Varying the location of the attention block in G and D at 128\u00d7128 did not show noticeable benefits, while moving the attention block up one stage at 256\u00d7256 did. Using filter sizes of 5 in G only provided a small improvement but came at a high compute cost. Varying the dilation for convolutional filters in both G and D at 128\u00d7128 did not yield significant improvements. In experiments with GANs, varying the dilation for convolutional filters in both G and D at 128\u00d7128 did not yield significant improvements. However, even a small amount of dilation in either network degraded performance. Bilinear upsampling in G instead of nearest-neighbors upsampling also degraded performance. Some models experienced class-conditional mode collapse, where certain classes only output one or two samples due to large embeddings. Weight decay applied to shared embeddings worsened performance, with only very small values preventing class vectors from exploding. Higher-resolution models were more resilient to this issue. Our final models did not suffer from class-conditional mode collapse. Experimenting with MLPs and Spectral Normalization did not provide any benefits. Gradient norm clipping did not alleviate instability issues."
}