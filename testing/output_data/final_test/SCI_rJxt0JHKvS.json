{
    "title": "rJxt0JHKvS",
    "content": "In this paper, a graph neural network called Colored Local Iterative Procedure (CLIP) is introduced to improve the expressive power of Message Passing Neural Networks (MPNNs) by using colors to disambiguate identical node attributes. The method relies on separability to extend neural networks into universal representations and is capable of capturing structural characteristics that traditional MPNNs fail to distinguish. CLIP is shown to be state-of-the-art on benchmark graph classification datasets. Theoretical framework analyzing the expressive power of deep learning architectures, proving multi-layer perceptrons can approximate any continuous function on a compact set. This result justifies neural networks' strong approximation capabilities and provides insights into their generalization capabilities. In this paper, the authors introduce Colored Local Iterative Procedure 1 (CLIP) to enhance Message Passing Neural Networks (MPNNs) by incorporating a coloring scheme. Theoretical and experimental evidence shows that this simple addition improves the flexibility and power of graph representations. The authors introduce CLIP to enhance MPNNs with a novel node coloring scheme, achieving state-of-the-art results on benchmark datasets. They provide a mathematical definition for universal graph representations and show that most current representations are not sufficiently expressive. Section 5 introduces CLIP, a universal extension of MPNNs, which achieves state-of-the-art accuracies on graph classification tasks and outperforms competitors on graph property testing. Initial works used recurrent neural networks for graphs, while more generic graph neural networks were later categorized into spectral methods and message passing neural networks. Message passing neural networks (MPNNs) are a category of graph representation methods that aggregate neighborhood information through a local iterative process. Recent studies have shown that MPNNs are as expressive as the Weisfeiler-Lehman test for graph isomorphism. Several works have proposed MPNN extensions to improve their expressivity and move towards universality. Recent studies have shown that some graph representations are as powerful as the k-WL test or provide universal graph representations under certain assumptions. More powerful GNNs have been studied and benchmarked on real datasets, addressing problems that classical MPNNs cannot handle. This work aims to provide a more general and powerful universality result, matching the original definition for MLPs. The theoretical tools used to design a universal graph representation are presented, showing that separable representations can capture all relevant information about an object. The text discusses universal representations in topological spaces, functions, group actions, permutation matrices, and Hausdorff spaces commonly encountered in machine learning. It assumes Hausdorff spaces where distinct points can be separated by disjoint open sets. The input information consists of objects like vectors, images, graphs, or temporal data. The text discusses vector representations for machine learning tasks, using functions to map elements to d-dimensional vectors. It focuses on supervised representation learning with a class of vector representations and neural network architectures that can output vectors of any size. The goal is to find a generic class F that leads to good accuracy on the learning task. The text discusses universal representations for vector spaces and neural networks, emphasizing the challenge of obtaining universal representations for graphs and structured objects due to their complex nature and invariance to transformations. In this paper, the concept of separability is introduced as a key topological property that may lead to universal representations of complex structures like graphs. Separability requires that representations are expressive enough to separate dissimilar elements, making it a necessary condition for universality. Proposition 1 states that a universal representation F of X must separate points of X, which is crucial for designing neural network architectures that can be extended into universal representations. Theorem 2 shows that a class of vector representations F can be composed with a universal approximation M_d of R^d to become universal, with stability by concatenation verified by most neural network architectures. The proof relies on the Stone-Weierstrass theorem, requiring continuity, separability, and the functions being an algebra. The class of functions is an algebra, and composing a separable and concatenable representation with a universal representation leads to the applicability of the Stone-Weierstrass theorem. MLPs are universal representations of R^d, allowing for the design of universal representations of more complex object spaces by creating a separable representation and combining it with a simple MLP. This method is used to design universal graph and neighborhood representations. In this section, a proper definition for graphs with node attributes is provided, showing that message passing neural networks are not universal. The space of graphs with node attributes is defined as a quotient space where adjacency matrix A and node attributes v are acted upon by permutation matrices. The focus is on graphs of maximum size n max, allowing for functions on graphs of different sizes. Graph m = n\u2264nmax Graph m,n is defined to allow for functions on graphs of different sizes without infinite dimensional spaces. A message passing neural network (MPNN) consists of three phases to create node representations and a final graph representation. The Colored Local Iterative Procedure (CLIP) is an extension of MPNNs that uses colors to differentiate identical nodes, addressing the limitations of MPNNs in creating universal representations. The Colored Local Iterative Procedure (CLIP) is an extension of MPNNs that uses colors to differentiate identical nodes, capturing complex structural graph characteristics. CLIP involves coloring graphs, neighborhood aggregation, and combining vector representations for a final output. The coloring scheme distinguishes non-isomorphic graphs by coloring nodes with identical attributes, inspired by classical graph isomorphism algorithms. The Colored Local Iterative Procedure (CLIP) extends MPNNs by using colors to differentiate identical nodes in graphs. Nodes are partitioned into groups based on attributes, and each group is assigned a distinct color. This coloring scheme helps distinguish non-isomorphic graphs by coloring nodes with identical attributes. The CLIP algorithm enhances MPNNs by incorporating a coloring scheme to differentiate identical node attributes in graphs. Node representations are initialized with their attributes concatenated with a randomly selected coloring, and an aggregation function using MLPs with continuous non-polynomial activation functions is applied. The proposed aggregation scheme is related to DeepSet, and the universality of the architecture is proven. Additional details and proof of universality can be found in Appendix C. The CLIP algorithm utilizes a coloring scheme to enhance MPNNs for graph data. Node representations are initialized with attributes concatenated with a random coloring, and an aggregation function using MLPs with continuous non-polynomial activation functions is applied. The approach is extendable to various types of graphs with different attributes. The number of colorings chosen impacts the randomness of the CLIP representation. The \u221e-CLIP algorithm with one local iteration (T = 1) is a universal representation of the space Graph m of graphs with node attributes. The proof relies on showing that \u221e-CLIP is separable and applying Corollary 1, achieved by fixing a coloring on one graph and identifying all nodes and edges of the second graph using dissimilar pairs. This counter-intuitive result is due to all nodes being identifiable by their color, requiring only one local iteration for universality. The 1-CLIP algorithm with one local iteration is a random representation that is a universal representation of the space Graph m of graphs with node attributes. The proof relies on using \u221e-CLIP on the augmented node attributes v i = (v i , c i), where all node attributes are different by design. The complexity of the algorithm is proportional to the number of edges of the graph E, the number of steps T, and the number of chosen colorings k. The complexity of the algorithm is in O(kET) due to the exponential dependency in the number of possible colorings for a given graph. Universal graph representations face challenges in solving the graph isomorphism problem, which remains quasi-polynomial in general. Creating a universal graph representation with polynomial complexity for all graphs and functions is highly unlikely. In this section, empirical evidence demonstrates the practical efficiency of CLIP and its relaxation in solving a long-standing problem in theoretical computer science. Two sets of experiments were conducted to compare CLIP with state-of-the-art methods in supervised learning settings using real-world and synthetic datasets. The experiments followed a standardized protocol with 10-fold cross-validation and hyper-parameter optimization. Benchmark datasets from social networks and bioinformatics databases were used, with details available in the appendix. In Appendix E, the experimental setup details are provided. One-hot encodings of node degrees are used as node attributes for IMDBb and IMDBm datasets. CLIP is compared with six baseline algorithms for graph classification. The results are presented in Table 1 following the experimental protocol of Xu et al. (2019). CLIP achieved state-of-the-art performance on five benchmark datasets, showing consistency across all datasets. The addition of colors did not improve accuracy except for the MUTAG dataset. In three out of five datasets, recent state-of-the-art algorithms did not have significantly better results. The study found that recent algorithms did not outperform older methods like WL in graph property testing. CLIP was evaluated for identifying structural graph properties and compared against GIN for binary classification. Table 2 shows that CLIP can capture structural information such as connectivity, bipartiteness, triangle-freeness, and circular skip links, outperforming MPNN variants like GIN, Ring-GNN, and RP-GIN in identifying these properties. CLIP outperforms MPNN variants in identifying graph properties like triangle-freeness and circular skip links. Increasing permutations and colorings in CLIP lead to higher accuracies, with as little as k = 16 colorings capturing different graph properties almost perfectly. CLIP is the first universal graph representation, proven to be state-of-the-art in graph classification and property testing using separable neural networks. CLIP is state-of-the-art for graph classification and property testing tasks, especially suited for complex structural information. It can be extended to various data structures like directed, weighted, or labeled graphs. Future work includes detailed quantitative results based on architecture parameters like the number of colors or hops. Proof of Theorem 2 relies on the Stone-Weierstrass theorem. The Stone-Weierstrass theorem is used to prove Theorem 2 in the context of graph classification and property testing tasks. The proof involves showing that a subset A 0 is uniformly dense in C(K, R) and that A is dense in A 0. This is done by demonstrating that A 0 contains zero and all constants, and that it separates the points of X. The Stone-Weierstrass theorem is utilized to prove Theorem 2 in graph classification and property testing tasks. It is shown that subset A 0 is uniformly dense in C(K, R) and A is dense in A 0 by demonstrating A 0 contains zero and all constants, and separates the points of X. Additionally, Proposition 1 is proven by assuming the existence of x, y in X such that for all f in F 1, f(x) = f(y), leading to a contradiction with universality. X is a topological set with G as a group of transformations acting on it. The orbits of X under the action of group G are sets Gx = {g \u00b7 x : g \u2208 G}. X /G is the quotient space of orbits, defined by the equivalence relation x \u223c y \u21d0\u21d2 \u2203g \u2208 G s.t. x = g \u00b7 y. Graphs with node attributes can be defined using label permutation invariance, resulting in Hausdorff spaces. Proposition 2 states that the orbit space X /G is Hausdorff when G is a finite group acting on an Hausdorff space X. The application of Corollary 1 in designing universal representations for node neighborhoods is explained. Each local aggregation step in CLIP involves inputting a node's representation and its neighbors' vector representations. The set of node neighborhoods for m-dimensional node attributes is defined using permutation matrices. The difficulty in designing universal neighborhood representations lies in the permutation invariance of node neighborhoods. Various deep learning architectures, such as PointNet and DeepSet, offer provably universal solutions for permutation invariant sets. By combining a separable permutation invariant network with an MLP, information from nodes and their neighborhoods can be effectively aggregated. This approach is similar to DeepSet but emphasizes the applicability of general universality theorems in different settings. The universality theorems in Section 3 are easily applicable in various settings, including permutation invariant networks. The aggregation step of CLIP uses a permutation invariant set representation with MLPs and continuous non-polynomial activation functions. The set representation described in Eq. (9) proves separability and universality. CLIP is continuous and concatenable due to the continuous activation functions of the MLPs. The node aggregation step is a universal set representation capable of approximating any continuous function. The node aggregation step in CLIP is a universal set representation that can approximate any continuous function by replacing it with a continuous function \u03c6. This is achieved through a density argument and the use of a coloring function. The existence of a continuous function \u03c6 is assured by Urysohn's lemma, leading to the desired result with an \u03b5-approximation of \u03c6 as NODEAGGREGATION (1). The characteristics of benchmark graph classification datasets used in Section 6.1 are summarized in Table 3. Additional information is provided on social network datasets such as IMDBb and IMDBm. Social Network Datasets (IMDBb, IMDBm) involve collaborations between actors/actresses in movies. IMDBb is for single-class classification, while IMDBm is multi-class. One-hot encodings of node degrees are used as node attribute vectors. Bio-informatics Datasets (MUTAG, PROTEINS, PTC) include mutagenic compounds, protein structures, and carcinogenicity data for rats. The curr_chunk discusses the experimentation protocol used for optimizing CLIP hyperparameters in bio-informatics datasets. It includes details such as using 2-layer MLPs, grid search for hyperparameters, and testing various settings like hidden units, colorings, MPNN layers, batch size, and epochs. The results are reported in Table 1 based on the accuracy of six baselines. The performance of CLIP is evaluated by increasing the number of colorings k, showing a small increase in performance and reduction of variances. However, no model stands out significantly. In Section 6.2, the expressive power of CLIP is tested on synthetic datasets to distinguish basic graph properties that classical MPNNs cannot. The 20-node graphs are generated using the Erd\u00f6s-R\u00e9nyi model. The text discusses the generation of synthetic datasets for various graph properties using the Erd\u00f6s-R\u00e9nyi model. Different tasks such as connectivity, bipartiteness, triangle-freeness, and circular skip links are considered. The protocol involves creating random graphs with a specific property and then modifying them to remove that property while maintaining other structural characteristics. The dataset generation process involves creating graphs with specific properties like connectivity, bipartiteness, and triangle-freeness using the Erd\u00f6s-R\u00e9nyi model. Positive and negative samples are constructed by adding or removing edges while preserving other structural characteristics. The Circular Skip Links dataset consists of 150 undirected regular graphs with 41 nodes. Each graph is denoted by Gn,k, where edges are defined by |i \u2212 j| \u2261 1 or k (mod n). The objective is a 10-class multiclass classification task to classify graphs based on their isomorphism class. In evaluating CLIP and its competitors GIN and RP-GIN, the architecture implementation of GIN followed the best performing architecture by Xu et al. (2019). The configurations included fixed hyper-parameters such as 2-layer MLPs, Adam optimizer with initial learning rate of 0.001, and a scheduler decaying the learning rate by 0.5 every 50 epochs. Other hyper-parameters were optimized during the experimentation protocol. During the experimentation, hyper-parameters were optimized, including the number of hidden units within {16, 32, 64} for 50 epochs."
}